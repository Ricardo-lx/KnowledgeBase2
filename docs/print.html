<!DOCTYPE HTML>
<html lang="en" class="sidebar-visible no-js light">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>KnowledgeBase</title>
        <meta name="robots" content="noindex" />


        <!-- Custom HTML head -->
        
        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff" />

        <link rel="icon" href="favicon.svg">
        <link rel="shortcut icon" href="favicon.png">
        <link rel="stylesheet" href="css/variables.css">
        <link rel="stylesheet" href="css/general.css">
        <link rel="stylesheet" href="css/chrome.css">
        <link rel="stylesheet" href="css/print.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="FontAwesome/css/font-awesome.css">
        <link rel="stylesheet" href="fonts/fonts.css">

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" href="highlight.css">
        <link rel="stylesheet" href="tomorrow-night.css">
        <link rel="stylesheet" href="ayu-highlight.css">

        <!-- Custom theme stylesheets -->

    </head>
    <body>
    <div id="body-container">
        <!-- Provide site root to javascript -->
        <script>
            var path_to_root = "";
            var default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? "navy" : "light";
        </script>

        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script>
            try {
                var theme = localStorage.getItem('mdbook-theme');
                var sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script>
            var theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            var html = document.querySelector('html');
            html.classList.remove('no-js')
            html.classList.remove('light')
            html.classList.add(theme);
            html.classList.add('js');
        </script>

        <!-- Hide / unhide sidebar before it is displayed -->
        <script>
            var html = document.querySelector('html');
            var sidebar = null;
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            } else {
                sidebar = 'hidden';
            }
            html.classList.remove('sidebar-visible');
            html.classList.add("sidebar-" + sidebar);
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <div class="sidebar-scrollbox">
                <ol class="chapter"><li class="chapter-item expanded "><a href="HaskellProgramming/HaskellProgramming.html"><strong aria-hidden="true">1.</strong> Haskell Programming</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="HaskellProgramming/part1.html"><strong aria-hidden="true">1.1.</strong> Part1</a></li><li class="chapter-item expanded "><a href="HaskellProgramming/part2.html"><strong aria-hidden="true">1.2.</strong> Part2</a></li><li class="chapter-item expanded "><a href="HaskellProgramming/part3.html"><strong aria-hidden="true">1.3.</strong> Part3</a></li><li class="chapter-item expanded "><a href="HaskellProgramming/part4.html"><strong aria-hidden="true">1.4.</strong> Part4</a></li><li class="chapter-item expanded "><a href="HaskellProgramming/part5.html"><strong aria-hidden="true">1.5.</strong> Part5</a></li></ol></li><li class="chapter-item expanded "><a href="csapp/csapp.html"><strong aria-hidden="true">2.</strong> csapp</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="csapp/part1.html"><strong aria-hidden="true">2.1.</strong> Part1</a></li><li class="chapter-item expanded "><a href="csapp/part2.html"><strong aria-hidden="true">2.2.</strong> Part2</a></li><li class="chapter-item expanded "><a href="csapp/part3.html"><strong aria-hidden="true">2.3.</strong> Part3</a></li><li class="chapter-item expanded "><a href="csapp/part4.html"><strong aria-hidden="true">2.4.</strong> Part4</a></li><li class="chapter-item expanded "><a href="csapp/part5.html"><strong aria-hidden="true">2.5.</strong> Part5</a></li><li class="chapter-item expanded "><a href="csapp/part6.html"><strong aria-hidden="true">2.6.</strong> Part6</a></li><li class="chapter-item expanded "><a href="csapp/part7.html"><strong aria-hidden="true">2.7.</strong> Part7</a></li><li class="chapter-item expanded "><a href="csapp/part8.html"><strong aria-hidden="true">2.8.</strong> Part8</a></li><li class="chapter-item expanded "><a href="csapp/part9.html"><strong aria-hidden="true">2.9.</strong> Part9</a></li><li class="chapter-item expanded "><a href="csapp/part10.html"><strong aria-hidden="true">2.10.</strong> Part10</a></li></ol></li><li class="chapter-item expanded "><a href="midjourney/combined_html_page.html"><strong aria-hidden="true">3.</strong> midjourney</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="midjourney/mjprompt.html"><strong aria-hidden="true">3.1.</strong> MjPrompt</a></li></ol></li></ol>
            </div>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle"></div>
        </nav>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky bordered">
                    <div class="left-buttons">
                        <button id="sidebar-toggle" class="icon-button" type="button" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </button>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="light">Light</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                    </div>

                    <h1 class="menu-title">KnowledgeBase</h1>

                    <div class="right-buttons">
                        <a href="print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>

                    </div>
                </div>


                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script>
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <p><strong>HaskellProgramming</strong></p>
<div style="break-before: page; page-break-before: always;"></div><p>i
Reader feedback
“Astonishingly insightful examples. This book is a lot like
having a good teacher — it never fails to provide the low-end
information even though I have already moved on. So just
like a good teacher isn’t presumptuous in what I’m supposed
to know (which might force me to try and save face in case I
do not, yet), information conveniently resurfaces.” – David
Deutsch
“When @haskellbook is done, it will be an unexpected mile-
stone for #haskell. There will forever be Haskell before, and
Haskell after.” – Jason Kuhrt
“I feel safe recommending Haskell to beginners now that
@haskellbook is available, which is very beginner friendly” –
Gabriel Gonzalez
“”Structure and Interpretation of Computer Programs” has
its credit, but @haskellbook is now my #1 recommendation
for FP beginners.” – Irio Musskopf
“The book is long, but not slow — a large fraction of it is
made up of examples and exercises. You can tell it’s written
by someone who’s taught Haskell to programmers before.” –
Christopher Jones</p>
<p>ii
“I already have a lot of experience with Haskell, but I’ve
never felt confident in it the way this book has made me feel.”
– Alain O’Dea
“Real deal with @haskellbook is that you don’t just learn
Haskell; you get a hands on experience as to why functional
programming works.” – George Makrydakis
“One of my goals this year is to evangelize @haskellbook
and @HaskellForMac. I think these tools will make anyone
who uses them better. I want to get comfortable with it so that
I can shift how I think about Swift.” – Janie Clayton</p>
<p>Contents
Reader feedback . . . . . . . . . . . . . . . . . . . . . . . . . . i
Contents iii
Authors’ preface . . . . . . . . . . . . . . . . . . . . . . . . . . . xx
Acknowledgements . . . . . . . . . . . . . . . . . . . . . . . . xxv
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . xxix
Why This Book . . . . . . . . . . . . . . . . . . . . . . . . . . . xxix
A few words to new programmers . . . . . . . . . . . . . . . xxxiv
Haskevangelism . . . . . . . . . . . . . . . . . . . . . . . . . . . xxxv
What’s in this book? . . . . . . . . . . . . . . . . . . . . . . . . xxxix
Best practices for examples and exercises . . . . . . . . . . xliii
1 All You Need is Lambda 1
1.1 All You Need is Lambda . . . . . . . . . . . . . . . 2
1.2 What is functional programming? . . . . . . . . 2
1.3 What is a function? . . . . . . . . . . . . . . . . . . 4
1.4 The structure of lambda terms . . . . . . . . . . 7
1.5 Beta reduction . . . . . . . . . . . . . . . . . . . . . 10
1.6 Multiple arguments . . . . . . . . . . . . . . . . . . 15
1.7 Evaluation is simplification . . . . . . . . . . . . . 20
1.8 Combinators . . . . . . . . . . . . . . . . . . . . . . 21
iii</p>
<p>CONTENTS iv
1.9 Divergence . . . . . . . . . . . . . . . . . . . . . . . 23
1.10 Summary . . . . . . . . . . . . . . . . . . . . . . . . 24
1.11 Chapter Exercises . . . . . . . . . . . . . . . . . . . 25
1.12 Answers . . . . . . . . . . . . . . . . . . . . . . . . . 27
1.13 Definitions . . . . . . . . . . . . . . . . . . . . . . . . 31
1.14 Follow-up resources . . . . . . . . . . . . . . . . . 33
2 Hello, Haskell! 34
2.1 Hello, Haskell . . . . . . . . . . . . . . . . . . . . . . 35
2.2 Interacting with Haskell code . . . . . . . . . . . 36
2.3 Understanding expressions . . . . . . . . . . . . . 40
2.4 Functions . . . . . . . . . . . . . . . . . . . . . . . . 43
2.5 Evaluation . . . . . . . . . . . . . . . . . . . . . . . . 47
2.6 Infix operators . . . . . . . . . . . . . . . . . . . . . 50
2.7 Declaring values . . . . . . . . . . . . . . . . . . . . 57
2.8 Arithmetic functions in Haskell . . . . . . . . . . 67
2.9 Parenthesization . . . . . . . . . . . . . . . . . . . . 78
2.10 Let and where . . . . . . . . . . . . . . . . . . . . . 85
2.11 Chapter Exercises . . . . . . . . . . . . . . . . . . . 90
2.12 Definitions . . . . . . . . . . . . . . . . . . . . . . . . 94
2.13 Follow-up resources . . . . . . . . . . . . . . . . . 96
3 Strings 97
3.1 Printing strings . . . . . . . . . . . . . . . . . . . . . 98
3.2 A first look at types . . . . . . . . . . . . . . . . . . 98
3.3 Printing simple strings . . . . . . . . . . . . . . . . 100</p>
<p>CONTENTS v
3.4 Top-level versus local definitions . . . . . . . . . 107
3.5 Types of concatenation functions . . . . . . . . . 110
3.6 Concatenation and scoping . . . . . . . . . . . . . 115
3.7 More list functions . . . . . . . . . . . . . . . . . . 119
3.8 Chapter Exercises . . . . . . . . . . . . . . . . . . . 122
3.9 Definitions . . . . . . . . . . . . . . . . . . . . . . . . 128
4 Basic datatypes 131
4.1 Basic Datatypes . . . . . . . . . . . . . . . . . . . . 132
4.2 What are types? . . . . . . . . . . . . . . . . . . . . 133
4.3 Anatomy of a data declaration . . . . . . . . . . . 133
4.4 Numeric types . . . . . . . . . . . . . . . . . . . . . 137
4.5 Comparing values . . . . . . . . . . . . . . . . . . . 147
4.6 Go on and Bool me . . . . . . . . . . . . . . . . . . 152
4.7 Tuples . . . . . . . . . . . . . . . . . . . . . . . . . . 160
4.8 Lists . . . . . . . . . . . . . . . . . . . . . . . . . . . . 165
4.9 Chapter Exercises . . . . . . . . . . . . . . . . . . . 167
4.10 Definitions . . . . . . . . . . . . . . . . . . . . . . . . 172
4.11 Names and variables . . . . . . . . . . . . . . . . . 175
5 Types 178
5.1 Types . . . . . . . . . . . . . . . . . . . . . . . . . . . 179
5.2 What are types for? . . . . . . . . . . . . . . . . . . 180
5.3 How to read type signatures . . . . . . . . . . . . 182
5.4 Currying . . . . . . . . . . . . . . . . . . . . . . . . . 192
5.5 Polymorphism . . . . . . . . . . . . . . . . . . . . . 208</p>
<p>CONTENTS vi
5.6 Type inference . . . . . . . . . . . . . . . . . . . . . 217
5.7 Asserting types for declarations . . . . . . . . . . 222
5.8 Chapter Exercises . . . . . . . . . . . . . . . . . . . 225
5.9 Definitions . . . . . . . . . . . . . . . . . . . . . . . . 239
5.10 Follow-up resources . . . . . . . . . . . . . . . . . 246
6 Typeclasses 247
6.1 Typeclasses . . . . . . . . . . . . . . . . . . . . . . . 248
6.2 What are typeclasses? . . . . . . . . . . . . . . . . . 248
6.3 Back to Bool . . . . . . . . . . . . . . . . . . . . . . 250
6.4 Eq . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 252
6.5 Writing typeclass instances . . . . . . . . . . . . . 257
6.6 Num . . . . . . . . . . . . . . . . . . . . . . . . . . . . 273
6.7 Type-defaulting typeclasses . . . . . . . . . . . . 278
6.8 Ord . . . . . . . . . . . . . . . . . . . . . . . . . . . . 284
6.9 Enum . . . . . . . . . . . . . . . . . . . . . . . . . . . 294
6.10 Show . . . . . . . . . . . . . . . . . . . . . . . . . . . 296
6.11 Read . . . . . . . . . . . . . . . . . . . . . . . . . . . . 303
6.12 Instances are dispatched by type . . . . . . . . . 304
6.13 Gimme more operations . . . . . . . . . . . . . . 309
6.14 Chapter Exercises . . . . . . . . . . . . . . . . . . . 314
6.15 Chapter Definitions . . . . . . . . . . . . . . . . . . 323
6.16 Typeclass inheritance, partial . . . . . . . . . . . 326
6.17 Follow-up resources . . . . . . . . . . . . . . . . . 326
7 More functional patterns 328</p>
<p>CONTENTS vii
7.1 Make it func-y . . . . . . . . . . . . . . . . . . . . . 329
7.2 Arguments and parameters . . . . . . . . . . . . . 329
7.3 Anonymous functions . . . . . . . . . . . . . . . . 339
7.4 Pattern matching . . . . . . . . . . . . . . . . . . . 344
7.5 Case expressions . . . . . . . . . . . . . . . . . . . . 360
7.6 Higher-order functions . . . . . . . . . . . . . . . 365
7.7 Guards . . . . . . . . . . . . . . . . . . . . . . . . . . 377
7.8 Function composition . . . . . . . . . . . . . . . . 387
7.9 Pointfree style . . . . . . . . . . . . . . . . . . . . . 392
7.10 Demonstrating composition . . . . . . . . . . . . 396
7.11 Chapter Exercises . . . . . . . . . . . . . . . . . . . 400
7.12 Chapter Definitions . . . . . . . . . . . . . . . . . . 406
7.13 Follow-up resources . . . . . . . . . . . . . . . . . 417
8 Recursion 419
8.1 Recursion . . . . . . . . . . . . . . . . . . . . . . . . 420
8.2 Factorial! . . . . . . . . . . . . . . . . . . . . . . . . . 421
8.3 Bottom . . . . . . . . . . . . . . . . . . . . . . . . . . 431
8.4 Fibonacci numbers . . . . . . . . . . . . . . . . . . 435
8.5 Integral division from scratch . . . . . . . . . . . 441
8.6 Chapter Exercises . . . . . . . . . . . . . . . . . . . 448
8.7 Definitions . . . . . . . . . . . . . . . . . . . . . . . . 455
9 Lists 457
9.1 Lists . . . . . . . . . . . . . . . . . . . . . . . . . . . . 458
9.2 The list datatype . . . . . . . . . . . . . . . . . . . . 458</p>
<p>CONTENTS viii
9.3 Pattern matching on lists . . . . . . . . . . . . . . 460
9.4 List’s syntactic sugar . . . . . . . . . . . . . . . . . 464
9.5 Using ranges to construct lists . . . . . . . . . . . 465
9.6 Extracting portions of lists . . . . . . . . . . . . . 469
9.7 List comprehensions . . . . . . . . . . . . . . . . . 477
9.8 Spines and nonstrict evaluation . . . . . . . . . . 485
9.9 Transforming lists of values . . . . . . . . . . . . 500
9.10 Filtering lists of values . . . . . . . . . . . . . . . . 511
9.11 Zipping lists . . . . . . . . . . . . . . . . . . . . . . . 513
9.12 Chapter Exercises . . . . . . . . . . . . . . . . . . . 517
9.13 Definitions . . . . . . . . . . . . . . . . . . . . . . . . 526
9.14 Follow-up resources . . . . . . . . . . . . . . . . . 529
10 Folding lists 530
10.1 Folds . . . . . . . . . . . . . . . . . . . . . . . . . . . 531
10.2 Bringing you into the fold . . . . . . . . . . . . . . 531
10.3 Recursive patterns . . . . . . . . . . . . . . . . . . . 534
10.4 Fold right . . . . . . . . . . . . . . . . . . . . . . . . 535
10.5 Fold left . . . . . . . . . . . . . . . . . . . . . . . . . 548
10.6 How to write fold functions . . . . . . . . . . . . . 561
10.7 Folding and evaluation . . . . . . . . . . . . . . . . 568
10.8 Summary . . . . . . . . . . . . . . . . . . . . . . . . 571
10.9 Scans . . . . . . . . . . . . . . . . . . . . . . . . . . . 573
10.10 Chapter Exercises . . . . . . . . . . . . . . . . . . . 578
10.11 Definitions . . . . . . . . . . . . . . . . . . . . . . . . 585
10.12 Follow-up resources . . . . . . . . . . . . . . . . . 589</p>
<p>CONTENTS ix
11 Algebraic datatypes 590
11.1 Algebraic datatypes . . . . . . . . . . . . . . . . . . 591
11.2 Data declarations review . . . . . . . . . . . . . . . 592
11.3 Data and type constructors . . . . . . . . . . . . . 594
11.4 Type constructors and kinds . . . . . . . . . . . . 597
11.5 Data constructors and values . . . . . . . . . . . . 599
11.6 What’s a type and what’s data? . . . . . . . . . . . 605
11.7 Data constructor arities . . . . . . . . . . . . . . . 611
11.8 What makes these datatypes algebraic? . . . . . 614
11.9 newtype . . . . . . . . . . . . . . . . . . . . . . . . . 620
11.10 Sum types . . . . . . . . . . . . . . . . . . . . . . . . 627
11.11 Product types . . . . . . . . . . . . . . . . . . . . . . 631
11.12 Normal form . . . . . . . . . . . . . . . . . . . . . . 636
11.13 Constructing and deconstructing values . . . . 642
11.14 Function type is exponential . . . . . . . . . . . . 667
11.15 Higher-kinded datatypes . . . . . . . . . . . . . . 674
11.16 Lists are polymorphic . . . . . . . . . . . . . . . . 677
11.17 Binary Tree . . . . . . . . . . . . . . . . . . . . . . . 681
11.18 Chapter Exercises . . . . . . . . . . . . . . . . . . . 690
11.19 Definitions . . . . . . . . . . . . . . . . . . . . . . . . 703
12 Signaling adversity 704
12.1 Signaling adversity . . . . . . . . . . . . . . . . . . 705
12.2 How I learned to stop worrying and love Nothing 705
12.3 Bleating either . . . . . . . . . . . . . . . . . . . . . 709
12.4 Kinds, a thousand stars in your types . . . . . . 720</p>
<p>CONTENTS x
12.5 Chapter Exercises . . . . . . . . . . . . . . . . . . . 732
12.6 Definitions . . . . . . . . . . . . . . . . . . . . . . . . 748
13 Building projects 750
13.1 Modules . . . . . . . . . . . . . . . . . . . . . . . . . 751
13.2 Making packages with Stack . . . . . . . . . . . . 753
13.3 Working with a basic project . . . . . . . . . . . . 754
13.4 Making our project a library . . . . . . . . . . . . 759
13.5 Module exports . . . . . . . . . . . . . . . . . . . . 762
13.6 More on importing modules . . . . . . . . . . . . 765
13.7 Making our program interactive . . . . . . . . . 773
13.8 do syntax and IO . . . . . . . . . . . . . . . . . . . . 779
13.9 Hangman game . . . . . . . . . . . . . . . . . . . . 784
13.10 Step One: Importing modules . . . . . . . . . . . 787
13.11 Step Two: Generating a word list . . . . . . . . . 793
13.12 Step Three: Making a puzzle . . . . . . . . . . . . 798
13.13 Adding a newtype . . . . . . . . . . . . . . . . . . . 810
13.14 Chapter exercises . . . . . . . . . . . . . . . . . . . 811
13.15 Follow-up resources . . . . . . . . . . . . . . . . . 815
14 Testing 817
14.1 Testing . . . . . . . . . . . . . . . . . . . . . . . . . . 818
14.2 A quick tour of testing for the uninitiated . . . 819
14.3 Conventional testing . . . . . . . . . . . . . . . . . 821
14.4 Enter QuickCheck . . . . . . . . . . . . . . . . . . . 833
14.5 Morse code . . . . . . . . . . . . . . . . . . . . . . . 846</p>
<p>CONTENTS xi
14.6 Arbitrary instances . . . . . . . . . . . . . . . . . . 863
14.7 Chapter Exercises . . . . . . . . . . . . . . . . . . . 875
14.8 Definitions . . . . . . . . . . . . . . . . . . . . . . . . 885
14.9 Follow-up resources . . . . . . . . . . . . . . . . . 886
15 Monoid, Semigroup 887
15.1 Monoids and semigroups . . . . . . . . . . . . . . 888
15.2 What we talk about when we talk about algebras 889
15.3 Monoid . . . . . . . . . . . . . . . . . . . . . . . . . . 890
15.4 How Monoid is defined in Haskell . . . . . . . . 892
15.5 Examples of using Monoid . . . . . . . . . . . . . 893
15.6 Why Integer doesn’t have a Monoid . . . . . . . 895
15.7 Why bother? . . . . . . . . . . . . . . . . . . . . . . 901
15.8 Laws . . . . . . . . . . . . . . . . . . . . . . . . . . . . 903
15.9 Diﬀerent instance, same representation . . . . . 908
15.10 Reusing algebras by asking for algebras . . . . . 911
15.11 Madness . . . . . . . . . . . . . . . . . . . . . . . . . 923
15.12 Better living through QuickCheck . . . . . . . . 925
15.13 Semigroup . . . . . . . . . . . . . . . . . . . . . . . . 936
15.14 Strength can be weakness . . . . . . . . . . . . . . 941
15.15 Chapter exercises . . . . . . . . . . . . . . . . . . . 944
15.16 Definitions . . . . . . . . . . . . . . . . . . . . . . . . 955
15.17 Follow-up resources . . . . . . . . . . . . . . . . . 956
16 Functor 957
16.1 Functor . . . . . . . . . . . . . . . . . . . . . . . . . . 958</p>
<p>CONTENTS xii
16.2 What’s a functor? . . . . . . . . . . . . . . . . . . . 959
16.3 There’s a whole lot of fmapgoin’ round . . . . . 962
16.4 Let’s talk about 𝑓, baby . . . . . . . . . . . . . . . . 965
16.5 Functor Laws . . . . . . . . . . . . . . . . . . . . . . 979
16.6 The Good, the Bad, and the Ugly . . . . . . . . . 981
16.7 Commonly used functors . . . . . . . . . . . . . . 987
16.8 Transforming the unapplied type argument . . 1005
16.9 QuickChecking Functor instances . . . . . . . . 1010
16.10 Exercises: Instances of Func . . . . . . . . . . . . 1014
16.11 Ignoring possibilities . . . . . . . . . . . . . . . . . 1015
16.12 A somewhat surprising functor . . . . . . . . . . 1024
16.13 More structure, more functors . . . . . . . . . . . 1028
16.14 IO Functor . . . . . . . . . . . . . . . . . . . . . . . . 1030
16.15 What if we want to do something diﬀerent? . . 1034
16.16 Functors are unique to a datatype . . . . . . . . . 1039
16.17 Chapter exercises . . . . . . . . . . . . . . . . . . . 1041
16.18 Definitions . . . . . . . . . . . . . . . . . . . . . . . . 1046
16.19 Follow-up resources . . . . . . . . . . . . . . . . . 1050
17 Applicative 1052
17.1 Applicative . . . . . . . . . . . . . . . . . . . . . . . 1053
17.2 Defining Applicative . . . . . . . . . . . . . . . . . 1054
17.3 Functor vs. Applicative . . . . . . . . . . . . . . . . 1057
17.4 Applicative functors are monoidal functors . . 1059
17.5 Applicative in use . . . . . . . . . . . . . . . . . . . 1067
17.6 Applicative laws . . . . . . . . . . . . . . . . . . . . 1106</p>
<p>CONTENTS xiii
17.7 You knew this was coming . . . . . . . . . . . . . 1115
17.8 ZipList Monoid . . . . . . . . . . . . . . . . . . . . 1120
17.9 Chapter Exercises . . . . . . . . . . . . . . . . . . . 1135
17.10 Definitions . . . . . . . . . . . . . . . . . . . . . . . . 1138
17.11 Follow-up resources . . . . . . . . . . . . . . . . . 1138
18 Monad 1140
18.1 Monad . . . . . . . . . . . . . . . . . . . . . . . . . . 1141
18.2 Sorry — a monad is not a burrito . . . . . . . . . 1141
18.3 Do syntax and monads . . . . . . . . . . . . . . . . 1154
18.4 Examples of Monaduse . . . . . . . . . . . . . . . . 1163
18.5 Monad laws . . . . . . . . . . . . . . . . . . . . . . . 1188
18.6 Application and composition . . . . . . . . . . . 1199
18.7 Chapter Exercises . . . . . . . . . . . . . . . . . . . 1206
18.8 Definition . . . . . . . . . . . . . . . . . . . . . . . . 1209
18.9 Follow-up resources . . . . . . . . . . . . . . . . . 1211
19 Applying structure 1212
19.1 Applied structure . . . . . . . . . . . . . . . . . . . 1213
19.2 Monoid . . . . . . . . . . . . . . . . . . . . . . . . . . . 1213
19.3 Functor . . . . . . . . . . . . . . . . . . . . . . . . . . 1221
19.4 Applicative . . . . . . . . . . . . . . . . . . . . . . . . 1226
19.5 Monad. . . . . . . . . . . . . . . . . . . . . . . . . . . . 1233
19.6 An end-to-end example: URL shortener . . . . 1237
19.7 That’s a wrap! . . . . . . . . . . . . . . . . . . . . . . 1257
19.8 Follow-up resources . . . . . . . . . . . . . . . . . 1258</p>
<p>CONTENTS xiv
20 Foldable 1259
20.1 Foldable . . . . . . . . . . . . . . . . . . . . . . . . . 1260
20.2 The Foldable class . . . . . . . . . . . . . . . . . . . 1261
20.3 Revenge of the monoids . . . . . . . . . . . . . . . 1262
20.4 Demonstrating Foldable instances . . . . . . . . . 1268
20.5 Some basic derived operations . . . . . . . . . . 1273
20.6 Chapter Exercises . . . . . . . . . . . . . . . . . . . 1280
20.7 Follow-up resources . . . . . . . . . . . . . . . . . 1281
21 Traversable 1282
21.1 Traversable . . . . . . . . . . . . . . . . . . . . . . . 1283
21.2 The Traversable typeclass definition . . . . . . . 1284
21.3 sequenceA . . . . . . . . . . . . . . . . . . . . . . . . . 1285
21.4 traverse . . . . . . . . . . . . . . . . . . . . . . . . . . 1287
21.5 So, what’s Traversable for? . . . . . . . . . . . . . . 1291
21.6 Morse code revisited . . . . . . . . . . . . . . . . . 1292
21.7 Axing tedious code . . . . . . . . . . . . . . . . . . 1296
21.8 Do all the things . . . . . . . . . . . . . . . . . . . . 1300
21.9 Traversable instances . . . . . . . . . . . . . . . . . 1304
21.10 Traversable Laws . . . . . . . . . . . . . . . . . . . . 1307
21.11 Quality Control . . . . . . . . . . . . . . . . . . . . 1308
21.12 Chapter Exercises . . . . . . . . . . . . . . . . . . . 1309
21.13 Follow-up resources . . . . . . . . . . . . . . . . . 1314
22 Reader 1315
22.1 Reader . . . . . . . . . . . . . . . . . . . . . . . . . . 1316</p>
<p>CONTENTS xv
22.2 A new beginning . . . . . . . . . . . . . . . . . . . . 1317
22.3 This is Reader . . . . . . . . . . . . . . . . . . . . . 1327
22.4 Breaking down the Functor of functions . . . . . 1328
22.5 But uh, Reader ? . . . . . . . . . . . . . . . . . . . . . 1334
22.6 Functions have an Applicative too . . . . . . . . . 1337
22.7 The Monadof functions . . . . . . . . . . . . . . . . 1345
22.8 Reader Monad by itself is boring . . . . . . . . . . . 1351
22.9 You can change what comes below, but not above 1354
22.10 You tend to see ReaderT , notReader . . . . . . . . . 1355
22.11 Chapter Exercises . . . . . . . . . . . . . . . . . . . 1355
22.12 Definition . . . . . . . . . . . . . . . . . . . . . . . . 1362
22.13 Follow-up resources . . . . . . . . . . . . . . . . . 1363
23 State 1364
23.1 State . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1365
23.2 What is state? . . . . . . . . . . . . . . . . . . . . . . 1365
23.3 Random numbers . . . . . . . . . . . . . . . . . . . 1367
23.4 The Statenewtype . . . . . . . . . . . . . . . . . . . 1371
23.5 Throw down . . . . . . . . . . . . . . . . . . . . . . 1374
23.6 Write Statefor yourself . . . . . . . . . . . . . . . 1383
23.7 Get a coding job with one weird trick . . . . . . 1385
23.8 Chapter exercises . . . . . . . . . . . . . . . . . . . 1392
23.9 Follow-up resources . . . . . . . . . . . . . . . . . 1394
24 Parser combinators 1395
24.1 Parser combinators . . . . . . . . . . . . . . . . . . 1396</p>
<p>CONTENTS xvi
24.2 A few more words of introduction . . . . . . . . 1398
24.3 Understanding the parsing process . . . . . . . . 1399
24.4 Parsing fractions . . . . . . . . . . . . . . . . . . . . 1416
24.5 Haskell’s parsing ecosystem . . . . . . . . . . . . . 1425
24.6 Alternative . . . . . . . . . . . . . . . . . . . . . . . . 1429
24.7 Parsing configuration files . . . . . . . . . . . . . . 1444
24.8 Character and token parsers . . . . . . . . . . . . 1460
24.9 Polymorphic parsers . . . . . . . . . . . . . . . . . 1465
24.10 Marshalling from an AST to a datatype . . . . . 1474
24.11 Chapter Exercises . . . . . . . . . . . . . . . . . . . 1491
24.12 Definitions . . . . . . . . . . . . . . . . . . . . . . . . 1500
24.13 Follow-up resources . . . . . . . . . . . . . . . . . 1501
25 Composing types 1504
25.1 Composing types . . . . . . . . . . . . . . . . . . . 1505
25.2 Common functions as types . . . . . . . . . . . . 1506
25.3 Two little functors sittin’ in a tree,
L-I-F-T-I-N-G . . . . . . . . . . . . . . . . . . . . . 1511
25.4 Twinplicative . . . . . . . . . . . . . . . . . . . . . . 1514
25.5 Twonad? . . . . . . . . . . . . . . . . . . . . . . . . . 1516
25.6 Exercises: Compose Instances . . . . . . . . . . . 1518
25.7 Monad transformers . . . . . . . . . . . . . . . . . 1520
25.8 IdentityT . . . . . . . . . . . . . . . . . . . . . . . . . 1523
25.9 Finding a pattern . . . . . . . . . . . . . . . . . . . 1542
26 Monad transformers 1546</p>
<p>CONTENTS xvii
26.1 Monad transformers . . . . . . . . . . . . . . . . . 1547
26.2 MaybeT . . . . . . . . . . . . . . . . . . . . . . . . . . 1547
26.3 EitherT . . . . . . . . . . . . . . . . . . . . . . . . . . 1555
26.4 ReaderT . . . . . . . . . . . . . . . . . . . . . . . . . 1557
26.5 StateT . . . . . . . . . . . . . . . . . . . . . . . . . . . 1561
26.6 Types you probably don’t want to use . . . . . . 1566
26.7 Recovering an ordinary type from a transformer 1568
26.8 Lexically inner is structurally outer . . . . . . . 1570
26.9 MonadTrans . . . . . . . . . . . . . . . . . . . . . . 1574
26.10 MonadIO aka zoom-zoom . . . . . . . . . . . . . 1597
26.11 Monad transformers in use . . . . . . . . . . . . . 1601
26.12 Monads do not commute . . . . . . . . . . . . . . 1617
26.13 Transform if you want to . . . . . . . . . . . . . . 1617
26.14 Chapter Exercises . . . . . . . . . . . . . . . . . . . 1618
26.15 Defintion . . . . . . . . . . . . . . . . . . . . . . . . . 1627
26.16 Follow-up resources . . . . . . . . . . . . . . . . . 1628
27 Nonstrictness 1629
27.1 Laziness . . . . . . . . . . . . . . . . . . . . . . . . . 1630
27.2 Observational Bottom Theory . . . . . . . . . . . 1631
27.3 Outside in, inside out . . . . . . . . . . . . . . . . . 1633
27.4 What does the other way look like? . . . . . . . . 1637
27.5 Can we make Haskell strict? . . . . . . . . . . . . . 1638
27.6 Call by name, call by need . . . . . . . . . . . . . 1657
27.7 Nonstrict evaluation changes what we can do . 1658
27.8 Thunk Life . . . . . . . . . . . . . . . . . . . . . . . 1660</p>
<p>CONTENTS xviii
27.9 Sharing is caring . . . . . . . . . . . . . . . . . . . . 1664
27.10 Refutable and irrefutable patterns . . . . . . . . 1686
27.11 Bang patterns . . . . . . . . . . . . . . . . . . . . . . 1689
27.12 Strict and StrictData . . . . . . . . . . . . . . . . . 1693
27.13 Adding strictness . . . . . . . . . . . . . . . . . . . . 1695
27.14 Chapter Exercises . . . . . . . . . . . . . . . . . . . 1702
27.15 Follow-up resources . . . . . . . . . . . . . . . . . 1705
28 Basic libraries 1707
28.1 Basic libraries and data structures . . . . . . . . . 1708
28.2 Benchmarking with Criterion . . . . . . . . . . . 1709
28.3 Profiling your programs . . . . . . . . . . . . . . . 1727
28.4 Constant applicative forms . . . . . . . . . . . . . 1732
28.5 Map . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1737
28.6 Set . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1741
28.7 Sequence . . . . . . . . . . . . . . . . . . . . . . . . . 1744
28.8 Vector . . . . . . . . . . . . . . . . . . . . . . . . . . . 1748
28.9 String types . . . . . . . . . . . . . . . . . . . . . . . 1762
28.10 Chapter Exercises . . . . . . . . . . . . . . . . . . . 1775
28.11 Follow-up resources . . . . . . . . . . . . . . . . . 1779
29 IO 1781
29.1 IO . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1782
29.2 Where IO explanations go astray . . . . . . . . . 1783
29.3 The reason we need this type . . . . . . . . . . . 1786
29.4 Sharing . . . . . . . . . . . . . . . . . . . . . . . . . . 1788</p>
<p>CONTENTS xix
29.5 IO doesn’t disable sharing for everything . . . . 1795
29.6 Purity is losing meaning . . . . . . . . . . . . . . . 1797
29.7 IO’s Functor, Applicative, and Monad . . . . . . 1800
29.8 Well, then, how do we MVar? . . . . . . . . . . . . 1806
29.9 Chapter Exercises . . . . . . . . . . . . . . . . . . . 1809
29.10 Follow-up resources . . . . . . . . . . . . . . . . . 1810
30 When things go wrong 1812
30.1 Exceptions . . . . . . . . . . . . . . . . . . . . . . . . 1813
30.2 The Exception class and methods . . . . . . . . . 1814
30.3 This machine kills programs . . . . . . . . . . . . 1825
30.4 Want either? Try! . . . . . . . . . . . . . . . . . . . 1832
30.5 The unbearable imprecision of trying . . . . . . 1837
30.6 Why throwIO? . . . . . . . . . . . . . . . . . . . . . 1840
30.7 Making our own exception types . . . . . . . . . 1844
30.8 Surprising interaction with bottom . . . . . . . . 1851
30.9 Asynchronous Exceptions . . . . . . . . . . . . . . 1853
30.10 Follow-up Reading . . . . . . . . . . . . . . . . . . 1857
31 Final project 1859
31.1 Final project . . . . . . . . . . . . . . . . . . . . . . . 1860
31.2 fingerd . . . . . . . . . . . . . . . . . . . . . . . . . . 1860
31.3 Exploring finger . . . . . . . . . . . . . . . . . . . . 1862
31.4 Slightly modernized fingerd . . . . . . . . . . . . 1872
31.5 Chapter Exercises . . . . . . . . . . . . . . . . . . . 1887</p>
<p>CONTENTS xx
Authors’ preface
Chris’s story
I’ve been programming for over 15 years, 8 of them profes-
sionally. I’ve worked primarily in Common Lisp, Clojure, and
Python. I became interested in Haskell about 6 years ago.
Haskell was the language that made me aware that progress
is being made in programming language research and that
there are benefits to using a language with a design informed
by knowledge of those advancements.
I’ve had type errors in Clojure that multiple professional
Clojure devs (including myself) couldn’t resolve in less than
2 hours because of the source-to-sink distance caused by dy-
namic typing. We had copious tests. We added println s ev-
erywhere. We tested individual functions from the REPL. It
still took ages. It was only 250 lines of Clojure. I did finally
fix it and found it was due to vectors in Clojure implementing
IFn. The crazy values that propagated from the IFnusage of
the vector allowed malformed data to propagate downward
far away from the origin of the problem. I’ve had similar hap-
pen in Python and Common Lisp as well. The same issue
in Haskell would be trivially resolved in a minute or less be-
cause the typechecker will identify precisely where you were
inconsistent.
I use Haskell because I want to be able to refactor without</p>
<p>CONTENTS xxi
fear, because I want maintenance to be something I don’t re-
sent, so I can reuse code freely. This doesn’t come without
learning new things. The diﬀerence between people that are
“good at math” who “do it in their head” and professional math-
ematicians is that the latter show their work and use tools that
help them get the job done. When you’re using a dynamically
typed language, you’re forcing yourself unnecessarily to do it
“in your head.” As a human with limited working memory, I
want all the help I can get to reason about and write correct
code. Haskell provides that help.
Haskell is not a difficult language to use — quite the opposite.
I’m now able to tackle problems that I couldn’t have tackled
when I was primarily a Clojure, Common Lisp, or Python
user. Haskell is difficult to teach eﬀectively, and the ineﬀective
pedagogy has made it hard for many people to learn.
It doesn’t have to be that way.
I’ve spent the last two years actively teaching Haskell on-
line and in person. Along the way, I started keeping notes
on exercises and methods of teaching specific concepts and
techniques that worked. Those notes eventually turned into
my guide for learning Haskell. I’m still learning how to teach
Haskell better by working with people locally in Austin, Texas,
as well as online in the IRC channel I made for beginners to
get help with learning Haskell.
I wrote this book because I had a hard time learning Haskell,
and I don’t want others to struggle the way I did.</p>
<p>CONTENTS xxii
Julie’s story
I met Chris Allen in spring 2014. We met on Twitter and
quickly became friends. As anyone who has encountered
Chris — probably in any medium, but certainly on Twitter —
knows, it doesn’t take long before he starts urging you to learn
Haskell.
I told him I had no interest in programming. I told him
nothing and nobody had ever been able to interest me in pro-
gramming before. When Chris learned of my background
in linguistics, he thought I might be interested in natural lan-
guage processing and exhorted me to learn Haskell for that
purpose. I remained unconvinced.
Then he tried a diﬀerent approach. He was spending a lot of
time gathering and evaluating resources for teaching Haskell
and refining his pedagogical techniques, and he convinced me
to try to learn Haskell so that he could gain the experience
of teaching a code-neophyte. Finally, with an “anything for
science” attitude, I gave in.
Chris had already known that the available Haskell learning
materials each had problems, but I don’t think even he realized
just how frustrating they would be to me. All of the materials
I ran across relied on a background with other programming
languages and left many terms undefined or explained fea-
tures of Haskell by analogy (often faulty) to features of other
languages — features I had no experience with.</p>
<p>CONTENTS xxiii
When I say I had no programming experience, I really, truly
mean it. I had to start from learning what a compiler does,
what version control means, what constitutes side eﬀects, what
is a library, what is a module, what on earth is a stack overflow.
At the time of this writing, that is where I was less than a year
ago; by the time we finish writing this book and it is published,
it will be a little over two years.
Eventually, as he realized that a new type of book for learn-
ing Haskell was necessary, he decided to write one. I agreed at
the time to be his guinea pig. He would send me chapters and
I would learn Haskell from them and send feedback. Through
the fall, we worked like this, on and oﬀ, in short bursts. Even-
tually we found it more efficient for me to take on authorship
duties. We developed a writing process where Chris made
the first pass at a chapter, scaﬀolding the material it needed to
cover. Then I filled in the parts that I understood and came
up with questions that would elaborate and clarify the parts I
didn’t already know. He answered my questions until I under-
stood, and I continued adding to and refining what was there.
We each wrote exercises — I write much easier ones than he
does, but the variety is beneficial.
I have tried, throughout the process, to keep thinking from
the perspective of the absolute beginner. For one thing, I
wanted my own understanding of Haskell to deepen as I wrote
soIkeptquestioningthethingsIthoughtIknew. Also, Iwanted
this book to be accessible to everyone.</p>
<p>CONTENTS xxiv
In interacting with other Haskell learners I often hear that
other materials leave them feeling like Haskell is difficult and
mysterious, a programming language best left to wizards.
It doesn’t have to be that way.</p>
<p>CONTENTS xxv
Acknowledgements
This book developed out of many eﬀorts to teach and learn
Haskell, online and oﬀ. We could not have done this without
the help of the growing community of friendly Haskellers as
well as the Haskell learners who have graciously oﬀered time
to help us make the book better.
First and foremost, we owe a huge debt of gratitude to our
first-round reviewers, Angela May O’Connor and Martin Vlk,
for their tremendous patience. We have sent them each some
very rough material, and they have been willing to work with
it and send detailed feedback about what worked and what
didn’t. Their reviews helped ensure the book is suitable for
both beginners and comprehensive. Also, they’re both just
wonderful people all around.
Martin DeMello, Daniel Gee, and Simon Yang have each
sent us (many) smart criticisms and helpful suggestions. The
book would have been shorter without their help, we think,
but it’s much more thorough and clear now.
A number of people have contributed feedback and tech-
nical review for limited parts of the book. Thanks to Sean
Chalmers, Erik de Castro Lopo, Alp Mestanogullari, Juan Al-
berto Sanchez, Jonathan Ferguson, Deborah Newton, Matt
Parsons, Peter Harpending, Josh Cartwright, Eric Mertens,
and George Makrydakis, who have all oﬀered critiques of our
writing and our technical coverage of diﬀerent topics.</p>
<p>CONTENTS xxvi
We have some very active early access readers who send us
a stream of feedback, everything from minor typographical
errors they find to questions about exercises, and we’re pleased
and grateful to have their input. The book would be messier
and the exercises less useful if not for their help. Julien Baley
and Jason Kuhrt have been particularly outstanding on this
front, not only representing a nontrivial portion of our reader
feedback over the course of several releases of the book, but
also catching things nobody else noticed.
The book cover was designed by David Deutsch (skore_de
on Twitter). He took pity on the state of our previous, original,
super special early access cover, and took it upon himself to
redesign it. We liked it so much we asked him to redo the book
web site as well. He’s a talented designer, and we’re grateful
for all the work he’s done for us.
A special thank-you is owed to Soryu Moronuki, Julie’s son,
who agreed to try to use the book to teach himself Haskell and
allowed us to use his feedback and occasionally blog about his
progress.
A warm hello to all the reading groups, both online and
in meatspace, that have formed to work through the book
together. We’ve had some great feedback from these groups
and hope to visit with you all someday. We’re delighted to see
the Haskell community growing.
We would also like to thank Michael Neale for being funny
and letting us use something he said on Twitter as an epigraph.</p>
<p>CONTENTS xxvii
Some day we hope to buy the gentleman a beer.
Thank you as well to Steven Proctor for having hosted us on
his Functional Geekery podcast, and to Adam Stacoviak and
Jerod Santo for inviting us onto their podcast, The Changelog
— and to Zaki Manian for bringing us to Adam and Jerod’s
attention.
Chris I would like to thank the participants in the #haskell-
beginners IRC channel, the teachers and the students, who have
helped me practice and refine my teaching techniques. Many
of the exercises and approaches in the book would’ve never
happened without the wonderful Haskell IRC community to
learn from.
I owe Alex Kurilin, Carter Schonwald, Aidan Coyne, and
Mark Wotton thanks for being there when I was reallybad at
teaching, being kind and patient friends, and for giving me
advice when I needed it. I wouldn’t have scratched this itch
without y’all.
Julie I would like to send a special shout-out to the Austin
Haskell meetup group, especially Sukant Hajra and Austin
Seipp for giving me the opportunity to teach the meetup.
The list of Haskellers who have responded to the kvetches
and confusions of a Haskell beginner with assistance, humor,
and advice would be very long indeed, but I owe special grati-
tude to Sooraj Bhat, Reid McKenzie, Dan Lien, Zaki Manian,</p>
<p>CONTENTS xxviii
and Alex Feldman-Crough for their help and encouragement.
I wouldn’t have made it through the last few months of fin-
ishing this thing without the patient advice and friendship of
Chris Martin.
My husband and children have tolerated me spending un-
countable hours immersed in the dark arts of thunkery. I am
grateful for their love, patience, and support and hope that
my kids will remember this: that it’s never too late to learn
something new. Besos, mijos.
Finally, a warm thank you to George Makrydakis for the
ongoing discussion on matters to do with math, programming,
and the weirding way.
Any errors in the book, of course, remain the sole responsi-
bility of the authors.</p>
<p>CONTENTS xxix
Introduction
Welcome to a new way to learn Haskell. Perhaps you are
coming to this book frustrated by previous attempts to learn
Haskell. Perhaps you have only the faintest notion of what
Haskell is. Perhaps you are coming here because you are not
convinced that anything will ever be better than Common
Lisp/Scala/Ruby/whatever language you love, and you want
to argue with us. Perhaps you were just looking for the 18
billionth (n.b.: this number may be inaccurate) monad tutorial,
certain that this time around you will understand monads
once and for all. Whatever your situation, welcome and read
on! It is our goal here to make Haskell as clear, painless, and
practical as we can, no matter what prior experiences you’re
bringing to the table.
Why This Book
If you are new to programming entirely, Haskell is a great
first language. Haskell is a general purpose, functional pro-
gramming1language. It’s applicable virtually anywhere one
would use a program to solve a problem, save for some specific
embedded applications. If you could write software to solve a
1Functional programming is a style of programming in which function calls, rather
than a series of instructions for the computer to execute, are the primary constructs of
your program. What it is doesn’t matter much right now; Haskell completely embodies
the functional style, so it will become clear over the course of the book.</p>
<p>CONTENTS xxx
problem, you could probably use Haskell.
If you are already a programmer, you may be looking to
enrich your skills by learning Haskell for a variety of reasons
— from love of pure functional programming itself to wanting
to write functional Scala code to finding a bridge to PureScript
or Idris. Languages such as Java are gradually adopting func-
tional concepts, but most were not designed to be functional
languages. Because Haskell is a pure functional language, it is
a fertile environment for mastering functional programming.
That way of thinking and problem solving is useful, no matter
what other languages you might know or learn. We’ve heard
from readers who are finding this book useful to their work in
diverse languages such as Scala, F#, Frege, Swift, PureScript,
Idris, and Elm.
Haskell has a bit of a reputation for being difficult. Writing
Haskellmayseemtobemoredifficultupfront, notjustbecause
of the hassle of learning a language that is syntactically and
conceptually diﬀerent from a language you already know, but
also because of features such as strong typing that enforce
some discipline in how you write your code. But what seems
like a bug is a feature. Humans, unfortunately, have relatively
limited abilities of short-term memory and concentration,
even if we don’t like to admit it. We cannot track all relevant
metadata about our programs in our heads. Using up working
memory for anything a computer can do for us is counter-
productive, and computers are very good at keeping track of</p>
<p>CONTENTS xxxi
data for us, including metadata such as types.
We don’t write Haskell because we’re geniuses — we use
tools like Haskell because they help us. Good tools like Haskell
enable us to work faster, make fewer mistakes, and have more
information about what our code is supposed to do as we read
it.
We use Haskell because it is easier (over the long run) and enables
us to do a better job. That’s it. There’s a ramp-up required in
order to get started, but that can be ameliorated with patience
and a willingness to work through exercises.
OK, but I was just looking for a monad tutorial...
The bad news is looking for an easy route into Haskell and
functional programming is how a lot of people end up think-
ing it’s “too hard” for them. The good news is we have a lot
of experience teaching and we don’t want that to happen to
anyone, but especially not you, gentle reader.
We encourage you to forget what you might already know
about programming and come at this course in Haskell with a
beginner’s mindset. Make yourself an empty vessel, ready to
let the types flow through you.
If you are an experienced programmer, learning Haskell is
more like learning to program all over again. Learning Haskell
imposes new ways of thinking about and structuring programs
on most people already comfortable with an imperative or</p>
<p>CONTENTS xxxii
untyped programming language. This makes it harder to
learn not because it is intrinsically harder, but because most
people who have learned at least a couple of programming
languages are accustomed to the process being trivial, and
their expectations have been set in a way that lends itself to
burnout and failure.
If Haskell is your first language, or even if it is not, you may
have noticed a specific problem with many Haskell learning
resources: they assume a certain level of background with
programming, so they frequently explain Haskell concepts in
terms, by analogy or by contrast, of programming concepts
from other languages. This is confusing for the student who
doesn’t know those other languages, but we posit that it is just
as unhelpful for experienced programmers. Most attempts to
compare Haskell with other languages only lead to a superficial
understanding of Haskell, and making analogies to loops and
other such constructs can lead to bad intuitions about how
Haskell code works. For all of these reasons, we have tried to
avoid relying on knowledge of other programming languages.
Just as you can’t achieve fluency in a human language so long
as you are still attempting direct translations of concepts and
structures from your native language to the target language,
it’s best to learn to understand Haskell on its own terms.</p>
<p>CONTENTS xxxiii
But I’ve heard Haskell is hard...
There’s a wild rumor that goes around the internet from time
to time about needing a Ph.D. in mathematics and an under-
standing of monads just to write “hello, world”2in Haskell.
We will write “hello, world” in Chapter 3. We’re going to do
some arithmetic before that to get you used to function syntax
and application in Haskell, but you will not need a Ph.D. in
monadology to write it.
Intruth, therewill be a monad underlyingour “hello, world,”
and by the end of the book, you willunderstand monads,
but you’ll be interacting with monadic code long before you
understand how it all works. You’ll find, at times, this book
goes into more detail than you strictly need to be able to write
Haskell successfully. There is no problem with that. You do
not need to understand everything in here perfectly on the
first try.
You are not a Spartan warrior who must come back with
your shield or on it. Returning later to investigate things more
deeply is an efficient technique, not a failure.
2Writing “hello, world” in a new programming language is a standard sort of “baby’s
first program,” so the idea here is that if it’s difficult to write a “hello, world” program,
then the language must be impossible. There are languages that have purposely made it
inhumanly difficult to write such programs, but Haskell is not one of them.</p>
<p>CONTENTS xxxiv
A few words to new programmers
We’ve tried very hard to make this book as accessible as possi-
ble, no matter your level of previous experience. We have kept
comparisons and mentions of other languages to a minimum,
and we promise that if we compare something in Haskell to
something in another language, that comparison is not itself
crucial to understanding the Haskell — it’s just a little extra for
those who do know the other language.
However, especially as the book progresses and the exercises
and projects get more “real,” there are going to be terms and
concepts that we do not have the space to explain fully but
that are relatively well known among programmers. You may
have to do internet searches for terms like JSON. The next
section of this introduction references things that you may not
know about but programmers will — don’t panic. We think
you’ll still get something out of reading it, but if not, it’s not
something to worry about. The fact that you don’t know every
term in this book before you come to it is not a sign that you
can’t learn Haskell or aren’t ready for this: it’s only a sign that
you don’t know everything yet , and since no one does, you’re in
fine company.
Along those same lines, this book does not oﬀer much in-
struction on using the terminal and text editor. The instruc-
tions provided assume you know how to find your way around
your terminal and understand how to do simple tasks like</p>
<p>CONTENTS xxxv
make a directory or open a file. Due to the number of text
editors available, we do not provide specific instructions for
any of them.3
If you need help or would like to start getting to know the
communities of functional programmers, there are several
options. The Freenode IRC channel #haskell-beginners has
teachers who will be glad to help you, and they especially
welcome questions regarding specific problems that you are
trying to solve.4There are also Slack channels and subreddits
where Haskellers congregate, along with a plethora of Haskell-
oriented blogs, many of which are mentioned in footnotes
and recommended readings throughout the book. Many of
our readers also program in languages like Swift and Scala, so
you may want to investigate those communities as well.
Haskevangelism
The rest of this introduction will give some background of
Haskell and will make reference to other programming lan-
3If you’re quite new and unsure what to do about text editors, you might consider
Atom. It’s free, open-source, and configurable. Sublime Text has served Julie well through-
out the writing of the book, but is not free. Chris uses Emacs most of the time; Emacs
is very popular among programmers, but has its own learning curve. Vim is another
popular text editor with itsown learning curve. If you have no experience with Emacs or
Vim, we’d really recommend sticking with something like Sublime or Atom for now.
4Freenode IRC (Internet Relay Chat) is a network of channels for textual chat. There
are other IRC networks around, as well as other group chat platforms, but the Freenode
IRC channels for Haskell are popular meeting places for the Haskell community. There
are several ways to access Freenode IRC, including Irssi and HexChat, if you’re interested
in getting to know the community in their natural habitat.</p>
<p>CONTENTS xxxvi
guages and styles. If you’re a new programmer, it is possible
not all of this will make sense, and that’s okay. The rest of the
book is written with beginners in mind, and the features we’re
outlining will make more sense as you work through the book.
We’re going to compare Haskell a bit with other languages
to demonstrate why we think using Haskell is valuable. Haskell
is a language in a progression of languages dating back to 1973,
when ML was invented by Robin Milner and others at the
University of Edinburgh. ML was itself influenced by ISWIM,
which was in turn influenced by ALGOL 60 and Lisp. We
mention this lineage because Haskell isn’tnew. The most pop-
ular implementation of Haskell, the Glasgow Haskell Compiler
(GHC), is mature and well-made. Haskell brings together some
nice design choices that make for a language that oﬀers more
expressiveness than Ruby, but more type safety than any lan-
guage presently in wide use commercially.
In 1968, the ALGOL68 dialect had the following features
built into the language:
1.User-defined record types.
2.User-defined sum types (unions not limited to simple
enumerations).
3.Switch/case expressions supporting the sum types.
4.Compile-time enforced constant values, declared with =
rather than :=.</p>
<p>CONTENTS xxxvii
5.Unified syntax for using value and reference types — no
manual pointer dereferencing.
6.Closures with lexical scoping (without this, many func-
tional patterns fall apart).
7.Implementation-agnostic parallelized execution of pro-
cedures.
8.Multi-pass compilation — you can declare stuﬀ after you
use it.
As of the early 21st century, many popular languages used
commercially don’t have anything equivalent to or better than
what ALGOL68 had. We mention this because we believe
technological progress in computer science, programming,
and programming languages is possible, desirable, and critical
to software becoming a true engineering discipline. By that,
we mean that while the phrase “software engineering” is in
common use, engineering disciplines involve the application
of both scientific and practical knowledge to the creation and
maintenance of better systems. As the available materials
change and as knowledge grows, so must engineers.
Haskell leverages more of the developments in program-
ming languages invented since ALGOL68 than most languages
in popular use, but with the added benefit of a mature imple-
mentation and sound design. Sometimes we hear Haskell be-
ing dismissed as “academic” because it is relatively up-to-date</p>
<p>CONTENTS xxxviii
with the current state of mathematics and computer science
research. In our view, that progress is good and helps us solve
practical problems in modern computing and software design.
Progress is possible and desirable, but it is not monotonic or
inevitable. The history of the world is riddled with examples
of uneven progress. For example, it is estimated that scurvy
killed two million sailors between the years 1500 and 1800.
Western culture has forgotten the cure for scurvy multiple
times. As early as 1614, the Surgeon General of the East In-
dia Company recommended bringing citrus on voyages for
scurvy. It saved lives, but the understanding of whycitrus
cured scurvy was incorrect. This led to the use of limes, which
have a lower vitamin C content than lemons, and scurvy re-
turned until ascorbic acid was discovered in 1932. Indiscipline
and stubbornness (the British Navy stuck with limes despite
sailors continuing to die from scurvy) can hold back progress.
We’d rather have a doctor who is willing to understand that
he makes mistakes, will be responsive to new information,
and even actively seek to expand his understanding rather
than one that hunkers down with a pet theory informed by
anecdote.
There are other ways to prevent scurvy, just as there are
other programming languages you can use to write software.
Or perhaps you are an explorer who doesn’t believe scurvy
can happen to you. But packing lemons provides some in-
surance on those long voyages. Similarly, having Haskell in</p>
<p>CONTENTS xxxix
your toolkit, even when it’s not your only tool, provides type
safety and predictability that can improve your software devel-
opment. Buggy software might not literally make your teeth
fall out, but software problems are far from trivial, and when
there are better ways to solve those problems — not perfect,
but better — it’s worth your time to investigate them.
Set your limes aside for now, and join us at the lemonade
stand.
What’s in this book?
This book is more of a course than a book, something to
be worked through. There are exercises sprinkled liberally
throughout the book; we encourage you to do them, even
when they seem simple. Those exercises are where the major-
ity of your epiphanies will come from. No amount of chatter-
ing, no matter how well structured and suited to your temper-
ament, will be as eﬀective as doing the work . If you do get to
a later chapter and find you did not understand a concept or
structure well enough, you may want to return to an earlier
chapter and do more exercises until you understand it.
We believe that spaced repetition and iterative deepening
are eﬀective strategies for learning, and the structure of the
book reflects this. You may notice we mention something
only briefly at first, then return to it over and over. As your
experience with Haskell deepens, you have a base from which</p>
<p>CONTENTS xl
to move to a deeper level of understanding. Try not to worry
that you don’t understand something completely the first time
we mention it. By moving through the exercises and returning
to concepts, you can develop a solid intuition for functional
programming.
Theexercisesinthefirstfewchaptersaredesignedtorapidly
familiarize you with basic Haskell syntax and type signatures,
but you should expect exercises to grow more challenging
in each successive chapter. Where possible, reason through
the code samples and exercises in your head first, then type
them out — either into the REPL5or into a source file — and
check to see if you were right. This will maximize your ability
to understand and reason about programs and about Haskell.
Later exercises may be difficult. If you get stuck on an exercise
for an extended period of time, proceed and return to it at a
later date.
We cover a mix of practical and abstract matters required
to use Haskell for a wide variety of projects. Chris’s experience
is principally with production backend systems and frontend
web applications. Julie is a linguist and teacher by training
and education, and learning Haskell was her first experience
with computer programming. The educational priorities of
this book are biased by those experiences. Our goal is to help
5This is short for read-eval-print loop, an interactive programming shell that evaluates
expressions and returns results in the same environment. The REPL we’ll be using is
called GHCi — ‘i’ for “interactive.”</p>
<p>CONTENTS xli
you not just write typesafe functional code but to understand
it on a deep enough level that you can go from here to more
advanced Haskell projects in a variety of ways, depending on
your own interests and priorities.
Each chapter focuses on diﬀerent aspects of a particular
topic. We start with a short introduction to the lambda calcu-
lus. What does this have to do with programming? All modern
functional languages are based on the lambda calculus, and a
passing familiarity with it will help you down the road with
Haskell. If you’ve understood the lambda calculus, under-
standing the feature known as currying will be a breeze, for
example.
The next few chapters cover basic expressions and functions
in Haskell, some simple operations with strings (text), and a few
essential types. You may feel a strong temptation, especially if
you have programmed previously, to skim or skip those first
chapters. Please do not do this. Even if those first chapters are
covering concepts you’re familiar with, it’s important to spend
time getting comfortable with Haskell’s terse syntax, making
sure you understand the diﬀerence between working in the
REPL and working in source files, and becoming familiar with
the compiler’s sometimes quirky error messages. Certainly
you may work quickly through those chapters — just don’t
skip them.
From there, we build both outward and upward so that your
understanding of Haskell both broadens and deepens. When</p>
<p>CONTENTS xlii
you finish this book, you will not just know what monads
are, you will know how to use them eﬀectively in your own
programs and understand the underlying algebra involved.
We promise — you will. We only ask that you do not go on to
write a monad tutorial on your blog that explains how monads
are really just like jalapeno poppers.
In each chapter you can expect:
•additions to your vocabulary of standard functions;
•syntactic patterns that build on each other;
•theoretical foundations so you understand how Haskell
works;
•illustrative examples of how to read Haskell code;
•step-by-step demonstrations of how to write your own
functions;
•explanations of how to read common error messages and
how to avoid those errors;
•exercises of varying difficulty sprinkled throughout;
•definitions of important terms.
We have put definitions at the end of most chapters. Each
term is, of course, defined within the body of the chapter, but</p>
<p>CONTENTS xliii
we added separate definitions at the end as a point of review.
If you’ve taken some time oﬀ between one chapter and the
next, the definitions can remind you of what you have already
learned, and, of course, they may be referred to any time you
need a refresher.
There are also recommendations at the end of most chap-
ters for followup reading. They are certainly not required but
are resources we personally found accessible and helpful that
may help you learn more about topics covered in the chapter.
Best practices for examples and exercises
We have tried to include a variety of examples and exercises in
each chapter. While we have made every eﬀort to include only
exercises that serve a clear pedagogical purpose, we recognize
that not all individuals enjoy or learn as much from every
type of demonstration or exercise. Also, since our readers
will necessarily come to the book with diﬀerent backgrounds,
some exercises may seem too easy or difficult to you but be
just right for someone else. Do your best to work through
as many exercises as seems practical for you. But if you skip
all the types and typeclasses exercises and then find yourself
confused when we get to Monoid, by all means, come back
and do more exercises until you understand.
Here are a few things to keep in mind to get the most out
of them:</p>
<p>CONTENTS xliv
•Examples are usually designed to demonstrate, with real
code, what we’ve just talked or are about to talk about in
further detail.
•You are intended to typeall of the examples into the REPL
or a file and load them. We strongly encourage you to
attempt to modify the example and play with the code af-
ter you’ve made it work. Forming hypotheses about what
eﬀect changes will have and verifying them is critical! It
is better to type the code examples and exercises yourself
rather than copy and paste because typing makes you pay
more attention to it.
•Sometimes the examples are designed intentionally to be
broken. Check surrounding prose if you’re confused by
an unexpected error as we will not show you code that
doesn’t work without commenting on the breakage. If it’s
still broken and it’s not supposed to be, you should start
checking your syntax and formatting for errors.
•Not every example is designed to be entered into the
REPL; not every example is designed to be entered into
a file. Once we have explained the syntactic diﬀerences
between files and REPL expressions, you are expected to
perform the translation between the two yourself. You
should be accustomed to working with code in an interac-
tive manner by the time you finish the book. You’ll want</p>
<p>CONTENTS xlv
to gradually move away from typing code examples and
exercises, except in limited cases, directly into GHCi and
develop the habit of working in source files. Editing and
modifying code, as you will be doing a lot as you rework
exercises, is easier and more practical in a source file. You
will still load your code into GHCi to run it.
•You may want to keep exercises, especially longer ones, as
named modules. There are several exercises, especially
later in the book, that we return to several times and being
able to reload the work you’ve already done and add only
the new parts will save you a lot of time and grief. We
have tried to note some of the exercises where this will
be especially helpful.
•Exercises at the end of the chapter may include some re-
view questions covering material from previous chapters
and are more or less ordered from least to most challeng-
ing. Your mileage may vary.
•Even exercises that seem easy can increase your fluency
in a topic. We do not fetishize difficulty for difficulty’s
sake. We just want you to understand the topics as well
as possible. That can mean coming at the same problem
from diﬀerent angles.
•We ask you to write and then rewrite (using diﬀerent
syntax) a lot of functions. Few problems have only one</p>
<p>CONTENTS xlvi
possible solution, and solving the same problem in dif-
ferent ways increases your fluency and comfort with the
way Haskell works (its syntax, its semantics, and in some
cases, its evaluation order).
•Do not feel obligated to do all the exercises in a single
sitting or even in a first pass through the chapter. In fact,
spaced repetition is generally a more eﬀective strategy.
•Some exercises, particularly in the earlier chapters, may
seem very contrived. Well, they are. But they are con-
trived to pinpoint certain lessons. As the book goes on
and you have more Haskell under your belt, the exercises
become less contrived and more like “real Haskell.”
•Another benefit to writing code in a source file and then
loading it into the REPL is that you can write comments
about the process you went through in solving a problem.
Writing out your own thought process can clarify your
thoughts and make the solving of similar problems easier.
At the very least, you can refer back to your comments
and learn from yourself.
•Sometimes we intentionally underspecify function def-
initions. You’ll commonly see things like:
f=undefined</p>
<p>CONTENTS xlvii
Even when 𝑓will probably take named arguments in your
implementation, we’re not going to name them for you.
Nobody will scaﬀold your code for you in your future
projects, so don’t expect this book to either.</p>
<p>Chapter 1
All You Need is Lambda
Even the greatest
mathematicians, the ones
that we would put into
our mythology of great
mathematicians, had to
do a great deal of leg
work in order to get to
the solution in the end.
Daniel Tammett
1</p>
<p>CHAPTER 1. ANYTHING FROM ALMOST NOTHING 2
1.1 All You Need is Lambda
This chapter provides a very brief introduction to the lambda
calculus, a model of computation devised in the 1930s by
Alonzo Church. A calculus is a method of calculation or rea-
soning; the lambda calculus is one process for formalizing a
method. Like Turing machines, the lambda calculus formal-
izes the concept of eﬀective computability, thus determining
which problems, or classes of problems, can be solved.
You may be wondering where the Haskell is. You may be
contemplating skipping this chapter. You may feel tempted
to skip ahead to the fun stuﬀ when we build a project.
DON’T.
We’re starting from first principles here so that when we
get around to building projects you know what you’re doing.
You don’t start building a house from the attic down; you start
from the foundation. Lambda calculus is your foundation,
because Haskell is a lambda calculus.
1.2 What is functional programming?
Functionalprogrammingisacomputerprogrammingparadigm
that relies on functions modeled on mathematical functions.
The essence of functional programming is that programs are
a combination of expressions . Expressions include concrete
values, variables, and also functions. Functions have a more</p>
<p>CHAPTER 1. ANYTHING FROM ALMOST NOTHING 3
specific definition: they are expressions that are applied to
an argument or input, and once applied, can be reduced or
evaluated . In Haskell, and in functional programming more
generally, functions are first-class : they can be used as values
or passed as arguments, or inputs, to yet more functions. We’ll
define these terms more carefully as we progress through the
chapter.
Functional programming languages are all based on the
lambda calculus. Some languages in this general category
incorporate features into the language that aren’t translatable
into lambda expressions. Haskell is a purefunctional language,
because it does not. We’ll address this notion of purity more
later in the book, but it isn’t a judgment of the moral worth of
other languages.
The word purity in functional programming is sometimes
also used to mean what is more properly called referential
transparency . Referential transparency means that the same
function, given the same values to evaluate, will always return
the same result in pure functional programming, as they do
in math.
Haskell’s pure functional basis also lends it a high degree
of abstraction and composability. Abstraction allows you to
write shorter, more concise programs by factoring common,
repeated structures into more generic code that can be reused.
Haskell programs are built from separate, independent func-
tions, kind of like LEGO®: the functions are bricks that can be</p>
<p>CHAPTER 1. ANYTHING FROM ALMOST NOTHING 4
assembled and reassembled.
These features also make Haskell’s syntax rather minimalist,
as you’ll soon see.
1.3 What is a function?
If we step back from using the word “lambda,” you most likely
already know what a function is. A function is a relation be-
tween a set of possible inputs and a set of possible outputs. The
function itself defines and represents the relationship. When
you apply a function such as addition to two inputs, it maps
those two inputs to an output — the sum of those numbers.
For example, let’s imagine a function named 𝑓that defines
the following relations where the first value is the input and
the second is the output:
𝑓(1) = 𝐴
𝑓(2) = 𝐵
𝑓(3) = 𝐶
The input set is {1,2,3} and the output set is {𝐴,𝐵,𝐶} .1A
crucial point about how these relations are defined: our hypo-
thetical function will always return the value 𝐴given the input
1— no exceptions!
1For those who would like precise terminology, the input set is known as the domain.
The set of possible outputs for the function is called the codomain. All domains and</p>
<p>CHAPTER 1. ANYTHING FROM ALMOST NOTHING 5
In contrast, the following is nota valid function:
𝑓(1) = 𝑋
𝑓(1) = 𝑌
𝑓(2) = 𝑍
This gets back to the referential transparency we mentioned
earlier: given the same input, the output should be predictable.
Is the following function valid?
𝑓(1) = 𝐴
𝑓(2) = 𝐴
𝑓(3) = 𝐴
Yes, having the same output for more than one input is valid.
Imagine, for example, that you need a function that tests a
positive integer for being less than 10. You’d want it to return
Truewhen the input was less than 10 and Falsefor all other
cases. In that case, several diﬀerent inputs will result in the
output True; many more will give a result of False. Diﬀerent
inputs can lead to the same output.
codomains are sets of unique values. The subset of the codomain that contains possible
outputs related to diﬀerent inputs is known as the image. The mapping between the
domain and the image or codomain need not be one-to-one; in some cases, multiple
input values will map to the same value in the image, as when a function returns either
‘true’ or ‘false’ so that many diﬀerent inputs map to each of those output values. However,
a given input should not map to multiple outputs.</p>
<p>CHAPTER 1. ANYTHING FROM ALMOST NOTHING 6
What matters here is that the relationship of inputs and
outputs is defined by the function, and that the output is pre-
dictable when you know the input and the function definition.
In the above examples, we didn’t demonstrate a relationship
between the inputs and outputs. Let’s look at an example that
does define the relationship. This function is again named 𝑓:
𝑓(𝑥) = 𝑥+1
This function takes one argument, which we have named
𝑥. The relationship between the input, 𝑥, and the output is
described in the function body. It will add 1 to whatever value
𝑥is and return that result. When we apply this function to a
value, such as 1, we substitute the value in for 𝑥:
𝑓(1) = 1+1
𝑓applied to 1 equals 1 + 1. That tells us how to map the input
to an output: 1 added to 1 becomes 2:
𝑓(1) = 2
Understanding functions in this way — as a mapping of a
set of inputs to a set of outputs — is crucial to understanding
functional programming.</p>
<p>CHAPTER 1. ANYTHING FROM ALMOST NOTHING 7
1.4 The structure of lambda terms
The lambda calculus has three basic components, or lambda
terms: expressions, variables, and abstractions. The word ex-
pression refers to a superset of all those things: an expression
can be a variable name, an abstraction, or a combination of
those things. The simplest expression is a single variable. Vari-
ables here have no meaning or value; they are only names for
potential inputs to functions.
Anabstraction is afunction . It is a lambda term that has a
head (a lambda) and a body and is applied to an argument. An
argument is an input value.
Abstractions consist of two parts: the headand the body.
The head of the function is a 𝜆(lambda) followed by a variable
name. The body of the function is another expression. So, a
simple function might look like this:
𝜆𝑥.𝑥
The variable named in the head is the parameter andbinds
all instances of that same variable in the body of the function.
That means, when we apply this function to an argument,
each𝑥in the body of the function will have the value of that
argument. We’ll demonstrate this in the next section.
In the previous section, we were talking about functions
called𝑓, but the lambda abstraction 𝜆𝑥.𝑥has no name. It is an</p>
<p>CHAPTER 1. ANYTHING FROM ALMOST NOTHING 8
anonymous function . A named function can be called by name
by another function; an anonymous function cannot.
Let’s break down the basic structure:
λ x . x
^─┬─^
└────── extent of the head of the lambda.
λ x . x
^────── the single parameter of the
function. This binds any
variables with the same name
in the body of the function.
λ x . x
^── body, the expression the lambda
returns when applied. This is a
bound variable.
The dot ( .) separates the parameters of the lambda from
the function body.
The abstraction as a whole has no name, but the reason
we call it an abstraction is that it is a generalization, or abstrac-
tion, from a concrete instance of a problem, and it abstracts
through the introduction of names. The names stand for con-
crete values, but by using named variables, we allow for the</p>
<p>CHAPTER 1. ANYTHING FROM ALMOST NOTHING 9
possibility of applying the general function to diﬀerent values
(or, perhaps even values of diﬀerent types, as we’ll see later).
When we apply the abstraction to arguments, we replace the
names with values, making it concrete.
Alpha equivalence
Often when people express this function in lambda calculus
you’ll see something like
𝜆𝑥.𝑥
The variable 𝑥here is not semantically meaningful except in
its role in that single expression. Because of this, there’s a form
of equivalence between lambda terms called alpha equivalence .
This is a way of saying that:
𝜆𝑥.𝑥
𝜆𝑑.𝑑
𝜆𝑧.𝑧
all mean the same thing. They’re all the same function.
Let’s look next at what happens when we apply this abstrac-
tion to a value.</p>
<p>CHAPTER 1. ANYTHING FROM ALMOST NOTHING 10
1.5 Beta reduction
When we apply a function to an argument, we substitute the
input expression for all instances of bound variables within
the body of the abstraction. You also eliminate the head of the
abstraction, since its only purpose was to bind a variable. This
process is called beta reduction .
Let’s use the function we had above:
𝜆𝑥.𝑥
We’ll do our first beta reduction using a number.2We apply
the function above to 2, substitute 2 for each bound variable
in the body of the function, and eliminate the head:
(𝜆𝑥.𝑥) 2
2
The only bound variable is the single 𝑥, so applying this
function to 2 returns 2. This function is the identity function.3
All it does is accept a single argument 𝑥and return that same
argument. The 𝑥has no inherent meaning, but, because it
is bound in the head of this function, when the function is
2The lambda calculus can derive numbers from lambda abstractions, rather than using
the numerals we are familiar with, but the applications can become quite cumbersome
and difficult to read.</p>
<p>CHAPTER 1. ANYTHING FROM ALMOST NOTHING 11
applied to an argument, all instances of 𝑥within the function
body must have the same value.
Let’s use an example that mixes some arithmetic into our
lambda calculus. We use the parentheses here to clarify that
thebodyexpressionis 𝑥+1. In otherwords, wearenotapplying
the function to the 1:
(𝜆𝑥.𝑥+1)
What is the result if we apply this abstraction to 2? How
about to 10?
Beta reduction is this process of applying a lambda term
to an argument, replacing the bound variables with the value
of the argument, and eliminating the head. Eliminating the
head tells you the function has been applied.
We can also apply our identity function to another lambda
abstraction:
(𝜆𝑥.𝑥)(𝜆𝑦.𝑦)
In this case, we’d substitute the entire abstraction in for 𝑥.
We’ll use a new syntax here, [𝑥 ∶= 𝑧] , to indicate that 𝑧will be
substituted for all occurrences of 𝑥(here𝑧is the function 𝜆𝑦.𝑦).
We reduce this application like this:
3Note that this is the same as the identity function in mathematical notation: u?(u?) = u? .
One diﬀerence is that u?(u?) = u? is a declaration involving a function named u?while the
above lambda abstraction isa function.</p>
<p>CHAPTER 1. ANYTHING FROM ALMOST NOTHING 12
(𝜆𝑥.𝑥)(𝜆𝑦.𝑦)
[𝑥 ∶= (𝜆𝑦.𝑦)]
𝜆𝑦.𝑦
Our final result is another identity function. There is no
argument to apply it to, so we have nothing to reduce.
Once more, but this time we’ll add another argument:
(𝜆𝑥.𝑥)(𝜆𝑦.𝑦)𝑧
Applications in the lambda calculus are left associative . That
is, unless specific parentheses suggest otherwise, they associate,
or group, to the left. So, this:
(𝜆𝑥.𝑥)(𝜆𝑦.𝑦)𝑧
can be rewritten as:
((𝜆𝑥.𝑥)(𝜆𝑦.𝑦))𝑧
Onward with the reduction:
((𝜆𝑥.𝑥)(𝜆𝑦.𝑦))𝑧
[𝑥 ∶= (𝜆𝑦.𝑦)]
(𝜆𝑦.𝑦)𝑧
[𝑦 ∶= 𝑧]
𝑧</p>
<p>CHAPTER 1. ANYTHING FROM ALMOST NOTHING 13
We can’t reduce this any further as there is nothing left to
apply, and we know nothing about 𝑧.
We’ll look at functions below that have multiple heads and
alsofree variables (that is, variables in the body that are not
bound by the head), but the basic process will remain the same.
The process of beta reduction stops when there are either no
more heads, or lambdas, left to apply or no more arguments
to apply functions to. A computation therefore consists of an
initial lambda expression (or two, if you want to separate the
function and its input) plus a finite sequence of lambda terms,
each deduced from the preceding term by one application of
beta reduction. We keep following the rules of application,
substituting arguments in for bound variables until there are
no more heads left to evaluate or no more arguments to apply
them to.
Free variables
The purpose of the head of the function is to tell us which
variables to replace when we apply our function, that is, to
bind the variables. A bound variable must have the same value
throughout the expression.
But sometimes the body expression has variables that are
not named in the head. We call those variables free variables .
In the following expression:</p>
<p>CHAPTER 1. ANYTHING FROM ALMOST NOTHING 14
𝜆𝑥.𝑥𝑦
The𝑥in the body is a bound variable because it is named in
the head of the function, while the 𝑦is a free variable because
it is not. When we apply this function to an argument, nothing
can be done with the 𝑦. It remains irreducible.
That whole abstraction can be applied to an argument, 𝑧,
like this: (𝜆𝑥.𝑥𝑦)𝑧 . We’ll show an intermediate step, using the
:=syntax we introduced above, that most lambda calculus
literature does not show:
1.(𝜆𝑥.𝑥𝑦)𝑧
We apply the lambda to the argument 𝑧.
2.(𝜆[𝑥 ∶= 𝑧].𝑥𝑦)
Since𝑥is the bound variable, all instances of 𝑥in the body
of the function will be replaced with 𝑧. The head will be
eliminated, and we replace any 𝑥in the body with a 𝑧.
3.𝑧𝑦
The head has been applied away, and there are no more
heads or bound variables. Since we know nothing about
𝑧or𝑦, we can reduce this no further.
Note that alpha equivalence does not apply to free vari-
ables. That is, 𝜆𝑥.𝑥𝑧and𝜆𝑥.𝑥𝑦are not equivalent because 𝑧
and𝑦might be diﬀerent things. However, 𝜆𝑥𝑦.𝑦𝑥 and𝜆𝑎𝑏.𝑏𝑎</p>
<p>CHAPTER 1. ANYTHING FROM ALMOST NOTHING 15
are equivalent due to alpha equivalence, as are 𝜆𝑥.𝑥𝑧and𝜆𝑦.𝑦𝑧
because the free variable is left alone.
1.6 Multiple arguments
Each lambda can only bind one parameter and can only accept
one argument. Functions that require multiple arguments
have multiple, nested heads. When you apply it once and
eliminate the first (leftmost) head, the next one is applied and
so on. This formulation was originally discovered by Moses
Schönfinkel in the 1920s but was later rediscovered and named
after Haskell Curry and is commonly called currying .
What we mean by this description is that the following:
𝜆𝑥𝑦.𝑥𝑦
is a convenient shorthand for two nested lambdas (one for
each argument, 𝑥and𝑦):
𝜆𝑥.(𝜆𝑦.𝑥𝑦)
When you apply the first argument, you’re binding 𝑥, elimi-
nating the outer lambda, and have 𝜆𝑦.𝑥𝑦with x being whatever
the outer lambda was bound to.
To try to make this a little more concrete, let’s suppose
that we apply these lambdas to specific values. First, a simple
example with the identity function:</p>
<p>CHAPTER 1. ANYTHING FROM ALMOST NOTHING 16
1.𝜆𝑥.𝑥
2.(𝜆𝑥.𝑥) 1
3.[𝑥 ∶= 1]
4.1
Now let’s look at a “multiple” argument lambda:
1.𝜆𝑥𝑦.𝑥𝑦
2.(𝜆𝑥𝑦.𝑥𝑦) 1 2
3.(𝜆𝑥.(𝜆𝑦.𝑥𝑦)) 1 2
4.[𝑥 ∶= 1]
5.(𝜆𝑦.1𝑦) 2
6.[𝑦 ∶= 2]
7.1 2
That wasn’t too interesting because it’s like nested identity
functions! We can’t meaningfully apply a 1 to a 2. Let’s try
something diﬀerent:
1.𝜆𝑥𝑦.𝑥𝑦
2.(𝜆𝑥𝑦.𝑥𝑦)(𝜆𝑧.𝑎) 1</p>
<p>CHAPTER 1. ANYTHING FROM ALMOST NOTHING 17
3.(𝜆𝑥.(𝜆𝑦.𝑥𝑦))(𝜆𝑧.𝑎) 1
4.[𝑥 ∶= (𝜆𝑧.𝑎)]
5.(𝜆𝑦.(𝜆𝑧.𝑎)𝑦) 1
6.[𝑦 ∶= 1]
7.(𝜆𝑧.𝑎) 1 We still can apply this one more time.
8.[𝑧 ∶= 1] But there is no 𝑧in the body of the function, so
there is nowhere to put a 1. We eliminate the head, and
the final result is
9.𝑎
It’s more common in academic lambda calculus materi-
als to refer to abstract variables rather than concrete values.
The process of beta reduction is the same, regardless. The
lambda calculus is a process or method, like a game with a few
simple rules for transforming lambdas, but no specific mean-
ing. We’ve introduced concrete values to make the reduction
somewhat easier to see.
The next example uses only abstract variables. Due to al-
pha equivalence, you sometimes see expressions in lambda
calculus literature such as:
(𝜆𝑥𝑦.𝑥𝑥𝑦)(𝜆𝑥.𝑥𝑦)(𝜆𝑥.𝑥𝑧)</p>
<p>CHAPTER 1. ANYTHING FROM ALMOST NOTHING 18
The substitution process can become a tangle of 𝑥s that are
not the same 𝑥because each was bound by a diﬀerent head. To
help make the reduction easier to read we’re going to use dif-
ferent variables in each abstraction, but it’s worth emphasizing
that the name of the variable (the letter) has no meaning or
significance:
1.(𝜆𝑥𝑦𝑧.𝑥𝑧(𝑦𝑧))(𝜆𝑚𝑛.𝑚)(𝜆𝑝.𝑝)
2.(𝜆𝑥.𝜆𝑦.𝜆𝑧.𝑥𝑧(𝑦𝑧))(𝜆𝑚.𝜆𝑛.𝑚)(𝜆𝑝.𝑝)
We’ve not reduced or applied anything here, but made
the currying explicit.
3.(𝜆𝑦.𝜆𝑧.(𝜆𝑚.𝜆𝑛.𝑚)𝑧(𝑦𝑧))(𝜆𝑝.𝑝)
Ourfirstreductionstepwastoapplytheoutermostlambda,
which was binding the 𝑥, to the first argument, (𝜆𝑚.𝜆𝑛.𝑚) .
4.𝜆𝑧.(𝜆𝑚.𝜆𝑛.𝑚)(𝑧)((𝜆𝑝.𝑝)𝑧)
We applied the 𝑦and replaced the single occurrence of
𝑦with the next argument, the term 𝜆𝑝.𝑝. The outermost
lambda binding 𝑧is, at this point, irreducible because it
has no argument to apply to. What remains is to go inside
the terms one layer at a time until we find something
reducible.
5.𝜆𝑧.(𝜆𝑛.𝑧)((𝜆𝑝.𝑝)𝑧)
We can apply the lambda binding 𝑚to the argument 𝑧.
We keep searching for terms we can apply. The next thing</p>
<p>CHAPTER 1. ANYTHING FROM ALMOST NOTHING 19
we can apply is the lambda binding 𝑛to the lambda term
((𝜆𝑝.𝑝)𝑧) .
6.𝜆𝑧.𝑧
In the final step, the reduction takes a turn that might look
slightly odd. Here the outermost, leftmost reducible term
is𝜆𝑛.𝑧applied to the entirety of ((𝜆𝑝.𝑝)𝑧) . As we saw in
an example above, it doesn’t matter what 𝑛got bound to,
𝜆𝑛.𝑧unconditionally tosses the argument and returns 𝑧.
So, we are left with an irreducible lambda expression.
Intermission: Equivalence Exercises
We’ll give you a lambda expression. Keeping in mind both
alpha equivalence and how multiple heads are nested, choose
an answer that is equivalent to the listed lambda term.
1.𝜆𝑥𝑦.𝑥𝑧
a)𝜆𝑥𝑧.𝑥𝑧
b)𝜆𝑚𝑛.𝑚𝑧
c)𝜆𝑧.(𝜆𝑥.𝑥𝑧)
2.𝜆𝑥𝑦.𝑥𝑥𝑦
a)𝜆𝑚𝑛.𝑚𝑛𝑝
b)𝜆𝑥.(𝜆𝑦.𝑥𝑦)</p>
<p>CHAPTER 1. ANYTHING FROM ALMOST NOTHING 20
c)𝜆𝑎.(𝜆𝑏.𝑎𝑎𝑏)
3.𝜆𝑥𝑦𝑧.𝑧𝑥
a)𝜆𝑥.(𝜆𝑦.(𝜆𝑧.𝑧))
b)𝜆𝑡𝑜𝑠.𝑠𝑡
c)𝜆𝑚𝑛𝑝.𝑚𝑛
1.7 Evaluation is simplification
There are multiple normal forms in lambda calculus, but here
when we refer to normal form we mean beta normal form . Beta
normalformiswhenyoucannotbetareduce(applylambdasto
arguments) the terms any further. This corresponds to a fully
evaluated expression, or, in programming, a fully executed
program. This is important to know so that you know when
you’re done evaluating an expression. It’s also valuable to have
an appreciation for evaluation as a form of simplification when
you get to the Haskell code as well.
Don’t be intimidated by calling the reduced form of an
expression its normal form. When you want to say “2,” do
you say 2000/1000 each time or do you say 2? The expression
2000/1000 is not fully evaluated because the division function
has been fully applied (two arguments), so it could be reduced,
or evaluated. In other words, there’s a simpler form it can be</p>
<p>CHAPTER 1. ANYTHING FROM ALMOST NOTHING 21
reduced to — the number two. The normal form, therefore,
is 2.
The point is that if you have a function, such as (/), satu-
rated (all arguments applied) but you haven’t yet simplified it
to the final result then it is not fully evaluated, only applied.
Application is what makes evaluation/simplification possible.
Similarly, the normal form of the following is 600:
(10+2)∗100/2
We cannot reduce the number 600 any further. There are
no more functions that we can beta reduce. Normal form
means there is nothing left that can be reduced.
The identity function, 𝜆𝑥.𝑥, is fully reduced (that is, in nor-
mal form) because it hasn’t yet been applied to anything. How-
ever,(𝜆𝑥.𝑥)𝑧 isnotin beta normal form because the identity
function hasn’t been applied to a free variable 𝑧and hasn’t
been reduced. If we did reduce it, the final result, in beta
normal form, would be 𝑧.
1.8 Combinators
A combinator is a lambda term with no free variables. Combi-
nators, as the name suggests, serve only to combine the argu-
ments they are given.</p>
<p>CHAPTER 1. ANYTHING FROM ALMOST NOTHING 22
So the following are combinators because every term in the
body occurs in the head:
1.𝜆𝑥.𝑥
𝑥is the only variable and is bound because it is bound by
the enclosing lambda.
2.𝜆𝑥𝑦.𝑥
3.𝜆𝑥𝑦𝑧.𝑥𝑧(𝑦𝑧)
And the following are not because there’s one or more free
variables:
1.𝜆𝑦.𝑥
Here𝑦is bound (it occurs in the head of the lambda) but
𝑥is free.
2.𝜆𝑥.𝑥𝑧
𝑥is bound and is used in the body, but 𝑧is free.
We won’t have a lot to say about combinators per se. The
point is to call out a special class of lambda expressions that
canonlycombine the arguments it is given, without injecting
any new values or random data.</p>
<p>CHAPTER 1. ANYTHING FROM ALMOST NOTHING 23
1.9 Divergence
Not all reducible lambda terms reduce neatly to a beta normal
form. This isn’t because they’re already fully reduced, but
rather because they diverge . Divergence here means that the
reduction process never terminates or ends. Reducing terms
should ordinarily converge to beta normal form, and diver-
gence is the opposite of convergence, or normal form. Here’s
an example of a lambda term called omega that diverges:
1.(𝜆𝑥.𝑥𝑥)(𝜆𝑥.𝑥𝑥)
𝑥in the first lambda’s head becomes the second lambda
2.([𝑥 ∶= (𝜆𝑥.𝑥𝑥)]𝑥𝑥)
Using[𝑣𝑎𝑟 ∶= 𝑒𝑥𝑝𝑟] to denote what 𝑥has been bound to.
3.(𝜆𝑥.𝑥𝑥)(𝜆𝑥.𝑥𝑥)
Substituting (𝜆𝑥.𝑥𝑥) for each occurence of 𝑥. We’re back
to where we started and this reduction process never ends
— we can say omega diverges.
This matters in programming because terms that diverge
are terms that don’t produce an answer or meaningful result.
Understanding what will terminate means understanding what
programs will do useful work and return the answer we want.
We’ll cover this idea more later.</p>
<p>CHAPTER 1. ANYTHING FROM ALMOST NOTHING 24
1.10 Summary
The main points you should take away from this chapter are:
•Functional programming is based on expressions that in-
clude variables or constant values, expressions combined
with other expressions, and functions.
•Functions have a head and a body and are those expres-
sions that can be applied to arguments and reduced, or
evaluated, to a result.
•Variables may be bound in the function declaration, and
every time a bound variable shows up in a function, it has
the same value.
•All functions take one argument and return one result.
•Functions are a mapping of a set of inputs to a set of
outputs. Given the same input, they always return the
same result.
These things all apply to Haskell, as they do to any pure
functional languages, because semantically Haskell is a lambda
calculus. Haskell is a typedlambda calculus — more on types
later — with a lot of surface-level decoration sprinkled on top,
to make it easier for humans to write, but the semantics of the
core language are the same as the lambda calculus. That is,</p>
<p>CHAPTER 1. ANYTHING FROM ALMOST NOTHING 25
the meaning of Haskell programs is centered around evaluat-
ing expressions rather than executing instructions, although
Haskell has a way to execute instructions, too. We will still
be making reference to the lambda calculus when we write
about all the later, apparently very complex topics: function
composition, monads, parser combinators. Don’t worry if you
don’t know those words yet. If you understood this chapter,
you have the foundation you need to understand them all.
1.11 Chapter Exercises
We’re going to do the following exercises a bit diﬀerently than
what you’ll see in the rest of the book, as we will be providing
some answers and explanations for the questions below.
Combinators Determine if each of the following are combi-
nators or not.
1.𝜆𝑥.𝑥𝑥𝑥
2.𝜆𝑥𝑦.𝑧𝑥
3.𝜆𝑥𝑦𝑧.𝑥𝑦(𝑧𝑥)
4.𝜆𝑥𝑦𝑧.𝑥𝑦(𝑧𝑥𝑦)
5.𝜆𝑥𝑦.𝑥𝑦(𝑧𝑥𝑦)</p>
<p>CHAPTER 1. ANYTHING FROM ALMOST NOTHING 26
Normal form or diverge? Determine if each of the following
can be reduced to a normal form or if they diverge.
1.𝜆𝑥.𝑥𝑥𝑥
2.(𝜆𝑧.𝑧𝑧)(𝜆𝑦.𝑦𝑦)
3.(𝜆𝑥.𝑥𝑥𝑥)𝑧
Beta reduce Evaluate (that is, beta reduce) each of the fol-
lowing expressions to normal form. We strongly recommend
writing out the steps on paper with a pencil or pen.
1.(𝜆𝑎𝑏𝑐.𝑐𝑏𝑎)𝑧𝑧(𝜆𝑤𝑣.𝑤)
2.(𝜆𝑥.𝜆𝑦.𝑥𝑦𝑦)(𝜆𝑎.𝑎)𝑏
3.(𝜆𝑦.𝑦)(𝜆𝑥.𝑥𝑥)(𝜆𝑧.𝑧𝑞)
4.(𝜆𝑧.𝑧)(𝜆𝑧.𝑧𝑧)(𝜆𝑧.𝑧𝑦)
Hint: alpha equivalence.
5.(𝜆𝑥.𝜆𝑦.𝑥𝑦𝑦)(𝜆𝑦.𝑦)𝑦
6.(𝜆𝑎.𝑎𝑎)(𝜆𝑏.𝑏𝑎)𝑐
7.(𝜆𝑥𝑦𝑧.𝑥𝑧(𝑦𝑧))(𝜆𝑥.𝑧)(𝜆𝑥.𝑎)</p>
<p>CHAPTER 1. ANYTHING FROM ALMOST NOTHING 27
1.12 Answers
Please note: At this time, this is the only chapter in the book for
which we have provided answers. We provide them here due
to the importance of being able to check your understanding
of this material and the relative difficulty of checking answers
that you probably wrote by hand in a notebook.
Equivalence Exercises
1.b
2.c
3.b
Combinators
1.𝜆𝑥.𝑥𝑥𝑥 is indeed a combinator, it refers only to the variable
x which is introduced as an argument.
2.𝜆𝑥𝑦.𝑧𝑥 is not a combinator, the variable z was not intro-
duced as an argument and is thus a free variable.
3.𝜆𝑥𝑦𝑧.𝑥𝑦(𝑧𝑥) is a combinator, all terms are bound. The head
is𝜆𝑥𝑦𝑧.and the body is 𝑥𝑦(𝑧𝑥). None of the arguments in
the head have been applied so it’s irreducible. The vari-
ables x, y, and z are all bound in the head and are not free.</p>
<p>CHAPTER 1. ANYTHING FROM ALMOST NOTHING 28
This makes the lambda a combinator - no occurrences of
free variables.
4.𝜆𝑥𝑦𝑧.𝑥𝑦(𝑧𝑥𝑦) is a combinator. The lambda has the head
𝜆𝑥𝑦𝑧.and the body: 𝑥𝑦(𝑧𝑥𝑦) . Again, none of the arguments
have been applied so it’s irreducible. All that is diﬀerent
is that the bound variable y is referenced twice rather
than once. There are still no free variables so this is also a
combinator.
5.𝜆𝑥𝑦.𝑥𝑦(𝑧𝑥𝑦) is not a combinator, z is free. Note that z isn’t
bound in the head.
Normal form or diverge?
1.𝜆𝑥.𝑥𝑥𝑥 doesn’t diverge, has no further reduction steps. If
it had been applied to itself, it would diverge, but by itself
does not as it is already in normal form.
2.(𝜆𝑧.𝑧𝑧)(𝜆𝑦.𝑦𝑦) diverges, it never reaches a point where the
reduction is done. This is the omega term we showed you
earlier, with diﬀerent names for the bindings. It’s alpha
equivalent to(𝜆𝑥.𝑥𝑥)(𝜆𝑥.𝑥𝑥) .
3.(𝜆𝑥.𝑥𝑥𝑥)𝑧 doesn’t diverge, it reduces to 𝑧𝑧𝑧.
Betareduce Thefollowingareevaluatedin normalorder , which
is where terms in the outer-most and left-most positions get</p>
<p>CHAPTER 1. ANYTHING FROM ALMOST NOTHING 29
evaluated (applied) first. This means that if all terms are in
the outermost position (none are nested), then it’s left-to-right
application order.
1.(𝜆𝑎𝑏𝑐.𝑐𝑏𝑎)𝑧𝑧(𝜆𝑤𝑣.𝑤)
(𝜆𝑎.𝜆𝑏.𝜆𝑐.𝑐𝑏𝑎)(𝑧)𝑧(𝜆𝑤.𝜆𝑣.𝑤)
(𝜆𝑏.𝜆𝑐.𝑐𝑏𝑧)(𝑧)(𝜆𝑤.𝜆𝑣.𝑤)
(𝜆𝑐.𝑐𝑧𝑧)(𝜆𝑤.𝜆𝑣.𝑤)
(𝜆𝑤.𝜆𝑣.𝑤)(𝑧)𝑧
(𝜆𝑣.𝑧)(𝑧)
𝑧
2.(𝜆𝑥.𝜆𝑦.𝑥𝑦𝑦)(𝜆𝑎.𝑎)𝑏
(𝜆𝑦(𝜆𝑎.𝑎)𝑦𝑦)(𝑏)
(𝜆𝑎.𝑎)(𝑏)𝑏
𝑏𝑏
3.(𝜆𝑦.𝑦)(𝜆𝑥.𝑥𝑥)(𝜆𝑧.𝑧𝑞)
(𝜆𝑥.𝑥𝑥)(𝜆𝑧.𝑧𝑞)
(𝜆𝑧.𝑧𝑞)(𝜆𝑧.𝑧𝑞)
(𝜆𝑧.𝑧𝑞)(𝑞)
𝑞𝑞
4.(𝜆𝑧.𝑧)(𝜆𝑧.𝑧𝑧)(𝜆𝑧.𝑧𝑦)
(𝜆𝑧.𝑧𝑧)(𝜆𝑧.𝑧𝑦)
(𝜆𝑧.𝑧𝑦)(𝜆𝑧.𝑧𝑦)
(𝜆𝑧.𝑧𝑦)(𝑦)
𝑦𝑦</p>
<p>CHAPTER 1. ANYTHING FROM ALMOST NOTHING 30
5.(𝜆𝑥.𝜆𝑦.𝑥𝑦𝑦)(𝜆𝑦.𝑦)𝑦
(𝜆𝑦(𝜆𝑦.𝑦)𝑦𝑦)(𝑦)
(𝜆𝑦.𝑦)(𝑦)𝑦
𝑦𝑦
6.(𝜆𝑎.𝑎𝑎)(𝜆𝑏.𝑏𝑎)𝑐
(𝜆𝑏.𝑏𝑎)(𝜆𝑏.𝑏𝑎)𝑐
(𝜆𝑏.𝑏𝑎)(𝑎)𝑐
𝑎𝑎𝑐
7.Steps we took
a)(𝜆𝑥𝑦𝑧.𝑥𝑧(𝑦𝑧))(𝜆𝑥.𝑧)(𝜆𝑥.𝑎)
b)(𝜆𝑥.𝜆𝑦.𝜆𝑧.𝑥𝑧(𝑦𝑧))(𝜆𝑥.𝑧)(𝜆𝑥.𝑎)
c)(𝜆𝑦.𝜆𝑧1(𝜆𝑥.𝑧)𝑧1(𝑦𝑧1))(𝜆𝑥.𝑎)
d)(𝜆𝑧1.(𝜆𝑥.𝑧)(𝑧1)((𝜆𝑥.𝑎)𝑧1))
e)(𝜆𝑧1.𝑧((𝜆𝑥.𝑎)(𝑧1)))
f)(𝜆𝑧1.𝑧𝑎) The𝑧1notation allows us to distinguish two
variables named 𝑧that came from diﬀerent places.
One is bound by the first head; the second is a free
variable in the second lambda expression.
How we got there, step by step
a)Our expression we’ll reduce.
b)Add the implied lambdas to introduce each argument.</p>
<p>CHAPTER 1. ANYTHING FROM ALMOST NOTHING 31
c)Apply the leftmost 𝑥and bind it to (𝜆𝑥.𝑧), rename
leftmost 𝑧to𝑧1for clarity to avoid confusion with the
other z. Hereafter, “z” is exclusively the z in (𝜆𝑥.𝑧).
d)Apply𝑦, it gets bound to (𝜆𝑥.𝑎).
e)Can’t apply z1 to anything, evaluation strategy is nor-
mal order so leftmost outermost is the order of the
day. Our leftmost, outermost lambda has no remain-
ing arguments to be applied so we now examine the
terms nested within to see if they are in normal form.
(𝜆𝑥.𝑧)gets applied to 𝑧1, tosses the 𝑧1away and returns
𝑧.𝑧is now being applied to ((𝜆𝑥.𝑎)(𝑧1)) .
f)Cannot reduce 𝑧further, it’s free and we know noth-
ing, so we go inside yet another nesting and reduce
((𝜆𝑥.𝑎)(𝑧1)) .𝜆𝑥.𝑎gets applied to 𝑧1, but tosses it away
and returns the free variable 𝑎. The𝑎is now part of
the body of that expression. All of our terms are in
normal order now.
1.13 Definitions
1.Thelambda in lambda calculus is the greek letter 𝜆used
to introduce, or abstract, arguments for binding in an
expression.
2.A lambda abstraction is an anonymous function or lambda</p>
<p>CHAPTER 1. ANYTHING FROM ALMOST NOTHING 32
term.
(𝜆𝑥.𝑥+1)
The head of the expression, 𝜆𝑥., abstracts out the term
𝑥+1. We can apply it to any 𝑥and recompute diﬀerent
results for each 𝑥we applied the lambda to.
3.Application is how one evaluates or reduces lambdas, this
binds the argument to whatever the lambda was applied
to. Computations are performed in lambda calculus by
applying lambdas to arguments until you run out of ar-
guments to apply lambdas to.
(𝜆𝑥.𝑥)1
This example reduces to 1, the identity 𝜆𝑥.𝑥was applied
to the value 1,𝑥was bound to 1, and the lambda’s body is
𝑥, so it just kicks the 1out. In a sense, applying the 𝜆𝑥.𝑥
consumed it. Wereduced the amount of structure we had.
4.Lambda calculus is a formal system for expressing pro-
grams in terms of abstraction and application.
5.Normal order is a common evaluation strategy in lambda
calculi. Normal order means evaluating (ie, applying or
beta reducing) the leftmost outermost lambdas first, eval-
uating terms nested within after you’ve run out of argu-
ments to apply. Normal order isn’t how Haskell code is
evaluated - it’s call-by-need instead. We’ll explain this more</p>
<p>CHAPTER 1. ANYTHING FROM ALMOST NOTHING 33
later. Answers to the evaluation exercises were written in
normal order.
1.14 Follow-up resources
These are optional and intended only to oﬀer suggestions on
how you might deepen your understanding of the preceding
topic. Ordered approximately from most approachable to
most thorough.
1.Raul Rojas. A Tutorial Introduction to the Lambda Calcu-
lus
http://www.inf.fu-berlin.de/lehre/WS03/alpi/lambda.pdf
2.Henk Barendregt; Erik Barendsen. Introduction to
Lambda Calculus
http://www.cse.chalmers.se/research/group/logic/
TypesSS05/Extra/geuvers.pdf
3.Jean-Yves Girard; P. Taylor; Yves Lafon. Proofs and Types
http://www.paultaylor.eu/stable/prot.pdf</p>
<p>Chapter 2
Hello, Haskell!
Functions are beacons of
constancy in a sea of
turmoil.
Mike Hammond
34</p>
<p>CHAPTER 2. BASIC EXPRESSIONS AND FUNCTIONS 35
2.1 Hello, Haskell
Welcome to your first step in learning Haskell. Before you be-
gin with the main course of this book, you will need to install
the necessary tools in order to complete the exercises as you
work through the book. At this time, we recommend installing
Stack, which will install GHC Haskell, the interactive environ-
ment called GHCi, and a project build tool and dependency
manager all at once.
You can find the installation instructions online at http://
docs.haskellstack.org/en/stable/README/ , and there is also great
documentation that can help you get started using Stack. You
can also find installation instructions at https://github.com/
bitemyapp/learnhaskell ; there you will also find advice on learn-
ing Haskell and links to more exercises that may supplement
what you’re doing with this book.
Therestofthischapterwillassumethatyouhavecompleted
the installation and are ready to begin working. In this chapter,
you will
•use Haskell code in the interactive environment and also
from source files;
•understand the building blocks of Haskell: expressions
and functions;</p>
<p>CHAPTER 2. BASIC EXPRESSIONS AND FUNCTIONS 36
•learn some features of Haskell syntax and conventions of
good Haskell style;
•modify simple functions.
2.2 Interacting with Haskell code
Haskell oﬀers two primary ways of working with code. The
first is inputting it directly into the interactive environment
known as GHCi, or the REPL. The second is typing it into a
text editor, saving, and then loading that source file into GHCi.
This section oﬀers an introduction to each method.
Using the REPL
REPL is an acronym short for read-eval-print loop . REPLs are
interactive programming environments where you can input
code, have it evaluated, and see the result. They originated
with Lisp but are now common to modern programming
languages including Haskell.
Assuming you’ve completed your installation, you should
be able to open your terminal or command prompt, type ghci
orstack ghci1, hit enter, and see something like the following:
GHCi, version 7.10.3:
1If you have installed GHC outside of Stack, then you should be able to open it with
just the ghcicommand, but if your only GHC installation is what Stack installed, then you
will need stack ghci .</p>
<p>CHAPTER 2. BASIC EXPRESSIONS AND FUNCTIONS 37
http://www.haskell.org/ghc/ :? for help
Prelude&gt;
Ifyouused stack ghci2therewasprobablyalotmorestartup
text, and the prompt might be something other than Prelude .
That’s all fine. You may also have a diﬀerent version of GHC.
As long as your GHC version is between 7.8 and 8.0, it should
be compatible with everything in this book.
Now try entering some simple arithmetic at your prompt:
Prelude&gt; 2 + 2
4
Prelude&gt; 7 &lt; 9
True
Prelude&gt; 10 ^ 2
100
If you can enter simple equations at the prompt and get the
expected results, congratulations — you are now a functional
programmer! More to the point, your REPL is working well
and you are ready to proceed.
To exit GHCi, use the command :quitor:q.
What is Prelude ?Prelude is a library of standard functions.
Opening GHCi or Stack GHCi automatically loads those func-
2At this point in the book, you don’t need to use stack ghci , but in later chapters when
we’re importing a lot of modules and building projects, it will be much more convenient
to use it.</p>
<p>CHAPTER 2. BASIC EXPRESSIONS AND FUNCTIONS 38
tions so they can be used without needing to do anything
special. You can turn Prelude oﬀ, as we will show you later, and
there are alternative preludes, though we won’t use them in
the book. Prelude is contained in Haskell’s basepackage, which
can be found at https://www.stackage.org/package/base . You’ll
see us mention sometimes that something or other is “in base”
which means it’s contained in that large standard package.
GHCi commands
Throughout the book, we’ll be using GHCi commands, such
as:quitand:infoin the REPL. Special commands that only
GHCi understands begin with the :character. :quitisnot
Haskell code; it’s just a GHCi feature.
We will present them in the text spelled out, but they can
generally be abbreviated to just the colon and the first letter.
That is, :quitbecomes :q,:infobecomes :iand so forth. It’s
good to type the word out the first few times you use it, to help
you remember what the abbreviation stands for, but after a
few mentions, we will start abbreviating them.
Working from source files
As nice as REPLs are, usually you want to store code in a file
so you can build it incrementally. Almost all nontrivial pro-
gramming you do will involve editing libraries or applications
made of nested directories containing files with Haskell code</p>
<p>CHAPTER 2. BASIC EXPRESSIONS AND FUNCTIONS 39
in them. The basic process is to have the code and imports
(more on that later) in a file, load it into the REPL, and interact
with it there as you’re building, modifying, and testing it.
You’ll need a file named test.hs . The.hsfile extension de-
notes a Haskell source code file. Depending on your setup and
the workflow you’re comfortable with, you can make a file by
that name and then open it in your text editor or you can open
your text editor, open a new file, and then save the file with
that file name.
Then enter the following code into the file and save it:
sayHello ::String-&gt;IO()
sayHello x=
putStrLn ( &quot;Hello, &quot; ++x++&quot;!&quot;)
Here,::is a way to write down a type signature. You can
think of it as saying hasthetype . So,sayHello has the type String
-&gt; IO () . These first chapters are focused on syntax, and we’ll
talk about types in a later chapter.
Theninthesamedirectorywhereyou’vestoredyour test.hs
file, open your ghciREPL and do the following:
Prelude&gt; :load test.hs
Prelude&gt; sayHello &quot;Haskell&quot;
Hello, Haskell!
Prelude&gt;</p>
<p>CHAPTER 2. BASIC EXPRESSIONS AND FUNCTIONS 40
After using :loadto load your test.hs , thesayHello function
is visible in the REPL and you can pass it a string argument,
such as “Haskell” (note the quotation marks), and see the out-
put.
You may notice that after loading code from a source file,
the GHCi prompt is no longer Prelude&gt; . To return to the
Prelude&gt; prompt, usethecommand :m, whichisshortfor :module .
This will unload the file from GHCi, so the code in that file
will no longer be in scope in your REPL.
2.3 Understanding expressions
Everything in Haskell is an expression or declaration. Expres-
sionsmay be values, combinations of values, and/or functions
applied to values. Expressions evaluate to a result. In the case
of a literal value, the evaluation is trivial as it only evaluates
to itself. In the case of an arithmetic equation, the evaluation
process is the process of computing the operator and its ar-
guments, as you might expect. But, even though not all of
your programs will be about doing arithmetic, all of Haskell’s
expressions work in a similar way, evaluating to a result in a
predictable, transparent manner. Expressions are the building
blocks of our programs, and programs themselves are one big
expression made of smaller expressions.
Regarding declarations , it suffices to say for now that they
are top-level bindings which allows us to name expressions.</p>
<p>CHAPTER 2. BASIC EXPRESSIONS AND FUNCTIONS 41
We can then use those names to refer to them multiple times
without copying and pasting the expressions.
The following are all expressions:
1
1+1
&quot;Icarus&quot;
Each can be examined in the GHCi REPL by entering the
code at the prompt, then hitting ‘enter’ to see the result of
evaluating the expression. The numeric value 1, for example,
has no further reduction step, so it stands for itself.
If you haven’t already, open up your terminal and get your
REPL going to start following along with the code examples.
When we enter this into GHCi:
Prelude&gt; 1
1
We see 1 printed because it cannot be reduced any further.
In the next example, GHCi reduces the expression 1 + 2to
3, then prints the number 3. The reduction terminates with
the value 3 because there are no more terms to evaluate:
Prelude&gt; 1 + 2
3</p>
<p>CHAPTER 2. BASIC EXPRESSIONS AND FUNCTIONS 42
Expressions can be nested in numbers limited only by our
willingness to take the time to write them down, much like in
arithmetic:
Prelude&gt; (1 + 2) * 3
9
Prelude&gt; ((1 + 2) * 3) + 100
109
You can keep expanding on this, nesting as many expres-
sions as you’d like and evaluating them. But, we don’t have to
limit ourselves to expressions such as these.
Normal form We say that expressions are in normal form
when there are no more evaluation steps that can be taken,
or, put diﬀerently, when they’ve reached an irreducible form.
The normal form of 1 + 1is2. Why? Because the expression 1</p>
<ul>
<li>1can be evaluated or reduced by applying the addition oper-
ator to the two arguments. In other words, 1 + 1is a reducible
expression, while 2is an expression but is no longer reducible
— it can’t evaluate into anything other than itself. Reducible ex-
pressions are also called redexes . While we will generally refer
to this process as evaluation or reduction, you may also hear
it called “normalizing” or “executing” an expression, though
these are somewhat imprecise.</li>
</ul>
<p>CHAPTER 2. BASIC EXPRESSIONS AND FUNCTIONS 43
2.4 Functions
Expressions are the most basic unit of a Haskell program, and
functions are a specific type of expression. Functions in Haskell
are related to functions in mathematics, which is to say they
map an input or set of inputs to an output. A function is an
expression that is applied to an argument and always returns a
result. Because they are built purely of expressions, they will
always evaluate to the same result when given the same values.
As in the lambda calculus, all functions in Haskell take one
argument and return one result. The way to think of this is that,
in Haskell, when it seems we are passing multiple arguments to
a function, we are actually applying a series of nested functions,
each to one argument. This is called currying .
You may have noticed that the expressions we’ve looked
at so far use literal values with no variables or abstractions.
Functions allow us to abstract the parts of code we’d want to
reuse for diﬀerent literal values. Instead of nesting addition
expressions, for example, we could write a function that would
add the value we wanted wherever we called that function.
Forexample, sayyouhadabunchofexpressionsyouneeded
to multiply by 3. You could keep entering them as individual
expressions like this:
Prelude&gt; (1 + 2) * 3</p>
<p>CHAPTER 2. BASIC EXPRESSIONS AND FUNCTIONS 44
9
Prelude&gt; (4 + 5) * 3
27
Prelude&gt; (10 + 5) * 3
45
But you don’t want to do that. Functions are how we factor
out the pattern into something we can reuse with diﬀerent in-
puts. You do that by naming the function and introducing an
independent variable as the argument to the function. Func-
tions can also appear in the expressions that form the bodies
of other functions or be used as arguments to functions, just
as any other value can be.
In this case, we have a series of expressions that we want to
multiply by 3. Let’s think in terms of a function: what part is
common to all the expressions? What part varies? We know we
have to give functions a name and apply them to an argument,
so what could we call this function and what sort of argument
might we apply it to?
The common pattern is the * 3bit. The part that varies
is the addition expression before it, so we will make that a
variable. Wewillnameourfunctionandapplyittothevariable.
When we input a value for the variable, our function will
evaluate that, multiply it by 3, and return a result. In the next
section, we will formalize this into a Haskell function.</p>
<p>CHAPTER 2. BASIC EXPRESSIONS AND FUNCTIONS 45
Defining functions
Function definitions all share a few things in common. First,
they start with the name of the function. This is followed by
the formal parameters3of the function, separated only by white
space. Next there is an equal sign, which expresses equality of
the terms. Finally there is an expression that is the body of
the function and can be evaluated to return a value.
Defining functions in a Haskell source code file and in GHCi
are a little diﬀerent. To introduce definitions of values or
functions in GHCi, you must use let,4which looks like this:
Prelude&gt; let triple x = x * 3
In a source file we would enter it like this:
triplex=x<em>3
Let’s examine each part of that:
triple x =x</em>3
-- [1] [2] [3] [ 4 ]
3In practice, the terms argument andparameter are often used interchangeably, but
there is a diﬀerence. Argument properly refers to the value(s) that are passed to the
function’s parameters when the function is applied, not to the variables that represent
them in the function definition (or those in the type signature). See the definitions at the
end of the chapter for more information.
4This has changed as of the release of GHC 8.0.1; using letin declarations in GHCi is
no longer necessary. As we assume most readers of this edition will be using an earlier
version of GHC, we have kept the letnotation throughout the book, and this shouldn’t
cause any errors or breakage.</p>
<p>CHAPTER 2. BASIC EXPRESSIONS AND FUNCTIONS 46
1.This is the name of the function we are defining; it is a
function declaration . Note that it begins with a lowercase
letter.
2.This is the parameter of the function. The parameters
of our function correspond to the head of a lambda and
bind variables that appear in the body expression.
3.The=is used to define (or declare ) values and functions.
This isnothow we test for equality between two values in
Haskell.
4.This is the body of the function, an expression that could
be evaluated if the function is applied to a value. If triple
is applied, the argument it’s applied to will be the value
to which the 𝑥is bound. Here the expression x * 3consti-
tutes the body of the function. So, if you have an expres-
sion like triple 6 ,𝑥is bound to 6. Since you’ve applied the
function, you can also replace the fully applied function
with its body and bound arguments.
Capitalization matters! Function names start with lowercase
letters. Sometimes for clarity in function names, you may
want camelCase style, and that is good style provided the first
letter remains lowercase.
Variables must also begin with lowercase letters. They need
not be single letters.</p>
<p>CHAPTER 2. BASIC EXPRESSIONS AND FUNCTIONS 47
Playing with the triple function First, try entering the triple
function directly into the REPL using let. Now call the func-
tion by name and introduce a numeric value for the 𝑥argu-
ment:
Prelude&gt; triple 2
6
Next, enter the second version (the one without let) into a
source file and save the file. Load it into GHCi, using the :load
or:lcommand. Once it’s loaded, you can call the function
at the prompt using the function name, triple, followed by
a numeric value, just as you did in the REPL example above.
Try using diﬀerent values for 𝑥— integer values or other arith-
metic expressions. Then try changing the function itself in
the source file and reloading it to see what changes. You can
use:reload , or:r, to reload the same file.
2.5 Evaluation
When we talk about evaluating an expression, we’re talking
about reducing the terms until the expression reaches its sim-
plest form. Once a term has reached its simplest form, we say
that it is irreducible or finished evaluating. Usually, we call this
a value. Haskell uses a nonstrict evaluation (sometimes called
“lazy evaluation”) strategy which defers evaluation of terms
until they’re forced by other terms referring to them.</p>
<p>CHAPTER 2. BASIC EXPRESSIONS AND FUNCTIONS 48
Values are irreducible, but applications of functions to ar-
guments are reducible. Reducing an expression means evalu-
ating the terms until you’re left with a value. As in the lambda
calculus, application is evaluation: applying a function to an
argument allows evaluation or reduction.
Values are expressions, but cannot be reduced further. Val-
ues are a terminal point of reduction:
1
&quot;Icarus&quot;
The following expressions can be reduced to a value:
1+1
2*3+1
Each can be evaluated in the REPL, which reduces the ex-
pressions and then prints what it reduced to.
Let’s get back to our triple function. Calling the function
by name and applying it to an argument makes it a reducible
expression. In a pure functional language like Haskell, we can
replace applications of functions with their definitions and get
the same result, like in math. As a result when we see:
triple2
We can know that, since triple is defined as x = x * 3 , the
expression is equivalent to:</p>
<p>CHAPTER 2. BASIC EXPRESSIONS AND FUNCTIONS 49
triple2
-- [triple x = x * 3; x:= 2]
2*3
6
We’ve applied triple to the value 2 and then reduced the
expression to the final result 6. Our expression triple 2 is in
canonical or normalform when it reaches the number 6 because
the value 6 has no remaining reducible expressions.
Haskell doesn’t evaluate everything to canonical or normal
form by default. Instead, it only evaluates to weak head normal
form (WHNF) by default. What this means is that not every-
thing will get reduced to its irreducible form immediately, so
this:
(\f-&gt;(1,2+f))2
reduces to the following in WHNF:
(1,2+2)
This representation is an approximation, but the key point
here is that 2 + 2is not evaluated to 4 until the last possible
moment.</p>
<p>CHAPTER 2. BASIC EXPRESSIONS AND FUNCTIONS 50
Exercises: Comprehension Check
1.Given the following lines of code as they might appear
in a source file, how would you change them to use them
directly in the REPL?
halfx=x/2
squarex=x<em>x
2.Write one function that has one parameter and works
for all the following expressions. Be sure to name the
function.
3.14</em>(5<em>5)
3.14</em>(10<em>10)
3.14</em>(2<em>2)
3.14</em>(4*4)
3.There is a value in Prelude calledpi. Rewrite your function
to usepiinstead of 3.14.
2.6 Infix operators
Functions in Haskell default to prefix syntax, meaning that the
function being applied is at the beginning of the expression
rather than the middle. We saw that with our triple function,</p>
<p>CHAPTER 2. BASIC EXPRESSIONS AND FUNCTIONS 51
and we see it with standard functions such as the identity, or
id, function. This function returns whatever value it is given
as an argument:
Prelude&gt; id 1
1
While this is the default syntax for functions, not all func-
tions are prefix. There are a group of operators, such as the
arithmetic operators we’ve been using, that are indeed func-
tions (they apply to arguments to produce an output) but ap-
pear by default in an infix position.
Operators are functions which can be used in infix style.
All operators are functions; not all functions are operators.
Whiletriple andidare prefix functions ( notoperators), the +
function is an infix operator:
Prelude&gt; 1 + 1
2
Now we’ll try a few other arithmetic operators:
Prelude&gt; 100 + 100
200
Prelude&gt; 768395 * 21356345
16410108716275
Prelude&gt; 123123 / 123</p>
<p>CHAPTER 2. BASIC EXPRESSIONS AND FUNCTIONS 52
1001.0
Prelude&gt; 476 - 36
440
Prelude&gt; 10 / 4
2.5
You can sometimes use functions infix style, with a small
change in syntax:
Prelude&gt; 10 <code>div</code> 4
2
Prelude&gt; div 10 4
2
And you can use infix operators in prefix fashion by wrap-
ping them in parentheses:
Prelude&gt; (+) 100 100
200
Prelude&gt; (*) 768395 21356345
16410108716275
Prelude&gt; (/) 123123 123
1001.0
If the function name is alphanumeric, it is a prefix function
by default, and not all prefix functions can be made infix. If</p>
<p>CHAPTER 2. BASIC EXPRESSIONS AND FUNCTIONS 53
the name is a symbol, it is infix by default but can be made
prefix by wrapping it in parentheses.5
Associativity and precedence
As you may remember from your math classes, there’s a de-
fault associativity and precedence to the infix operators (<em>),
(+),(-), and(/).
We can ask GHCi for information such as associativity and
precedence of operators and functions by using the :infocom-
mand. When you ask GHCi for the :infoabout an operator
or function, it provides the type information. It also tells you
whether it’s an infix operator, and, if it is, its associativity and
precedence. Let’s talk about that associativity and precedence
briefly. We will elide the type information and so forth for
now.
Here’s what the code in Prelude says for (</em>),(+), and(-)at
time of writing:
5For people who like nitpicky details: you cannot make a prefix function into an infix
function using backticks, then wrap that in parentheses and make it into a prefix function.
We’re not clear why you’d want to do that anyway. Cut it out.</p>
<p>CHAPTER 2. BASIC EXPRESSIONS AND FUNCTIONS 54
:info (<em>)
infixl 7</em>
-- [1] [2] [3]
:info (+) (-)
infixl6+
infixl6-
1.infixl means it’s an infix operator; the lmeans it’s left
associative.
2.7 is the precedence: higher is applied first, on a scale of
0-9.
3.Infix function name: in this case, multiplication.
The information about addition and subtraction tell us they
are both left-associative, infix operators with the same prece-
dence (6).
Let’s play with parentheses and see what it means that these
associate to the left. Continue to follow along with the code
via the REPL:
This:
2<em>3</em>4
is evaluated as if it were:</p>
<p>CHAPTER 2. BASIC EXPRESSIONS AND FUNCTIONS 55
(2*3)*4
because of left associativity.
Here’s an example of a right-associative infix operator:
Prelude&gt; :info (^)
infixr 8 ^
-- [1] [2] [3]
1.infixr means infix operator; the rmeans it’s rightassocia-
tive.
2.8 is the precedence. Higher precedence, indicated by
higher numbers, is applied first, so this is higher prece-
dence than multiplication (7), addition, or subtraction
(both 6).
3.Infix function name: in this case, exponentiation.
It was hard to tell with multiplication why associativity mat-
tered, because multiplication is commutative. So shifting the
parentheses around never changes the result. Exponentiation,
however, is not associative and thus makes a prime candidate
for demonstrating left vs. right associativity.
Prelude&gt; 2 ^ 3 ^ 4
2417851639229258349412352
Prelude&gt; 2 ^ (3 ^ 4)</p>
<p>CHAPTER 2. BASIC EXPRESSIONS AND FUNCTIONS 56
2417851639229258349412352
Prelude&gt; (2 ^ 3) ^ 4
4096
As you can see, adding parentheses starting from the right-
handsideoftheexpressionwhentheoperatorisright-associative
doesn’t change anything. However, if we parenthesize from
theleft, we get a diﬀerent result when the expression is evalu-
ated.
Your intuitions about precedence, associativity, and paren-
thesization from math classes will generally hold in Haskell:
2+3*4
(2+3)*4
What’s the diﬀerence between these two? Why are they
diﬀerent?
Exercises: Parentheses and Association
Below are some pairs of functions that are alike except for
parenthesization. Read them carefully and decide if the paren-
theses change the results of the function. Check your work in
GHCi.
1.a)8 + 7 * 9</p>
<p>CHAPTER 2. BASIC EXPRESSIONS AND FUNCTIONS 57
b)(8 + 7) * 9
2.a)perimeter x y = (x * 2) + (y * 2)
b)perimeter x y = x * 2 + y * 2
3.a)f x = x / 2 + 9
b)f x = x / (2 + 9)
2.7 Declaring values
The order of declarations in a source code file doesn’t matter
because GHCi loads the entire file at once, so it knows all the
values that have been defined. On the other hand, when you
enter them one by one into the REPL, the order does matter.
For example, we can declare a series of expressions in the
REPL like this:
Prelude&gt; let y = 10
Prelude&gt; let x = 10 * 5 + y
Prelude&gt; let myResult = x * 5
As we saw above with the triple function, we have to use
letto declare something in the REPL.
We can now type the names of the values and hit enter to
see their values:
Prelude&gt; x</p>
<p>CHAPTER 2. BASIC EXPRESSIONS AND FUNCTIONS 58
60
Prelude&gt; y
10
Prelude&gt; myResult
300
Let’s see how to declare those values in a file called learn.hs .
First, we declare the name of our module so it can be imported
by name in a project (we won’t be doing a project of this size for
a while yet, but it’s good to get in the habit of having module
names):
-- learn.hs
moduleLearnwhere
x=10<em>5+y
myResult =x</em>5
y=10
Module names are capitalized. Also, in the variable name,
we’ve used camelCase: the first letter is still lowercase, but we
use an uppercase to delineate a word boundary for readability.</p>
<p>CHAPTER 2. BASIC EXPRESSIONS AND FUNCTIONS 59
Troubleshooting
It is easy to make mistakes in the process of typing learn.hs
into your editor. We’ll look at a few common mistakes in
this section. One thing to keep in mind is that indentation of
Haskell code is significant and can change the meaning of the
code. Incorrect indentation of code can also break your code.
Use spaces, nottabs, to indent your source code.
In general, whitespace is significant in Haskell. Efficient
use of whitespace makes the syntax more concise. This can
take some getting used to if you’ve been working in another
programming language. Whitespace is often the only mark of
a function call, unless parentheses are necessary due to con-
flicting precedence. Trailing whitespace, that is, extraneous
whitespace at the end of lines of code, is considered bad style.
In source code files, indentation often replaces syntactic
markers like curly brackets, semicolons, and parentheses. The
basic rule is that code that is part of an expression should be
indented under the beginning of that expression, even when
the beginning of the expression is not at the leftmost margin.
Furthermore, parts of the expression that are grouped should
be indented to the same level. For example, in a block of code
introduced by letordo, you might see something like this:</p>
<p>CHAPTER 2. BASIC EXPRESSIONS AND FUNCTIONS 60
let
x=3
y=4
-- or
letx=3
y=4
This wouldn’t work in a source file unless they were embed-
ded in a top-level declaration.
Noticethatthetwodefinitionsthatarepartoftheexpression
line up in either case. It is incorrect to write:
letx=3
y=4
-- or
let
x=3
y=4
If you have an expression that has multiple parts, your
indentation will follow a pattern like this:</p>
<p>CHAPTER 2. BASIC EXPRESSIONS AND FUNCTIONS 61
foox=
lety=x<em>2
z=x^2
in2</em>y*z
Notice that the definitions of 𝑦and𝑧line up, and the def-
initions of letandinare also aligned. As you work through
the book, pay attention to the indentation patterns as we have
them printed. There are many cases where improper inden-
tation will cause code not to work. Indentation can easily go
wrong in a copy-and-paste job as well.
If you make a mistake like breaking up the declaration of 𝑥
such that the rest of the expression began at the beginning of
the next line:
moduleLearnwhere
-- module declaration at the top
x=10
<em>5+y
myResult =x</em>5
y=10
You might see an error like:</p>
<p>CHAPTER 2. BASIC EXPRESSIONS AND FUNCTIONS 62
Prelude&gt; :l code/learn.hs
[1 of 1] Compiling Learn
code/learn.hs:10:1:
parse error on input ‘<em>’
Failed, modules loaded: none.
Note that the first line of the error message tells you where
the error occurred: code/learn.hs:10:1 indicates that the mis-
take is in line 10, column 1, of the named file. That can make it
easier to find the problem that needs to be fixed. Please note
that the exact line and column numbers in your own error
messages might be diﬀerent from ours, depending on how
you’ve entered the code into the file.
The way to fix this is to either put it all on one line, like this:
x=10</em>5+y
or to make certain when you break up lines of code that the
second line begins at least one space from the beginning of
that line (either of the following should work):</p>
<p>CHAPTER 2. BASIC EXPRESSIONS AND FUNCTIONS 63
x=10
<em>5+y
-- or
x=10
<em>5+y
The second one looks a little better. Generally, you should
reserve breaking up of lines for when you have code exceeding
100 columns in width.
Another possible error is not starting a declaration at the
beginning (left) column of the line:
-- learn.hs
moduleLearnwhere
x=10</em>5+y
myResult =x</em>5
y=10
See that space before 𝑥? That will cause an error like:
Prelude&gt; :l code/learn.hs</p>
<p>CHAPTER 2. BASIC EXPRESSIONS AND FUNCTIONS 64
[1 of 1] Compiling Learn
code/learn.hs:11:1:
parse error on input ‘myResult’
Failed, modules loaded: none.
This may confuse you, as myResult is not where you need to
modify your code. The error is only an extraneous space, but
all declarations in the module must start at the same column.
The column that all declarations within a module must start
in is determined by the first declaration in the module. In this
case, the error message gives a location that is diﬀerent from
where you should fix the problem because all the compiler
knows is that the declaration of 𝑥made a single space the ap-
propriate indentation for all declarations within that module,
and the declaration of myResult began a column too early.
It is possible to fix this error by indenting the myResult and
𝑦declarations to the same level as the indented 𝑥declaration:</p>
<p>CHAPTER 2. BASIC EXPRESSIONS AND FUNCTIONS 65
-- learn.hs
moduleLearnwhere
x=10<em>5+y
myResult =x</em>5
y=10
However, this is considered bad style and is not standard
Haskell practice. There is almost never a good reason to indent
all your declarations in this way, but noting this gives us some
idea of how the compiler is reading the code. It is better, when
confronted with an error message like this, to make sure that
your first declaration is at the leftmost margin and proceed
from there.
Another possible mistake is that you might’ve missed the
second -in the--used to comment out source lines of code.
So this code:</p>
<p>CHAPTER 2. BASIC EXPRESSIONS AND FUNCTIONS 66
-learn.hs
moduleLearnwhere
x=10<em>5+y
myResult =x</em>5
y=10
will cause this error:
code/learn.hs:7:1:
parse error on input ‘module’
Failed, modules loaded: none.
Note again that it says the parse error occurred at the be-
ginning of the module declaration, but the issue is that the
comment line, - learn.hs , had only one dash, when it needed
two to form a syntactically correct Haskell comment.
Now we can see how to work with code that is saved in a
source file from GHCi without manually copying and pasting
the definitions into our REPL. Assuming we open our REPL
in the same directory as we have learn.hs saved, we can do the
following:
Prelude&gt; :load learn.hs</p>
<p>CHAPTER 2. BASIC EXPRESSIONS AND FUNCTIONS 67
[1 of 1] Compiling Learn
Ok, modules loaded: Learn.
Prelude&gt; x
60
Prelude&gt; y
10
Prelude&gt; myResult
300
Exercises: Heal the Sick
The following code samples are broken and won’t compile.
The first two are as you might enter into the REPL; the third
is from a source file. Find the mistakes and fix them so that
they will.
1.let area x = 3. 14 * (x * x)
2.let double x = b * 2
3.x=7
y=10
f=x+y
2.8 Arithmetic functions in Haskell
This section will explore some basic arithmetic using some
common operators and functions for arithmetic. We’ll focus</p>
<p>CHAPTER 2. BASIC EXPRESSIONS AND FUNCTIONS 68
on the following subset of them:
Operator Name Purpose/application</p>
<ul>
<li>plus addition</li>
</ul>
<ul>
<li>minus subtraction</li>
</ul>
<ul>
<li>asterisk multiplication
/ slash fractional division
div divide integral division, round down
mod modulo like ‘rem’, but after modular division
quot quotient integral division, round towards zero
rem remainder remainder after division
At the risk of stating the obvious, “integral” division refers
to division of integers. Because it’s integral and not fractional,
it takes integers as arguments and returns integers as results.
That’s why the results are rounded.
Here’s an example of each in the REPL:
Prelude&gt; 1 + 1
2
Prelude&gt; 1 - 1
0
Prelude&gt; 1 * 1
1
Prelude&gt; 1 / 1
1.0</li>
</ul>
<p>CHAPTER 2. BASIC EXPRESSIONS AND FUNCTIONS 69
Prelude&gt; div 1 1
1
Prelude&gt; mod 1 1
0
Prelude&gt; quot 1 1
1
Prelude&gt; rem 1 1
0
You will usually want divfor integral division, due to the
waydivandquotround:
-- rounds down
Prelude&gt; div 20 (-6)
-4
-- rounds toward zero
Prelude&gt; quot 20 (-6)
-3
Also,remandmodhave slightly diﬀerent use cases; we’ll look
atmodin a little more detail in this chapter. We will cover (/)
in more detail in a later chapter, as that will require some
explanation of types and typeclasses.</p>
<p>CHAPTER 2. BASIC EXPRESSIONS AND FUNCTIONS 70
Laws for quotients and remainders
Programming often makes use of more division and remain-
der functions than standard arithmetic does, and it’s helpful
to be familiar with the laws about quotandrem, anddivand
mod.6We’ll take a look at those here.
(quot x y)*y + (rem x y) == x
(div x y)<em>y + (mod x y) == x
We won’t walk through a proof exercise, but we can demon-
strate these laws a bit:
(quot x y)<em>y + (rem x y)
Given x is 10 and y is (-4)
(quot 10 (-4))</em>(-4) + (rem 10 (-4))
quot 10 (-4) == (-2) and rem 10 (-4) == 2
(-2)</em>(-4) + (2) == 10
10 == x
6From Lennart Augustsson’s blog http://augustss.blogspot.com or Stack Overflow an-
swer at http://stackoverflow.com/a/8111203</p>
<p>CHAPTER 2. BASIC EXPRESSIONS AND FUNCTIONS 71
Yes, we got to the result we wanted.
Now for divandmod:
(div x y)<em>y + (mod x y)
Given x is 10 and y is (-4)
(div 10 (-4))</em>(-4) + (mod 10 (-4))
div 10 (-4) == (-3) and mod 10 (-4) == -2
(-3)*(-4) + (-2) == 10
10 == x
Our result indicates all is well in the world of integral divi-
sion.
Usingmod
This section is not a full discussion of modular arithmetic, but
we want to give more direction in how to use modin general,
for those who may be unfamiliar with it, and how it works in
Haskell specifically.
We’ve already mentioned that modgives the remainder of
a modular division. If you’re not already familiar with mod-</p>
<p>CHAPTER 2. BASIC EXPRESSIONS AND FUNCTIONS 72
ular division, you may not understand the useful diﬀerence
between modandrem.
Modular arithmetic is a system of arithmetic for integers
where numbers “wrap around” upon reaching a certain value,
called the modulus . It is often explained in terms of a clock.
When we count time by a 12-hour clock, we have to wrap
the counting around the 12. For example, if the time is now
8:00 and you want to know what time it will be 8 hours from
now, you don’t simply add 8 + 8 and get a result of 16 o’clock.7
Instead, you wrap the count around every 12 hours. So,
adding 8 hours to 8:00 means that we add 4 hours to get to
the 12, and at the 12 we start over again as if it’s 0 and add the
remaining 4 hours of our 8, for an answer of 4:00. That is, 8
hours after 8:00 is 4:00.
This is arithmetic modulo 12. In our 12-hour clock, 12 is
equivalent to both itself and to 0, so the time at 12:00 is also,
in some sense 0:00. Arithmetic modulo 12 means that 12 is
both 12 and 0.
Often, this will give you the same answer that remdoes:
Prelude&gt; mod 15 12
3
Prelude&gt; rem 15 12
3
7Obviously, with a 24-hour clock, such a time is possible; however, if we were starting
from 8:00 p.m. and trying to find the time 8 hours later, the answer would not be 16:00
a.m. A 24-hour clock has a diﬀerent modulus than a 12-hour clock.</p>
<p>CHAPTER 2. BASIC EXPRESSIONS AND FUNCTIONS 73
Prelude&gt; mod 21 12
9
Prelude&gt; rem 21 12
9
Prelude&gt; mod 3 12
3
Prelude&gt; rem 3 12
3
If you’re wondering what the deal is with the last two ex-
amples, it’s because modandremcan only represent integral
division. If all you have to work with is integers, then dividing
a smaller number by a larger number results in an answer of
0 with a remainder of whatever the smaller number (the divi-
dend) is. If you want to divide a smaller number by a larger
number and return a fractional answer, then you need to use
(/), and you won’t have a remainder.
Let’s say we need to write a function that will determine
what day of the week it was or will be a certain number of
days before or after this one. For our purposes here, we will
assign a number to each day of the week, using 0 to represent
Sunday.8Then if today is Monday, and we want to know what
8Sure, you may naturally think of the days of week as being numbered 1-7. But
programmers like to index things from zero.</p>
<p>CHAPTER 2. BASIC EXPRESSIONS AND FUNCTIONS 74
day of the week it will be 23 days from now, we could do this:
Prelude&gt; mod (1 + 23) 7
3
The 1 represents Monday, the current day, while 23 is the
number of days we’re trying to add. Using modto wrap it around
the 7 means it will return a number that corresponds to a day
of the week in our numbering.
And 5 days from Saturday will be Thursday:
Prelude&gt; mod (6 + 5) 7
4
We can use remto do the same thing with apparently equiv-
alent accuracy:
Prelude&gt; rem (1 + 23) 7
3
However, if we want to subtract and find out what day of
the week it was some number of days ago, then we’ll see a
diﬀerence. Let’s try asking, if today is Wednesday (3), what
day it was 12 days ago:
Prelude&gt; mod (3 - 12) 7
5
Prelude&gt; rem (3 - 12) 7
-2</p>
<p>CHAPTER 2. BASIC EXPRESSIONS AND FUNCTIONS 75
The version with modgives us a correct answer, while the rem
version does not.
One key diﬀerence here is that, in Haskell (not in all lan-
guages), if one or both arguments are negative, the results of
modwill have the same sign as the divisor, while the result of
remwill have the same sign as the dividend:
Prelude&gt; (-5) <code>mod</code> 2
1
Prelude&gt; 5 <code>mod</code> (-2)
-1
Prelude&gt; (-5) <code>mod</code> (-2)
-1
But:
Prelude&gt; (-5) <code>rem</code> 2
-1
Prelude&gt; 5 <code>rem</code> (-2)
1
Prelude&gt; (-5) <code>rem</code> (-2)
-1
Figuring out when you need modtakes some experience.</p>
<p>CHAPTER 2. BASIC EXPRESSIONS AND FUNCTIONS 76
Negative numbers
Due to the interaction of parentheses, currying, and infix syn-
tax, negative numbers get special treatment in Haskell.
If you want a value that is a negative number by itself, this
will work fine:
Prelude&gt; -1000
-1000
However, this will not work in some cases:
Prelude&gt; 1000 + -9
<interactive>:3:1:
Precedence parsing error
cannot mix ‘+’ [infixl 6] and
prefix <code>-</code> [infixl 6]
in the same infix expression
Fortunately, we were told about our mistake before any
of our code was executed. Note how the error message tells
you the problem has to do with precedence. Addition and
subtraction have the same precedence (6), and GHCi thinks
we are trying to add and subtract, not add a negative number,
so it doesn’t know how to resolve the precedence and evaluate
the expression. We need to make a small change before we
can add a positive and a negative number together:</p>
<p>CHAPTER 2. BASIC EXPRESSIONS AND FUNCTIONS 77
Prelude&gt; 1000 + (-9)
991
The negation of numbers in Haskell by the use of a unary -
is a form of syntactic sugar . Syntax is the grammar and struc-
ture of the text we use to express programs, and syntactic
sugar is a means for us to make that text easier to read and
write. Syntactic sugar can make the typing or reading of code
nicer but changes nothing about the semantics, or meaning, of
programs and doesn’t change how we solve problems in code.
Typically when code with syntactic sugar is processed by our
REPL or compiler, a simple transformation from the shorter
(“sweeter”) form to a more verbose, truer representation is
performed after the code has been parsed.
In the specific case of -, the syntactic sugar means the oper-
ator now has two possible interpretations. The two possible
interpretations of the syntactic -are that -is being used as an
alias for negate or that it is the subtraction function. The fol-
lowing are semantically identical (that is, they have the same
meaning, despite diﬀerent syntax) because the -is translated
intonegate :
Prelude&gt; 2000 + (-1234)
766
Prelude&gt; 2000 + (negate 1234)
766</p>
<p>CHAPTER 2. BASIC EXPRESSIONS AND FUNCTIONS 78
Whereas this is -being used for subtraction:
Prelude&gt; 2000 - 1234
766
Fortunately, syntactic overloading like this isn’t common
in Haskell.
2.9 Parenthesization
Here we’ve listed the information that GHCi gives us for var-
ious infix operators. We have left the type signatures in, al-
though it is not directly relevant at this time. This will give
you a chance to look at the types if you’re curious and also
provide a more accurate picture of the :infocommand.
Prelude&gt; :info (^)
(^) :: (Num a, Integral b) =&gt; a -&gt; b -&gt; a
infixr 8 ^
Prelude&gt; :info (<em>)
class Num a where
(</em>) :: a -&gt; a -&gt; a
infixl 7 *
Prelude&gt; :info (+)
class Num a where</p>
<p>CHAPTER 2. BASIC EXPRESSIONS AND FUNCTIONS 79
(+) :: a -&gt; a -&gt; a
infixl 6 +
Prelude&gt; :info (-)
class Num a where
(-) :: a -&gt; a -&gt; a
infixl 6 -
Prelude&gt; :info ($)
($) :: (a -&gt; b) -&gt; a -&gt; b
infixr 0 $
We should take a moment to explain and demonstrate the
($)operator as you will run into it fairly frequently in Haskell
code. The good news is it does almost nothing. The bad news
is this fact sometimes trips people up.
First, here’s the definition of ($):
f$a=f a
Immediately this seems a bit pointless until we remember
that it’s defined as an infix operator with the lowest possible
precedence. The ($)operator is a convenience for when you
want to express something with fewer pairs of parentheses:
Prelude&gt; (2^) (2 + 2)
16</p>
<p>CHAPTER 2. BASIC EXPRESSIONS AND FUNCTIONS 80
-- can replace those parentheses
Prelude&gt; (2^) $ 2 + 2
16
-- without either parentheses or $
Prelude&gt; (2^) 2 + 2
6
The($)will allow everything to the right of it to be evalu-
ated first and can be used to delay function application. You’ll
see what we mean about delaying function application in par-
ticular when we get to Chapter 7 and use it with function
composition.
Also note that you can stack up multiple uses of ($)in the
same expression. For example, this works:
Prelude&gt; (2^) $ (+2) $ 3*2
256
But this does not:
Prelude&gt; (2^) $ 2 + 2 $ (*30)
A rather long and ugly type error about trying to use num-
bers as if they were functions follows. We can see why this
code doesn’t make sense if we examine the reduction steps:</p>
<p>CHAPTER 2. BASIC EXPRESSIONS AND FUNCTIONS 81
-- Remember ($)'s definition
f$a=f a
(2^)$2+2$(*30)
Given the right-associativity ( infixr ) of$we must begin at
the right-most position:
2+2$(*30)
-- reduce ($)
(2+2) (*30)
Then we must evaluate (2 + 2) before we can apply it:
4(*30)
You might think that this could evaluate as (4 * 30), but it’s
trying to apply 4 as if it was a function to the argument (*30)!
Writing expressions like (*30)is called sectioning .
Now let’s flip that expression around a bit so it works and
then walk through a reduction:</p>
<p>CHAPTER 2. BASIC EXPRESSIONS AND FUNCTIONS 82
(2^)$(*30)$2+2
-- must evaluate right-side first
(2^)$(*30)$2+2
-- application of the function (*30) to the
-- expression (2 + 2) forces evaluation
(2^)$(*30)4
-- then we reduce (*30) 4
(2^)$120
-- reduce ($) again.
(2^)120
-- reduce (2^)
1329227995784915872903807060280344576
Some Haskellers find parentheses more readable than the
dollar sign, but it’s too common in idiomatic Haskell code for
you to not at least be familiar with it.
Parenthesizing infix operators
There are times when you want to refer to an infix function
without applying any arguments, and there are also times
when you want to use them as prefix operators instead of infix.
In both cases you must wrap the operator in parentheses. Let’s
look at how we use infix operators as prefixes.
If your infix function is &gt;&gt;then you must write (&gt;&gt;)to refer
to it as a value. (+)is the addition infix function without any</p>
<p>CHAPTER 2. BASIC EXPRESSIONS AND FUNCTIONS 83
arguments applied yet and (+1)is the same addition function
but with one argument applied, making it return the next
argument it’s applied to plus one:
Prelude&gt; 1 + 2
3
Prelude&gt; (+) 1 2
3
Prelude&gt; (+1) 2
3
The last case is known as sectioning and allows you to pass
around partially applied functions. With commutative func-
tions, such as addition, it makes no diﬀerence if you use (+1)
or(1+)because the order of the arguments won’t change the
result.
If you use sectioning with a function that is not commuta-
tive, the order matters:
Prelude&gt; (1/) 2
0.5
Prelude&gt; (/1) 2
2.0
Subtraction, (-), is a special case. These will work:
Prelude&gt; 2 - 1</p>
<p>CHAPTER 2. BASIC EXPRESSIONS AND FUNCTIONS 84
1
Prelude&gt; (-) 2 1
1
The following, however, won’t work:
Prelude&gt; (-2) 1
Enclosing a value inside the parentheses with the -indi-
cates to GHCi that it’s the argument of a function. Because
the-function represents negation, not subtraction, when it’s
applied to a single argument, GHCi does not know what to do
with that, and so it returns an error message. Here, -is a case
of syntactic overloading disambiguated by how it is used.
You can use sectioning for subtraction, but it must be the
first argument:
Prelude&gt; let x = 5
Prelude&gt; let y = (1 -)
Prelude&gt; y x
-4
Or instead of (- x), you can write (subtract x):
Prelude&gt; (subtract 2) 3
1</p>
<p>CHAPTER 2. BASIC EXPRESSIONS AND FUNCTIONS 85
It may not be immediately obvious why you would ever
want to do this, but you will see this syntax used throughout the
book, for example, once we start wanting to apply functions
to each value inside a list or other data structure.
2.10 Let and where
You will often see letandwhereused to introduce components
of expressions, and they seem similar. It takes some practice
to get used to the appropriate times to use each.
The contrast here is that letintroduces an expression , so it
can be used wherever you can have an expression, but whereis
adeclaration and is bound to a surrounding syntactic construct.
We’ll start with an example of where:
-- FunctionWithWhere.hs
moduleFunctionWithWhere where
printInc n=print plusTwo
whereplusTwo =n+2
And if we use this in the REPL:
Prelude&gt; :l FunctionWithWhere.hs
[1 of 1] Compiling FunctionWithWhere ...</p>
<p>CHAPTER 2. BASIC EXPRESSIONS AND FUNCTIONS 86
Ok, modules loaded: FunctionWithWhere.
Prelude&gt; printInc 1
3
Prelude&gt;
Now we have the same function, but using letin the place
ofwhere:
-- FunctionWithLet.hs
moduleFunctionWithLet where
printInc2 n= letplusTwo =n+2
inprint plusTwo
When you see letfollowed by in, you’re looking at a let
expression . Here’s that function in the REPL:
Prelude&gt; :load FunctionWithLet.hs
[1 of 1] Compiling FunctionWithLet ...
Ok, modules loaded: FunctionWithLet.
Prelude&gt; printInc2 3
5
If you loaded FunctionWithLet in the same REPL session as
FunctionWithWhere , then it will have unloaded the first one be-
fore loading the new one:</p>
<p>CHAPTER 2. BASIC EXPRESSIONS AND FUNCTIONS 87
Prelude&gt; :load FunctionWithWhere.hs
[1 of 1] Compiling FunctionWithWhere ...
Ok, modules loaded: FunctionWithWhere.
Prelude&gt; printInc 1
3
Prelude&gt; :load FunctionWithLet.hs
[1 of 1] Compiling FunctionWithLet ...
Ok, modules loaded: FunctionWithLet.
Prelude&gt; printInc2 10
12
Prelude&gt; printInc 10
<interactive>:6:1:
Not in scope: ‘printInc’
Perhaps you meant ‘printInc2’ (line 4)
printInc isn’t in scope anymore because GHCi unloaded
everything you’d defined or loaded after you used :loadto load
theFunctionWithLet.hs source file. Scopeis the area of source
code where a binding of a variable applies.
That is one limitation of the :loadcommand in GHCi. As
we build larger projects that require having multiple modules
in scope, we will use a project manager called Stack rather
than GHCi itself.</p>
<p>CHAPTER 2. BASIC EXPRESSIONS AND FUNCTIONS 88
Exercises: A Head Code
Now for some exercises. First, determine in your head what
the following expressions will return, then validate in the
REPL:
1.letx=5inx
2.letx=5inx<em>x
3.letx=5; y=6inx</em>y
4.letx=3; y=1000inx+3
Above, you entered some letexpressions into your REPL
to evaluate them. Now, we’re going to open a file and rewrite
someletexpressions using wheredeclarations. You will have
to give the value you’re binding a name, although the name
can be a single letter if you like. For example,
-- this should work in GHCi
letx=5; y=6inx<em>y
could be rewritten as
-- put this in a file
mult1 =x</em>y
wherex=5
y=6</p>
<p>CHAPTER 2. BASIC EXPRESSIONS AND FUNCTIONS 89
Making the equals signs line up is a stylistic choice. As long
as things are nested in that way, the equals signs do not have
to line up. But notice we use a name that we will use to refer
to this value in the REPL:
Prelude&gt; :l practice.hs
[1 of 1] Compiling Main
Ok, modules loaded: Main.
Prelude&gt; mult1
30
The prompt changes to <em>Maininstead of Prelude to indicate
that you have a module called Mainloaded.
Rewrite with whereclauses:
1.letx=3; y=1000inx</em>3+y
2.lety=10; x=10<em>5+yinx</em>5
3.letx=7
y=negate x
z=y*10
inz/x+y
Note: the filename you choose is unimportant except for
the.hsextension.</p>
<p>CHAPTER 2. BASIC EXPRESSIONS AND FUNCTIONS 90
2.11 Chapter Exercises
The goal for all the following exercises is to get you playing
with code and forming hypotheses about what it should do.
Read the code carefully, using what we’ve learned so far. Gen-
erate a hypothesis about what you think the code will do. Play
with it in the REPL and find out where you were right or wrong.
Parenthesization
Given what we know about the precedence of (<em>),(+), and(^),
how can we parenthesize the following expressions more ex-
plicitly without changing their results? Put together an answer
you think is correct, then test in the GHCi REPL.
For example, we want to make this more explicit
2+2</em>3-3
This will produce the same result:
2+(2<em>3)-3
Attempt the above on the following expressions:
1.2+2</em>3-1
2.(^)10$1+1
3.2^2*4^5+1</p>
<p>CHAPTER 2. BASIC EXPRESSIONS AND FUNCTIONS 91
Equivalent expressions
Which of the following pairs of expressions will return the
same result when evaluated? Try to reason them out by read-
ing the code and then enter them into the REPL to check your
work:
1.1+1
2
2.10^2
10+9<em>10
3.400-37
(-)37400
4.100<code>div</code>3
100/3
5.2</em>5+18
2*(5+18)</p>
<p>CHAPTER 2. BASIC EXPRESSIONS AND FUNCTIONS 92
More fun with functions
Here is a bit of code as it might be entered into a source file.
Remember that when you write code in a source file, the order
is unimportant, but when writing code directly into the REPL
the order does matter. Given that, look at this code and rewrite
it such that it could be evaluated in the REPL (remember: you
may need letwhen entering it directly into the REPL). Be sure
to enter your code into the REPL to make sure it evaluates
correctly.
z=7
x=y^2
waxOn=x*5
y=z+8
1.Now you have a value called waxOnin your REPL. What do
you think will happen if you enter:</p>
<p>CHAPTER 2. BASIC EXPRESSIONS AND FUNCTIONS 93
10+waxOn
-- or
(+10) waxOn
-- or
(-)15waxOn
-- or
(-) waxOn 15
2.Earlier we looked at a function called triple . While your
REPL has waxOnin session, re-enter the triple function at
the prompt:
lettriple x =x*3
3.Now, whatwillhappenifweenterthisatourGHCiprompt?
What do you think will happen first, considering what role
waxOnis playing in this function call? Then enter it, see
what does happen, and check your understanding:
triplewaxOn
4.Rewrite waxOnas an expression with a whereclause in your
source file. Load it into your REPL and make sure it still
works as expected.
5.To the same source file where you have waxOn, add the
triple function. Remember: You don’t need letand the</p>
<p>CHAPTER 2. BASIC EXPRESSIONS AND FUNCTIONS 94
function name should be at the left margin (that is, not
nested as one of the waxOnexpressions). Make sure it works
by loading it into your REPL and then entering triple
waxOnagain at the REPL prompt. You should have the
same answer as you did above.
6.Now, without changing what you’ve done so far in that
file, add a new function called waxOff that looks like this:
waxOffx=triple x
7.Load the source file into your REPL and enter waxOff waxOn
at the prompt.
You now have a function, waxOff that can be applied to a
variety of arguments — not just waxOnbut any (numeric)
value you want to put in for 𝑥. Play with that a bit. What
is the result of waxOff 10 orwaxOff (-50) ? Try modifying
yourwaxOff function to do something new — perhaps
you want to first triple the 𝑥value and then square it or
divide it by 10. Spend some time getting comfortable with
modifying the source file code, reloading it, and checking
your modification in the REPL.
2.12 Definitions
1.The terms argument andparameter are often used inter-
changeably. However, it is worthwhile to understand the</p>
<p>CHAPTER 2. BASIC EXPRESSIONS AND FUNCTIONS 95
distinction. A parameter , or formal parameter, represents a
value that will be passed to the function when the func-
tion is called. Thus, parameters are usually variables. An
argument is an input value the function is applied to. A
function’s parameter is bound to the value of an argument
when the function is applied to that argument. For exam-
ple, inf x = x + 2 which takes an argument and returns
that value added to 2, 𝑥is the one parameter of our func-
tion. We run the code by applying 𝑓to some argument.
If the argument we passed to the parameter 𝑥were 2, our
result would be 4. However, arguments can themselves
be variables or be expressions that include variables, thus
the distinction is not always clear. When we use “param-
eter” in this book, it will always be referring to formal
parameters, usually in a type signature, but we’ve taken
the liberty of using “argument” somewhat more loosely.
2.Anexpression is a combination of symbols that conforms
to syntactic rules and can be evaluated to some result. In
Haskell, an expression is a well-structured combination
of constants, variables, and functions. While irreducible
constants are technically expressions, we usually refer to
those as “values”, so we usually mean “reducible expres-
sion” when we use the term expression .
3.Avalueis an expression that cannot be reduced or evalu-
ated any further. 2 * 2is an expression, but not a value,</p>
<p>CHAPTER 2. BASIC EXPRESSIONS AND FUNCTIONS 96
whereas what it evaluates to, 4, is a value.
4.Afunction is a mathematical object whose capabilities are
limited to being applied to an argument and returning a
result. Functions can be described as a list of ordered pairs
of their inputs and the resulting outputs, like a mapping.
Given the function f x = x + 2 applied to the argument
2, we would have the ordered pair (2, 4) of its input and
output.
5.Infixnotation is the style used in arithmetic and logic. Infix
means that the operator is placed between the operands
orarguments . An example would be the plus sign in an
expression like 2 + 2.
6.Operators are functions that are infix by default. In Haskell,
operators must use symbols and not alphanumeric char-
acters.
7.Syntactic sugar is syntax within a programming language
designed to make expressions easier to write or read.
2.13 Follow-up resources
1.Haskell wiki article on Let vs. Where
https://wiki.haskell.org/Let_vs._Where
2.How to desugar Haskell code; Gabriel Gonzalez</p>
<p>Chapter 3
Strings
Like punning,
programming is a play on
words
Alan Perlis
97</p>
<p>CHAPTER 3. SIMPLE OPERATIONS WITH TEXT 98
3.1 Printing strings
So far we’ve been doing arithmetic using simple expressions.
In this chapter, we will turn our attention to a diﬀerent type
of data called String .
Most programming languages refer to the data structures
used to contain text as “strings,” usually represented as se-
quences, or lists, of characters. In this section, we will
•take an introductory look at types to understand the data
structure called String ;
•talk about the special syntax, or syntactic sugar, used for
strings;
•print strings in the REPL environment;
•work with some standard functions that operate on this
datatype.
3.2 A first look at types
First, since we will be working with strings, we want to start by
understanding what these data structures are in Haskell as well
as a bit of special syntax we use for them. We haven’t talked
much about types yet, although you saw some examples of
them in the last chapter. Types are important in Haskell, and
the next two chapters are entirely devoted to them.</p>
<p>CHAPTER 3. SIMPLE OPERATIONS WITH TEXT 99
Types are a way of categorizing values. There are sev-
eral types for numbers, for example, depending on whether
they are integers, fractional numbers, etc. There is a type
for boolean values, specifically the values TrueandFalse. The
types we are primarily concerned with in this chapter are Char
‘character’ and String .String s are lists of characters.
It is easy to find out the type of a value, expression, or
function in GHCi. We do this with the :typecommand.
Open up your REPL, enter :type 'a' at the prompt, and
you should see something like this:
Prelude&gt; :type 'a'
'a' :: Char
We’ll highlight a few things here. First, we’ve enclosed
our character in single quotes. This lets GHCi know that the
character is not a variable. If you enter :type a instead, it will
think it’s a variable and give you an error message that the 𝑎is
not in scope. That is, the variable 𝑎hasn’t been defined (is not
in scope), so it has no way to know what the type of it is.
Second, the ::symbol is read as “has the type.” You’ll see
this often in Haskell. Whenever you see that double colon,
you know you’re looking at a type signature. A type signature
is a line of code that defines the types for a value, expression,
or function.
And, finally, there is Char, the type. Charis the type that
includes alphabetic characters, Unicode characters, symbols,</p>
<p>CHAPTER 3. SIMPLE OPERATIONS WITH TEXT 100
etc. So, asking GHCi :type 'a' , that is, “what is the type of ‘a’?”,
gives us the information, 'a' :: Char , that is, “‘a’ has the type
of Char.”
Now, let’s try a string of text. This time we have to use
double quotation marks, not single, to tell GHCi we have a
string, not a single character:
Prelude&gt; :type &quot;Hello!&quot;
&quot;Hello!&quot; :: [Char]
We have something new in the type information. The
square brackets around Charhere are the syntactic sugar for a
list.String is atype alias , or type synonym, for a list of Char. A
type alias is what it sounds like: we use one name for a type,
usually for convenience, that has a diﬀerent type name under-
neath. Here String is another name for a list of characters. By
using the name String we are able to visually diﬀerentiate it
from other types of lists. When we talk about lists in more
detail later, we’ll see why the square brackets are considered
syntactic sugar; for now, we only need to understand that
GHCi says “Hello!” has the type list of Char.
3.3 Printing simple strings
Now, let’s see how to print strings of text in the REPL:
Prelude&gt; print &quot;hello world!&quot;</p>
<p>CHAPTER 3. SIMPLE OPERATIONS WITH TEXT 101
&quot;hello world!&quot;
We used printto tell GHCi to print the string to the display,
so it does, with the quotation marks still around it. The print
function is not specific to strings of text, though; it can be used
to print diﬀerent types of data to the screen.
The following also tell GHCi to print to the screen but are
specific to String :
Prelude&gt; putStrLn &quot;hello world!&quot;
hello world!
Prelude&gt;
Prelude&gt; putStr &quot;hello world!&quot;
hello world!Prelude&gt;
You can probably see that putStr andputStrLn are similar,
with one key diﬀerence. We also notice that both of these print
the string to the display without the quotation marks. This is
because, while they are superficially similar to print, they have
a diﬀerent type than printdoes. Functions that are similar on
the surface can behave diﬀerently depending on the type or
category they belong to.
Next, let’s take a look at how to do these things from source
files. Type the following into a file named print1.hs :</p>
<p>CHAPTER 3. SIMPLE OPERATIONS WITH TEXT 102
-- print1.hs
modulePrint1where
main::IO()
main=putStrLn &quot;hello world!&quot;
Here’s what you should see when you load it in GHCi and
runmain:
Prelude&gt; :l print1.hs
[1 of 1] Compiling Print1
Ok, modules loaded: Print1.
*Print1&gt; main
hello world!
*Print1&gt;
First, note that your Prelude&gt; prompt may have changed
to reflect the name of the module you loaded. You can use
:module or:mto unload the module and return to Prelude if
you wish. You can also set your prompt to something specific,
which means it won’t change every time you load or unload a
module1:
Prelude&gt; :set prompt &quot;λ&gt; &quot;
λ&gt; :r
1You can set it permanently if you prefer by setting the configuration in your ~/.ghci
file. You may have to create that file yourself in order to do so.</p>
<p>CHAPTER 3. SIMPLE OPERATIONS WITH TEXT 103
Ok, modules loaded: Print1.
λ&gt; main
hello world!
λ&gt;
Looking at the code, mainis the default action when you
build an executable or run it in a REPL. It is not a function but
is often a series of instructions to execute, which can include
applying functions and producing side-eﬀects. When building
a project with Stack, having a mainexecutable in a Main.hs file
is obligatory, but you can have source files and load them in
GHCi without necessarily having a mainblock.
As you can see, mainhas the type IO (). IO, or I/O, stands
for input/output. In Haskell, it is a special type, called IO,
used when the result of running the program involves eﬀects
beyond evaluating a function or expression. Printing to the
screen is an eﬀect, so printing the output of a module must be
wrapped in this IOtype. When you enter functions directly
into the REPL, GHCi implicitly understands and implements
IOwithout you having to specify that. Since the mainaction is
the default executable, you will see it in a lot of source files
that we build from here on out. We will explain its meaning
in more detail in a later chapter.
Let’s start another file:</p>
<p>CHAPTER 3. SIMPLE OPERATIONS WITH TEXT 104
-- print2.hs
modulePrint2where
main::IO()
main= do
putStrLn &quot;Count to four for me:&quot;
putStr &quot;one, two&quot;
putStr &quot;, three, and&quot;
putStrLn &quot; four!&quot;
Thisdosyntax is a special syntax that allows for sequencing
actions. It is most commonly used to sequence the actions
that constitute your program, some of which will necessarily
perform eﬀects such as printing to the screen (that’s why the
obligatory type of mainisIO ()).donotation isn’t strictly neces-
sary, but since it often makes for more readable code than the
alternatives, you’ll see it a lot.
Here’s what you should see when you run this one:
Prelude&gt; :l print2.hs
[1 of 1] Compiling Print2
Ok, modules loaded: Print2.
Prelude&gt; main
Count to four for me:
one, two, three, and four!
Prelude&gt;</p>
<p>CHAPTER 3. SIMPLE OPERATIONS WITH TEXT 105
For a bit of fun, change the invocations of putStr toputStrLn
and vice versa. Rerun the program and see what happens. The
LninputStrLn indicates that it starts a new line.
String concatenation
Toconcatenate something means to link together . Usually when
we talk about concatenation in programming we’re talking
about linear sequences such as lists or strings of text. If we
concatenate two strings &quot;Curry&quot; and&quot; Rocks!&quot; we will get the
string&quot;Curry Rocks!&quot; . Note the space at the beginning of &quot;
Rocks!&quot; . Without that, we’d get &quot;CurryRocks!&quot; .
Let’s start a new text file and type the following:</p>
<p>CHAPTER 3. SIMPLE OPERATIONS WITH TEXT 106
-- print3.hs
modulePrint3where
myGreeting ::String
myGreeting =&quot;hello&quot; ++&quot; world!&quot;
hello::String
hello=&quot;hello&quot;
world::String
world=&quot;world!&quot;
main::IO()
main= do
putStrLn myGreeting
putStrLn secondGreeting
wheresecondGreeting =
concat [hello, &quot; &quot;, world]
We used ::to declare the types of each top-level expression.
It isn’t strictly necessary, as the compiler can infer these types,
but it is a good habit to be in as you write longer programs.
Remember, String is a type synonym for [Char]. You can
try changing the type signatures to reflect that and see if it
changes anything in the program execution.</p>
<p>CHAPTER 3. SIMPLE OPERATIONS WITH TEXT 107
If you execute this, you should see something like:
Prelude&gt; :load print3.hs
[1 of 1] Compiling Print3
Ok, modules loaded: Print3.
*Print3&gt; main
hello world!
hello world!
*Print3&gt;
This little exercise demonstrates a few things:
1.We defined values at the top level of a module: ( myGreeting ,
hello,world, andmain). That is, they were declared at the
top level so that they are available throughout the module.
2.We specify explicit types for top-level definitions.
3.We concatenate strings with (++)andconcat .
3.4 Top-level versus local definitions
What does it mean for something to be at the top level of a
module? It doesn’t necessarily mean it’s defined at the top of
the file. When the compiler reads the file, it will see all the
top-level declarations, no matter what order they come in the</p>
<p>CHAPTER 3. SIMPLE OPERATIONS WITH TEXT 108
file (with some limitations which we’ll see later). Top-level
declarations are not nested within anything else and they are
in scope throughout the whole module.
We can contrast a top-level definition with a local definition.
To be locally defined would mean the declaration is nested
within some other expression and is not visible outside that
expression. We practiced this in the previous chapter with let
andwhere. Here’s an example for review:
moduleTopOrLocal where
topLevelFunction ::Integer -&gt;Integer
topLevelFunction x=
x+woot+topLevelValue
wherewoot::Integer
woot=10
topLevelValue ::Integer
topLevelValue =5
In the above, you could import and use topLevelFunction
ortopLevelValue from another module, and they are accessi-
ble to everything else in the module. However, wootis eﬀec-
tively invisible outside of topLevelFunction . Thewhereandlet
clauses in Haskell introduce local bindings or declarations.
To bind or declare something means to give an expression a</p>
<p>CHAPTER 3. SIMPLE OPERATIONS WITH TEXT 109
name. You could pass around and use an anonymous version
oftopLevelFunction manually, but giving it a name and reusing
it by that name is less repetitious.
Also note we explicitly declared the type of wootin thewhere
clause, using the ::syntax. This wasn’t necessary (Haskell’s
type inference would’ve figured it out), but it was done here
to show you how. Be sure to load and run this code in your
REPL:
Prelude&gt; :l TopOrLocal.hs
[1 of 1] Compiling TopOrLocal
Ok, modules loaded: TopOrLocal.
*TopOrLocal&gt; topLevelFunction 5
20
Experiment with diﬀerent arguments and make sure you
understand the results you’re getting by walking through the
arithmetic in your head (or on paper).
Exercises: Scope
1.These lines of code are from a REPL session. Is 𝑦in scope
for𝑧?
Prelude&gt; let x = 5
Prelude&gt; let y = 7
Prelude&gt; let z = x * y</p>
<p>CHAPTER 3. SIMPLE OPERATIONS WITH TEXT 110
2.These lines of code are from a REPL session. Is ℎin scope
for𝑔? Go with your gut here.
Prelude&gt; let f = 3
Prelude&gt; let g = 6 * f + h
3.This code sample is from a source file. Is everything we
need to execute areain scope?
aread=pi*(r<em>r)
r=d/2
4.This code is also from a source file. Now are 𝑟and𝑑in
scope for area?
aread=pi</em>(r*r)
wherer=d/2
3.5 Types of concatenation functions
Let’s look at the types of (++)andconcat. The++function is
an infix operator. When we need to refer to an infix operator
in a position that is not infix — such as when we are using it
in a prefix position or having it stand alone in order to query
its type — we must put parentheses around it. On the other
hand,concat is a normal (not infix) function, so parentheses
aren’t necessary:</p>
<p>CHAPTER 3. SIMPLE OPERATIONS WITH TEXT 111
++ has the type [a] -&gt; [a] -&gt; [a]
concat has the type [[a]] -&gt; [a]
Here’s how we query that in GHCi:
Prelude&gt; :t (++)
(++) :: [a] -&gt; [a] -&gt; [a]
Prelude&gt; :t concat
concat :: [[a]] -&gt; [a]
The type of concat says that we have a list of lists as input
and we will return a list. It will have the same values inside it
as the list of lists did; it just flattens it into one list structure, in
a manner of speaking. A String is a list, a list of Charspecifically,
andconcat can work on lists of strings or lists of lists of other
things:
Prelude&gt; concat [[1, 2], [3, 4, 5], [6, 7]]
[1,2,3,4,5,6,7]
Prelude&gt; concat [&quot;Iowa&quot;, &quot;Melman&quot;, &quot;Django&quot;]
&quot;IowaMelmanDjango&quot;
(n.b., Assuming you are using GHC 7.10 or higher, if you
check this type signature in your REPL, you will see a dif-
ference. We’ll explain it in detail later; for now, please read
Foldable t =&gt; t [a] as being [[a]]. TheFoldable t , for our cur-
rent purposes, can be thought of as another list. In truth, list is</p>
<p>CHAPTER 3. SIMPLE OPERATIONS WITH TEXT 112
only one of the possible types here — types that have instances
of theFoldable typeclass — but right now, lists are the only one
we care about.)
But what do these types mean? Here’s how we can break it
down:
(++)::[a]-&gt;[a]-&gt;[a]
-- [1] [2] [3]
Everything after the ::is about our types, not our values.
The ‘a’ inside the list type constructor []is a type variable.
1.Take an argument of type [a]. This type is a list of ele-
ments of some type 𝑎. This function does not know what
type𝑎is. It doesn’t need to know. In the context of the
program, the type of 𝑎will be known and made concrete
at some point.
2.Take another argument of type [a], a list of elements
whose type we don’t know. Because the variables are the
same, they must be the same type throughout (a == a).
3.Return a result of type [a].
As we’ll see, because String is a type of list, the operators
we use with strings can also be used on lists of other types,
such as lists of numbers. The type [a]means that we have
a list with elements of a type 𝑎we do not yet know. If we</p>
<p>CHAPTER 3. SIMPLE OPERATIONS WITH TEXT 113
use the operators to concatenate lists of numbers, then the
𝑎in[a]will be some type of number (for example, integers).
If we are concatenating lists of characters, then 𝑎represents
aCharbecause String is[Char]. The type variable 𝑎in[a]is
polymorphic. Polymorphism is an important feature of Haskell.
For concatenation, every list must be the same type of list; we
cannot concatenate a list of numbers with a list of characters,
for example. However, since the 𝑎is a variable at the type level,
the literal values in each list we concatenate need not be the
same, only the same type. In other words, 𝑎must equal 𝑎(a ==
a).
Prelude&gt; &quot;hello&quot; ++ &quot; Chris&quot;
&quot;hello Chris&quot;
But:
Prelude&gt; &quot;hello&quot; ++ [1, 2, 3]
<interactive>:14:13:
No instance for (Num Char) arising
from the literal ‘1’
In the expression: 1
In the second argument of ‘(++)’,
namely ‘[1, 2, 3]’
In the expression: &quot;hello&quot; ++ [1, 2, 3]</p>
<p>CHAPTER 3. SIMPLE OPERATIONS WITH TEXT 114
In the first example, we have two strings, so the type of 𝑎
matches — they’re both Charin[Char] , even though the literal
values are diﬀerent. Since the type matches, no type error
occurs and we see the concatenated result.
In the second example, we have two lists (a String and a
list of numbers) whose types do not match, so we get the
error message. GHCi asks for an instance of the numeric
typeclass Numfor the type Char.Typeclasses provide definitions
of operations, or functions, that can be shared across sets of
types. For now, you can understand this message as telling
you the types don’t match so it can’t concatenate the two lists.
Exercises: Syntax Errors
Read the syntax of the following functions and decide whether
it will compile. Test them in your REPL and try to fix the
syntax errors where they occur.
1.++ [1, 2, 3] [4, 5, 6]
2.'&lt;3' ++ ' Haskell'
3.concat [&quot;&lt;3&quot;, &quot; Haskell&quot;]</p>
<p>CHAPTER 3. SIMPLE OPERATIONS WITH TEXT 115
3.6 Concatenation and scoping
We will use parentheses to call ++as a prefix (not infix) function:</p>
<p>CHAPTER 3. SIMPLE OPERATIONS WITH TEXT 116
-- print3flipped.hs
modulePrint3Flipped where
myGreeting ::String
myGreeting =(++)&quot;hello&quot; &quot; world!&quot;
hello::String
hello=&quot;hello&quot;
world::String
world=&quot;world!&quot;
main::IO()
main= do
putStrLn myGreeting
putStrLn secondGreeting
wheresecondGreeting =
(++) hello (( ++)&quot; &quot;world)
-- could've been:
-- secondGreeting =
-- hello ++ &quot; &quot; ++ world
InsecondGreeting , using++as a prefix function forces us to
shift some things around. Parenthesizing it that way empha-
sizes the right associativity of the ++function. Since it’s an infix</p>
<p>CHAPTER 3. SIMPLE OPERATIONS WITH TEXT 117
operator, you can check for yourself that it’s right associative:
Prelude&gt; :i (++)
(++) :: [a] -&gt; [a] -&gt; [a] -- Defined in ‘GHC.Base’
infixr 5 ++
Thewhereclause creates local bindings for expressions that
are not visible at the top level. In other words, the whereclause
inmainintroduces a definition visible only within the expres-
sion or function it’s attached to, rather than making it visible to
theentiremodule. Somethingvisibleatthetoplevelisinscope
for all parts of the module and may be exported by the module
or imported by a diﬀerent module. Local definitions, on the
other hand, are only visible to that one function. You cannot
import into a diﬀerent module and reuse secondGreeting .
To illustrate:</p>
<p>CHAPTER 3. SIMPLE OPERATIONS WITH TEXT 118
-- print3broken.hs
modulePrint3Broken where
printSecond ::IO()
printSecond = do
putStrLn greeting
main::IO()
main= do
putStrLn greeting
printSecond
wheregreeting =&quot;Yarrrrr&quot;
You should get an error like this:
Prelude&gt; :l print3broken.hs
[1 of 1] Compiling Print3Broken
( print3broken.hs, interpreted )
print3broken.hs:6:12: Not in scope: ‘greeting’
Failed, modules loaded: none.
Let’s take a closer look at this error:
print3broken.hs:6:12: Not in scope: ‘greeting’</p>
<h1 id="12-3-4"><a class="header" href="#12-3-4">[1][2] [3] [4]</a></h1>
<p>CHAPTER 3. SIMPLE OPERATIONS WITH TEXT 119
1.The line the error occurred on: in this case, line 6.
2.The column the error occurred on: column 12. Text
on computers is often described in terms of lines and
columns. These line and column numbers are about lines
and columns in your text file containing the source code.
3.The actual problem: we refer to something not in scope,
that is, not visible to theprintSecond function.
4.The thing we referred to that isn’t visible or in scope.
Now make the Print3Broken code compile. It should print
“Yarrrrr” twice on two diﬀerent lines and then exit.
3.7 More list functions
Since a String is a specialized type of list, you can use standard
list operations on strings as well.
Here are some examples:
Prelude&gt; :t 'c'
'c' :: Char
Prelude&gt; :t &quot;c&quot;
&quot;c&quot; :: [Char]
-- [Char] is String
The(:)operator, called cons, builds a list:</p>
<p>CHAPTER 3. SIMPLE OPERATIONS WITH TEXT 120
Prelude&gt; 'c' : &quot;hris&quot;
&quot;chris&quot;
Prelude&gt; 'P' : &quot;&quot;
&quot;P&quot;
Next up, headreturns the head or first element of a list:
Prelude&gt; head &quot;Papuchon&quot;
'P'
The complementary function tailreturns the list with the
head chopped oﬀ:
Prelude&gt; tail &quot;Papuchon&quot;
&quot;apuchon&quot;
takereturns the specified number of elements from the list,
starting from the left:
Prelude&gt; take 1 &quot;Papuchon&quot;
&quot;P&quot;
Prelude&gt; take 0 &quot;Papuchon&quot;
&quot;&quot;
Prelude&gt; take 6 &quot;Papuchon&quot;
&quot;Papuch&quot;
Anddropreturns the remainder of the list after the specified
number of elements has been dropped:</p>
<p>CHAPTER 3. SIMPLE OPERATIONS WITH TEXT 121
Prelude&gt; drop 4 &quot;Papuchon&quot;
&quot;chon&quot;
Prelude&gt; drop 9001 &quot;Papuchon&quot;
&quot;&quot;
Prelude&gt; drop 1 &quot;Papuchon&quot;
&quot;apuchon&quot;
We’ve already seen the (++)operator:
Prelude&gt; &quot;Papu&quot; ++ &quot;chon&quot;
&quot;Papuchon&quot;
Prelude&gt; &quot;Papu&quot; ++ &quot;&quot;
&quot;Papu&quot;
The infix operator, (!!), returns the element that is in the
specified position in the list. Note that this is an indexing func-
tion, and indices start from 0. That means the first element of
your list is 0, not 1, when using this function:
Prelude&gt; &quot;Papuchon&quot; !! 0
'P'
Prelude&gt; &quot;Papuchon&quot; !! 4
'c'
Note that while all these functions are standard Prelude func-
tions, many of them are considered unsafe. They are unsafe
because they do not cover the case where they are given an</p>
<p>CHAPTER 3. SIMPLE OPERATIONS WITH TEXT 122
empty list as input. Instead they throw out an error message,
orexception :
Prelude&gt; head &quot;&quot;
*** Exception: Prelude.head: empty list
Prelude&gt; &quot;&quot; !! 4
*** Exception: Prelude.!!: index too large
This isn’t ideal behavior, so the use of these functions is
considered unwise for programs of any real size or complexity,
although we will use them in these first few chapters. We will
address how to cover all cases and make safer versions of such
functions in a later chapter.
3.8 Chapter Exercises
Reading syntax
1.For the following lines of code, read the syntax carefully
and decide if they are written correctly. Test them in your
REPL after you’ve decided to check your work. Correct
as many as you can.
a)concat[[1,2,3], [4,5,6]]
b)++[1,2,3] [4,5,6]
c)(++)&quot;hello&quot; &quot; world&quot;</p>
<p>CHAPTER 3. SIMPLE OPERATIONS WITH TEXT 123
d)[&quot;hello&quot; ++&quot; world]
e)4!!&quot;hello&quot;
f)(!!)&quot;hello&quot; 4
g)take&quot;4 lovely&quot;
h)take3&quot;awesome&quot;
2.Next we have two sets: the first set is lines of code and
the other is a set of results. Read the code and figure out
which results came from which lines of code. Be sure to
test them in the REPL.
a)concat[[1<em>6], [2</em>6], [3<em>6]]
b)&quot;rain&quot;++drop2&quot;elbow&quot;
c)10</em>head [1,2,3]
d)(take3&quot;Julie&quot;)++(tail&quot;yes&quot;)
e)concat[tail [1,2,3],
tail [4,5,6],
tail [7,8,9]]
Can you match each of the previous expressions to one
of these results presented in a scrambled order?
a)&quot;Jules&quot;
b)[2,3,5,6,8,9]</p>
<p>CHAPTER 3. SIMPLE OPERATIONS WITH TEXT 124
c)&quot;rainbow&quot;
d)[6,12,18]
e)10
Building functions
1.Given the list-manipulation functions mentioned in this
chapter, write functions that take the following inputs and
return the expected outputs. Do them directly in your
REPL and use the takeanddropfunctions you’ve already
seen.
Example
-- If you apply your function
-- to this value:
&quot;Hello World&quot;
-- Your function should return:
&quot;ello World&quot;
The following would be a fine solution:
Prelude&gt; drop 1 &quot;Hello World&quot;
&quot;ello World&quot;</p>
<p>CHAPTER 3. SIMPLE OPERATIONS WITH TEXT 125
Now write expressions to perform the following trans-
formations, just with the functions you’ve seen in this
chapter. You do not need to do anything clever here.
a)-- Given
&quot;Curry is awesome&quot;
-- Return
&quot;Curry is awesome!&quot;
b)-- Given
&quot;Curry is awesome!&quot;
-- Return
&quot;y&quot;
c)-- Given
&quot;Curry is awesome!&quot;
-- Return
&quot;awesome!&quot;
2.Now take each of the above and rewrite it in a source
file as a general function that could take diﬀerent string
inputs as arguments but retain the same behavior. Use
a variable as the argument to your (named) functions. If
you’re unsure how to do this, refresh your memory by
looking at the waxOff exercise from the previous chapter
and the TopOrLocal module from this chapter.</p>
<p>CHAPTER 3. SIMPLE OPERATIONS WITH TEXT 126
3.Write a function of type String -&gt; Char which returns the
third character in a String . Remember to give the function
a name and apply it to a variable, not a specific String,
so that it could be reused for diﬀerent String inputs, as
demonstrated (feel free to name the function something
else. Be sure to fill in the type signature and fill in the
function definition after the equals sign):
thirdLetter ::
thirdLetter x=
-- If you apply your function
-- to this value:
&quot;Curry is awesome&quot;
-- Your function should return
`r'
Note that programming languages conventionally start
indexing things by zero, so getting the zeroth index of a
string will get you the first letter. Accordingly, indexing
with 3 will get you the fourth. Keep this in mind as you
write this function.
4.Now change that function so the string operated on is
always the same and the variable represents the number
of the letter you want to return (you can use “Curry is</p>
<p>CHAPTER 3. SIMPLE OPERATIONS WITH TEXT 127
awesome!” as your string input or a diﬀerent string if you
prefer).
letterIndex ::Int-&gt;Char
letterIndex x=
5.Using the takeanddropfunctions we looked at above, see
if you can write a function called rvrs(an abbreviation of
‘reverse’ used because there is a function called ‘reverse’
already in Prelude, so if you call your function the same
name, you’ll get an error message). rvrsshould take the
string “Curry is awesome” and return the result “awesome
is Curry.” This may not be the most lovely Haskell code
you will ever write, but it is quite possible using only what
we’ve learned so far. First write it as a single function in
a source file. This doesn’t need to, and shouldn’t, work
for reversing the words of anysentence. You’re expected
only to slice and dice this particular string with takeand
drop.
6.Let’s see if we can expand that function into a module.
Why would we want to? By expanding it into a module,
we can add more functions later that can interact with
each other. We can also then export it to other modules
if we want to and use this code in those other modules.
There are diﬀerent ways you could lay it out, but for the</p>
<p>CHAPTER 3. SIMPLE OPERATIONS WITH TEXT 128
sake of convenience, we’ll show you a sample layout so
that you can fill in the blanks:
moduleReverse where
rvrs::String-&gt;String
rvrsx=
main::IO()
main=print()
Into the parentheses after printyou’ll need to fill in your
function name rvrsplus the argument you’re applying
rvrsto, in this case “Curry is awesome.” That rvrsfunction
plus its argument are now the argument to print. It’s
important to put them inside the parentheses so that that
function gets applied and evaluated first, and then that
result is printed.
Of course, we have also mentioned that you can use the $
symbol to avoid using parentheses, too. Try modifying
your main function to use that instead of the parentheses.
3.9 Definitions
1.AString is a sequence of characters. In Haskell, String is
represented by a linked-list of Charvalues, aka [Char] .</p>
<p>CHAPTER 3. SIMPLE OPERATIONS WITH TEXT 129
2.Atypeor datatype is a classification of values or data.
Types in Haskell determine what values are members
of the type or that inhabit the type. Unlike in other lan-
guages, datatypes in Haskell by default do not delimit the
operations that can be performed on that data.
3.Concatenation is the joining together of sequences of val-
ues. Often in Haskell this is meant with respect to the
[], or list, datatype, which also applies to String which is
[Char] . Theconcatenation function in Haskell is (++)which
has type [a] -&gt; [a] -&gt; [a] . For example:
Prelude&gt; &quot;tacos&quot; ++ &quot; &quot; ++ &quot;rock&quot;
&quot;tacos rock&quot;
4.Scope is where a variable referred to by name is valid.
Another word used with the same meaning is visibility ,
because if a variable isn’t visible it’s not in scope.
5.Local bindings are bindings local to particular expressions.
The primary delineation here from top level bindings is
thatlocalbindings cannot be imported by other programs
or modules.
6.Toplevelbindings in Haskell are bindings that stand outside
of any other declaration. The main feature of top-level</p>
<p>CHAPTER 3. SIMPLE OPERATIONS WITH TEXT 130
bindings is that they can be made available to other mod-
ules within your programs or to other people’s programs.
7.Data structures are a way of organizing data so that the
data can be accessed conveniently or efficiently.</p>
<p>Chapter 4
Basic datatypes
There are many ways of
trying to understand
programs. People often
rely too much on one
way, which is called
“debugging” and consists
of running a
partly-understood
program to see if it does
what you expected.
Another way, which ML
advocates, is to install
some means of
understanding in the very
programs themselves.
Robin Milner
131</p>
<p>CHAPTER 4. BECAUSE PIGS CAN’T FLY 132
4.1 Basic Datatypes
Haskell has a robust and expressive type system. Types play an
important role in the readability, safety, and maintainability of
Haskell code as they allow us to classify and delimit data, thus
restricting the forms of data our programs can process. Types,
also called datatypes , provide the means to build programs
more quickly and also allow for greater ease of maintenance.
As we learn more Haskell, we’ll learn how to leverage types
in a way that lets us accomplish the same things but with less
code.
So far, we’ve looked at expressions involving numbers, char-
acters, and lists of characters, also called strings. These are
some of the standard datatypes and are built into the standard
library. While those are useful datatypes and cover a lot of
types of values, they don’t cover every type of data. In this
chapter, we will
•review types we have seen in previous chapters;
•learn about datatypes, type constructors, and data con-
structors;
•work with predefined datatypes;
•learn more about type signatures and a bit about type-
classes.</p>
<p>CHAPTER 4. BECAUSE PIGS CAN’T FLY 133
4.2 What are types?
Expressions, when evaluated, reduce to values. Every value
has a type. Types are how we group a set of values together
that share something in common. Sometimes that common-
ality is abstract; sometimes it’s a specific model of a particular
concept or domain. If you’ve taken a mathematics course that
covered sets, thinking about types as being like sets will help
guide your intuition on what types are and how they work in
a mathematical1sense.
4.3 Anatomy of a data declaration
Data declarations are how datatypes are defined.
The type constructor is the name of the type and is capi-
talized. When you are reading or writing type signatures (the
type level of your code), the type names or type constructors
are what you use.
Data constructors are the values that inhabit the type they
are defined in. They are the values that show up in your code,
at the term level instead of the type level. By term level , we
mean they are the values as they appear in your code or the
values that your code evaluates to.
1Set theory is the study of mathematical collections of objects. Set theory was a
precursor to type theory, the latter being used prolifically in the design of programming
languages like Haskell. Logical operations like disjunction (or) and conjunction (and) used
in the manipulation of sets have equivalents in Haskell’s type system.</p>
<p>CHAPTER 4. BECAUSE PIGS CAN’T FLY 134
We will start with a basic datatype to see how datatypes are
structured and get acquainted with the vocabulary. Boolisn’t a
datatype we’ve seen yet in the book, but it provides for truth
values. It is named after the great logician George Boole and
the system of logic named for him. Because there are only
two truth values, there are only two data constructors:
-- the definition of Bool
dataBool=False|True
-- [1] [2] [3] [4]
1.Type constructor for datatype Bool. This is the name of
the type and shows up in type signatures.
2.Data constructor for the value False.
3.Pipe|indicates a sum type or logical disjunction: or. So, a
Boolvalue is TrueorFalse.
4.Data constructor for the value True.
The whole thing is called a data declaration. Data declara-
tions do not always follow precisely the same pattern — there
are datatypes that use logical conjunction ( and) instead of dis-
junction, and some type constructors and data constructors
may have arguments. The thing they have in common is the
keyword datafollowed by the type constructor (or name of
the type that will appear in type signatures), the equals sign to</p>
<p>CHAPTER 4. BECAUSE PIGS CAN’T FLY 135
denote a definition, and then data constructors (or names of
values that inhabit your term-level code).
You can find the datatype definition for built-in datatypes
by using :infoin GHCi:
Prelude&gt; :info Bool
data Bool = False | True
Let’s look at where diﬀerent parts of datatypes show up
in our code. If we query the type information for a function
callednotwe see that it takes one Boolvalue and returns another
Boolvalue, so the type signature makes reference to the type
constructor, or datatype name:
Prelude&gt; :t not
not :: Bool -&gt; Bool
But when we use the notfunction, we use the data construc-
tors, or values:
Prelude&gt; not True
False
And our expression evaluates to another data constructor,
or value — in this case, the other data constructor for the same
datatype.</p>
<p>CHAPTER 4. BECAUSE PIGS CAN’T FLY 136
Exercises: Mood Swing
Given the following datatype, answer the following questions:
dataMood=Blah|Wootderiving Show
Thederiving Show part is not something we’ve explained
yet. For now, all we’ll say is that when you make your own
datatypes, deriving Showallows the values of that type to be
printed to the screen. We’ll talk about it more when we talk
about typeclasses in detail.
1.What is the type constructor, or name of this type?
2.If the function requires a Moodvalue, what are the values
you could possibly use?
3.We are trying to write a function changeMood to change
Chris’s mood instantaneously. It should act like notin
that, given one value, it returns the othervalue of the same
type. So far, we’ve written a type signature changeMood ::
Mood -&gt; Woot . What’s wrong with that?
4.Now we want to write the function that changes his mood.
Given an input mood, it gives us the other one. Fix any
mistakes and complete the function:
changeMood Mood=Woot
changeMood _ =Blah</p>
<p>CHAPTER 4. BECAUSE PIGS CAN’T FLY 137
We’re doing something here called pattern matching . We
can define a function by matching on a data constructor,
or value, and describing the behavior that the function
should have based on which value it matches. The un-
derscore in the second line denotes a catch-all, otherwise
case. So, in the first line of the function, we’re telling it
what to do in the case of a specific input. In the second
one, we’re telling it what to do regardless of all potential
inputs. It’s trivial when there are only two potential values
of a given type, but as we deal with more complex cases,
it can be convenient.
5.Enter all of the above — datatype (including the deriving
Showbit), your corrected type signature, and the corrected
function into a source file. Load it and run it in GHCi to
make sure you got it right.
4.4 Numeric types
Let’s look next at numeric types, because we’ve already seen
these types in previous chapters, and numbers are familiar
territory. It’s important to understand that Haskell does not
use only one type of number. For most purposes, the types of
numbers we need to be concerned with are:</p>
<p>CHAPTER 4. BECAUSE PIGS CAN’T FLY 138
Integral numbers These are whole numbers, positive and
negative.
1.Int: This type is a fixed-precision integer. By “fixed pre-
cision,” we mean it has a range, with a maximum and a
minimum, and so it cannot be arbitrarily large or small
— more about this in a moment.
2.Integer : This type is also for integers, but this one supports
arbitrarily large (or small) numbers.
Fractional These are not integers. Fractional values include
the following four types:
1.Float: This is the type used for single-precision float-
ing point numbers. Fixed-point number representations
have immutable numbers of digits assigned for the parts
of the number before and after the decimal point. In
contrast, floating point can shift how many bits it uses to
represent numbers before or after the decimal point. This
flexibility does, however, mean that floating point arith-
metic violates some common assumptions and should
only be used with great care. Generally, floating point
numbers should not be used at all in business applications.</p>
<p>CHAPTER 4. BECAUSE PIGS CAN’T FLY 139
2.Double : This is a double-precision floating point number
type. It has twice as many bits with which to describe
numbers as the Floattype.
3.Rational : This is a fractional number that represents a
ratio of two integers. The value 1 / 2 :: Rational will be a
value carrying two Integer values, the numerator 1and the
denominator 2, and represents a ratio of 1 to 2. Rational is
arbitrarily precise but not as efficient as Scientific .
4.Scientific : This is a space efficient and almost arbitrary
precision scientific number type. Scientific numbers are
represented using scientific notation. It stores the coef-
ficient as an Integer and the exponent as an Int. Since
Intisn’t arbitrarily large, there is technically a limit to
the size of number you can express with Scientific , but
hitting that limit is quite unlikely. Scientific is available
in a library2and can be installed using cabal install or
stack install .
These numeric datatypes all have instances of a typeclass
calledNum. We will talk about typeclasses in the upcoming
chapters, but as we look at the types in this section, you will
seeNumlisted in some of the type information.
Typeclasses are a way of adding functionality to types that
is reusable across all the types that have instances of that type-
2Hackage page for scientific :https://hackage.haskell.org/package/scientific</p>
<p>CHAPTER 4. BECAUSE PIGS CAN’T FLY 140
class.Numis a typeclass for which most numeric types will
have an instance because there are standard functions that are
convenient to have available for all types of numbers. The Num
typeclass is what provides your standard (+),(-), and(*)oper-
ators along with a few others. Any type that has an instance of
Numcan be used with those functions. An instance defines how
the functions work for that specific type. We will talk about
typeclasses in much more detail soon.
Integral numbers
As we noted above, there are two main types of integral num-
bers:IntandInteger .
Integral numbers are whole numbers with no fractional
component. The following are integral numbers:
1219932442353464675685678
The following are not integral numbers:
1.31/2
Integer
The numbers of type Integer are the sorts of numbers we’re
used to working with in arithmetic equations that involve
whole numbers. They can be positive or negative, and Integer
extends as far in either direction as one needs them to go.</p>
<p>CHAPTER 4. BECAUSE PIGS CAN’T FLY 141
TheBooldatatype only has two possible values, so we can list
them explicitly as data constructors. In the case of Integer , and
most numeric datatypes, these data constructors are not writ-
ten out because they include an infinite series of whole num-
bers. We’d need infinite data constructors stretching up and
down from zero. Hypothetically we could represent Integer as
a sum of three cases, recursive constructors headed towards
negative infinity, zero, and recursive constructors headed to-
wards positive infinity. This representation would be terribly
inefficient so there’s some GHC magic sprinkled on it.
Why do we have Int?
TheIntnumeric types are artifacts of what computer hardware
has supported natively over the years. Most programs should
useInteger , notInt, unless the limitations of the type are un-
derstood and the additional performance makes a diﬀerence.
The danger of Intand the related types Int8,Int16, et al. is
that they cannot express arbitrarily large quantities of infor-
mation. Since they are integral, this means they cannot be
arbitrarily large in the positive or negative sense.
Here’s what happens if we try to represent a number too
large for Int8:
Prelude&gt; import GHC.Int
Prelude&gt; 127 :: Int8</p>
<p>CHAPTER 4. BECAUSE PIGS CAN’T FLY 142
127
Prelude&gt; 128 :: Int8
<interactive>:11:1: Warning:
Literal 128 is out of the
Int8 range -128..127
If you are trying to write a large
negative literal,
use NegativeLiterals
-128
Prelude&gt; (127 + 1) :: Int8
-128
The syntax you see there, :: Int8 is us assigning the Int8
type to these numbers. As we will explain in more detail in
the next chapter, numbers are polymorphic under the surface,
and the compiler doesn’t assign them a concrete type until it
is forced to. It would be weird and unexpected if the compiler
defaulted all numbers to Int8, so in order to reproduce the
situation of having a number too large for an Inttype, we had
to assign that concrete type to it.
As you can see, 127 is fine because it is within the range of
valid values of type Int8, but 128 gives you a warning about
the impending overflow, and 127 + 1 overflows the bounds
and resets back to its smallest numeric value. Because the
memory the value is allowed to occupy is fixed for Int8, it</p>
<p>CHAPTER 4. BECAUSE PIGS CAN’T FLY 143
cannot grow to accommodate the value 128 the way Integer
can. Here the 8 represents how many bits the type uses to
represent integral numbers.3Being of a fixed size can be
useful in some applications, but most of the time, Integer is
preferred.
You can find out the minimum and maximum bounds of
numeric types using maxBound andminBound from the Bounded
typeclass. Here’s an example using our Int8andInt16example:
Prelude&gt; import GHC.Int
Prelude&gt; :t minBound
minBound :: Bounded a =&gt; a
Prelude&gt; :t maxBound
maxBound :: Bounded a =&gt; a
Prelude&gt; minBound :: Int8
-128
Prelude&gt; minBound :: Int16
-32768
Prelude&gt; minBound :: Int32
-2147483648
Prelude&gt; minBound :: Int64
-9223372036854775808
3The representation used for the fixed-size Inttypes is two’s complement .</p>
<p>CHAPTER 4. BECAUSE PIGS CAN’T FLY 144
Prelude&gt; maxBound :: Int8
127
Prelude&gt; maxBound :: Int16
32767
Prelude&gt; maxBound :: Int32
2147483647
Prelude&gt; maxBound :: Int64
9223372036854775807
Thus you can find the limitations of possible values for
any type that has an instance of that particular typeclass. In
this case, we are able to find out the range of values we can
represent with an Int8is -128 to 127.
You can find out if a type has an instance of Bounded , or any
other typeclass, by asking GHCi for the :infofor that type.
Doing this will also give you the datatype representation for
the type you queried:
Prelude&gt; :i Int
data Int = GHC.Types.I# GHC.Prim.Int#
-- Defined in ‘GHC.Enum’
instance Bounded Int
Intof course has many more typeclass instances, but Bounded
is the one we cared about at this time.</p>
<p>CHAPTER 4. BECAUSE PIGS CAN’T FLY 145
Fractional numbers
The four common Fractional types in use in Haskell are Float,
Double ,Rational , andScientific .Rational ,Double , andFloatcome
with your install of GHC. Scientific comes from a library, as
we mentioned previously. Rational andScientific are arbi-
trary precision, with the latter being more efficient. Arbitrary
precision means that these can be used to do calculations re-
quiring a high degree of precision rather than being limited
to a specific degree of precision, the way FloatandDouble are.
You almost never want a Floatunless you’re doing graphics
programming such as with OpenGL.
Some computations involving numbers will be fractional
rather than integral. A good example of this is the division
function (/)which has the type:
Prelude&gt; :t (/)
(/) :: Fractional a =&gt; a -&gt; a -&gt; a
The notation Fractional a =&gt; denotes a typeclass constraint .
It tells us the type variable 𝑎must implement the Fractional
typeclass. Whatever type of number 𝑎turns out to be, it must
be a type that has an instance of the Fractional typeclass; that
is, there must be a declaration of how the operations from
that typeclass will work for the type. The /function will take
one number that implements Fractional , divide it by another</p>
<p>CHAPTER 4. BECAUSE PIGS CAN’T FLY 146
of the same type, and return a value of the same type as the
result.
Fractional is a typeclass that requires types to already have
an instance of the Numtypeclass. We describe this relation-
ship between typeclasses by saying that Numis a superclass of
Fractional . So(+)and other functions from the Numtypeclass
can be used with Fractional numbers, but functions from the
Fractional typeclass cannot be used with all types that have a
Numinstance:
Here’s what happens when we use (/)in the REPL:
Prelude&gt; 1 / 2
0.5
Prelude&gt; 4 / 2
2.0
Note that even when we had a whole number as a result,
the result was fractional. This is because values of Fractional
a =&gt; a default to the floating point type Double. In most cases,
you won’t want to explicitly use Double. You may be better
oﬀ using the arbitrary precision sibling to Integer ,Scientific .
Most people do not find it easy to reason about floating point
arithmetic and find it difficult to code around the quirks (those
quirks exist by design, but that’s another story), so in order
to avoid making mistakes, use arbitrary-precision types as a
matter of course.</p>
<p>CHAPTER 4. BECAUSE PIGS CAN’T FLY 147
4.5 Comparing values
Up to this point, most of our operations with numbers have
involved doing simple arithmetic. We can also compare num-
bers to determine whether they are equal, greater than, or less
than:
Prelude&gt; let x = 5
Prelude&gt; x == 5
True
Prelude&gt; x == 6
False
Prelude&gt; x &lt; 7
True
Prelude&gt; x &gt; 3
True
Prelude&gt; x /= 5
False
Notice here that we first declared a value for 𝑥using the
standard equals sign. Now we know that for the remainder of
our REPL session, all instances of 𝑥will be the value 5. Because
the equals sign in Haskell is already used to define variables
and functions, we must use a double equals sign, ==, to have
the specific meaning is equal to . The/=symbol means is not
equal to . The other symbols should already be familiar to you.</p>
<p>CHAPTER 4. BECAUSE PIGS CAN’T FLY 148
Having done this, we see that GHCi is returning a result of
eitherTrueorFalse, depending on whether the expression is
true or false. TrueandFalseare the data constructors for the
Booldatatype we saw above. If you look at the type information
for any of these infix operators, you’ll find the result type listed
asBool:
Prelude&gt; :t (==)
(==) :: Eq a =&gt; a -&gt; a -&gt; Bool
Prelude&gt; :t (&lt;)
(&lt;) :: Ord a =&gt; a -&gt; a -&gt; Bool
Notice that we get some typeclass constraints again. Eqis a
typeclass that includes everything that can be compared and
determined to be equal in value; Ordis a typeclass that includes
all things that can be ordered. Note that neither of these is
limited to numbers. Numbers can be compared and ordered,
of course, but so can letters, so this typeclass constraint allows
for flexibility. These equality and comparison functions can
take any values that can be said to have equal value or can be
ordered. The rest of the type information tells us that it takes
one of these values, compares it to another value of the same
type, and returns a Boolvalue. As we’ve already seen, the Bool
values are TrueorFalse.
With this information, let’s try playing with some other
values:</p>
<p>CHAPTER 4. BECAUSE PIGS CAN’T FLY 149
Prelude&gt; 'a' == 'a'
True
Prelude&gt; 'a' == 'b'
False
Prelude&gt; 'a' &lt; 'b'
True
Prelude&gt; 'a' &gt; 'b'
False
Prelude&gt; 'a' == 'A'
False
Prelude&gt; &quot;Julie&quot; == &quot;Chris&quot;
False
We know that alphabetical characters can be ordered, al-
though we do not normally think of ‘a’ as being “less than” ‘b.’
But we can understand that here it means ‘a’ comes before ‘b’ in
alphabetical order. Further, we see this also works with strings
such as “Julie” or “Chris.” GHCi has faithfully determined that
those two strings are not equal in value.
Now use your REPL to determine whether ‘a’ or ‘A’ is greater.
Next, take a look at this sample and see if you can figure out
why GHCi returns the given results:
Prelude&gt; &quot;Julie&quot; &gt; &quot;Chris&quot;
True
Prelude&gt; &quot;Chris&quot; &gt; &quot;Julie&quot;
False</p>
<p>CHAPTER 4. BECAUSE PIGS CAN’T FLY 150
Good to see Haskell code that reflects reality. “Julie” is
greater than “Chris” because J &gt;C, if the words had been “Back”
and “Brack” then it would’ve skipped the first letter to deter-
mine which was greater because B == B, then “Brack” would
have been greater because ‘r’ &gt;‘a’ in the lexicographic ordering.
Note that this is leaning on the Ordtypeclass instances for the
list type andChar. You can only compare lists of items where
the items themselves also have an instance of Ord. Accordingly,
the following will work because CharandInteger have instances
ofOrd:
Prelude&gt; ['a', 'b'] &gt; ['b', 'a']
False
Prelude&gt; 1 &gt; 2
False
Prelude&gt; [1, 2] &gt; [2, 1]
False
A datatype that has no instance of Ordwill not work with
these functions:
Prelude&gt; data Mood = G | B deriving Show
Prelude&gt; [G, B]
[G,B]
Prelude&gt; [G, B] &gt; [B, G]
<interactive>:28:14:</p>
<p>CHAPTER 4. BECAUSE PIGS CAN’T FLY 151
No instance for (Ord Mood) arising
from a use of ‘&gt;’
In the expression: [G, B] &gt; [B, G]
In an equation for ‘it’:
it = [G, B] &gt; [B, G]
“No instance for (Ord Mood) ” means it doesn’t have an Ord
instance to know how to order these values.
Here is another thing that doesn’t work with these functions:
Prelude&gt; &quot;Julie&quot; == 8
<interactive>:38:12:
No instance for (Num [Char]) arising from
the literal ‘8’
In the second argument of ‘(==)’,
namely ‘8’
In the expression: &quot;Julie&quot; == 8
In an equation for ‘it’: it = &quot;Julie&quot; == 8
We said above that comparison functions are polymorphic
and can work with a lot of diﬀerent types. But we also noted
that the type information only admitted values of matching
types. Once you’ve given a term-level value that is a String
such as “Julie,” the type is determined and the other argument
must have the same type. The error message we see above is</p>
<p>CHAPTER 4. BECAUSE PIGS CAN’T FLY 152
telling us that the type of the literal value 8 doesn’t match the
type of the first value, and for this function, it must.
4.6 Go on and Bool me
TheBooldatatype comes standard in the Prelude . As we saw
earlier, Boolis a sum type with two constructors:
dataBool=False|True
This declaration creates a datatype with the type construc-
torBool, and we refer to specific types by their type construc-
tors. We use type constructors in type signatures, not in the
expressions that make up our term-level code. The type con-
structor Booltakes no arguments (some type constructors do
take arguments). The definition of Boolabove also creates two
data constructors, TrueandFalse. Both of these values are of
typeBool. Any function that accepts values of type Bool must
allow for the possibility of TrueorFalse; you cannot specify
in the type that it should only accept one specific value. An
English language formulation of this datatype would be some-
thing like: “The datatype Boolis represented by the values True
orFalse.”
Remember, you can find the type of any value by asking
for it in GHCi, just as you can with functions:
Prelude&gt; :t True</p>
<p>CHAPTER 4. BECAUSE PIGS CAN’T FLY 153
True :: Bool
Prelude&gt; :t &quot;Julie&quot;
&quot;Julie&quot; :: [Char]
Now let’s have some fun with Bool. We’ll start by reviewing
thenotfunction:
Prelude&gt; :t not
not :: Bool -&gt; Bool
Prelude&gt; not True
False
Note that we capitalize TrueandFalsebecause they are our
data constructors. What happens if you try to use notwithout
capitalizing them?
Let’s try something slightly more complex:
Prelude&gt; let x = 5
Prelude&gt; not (x == 5)
False
Prelude&gt; not (x &gt; 7)
True
We know that comparison functions evaluate to a Boolvalue,
so we can use them with not.
Let’splaywithinfixoperatorsthatdealdirectlywithboolean
logic. How do we use Booland these associated functions?</p>
<p>CHAPTER 4. BECAUSE PIGS CAN’T FLY 154
First,(&amp;&amp;)is the infix operator for boolean conjunction. The
first example reads, colloquially, “true and true:”
Prelude&gt; True &amp;&amp; True
True
Prelude&gt; (8 &gt; 4) &amp;&amp; (4 &gt; 5)
False
Prelude&gt; not (True &amp;&amp; True)
False
The infix operator for boolean disjunction is (||), so the
first example here reads “false or true:”
Prelude&gt; False || True
True
Prelude&gt; (8 &gt; 4) || (4 &gt; 5)
True
Prelude&gt; not ((8 &gt; 4) || (4 &gt; 5))
False
We can look up info about datatypes that are in scope (if
they’re not in scope, we have to import the module they live
in to bring them into scope) using the :infocommand GHCi
provides. Scope is a way to refer to where a named binding to
an expression is valid. When we say something is in scope , it
means you can use that expression by its bound name, either
because it was defined in the same function or module, or</p>
<p>CHAPTER 4. BECAUSE PIGS CAN’T FLY 155
because you imported it. So, it’s visible to the program we’re
trying to run right now. What is built into Haskell’s Prelude
module is significant because everything in it is automatically
imported and in scope. For now, this is what we want so we
don’t have to write every function from scratch.
Exercises: Find the Mistakes
The following lines of code may have mistakes — some of
them won’t compile! You know what you need to do.
1.not True &amp;&amp; true
2.not (x = 6)
3.(1 * 2) &gt; 5
4.[Merry] &gt; [Happy]
5.[1, 2, 3] ++ &quot;look at me!&quot;
Conditionals with if-then-else
Haskell doesn’t have ‘if’ statements, but it does have if ex-
pressions . It’s a built-in bit of syntax that works with the Bool
datatype.
Prelude&gt; let t = &quot;Truthin'&quot;
Prelude&gt; let f = &quot;Falsin'&quot;</p>
<p>CHAPTER 4. BECAUSE PIGS CAN’T FLY 156
Prelude&gt; if True then t else f
&quot;Truthin'&quot;
The expression if True evaluates to True, hence we return 𝑡.
Prelude&gt; if False then t else f
&quot;Falsin'&quot;
Prelude&gt; :t if True then t else f
if True then &quot;Truthin'&quot; else &quot;Falsin'&quot;
:: [Char]
Andif False evaluates to False, so we return the elsevalue.
The type of the whole expression is String (aka[Char] ) because
that’s the type of the value that is returned as a result.
The structure here is:
if CONDITION
then EXPRESSION_A
else EXPRESSION_B
If the condition (which must evaluate to Bool) reduces to the
valueTrue, thenEXPRESSION_A istheresult, otherwise EXPRESSION_B .
If-expressions can be thought of as a way to choose between
two values. You can embed a variety of expressions within
theifof an if-then-else, as long as it evaluates to Bool. The
types of the expressions in the thenandelseclauses must be
the same, as in the following:</p>
<p>CHAPTER 4. BECAUSE PIGS CAN’T FLY 157
Prelude&gt; let x = 0
Prelude&gt; let a = &quot;AWESOME&quot;
Prelude&gt; let w = &quot;wut&quot;
Prelude&gt; if (x + 1 == 1) then a else w
&quot;AWESOME&quot;
Here’s how it reduces:
-- Given:
x=0
if(x+1==1)then&quot;AWESOME&quot; else&quot;wut&quot;
-- x is zero
if(0+1==1)then&quot;AWESOME&quot; else&quot;wut&quot;
-- reduce 0 + 1 so we can see
-- if it's equal to 1
if(1==1)then&quot;AWESOME&quot; else&quot;wut&quot;
-- Does 1 equal 1?
ifTruethen&quot;AWESOME&quot; else&quot;wut&quot;
-- pick the branch based on the Bool value
&quot;AWESOME&quot;
-- dunzo</p>
<p>CHAPTER 4. BECAUSE PIGS CAN’T FLY 158
But this does not work:
Prelude&gt; let dog = &quot;adopt a dog&quot;
Prelude&gt; let cat = &quot;or a cat&quot;
Prelude&gt; let x = 0
Prelude&gt; if x * 100 then dog else cat
<interactive>:15:7:
No instance for (Num Bool) arising
from a use of ‘*’
In the expression: (x * 100)
In the expression:
if (x * 100)
then &quot;adopt a dog&quot;
else &quot;or a cat&quot;
In an equation for ‘it’:
it = if (x * 100)
then &quot;adopt a dog&quot;
else &quot;or a cat&quot;
We got this type error because the condition passed to the
if-expression is of type Num a =&gt; a , notBoolandBooldoesn’t
implement the Numtypeclass. To oversimplify, (x * 100) evalu-
ates to a numeric result, and numbers aren’t truth values. It
would have typechecked had the condition been x * 100 == 0</p>
<p>CHAPTER 4. BECAUSE PIGS CAN’T FLY 159
orx * 100 == 9001 . In those cases, it would’ve been an equality
check of two numbers which reduces to a Boolvalue.
Here’s an example of a function that uses a Boolvalue in an
if-expression:
-- greetIfCool1.hs
moduleGreetIfCool1 where
greetIfCool ::String-&gt;IO()
greetIfCool coolness =
ifcool
thenputStrLn &quot;eyyyyy. What's shakin'?&quot;
else
putStrLn &quot;pshhhh.&quot;
wherecool=
coolness ==&quot;downright frosty yo&quot;
When you test this in the REPL, it should play out like this:
Prelude&gt; :l greetIfCool1.hs
[1 of 1] Compiling GreetIfCool1
Ok, modules loaded: GreetIfCool1.
Prelude&gt; greetIfCool &quot;downright frosty yo&quot;
eyyyyy. What's shakin'?
Prelude&gt; greetIfCool &quot;please love me&quot;
pshhhh.</p>
<p>CHAPTER 4. BECAUSE PIGS CAN’T FLY 160
Also note that greetIfCool could’ve been written with cool
being a function rather than a value defined against the argu-
ment directly like so:
-- greetIfCool2.hs
moduleGreetIfCool2 where
greetIfCool ::String-&gt;IO()
greetIfCool coolness =
ifcool coolness
thenputStrLn &quot;eyyyyy. What's shakin'?&quot;
else
putStrLn &quot;pshhhh.&quot;
wherecool v=
v==&quot;downright frosty yo&quot;
4.7 Tuples
Tupleis a type that allows you to store and pass around multiple
values within a single value. Tuples have a distinctive, built-
in syntax that is used at both type and term levels, and each
tuple has a fixed number of constituents. We refer to tuples
by the number of values in each tuple: the two-tuple or pair,
for example, has two values inside it, (x, y); the three-tuple
or triple has three, (x, y, z) , and so on. This number is also</p>
<p>CHAPTER 4. BECAUSE PIGS CAN’T FLY 161
known as the tuple’s arity. As we will see, the values within a
tuple do not have to be of the same type.
We will start by looking at the two-tuple, a tuple with two
elements. The two-tuple is expressed at both the type level and
term level with the constructor (,). The datatype declaration
looks like this:
Prelude&gt;:info (,)
data(,) a b =(,) a b
This is diﬀerent from the Booltype we looked at earlier in a
couple of important ways, even apart from that special type
constructor syntax. The first is that it has two parameters,
represented by the type variables 𝑎and𝑏. Those have to be
applied to concrete types, much as variables at the term level
have to be applied to values to evaluate a function. The second
major diﬀerence is that this is a product type , not a sum type.
A product type represents a logical conjunction: you must
supplybotharguments to construct a value.
Notice that the two type variables are diﬀerent, so that al-
lows for tuples that contain values of two diﬀerent types. The
types are not, however, required to be diﬀerent:
λ&gt; (,) 8 10
(8,10)
λ&gt; (,) 8 &quot;Julie&quot;
(8,&quot;Julie&quot;)</p>
<p>CHAPTER 4. BECAUSE PIGS CAN’T FLY 162
λ&gt; (,) True 'c'
(True,'c')
But if we try to apply it to only one argument:
λ&gt; (,) 9
<interactive>:34:1:
No instance for (Show (b0 -&gt; (a0, b0)))
(maybe you haven't applied enough
arguments to a function?)
arising from a use of ‘print’
In the first argument of ‘print’,
namely ‘it’
In a stmt of an interactive
GHCi command: print it
Well, look at that error. This is one we will explore in detail
soon, but for now the important part is the part in parenthe-
ses: we haven’t applied the function — in this case, the data
constructor — to enough arguments.
The two-tuple in Haskell has some default convenience
functions for getting the first or second value. They’re named
fstandsnd:
fst::(a, b)-&gt;a
snd::(a, b)-&gt;b</p>
<p>CHAPTER 4. BECAUSE PIGS CAN’T FLY 163
The type signature tells us there’s nothing those functions
could do other than return the first or second value, respec-
tively.
Here are some examples of manipulating tuples, specifically
the two-tuple:
Prelude&gt; let myTup = (1 :: Integer, &quot;blah&quot;)
Prelude&gt; :t myTup
myTup :: (Integer, [Char])
Prelude&gt; fst myTup
1
Prelude&gt; snd myTup
&quot;blah&quot;
Prelude&gt; import Data.Tuple
Prelude&gt; swap myTup
(&quot;blah&quot;,1)
We had to import Data.Tuple because swapisn’t included in
thePrelude .
We can also combine tuples with other expressions:
Prelude&gt; 2 + fst (1, 2)
3
Prelude&gt; 2 + snd (1, 2)
4
The(x, y) syntax of the tuple is special. The constructors
you use in the type signatures and in your code (terms) are</p>
<p>CHAPTER 4. BECAUSE PIGS CAN’T FLY 164
syntactically identical even though they’re diﬀerent things.
Sometimes that type constructor is referred to without the
type variables explicitly inside of it such as (,). Other times,
you’ll see (a, b) — particularly in type signatures.
You can use that syntax to pattern match when you write
functions, too. One nice thing about that is that the func-
tion definition can look very much like the type signature
sometimes. For example, we can implement fstandsndfor
ourselves like this:
fst'::(a, b)-&gt;a
fst'(a, b)=a
snd'::(a, b)-&gt;b
snd'(a, b)=b
Let’s look at another example of pattern matching on tuples:
tupFunc ::(Int, [a])
-&gt;(Int, [a])
-&gt;(Int, [a])
tupFunc (a, b) (c, d) =
((a+c), (b++d))
It’s generally unwise to use tuples of an overly large size,
both for efficiency and sanity reasons. Most tuples you see will
be( , , , , ) (5-tuple) or smaller.</p>
<p>CHAPTER 4. BECAUSE PIGS CAN’T FLY 165
4.8 Lists
Lists are another type used to contain multiple values within
a single value. However, they diﬀer from tuples in three im-
portant ways: First, all elements of a list must be of the same
type. Second, lists have their own distinct []syntax. Like the
tuple syntax, it is used for both the type constructor in type
signatures and at the term level to express list values. Third,
the number of values that will be in the list isn’t specified in
the type — unlike tuples where the arity is set in the type and
immutable.
Here’s an example for your REPL:
Prelude&gt; let p = &quot;Papuchon&quot;
Prelude&gt; let awesome = [p, &quot;curry&quot;, &quot;:)&quot;]
Prelude&gt; awesome
[&quot;Papuchon&quot;,&quot;curry&quot;,&quot;:)&quot;]
Prelude&gt; :t awesome
awesome :: [[Char]]
First thing to note is that awesome is a list of lists of Charvalues
because it is a list of strings, and String is a type alias for [Char] .
This means all the functions and operations valid for lists of
any value, usually expressed as [a], are valid for String because
[Char] is more specific than [a].</p>
<p>CHAPTER 4. BECAUSE PIGS CAN’T FLY 166
Prelude&gt; let s = &quot;The Simons&quot;
Prelude&gt; let also = [&quot;Quake&quot;, s]
Prelude&gt; :t (++)
(++) :: [a] -&gt; [a] -&gt; [a]
Prelude&gt; awesome ++ also
[&quot;Papuchon&quot;,
&quot;curry&quot;,
&quot;:)&quot;,
&quot;Quake&quot;,
&quot;The Simons&quot;]
Prelude&gt; let allAwesome = [awesome, also]
Prelude&gt; allAwesome
[[&quot;Papuchon&quot;,&quot;curry&quot;,&quot;:)&quot;],
[&quot;Quake&quot;,&quot;The Simons&quot;]]
Prelude&gt; :t allAwesome
allAwesome :: [[[Char]]]
Prelude&gt; :t concat
concat :: [[a]] -&gt; [a]
Prelude&gt; concat allAwesome
[&quot;Papuchon&quot;,
&quot;curry&quot;,
&quot;:)&quot;,
&quot;Quake&quot;,
&quot;The Simons&quot;]
We’ll save a full exploration of the list datatype until we</p>
<p>CHAPTER 4. BECAUSE PIGS CAN’T FLY 167
get to the chapter on lists. The list data structure gets a whole
chapter because lists have some interesting complexity, we’re
going to use them to demonstrate some things about Haskell’s
nonstrict evaluation, and there are manystandard functions
and constructs that can be used with lists.
4.9 Chapter Exercises
As in previous chapters, you will gain more by working out the
answer before you check what GHCi tells you, but be sure
to use your REPL to check your answers to the following
exercises. Also, you will need to have the awesome ,also, and
allAwesome code from above in scope for this REPL session. For
convenience of reference, here are those values again:
awesome =[&quot;Papuchon&quot; ,&quot;curry&quot;,&quot;:)&quot;]
also=[&quot;Quake&quot;,&quot;The Simons&quot; ]
allAwesome =[awesome, also]
length is a function that takes a list and returns a result that
tells how many items are in the list.
1.Given the definition of length above, what would the type
signature be? How many arguments, of what type does it
take? What is the type of the result it evaluates to?
2.What are the results of the following expressions?</p>
<p>CHAPTER 4. BECAUSE PIGS CAN’T FLY 168
a)length [1, 2, 3, 4, 5]
b)length [(1, 2), (2, 3), (3, 4)]
c)length allAwesome
d)length (concat allAwesome)
3.Given what we know about numeric types and the type
signature of length, look at these two expressions. One
works and one returns an error. Determine which will
return an error and why.
(n.b., you will find Foldable t =&gt; t a representing [a], as
withconcat inthepreviouschapter. Again, consider Foldable
tto represent a list here, even though list is only one of
the possible types.)
Prelude&gt; 6 / 3
-- and
Prelude&gt; 6 / length [1, 2, 3]
4.How can you fix the broken code from the preceding
exercise using a diﬀerent division function/operator?
5.What is the type of the expression 2 + 3 == 5 ? What would
we expect as a result?
6.What is the type and expected result value of the follow-
ing:</p>
<p>CHAPTER 4. BECAUSE PIGS CAN’T FLY 169
Prelude&gt; let x = 5
Prelude&gt; x + 3 == 5
7.Below are some bits of code. Which will work? Why or
why not? If they will work, what value would these reduce
to?
Prelude&gt; length allAwesome == 2
Prelude&gt; length [1, 'a', 3, 'b']
Prelude&gt; length allAwesome + length awesome
Prelude&gt; (8 == 8) &amp;&amp; ('b' &lt; 'a')
Prelude&gt; (8 == 8) &amp;&amp; 9
8.Write a function that tells you whether or not a given
String (or list) is a palindrome. Here you’ll want to use
a function called reverse a predefined function that does
what it sounds like.
reverse :: [a] -&gt; [a]
reverse &quot;blah&quot;
&quot;halb&quot;
isPalindrome ::(Eqa)=&gt;[a]-&gt;Bool
isPalindrome x=undefined
9.Write a function to return the absolute value of a number
using if-then-else</p>
<p>CHAPTER 4. BECAUSE PIGS CAN’T FLY 170
myAbs::Integer -&gt;Integer
myAbs=undefined
10.Fill in the definition of the following function, using fst
andsnd:
f::(a, b)-&gt;(c, d)-&gt;((b, d), (a, c))
f=undefined
Correcting syntax
In the following examples, you’ll be shown syntactically incor-
rect code. Type it in and try to correct it in your text editor,
validating it with GHC or GHCi.
1.Here, we want a function that adds 1 to the length of a
string argument and returns that result.
x=(+)
Fxs=w'x'1
wherew=length xs
2.This is supposed to be the identity function, id.
\X=x
3.When fixed, this function will return 1 from the value (1,
2).</p>
<p>CHAPTER 4. BECAUSE PIGS CAN’T FLY 171
f(a b)=A
Match the function names to their types
1.Which of the following types is the type of show?
a)showa=&gt;a-&gt;String
b)Showa-&gt;a-&gt;String
c)Showa=&gt;a-&gt;String
2.Which of the following types is the type of (==)?
a)a-&gt;a-&gt;Bool
b)Eqa=&gt;a-&gt;a-&gt;Bool
c)Eqa-&gt;a-&gt;a-&gt;Bool
d)Eqa=&gt;A-&gt;Bool
3.Which of the following types is the type of fst?
a)(a, b)-&gt;a
b)b-&gt;a
c)(a, b)-&gt;b
4.Which of the following types is the type of (+)?
a)(+)::Numa-&gt;a-&gt;a-&gt;Bool
b)(+)::Numa=&gt;a-&gt;a-&gt;Bool</p>
<p>CHAPTER 4. BECAUSE PIGS CAN’T FLY 172
c)(+)::num a=&gt;a-&gt;a-&gt;a
d)(+)::Numa=&gt;a-&gt;a-&gt;a
e)(+)::a-&gt;a-&gt;a
4.10 Definitions
1.Atupleis an ordered grouping of values. In Haskell, you
cannot have a tuple with only one element, but there is a
zero tuple also called unitor(). The types of the elements
of tuples are allowed to vary, so you can have both (String,
String) or (Integer, String). Tuples in Haskell are the usual
means of briefly carrying around multiple values without
giving that combination its own name.
2.Atypeclass is a set of operations defined with respect to
a polymorphic type. When a type has an instance of a
typeclass, values of that type can be used in the standard
operations defined for that typeclass. In Haskell, type-
classes are unique pairings of class and concrete instance.
This means that if a given type 𝑎has an instance of Eq, it
hasonlyone instance of Eq.
3.Data constructors in Haskell provide a means of creating
values that inhabit a given type. Data constructors in
Haskell have a type and can either be constant values
(nullary) or take one or more arguments, like functions.</p>
<p>CHAPTER 4. BECAUSE PIGS CAN’T FLY 173
In the following example, Catis a nullary data constructor
forPetandDogis a data constructor that takes an argument:
-- Why name a cat?
-- They don't answer anyway.
typeName=String
dataPet=Cat|DogName
The data constructors have the following types:
Prelude&gt; :t Cat
Cat :: Pet
Prelude&gt; :t Dog
Dog :: Name -&gt; Pet
4.Type constructors in Haskell are notvalues and can only be
used in type signatures. Just as data declarations generate
data constructors to create values that inhabit that type,
data declarations generate type constructors which can be
used to denote that type. In the above example, Petis the
type constructor. A guideline for diﬀerentiating the two
kinds of constructors is that type constructors always go
to the left of the =in a data declaration.</p>
<p>CHAPTER 4. BECAUSE PIGS CAN’T FLY 174
5.Data declarations define new datatypes in Haskell. Data
declarations always create a new type constructor, but may
ormaynot create new data constructors. Data declarations
are how we refer to the entire definition that begins with
thedatakeyword.
6.Atype alias is a way to refer to a type constructor or type
constant by an alternate name, usually to communicate
something more specific or for brevity.
typeName=String
-- creates a new type alias Name of the
-- type String <em>not</em> a data declaration,
-- just a type alias declaration
7.Arityis the number of arguments a function accepts. This
notion is a little slippery in Haskell as, due to currying, all
functions are 1-arity and we handle accepting multiple
arguments by nesting functions.
8.Polymorphism in Haskell means being able to write code
in terms of values which may be one of several, or any,
type. Polymorphism in Haskell is either parametric or
constrained . The identity function, id, is an example of a
parametrically polymorphic function:
id::a-&gt;a
idx=x</p>
<p>CHAPTER 4. BECAUSE PIGS CAN’T FLY 175
Hereidworks for a value of anytype because it doesn’t
use any information specific to a given type or set of types.
Whereas, the following function isEqual :
isEqual ::Eqa=&gt;a-&gt;a-&gt;Bool
isEqual x y=x==y
Is polymorphic, but constrained orbounded to the set of
types which have an instance of the Eqtypeclass. The dif-
ferent kinds of polymorphism will be discussed in greater
detail in a later chapter.
4.11 Names and variables
Names
In Haskell there are seven categories of entities that have
names: functions, term-level variables, data constructors, type
variables, type constructors, typeclasses, and modules. Term-
level variables and data constructors exist in your terms. Term
levelis where your values live and is the code that executes
when your program is running. At the type level , which is used
during the static analysis &amp; verification of your program, we
have type variables, type constructors, and typeclasses. Lastly,
for the purpose of organizing our code into coherent group-
ings across diﬀerent files, we have modules.</p>
<p>CHAPTER 4. BECAUSE PIGS CAN’T FLY 176
Conventions for variables
Haskell uses a lot of variables, and some conventions have
developed. It’s not critical that you memorize this, because for
the most part, these are merely conventions, but familiarizing
yourself with them will help you read Haskell code.
Type variables (that is, variables in type signatures) gener-
ally start at 𝑎and go from there: 𝑎,𝑏,𝑐, and so forth. You may
sometimes see them with numbers appended to them, e.g., 𝑎1.
Functions can be used as arguments and in that case are
typically labeled with variables starting at 𝑓(followed by 𝑔and
so on). They may sometimes have numbers appended (e.g., 𝑓1)
and may also sometimes be decorated with the′character as
in𝑓′. This would be pronounced “eﬀ-prime,” should you have
need to say it aloud. Usually this denotes a function that is
closely related to or a helper function to function 𝑓. Functions
may also be given variable names that are not on this spectrum
as a mnemonic. For example, a function that results in a list
of prime numbers might be called 𝑝, or a function that fetches
some text might be called 𝑡𝑥𝑡.
Variables do not have to be a single letter. In small programs,
they often are; in larger programs, they usually should not
be a single letter. If there are many variables in a function
or program, as is common, it is helpful to have descriptive
variable names. It is often advisable in domain-specific code</p>
<p>CHAPTER 4. BECAUSE PIGS CAN’T FLY 177
to use domain-specific variable names.
Arguments to functions are most often given names start-
ing at𝑥, again occasionally seen numbered as in 𝑥1. Other
single-letter variable names may be chosen when they serve a
mnemonic role, such as choosing 𝑟to represent a value that is
the radius of a circle.
If you have a list of things you have named 𝑥, by convention
that will usually be called 𝑥𝑠, that is, the plural of 𝑥. You will
see this convention often in the form (x:xs) , which means you
have a list in which the head of the list is 𝑥and the rest of the
list is𝑥𝑠.
All of these, though, are conventions, not definite rules.
While we will generally adhere to the conventions in this book,
any Haskell code you see out in the wild may not. Calling a
type variable 𝑥instead of 𝑎is not going to break anything. As
in the lambda calculus, the names don’t have any inherent
meaning. We oﬀer this information as a descriptive guide of
Haskell conventions, not as rules you must follow in your own
code.</p>
<p>Chapter 5
Types
She was the single
artificer of the world
In which she sang. And
when she sang, the sea,
Whatever self it had,
became the self
That was her song, for she
was the maker.
Wallace Stevens, “The
Idea of Order at Key
West”
178</p>
<p>CHAPTER 5. TYPES 179
5.1 Types
In the last chapter, we looked at some built-in datatypes, such
asBooland tuples and had a brief run-in with the typeclasses
NumandEq. However, a deep understanding of types and how to
read and interpret them is fundamental to reading and writing
Haskell.
As we have seen, a datatype declaration defines a type con-
structor and data constructors. Data constructors are the val-
ues of a particular type; they are also functions that let us
create data, or values, of a particular type, although it will
take some time before the full import of this becomes clear.
In Haskell, you cannot create untyped data, so except for a
sprinkling of syntactic sugar for things like numbers or func-
tions, everything originates in a data constructor from some
definition of a type.
In this chapter, we’re going to take a deeper look at the type
system and
•learn more about querying and reading type signatures;
•see that currying has, unfortunately, nothing to do with
food;
•take a closer look at diﬀerent kinds of polymorphism;
•look at type inference and how to declare types for our
functions.</p>
<p>CHAPTER 5. TYPES 180
5.2 What are types for?
Haskell is an implementation of a pure lambda calculus, in
the sense that it isn’t much more than syntactic sugar over the
basic system of variables, abstractions, and applications that
constitute the rules of the lambda calculus — at least, of a typed
lambda calculus. Developments in logic, mathematics, and
computer science led to the discovery (or invention, take your
pick) of a typed lambda calculus called System F in the 1970s.
Haskell has improved on System F in some key ways, such as
by allowing general recursion (more on that in a later chapter)
and the Hindley-Milnersystemto permit type inference (more
on that later in this chapter), but the core logic is the same.
So, why do we want types? Type systems in logic and math-
ematics have been designed to impose constraints that enforce
correctness. For our purposes, we can say that well-designed
type systems help eliminate some classes of errors as well as
concerns such as what the eﬀect of a conditional over a non-
Boolean value might be. A type system defines the associations
between diﬀerent parts of a program and checks that all the
parts fit together in a logically consistent, provably correct
way.
Let’s consider a short, somewhat oversimplified example.
TheBooltype is a set with two inhabitants, TrueandFalse, as
we saw in the last chapter. Anytime the value TrueorFalse
occurs in a Haskell program, the typechecker will know they’re</p>
<p>CHAPTER 5. TYPES 181
members of the Booltype. The inverse is that whenever the
typeBoolis declared in a type signature, the compiler will
expect one of those two values and only one of those two
values; you get a type error if you try to pass a number where
aBoolis expected.
In Haskell, where typing is static, typechecking occurs at
compile time . That means many errors will be caught before
you try to execute, or run, your program. The diﬀerence isn’t
always obvious because GHCi allows you to typecheck things
interactively, as you’re writing them, as well as execute them if
they typecheck. No type system can eliminate all possibilities
for error, so the possibility of runtime errors and exceptions
still exists, and testing of programs is necessary, but the type
system reduces the number and kinds of tests you must write.
Good type systems can also enable compiler optimizations,
because the compiler can know and predict certain things
about the execution of a program based on the types. Types
can also serve as documentation of your program, which is
one reason we encourage you to declare types (that is, write
the type signatures) for your functions. It won’t matter too
much when you’re writing small programs, but as programs
get longer, type signatures can help you read your program
and remember what you were doing, and help anyone else
who might be trying to use your code as well.
You may feel that Haskell’s type system requires a lot of up-</p>
<p>CHAPTER 5. TYPES 182
front work. This upfront cost comes with a later payoﬀ: code
that is safer and, down the line, easier to maintain. Working
with a good type system can eliminate those tests that only
check that you’re passing the right sort of data around, and
since tests are more code that you have to write (correctly) and
maintain, it will eventually save you time and eﬀort.
Many, perhaps most, programming languages have type
systems that feel like haggling with a petty merchant. However,
we believe Haskell provides a type system that more closely
resembles a quiet, pleasant conversation with a colleague than
an argument in the bazaar. Much of what we suggest with
regards to putting code in a file, loading it in a REPL, querying
types in the REPL, and so forth, is about creating habits con-
ducive to having this pleasant back and forth with your type
system.
5.3 How to read type signatures
In previous chapters, we’ve seen that we can query types in
the REPL with the :typeor:tcommand. You can query types
for functions, partially applied functions, and values, which
are, in a way, fully applied functions.
When we query the types of values, we see something like
this:
Prelude&gt; :type 't'</p>
<p>CHAPTER 5. TYPES 183
't' :: Char -- 't' has the type Char
Prelude&gt; :type &quot;julie&quot;
&quot;julie&quot; :: [Char] -- &quot;julie&quot; has the type String
Prelude&gt; :type True
True :: Bool -- True has the type Bool
When we query the types of numeric values, we see type-
class information instead of a concrete type, because the com-
piler doesn’t know which specific numeric type a value is until
the type is either declared or the compiler is forced to infer
a specific type based on the function. For example, 13may
look like an integer to us, but that would only allow us to use it
in computations that take integers (and not, say, in fractional
division). For that reason, the compiler gives it the type with
the broadest applicability (most polymorphic) and says it’s a
constrained polymorphic Num a =&gt; a value:
Prelude&gt; :type 13
13 :: Num a =&gt; a
-- we can give it a concrete type
-- by declaring it
Prelude&gt; let x = 13 :: Integer
Prelude&gt; :t x
x :: Integer</p>
<p>CHAPTER 5. TYPES 184
You can also query the type signatures of functions, as we’ve
seen:
Prelude&gt; :type not
not :: Bool -&gt; Bool
This takes one input of a Boolvalue and returns one Bool
value. Given that type, there aren’t too many things it even
coulddo.1
Understanding the function type
The arrow, (-&gt;), is the type constructor for functions in Haskell.
It’s baked into the language, but syntactically it works in very
much the same way as all the other types you’ve seen so far. It’s
a type constructor, like Bool, except the (-&gt;)type constructor
takes arguments and has no data constructors:
Prelude&gt; :info (-&gt;)
data (-&gt;) a b
-- some further information is elided
If you compare this to the type constructor for the two-
tuple, you see the similarity:
Prelude&gt; :info (,)
data (,) a b = (,) a b
1Four, to be precise. But if we assume that the standard Prelude functions are generally
useful functions, it helps narrow it down considerably.</p>
<p>CHAPTER 5. TYPES 185
We saw earlier that the tuple constructor needs to be applied
to two values in order to construct a tuple. A function must
similarly have two arguments — one input and one result —
in order to be a function. Unlike the tuple constructor, though,
the function type has no data constructors. The value that
shows up at term level is the function. Functions are values .
As we’ve said, the hallmark of a function is that it can be
applied , and the structure of the type demonstrates this. The
arrow is an infixoperator that has two parameters and associates
to the right (although function application is left associative).
The parameterization suggests that we will apply the function
to some argument that will be bound to the first parameter,
with the second parameter, 𝑏, representing the return or result
type. We will cover these things in more detail throughout
this chapter.
Let’s return to reading type signatures. The function fstis
a value of type (a, b) -&gt; a where-&gt;is an infix type constructor
that takes two arguments:
fst::(a, b) -&gt;a
-- [1] [2] [3]
1.The first parameter of fsthas the type (a, b). Note that
the tuple type itself (,)takes two arguments 𝑎and𝑏.
2.The function type, (-&gt;), has two parameters. One is (a,
b)and one is the result 𝑎.</p>
<p>CHAPTER 5. TYPES 186
3.The result of the function, which has type 𝑎. It’s the same
𝑎that was in the tuple (a, b) .
How do we know it’s the same 𝑎? We can say that we know
the input type 𝑎and the output type 𝑎must be the same type,
and we can see that nothing happens between the input and the
output; that is, there is no operation that comes between them
that could transform that 𝑎into some other value of that type.
Let’s look at another function:
Prelude&gt; :type length
length :: [a] -&gt; Int
Thelength function takes one argument that is a list — note
the square brackets — and returns an Intresult. The Intresult
in this case will be the number of items in the list. The type
of the inhabitants of the list is left unspecified; this function
does not care — in fact, cannot care — what types of values
are inside the list.
Typeclass-constrained type variables
Next, let’s look at the types of some arithmetic functions. You
may recall that the act of wrapping an infix operator in paren-
theses allows us to use the function just like a normal prefix
function, including being able to query the type:
Prelude&gt; :type (+)</p>
<p>CHAPTER 5. TYPES 187
(+) :: Num a =&gt; a -&gt; a -&gt; a
Prelude&gt; :type (/)
(/) :: Fractional a =&gt; a -&gt; a -&gt; a
To describe these casually, we could say addition takes one
numeric argument, adds it to a second numeric argument of
the same type, and returns a numeric value of the same type
as a result. Similarly, the fractional division function takes a
fractional value, divides it by a second fractional value, and
returns a third fractional value as a result. This isn’t precise,
but it will do for now.
The compiler gives the least specific and most general type
it can. Instead of limiting this function to a concrete type, we
get a typeclass-constrained polymorphic type variable. We’ll
save a fuller explanation of typeclasses for the next chapter.
What we need to know here is that each typeclass oﬀers a stan-
dard set of functions that can be used across several concrete
types. When a typeclass is constraining a type variable in this
way, the variable could represent any of the concrete types
that have instances of that typeclass so that specific operations
on which the function depends are defined for that type. We
say it’s constrained because we still don’t know the concrete
type of 𝑎, but we do know it can onlybe one of the types that
has the required typeclass instance.
This generalization of numberhood is what lets us use the
same numerical literals to represent numeric values of dif-</p>
<p>CHAPTER 5. TYPES 188
ferent types. We can start with a Num a =&gt; a value and then
create specific versions of it with a concrete type using the ::
to assign a type to the value:
Prelude&gt; let fifteen = 15
Prelude&gt; :t fifteen
fifteen :: Num a =&gt; a
Prelude&gt; let fifteenInt = fifteen :: Int
Prelude&gt; let fifteenDouble = fifteen :: Double
Prelude&gt; :t fifteenInt
fifteenInt :: Int
Prelude&gt; :t fifteenDouble
fifteenDouble :: Double
We went from Num a =&gt; a toIntandDouble. This works be-
causeIntandDouble each have an instance of the Numtypeclass:
Prelude&gt; :info Num
[...irrelevant bits elided...]
instance Num Int -- Defined in ‘GHC.Num’
instance Num Double -- Defined in ‘GHC.Float’
Since they both have instances of Num, the operations from
Num, such as addition, are defined for both of them:
Prelude&gt; fifteenInt + fifteenInt
30</p>
<p>CHAPTER 5. TYPES 189
Prelude&gt; fifteenDouble + fifteenDouble
30.0
We can also make more specific versions of our Num a =&gt; a
value named fifteen by using it in a way that requires it to
become something more specific:
Prelude&gt; fifteenDouble + fifteen
30.0
Prelude&gt; fifteenInt + fifteen
30
What we cannot do is this:
Prelude&gt; fifteenDouble + fifteenInt
Couldn't match expected type ‘Double’
with actual type ‘Int’
In the second argument of ‘(+)’,
namely ‘fifteenInt’
In the expression: fifteenDouble + fifteenInt
We can’t add those two values because their types are no
longer polymorphic, and their concrete types are diﬀerent so
they have diﬀerent definitions of how to implement addition.
The type error message contrasts the actual type with the ex-
pected type . The actual type is what we provided; the expected</p>
<p>CHAPTER 5. TYPES 190
type is what the compiler expected. Since we had fifteenDouble
as our first argument, it expected the second value to also have
the type Double but itactually has the type Int.
A type signature might have multiple typeclass constraints
on one or more of the variables. You will sometimes see (or
write) type signatures such as:
(Numa,Numb)=&gt;a-&gt;b-&gt;b
-- or
(Orda,Numa)=&gt;a-&gt;a-&gt;Ordering
Here, the constraints look like a tuple, although they don’t
add another function argument that you must provide, and
they don’t appear as a tuple at the value or term level. Nothing
to the left of the typeclass arrow, =&gt;, shows up at term level. The
tuple of constraints doesrepresent a product, or conjunction,
of constraints.
In the first example above, there are two constraints, one
for each variable. Both 𝑎and𝑏must have instances of the
Numtypeclass. In the second example, both of the constraints
are on the one variable 𝑎— that is, 𝑎must be a type that
implements bothOrdandNum.</p>
<p>CHAPTER 5. TYPES 191
Exercises: Type Matching
Below you’ll find a list of several standard functions we’ve
talked about previously. Under that is a list of their type sig-
natures. Match the function to its type signature. Try to do
it without peeking at the type signatures (either in the text or
in GHCi) and then check your work. You may find it easier to
start from the types and work out what you think a function
of that type would do.
1.Functions:
a)not
b)length
c)concat
d)head
e)(&lt;)
2.Type signatures:
a)_ ::[a]-&gt;a
b)_ ::[[a]]-&gt;[a]
c)_ ::Bool-&gt;Bool
d)_ ::[a]-&gt;Int
e)_ ::Orda=&gt;a-&gt;a-&gt;Bool</p>
<p>CHAPTER 5. TYPES 192
5.4 Currying
As in the lambda calculus, arguments ( plural) is a shorthand
for the truth in Haskell: all functions in Haskell take one argu-
ment and return one result. Other programming languages,
if you have any experience with them, typically allow you to
define functions that can take multiple arguments. There is
no support for this built into Haskell. Instead there are syn-
tactic conveniences that construct curried functions by default.
Currying refers to the nesting of multiple functions, each ac-
cepting one argument and returning one result, to allow the
illusion of multiple-parameter functions.
The arrows we’ve seen in type signatures denote the func-
tion type. We looked at the datatype definition earlier, but
let’s review:
data(-&gt;) a b
In order to have a function, you must have one input, the 𝑎,
to apply the function to, and you’ll get one result, the 𝑏, back.
Each arrow in a type signature represents one argument and
one result, with the final type being the final result. If you
are constructing a function that requires multiple parameters,
then the 𝑏can be another function (the 𝑎can be another func-
tion as well). In that case, just like in lambda abstractions that
have multiple heads, they are nested.</p>
<p>CHAPTER 5. TYPES 193
Let’s break this down by looking at the type signature for
addition, a function that needs multiple inputs:
(+)::Numa=&gt;a-&gt;a-&gt;a
-- | 1 |
(+)::Numa=&gt;a-&gt;a-&gt;a
-- | 2 |
(+)::Numa=&gt;a-&gt;a-&gt;a
-- [3]
1.Here is the typeclass constraint saying that 𝑎must have
an instance of Num. Addition is defined in the Numtypeclass.
2.The boundaries of 2 demarcate what you might call the
two parameters to the function (+), but all functions in
Haskell take one argument and return one result. This is
because functions in Haskell are nested like Matryoshka
dolls in order to accept “multiple” arguments. The way
the(-&gt;)type constructor for functions works means a -&gt;
a -&gt; a represents successive function applications, each
taking one argument and returning one result. The dif-
ference is that the function at the outermost layer is re-
turning another function that accepts the next argument.
This is called currying.</p>
<p>CHAPTER 5. TYPES 194
3.This is the result type for this function. It will be a number
of the same type as the two inputs.
The way the type constructor for functions, (-&gt;), is defined
makes currying the default in Haskell. This is because it is an
infix operator and right associative. Because it associates to
the right, types are implicitly parenthesized like so:
f::a-&gt;a-&gt;a
-- associates to
f::a-&gt;(a-&gt;a)
and
map::(a-&gt;b)-&gt;[a]-&gt;[b]
-- associates into
map::(a-&gt;b)-&gt;([a]-&gt;[b])
Let’s see if we can unpack the notion of a right-associating
infix operator giving us curried functions. The association
here, or grouping into parentheses, is not to control prece-
dence or order of evaluation; it only serves to group the pa-
rameters into argument and result, since there can only be</p>
<p>CHAPTER 5. TYPES 195
one argument and one result per arrow. Since all the arrows
have the same precedence, the associativity does not change
the precedence or order of evaluation.
Remember, when we have a lambda expression that appears
to have two parameters, they are nested lambdas. Applying the
expression to one argument returns a function that awaits ap-
plication to a second argument. After you apply it to a second
argument, you have a final result. You can nest more lambdas
than two, of course, but the process is the same: one argument,
one result, even though that result may be a function awaiting
application to another argument.
The type constructor for functions and the types we see
above are the same thing, but written in Haskell. When there
are “two arguments” in Haskell, we apply our function to an
argument, just like when we apply a lambda expression to an
argument, and then return a result that is a function and needs
to be applied to a second argument.
Explicit parenthesization, as when an input parameter is
itself a function (such as in map, above), may be used to indicate
order of evaluation, but the implicit associativity of the func-
tion type does not mean the inner or final set of parentheses,
i.e., the result type, evaluates first. Application is evaluation;
in other words, the only way to evaluate anything is by apply-
ing functions, and function application is leftassociative. So,
the leftmost, or outermost, arguments will be evaluated first,
assuming anything gets evaluated (since Haskell is nonstrict,</p>
<p>CHAPTER 5. TYPES 196
you can’t assume that anything will be evaluated, but this will
be more clear later).
Partial application
Currying may be interesting, but many people wonder what
the practical eﬀect or value of currying is. We’ll look now at
a strategy called partial application to see what currying does
for us. It’s something we’ll explore more as we go through the
book as well.
We use the double colon to assign a type. Making the type
concrete will eliminate the typeclass constraint:
addStuff ::Integer -&gt;Integer -&gt;Integer
addStuff a b=a+b+5
So,addStuff appears to take two Integer arguments and re-
turn an Integer result. But after loading that in GHCi we see
that it is taking one argument and returning a function that
takes one argument and returns one result:
Prelude&gt; :t addStuff
addStuff :: Integer -&gt; Integer -&gt; Integer
Prelude&gt; let addTen = addStuff 5
Prelude&gt; :t addTen
addTen :: Integer -&gt; Integer
Prelude&gt; let fifteen = addTen 5</p>
<p>CHAPTER 5. TYPES 197
Prelude&gt; fifteen
15
Prelude&gt; addTen 15
25
Prelude&gt; addStuff 5 5
15
Herefifteen is equal to addStuff 5 5 , because addTen is equal
toaddStuff 5 . The ability to apply only some of a function’s ar-
guments is called partial application. This lets us reuse addStuff
and create a new function from it with one of the arguments
applied.
If we recall that (-&gt;)is a type constructor and associates to
the right, this becomes more clear:
addStuff ::Integer -&gt;Integer -&gt;Integer
-- with explicit parenthesization
addStuff ::Integer -&gt;(Integer -&gt;Integer)
Applying addStuff to oneInteger argument gave us the func-
tionaddTen , which is the return function of addStuff . Applying
addTen to anInteger argument gives us a return value, so the
type of fifteen isInteger — no more function arrows.
Let’s check our understanding with a function that isn’t
commutative:</p>
<p>CHAPTER 5. TYPES 198
subtractStuff ::Integer
-&gt;Integer
-&gt;Integer
subtractStuff x y=x-y-10
subtractOne =subtractStuff 1
Prelude&gt; :t subtractOne
subtractOne :: Integer -&gt; Integer
Prelude&gt; let result = subtractOne 11
Prelude&gt; result
-20
Why did we get this result? Because of the order in which
we applied arguments, result is equal to 1 - 11 - 10 .
Manual currying and uncurrying
Haskell is curried by default, but you can uncurry functions.
Uncurrying means un-nesting the functions and replacing the
two functions with a tuple of two values (these would be the two
values you want to use as arguments). If you uncurry (+), the
type changes from Num a =&gt; a -&gt; a -&gt; a toNum a =&gt; (a, a) -&gt; a
which better fits the description “takes two arguments, returns
one result” than curried functions. Some older functional
languages default to using a product type like tuples to express
multiple arguments.</p>
<p>CHAPTER 5. TYPES 199
•Uncurried functions: One function, many arguments
•Curried functions: Many functions, one argument apiece
You can also desugar the automatic currying yourself, by
nesting the arguments with lambdas, though there’s rarely a
reason to do so.
We’ll use anonymous lambda syntax here to show you some
examples of uncurrying. You may want to review anonymous
lambda syntax or try comparing these functions directly and
thinking of the backslash as a lambda:
indexanonymous function ! syntax
nonsense ::Bool-&gt;Integer
nonsense True=805
nonsense False=31337
curriedFunction ::Integer
-&gt;Bool
-&gt;Integer
curriedFunction i b=
i+(nonsense b)
uncurriedFunction ::(Integer,Bool)
-&gt;Integer
uncurriedFunction (i, b)=
i+(nonsense b)</p>
<p>CHAPTER 5. TYPES 200
anonymous ::Integer -&gt;Bool-&gt;Integer
anonymous =\i b-&gt;i+(nonsense b)
anonNested ::Integer
-&gt;Bool
-&gt;Integer
anonNested =
\i-&gt;\b-&gt;i+(nonsense b)
Then when we test the functions from the REPL:
Prelude&gt; curriedFunction 10 False
31347
Prelude&gt; anonymous 10 False
31347
Prelude&gt; anonNested 10 False
31347
They are all the same function, all giving the same results.
InanonNested , we manually nested the anonymous lambdas to
get a function that was semantically identical to curriedFunction
but didn’t leverage the automatic currying. This means func-
tions that seemto accept multiple arguments such as with a -&gt;
a -&gt; a -&gt; a arehigher-order functions : they yield more function
values as each argument is applied until there are no more (-&gt;)
type constructors and it terminates in a non-function value.</p>
<p>CHAPTER 5. TYPES 201
Currying and uncurrying existing functions
It turns out, we can curry and uncurry functions with multiple
parameters generically without writing new code for each one.
Consider the following example for currying:
Prelude&gt; let curry f a b = f (a, b)
Prelude&gt; :t curry
curry :: ((t1, t2) -&gt; t) -&gt; t1 -&gt; t2 -&gt; t
Prelude&gt; :t fst
fst :: (a, b) -&gt; a
Prelude&gt; :t curry fst
curry fst :: t -&gt; b -&gt; t
Prelude&gt; fst (1, 2)
1
Prelude&gt; curry fst 1 2
1
Then for uncurrying:
Prelude&gt; let uncurry f (a, b) = f a b
Prelude&gt; :t uncurry
uncurry :: (t1 -&gt; t2 -&gt; t) -&gt; (t1, t2) -&gt; t
Prelude&gt; :t (+)
(+) :: Num a =&gt; a -&gt; a -&gt; a
Prelude&gt; (+) 1 2
3</p>
<p>CHAPTER 5. TYPES 202
Prelude&gt; uncurry (+) (1, 2)
3
Currying and uncurrying functions of three or more argu-
ments automatically is quite possible, but trickier. We’ll leave
that be, but investigate on your own if you like.
Sectioning
We mentioned sectioning in Chapter 2, and now that we’ve
talked a bit more about currying and partial application, it
may be more clear what’s happening there. The term section-
ingspecifically refers to partial application of infix operators,
which has a special syntax and allows you to choose whether
the argument you’re partially applying the operator to is the
first or second argument:
Prelude&gt; let x = 5
Prelude&gt; let y = (2^)
Prelude&gt; let z = (^2)
Prelude&gt; y x
32
Prelude&gt; z x
25
With commutative functions such as addition, the argu-
ment order does not matter. We will usually section addition</p>
<p>CHAPTER 5. TYPES 203
as, for example, (+3), but when we start using partially applied
functions a lot with maps and folds and so forth, you’ll be able
to see the diﬀerence that the argument order can make with
noncommutative operators.
This does not only work with arithmetic, though:
Prelude&gt; let celebrate = (++ &quot; woot!&quot;)
Prelude&gt; celebrate &quot;naptime&quot;
&quot;naptime woot!&quot;
Prelude&gt; celebrate &quot;dogs&quot;
&quot;dogs woot!&quot;
You can also use the syntax with functions that are normally
prefix if you use the backticks to make them infix (note the
..is a shorthand for constructing a list of all the elements
between the first and last values given — go ahead and play
with this in your REPL):
Prelude&gt; elem 9 [1..10]
True
Prelude&gt; 9 <code>elem</code> [1..10]
True
Prelude&gt; let c = (<code>elem</code> [1..10])
Prelude&gt; c 9
True
Prelude&gt; c 25
False</p>
<p>CHAPTER 5. TYPES 204
If you partially applied elemin its usual prefix form, then
the argument you apply it to would necessarily be the first
argument:
Prelude&gt; let hasTen = elem 10
Prelude&gt; hasTen [1..9]
False
Prelude&gt; hasTen [5..15]
True
Partial application is common enough in Haskell that, over
time, you’ll develop an intuition for it. The sectioning syntax
exists to allow some freedom in which argument of a binary
operator you apply the function to.
Exercises: Type Arguments
Given a function and its type, tell us what type results from
applying some or all of the arguments.
You can check your work in the REPL like this (using the
first question as an example):
Prelude&gt; let f :: a -&gt; a -&gt; a -&gt; a; f = undefined
Prelude&gt; let x :: Char; x = undefined
Prelude&gt; :t f x</p>
<p>CHAPTER 5. TYPES 205
It turns out that you can check the types of things that aren’t
implemented yet, so long as you give GHCi an undefined to
bind the signature to.
1.If the type of fisa -&gt; a -&gt; a -&gt; a , and the type of 𝑥isChar
then the type of f xis
a)Char -&gt; Char -&gt; Char
b)x -&gt; x -&gt; x -&gt; x
c)a -&gt; a -&gt; a
d)a -&gt; a -&gt; a -&gt; Char
2.If the type of gisa -&gt; b -&gt; c -&gt; b , then the type of
g 0 'c' &quot;woot&quot; is
a)String
b)Char -&gt; String
c)Int
d)Char
3.If the type of his(Num a, Num b) =&gt; a -&gt; b -&gt; b , then the
type of
h 1.0 2 is:
a)Double
b)Integer</p>
<p>CHAPTER 5. TYPES 206
c)Integral b =&gt; b
d)Num b =&gt; b
Note that because the type variables 𝑎and𝑏are diﬀerent,
the compiler mustassume that the types could be diﬀerent.
4.If the type of his(Num a, Num b) =&gt; a -&gt; b -&gt; b , then the
type of
h 1 (5.5 :: Double) is
a)Integer
b)Fractional b =&gt; b
c)Double
d)Num b =&gt; b
5.If the type of jackal is(Ord a, Eq b) =&gt; a -&gt; b -&gt; a , then
the type of
jackal &quot;keyboard&quot; &quot;has the word jackal in it&quot;
a)[Char]
b)Eq b =&gt; b
c)b -&gt; [Char]
d)b
e)Eq b =&gt; b -&gt; [Char]</p>
<p>CHAPTER 5. TYPES 207
6.If the type of jackal is(Ord a, Eq b) =&gt; a -&gt; b -&gt; a , then
the type of
jackal &quot;keyboard&quot;
a)b
b)Eq b =&gt; b
c)[Char]
d)b -&gt; [Char]
e)Eq b =&gt; b -&gt; [Char]
7.If the type of kessel is(Ord a, Num b) =&gt; a -&gt; b -&gt; a , then
the type of
kessel 1 2 is
a)Integer
b)Int
c)a
d)(Num a, Ord a) =&gt; a
e)Ord a =&gt; a
f)Num a =&gt; a
8.If the type of kessel is(Ord a, Num b) =&gt; a -&gt; b -&gt; a , then
the type of
kessel 1 (2 :: Integer) is
a)(Num a, Ord a) =&gt; a</p>
<p>CHAPTER 5. TYPES 208
b)Int
c)a
d)Num a =&gt; a
e)Ord a =&gt; a
f)Integer
9.If the type of kessel is(Ord a, Num b) =&gt; a -&gt; b -&gt; a , then
the type of
kessel (1 :: Integer) 2 is
a)Num a =&gt; a
b)Ord a =&gt; a
c)Integer
d)(Num a, Ord a) =&gt; a
e)a
5.5 Polymorphism
Polymorph is a word of relatively recent provenance. It was
invented in the early 19th century from the Greek words poly
for “many” and morph for “form”. The -icsuffix in polymorphic
means “made of.” So, ‘polymorphic’ means “made of many
forms.” In programming, this is understood to be in contrast
withmonomorphic , “made of one form.”</p>
<p>CHAPTER 5. TYPES 209
Polymorphic type variables give us the ability to implement
expressions that can accept arguments and return results of
diﬀerent types without having to write variations on the same
expression for each type. It would be inefficient if you were
doing arithmetic and had to write the same code over and
over for diﬀerent numeric types. The good news is the nu-
merical functions that come with your GHC installation and
thePrelude are polymorphic by default. Broadly speaking,
type signatures may have three kinds of types: concrete, con-
strained polymorphic, or parametrically polymorphic.
In Haskell, polymorphism divides into two categories: para-
metric polymorphism andconstrained polymorphism . If you’ve
encountered polymorphism before, it was probably a form
of constrained, often called ad-hoc, polymorphism. Ad-hoc
polymorphism2in Haskell is implemented with typeclasses.
Parametric polymorphism is broader than ad-hoc polymor-
phism. Parametric polymorphism refers to type variables, or
parameters , that are fully polymorphic. When unconstrained
by a typeclass, their final, concrete type could be anything.
Constrained polymorphism, on the other hand, puts typeclass
constraints on the variable, decreasing the number of concrete
types it could be, but increasing what you can do with it by
defining and bringing into scope a set of operations.
2Wadler’s paper on making Ad-hoc polymorphism less ad-hoc http://people.csail.
mit.edu/dnj/teaching/6898/papers/wadler88.pdf</p>
<p>CHAPTER 5. TYPES 210
Recall that when you see a lowercase name in a type sig-
nature, it is a type variable and polymorphic (like 𝑎,𝑡, etc). If
the type is capitalized, it is a specific, concrete type such as Int,
Bool, etc.
Let’s consider a parametrically polymorphic function: iden-
tity. Theidfunction comes with the Prelude and is called the
identity function because it is the identity for any value in
our language. In the next example, the type variable 𝑎is para-
metrically polymorphic and not constrained by a typeclass.
Passing any value to idwill return the same value:
id::a-&gt;a
This type says: for all 𝑎, get an argument of some type 𝑎
and return a value of the same type 𝑎.
This is the maximally polymorphic signature for id. It
allows this function to work with any type of data:
Prelude&gt; id 1
1
Prelude&gt; id &quot;blah&quot;
&quot;blah&quot;
Prelude&gt; let inc = (+1)
Prelude&gt; inc 2
3
Prelude&gt; (id inc) 2
3</p>
<p>CHAPTER 5. TYPES 211
Based on the type of id, we are guaranteed this behavior —
it cannot do anything else! The 𝑎in the type signature cannot
change because the type variable gets fixed to a concrete type
throughout the entire type signature (a == a). If one applies
idto a value of type Int, the𝑎is fixed to type Int. By default,
type variables are resolved at the left-most part of the type
signature and are fixed once sufficient information to bind
them to a concrete type is available.
The arguments in parametrically polymorphic functions,
likeid, could be anything, any type or typeclass, so the terms
of the function are more restricted because there are no meth-
ods or information attached to them. With the type id :: a
-&gt; a, it can do nothing other than return 𝑎because there is
no information or method attached to its parameter at all —
nothing can be done with𝑎. On the other hand, a function like
negate, with a similar-appearing type signature of Num a =&gt; a
-&gt; aconstrains the 𝑎variable as an instance of the Numtypeclass.
Now𝑎has fewer concrete types it could be, but there is a set of
methods you can use, a set of things that can be done with 𝑎.
If a variable represents a set of possible values, then a type
variable represents a set of possible types. When there is no
typeclass constraint, the set of possible types a variable could
represent is eﬀectively unlimited. Typeclass constraints limit
the set of potential types (and, thus, potential values) while
also passing along the common functions that can be used
with those values.</p>
<p>CHAPTER 5. TYPES 212
Concrete types have even more flexibility in terms of com-
putation. This has to do with the additive nature of typeclasses.
For example, an Intis only an Int, but it can make use of the
methods of the NumandIntegral typeclasses because it has in-
stances of both. We can describe Numas asuperclass of several
other numeric typeclasses that all inherit operations from Num.
In sum, if a variable could be anything , then there’s little
that can be done to it because it has no methods. If it can
besometypes (say, a type that has an instance of Num), then
it has some methods. If it is a concrete type, you lose the
type flexibility but, due to the additive nature of typeclass
inheritance, gain more potential methods. It’s important to
note that this inheritance extends downward from a superclass,
such as Num, to subclasses, such as Integral and then Int, but not
the other way around. That is, if something has an instance
ofNumbut not an instance of Integral , it can’t implement the
methods of the Integral typeclass. A subclass cannot override
the methods of its superclass.
A function is polymorphic when its type signature has vari-
ables that can represent more than one type. That is, its param-
eters are polymorphic. Parametric polymorphism refers to
fully polymorphic (unconstrained by a typeclass) parameters.
Parametricity is the property we get from having parametric
polymorphism. Parametricity means that the behavior of a
function with respect to the types of its (parametrically poly-</p>
<p>CHAPTER 5. TYPES 213
morphic) arguments is uniform. The behavior can notchange
just because it was applied to an argument of a diﬀerent type.
Exercises: Parametricity
All you can do with a parametrically polymorphic value is pass
or not pass it to some other expression. Prove that to yourself
with these small demonstrations.
1.Given the type a -&gt; a, which is the type for id, attempt
to make a function that terminates successfully that does
something other than returning the same value. This is
impossible, but you should try it anyway.
2.We can get a more comfortable appreciation of para-
metricity by looking at a -&gt; a -&gt; a . This hypothetical
function a -&gt; a -&gt; a has two–and only two–implementa-
tions. Write both possible versions of a -&gt; a -&gt; a . After
doing so, try to violate the constraints of parametrically
polymorphic values we outlined above.
3.Implement a -&gt; b -&gt; b . How many implementations can
it have? Does the behavior change when the types of 𝑎
and𝑏change?</p>
<p>CHAPTER 5. TYPES 214
Polymorphic constants
We’ve seen that there are several types of numbers in Haskell
and that there are restrictions on using diﬀerent types of num-
bers in diﬀerent functions. But intuitively we see it would be
odd if we could not do arithmetic along the lines of -10 + 6.3 .
Well, let’s try it:
Prelude&gt; (-10) + 6.3
-3.7
That works just fine. Why? Let’s look at the types and see if
we can find out:
Prelude&gt; :t (-10) + 6.3
(-10) + 6.3 :: Fractional a =&gt; a
Prelude&gt; :t (-10)
(-10) :: Num a =&gt; a
Numeric literals like (-10) and 6.3 are polymorphic and stay
so until given a more specific type. The Num a =&gt; orFractional
a =&gt;is a typeclass constraint and the 𝑎is the type variable
in scope. In the type for the entire equation, we see that the
compiler inferred that it was working with Fractional numbers.
It had to, to accommodate the fractional number 6.3. Fine,
but what about (-10)? We see that the type of (-10) is given
maximum polymorphism by only being an instance of the</p>
<p>CHAPTER 5. TYPES 215
Numtypeclass, which could be any type of number. We call
this a polymorphic constant; (-10) is not a variable, of course,
but the type that it instantiates could be any numeric type, so
its underlying representation is polymorphic. It will have to
resolve into a concrete type at some point in order to evaluate.
We can force the compiler to be more specific about the
types of numbers by declaring the type:
Prelude&gt; let x = 5 + 5
Prelude&gt; :t x
x :: Num a =&gt; a
Prelude&gt; let x = 5 + 5 :: Int
Prelude&gt; :t x
x :: Int
In the first example, we did not specify a type for the num-
bers, so the type signature defaulted to the broadest interpre-
tation, but in the second version, we told the compiler to use
theInttype.
Working around constraints
Previously, we’ve looked at a function called length that takes
a list and counts the number of members and returns that
number as an Intvalue. We saw in the last chapter that because
Intis not a Fractional number, this function won’t work:</p>
<p>CHAPTER 5. TYPES 216
Prelude&gt; 6 / length [1, 2, 3]
No instance for (Fractional Int) arising
from a use of ‘/’
In the expression: 6 / length [1, 2, 3]
In an equation for ‘it’: it = 6 / length [1, 2, 3]
Heretheproblemis length isn’tpolymorphicenough. Fractional
includes several types of numbers, but Intisn’t one of them,
and that’s all length can return. Haskell does oﬀer ways to
work around this type of conflict, though. In this case, we
will use a function called fromIntegral that takes an integral
value and forces it to implement the Numtypeclass, rendering
it polymorphic. Here’s what the type signature looks like:
Prelude&gt; :type fromIntegral
fromIntegral :: (Num b, Integral a) =&gt; a -&gt; b
So, it takes a value, 𝑎, of anIntegral type and returns it as
a value, 𝑏, of any Numtype. Let’s see how that works with our
fractional division problem:
Prelude&gt; 6 / fromIntegral (length [1, 2, 3])
2.0
And now all is right with the world once again.</p>
<p>CHAPTER 5. TYPES 217
5.6 Type inference
Haskell does not obligate us to assert a type for every expres-
sion or value in our programs because it has type inference .
Type inference is an algorithm for determining the types of
expressions. Haskell’s type inference is built on an extended
version of the Damas-Hindley-Milner type system.
Haskell will infer the most generally applicable (polymor-
phic) type that is still correct. Essentially, the compiler starts
from the values whose types it knows and then works out the
types of the other values. As you mature as a Haskell pro-
grammer, you’ll find this is principally useful for when you’re
still figuring out new code rather than for code that is “done”.
Once your program is “done,” you will certainly know the
types of all the functions, and it’s considered good practice
to explicitly declare them. Remember when we suggested
that a good type system was like a pleasant conversation with
a colleague? Think of type inference as a helpful colleague
working through a problem with you.
For example, we can write idourselves:
Prelude&gt; let ourId x = x
Prelude&gt; :t ourId
ourId :: t -&gt; t
Prelude&gt; ourId 1
1</p>
<p>CHAPTER 5. TYPES 218
Prelude&gt; ourId &quot;blah&quot;
&quot;blah&quot;
Here we let GHCi infer the type of ourIditself. Due to alpha
equivalence, the diﬀerence in letters ( 𝑡here versus 𝑎above)
makes no diﬀerence. Type variables have no meaning outside
of the type signatures where they are bound.
For this function, we again ask the compiler to infer the
type:
Prelude&gt; let myGreet x = x ++ &quot; Julie&quot;
Prelude&gt; myGreet &quot;hello&quot;
&quot;hello Julie&quot;
Prelude&gt; :type myGreet
myGreet :: [Char] -&gt; [Char]
The compiler knows the function (++)and has one value to
work with already that it knows is a String. It doesn’t have to
work very hard to infer a type signature from that information.
If, however, we take out the string value and replace it with
another variable, see what happens:
Prelude&gt; let myGreet x y = x ++ y
Prelude&gt; :type myGreet
myGreet :: [a] -&gt; [a] -&gt; [a]
We’re back to a polymorphic type signature, the same sig-
nature for (++)itself, because the compiler has no information</p>
<p>CHAPTER 5. TYPES 219
by which to infer the types for any of those variables (other
than that they are lists of some sort).
Let’s see type inference at work. Open your editor of choice
and enter the following snippet:
-- typeInference1.hs
moduleTypeInference1 where
f::Numa=&gt;a-&gt;a-&gt;a
fx y=x+y+3
Then load the code into GHCi to experiment:
Prelude&gt; :l typeInference1.hs
[1 of 1] Compiling TypeInference1
Ok, modules loaded: TypeInference1.
Prelude&gt; f 1 2
6
Prelude&gt; :t f
f :: Num a =&gt; a -&gt; a -&gt; a
Prelude&gt; :t f 1
f 1 :: Num a =&gt; a -&gt; a
Because the numeric literals in Haskell have the (typeclass
constrained) polymorphic type Num a =&gt; a , we don’t get a more
specific type when applying 𝑓to 1.</p>
<p>CHAPTER 5. TYPES 220
Look at what happens when we elide the explicit type sig-
nature for 𝑓:
-- typeInference2.hs
moduleTypeInference2 where
fx y=x+y+3
No type signature for 𝑓, so does it compile? Does it work?
Prelude&gt; :l typeInference2.hs
[1 of 1] Compiling TypeInference2
Ok, modules loaded: TypeInference2.
Prelude&gt; :t f
f :: Num a =&gt; a -&gt; a -&gt; a
Prelude&gt; f 1 2
6
Nothing changes. In certain cases there might be a change,
usually when you are using typeclasses in a way that doesn’t
make it clear which type you mean unless you assert one.
Exercises: Apply Yourself
Look at these pairs of functions. One function is unapplied,
so the compiler will infer maximally polymorphic type. The
second function has been applied to a value, so the inferred</p>
<p>CHAPTER 5. TYPES 221
type signature may have become concrete, or at least less
polymorphic. Figure out how the type would change and why,
make a note of what you think the new inferred type would
be and then check your work in GHCi.
1.-- Type signature of general function
(++)::[a]-&gt;[a]-&gt;[a]
-- How might that change when we apply
-- it to the following value?
myConcat x=x++&quot; yo&quot;
2.-- General function
(*)::Numa=&gt;a-&gt;a-&gt;a
-- Applied to a value
myMultx=(x/3)*5
3.take::Int-&gt;[a]-&gt;[a]
myTakex=take x&quot;hey you&quot;
4.(&gt;)::Orda=&gt;a-&gt;a-&gt;Bool
myComx=x&gt;(length [ 1..10])</p>
<p>CHAPTER 5. TYPES 222
5.(&lt;)::Orda=&gt;a-&gt;a-&gt;Bool
myAlphx=x&lt;'z'
5.7 Asserting types for declarations
Most of the time, we want to declare our types, rather than
relying on type inference. Adding type signatures to your code
can provide guidance to you as you write your functions. It
can also help the compiler give you information about where
your code is going wrong. As programs become longer and
more complex, type signatures become even more important,
as they help you or other programmers trying to use your
code read it and figure out what it’s supposed to do. This
section will look at how to declare types. We will start with
some trivial examples.
You may remember the triple function we’ve seen before.
If we allow the compiler to infer the type, we end up with this:
Prelude&gt; let triple x = x * 3
Prelude&gt; :type triple
triple :: Num a =&gt; a -&gt; a
Here the triple function was made from the (<em>)function
which has type (</em>) :: Num a =&gt; a -&gt; a -&gt; a , but we have al-
ready applied one of the arguments, which is the 3, so there is</p>
<p>CHAPTER 5. TYPES 223
one less parameter in this type signature. It is still polymor-
phic because it can’t tell what type 3 is yet. If, however, we
want to ensure that our inputs and result may only be integers,
this is how we declare that:
Prelude&gt; let triple x = x * 3 :: Integer
Prelude&gt; :t triple
triple :: Integer -&gt; Integer
Note the typeclass constraint is gone because Integer imple-
mentsNum, so that constraint is redundant.
Here’s another example of a type declaration for our triple
function; this one is more like what you would see in a source
file:
-- type declaration
triple::Integer -&gt;Integer
-- function declaration
triplex=x*3
This is how most Haskell code you look at will be laid out,
with separate top-level declarations for types and functions.
Such top-level declarations are in scope throughout the mod-
ule.</p>
<p>CHAPTER 5. TYPES 224
It is possible, though uncommon, to declare types locally
withletandwhere. Here’sanexampleofassigningatypewithin
awhereclause:
triplex=tripleItYo x
wheretripleItYo ::Integer -&gt;Integer
tripleItYo y =y*3
We don’t have to assert the type of triple :
Prelude&gt; :t triple
triple :: Integer -&gt; Integer
The assertion in the whereclause narrowed our type down
fromNum a =&gt; a -&gt; a toInteger -&gt; Integer . GHC will pick up
and propagate type information for inference from appli-
cations of functions, sub-expressions, definitions — almost
anywhere. The type inference is strong with this one.
Thereareconstraints on our ability to declare types. For
example, if we try to make the (+)function return a String , we
get an error message:
Prelude&gt; let x = 5 + 5 :: String
No instance for (Num String) arising from a use of ‘+’
In the expression: 5 + 5 :: String
In an equation for ‘x’: x = 5 + 5 :: String</p>
<p>CHAPTER 5. TYPES 225
This function cannot accept arguments of type String. In
this case, it’s overdetermined, both because the (+)function
is limited to types implementing the Numtypeclass and also
because we’ve already passed it two numeric literals as values.
The numeric literals could be any of several numeric types
under the hood, but they can’t be String because String does
not implement the Numtypeclass.
5.8 Chapter Exercises
Welcome to another round of “Knowing is not enough; we
must apply.”
Multiple choice
1.A value of type [a]is
a)a list of alphabetic characters
b)a list of lists
c)a list whose elements are all of some type 𝑎
d)a list whose elements are all of diﬀerent types
2.A function of type [[a]] -&gt; [a] could
a)take a list of strings as an argument
b)transform a character into a string</p>
<p>CHAPTER 5. TYPES 226
c)transform a string into a list of strings
d)take two arguments
3.A function of type [a] -&gt; Int -&gt; a
a)takes one argument
b)returns one element of type 𝑎from a list
c)must return an Intvalue
d)is completely fictional
4.A function of type (a, b) -&gt; a
a)takes a list argument and returns a Charvalue
b)has zero arguments
c)takes a tuple argument and returns the first value
d)requires that 𝑎and𝑏be of diﬀerent types
Determine the type
For the following functions, determine the type of the spec-
ified value. We suggest you type them into a file and load
the contents of the file in GHCi. In all likelihood, it initially
will not have the polymorphic types you might expect due to
themonomorphism restriction . That means that top-level dec-
larations by default will have a concrete type if any can be
determined. You can fix this by setting up your file like so:</p>
<p>CHAPTER 5. TYPES 227
{-# LANGUAGE NoMonomorphismRestriction #-}
moduleDetermineTheType where
-- simple example
example =1
If you had not included the NoMonomorphismRestriction exten-
sion,example would have had the type Integer instead of Num a
=&gt; a. Do your best to determine the mostpolymorphic type
an expression could have in the following exercises.
1.All function applications return a value. Determine the
value returned by these function applications and the type
of that value.
a)(*9)6
b)head[(0,&quot;doge&quot;),(1,&quot;kitteh&quot; )]
c)head[(0::Integer ,&quot;doge&quot;),(1,&quot;kitteh&quot; )]
d)ifFalsethenTrueelseFalse
e)length[1,2,3,4,5]
f)(length [ 1,2,3,4])&gt;(length &quot;TACOCAT&quot; )
2.Given</p>
<p>CHAPTER 5. TYPES 228
x=5
y=x+5
w=y<em>10
What is the type of w?
3.Given
x=5
y=x+5
zy=y</em>10
What is the type of z?
4.Given
x=5
y=x+5
f=4/y
What is the type of f?
5.Given
x=&quot;Julie&quot;
y=&quot; &lt;3 &quot;
z=&quot;Haskell&quot;
f=x++y++z
What is the type of f?</p>
<p>CHAPTER 5. TYPES 229
Does it compile?
For each set of expressions, figure out which expression, if any,
causes the compiler to squawk at you (n.b. we do not mean
literal squawking) and why. Fix it if you can.
1.bigNum=(^)5$10
wahoo=bigNum$10
2.x=print
y=print&quot;woohoo!&quot;
z=x&quot;hello world&quot;
3.a=(+)
b=5
c=b10
d=c200
4.a=12+b
b=10000*c
Type variable or specific type constructor?
1.You will be shown a type declaration, and you should
categorize each type. The choices are a fully polymorphic
type variable, constrained polymorphic type variable, or
concrete type constructor.</p>
<p>CHAPTER 5. TYPES 230
f::Numa=&gt;a-&gt;b-&gt;Int-&gt;Int
-- [0] [1] [2] [3]
Here, the answer would be: constrained polymorphic
(Num) ([0]), fully polymorphic ([1]), and concrete ([2] and
[3]).
2.Categorize each component of the type signature as de-
scribed in the previous example.
f::zed-&gt;Zed-&gt;Blah
3.Categorize each component of the type signature
f::Enumb=&gt;a-&gt;b-&gt;C
4.Categorize each component of the type signature
f::f-&gt;g-&gt;C
Write a type signature
For the following expressions, please add a type signature. You
should be able to rely on GHCi type inference to check your
work, although you might not have precisely the same answer
as GHCi gives (due to polymorphism, etc).
1.While we haven’t fully explained this syntax yet, you’ve
seen it in Chapter 2 and as a solution to an exercise in</p>
<p>CHAPTER 5. TYPES 231
Chapter 4. This syntax is a way of destructuring a single
element of a list by pattern matching.
functionH ::
functionH (x:_)=x
2.functionC ::
functionC x y=
if(x&gt;y)thenTrueelseFalse
3.functionS ::
functionS (x, y)=y
Given a type, write the function
You will be shown a type and a function that needs to be writ-
ten. Use the information the type provides to determine what
the function should do. We’ll also tell you how many ways
there are to write the function. Syntactically diﬀerent but
semantically equivalent implementations are not counted as
being diﬀerent. For example, writing a function one way then
rewriting the semantically identical function but using anony-
mous lambda syntax does not count as two implementations.
To make things a little easier, we’ll demonstrate how to solve
this kind of exercise. Given:</p>
<p>CHAPTER 5. TYPES 232
myFunc::(x-&gt;y)
-&gt;(y-&gt;z)
-&gt;c
-&gt;(a, x)
-&gt;(a, z)
myFuncxToY yToZ _(a, x)=undefined
Talking through the above, we have a function that takes
four arguments. The final result is a tuple with the type (a,
z). It turns out, the 𝑐argument is nowhere in our results and
there’s nothing to do with it, so we use the underscore to ignore
that. We named the two function arguments by their types
and pattern matched on the tuple argument. The only way to
get the second value of the tuple from the type 𝑥to the type 𝑧
is to use bothof the functions furnished to us. If we tried the
following:
myFuncxToY yToZ _(a, x)=
(a, (xToY x))
We would get a type error that it expected the type 𝑧but
the actual type was 𝑦. That’s because we’re on the right path,
but not quite done yet! Accordingly, the following should
typecheck:</p>
<p>CHAPTER 5. TYPES 233
myFunc::(x-&gt;y)
-&gt;(y-&gt;z)
-&gt;c
-&gt;(a, x)
-&gt;(a, z)
myFuncxToY yToZ _(a, x)=
(a, (yToZ (xToY x)))
1.There is only one function definition that typechecks and
doesn’t go into an infinite loop when you run it.
i::a-&gt;a
i=undefined
2.There is only one version that works.
c::a-&gt;b-&gt;a
c=undefined
3.Given alpha equivalence are c''andc(see above) the same
thing?
c''::b-&gt;a-&gt;b
c''= ?
4.Only one version that works.
c'::a-&gt;b-&gt;b
c'=undefined</p>
<p>CHAPTER 5. TYPES 234
5.There are multiple possibilities, at least two of which
you’ve seen in previous chapters.
r::[a]-&gt;[a]
r=undefined
6.Only one version that will typecheck.
co::(b-&gt;c)-&gt;(a-&gt;b)-&gt;a-&gt;c
co=undefined
7.One version will typecheck.
a::(a-&gt;c)-&gt;a-&gt;a
a=undefined
8.One version will typecheck.
a'::(a-&gt;b)-&gt;a-&gt;b
a'=undefined
Fix it
Won’t someone take pity on this poor broken code and fix it
up? Be sure to check carefully for things like capitalization,
parentheses, and indentation.
1.module sing where</p>
<p>CHAPTER 5. TYPES 235
fstString :: [Char] ++ [Char]
fstString x = x ++ &quot; in the rain&quot;
sndString :: [Char] -&gt; Char
sndString x = x ++ &quot; over the rainbow&quot;
sing = if (x &gt; y) then fstString x or sndString y
where x = &quot;Singin&quot;
x = &quot;Somewhere&quot;
2.Now that it’s fixed, make a minor change and make it sing
the other song. If you’re lucky, you’ll end up with both
songs stuck in your head!
3.-- arith3broken.hs
moduleArith3Broken where
main::IO()
Main= do
print1+2
putStrLn 10
print (negate -1)
print (( +)0blah)
whereblah=negate1</p>
<p>CHAPTER 5. TYPES 236
Type-Kwon-Do
The name is courtesy of Phillip Wright.3Thank you for the
idea!
The focus here is on manipulating terms in order to get the
types to fit. This sortof exercise is something you’ll encounter
in writing real Haskell code, so the practice will make it easier
to deal with when you get there. Practicing this will make you
better at writing ordinary code as well.
Weprovidethetypesandbottomedout(declaredas undefined )
terms.Bottom andundefined will be explained in more detail
later. The contents of the terms are irrelevant here. You’ll use
only the declarations provided and what the Prelude provides
by default unless otherwise specified. Your goal is to make the
???’d declaration pass the typechecker by modifying it alone.
Here’s a worked example for how we present these exercises
and how you are expected to solve them. Given the following:
3https://twitter.com/SixBitProxyWax</p>
<p>CHAPTER 5. TYPES 237
dataWoot
dataBlah
f::Woot-&gt;Blah
f=undefined
g::(Blah,Woot)-&gt;(Blah,Blah)
g= ???
Here it’s 𝑔that you’re supposed to implement; however,
you can’t evaluate anything. You’re to only use type-checking
and type-inference to validate your answers. Also note that
we’re using a trick for defining datatypes which can be named
in a type signature, but have no values. Here’s an example of
a valid solution:
g::(Blah,Woot)-&gt;(Blah,Blah)
g(b, w)=(b, f w)
The idea is to only fill in what we’ve marked with ???.
Not all terms will always be used in the intended solution for a
problem.</p>
<p>CHAPTER 5. TYPES 238
1.f::Int-&gt;String
f=undefined
g::String-&gt;Char
g=undefined
h::Int-&gt;Char
h= ???
2.dataA
dataB
dataC
q::A-&gt;B
q=undefined
w::B-&gt;C
w=undefined
e::A-&gt;C
e= ???</p>
<p>CHAPTER 5. TYPES 239
3.dataX
dataY
dataZ
xz::X-&gt;Z
xz=undefined
yz::Y-&gt;Z
yz=undefined
xform::(X,Y)-&gt;(Z,Z)
xform= ???
4.munge::(x-&gt;y)
-&gt;(y-&gt;(w, z))
-&gt;x
-&gt;w
munge= ???
5.9 Definitions
1.Polymorphism refers to type variables which may refer to
more than one concrete type. In Haskell, this will usually
manifest as parametric orad-hoc polymorphism. By hav-
ing a larger set of types, we intersect the commonalities</p>
<p>CHAPTER 5. TYPES 240
of them all to produce a smaller set of correct terms. This
makes it less likely we’ll write an incorrect program and
lets us reuse the code with other types.
2.Type inference is a faculty some programming languages,
most notably Haskell and ML, have to inferprincipal types
from terms without needing explicit type annotations.
There are, in some cases, terms in Haskell which can be
well-typed but which have no principal type. In those
cases, an explicit type annotation must be added.
With respect to Haskell, the principal type is the most
generic type which still typechecks. More generally, Prin-
cipal type is a property of the type system you’re interact-
ing with. Principal typing holds for that type system if a
type can be found for a term in an environment for which
all other types for that term are instances of the principal
type. Here are some examples:</p>
<p>CHAPTER 5. TYPES 241
-- Given the inferred types
a
Numa=&gt;a
Int
-- The principal type here is the
-- parametrically polymorphic 'a'.
-- Given these types
(Orda,Numa)=&gt;a
Integer
-- The principal type is
-- (Ord a, Num a) =&gt; a
3.Type variable is a way to refer to an unspecified type or
set of types in Haskell type signatures. Type variables
ordinarily will be equal to themselves throughout a type
signature. Let us consider some examples.
id::a-&gt;a
-- One type variable 'a' that occurs twice,
-- once as an argument, once as a result.
-- Parametrically polymorphic, could be
-- strictly anything</p>
<p>CHAPTER 5. TYPES 242
(+)::Numa=&gt;a-&gt;a-&gt;a
-- One type variable 'a', constrained
-- to needing an instance of Num. Two
-- arguments, one result.
-- All the same type.
4.Atypeclass is a means of expressing faculties or interfaces
that multiple datatypes may have in common. This en-
ables us to write code exclusively in terms of those com-
monalities without repeating yourself for each instance.
Just as one may sum values of type Int,Integer ,Float,
Double , andRational , we can avoid having diﬀerent (+),(*),
(-),negate , etc. functions for each by unifying them into
a single typeclass. Importantly, these can then be used
withalltypes that have a Numinstance. Thus, a typeclass
provides us a means to write code in terms of those oper-
ators and have our functions be compatible with all types
that have instances of that typeclass, whether they already
exist or are yet to be invented (by you, perhaps).
5.Parametricity is the property that holds in the presence of
parametric polymorphism. Parametricity states that the
behavior of a function will be uniform across all concrete
applications of the function. Parametricity4tells us that</p>
<p>CHAPTER 5. TYPES 243
the function:
id::a-&gt;a
Can be understood to have the same exact behavior for
every type in Haskell without us needing to see how it
was written. It is the same property that tells us:
const::a-&gt;b-&gt;a
constmustreturn the first value — parametricity and the
definition of the type requires it!
f::a-&gt;a-&gt;a
Here,𝑓can only return the first or second value, nothing
else, and it will always return one or the other consistently
without changing. If the function 𝑓made use of (+)or
(*), its type would necessarily be constrained by the type-
class Num and thus be an example of ad-hoc, rather than
parametric, polymorphism.
blahFunc ::b-&gt;String
blahFunc totally ignores its argument and is eﬀectively a
constant value of type String which requires a throw-away
argument for no reason.
4Examples are courtesy of the @parametricity twitter account.
https://twitter.com/parametricity</p>
<p>CHAPTER 5. TYPES 244
convList ::a-&gt;[a]
Unless the result is [], the resulting list has values that are
all the same value. The list will always be the same length.
6.Ad-hoc polymorphism (sometimes called “constrained poly-
morphism”) is polymorphism that applies one or more
typeclass constraints to what would’ve otherwise been a
parametrically polymorphic type variable. Here, rather
than representing a uniformity of behavior across all con-
crete applications, the purpose of ad-hoc polymorphism
istoallowthefunctionstohavediﬀerentbehaviorforeach
instance. This ad-hoc-ness is constrained by the types
in the typeclass that defines the methods and Haskell’s
requirement that typeclass instances be unique for a given
type. For any given combination of typeclass and a type,
such as OrdandBool, there must only exist one unique
instance in scope. This makes it considerably easier to
reason about typeclasses. See the example for a disam-
biguation.</p>
<p>CHAPTER 5. TYPES 245
(+)::Numa=&gt;a-&gt;a-&gt;a
-- the above function is leveraging
-- ad-hoc polymorphism via the
-- Num typeclass
c'::a-&gt;a-&gt;a
-- This function is not,
-- it's parametrically polymorphic in 'a'.
7.Amodule is the unit of organization that the Haskell pro-
gramming language uses to collect together declarations
of values, functions, data types, typeclasses, and typeclass
instances. Any time you use “import” in Haskell, you are
importing declarations from a module . Let us look at an
example from the chapter exercises:
{-# LANGUAGE NoMonomorphismRestriction #-}
moduleDetermineTheType where
-- ^ name of our module
Here we made our Haskell source file have a module and
we named it DetermineTheType . We included a directive to
the compiler to disable the monomorphism restriction</p>
<p>CHAPTER 5. TYPES 246
before we declared the module. Also consider the follow-
ing example using import :
importData.Aeson (encode)
-- ^ the module Data.Aeson
importDatabase.Persist
-- ^ the module Database.Persist
In the above example, we are importing the function
encode declared in the module Data.Aeson along with any
typeclass instances. With the module Database.Persist we
are importing everything it makes available.
5.10 Follow-up resources
1.Luis Damas; Robin Milner. Principal type-schemes for
functional programs
2.Christopher Strachey. Fundamental Concepts in Pro-
gramming Languages
Popular origin of the parametric/ad-hoc polymorphism
distinction.</p>
<p>Chapter 6
Typeclasses
A blank cheque kills
creativity.
Mokokoma Mokhonoana
247</p>
<p>CHAPTER 6. LESS AD-HOC POLYMORPHISM 248
6.1 Typeclasses
You may have realized that it is very difficult to talk about or
understand Haskell’s type system without also talking about
typeclasses. So far we’ve been focused on the way they interact
with type variables and numeric types, especially. This chapter
explains some important predefined typeclasses, only some
of which have to do with numbers, and provides more detail
about how typeclasses work more generally. In this chapter,
we will
•examine the typeclasses Eq,Num,Ord,Enum, andShow;
•learn about type-defaulting typeclasses and typeclass in-
heritance;
•look at some common but often implicit functions that
create side eﬀects.
6.2 What are typeclasses?
Typeclasses and types in Haskell are, in a sense, opposites.
Where a declaration of a type defines how that type in partic-
ular is created, a declaration of a typeclass defines how a set
of types are consumed or used in computations. This tension
is related to the expression problem which is about defining
code in terms of how data is created or processed. As Philip</p>
<p>CHAPTER 6. LESS AD-HOC POLYMORPHISM 249
Wadler put it, “The goal is to define a datatype by cases, where
one can add new cases to the datatype and new functions over
the datatype, without recompiling existing code, and while
retaining static type safety (e.g., no casts).”1If you know other
programming languages with a similar concept, it may help to
think of typeclasses as being like interfaces to data that can work
across multiple datatypes. The latter facility is why typeclasses
are a means of ad hoc polymorphism — ad hoc because type-
class code is dispatched by type, something we will explain
later in this chapter. We will continue calling it constrained
polymorphism, though, as we think that term is generally
more clear.
Typeclasses allow us to generalize over a set of types in
order to define and execute a standard set of features for those
types. For example, the ability to test values for equality is
useful, and we’d want to be able to use that function for data
of various types. In fact, we can test any data of a type that
implements the typeclass known as Eqfor equality. We do
not need separate equality functions for each diﬀerent type
of data; as long as our datatype implements, or instantiates,
theEqtypeclass, we can use the standard functions. Similarly,
all the numeric literals and their various types implement a
typeclass called Num, which defines a standard set of operators
that can be used with any type of numbers.
1Philip Wadler, “The Expression Problem” http://homepages.inf.ed.ac.uk/wadler/
papers/expression/expression.txt</p>
<p>CHAPTER 6. LESS AD-HOC POLYMORPHISM 250
We’ll get into more detail about what it means for a type
to have an “instance” of a typeclass in this chapter, but briefly
stated, it means that there is code that defines how the values
and functions from that typeclass work for that type. When
you use a typeclass method with one of the types that has such
an instance, the compiler looks up the code that dictates how
the function works for that type. We’ll see this more as we
write our own instances.
6.3 Back to Bool
Let’s return briefly to the Booltype to get a feel for what type-
class information looks like. As you may recall, we can use
the GHCi command :infoto query information, including
typeclass information about any function or type (and some
values):
Prelude&gt; :info Bool
data Bool = False | True
instance Bounded Bool
instance Enum Bool
instance Eq Bool
instance Ord Bool
instance Read Bool
instance Show Bool</p>
<p>CHAPTER 6. LESS AD-HOC POLYMORPHISM 251
The information includes the data declaration for Booland
which typeclasses it already has instances of. It also tells you
where the datatype and its instances are defined for the com-
piler, if you want to look at the source code, but we’ve left that
information out.
Let’s look at that list of instances. Each of these instances
is a typeclass that Boolimplements, and the instances are the
unique specifications of how Boolmakes use of the methods
from that typeclass. In this chapter, we’re only going to exam-
ine a few of these, namely Eq,Ord, andShow. Briefly, however,
they mean the following:
1.instance Bounded Bool –Bounded for types that have an up-
per and lower bound
2.instance Enum Bool –Enumfor things that can be enumer-
ated
3.instance Eq Bool –Eqfor things that can be tested for equal-
ity
4.instance Ord Bool –Ordfor things that can be put into a
sequential order
5.instance Read Bool –Readparses strings into things. Don’t
use it. No seriously, don’t.
6.instance Show Bool –Showrenders things into strings.</p>
<p>CHAPTER 6. LESS AD-HOC POLYMORPHISM 252
Typeclasses have a hierarchy of sorts,2as you might recall
from our discussion of numeric types. All Fractional numbers
implement the Numtypeclass, but not all NumareFractional . All
members of Ordmust be members of Eq, and all members of
Enummust be members of Ord. To be able to put something
in an enumerated list, they must be able to be ordered; to be
able to order something, they must be able to be compared
for equality.
6.4 Eq
In Haskell, equality is implemented with a typeclass called Eq.
Some programming languages bake equality into every object
in the language, but some datatypes do not have a sensible
notion of equality3, so Haskell does not encode equality into
every type. Eqallows us to use standard measures of equality
for quite a few datatypes, though.
Eqis defined this way:
Prelude&gt; :info Eq
class Eq a where
(==) :: a -&gt; a -&gt; Bool
2You can use a search engine like Hoogle at http://haskell.org/hoogle to find informa-
tion on Haskell datatypes and typeclasses. Hoogle is a Haskell API search engine, which
allows you to search many standard Haskell libraries by function name or type signature.
As you become fluent in Haskell types, you will be able to input the type of the function
you want and find the functions that match.
3Most importantly, the function type does not have an Eqinstance for reasons we will
not get into here.</p>
<p>CHAPTER 6. LESS AD-HOC POLYMORPHISM 253
(/=) :: a -&gt; a -&gt; Bool
First, it tells us we have a typeclass called Eqwhere there are
two basic functions, equality and nonequality, and gives their
type signatures. Next it lists the instances of Eq:
-- partial list
instance Eq a =&gt; Eq [a]
instance Eq Ordering
instance Eq Int
instance Eq Float
instance Eq Double
instance Eq Char
instance Eq Bool
instance (Eq a, Eq b) =&gt; Eq (a, b)
instance Eq ()
instance Eq a =&gt; Eq (Maybe a)
instance Eq Integer
We see several numeric types, our old friend Bool,Char(un-
surprising, as we’ve seen that we can compare characters for
equality), and tuples. We know from this that any time we are
using data of these types, we are implementing the Eqtypeclass
and therefore have generic functions we can use to compare
their equality. Any type that has an instance of this typeclass
implements the methods of the typeclass.
Here are some examples using this typeclass:</p>
<p>CHAPTER 6. LESS AD-HOC POLYMORPHISM 254
Prelude&gt; 132 == 132
True
Prelude&gt; 132 /= 132
False
Prelude&gt; (1, 2) == (1, 1)
False
Prelude&gt; (1, 1) == (1, 2)
False
Prelude&gt; &quot;doge&quot; == &quot;doge&quot;
True
Prelude&gt; &quot;doge&quot; == &quot;doggie&quot;
False
The types of (==)and(/=)inEqtell us something important
about these functions:
(==)::Eqa=&gt;a-&gt;a-&gt;Bool
(/=)::Eqa=&gt;a-&gt;a-&gt;Bool
Given these types, we know that they can be used for any
type𝑎which implements the Eqtypeclass. We also know that
both functions will take two arguments of the same type 𝑎and
returnBool. We know they have to be the same because 𝑎must
equal𝑎in the same type signature.
When we apply (==)to a single argument, we can see how
it specializes the arguments:</p>
<p>CHAPTER 6. LESS AD-HOC POLYMORPHISM 255
(==) ::Eqa=&gt;a-&gt;a-&gt;Bool
-- if we specialized (==)
-- for [Char] aka String
(==)
::[Char]-&gt;[Char]-&gt;Bool
(==)&quot;cat&quot;
:: [Char]-&gt;Bool
(==)&quot;cat&quot;&quot;cat&quot;
:: Bool
You can experiment with this further in the REPL to see
how applying types to arguments makes the type variables
more specific.
What happens if the first two arguments 𝑎and𝑎aren’t the
same type?
Prelude F M&gt; (1, 2) == &quot;puppies!&quot;
Couldn't match expected type ‘(t0, t1)’
with actual type ‘[Char]’
In the second argument of ‘(==)’, namely ‘&quot;puppies!&quot;’
In the expression: (1, 2) == &quot;puppies!&quot;
In an equation for ‘it’: it = (1, 2) == &quot;puppies!&quot;
Let’s break down this type error:
Couldn't match expected type ‘(t0, t1)’</p>
<p>CHAPTER 6. LESS AD-HOC POLYMORPHISM 256
with actual type ‘[Char]’
This error means our [Char] wasn’t the tuple of types 𝑡0and
𝑡1that was expected. (t0, t1) was expected for the second
argument (where we supplied &quot;puppies!&quot; ) because that’s the
type of the first argument. Remember: the type of 𝑎is usually
setbytheleftmostoccurrenceandcan’tchangeinthesignature
Eq a =&gt; a -&gt; a -&gt; Bool .
Applying (==)toInteger will bind the 𝑎type variable to
Integer . This is as if the signature changed to:
EqInteger =&gt;Integer -&gt;Integer -&gt;Bool
The typeclass constraint Eq Integer =&gt; gets dropped because
it’s redundant. We can see the issue more clearly if we look at
the typeclass instances on the 2-tuple (,):
data(,) a b =(,) a b
instance (Eqa,Eqb)=&gt;Eq(a, b)
instance (Orda,Ordb)=&gt;Ord(a, b)
instance (Reada,Readb)=&gt;Read(a, b)
instance (Showa,Showb)=&gt;Show(a, b)
We saw the Eqinstance of (,)getting used earlier when we
tested code like (1, 2) == (1, 2) . Critically, the Eqinstance of
(a, b) relies on the Eqinstances of 𝑎and𝑏. This tells us the
equality of two tuples (a, b) depends on the equality of their
constituent values 𝑎and𝑏. This is why this works:</p>
<p>CHAPTER 6. LESS AD-HOC POLYMORPHISM 257
Prelude&gt; (1, 'a') == (2, 'b')
False
But neither of these will work:
Prelude&gt; (1, 2) == ('a', 'b')
Prelude&gt; (1, 'a') == ('a', 1)
Typeclass deriving Typeclass instances we can magically de-
rive are Eq,Ord,Enum,Bounded ,Read, andShow, though there are
some constraints on deriving some of these. Deriving means
you don’t have to manually write instances of these typeclasses
for each new datatype you create. We’ll address this a bit more
in the chapter on Algebraic Datatypes.
6.5 Writing typeclass instances
We haven’t talked much about writing your own datatypes yet,
or about writing your own typeclass; however, you can and
will do both. In either case, you will sometimes find yourself
needing to write your own typeclass instances. While Eqis one
of the typeclasses you can simply derive, it’s also one of the
least complicated typeclasses to write instances for, so we’re
going to use it here, to demonstrate how to write your own
instances.</p>
<p>CHAPTER 6. LESS AD-HOC POLYMORPHISM 258
Eq instances
As we’ve seen, Eqprovides instances for determining equality
of values, so making an instance of it for a given datatype is
usually straightforward.
You can investigate a typeclass by referring to the Hack-
age documentation for that typeclass. Typeclasses like Eq
come with the core baselibrary that is located at http://hackage.
haskell.org/package/base .Eqspecifically is located at http://
hackage.haskell.org/package/base/docs/Data-Eq.html .
In that documentation, you’ll want to note a particular bit
of wording:
Minimal complete definition: either == or /=.
This tells you what methods you need to define to have
a valid Eqinstance. In this case, either (==)(equal) or (/=)
(unequal) will suffice, as one can be defined as the negation
of the other. Why not only (==)? Although it’s rare, you may
have something clever to do for each case that could make
equality checking faster for a particular datatype, so you’re
allowed to specify both if you want to. We won’t do that here
because (/=)is the negation of (==), and we won’t be working
with any clever datatypes.
First, we’ll work with a tiny, trivial datatype called... Trivial !</p>
<p>CHAPTER 6. LESS AD-HOC POLYMORPHISM 259
dataTrivial =
Trivial
With no deriving clause hanging oﬀ the butt of this datatype
declaration, we’ll have no typeclass instances of any kind. If we
try to load this up and test equality without adding anything
further, GHC will throw a type error:
Prelude&gt; Trivial == Trivial
No instance for (Eq Trivial) arising
from a use of ‘==’
In the expression: Trivial == Trivial
In an equation for ‘it’: it = Trivial == Trivial
GHC can’t find an instance of Eqfor our datatype Trivial .
We could’ve had GHC generate one for us using deriving Eq
or we could’ve written one, but we did neither, so none exists
and it fails at compile time. In some languages, this sort of
mistake doesn’t become known until your code is already in
the middle of executing.
Unlike other languages, Haskell does not provide universal
stringification ( Show/ print) or equality ( Eq(value equality) or
pointer equality) as this is not always sound or safe, regardless
of what programming language you’re using.</p>
<p>CHAPTER 6. LESS AD-HOC POLYMORPHISM 260
So we must write our own! Fortunately, with Trivial this
is...trivial. Keep your typeclass instances for a type in the same
file as that type (we’ll explain why later):
dataTrivial =
Trivial'
instance EqTrivial where
Trivial' ==Trivial' =True
And that’s it! We wrote an instance that tells the compiler
how to test this datatype for equality. Data constructors and
type constructors often have the same name in Haskell, and
that can get confusing. We used the single quote at the end of
the data constructor here because they don’t have to have the
same name and it might make it easier to follow the examples.
If you load this up, you have only one possible expression
you can construct here:
Prelude&gt; Trivial' == Trivial'
True
Let’s drill down a bit into how this instance stuﬀ works:</p>
<p>CHAPTER 6. LESS AD-HOC POLYMORPHISM 261
instance EqTrivial where
-- [1] [2] [3] [4]
Trivial' ==Trivial' =True
-- [5] [6] [7] [8]
instance EqTrivial where
(==)Trivial' Trivial' =True
-- [ 9 ]
1.The keyword instance here begins a declaration of a type-
classinstance. TypeclassinstancesarehowyoutellHaskell
how equality, stringification ( Show), orderability ( Ord), enu-
meration ( Enum) or other typeclasses should work for a
particular datatype. Without this instance, we can’t test
the values for equality even though the answer will never
vary in the case of this particular datatype.
2.The first name to follow the instance is the typeclass the
instance is providing. Here that is Eq.
3.The type the instance is being provided for. In this case,
we’reimplementingthe Eqtypeclass fortheTrivial datatype.
4.The keyword whereterminates the initial declaration and
beginning of the instance. What follows are the methods
(functions) being implemented.</p>
<p>CHAPTER 6. LESS AD-HOC POLYMORPHISM 262
5.The data constructor (value) Trivial' is the first argument
to the==function we’re providing. Here we’re defining ==
using infix notation so the first argument is to the left.
6.The infix function ==, this is what we’re defining in this
declaration.
7.The second argument, which is the value Trivial' . Since
==is infix here, the second argument is to the right of ==.
8.The result of Trivial' == Trivial' , that is, True.
9.We could’ve written the definition of (==)using prefix no-
tation instead of infix by wrapping the operator in paren-
theses. Note this is being shown as an alterative; you can’t
have two typeclass instances for the same type. Typeclass
instances are unique to a given type. You can try having
both in the same file, but you’ll get an error.
Okay, let’s stretch our legs a bit and try something a bit less
Trivial ! We’ll make our own datatypes — one for the days of
the week and one for the date that makes use of the DayOfWeek
type:</p>
<p>CHAPTER 6. LESS AD-HOC POLYMORPHISM 263
dataDayOfWeek =
Mon|Tue|Weds|Thu|Fri|Sat|Sun
-- day of week and numerical day of month
dataDate=
DateDayOfWeek Int
Since these are not prebaked datatypes in Haskell, they have
no typeclass instances at all. As they stand, there is nothing you
can do with them because no operations are defined for them.
Let’s fix that. The first Eqinstance we’ll write is for DayOfWeek
and is a bit tedious to write out:
instance EqDayOfWeek where
(==)MonMon=True
(==)TueTue=True
(==)WedsWeds=True
(==)ThuThu=True
(==)FriFri=True
(==)SatSat=True
(==)SunSun=True
(==)_ _ = False
Now we’ll write an Eqinstance for our Datetype. This one is
more interesting:</p>
<p>CHAPTER 6. LESS AD-HOC POLYMORPHISM 264
instance EqDatewhere
(==) (Dateweekday dayOfMonth)
(Dateweekday' dayOfMonth') =
weekday ==weekday'
&amp;&amp;dayOfMonth ==dayOfMonth'
In theEqinstance for Date, we didn’t recapitulate how equal-
ity forDayOfWeek andIntvalues worked; we simply said that the
dates were equal if all of their constituent values were equal.
Note, also, that the compiler already expects the arguments
ofDateto be aDayOfWeek value and an Intso we do not need to
specify that. Based on what it knows about those three types,
this is enough information for us to test Datevalues for equality.
Does it work?
Prelude&gt; Date Thu 10 == Date Thu 10
True
Prelude&gt; Date Thu 10 == Date Thu 11
False
Prelude&gt; Date Thu 10 == Date Weds 10
False
It compiles, and it returns what we want after three cursory
checks — ship it!
We’ll point out one other thing about these types:</p>
<p>CHAPTER 6. LESS AD-HOC POLYMORPHISM 265
Prelude&gt; Date Thu 10
<interactive>:26:1:
No instance for (Show Date) arising from a use of ‘print’
In a stmt of an interactive GHCi command: print it
We wrote an Eqinstance, so we can test the values for equal-
ity, but we can’t print them in the REPL because we provided
noShowinstance. If you’d like to fix that, you can stick a deriving
Showclause on the end of each of the datatypes above.
Partial functions — not so strange danger
We’ve mentioned partial application of functions previously,
but the term partial function refers to something diﬀerent. A
partial function is one that doesn’t handle all the possible cases,
so there are possible scenarios in which we haven’t defined
any way for the code to evaluate.
We need to take care to avoid partial functions in general
in Haskell, but this must be especially kept in mind when we
have a type with multiple cases such as DayOfWeek . What if we
had made a mistake in the Eqinstance?</p>
<p>CHAPTER 6. LESS AD-HOC POLYMORPHISM 266
dataDayOfWeek =
Mon|Tue|Weds|Thu|Fri|Sat|Sun
instance EqDayOfWeek where
(==)MonMon=True
(==)TueTue=True
(==)WedsWeds=True
(==)ThuThu=True
(==)FriFri=True
(==)SatSat=True
(==)SunSun=True
What if the arguments are diﬀerent? We forgot our uncon-
ditional case. This will appear to be fine whenever the argu-
ments are the same, but blow up in our faces when they’re
not:
Prelude&gt; Mon == Mon
True
Prelude&gt; Mon == Tue
*** Exception: code/derivingInstances.hs:
(19,3)-(25,23):
Non-exhaustive patterns in function ==
Well, that stinks. We definitely didn’t start learning Haskell
because we wanted stuﬀ to blow up at runtime. So what gives?</p>
<p>CHAPTER 6. LESS AD-HOC POLYMORPHISM 267
The good news is there issomething you can do to get more
help from GHC on this. If we turn all warnings on with the
-Wallflag in our REPL (or in our build configuration), then
GHC will let us know when we’re not handling all cases:
Prelude&gt; :set -Wall
Prelude&gt; :l code/derivingInstances.hs
[1 of 1] Compiling DerivingInstances
code/derivingInstances.hs:19:3: Warning:
Pattern match(es) are non-exhaustive
In an equation for ‘==’:
Patterns not matched:
Mon Tue
Mon Weds
Mon Thu
Mon Fri
...
Ok, modules loaded: DerivingInstances.
You’ll find that if you fix your instance and provide the
fallback case that returns False, it’ll stop squawking about the
non-exhaustive patterns.
Partial functions are not only a concern with typeclass in-
stances, though. We will discuss this more in the next chapter,
but it’s also a concern with any function that doesn’t handle all</p>
<p>CHAPTER 6. LESS AD-HOC POLYMORPHISM 268
possible inputs, such as this, that blows up anytime the input
isn’t 2:
f::Int-&gt;Bool
f2=True
If you compile or load this, you’ll get another warning (as-
suming you still have -Wallturned on). In this case, because
Intis ahugetype with many values, it’s using notation that says
you’re not handling all inputs that aren’t the number 2:
Pattern match(es) are non-exhaustive
In an equation for ‘f’:
Patterns not matched:
GHC.Types.I# #x with #x <code>notElem</code> [2#]
If you add another case such that you’re handling one more
input, it will add that to the set of values you are handling:
f::Int-&gt;Bool
f1=True
f2=True
Pattern match(es) are non-exhaustive
In an equation for ‘f’:
Patterns not matched:
GHC.Types.I# #x with #x <code>notElem</code> [1#, 2#]</p>
<p>CHAPTER 6. LESS AD-HOC POLYMORPHISM 269
f::Int-&gt;Bool
f1=True
f2=True
f3=True
Pattern match(es) are non-exhaustive
In an equation for ‘f’:
Patterns not matched:
GHC.Types.I# #x with #x <code>notElem</code> [1#, 2#, 3#]
So on and so forth. The real answer here is to have an
unconditional case that matches everything. The following
will compile without complaint and is not partial:
f::Int-&gt;Bool
f1=True
f2=True
f3=True
f_ =False
Another solution is to use a datatype that isn’t hugelikeInt
if you only have a few cases you want to consider.
-- Seriously. It's huge.
Prelude&gt; minBound :: Int
-9223372036854775808</p>
<p>CHAPTER 6. LESS AD-HOC POLYMORPHISM 270
Prelude&gt; maxBound :: Int
9223372036854775807
If you want your data to describe only a handful of cases,
write them down in a sum type like the DayOfWeek datatype we
showed you earlier. Don’t use Intas an implicit sum type as C
programmers commonly do.
Sometimes we need to ask for more
When we’re writing an instance of a typeclass such as Eqfor
something with polymorphic parameters, such as Identity
below, we’ll sometimes need to require our argument or argu-
ments to provide some typeclass instances for us in order to
write an instance for the datatype containing them:
dataIdentity a=
Identity a
instance Eq(Identity a)where
(==) (Identity v) (Identity v')=v==v'
What we want to do here is rely on whatever Eqinstances the
argument to Identity (written as 𝑎in the datatype declaration
and𝑣in the instance definition) has already. There is one
problem with this as it stands, though:</p>
<p>CHAPTER 6. LESS AD-HOC POLYMORPHISM 271
No instance for (Eq a) arising from a use of ‘==’
Possible fix: add (Eq a) to the
context of the instance declaration
In the expression: v == v'
In an equation for ‘==’:
(==) (Identity v) (Identity v') = v == v'
In the instance declaration for ‘Eq (Identity a)’
The problem here is that 𝑣and𝑣′are both of type 𝑎but we
don’t know anything about 𝑎. In this case, we can’t assume
it has an Eqinstance. However, we can use the same type-
class constraint syntax we saw with functions, in our instance
declaration:
instance Eqa=&gt;Eq(Identity a)where
(==) (Identity v) (Identity v')=v==v'
Now it’ll work because we know 𝑎has to have an instance of
Eq. Additionally, Haskell will ensure we don’t attempt to check
equality with values that don’t have an Eqinstance at compile
time:
Prelude&gt; Identity NoEqInst == Identity NoEqInst
No instance for (Eq NoEqInst)
arising from a use of ‘==’</p>
<p>CHAPTER 6. LESS AD-HOC POLYMORPHISM 272
In the expression:
Identity NoEqInst == Identity NoEqInst
In an equation for ‘it’:
it = Identity NoEqInst == Identity NoEqInst
We could ask for more than we need in order to obtain an
answer, such as below where we ask for an Ordinstance for 𝑎,
but there’s no reason to do so since Eqrequires less than Ord
and does enough for what we need here:
instance Orda=&gt;Eq(Identity a)where
(==) (Identity v) (Identity v')=
compare v v' ==EQ
That will compile, but it’s not clear why you’d do it. Maybe
you have your own secret reasons.
Exercises: Eq Instances
Write the Eqinstance for the datatype provided.
1.It’s not a typo, we’re just being cute with the name.
dataTisAnInteger =
TisAnInteger</p>
<p>CHAPTER 6. LESS AD-HOC POLYMORPHISM 273
2.dataTwoIntegers =
TwoInteger Integer
3.dataStringOrInt =
TisAnInt Int
|TisAString String
4.dataPaira=
Paira a
5.dataTuplea b=
Tuplea b
6.dataWhicha=
ThisOne a
|ThatOne a
7.dataEitherOr a b=
Helloa
|Goodbye b
6.6 Num
We have seen a lot of Numat this point, so we’ll try not to go
on too long about it. It is a typeclass implemented by most
numeric types. As we did with Eqwe will query the information
and examine its set of predefined functions:</p>
<p>CHAPTER 6. LESS AD-HOC POLYMORPHISM 274
classNumawhere
(+)::a-&gt;a-&gt;a
(*)::a-&gt;a-&gt;a
(-)::a-&gt;a-&gt;a
negate::a-&gt;a
abs::a-&gt;a
signum::a-&gt;a
fromInteger ::Integer -&gt;a
And its list of instances (not quite complete):
instance NumInteger
instance NumInt
instance NumFloat
instance NumDouble
We’ve seen most of this information before, in one form
or another: common arithmetic functions with their type
signatures at the top ( fromInteger is similar to fromIntegral but
restricted to Integer rather than all integral numbers) plus a list
of types that implement this typeclass, numeric types we’ve
looked at previously. No surprises here.
Integral
The typeclass called Integral has the following definition:</p>
<p>CHAPTER 6. LESS AD-HOC POLYMORPHISM 275
class(Reala,Enuma)=&gt;Integral awhere
quot::a-&gt;a-&gt;a
rem::a-&gt;a-&gt;a
div::a-&gt;a-&gt;a
mod::a-&gt;a-&gt;a
quotRem ::a-&gt;a-&gt;(a, a)
divMod::a-&gt;a-&gt;(a, a)
toInteger ::a-&gt;Integer
The typeclass constraint (Real a, Enum a) =&gt; means that any
type that implements Integral must already have instances for
RealandEnumtypeclasses. In a very real sense the tuple syntax
here denotes the conjunction of typeclass constraints on your
type variables. An integral type must be both a real number
and enumerable and therefore may employ the methods of
each of those typeclasses. In turn, the Realtypeclass itself re-
quires an instance of Num. So, the Integral typeclass may put
the methods of RealandNuminto eﬀect (in addition to those
ofEnum). Since Realcannot override the methods of Num, this
typeclass inheritance is onlyadditive and the ambiguity prob-
lems caused by multiple inheritance in some programming
languages — the so-called “deadly diamond of death” — are
avoided.
Exercises: Tuple Experiment Look at the types given for
quotRem anddivMod . What do you think those functions do? Test</p>
<p>CHAPTER 6. LESS AD-HOC POLYMORPHISM 276
your hypotheses by playing with them in the REPL. We’ve
given you a sample to start with below:
Prelude&gt; let ones x = snd (divMod x 10)
Fractional
Numis a superclass of Fractional . TheFractional typeclass is
defined as follows:
class(Numa)=&gt;Fractional awhere
(/) ::a-&gt;a-&gt;a
recip ::a-&gt;a
fromRational ::Rational -&gt;a
This typeclass declaration creates a class named Fractional
which requires its type argument 𝑎to have an instance of Num
in order to create an instance of Fractional . This is another
example of typeclass inheritance. Fractional applies to fewer
numbers than Numdoes, and instances of the Fractional class
can use the functions defined in Num, but not all Numcan use
the functions defined in Fractional because nothing in Num’s
definition requires an instance of Fractional . There is a chart
at the end of the chapter to help you visualize this information.
We can see this with ordinary functions:</p>
<p>CHAPTER 6. LESS AD-HOC POLYMORPHISM 277
First let’s consider this function, intentionally without a
type provided:
divideThenAdd x y=(x/y)+1
We’ll load this with a type that asks only for a Numinstance:
divideThenAdd ::Numa=&gt;a-&gt;a-&gt;a
divideThenAdd x y=(x/y)+1
And you’ll get the type error:
Could not deduce (Fractional a)
arising from a use of ‘/’
from the context (Num a)
bound by the type signature for
divideThenAdd :: Num a =&gt; a -&gt; a -&gt; a
Now if we only cared about having the Numconstraint, we
couldmodifyourfunctiontonotuse (/)whichrequires Fractional :
-- This works fine.
-- (+) and (-) are both provided by Num
subtractThenAdd ::Numa=&gt;a-&gt;a-&gt;a
subtractThenAdd x y=(x-y)+1</p>
<p>CHAPTER 6. LESS AD-HOC POLYMORPHISM 278
Or we can change the type rather than modifying the func-
tion itself:
-- This works fine.
divideThenAdd ::Fractional a
=&gt;a-&gt;a-&gt;a
divideThenAdd x y=(x/y)+1
Put on your thinking cap Why didn’t we need to make the
type of the function we wrote require both typeclasses? Why
didn’t we have to do this:
f::(Numa,Fractional a)=&gt;a-&gt;a-&gt;a
Consider what it means for something to be a subset of a
larger set of objects.
6.7 Type-defaulting typeclasses
When you have a typeclass-constrained (ad hoc) polymorphic
value and need to evaluate it, the polymorphism must be re-
solved to a specific concrete type. The concrete type must
have an instance for all the required typeclass instances (that
is, if it is required to implement NumandFractional then the
concrete type can’t be an Int). Ordinarily the concrete type</p>
<p>CHAPTER 6. LESS AD-HOC POLYMORPHISM 279
would come from the type signature you’ve specified or from
type inference, such as when a Num a =&gt; a is used in an expres-
sion that expects an Integer which forces the polymorphic
number value to concretize as an Integer . But in some cases,
particularly when you’re working in the GHCi REPL, you will
not have specified a concrete type for a polymorphic value. In
those situations, the typeclass will default to a concrete type,
and the default types are already set in the libraries.
When we do this in the REPL:
Prelude&gt; 1 / 2
0.5
Our result 0.5appears the way it does because it defaults to
Double . Using the type assignment operator ::we can assign a
more specific type and circumvent the default to Double :
Prelude&gt; 1 / 2 :: Float
0.5
Prelude&gt; 1 / 2 :: Double
0.5
Prelude&gt; 1 / 2 :: Rational
1 % 2
The Haskell Report4specifies the following defaults relevant
to numerical computations:
4The Haskell Report is the standard that specifies the language and standard libraries</p>
<p>CHAPTER 6. LESS AD-HOC POLYMORPHISM 280
default Num Integer
default Real Integer
default Enum Integer
default Integral Integer
default Fractional Double
default RealFrac Double
default Floating Double
default RealFloat Double
Num,Real, etc., are typeclasses, and Integer andDouble are
the types they default to. This type defaulting for Fractional
means that:
(/)::Fractional a=&gt;a-&gt;a-&gt;a
changes to
(/)::Double-&gt;Double-&gt;Double
if you don’t specify the concrete type desired for (/). A
similar example but for Integral would be
div::Integral a=&gt;a-&gt;a-&gt;a
defaulting to
div::Integer -&gt;Integer -&gt;Integer
for Haskell. The most recent version is Haskell Report 2010, which can be found at
https://www.haskell.org/onlinereport/haskell2010/ .</p>
<p>CHAPTER 6. LESS AD-HOC POLYMORPHISM 281
The typeclass constraint is superfluous when the types are
concrete. On the other hand, you must specify which type-
classes you want type variables to have implemented. The use
of polymorphic values without the ability to infer a specific
type and no default rule will cause GHC to complain about an
ambiguous type.
The following will work because all the types below imple-
ment the Numtypeclass:
Prelude&gt; let x = 5 + 5 :: Int
Prelude&gt; x
10
Prelude&gt; let x = 5 + 5 :: Integer
Prelude&gt; x
10
Prelude&gt; let x = 5 + 5 :: Float
Prelude&gt; x
10.0
Prelude&gt; let x = 5 + 5 :: Double
Prelude&gt; x
10.0
Now we can make this type more specific, and the process</p>
<p>CHAPTER 6. LESS AD-HOC POLYMORPHISM 282
will be similar. In this case, let’s use Integer which implements
Num:
let x = 10 :: Integer
let y = 5 :: Integer
-- These are the declared types for these
-- functions, because they're from Num.
(+) :: Num a =&gt; a -&gt; a -&gt; a
(*) :: Num a =&gt; a -&gt; a -&gt; a
(-) :: Num a =&gt; a -&gt; a -&gt; a
Now any functions from Numare going to automatically get
specialized to Integer when we apply them to the 𝑥or𝑦values:
Prelude&gt; :t (x+)
(x+) :: Integer -&gt; Integer
-- For
(+) :: Num a =&gt; a -&gt; a -&gt; a
-- When 'a' is Integer
(+) :: Integer -&gt; Integer -&gt; Integer
-- Apply the first argument
(x+) :: Integer -&gt; Integer
-- Applying the second and last argument
(x+y) :: Integer</p>
<p>CHAPTER 6. LESS AD-HOC POLYMORPHISM 283
-- Final result was Integer.
We can declare more specific (monomorphic) functions
from more general (polymorphic) functions:
let add = (+) :: Integer -&gt; Integer -&gt; Integer
We cannot go in the other direction, because we lost the
generality of Numwhen we specialized to Integer :
Prelude&gt; :t id
id :: a -&gt; a
Prelude&gt; let numId = id :: Num a =&gt; a -&gt; a
Prelude&gt; let intId = numId :: Integer -&gt; Integer
Prelude&gt; let altNumId = intId :: Num a =&gt; a -&gt; a
Could not deduce (a1 ~ Integer)
from the context (Num a)
bound by the inferred type of
altNumId :: Num a =&gt; a -&gt; a
or from (Num a1)
bound by an expression type signature:
Num a1 =&gt; a1 -&gt; a1
‘a1’ is a rigid type variable bound by
an expression type signature:</p>
<p>CHAPTER 6. LESS AD-HOC POLYMORPHISM 284
Num a1 =&gt; a1 -&gt; a1
Expected type: a1 -&gt; a1
Actual type: Integer -&gt; Integer
In the expression: intId :: Num a =&gt; a -&gt; a
In an equation for ‘altNumId’:
altNumId = intId :: Num a =&gt; a -&gt; a
Theexpectedtype and the actualtype don’t match. Remember,
the actual type is the type we provided; the expected type
is what the compiler expects. Here, the actual type is more
concrete than the expected type. Types can be made more
specific, but not more general or polymorphic.
6.8 Ord
Next we’ll take a look at a typeclass called Ord. We’ve previously
noted that this typeclass covers the types of things that can be
put in order. If you use :infoforOrdin your REPL, you will
find a very large number of instances for this typeclass. We’re
going to pare it down a bit and focus on the essentials, but, as
always, we encourage you to explore this further on your own:
Prelude&gt; :info Ord</p>
<p>CHAPTER 6. LESS AD-HOC POLYMORPHISM 285
class Eq a =&gt; Ord a where
compare :: a -&gt; a -&gt; Ordering
(&lt;) :: a -&gt; a -&gt; Bool
(&gt;=) :: a -&gt; a -&gt; Bool
(&gt;) :: a -&gt; a -&gt; Bool
(&lt;=) :: a -&gt; a -&gt; Bool
max :: a -&gt; a -&gt; a
min :: a -&gt; a -&gt; a
instance Ord a =&gt; Ord (Maybe a)
instance (Ord a, Ord b) =&gt; Ord (Either a b)
instance Ord Integer
instance Ord a =&gt; Ord [a]
instance Ord Ordering
instance Ord Int
instance Ord Float
instance Ord Double
instance Ord Char
instance Ord Bool
Notably, at the top, we have another typeclass constraint.
Ordis constrained by Eqbecause if you’re going to compare
items in a list and put them in order, you need a way to de-
termine if they are equal. So, Ordrequires Eqand its methods.
The functions that come standard in this class have to do with
ordering. Some of them will give you a result of Bool, and</p>
<p>CHAPTER 6. LESS AD-HOC POLYMORPHISM 286
we’ve played a bit with those functions. Let’s see what a few
others do:
Prelude&gt; compare 7 8
LT
Prelude&gt; compare 4 (-4)
GT
Prelude&gt; compare 4 4
EQ
Prelude&gt; compare &quot;Julie&quot; &quot;Chris&quot;
GT
Prelude&gt; compare True False
GT
Prelude&gt; compare True True
EQ
Thecompare function works for any of the types listed above
that implement the Ordtypeclass, including Bool, but unlike
the&lt;, &gt;, &gt;= and&lt;=operators, this returns an Ordering value
instead of a Boolvalue.
You may notice that Trueis greater than False. Proximally
this is due to how the Booldatatype is defined: False | True .
There may be a more interesting underlying reason if you
prefer to ponder the philosophical implications.
Themaxandminfunctions work in a similarly straightfor-
ward fashion for any type that implements this typeclass:</p>
<p>CHAPTER 6. LESS AD-HOC POLYMORPHISM 287
Prelude&gt; max 7 8
8
Prelude&gt; min 10 (-10)
-10
Prelude&gt; max (3, 4) (2, 3)
(3,4)
Prelude&gt; min [2, 3, 4, 5] [3, 4, 5, 6]
[2,3,4,5]
Prelude&gt; max &quot;Julie&quot; &quot;Chris&quot;
&quot;Julie&quot;
By looking at the type signature, we can see that these func-
tions have two parameters. If you want to use these to deter-
mine the maximum or minimum of three values, you can nest
them:
Prelude&gt; max 7 (max 8 9)
9
If you try to give it too few arguments, you will get this
strange-seeming message:
Prelude&gt; max &quot;Julie&quot;
No instance for (Show ([Char] -&gt; [Char]))
-- [1] [2] [ 3 ]
arising from a use of ‘print’</p>
<p>CHAPTER 6. LESS AD-HOC POLYMORPHISM 288
-- [4]
In a stmt of an interactive GHCi command: print it
-- [ 5 ]
1.Haskell couldn’t find an instance of a typeclass for a value
of a given type.
2.The typeclass it couldn’t find an instance for was Show, the
typeclass that allows GHCi to print values in your terminal.
More on this in the following sections.
3.It couldn’t find an instance of Showfor the type String -&gt;
String . Nothing with type (-&gt;)should have a Showinstance
as a general rule because (-&gt;)denotes a function rather
than a constant value.
4.We wanted an instance of Showbecause we (indirectly)
invoked printwhich has type print :: Show a =&gt; a -&gt; IO
()— note the constraint for Show.
5.The interactive GHCi command print it invoked print
on our behalf.
Any time we ask GHCi to print a return value in our ter-
minal, we are indirectly invoking print, which has the type
Show a =&gt; a -&gt; IO () . The first argument to printmust have an
instance of Show. The error message is because maxapplied to</p>
<p>CHAPTER 6. LESS AD-HOC POLYMORPHISM 289
a single String argument needs another argument before it’ll
return a String (aka[Char] ) value that is Show-able or printable.
Until we apply it to a second argument, it’s still a function,
and a function has no instance of Show. The request to print
a function, rather than a constant value, results in this error
message.
Ord instances
We’ll see more examples of writing instances as we proceed
in the book and explain more thoroughly how to write your
own datatypes. We wrote some Eqinstances earlier. Now we’ll
practice our instance-writing skills (this is one of the most
necessary skills in Haskell) by writing Ordinstances.
When you derive Ordinstances for a datatype, they rely on
the way the datatype is defined, but if you write your own
instance, you can define the behavior you want. We’ll use the
days of the week again to demonstrate:
dataDayOfWeek =
Mon|Tue|Weds|Thu|Fri|Sat|Sun
deriving (Ord,Show)
We only derived OrdandShowthere because you should still
have the Eqinstance we wrote for this datatype in scope. If you
don’t, you have two options: bring it back into scope by putting
it into the file you’re currently using, or derive an Eqinstance</p>
<p>CHAPTER 6. LESS AD-HOC POLYMORPHISM 290
for the datatype now by adding it inside the parentheses. You
can’t have an Ordinstance unless you also have an Eqinstance,
so the compiler will complain if you don’t do one (not both)
of those two things.
Values to the left are less than values to the right, as if they
were placed on a number line:
Prelude&gt; Mon &gt; Tue
False
Prelude&gt; Sun &gt; Mon
True
Prelude&gt; compare Tue Weds
LT
But if we wanted to express that Friday is always the best
day, we can write our own Ordinstance to express that:
dataDayOfWeek =
Mon|Tue|Weds|Thu|Fri|Sat|Sun
deriving (Eq,Show)
instance OrdDayOfWeek where
compare FriFri=EQ
compare Fri_ =GT
compare _Fri=LT
compare _ _ = EQ</p>
<p>CHAPTER 6. LESS AD-HOC POLYMORPHISM 291
Now, if we compare Friday to any other day, Friday is always
greater. All other days, you notice, are equal in value:
Prelude&gt; compare Fri Sat
GT
Prelude&gt; compare Sat Mon
EQ
Prelude&gt; compare Fri Mon
GT
Prelude&gt; compare Sat Fri
LT
Prelude&gt; Mon &gt; Fri
False
Prelude&gt; Fri &gt; Sat
True
But we did derive an Eqinstance above, so we do get the
expected equality behavior:
Prelude&gt; Sat == Mon
False
Prelude&gt; Fri == Fri
True
A few things to keep in mind about writing Ordinstances:
First, it is wise to ensure that your Ordinstances agree with your
Eqinstances, whether the Eqinstances are derived or manually</p>
<p>CHAPTER 6. LESS AD-HOC POLYMORPHISM 292
written. If x == y , thencompare x y should return EQ. Also, you
want your Ordinstances to define a sensible total order. You
ensure this in part by covering all cases and not writing partial
instances, as we noted above with Eq. In general, your Ord
instance should be written such that, when compare x y returns
LT, thencompare y x returns GT.
Ord implies Eq
The following isn’t going to typecheck for reasons we already
covered:
check'::a-&gt;a-&gt;Bool
check'a a'=a==a'
The error we get mentions that we need Eq, which makes
sense!
No instance for (Eq a) arising from a use of ‘==’
Possible fix:
add (Eq a) to the context of
the type signature for check' :: a -&gt; a -&gt; Bool
In the expression: a == a'
In an equation for ‘check'’: check' a a' = a == a'
But what if we add Ordinstead of Eqas it asks?</p>
<p>CHAPTER 6. LESS AD-HOC POLYMORPHISM 293
check'::Orda=&gt;a-&gt;a-&gt;Bool
check'a a'=a==a'
It should compile. Now, Ordisn’t what GHC asked for, so
why did it work? It worked because anything that provides an
instance of Ordmustby definition also already have an instance
ofEq. How do we know? As we said above, logically it makes
sense that you can’t order things without the ability to check
for equality, but we can also check :info Ord in GHCi:
Prelude&gt; :info Ord
class Eq a =&gt; Ord a where
... buncha noise we don't care about...
The class definition of Ordsays that any 𝑎which wants to
define an Ordinstance must already provide an Eqinstance. We
can say that Eqis asuperclass ofOrd.
Usually, you want the minimally sufficient set of constraints
on all your functions — so we would use Eqinstead of Ordif the
above example was “real” code — but we did this so you could
get an idea of how constraints and superclassing in Haskell
work.
Exercises: Will They Work?
Next, take a look at the following code examples and try to
decide if they will work, what result they will return if they do,</p>
<p>CHAPTER 6. LESS AD-HOC POLYMORPHISM 294
and why or why not (be sure, as always, to test them in your
REPL once you have decided on your answer):
1.max(length [ 1,2,3])
(length [ 8,9,10,11,12])
2.compare (3<em>4) (3</em>5)
3.compare &quot;Julie&quot; True
4.(5+3)&gt;(3+6)
6.9 Enum
A typeclass known as Enumthat we have mentioned previously
seems similar to Ordbut is slightly diﬀerent. This typeclass
covers types that are enumerable, therefore have known pre-
decessors and successors. We shall try not to belabor the point,
because you are probably developing a good idea of how to
query and make use of typeclass information:
Prelude&gt; :info Enum
class Enum a where
succ :: a -&gt; a
pred :: a -&gt; a
toEnum :: Int -&gt; a
fromEnum :: a -&gt; Int</p>
<p>CHAPTER 6. LESS AD-HOC POLYMORPHISM 295
enumFrom :: a -&gt; [a]
enumFromThen :: a -&gt; a -&gt; [a]
enumFromTo :: a -&gt; a -&gt; [a]
enumFromThenTo :: a -&gt; a -&gt; a -&gt; [a]
instance Enum Ordering
instance Enum Integer
instance Enum Int
instance Enum Char
instance Enum Bool
instance Enum ()
instance Enum Float
instance Enum Double
Numbers and characters are known to have predictable
successors and predecessors, so these are paradigmatic cases
of enumerability:
Prelude&gt; succ 4
5
Prelude&gt; pred 'd'
'c'
Prelude&gt; succ 4.5
5.5
You can also see that some of these functions return a result
of a list type. They take a starting value and build a list with</p>
<p>CHAPTER 6. LESS AD-HOC POLYMORPHISM 296
the succeeding items of the same type:
Prelude&gt; enumFromTo 3 8
[3,4,5,6,7,8]
Prelude&gt; enumFromTo 'a' 'f'
&quot;abcdef&quot;
Finally, let’s take a short look at enumFromThenTo :
Prelude&gt; enumFromThenTo 1 10 100
[1,10,19,28,37,46,55,64,73,82,91,100]
Take a look at the resulting list and see if you can find the
pattern: what does this function do? What happens if we give
it the values 0 10 100 instead? How about 'a' 'c' 'z' ?
6.10 Show
Showis a typeclass that provides for the creating of human-
readable string representations of structured data. GHCi uses
Showto create String values it can print in the terminal.
Showis not a serialization format. Serialization is how data
is rendered to a textual or binary format for persistence or
communicating with other computers over a network. An
example of persistence would be saving data to a file on disk.
Showis not suitable for any of these purposes; it’s expressly for
human readability.</p>
<p>CHAPTER 6. LESS AD-HOC POLYMORPHISM 297
The typeclass information looks like this (truncated):
classShowawhere
showsPrec ::Int-&gt;a-&gt;ShowS
show::a-&gt;String
showList ::[a]-&gt;ShowS
instance Showa=&gt;Show[a]
instance ShowOrdering
instance Showa=&gt;Show(Maybea)
instance ShowInteger
instance ShowInt
instance ShowChar
instance ShowBool
instance Show()
instance ShowFloat
instance ShowDouble
Importantly, we see that various number types, Boolvalues,
tuples, and characters are all already instances of Show. That is,
they have a defined ability to be printed to the screen. There is
also a function showwhich takes a polymorphic 𝑎and returns
it as aString , allowing it to be printed.</p>
<p>CHAPTER 6. LESS AD-HOC POLYMORPHISM 298
Printing and side eﬀects
When you ask GHCi to return the result of an expression and
print it to the screen, you are indirectly invoking a function
calledprintthat we encountered briefly in the chapter about
printing and again in the section about Ordand the error mes-
sage that results from passing the maxfunction too few argu-
ments. As understanding printis important to understanding
this typeclass, we’re going to digress a bit and talk about it in
more detail.
Haskell is a pure functional programming language. The
functional part of that comes from the fact that programs are
written as functions, similar to mathematical equations, in
which an operation is applied to some arguments to produce
a result. The purepart of our description of Haskell means
expressions in Haskell can be expressed exclusively in terms
of a lambda calculus.
It may not seem obvious that printing results to the screen
could be a source of worry. The function is not just applied
to the arguments that are in its scope but also asked to aﬀect
the world outside its scope in some way, namely by showing
you its result on a screen. This is known as a side eﬀect , a po-
tentially observable result apart from the value the expression
evaluates to. Haskell manages eﬀects by separating eﬀectful
computations from pure computations in ways that preserve
the predictability and safety of function evaluation. Impor-</p>
<p>CHAPTER 6. LESS AD-HOC POLYMORPHISM 299
tantly, eﬀect-bearing computations themselves become more
composable and easier to reason about. The benefits of ex-
plicit eﬀects include the fact that it makes it relatively easy to
reason about and predict the results of our functions.
What sets Haskell apart from most other functional pro-
gramming languages is that it introduced and refined a means
of writing ordinary programs that talk to the outside world
without adding anything to the pure lambda calculus it is
founded on. This property — being lambda calculus and
nothing more — is what makes Haskell a purely functional
programming language.
TheprintfunctionissometimesinvokedindirectlybyGHCi,
but its type explicitly reveals that it is eﬀectful. Up to now,
we’ve been covering over how this works, but it’s time to dive
a bit deeper.
printis defined in the Prelude standard as a function to out-
put “a value of any printable type to the standard output device.
Printable types are those that are instances of class Show; print
converts values to strings for output using the show operation
and adds a newline.” Let’s look at the type of print:
Prelude&gt; :t print
print :: Show a =&gt; a -&gt; IO ()
As we see, printtakes an argument 𝑎that must be a type with
an instance of the Showtypeclass and returns an IO ()result.
This result is an IOaction that returns a value of the type ().</p>
<p>CHAPTER 6. LESS AD-HOC POLYMORPHISM 300
We saw this IO ()result previously when we talked about
printing strings. We also noted that it is the obligatory type
ofmainin a source code file. This is because running mainonly
produces side eﬀects. indexmain@ main
Statedassimplyaspossible, anI/O(input/output, frequently
written ‘IO’ without a slash; when referring to the Haskell
datatype, there is no slash) action is an action that, when per-
formed, has side eﬀects, including reading from input and
printing to the screen and will contain a return value. The
()denotes an empty tuple, which we refer to as unit. Unit
is a value, and also a type that has only this one inhabitant,
that essentially represents nothing. Printing a string to the
terminal doesn’t have a meaningful return value. But an IO
action, like any expression in Haskell, can’t return nothing ; it
must return something. So we use this empty tuple to rep-
resent the return value at the end of our I/O action. That is,
theprintfunction will first do the I/O action of printing the
string to the terminal and then complete the action, marking
an end to the execution of the function and a delimitation of
the side eﬀects, by returning this empty nothing tuple. It does
not print the empty tuple to the screen, but it is implicitly
there. The simplest way to think about the diﬀerence between
a value with a typical type like String and the same type but
fromIOsuch as with IO String is thatIOactions are formulas.
When you have a value of type IO String it’s more of a meansof
producing aString , which may require performing side eﬀects</p>
<p>CHAPTER 6. LESS AD-HOC POLYMORPHISM 301
along the way before you get your String value.
This is a String value:
myVal::String
This value is a method or means of obtaining a value, by
performing eﬀects or I/O, of type String :
ioString ::IOString
AnIOactionisperformedwhenwecall mainforourprogram,
as we have seen. But we also perform an IOaction when we
invokeprintimplicitly or explicitly. indexmain@ main
Working with Show
Up to now, we have only been deriving typeclass instances
forShowbecause deriving usually gives us the result we want
without a lot of fuss. Having a Showinstance is crucial to being
able to print anything to the terminal, so we’re going to look at
some examples of why Showis important and how it is imple-
mented. Invoking the Showtypeclass also invokes its methods,
specifically a method of taking your values and turning them
into values that can be printed to the screen.
A minimal implementation of an instance of Showonly re-
quires that showorshowsPrec be implemented, as in the follow-
ing example:</p>
<p>CHAPTER 6. LESS AD-HOC POLYMORPHISM 302
dataMood=Blah
instance ShowMoodwhere
show_ =&quot;Blah&quot;
Prelude&gt; Blah
Blah
Here’s what happens in GHCi when you define a datatype
and ask GHCi to show it without the instance for the Show
typeclass:
Prelude&gt; data Mood = Blah
Prelude&gt; Blah
No instance for (Show Mood) arising
from a use of ‘print’
In a stmt of an interactive GHCi command: print it
Next let’s look at how you define a datatype to have an in-
stance of Show. We can derive the Showinstance for Moodbecause
it’s one of the typeclasses GHC supports deriving instances for
by default:
Prelude&gt; data Mood = Blah deriving Show
Prelude&gt; Blah
Blah</p>
<p>CHAPTER 6. LESS AD-HOC POLYMORPHISM 303
And, in fact, most of the time that’s what you’ll do for your
own datatypes. In the chapter on building projects, we will
need to write a custom instance for Show, though, so that should
give you something exciting to look forward to.
6.11 Read
TheReadtypeclass...well, it’s... there. You’ll notice that, like Show,
a lot of types have instances of Read. This typeclass is essentially
the opposite of Show. Where Showtakes things and turns them
into human-readable strings, Readtakes strings and turns them
into things. Like Show, it’s not a serialization format. So, what’s
the problem? We gave that dire warning against using Read
earlier in the chapter, but this doesn’t seem like a big deal,
right?
The problem is in the String type. A String is a list, which
could be empty in some cases, or stretch on to infinity in other
cases.
We can begin to understand this by examining the types:
Prelude&gt; :t read
read :: Read a =&gt; String -&gt; a
There’s no way Read a =&gt; String -&gt; a will always work. Let’s
consider a type like Integer which has a Readinstance. We are in
no way guaranteed that the String will be a valid representation</p>
<p>CHAPTER 6. LESS AD-HOC POLYMORPHISM 304
of anInteger value. A String value can be anytext. That’s way
too big of a type for things we want to parse into numbers!
We can prove this for ourselves in the REPL:
Prelude&gt; read &quot;1234567&quot; :: Integer
1234567
Prelude&gt; read &quot;BLAH&quot; :: Integer
*** Exception: Prelude.read: no parse
That exception is a runtime error and means that readis a
partial function , a function that doesn’t return a proper value
as a result for all possible inputs. We have ways of cleaning
this up we’ll explain and demonstrate later. We should strive
to avoid writing or using such functions in Haskell because
Haskell gives us the tools necessary to avoid senseless sources
of errors in our code.
6.12 Instances are dispatched by type
We’ve said a few times, without explaining it, that typeclasses
are dispatched by type, but it’s an important thing to under-
stand. Typeclasses are defined by the set of operations and
values all instances will provide. Typeclass instances are unique
pairings of the typeclass and a type. They define the ways to
implement the typeclass methods for that type.
We’re going to walk through some code to illustrate what
this all means. The first thing you will see is that we’ve written</p>
<p>CHAPTER 6. LESS AD-HOC POLYMORPHISM 305
our own typeclass and instances for demonstration purposes.
Those details aren’t important for understanding this code.
Just remember:
•a typeclass defines a set of functions and/or values;
•types have instances of that typeclass;
•the instances specify the ways that type uses the functions
of the typeclass.
This is vacuous and silly. This is only to make a point. Please
do not write typeclasses like this:
classNumberish awhere
fromNumber ::Integer -&gt;a
toNumber ::a-&gt;Integer
-- pretend newtype is data for now
newtype Age=
AgeInteger
deriving (Eq,Show)
instance Numberish Agewhere
fromNumber n =Agen
toNumber ( Agen)=n</p>
<p>CHAPTER 6. LESS AD-HOC POLYMORPHISM 306
newtype Year=
YearInteger
deriving (Eq,Show)
instance Numberish Yearwhere
fromNumber n =Yearn
toNumber ( Yearn)=n
Then suppose we write a function using this typeclass and
the two types and instances:
sumNumberish ::Numberish a=&gt;a-&gt;a-&gt;a
sumNumberish a a'=fromNumber summed
whereintegerOfA =toNumber a
integerOfAPrime =toNumber a'
summed=
integerOfA +integerOfAPrime
Now let us think about this for a moment. The class def-
inition of Numberish doesn’t define any termsor code we can
compile and execute, only types. The code lives in the in-
stances for AgeandYear. So how does Haskell know where to
find code?
Prelude&gt; sumNumberish (Age 10) (Age 10)
Age 20</p>
<p>CHAPTER 6. LESS AD-HOC POLYMORPHISM 307
In the above, it knew to use the instance of Numberish forAge
because it could see that our arguments to sumNumberish were
of type Age. We can see this with the type inference, too:
Prelude&gt; :t sumNumberish
sumNumberish :: Numberish a =&gt; a -&gt; a -&gt; a
Prelude&gt; :t sumNumberish (Age 10)
sumNumberish (Age 10) :: Age -&gt; Age
After the first parameter is applied to a value of type Age, it
knows that all other occurrences of type Numberish a =&gt; a must
beAge.
To see a case where we’re notproviding enough information
to Haskell for it to identify a concrete type with which to get
the appropriate instance, we’re going to change our typeclass
and associated instances:
(This is even worse than the last one. Don’t use typeclasses
to define default values. Seriously. Haskell ninjas will find you
and replace your toothpaste with muddy chalk.)
classNumberish awhere
fromNumber ::Integer -&gt;a
toNumber ::a-&gt;Integer
defaultNumber ::a</p>
<p>CHAPTER 6. LESS AD-HOC POLYMORPHISM 308
instance Numberish Agewhere
fromNumber n =Agen
toNumber ( Agen)=n
defaultNumber =Age65
instance Numberish Yearwhere
fromNumber n =Yearn
toNumber ( Yearn)=n
defaultNumber =Year1988
Then in the REPL, we can see that in some cases, there’s no
way for Haskell to know what we want!
Prelude&gt; defaultNumber
No instance for (Show a0) arising
from a use of ‘print’
The type variable ‘a0’ is ambiguous
Note: there are several potential instances:
instance Show a =&gt; Show (Maybe a)
instance Show Ordering
instance Show Integer
...plus 24 others
This fails because it has no idea what type defaultNumber is
other than that it’s provided for by Numberish ’s instances. But</p>
<p>CHAPTER 6. LESS AD-HOC POLYMORPHISM 309
the good news is, even if it’s a value and doesn’t take any argu-
ments, we have a means of telling Haskell what we want:
Prelude&gt; defaultNumber :: Age
Age 65
Prelude&gt; defaultNumber :: Year
Year 1988
Just assign the type you expect and it works fine! Here,
Haskell is using the type assertion to dispatch , or specify, what
typeclass instance we want to get our defaultNumber from.
Why not write a typeclass like this? For reasons we’ll explain
when we talk about Monoid , it’s important that your typeclasses
have laws and rules about how they work. Numberish is a bit...
arbitrary. There are better ways to express what it does in
Haskell than a typeclass. Functions and values alone suffice
here.
6.13 Gimme more operations
We talked about the diﬀerent kinds of polymorphism in type
signatures — constrained versus parametric. Having no con-
straint on our term-level values means they could be any type,
but there isn’t much we can do with them. The methods and
operations are in the typeclasses, and so we get more utility</p>
<p>CHAPTER 6. LESS AD-HOC POLYMORPHISM 310
by specifying typeclass constraints. If your types are more
general than your terms are, then you need to constrain your
types with the typeclasses that provide the operations you
want to use. We looked at some examples of this in the sec-
tions above about Integral andFractional , but in this section,
we’ll be more specific about how to modify type signatures to
fit the terms.
We’ll start by looking at some examples of times when we
need to change our types because they’re more general than
our terms allow:
add::a-&gt;a-&gt;a
addx y=x+y
If you load it up, you’ll get the following error:
No instance for (Num a) arising from a use of ‘+’
Possible fix:
add (Num a) to the context of
the type signature for add :: a -&gt; a -&gt; a
Fortunately, this is one of those cases where GHC knows
precisely what the problem is and how to remedy it. We need
to add a Numconstraint to the type 𝑎. But why? Because our
function can’t accept a value of strictly anytype. We need
something that has an instance of Numbecause the (+)function
comes from Num:</p>
<p>CHAPTER 6. LESS AD-HOC POLYMORPHISM 311
add::Numa=&gt;a-&gt;a-&gt;a
addx y=x+y
With the constraint added to the type, it works fine! What
if we use a method from another operation?
addWeird ::Numa=&gt;a-&gt;a-&gt;a
addWeird x y=
ifx&gt;1
thenx+y
elsex
We get another error, but once again GHC helps us out, so
long as we resist the pull of tunnel vision5and look at what it’s
telling us:
Could not deduce (Ord a) arising from a use of ‘&gt;’
from the context (Num a)
bound by the type signature for
addWeird :: Num a =&gt; a -&gt; a -&gt; a
Possible fix:
add (Ord a) to the context of
the type signature for
addWeird :: Num a =&gt; a -&gt; a -&gt; a
5All programmers experience this. Just slow down and you’ll be okay.</p>
<p>CHAPTER 6. LESS AD-HOC POLYMORPHISM 312
The problem is that having a Numconstraint on our type 𝑎
isn’t enough. Numdoesn’t imply Ord. Given that, we have to add
another constraint which is what GHC told us to do:
addWeird ::(Orda,Numa)=&gt;a-&gt;a-&gt;a
addWeird x y=
ifx&gt;1
thenx+y
elsex
Now this should typecheck because our constraints are ask-
ing that 𝑎have instances of NumandOrd.
Concrete types imply all the typeclasses they
provide
We’ll be repurposing some examples from earlier in the chap-
ter, modifying them to all have a concrete type in the place of
𝑎:</p>
<p>CHAPTER 6. LESS AD-HOC POLYMORPHISM 313
add::Int-&gt;Int-&gt;Int
addx y=x+y
addWeird ::Int-&gt;Int-&gt;Int
addWeird x y=
ifx&gt;1
thenx+y
elsex
check'::Int-&gt;Int-&gt;Bool
check'a a'=a==a'
These will all typecheck! This is because the Inttype has
the typeclasses Num,Eq, andOrdall implemented. We don’t need
to sayOrd Int =&gt; Int -&gt; Int -&gt; Int because it doesn’t add any
information. A concrete type either has a typeclass instance or
it doesn’t — adding the constraint means nothing. A concrete
type always implies the typeclasses that are provided for it.
There are some caveats to keep in mind here when it comes
to using concrete types. One of the nice things about para-
metricity and typeclasses is that you are being explicit about
what you mean to do withyour data which means you are
less likely to make a mistake. Intis a big datatype with many
inhabitants and many typeclasses and operations defined for
it — it could be easy to make a function that does something
unintended. Whereas if we were to write a function, even if we</p>
<p>CHAPTER 6. LESS AD-HOC POLYMORPHISM 314
hadIntvalues in mind for it, which used a polymorphic type
constrained by the typeclass instances we wanted, we could
ensure we only used the operations we intended. This isn’t a
panacea, but it can be worth avoiding concrete types for these
(and other) reasons sometimes.
6.14 Chapter Exercises
Multiple choice
1.The Eq class
a)includes all types in Haskell
b)is the same as the Ord class
c)makes equality tests possible
d)only includes numeric types
2.The typeclass Ord
a)allows any two values to be compared
b)is a subclass of Eq
c)is a superclass of Eq
d)has no instance for Bool
3.Suppose the typeclass Ord has an operator &gt;. What is the
type of &gt;?</p>
<p>CHAPTER 6. LESS AD-HOC POLYMORPHISM 315
a)Ord a =&gt; a -&gt; a -&gt; Bool
b)Ord a =&gt; Int -&gt; Bool
c)Ord a =&gt; a -&gt; Char
d)Ord a =&gt; Char -&gt; [Char]
4.Inx = divMod 16 12
a)the type of 𝑥is Integer
b)the value of 𝑥is undecidable
c)the type of 𝑥is a tuple
d)𝑥is equal to 12 / 16
5.The typeclass Integral includes
a)Int and Integer numbers
b)integral, real, and fractional numbers
c)Schrodinger’s cat
d)only positive numbers
Does it typecheck?
For this section of exercises, you’ll be practicing looking for
type and typeclass errors.
For example, printIt will not work because functions like
𝑥have no instance of Show, the typeclass that lets you convert
things to Strings (usually for printing):</p>
<p>CHAPTER 6. LESS AD-HOC POLYMORPHISM 316
x::Int-&gt;Int
xblah=blah+20
printIt ::IO()
printIt =putStrLn (show x)
Here’s the type error you get if you try to load the code:
No instance for (Show (Int -&gt; Int)) arising
from a use of ‘show’
In the first argument of ‘putStrLn’, namely ‘(show x)’
In the expression: putStrLn (show x)
In an equation for ‘printIt’: printIt = putStrLn (show x)
It’s saying it can’t find an implementation of the typeclass
Showfor the type Int -&gt; Int , which makes sense. Nothing with
the function type constructor (-&gt;)has an instance of Show6by
default in Haskell.
Examine the following code and decide whether it will type-
check. Then load it in GHCi and see if you were correct. If
it doesn’t typecheck, try to match the type error against your
understanding of why it didn’t work. If you can, fix the error
and re-run the code.
6For an explanation and justification of why functions in Haskell cannot have a
Showinstance, see the wiki page on this topic. https://wiki.haskell.org/Show_instance_for_
functions</p>
<p>CHAPTER 6. LESS AD-HOC POLYMORPHISM 317
1.Does the following code typecheck? If not, why not?
dataPerson=PersonBool
printPerson ::Person-&gt;IO()
printPerson person=putStrLn (show person)
2.Does the following typecheck? If not, why not?
dataMood=Blah
|Wootderiving Show
settleDown x= ifx==Woot
thenBlah
elsex
3.If you were able to get settleDown to typecheck:
a)What values are acceptable inputs to that function?
b)What will happen if you try to run settleDown 9 ? Why?
c)What will happen if you try to run Blah &gt; Woot ? Why?
4.Does the following typecheck? If not, why not?</p>
<p>CHAPTER 6. LESS AD-HOC POLYMORPHISM 318
typeSubject =String
typeVerb=String
typeObject=String
dataSentence =
Sentence Subject VerbObject
deriving (Eq,Show)
s1=Sentence &quot;dogs&quot;&quot;drool&quot;
s2=Sentence &quot;Julie&quot; &quot;loves&quot; &quot;dogs&quot;
Given a datatype declaration, what can we do?
Given the following datatype definitions:
dataRocks=
RocksStringderiving (Eq,Show)
dataYeah=
YeahBoolderiving (Eq,Show)
dataPapu=
PapuRocksYeah
deriving (Eq,Show)</p>
<p>CHAPTER 6. LESS AD-HOC POLYMORPHISM 319
Which of the following will typecheck? For the ones that
don’t typecheck, why don’t they?
1.phew=Papu&quot;chases&quot; True
2.truth=Papu(Rocks&quot;chomskydoz&quot; )
(YeahTrue)
3.equalityForall ::Papu-&gt;Papu-&gt;Bool
equalityForall p p'=p==p'
4.comparePapus ::Papu-&gt;Papu-&gt;Bool
comparePapus p p'=p&gt;p'
Match the types
We’re going to give you two types and their implementations.
Then we’re going to ask you if you can substitute the second
typeforthefirst. Youcantestthisbytypingthefirstdeclaration
and its type into a file and editing in the new one, loading to
see if it fails. Don’tguess, test all your answers!
1.For the following definition.
a)i::Numa=&gt;a
i=1
b)Try replacing the type signature with the following:</p>
<p>CHAPTER 6. LESS AD-HOC POLYMORPHISM 320
i::a
After you’ve formulated your own answer, then tested
that answer and believe you understand why you were
right or wrong, make sure to use GHCi to check what
type GHC infersfor the definitions we provide without
a type assigned. For example, for this one, you’d type
in:
Prelude&gt; let i = 1
Prelude&gt; :t i
-- Result elided intentionally.
2.a)f::Float
f=1.0
b)f::Numa=&gt;a
3.a)f::Float
f=1.0
b)f::Fractional a=&gt;a
4.Hint for the following: type :info RealFrac in your REPL.
a)f::Float
f=1.0
b)f::RealFrac a=&gt;a</p>
<p>CHAPTER 6. LESS AD-HOC POLYMORPHISM 321
5.a)freud::a-&gt;a
freudx=x
b)freud::Orda=&gt;a-&gt;a
6.a)freud'::a-&gt;a
freud'x=x
b)freud'::Int-&gt;Int
7.a)myX=1::Int
sigmund ::Int-&gt;Int
sigmund x=myX
b)sigmund ::a-&gt;a
8.a)myX=1::Int
sigmund' ::Int-&gt;Int
sigmund' x=myX
b)sigmund' ::Numa=&gt;a-&gt;a
9.a)You’ll need to import sortfromData.List .
jung::Orda=&gt;[a]-&gt;a
jungxs=head (sort xs)
b)jung::[Int]-&gt;Int
10.a)young::[Char]-&gt;Char
youngxs=head (sort xs)</p>
<p>CHAPTER 6. LESS AD-HOC POLYMORPHISM 322
b)young::Orda=&gt;[a]-&gt;a
11.a)mySort::[Char]-&gt;[Char]
mySort=sort
signifier ::[Char]-&gt;Char
signifier xs=head (mySort xs)
b)signifier ::Orda=&gt;[a]-&gt;a
Type-Kwon-Do Two: Electric Typealoo
Round Two! Same rules apply — you’re trying to fill in terms
(code) which’ll fit the type. The idea with these exercises is that
you’ll derive the implementation from the type information.
You’ll probably need to use stuﬀ from Prelude.
1.chk::Eqb=&gt;(a-&gt;b)-&gt;a-&gt;b-&gt;Bool
chk= ???
2.-- Hint: use some arithmetic operation to
-- combine values of type 'b'. Pick one.
arith::Numb
=&gt;(a-&gt;b)
-&gt;Integer
-&gt;a
-&gt;b
arith= ???</p>
<p>CHAPTER 6. LESS AD-HOC POLYMORPHISM 323
6.15 Chapter Definitions
1.Typeclass inheritance is when a typeclass has a superclass.
This is a way of expressing that a typeclass requires another
typeclass to be available for a given type before you can
write an instance.
classNuma=&gt;Fractional awhere
(/)::a-&gt;a-&gt;a
recip::a-&gt;a
fromRational ::Rational -&gt;a
Here the typeclass Fractional inherits fromNum. We could
also say that Numis asuperclass ofFractional . The long
and short of it is that if you want to write an instance of
Fractional for some 𝑎, that type 𝑎, must already have an
instance of Numbefore you may do so.</p>
<p>CHAPTER 6. LESS AD-HOC POLYMORPHISM 324
-- Even though in principle
-- this could work, it will fail because
-- Nada doesn't have a Num instance
newtype Nada=
NadaDoublederiving (Eq,Show)
instance Fractional Nadawhere
(Nadax)/(Naday)=Nada(x/y)
recip (Nadan)=Nada(recip n)
fromRational r =Nada(fromRational r)
Then if you try to load it:
No instance for (Num Nada)
arising from the superclasses
of an instance declaration
In the instance declaration for
‘Fractional Nada’
You need a Numinstance first. Can’t write one that makes
sense? Then you’re not allowed to have a Fractional in-
stance either. Them’s the rules.
2.Eﬀects are how we refer to observable actions programs may
take other than compute a value. If a function modifies</p>
<p>CHAPTER 6. LESS AD-HOC POLYMORPHISM 325
some state or interacts with the outside world in a manner
that can be observed, then we say it has an eﬀecton the
world.
3.IOis the type for values whose evaluation bears the possi-
bility of causing side eﬀects, such as printing text, reading
text input from the user, reading or writing files, or con-
necting to remote computers. This will be explained in
muchmore depth in the chapter on IO.
4.Aninstance is the definition of how a typeclass should
work for a given type. Instances are unique for a given
combination of typeclass and type.
5.In Haskell we have derivedinstances so that obvious or com-
mon typeclasses, such as Eq,Enum,Ord, andShowcan have
the instances generated based only on how the datatype
is defined. This is so programmers can make use of these
conveniences without writing the code themselves, over
and over.</p>
<p>CHAPTER 6. LESS AD-HOC POLYMORPHISM 326
6.16 Typeclass inheritance, partial
This is not a complete chart of typeclass inheritance. It illus-
trates the relationship between a few of the typeclasses we’ve
talked about in this chapter. You can see, for example, that
the subclass Fractional inherits from the superclass Numbut not
vice versa. While many types have instances of ShowandRead,
they aren’t superclasses, so we’ve left them out of the chart for
clarity.
Figure 6.1: Chart of some typeclasses and their parentage.
Only the typeclasses seen so far are included.
6.17 Follow-up resources
1.P. Wadler and S. Blott. How to make ad-hoc polymor-
phism less ad hoc.</p>
<p>CHAPTER 6. LESS AD-HOC POLYMORPHISM 327
http://www.cse.iitk.ac.in/users/karkare/courses/2010/cs653/
Papers/ad-hoc-polymorphism.pdf
2.Cordelia V. Hall, Kevin Hammond, Simon L. Peyton Jones,
and Philip L. Wadler. Typeclasses in Haskell.
http://ropas.snu.ac.kr/lib/dock/HaHaJoWa1996.pdf</p>
<p>Chapter 7
More functional patterns
I would like to be able to
always…divide the things
up into as many pieces as
I can, each of which I
understand separately. I
would like to understand
the way of adding things
up, independently of
what it is I’m adding up.
Gerald Sussman
328</p>
<p>CHAPTER 7. MORE FUNCTIONAL PATTERNS 329
7.1 Make it func-y
You might be asking yourself what this chapter is all about:
haven’t we been talking about functions all along? We have,
but as you might guess from the fact that Haskell is a functional
programming language, there is more to say — so much more!
A function is an instruction for producing an output from
an input, or argument. Functions are applied to arguments
which binds their parameters to values. The fully applied
function with its arguments is then evaluated to produce the
output or result. In this chapter we will demonstrate
•Haskell functions are first-class entities that
•can be values in expressions, lists, or tuples;
•can be passed as arguments to a function;
•can be returned from a function as a result;
•make use of syntactic patterns.
7.2 Arguments and parameters
As you know from our discussion of currying, functions in
Haskell may appear to have multiple parameters but this is
only the surface appearance; in fact, all functions take one
argument and return one result. We construct functions in</p>
<p>CHAPTER 7. MORE FUNCTIONAL PATTERNS 330
Haskell through various syntactic means of denoting that an
expression takes arguments. Functions are defined by the fact
that they can be applied to an argument and return a result.
All Haskell values can be arguments to functions. A value
that can be used as an argument to a function is a first-class
value. In Haskell, this includes functions, which can be argu-
ments to more functions still. Not all programming languages
allow this, but hopefully the earlier discussion of the function
type and currying have given an idea of how and why this
works.
Setting parameters
You name parameters to functions in Haskell by declaring
them between the name of the function, which is always at
the left margin, and the equals sign, separating the name from
both the function name and the equals sign with white space.
The name is a variable, and when we apply the function to
an argument, the value of the argument is bound, or unified,
with the named parameter in our function definition.
First we’ll define a value with no parameters:
myNum::Integer
myNum=1
myVal=myNum</p>
<p>CHAPTER 7. MORE FUNCTIONAL PATTERNS 331
If we query the type of myVal:
Prelude&gt; :t myVal
myVal :: Integer
The value myValhas the same type as myNumbecause it is equal
to it. We can see from the type that it’s a value without any
parameters, so we can’t apply it to anything.
Now let’s introduce a parameter named 𝑓:
myNum::Integer
myNum=1
myValf=myNum
And let’s see how that changed the type:
Prelude&gt; :t myVal
myVal :: t -&gt; Integer
Bywriting 𝑓aftermyValweparameterized myVal, whichchanges
the type from Integer tot -&gt; Integer . The type 𝑡is polymor-
phic because we don’t do anything with it — it could be any-
thing. We didn’t do anything with 𝑓so the maximally poly-
morphic type was inferred. If we do something with 𝑓, the
type will change:</p>
<p>CHAPTER 7. MORE FUNCTIONAL PATTERNS 332
Prelude&gt; let myNum = 1 :: Integer
Prelude&gt; let myVal f = f + myNum
Prelude&gt; :t myVal
myVal :: Integer -&gt; Integer
Now it knows 𝑓has to be of type Integer because we added
it tomyNum.
We can tell a simple value from a function in part because
a value is not applied to any arguments, while functions nec-
essarily have parameters that can be applied to arguments.
Although Haskell functions only take one argument per
function, we can declare multiple parameters in a term-level
function definition:
myNum::Numa=&gt;a
myNum=1
-- [1]
myVal::Numa=&gt;a-&gt;a
myValf=f+myNum
-- [2]
stillAFunction ::[a]-&gt;[a]-&gt;[a]-&gt;[a]
stillAFunction a b c=a++b++c
-- [ 3 ]</p>
<p>CHAPTER 7. MORE FUNCTIONAL PATTERNS 333
1.Declaration of a value of type Num a =&gt; a . We can tell it’s
not a function because no parameters are named between
the name of the declared value and the =, so it accepts no
arguments, and the value 1 is not a function.
2.Here𝑓is a name for a parameter to the function myVal. It
represents the possibility of being applied to, or bound
to, an input value. The function type is Num a =&gt; a -&gt; a .
If you assign the type Integer tomyNum, as we had above,
myNumandmyValwould have the types Integer andInteger
-&gt; Integer , respectively.
3.Here𝑎,𝑏, and𝑐represent parameters for the function.
The underlying logic is of nested functions each applied
to one argument, rather than one function taking several
arguments, but this is how it appears at term level.
Notice what happens to the types as we name more param-
eters:
Prelude&gt; let myVal f g = myNum
Prelude&gt; :t myVal
myVal :: t -&gt; t1 -&gt; Integer
Prelude&gt; let myVal f g h = myNum
Prelude&gt; :t myVal
myVal :: t -&gt; t1 -&gt; t2 -&gt; Integer</p>
<p>CHAPTER 7. MORE FUNCTIONAL PATTERNS 334
Here the types are t,t1, andt2which could be diﬀerent
types. They are allowed but notrequired to be diﬀerent types.
They’re all polymorphic because we gave the type inference
nothing to go on with respect to what type they could be. The
type variables are diﬀerent because nothing in our code is
preventing them from varying, so they are potentially dif-
ferent types. The inference infers the most polymorphic type
that works.
Binding variables to values
Let’s consider how the binding of variables works. Applying
a function binds its parameters to values. Type parameters
become bound to a type, and function variables are bound
to a value. The binding of variables concerns not only the
application of function arguments, but also things like let
expressions and whereclauses. Consider the following function:
addOne::Integer -&gt;Integer
addOnex=x+1
We don’t know the result until the addOne function is applied
to anInteger value argument. When addOne is applied to a
value, we say that 𝑥is nowbound to the value the function was
applied to. Until a function’s arguments have been applied,</p>
<p>CHAPTER 7. MORE FUNCTIONAL PATTERNS 335
thereby binding the parameters to values, we cannot make use
of the result of the function.
addOne1-- x is now bound to 1
addOne1=1+1
=2
addOne10-- x is bound to 10
addOne10=10+1
=11
In addition to binding variables through function applica-
tion, we can use letexpressions to declare and bind variables
as well:
bindExp ::Integer -&gt;String
bindExp x=
lety=5in
&quot;the integer was: &quot; ++show x
++&quot; and y was: &quot; ++show y
Inshow y ,𝑦is in scope because the letexpression binds the
variable 𝑦to 5.𝑦is only in scope insidetheletexpression. Let’s
see something that won’t work:</p>
<p>CHAPTER 7. MORE FUNCTIONAL PATTERNS 336
bindExp ::Integer -&gt;String
bindExp x=
letz=y+xin
lety=5in
&quot;the integer was: &quot;
++show x++&quot; and y was: &quot;
++show y++&quot; and z was: &quot;
++show z
You should see an error, “Not in scope: ‘y’”. We are trying
to make 𝑧equal a value constructed from 𝑥and𝑦.𝑥is in
scope because the function argument is visible anywhere in
the function. However, 𝑦is bound in the expression that let z
= …wraps, so it’s not in scope yet — that is, it’s not visible to
the main function.
In some cases, function arguments are not visible in the
function if they have been shadowed. Let’s look at a case of
shadowing :
bindExp ::Integer -&gt;String
bindExp x=
letx=10; y=5in
&quot;the integer was: &quot; ++show x
++&quot; and y was: &quot; ++show y
If you apply this to an argument, you’ll notice the result
never changes:</p>
<p>CHAPTER 7. MORE FUNCTIONAL PATTERNS 337
Prelude&gt; bindExp 9001
&quot;the integer was: 10 and y was: 5&quot;
This is because the reference to 𝑥arising from the argument
𝑥was shadowed by the 𝑥from the letbinding. The definition
of𝑥that is innermost in the code (where the function name at
the left margin is the outside ) takes precedence because Haskell
islexically scoped . Lexical scoping means that resolving the
value for a named entity depends on the location in the code
and the lexical context, for example in letandwhereclauses.
Among other things, this makes it easier to know what values
referred to by name are and where they come from. Let’s
annotate the previous example and we’ll see what is meant
here:
bindExp ::Integer -&gt;String
bindExp x= letx=10
-- [1] [2]
y=5
in&quot;x: &quot;++show x
-- [3]
++&quot; y: &quot;++show y
1.The parameter 𝑥introduced in the definition of bindExp .
This gets shadowed by the 𝑥in[2].</p>
<p>CHAPTER 7. MORE FUNCTIONAL PATTERNS 338
2.This is a let-binding of 𝑥and shadows the definition of 𝑥
introduced as an argument at [1].
3.A use of the 𝑥bound by [2]. Given Haskell’s static (lexical)
scoping it will always refer to the 𝑥defined as x = 10 in the
letbinding!
You can also see the eﬀect of shadowing a name in scope in
GHCi using the letstatements you’ve been kicking around all
along:
Prelude&gt; let x = 5
Prelude&gt; let y = x + 5
Prelude&gt; y
10
Prelude&gt; y * 10
100
Prelude&gt; let z y = y * 10
Prelude&gt; x
5
Prelude&gt; y
10
Prelude&gt; z 9
90
-- but
Prelude&gt; z y</p>
<p>CHAPTER 7. MORE FUNCTIONAL PATTERNS 339
100
Note that while 𝑦is bound in GHCi’s scope to x + 5, the
introduction of z y = y * 10 creates a new inner scope which
shadowed the name𝑦. Now, when we call 𝑧, GHCi will use the
value we pass as 𝑦to evaluate the expression, not necessarily
the value 10from the letstatement y = x + 5 . Using 𝑦as an ar-
gument to 𝑧, as in the last example, means the value of 𝑦from
the outer scope is passed to 𝑧as an argument. The lexically
innermost binding for a variable of a particular name always
takes precedence. It does not matter that the 𝑦in𝑧’s parame-
ters has the same name as the 𝑦from earlier in GHCi: 𝑦will
always be bound to the value that 𝑧is applied to. (Incidentally,
the seeming-sequentiality of defining things in GHCi is, under
the hood, a never-ending series of nested lambda expressions,
similar to the way functions can seem to accept multiple argu-
ments but are, at root, a series of nested functions).
7.3 Anonymous functions
We have already seen how to write anonymous functions using
the lambda syntax represented by a backslash. Anonymous
means “without a name” and that gives us a clue to why we have
this syntax — to construct functions and use them without
giving them a name.</p>
<p>CHAPTER 7. MORE FUNCTIONAL PATTERNS 340
Forexample, earlierwelookedatthisnamed, i.e., not anony-
mous, function:
triple::Integer -&gt;Integer
triplex=x<em>3
And here is the same function but with anonymous function
syntax:
(\x-&gt;x</em>3)::Integer -&gt;Integer
You need the parentheses for the type assertion :: Integer
-&gt; Integer to apply to the entire anonymous function and not
just the Num a =&gt; a value 3. You can give this function a name,
making it not anonymous anymore, in GHCi like this:
Prelude&gt; :{
*Main| let trip :: Integer -&gt; Integer
<em>Main| trip = \x -&gt; x</em>3
*Main| :}
Prelude&gt;
Similarly, to apply an anonymous function we’ll often need
to wrap it in parentheses so that our intent is clear:
Prelude&gt; (\x -&gt; x * 3) 5
15
Prelude&gt; \x -&gt; x * 3 1</p>
<p>CHAPTER 7. MORE FUNCTIONAL PATTERNS 341
Could not deduce (Num (a0 -&gt; a))</p>
<div style="break-before: page; page-break-before: always;"></div><p>arising from the ambiguity check for ‘it’
from the context
(Num (a1 -&gt; a), Num a1, Num a)
bound by the inferred type for ‘it’:
(Num (a1 -&gt; a), Num a1, Num a) =&gt; a -&gt; a
at <interactive>:9:1-13
The type variable ‘a0’ is ambiguous
When checking that ‘it’
has the inferred type ‘forall a a1.
(Num (a1 -&gt; a), Num a1, Num a) =&gt;
a -&gt; a’
Probable cause:
the inferred type is ambiguous
The type error Could not deduce (Num (a0 -&gt; a)) is because
you can’t use Num a =&gt; a values as if they were functions. To
the computer, it looks like you’re trying to use 3 as a function
and apply 3 to 1. Here the itreferred to is 3 1which it thinks
is3applied to 1as if3were a function.1
1In GHCi error messages, itrefers to the last expression you entered.</p>
<p>CHAPTER 7. MORE FUNCTIONAL PATTERNS 342
Exercises: Grab Bag
Note the following exercises are from source code files, not
written for use directly in the REPL. Of course, you can change
them to test directly in the REPL if you prefer.
1.Which (two or more) of the following are equivalent?
a)mThx y z=x<em>y</em>z
b)mThx y=\z-&gt;x<em>y</em>z
c)mThx=\y-&gt;\z-&gt;x<em>y</em>z
d)mTh=\x-&gt;\y-&gt;\z-&gt;x<em>y</em>z
2.The type of mTh(above) is Num a =&gt; a -&gt; a -&gt; a -&gt; a .
Which is the type of mTh 3?
a)Integer -&gt; Integer -&gt; Integer
b)Num a =&gt; a -&gt; a -&gt; a -&gt; a
c)Num a =&gt; a -&gt; a
d)Num a =&gt; a -&gt; a -&gt; a
3.Next, we’ll practice writing anonymous lambda syntax.
For example, one could rewrite:
addOnex=x+1
Into:</p>
<p>CHAPTER 7. MORE FUNCTIONAL PATTERNS 343
addOne=\x-&gt;x+1
Try to make it so it can still be loaded as a top-level def-
inition by GHCi. This will make it easier to validate your
answers.
a)Rewrite the ffunction in the where clause.
addOneIfOdd n= caseodd nof
True-&gt;f n
False-&gt;n
wheref n=n+1
b)Rewrite the following to use anonymous lambda syn-
tax:
addFive x y=(ifx&gt;ythenyelsex)+5
c)Rewrite the following so that it doesn’t use anony-
mous lambda syntax:
mflipf=\x-&gt;\y-&gt;f y x
The utility of lambda syntax
You’re going to see this anonymous syntax a lot as we proceed
through the book, but right now it may not seem to be that
useful — it’s just another way to write functions.
You most often use this syntax when you’re passing a func-
tion in as an argument to a higher-order function (more on
this soon!) and that’s the only place in your program where</p>
<p>CHAPTER 7. MORE FUNCTIONAL PATTERNS 344
that particular function will be used. If you’re never going to
call it, then it doesn’t need to be given a name.
We won’t go into a lot of detail about this yet, but named
entities and anonymous entities evaluate a bit diﬀerently in
Haskell, and that can be one reason to use an anonymous
function in some cases.
7.4 Pattern matching
Pattern matching is an integral and ubiquitous feature of
Haskell — so integral and ubiquitous that we’ve been using it
throughout the book without saying anything about it. Once
you start, you can’t stop.
Pattern matching is a way of matching values against pat-
terns and, where appropriate, binding variables to successful
matches. It is worth noting here that patterns can include things
as diverse as undefined variables, numeric literals, and list syn-
tax. As we will see, pattern matching matches on any and all
data constructors.
Pattern matching allows you to expose data and dispatch
diﬀerent behaviors based on that data in your function defini-
tions by deconstructing values to expose their inner workings.
There is a reason we describe values as “data constructors ”, al-
though we haven’t explored that much yet. Pattern matching
also allows us to write functions that can decide between two
or more possibilities based on which value it matches.</p>
<p>CHAPTER 7. MORE FUNCTIONAL PATTERNS 345
Patterns are matched against values, or data constructors,
nottypes. Matching a pattern may fail, proceeding to the next
available pattern to match or succeed. When a match suc-
ceeds, the variables exposed in the pattern are bound. Pattern
matching proceeds from left to right and outside to inside.
We can pattern match on numbers. In the following exam-
ple, when the Integer argument to the function equals 2, this
returns True, otherwise, False:
isItTwo ::Integer -&gt;Bool
isItTwo 2=True
isItTwo _ =False
You can enter the same function directly into GHCi using
the:{and:}block syntax, enter :}and “return” to end the
block.
Prelude&gt; :{
*Main| let isItTwo :: Integer -&gt; Bool
*Main| isItTwo 2 = True
*Main| isItTwo _ = False
*Main| :}
Note the use of the underscore _after the match against the
value2. This is a means of defining a universal pattern that
never fails to match, a sort of “anything else” case.</p>
<p>CHAPTER 7. MORE FUNCTIONAL PATTERNS 346
Prelude&gt; isItTwo 2
True
Prelude&gt; isItTwo 3
False
Handling all the cases
The order of pattern matches matters! The following version
of the function will always return Falsebecause it will match
the “anything else” case first — and match it to everything —
so nothing will get through that to match with the pattern you
do want to match:
isItTwo ::Integer -&gt;Bool
isItTwo _ =False
isItTwo 2=True
<interactive>:9:33: Warning:
Pattern match(es) are overlapped
In an equation for ‘isItTwo’:
isItTwo 2 = ...
Prelude&gt; isItTwo 2
False
Prelude&gt; isItTwo 3
False</p>
<p>CHAPTER 7. MORE FUNCTIONAL PATTERNS 347
Try to order your patterns from most specific to least spe-
cific, particularly as it concerns the use of _to unconditionally
match any value. Unless you get fancy, you should be able
to trust GHC’s pattern match overlap warning and should
triple-check your code when it complains.
What happens if we forget to match a case in our pattern?
isItTwo ::Integer -&gt;Bool
isItTwo 2=True
Notice that now our function can only pattern match on the
value 2. This is an incomplete pattern match because it can’t
match any other data. Incomplete pattern matches applied to
data they don’t handle will return bottom , a non-value used to
denote that the program cannot return a value or result. This
will throw an exception, which if unhandled, will make your
program fail:
Prelude&gt; isItTwo 2
True
Prelude&gt; isItTwo 3
*** Exception: :50:33-48:
Non-exhaustive patterns
in function isItTwo
We’re going to get well acquainted with the idea of bottom
in upcoming chapters. For now, it’s enough to know that this
is what you get when you don’t handle all the possible data.</p>
<p>CHAPTER 7. MORE FUNCTIONAL PATTERNS 348
Fortunately, there’s a way to know at compile time when
your pattern matches are non-exhaustive and don’t handle
every case:
Prelude&gt; :set -Wall
Prelude&gt; :{
*Main| let isItTwo :: Integer -&gt; Bool
*Main| isItTwo 2 = True
*Main| :}
<interactive>:28:5: Warning:
This binding for ‘isItTwo’ shadows
the existing binding
defined at <interactive>:20:5
<interactive>:28:5: Warning:
Pattern match(es) are non-exhaustive
In an equation for ‘isItTwo’:
Patterns not matched:
#x with #x <code>notElem</code> [2#]
By turning on all warnings with -Wall, we’re now told ahead
of time that we’ve made a mistake. Do notignore the warnings
GHC provides for you!</p>
<p>CHAPTER 7. MORE FUNCTIONAL PATTERNS 349
Pattern matching against data constructors
Pattern matching serves a couple of purposes. It enables us to
vary what our functions do given diﬀerent inputs. It also allows
us to unpack and expose the contents of our data. The values
TrueandFalsedon’t have any other data to expose, but some
data constructors have parameters, and pattern matching can
let us expose and make use of the data in their arguments.
The next example uses newtype which is a special case of data
declarations. newtype is diﬀerent in that it permits only one
constructor and only one field. We will talk about newtype more
later. For now, we want to focus on how pattern matching can
be used to expose the contents of data and specify behavior
based on that data:</p>
<p>CHAPTER 7. MORE FUNCTIONAL PATTERNS 350
-- registeredUser1.hs
moduleRegisteredUser where
newtype Username =
Username String
newtype AccountNumber =
AccountNumber Integer
dataUser=
UnregisteredUser
|RegisteredUser Username AccountNumber
With the type User, we can use pattern matching to ac-
complish two things. First, Useris a sum with two construc-
tors,UnregisteredUser andRegisteredUser . We can use pattern
matching to dispatch our function diﬀerently depending on
which value we get. Then with the RegisteredUser construc-
tor we see that it is a product of two newtype s,Username and
AccountNumber . We can use pattern matching to break down
not only RegisteredUser ’s contents, but also that of the newtype s
if all the constructors are in scope. Let’s write a function to
pretty-print Uservalues:</p>
<p>CHAPTER 7. MORE FUNCTIONAL PATTERNS 351
-- registeredUser2.hs
moduleRegisteredUser where
newtype Username =
Username String
newtype AccountNumber =
AccountNumber Integer
dataUser=
UnregisteredUser
|RegisteredUser Username AccountNumber
printUser ::User-&gt;IO()
printUser UnregisteredUser =
putStrLn &quot;UnregisteredUser&quot;
printUser (RegisteredUser
(Username name)
(AccountNumber acctNum)) =
putStrLn $name++&quot; &quot;++show acctNum
Note that you can continue the pattern on the next line if it
gets too long. Next, let’s load this into the REPL and look at
the types:
Prelude&gt; :l code/registeredUser2.hs</p>
<p>CHAPTER 7. MORE FUNCTIONAL PATTERNS 352
...
Prelude&gt; :t RegisteredUser
RegisteredUser :: Username
-&gt; AccountNumber
-&gt; User
Prelude&gt; :t Username
Username :: String -&gt; Username
Prelude&gt; :t AccountNumber
AccountNumber :: Integer -&gt; AccountNumber
Notice how the type of RegisteredUser is a function that con-
structs a Userout of two arguments: Username andAccountNumber .
This is what we mean when we refer to a value as a “data con-
structor.”
Now, let’s use our functions. The argument names are te-
dious to type in, but they were chosen to ensure clarity. Passing
the function an UnregisteredUser returns the expected value:
Prelude&gt; printUser UnregisteredUser
UnregisteredUser
The following, though, asks it to match on data constructor
RegisteredUser and allows us to construct a Userout of the String
“callen” and the Integer 10456:
Prelude&gt; let myUser = Username &quot;callen&quot;
Prelude&gt; let myAcct = AccountNumber 10456</p>
<p>CHAPTER 7. MORE FUNCTIONAL PATTERNS 353
Prelude&gt; :{
*Main| let rUser =
*Main| RegisteredUser myUser myAcct
*Main| :}
Prelude&gt; printUser rUser
callen 10456
Through the use of pattern matching, we were able to un-
pack the RegisteredUser value of the Usertype and vary behav-
ior over the diﬀerent constructors of types.
This idea of unpacking and dispatching on data is impor-
tant, so let us examine another example. First, we’re going to
write a couple of new datatypes. Writing your own datatypes
won’t be fully explained until a later chapter, but most of the
structure here should be familiar already. We have a sum type
calledWherePenguinsLive :
dataWherePenguinsLive =
Galapagos
|Antarctica
|Australia
|SouthAfrica
|SouthAmerica
deriving (Eq,Show)
And a product type called Penguin . We haven’t given product
types much attention yet, but for now you can think of Penguin</p>
<p>CHAPTER 7. MORE FUNCTIONAL PATTERNS 354
as a type with only one value, Peng, and that value is a sort of
box that contains a WherePenguinsLive value:
dataPenguin =
PengWherePenguinsLive
deriving (Eq,Show)
Given these datatypes, we will write a couple functions for
processing the data:
-- is it South Africa? If so, return True
isSouthAfrica ::WherePenguinsLive -&gt;Bool
isSouthAfrica SouthAfrica =True
isSouthAfrica Galapagos =False
isSouthAfrica Antarctica =False
isSouthAfrica Australia =False
isSouthAfrica SouthAmerica =False
But that is redundant. We can use _to indicate an uncondi-
tional match on a value we don’t care about. The following is
better (more concise, easier to read) and does the same thing:
isSouthAfrica' ::WherePenguinsLive -&gt;Bool
isSouthAfrica' SouthAfrica =True
isSouthAfrica' _ = False
We can also use pattern matching to unpack Penguin values
to get at the WherePenguinsLive value it contains:</p>
<p>CHAPTER 7. MORE FUNCTIONAL PATTERNS 355
gimmeWhereTheyLive ::Penguin
-&gt;WherePenguinsLive
gimmeWhereTheyLive (Pengwhereitlives) =
whereitlives
Try using the gimmeWhereTheyLive function on some test data.
When you enter the name of the penguin (note the lowercase),
it will unpack the Pengvalue to return the WherePenguinsLive
that’s inside:
humboldt =PengSouthAmerica
gentoo=PengAntarctica
macaroni =PengAntarctica
little=PengAustralia
galapagos =PengGalapagos
Now a more elaborate example. We’ll expose the contents
ofPengand match on what WherePenguinLives value we care
about in one pattern match:
galapagosPenguin ::Penguin -&gt;Bool
galapagosPenguin (PengGalapagos )=True
galapagosPenguin _ = False
antarcticPenguin ::Penguin -&gt;Bool
antarcticPenguin (PengAntarctica )=True
antarcticPenguin _ = False</p>
<p>CHAPTER 7. MORE FUNCTIONAL PATTERNS 356
In this final function, the (||)operator is an orfunction that
will return Trueif either value is True:
antarcticOrGalapagos ::Penguin -&gt;Bool
antarcticOrGalapagos p=
(galapagosPenguin p)
||(antarcticPenguin p)
Note that we’re using pattern matching to accomplish two
things here. We’re using it to unpack the Penguin datatype.
We’re also specifying which WherePenguinsLive value we want
to match on.
Pattern matching tuples
You can also use pattern matching rather than functions for
operating on the contents of tuples. Remember this example
from Chapter 4?
f::(a, b)-&gt;(c, d)-&gt;((b, d), (a, c))
f=undefined
When you did that exercise, you may have written it like
this:
f::(a, b)-&gt;(c, d)-&gt;((b, d), (a, c))
fx y=((snd x, snd y), (fst x, fst y))</p>
<p>CHAPTER 7. MORE FUNCTIONAL PATTERNS 357
But we can use pattern matching on tuples to make a some-
what cleaner version of it:
f::(a, b)-&gt;(c, d)-&gt;((b, d), (a, c))
f(a, b) (c, d) =((b, d), (a, c))
One nice thing about this is that the tuple syntax allows the
function to look a great deal like its type. Let’s look at more
examples of pattern matching on tuples. Note that the second
example below is nota pattern match but the others are:</p>
<p>CHAPTER 7. MORE FUNCTIONAL PATTERNS 358
-- matchingTuples1.hs
moduleTupleFunctions where
-- These have to be the same type because
-- (+) is a -&gt; a -&gt; a
addEmUp2 ::Numa=&gt;(a, a)-&gt;a
addEmUp2 (x, y)=x+y
-- addEmUp2 could also be written like so
addEmUp2Alt ::Numa=&gt;(a, a)-&gt;a
addEmUp2Alt tup=(fst tup) +(snd tup)
fst3::(a, b, c) -&gt;a
fst3(x,<em>,</em>)=x
third3::(a, b, c) -&gt;c
third3(<em>,</em>, x)=x
Prelude&gt; :l code/matchingTuples1.hs
[1 of 1] Compiling TupleFunctions
Ok, modules loaded: TupleFunctions.
Now we’re going to use GHCi’s :browse to see a list of the
type signatures and functions we loaded from the module
TupleFunctions :</p>
<p>CHAPTER 7. MORE FUNCTIONAL PATTERNS 359
Prelude&gt; :browse TupleFunctions
addEmUp2 :: Num a =&gt; (a, a) -&gt; a
addEmUp2Alt :: Num a =&gt; (a, a) -&gt; a
fst3 :: (a, b, c) -&gt; a
third3 :: (a, b, c) -&gt; c
Prelude&gt; addEmUp2 (10, 20)
30
Prelude&gt; addEmUp2Alt (10, 20)
30
Prelude&gt; fst3 (&quot;blah&quot;, 2, [])
&quot;blah&quot;
Prelude&gt; third3 (&quot;blah&quot;, 2, [])
[]
Sweet. Let’s do some exercises. Pausing to exercise keeps
the muscles flexible, even the mental ones.
Exercises: Variety Pack
1.Given the following declarations
k(x, y)=x
k1=k ((4-1),10)
k2=k (&quot;three&quot;, (1+2))
k3=k (3,True)</p>
<p>CHAPTER 7. MORE FUNCTIONAL PATTERNS 360
a)What is the type of k?
b)What is the type of k2? Is it the same type as k1ork3?
c)Ofk1, k2, k3 , which will return the number 3 as the
result?
2.Fill in the definition of the following function:
-- Remember: Tuples have the
same syntax for their
-- type constructors and
-- their data constructors.
f::(a, b, c)
-&gt;(d, e, f)
-&gt;((a, d), (c, f))
f=undefined
7.5 Case expressions
Caseexpressionsareaway, similarinsomerespectsto if-then-else ,
of making a function return a diﬀerent result based on diﬀer-
ent inputs. You can use case expressions with any datatype that
has visible data constructors. When we consider the datatype
Bool:</p>
<p>CHAPTER 7. MORE FUNCTIONAL PATTERNS 361
dataBool=False|True
-- [1] [2] [3]
1.Type constructor, we only use this in type signatures, not
in term-level code like case expressions.
2.Data constructor for the value of Boolnamed False— we
can match on this.
3.Data constructor for the value of Boolnamed True— we
can match on this as well.
Any time we case match or pattern match on a sum type
likeBool, we should define how we handle each constructor
or provide a default that matches all of them. In fact, we must
handle both cases or use a function that handles both or we
will have written a partial function that can throw an error
at runtime. There is rarely a good reason to do this: write
functions that handle all possible inputs!
Let’s start by looking at an if-then-else expression that we
saw in a previous chapter:
ifx+1==1then&quot;AWESOME&quot; else&quot;wut&quot;
We can rewrite this as a case expression, matching on the
constructors of Bool:</p>
<p>CHAPTER 7. MORE FUNCTIONAL PATTERNS 362
funcZx=
casex+1==1of
True-&gt;&quot;AWESOME&quot;
False-&gt;&quot;wut&quot;
Note that while the syntax is considerably diﬀerent here,
the results will be the same. Be sure to load it in the REPL and
try it out.
We could also write a case expression to tell us whether or
not something is a palindrome:
palxs=
casexs==reverse xs of
True-&gt;&quot;yes&quot;
False-&gt;&quot;no&quot;
The above can also be written with a whereclause in cases
where you might need to reuse the 𝑦:
pal'xs=
caseyof
True-&gt;&quot;yes&quot;
False-&gt;&quot;no&quot;
wherey=xs==reverse xs
In either case, the function will first check if the input string
is equal to the reverse of it. If that returns True, then the string</p>
<p>CHAPTER 7. MORE FUNCTIONAL PATTERNS 363
is a palindrome, so your function says, “yes.” If not, then it’s
not.
Here is one more example, also matching on the data con-
structors from Bool, and you can compare its syntax to the
if-then-else version we’ve seen before:
-- greetIfCool3.hs
moduleGreetIfCool3 where
greetIfCool ::String-&gt;IO()
greetIfCool coolness =
casecoolof
True-&gt;
putStrLn &quot;eyyyyy. What's shakin'?&quot;
False-&gt;
putStrLn &quot;pshhhh.&quot;
wherecool=
coolness ==&quot;downright frosty yo&quot;
So far, the case expressions we’ve looked at rely on a straight-
forward pattern match with TrueandFalseexplicitly. In an
upcoming section, we’ll look at another way to write a case
expression.</p>
<p>CHAPTER 7. MORE FUNCTIONAL PATTERNS 364
Exercises: Case Practice
We’re going to practice using case expressions by rewriting
functions. Some of these functions you’ve seen in previous
chapters (and some you’ll see later using diﬀerent syntax yet
again!), but you’ll be writing new versions now. Please note
these are all written as they would be in source code files, and
we recommend you write your answers in source files and
then load into GHCi to check, rather than trying to do them
directly into the REPL.
First, rewrite if-then-else expressions into case expressions.
1.The following should return xwhenxis greater than y.
functionC x y= if(x&gt;y)thenxelsey
2.The following will add 2 to even numbers and otherwise
simply return the input value.
ifEvenAdd2 n= ifeven nthen(n+2)elsen
The next exercise doesn’t have all the cases covered. See
if you can fix it.
3.The following compares a value, x, to zero and returns an
indicator for whether xis a postive number or negative
number. But what if xis 0? You may need to play with
thecompare function a bit to find what to do.</p>
<p>CHAPTER 7. MORE FUNCTIONAL PATTERNS 365
numsx=
casecompare x 0of
LT-&gt; -1
GT-&gt;1
7.6 Higher-order functions
Higher-order functions (HOFs) are functions that accept func-
tions as arguments. Functions are values — why couldn’t they
be passed around like any other values? This is an important
component of functional programming and gives us a way to
combine functions efficiently.
Let’s examine a standard higher-order function, flip:
Prelude&gt; :t flip
flip :: (a -&gt; b -&gt; c) -&gt; b -&gt; a -&gt; c
-- using (-) as our (a -&gt; b -&gt; c)
Prelude&gt; (-) 10 1
9
Prelude&gt; let fSub = flip (-)
Prelude&gt; fSub 10 1
-9
Prelude&gt; fSub 5 10
5</p>
<p>CHAPTER 7. MORE FUNCTIONAL PATTERNS 366
The first parameter of flipis a function, such as (-), that
itself has two parameters. flipflips the order of the arguments.
We can implement fliplike this, using the variable 𝑓to
represent the function (a -&gt; b -&gt; c) :
flip::(a-&gt;b-&gt;c)-&gt;b-&gt;a-&gt;c
flipf x y=f y x
Alternately, it could’ve been written as:
myFlip::(a-&gt;b-&gt;c)-&gt;b-&gt;a-&gt;c
myFlipf=\x y-&gt;f y x
There’s no diﬀerence in what flipandmyFlip do: one de-
clares parameters in the function definition, and the other
declares them instead in the anonymous function value being
returned. But what makes flip a higher-order function? Well,
it’s this:
flip::(a-&gt;b-&gt;c)-&gt;b-&gt;a-&gt;c
[1]
flipf x y=f y x
[2] [3]
1.When we want to express a function argument within a
function type, we must use parentheses to nest it.
2.The argument 𝑓is the function a -&gt; b -&gt; c .</p>
<p>CHAPTER 7. MORE FUNCTIONAL PATTERNS 367
3.We apply 𝑓to𝑥and𝑦butflipwill flip the order of ap-
plication and apply 𝑓to𝑦and then 𝑥instead of the usual
order.
To better understand how HOFs work syntactically, it’s
worth remembering how parentheses associate in type signa-
tures.
Let’s look at the type of the following function:
returnLast ::a-&gt;b-&gt;c-&gt;d-&gt;d
returnLast _ _ _d=d
If we explicitly parenthesize returnLast , it must match the
associativity of -&gt;, which is right-associative. The following
parenthesization works fine. Note that this makes the default
currying explicit:
returnLast' ::a-&gt;(b-&gt;(c-&gt;(d-&gt;d)))
returnLast' _ _ _d=d
However, this will not work. This is not how -&gt;associates:
returnBroke ::(((a-&gt;b)-&gt;c)-&gt;d)-&gt;d
returnBroke _ _ _d=d
If you attempt to load returnBroke , you’ll get a type error.</p>
<p>CHAPTER 7. MORE FUNCTIONAL PATTERNS 368
Couldn't match expected type
‘t0 -&gt; t1 -&gt; t2 -&gt; t2’
with actual type ‘d’
‘d’ is a rigid type variable bound by
the type signature for
returnBroke :: (((a -&gt; b) -&gt; c) -&gt; d) -&gt; d
Relevant bindings include
returnBroke :: (((a -&gt; b) -&gt; c) -&gt; d) -&gt; d
The equation(s) for ‘returnBroke’
have four arguments,
but its type ‘(((a -&gt; b) -&gt; c) -&gt; d) -&gt; d’
has only one
This type error is telling us that the type of returnBroke only
specifies one argument that has the type ((a -&gt; b) -&gt; c) -&gt; d ,
yet our function definition seems to expect fourarguments.
The type signature of returnBroke specifies a single function as
the sole argument to returnBroke .2
Wecanhave a type that is parenthesized in that fashion as
long as we want to do something diﬀerent than what returnLast
does:
2Fun fact: returnBroke is an impossible function.</p>
<p>CHAPTER 7. MORE FUNCTIONAL PATTERNS 369
returnAfterApply ::(a-&gt;b)-&gt;a-&gt;c-&gt;b
returnAfterApply f a c=f a
What we’re doing here is parenthesizing to the leftso that
we can refer to a separate function, with its own parameters
and result, as an argument to our top level function. Here the
(a -&gt; b) is the𝑓argument we use to produce a value of type 𝑏
from a value of type 𝑎.
One reason we want HOFs is to manipulate how functions
are applied to arguments. To understand another reason, let’s
revisit the compare function from the Ordtypeclass:
Prelude&gt; :t compare
compare :: Ord a =&gt; a -&gt; a -&gt; Ordering
Prelude&gt; :info Ordering
data Ordering = LT | EQ | GT
Prelude&gt; compare 10 9
GT
Prelude&gt; compare 9 9
EQ
Prelude&gt; compare 9 10
LT
Now we’ll write a function that makes use of this:</p>
<p>CHAPTER 7. MORE FUNCTIONAL PATTERNS 370
dataEmployee =Coder
|Manager
|Veep
|CEO
deriving (Eq,Ord,Show)
reportBoss ::Employee -&gt;Employee -&gt;IO()
reportBoss e e'=
putStrLn $show e++
&quot; is the boss of &quot; ++
show e'
employeeRank ::Employee
-&gt;Employee
-&gt;IO()
employeeRank e e'=
casecompare e e' of
GT-&gt;reportBoss e e'
-- [ 1 ]
EQ-&gt;putStrLn &quot;Neither employee <br />
\is the boss&quot;
-- [ 2 ]
LT-&gt;(flip reportBoss) e e'
-- [ 3 ]</p>
<p>CHAPTER 7. MORE FUNCTIONAL PATTERNS 371
Thecasein theemployeeRank function is a case expression.
This function says: case expression
1.In the case of comparing 𝑒and𝑒′and finding 𝑒is greater
than𝑒′, return reportBoss e e' .
2.In the case of finding them equal, return the string “Nei-
ther employee is the boss.”
3.Inthecaseoffinding 𝑒lessthan 𝑒′, flipthefunction reportBoss .
This could also have been written reportBoss e' e .
Thecompare function uses the behavior of the Ordinstance
defined for a given type in order to compare them. In this
case, our data declaration lists them in order from Coderin
the lowest rank and CEOin the top rank, so compare will use that
ordering to evaluate the result of the function.
If we load this up and try it out:
Prelude&gt; employeeRank Veep CEO
CEO is the boss of Veep
That’s probably true in most companies! Being industrious
programmers, we naturally want to refactor this a bit to be
more flexible — notice how we change the type of employeeRank :</p>
<p>CHAPTER 7. MORE FUNCTIONAL PATTERNS 372
dataEmployee =Coder
|Manager
|Veep
|CEO
deriving (Eq,Ord,Show)
reportBoss ::Employee -&gt;Employee -&gt;IO()
reportBoss e e'=
putStrLn $show e++
&quot; is the boss of &quot; ++
show e'
employeeRank ::(Employee
-&gt;Employee
-&gt;Ordering )
-&gt;Employee
-&gt;Employee
-&gt;IO()
employeeRank f e e'=
casef e e'of
GT-&gt;reportBoss e e'
EQ-&gt;putStrLn &quot;Neither employee <br />
\is the boss&quot;
LT-&gt;(flip reportBoss) e e'</p>
<p>CHAPTER 7. MORE FUNCTIONAL PATTERNS 373
Now our employeeRank function will accept a function argu-
ment with the type Employee -&gt; Employee -&gt; Ordering , which we
named 𝑓, in the place where we had compare before. You’ll no-
tice we have the same case expressions here again. We can get
the same behavior we had last time by passing it compare as the
function argument:
Prelude&gt; employeeRank compare Veep CEO
CEO is the boss of Veep
Prelude&gt; employeeRank compare CEO Veep
CEO is the boss of Veep
But since we’re clever hackers, we can subvert the hierarchy
with a comparison function that does something a bit diﬀerent
with the following code:</p>
<p>CHAPTER 7. MORE FUNCTIONAL PATTERNS 374
dataEmployee =Coder
|Manager
|Veep
|CEO
deriving (Eq,Ord,Show)
reportBoss ::Employee -&gt;Employee -&gt;IO()
reportBoss e e'=
putStrLn $show e++
&quot; is the boss of &quot; ++
show e'
codersRuleCEOsDrool ::Employee
-&gt;Employee
-&gt;Ordering
codersRuleCEOsDrool CoderCoder=EQ
codersRuleCEOsDrool Coder_ =GT
codersRuleCEOsDrool _Coder=LT
codersRuleCEOsDrool e e'=
compare e e'
employeeRank ::(Employee
-&gt;Employee
-&gt;Ordering )
-&gt;Employee
-&gt;Employee
-&gt;IO()
employeeRank f e e'=
casef e e'of
GT-&gt;reportBoss e e'
EQ-&gt;putStrLn &quot;Neither employee <br />
\is the boss&quot;
LT-&gt;(flip reportBoss) e e'</p>
<p>CHAPTER 7. MORE FUNCTIONAL PATTERNS 375
Herewe’vecreateda newfunction that changesthe behavior
of the normal compare function by pattern matching on our
data constructor, Coder. In a case where Coderis the first value
(and the second value is anything — note the underscore used
as a catchall), the result will be GTor greater than. In a case
whereCoderis the second value passed, this function will return
LT, orless than . In any case where Coderis not one of the values,
compare will exhibit its normal behavior. The case expression
in theemployeeRank function is otherwise unchanged.
And here’s how that works:
Prelude&gt; employeeRank compare Coder CEO
CEO is the boss of Coder
Prelude&gt; let cs = codersRuleCEOsDrool
Prelude&gt; employeeRank cs Coder CEO
Coder is the boss of CEO
Prelude&gt; employeeRank cs CEO Coder
Coder is the boss of CEO
If we use compare as our 𝑓argument, then the behavior
is unchanged. If, on the other hand, we use our new func-
tion,codersRuleCEOsDrool as the𝑓argument, then the behavior
changes and we unleash anarchy in the cubicle farm.
We were able to rely on the behavior of compare but make
changes in the part we wanted to change. This is the value of
HOFs. They give us the beginnings of a powerful method for
reusing and composing code.</p>
<p>CHAPTER 7. MORE FUNCTIONAL PATTERNS 376
Exercises: Artful Dodgy
Given the following definitions tell us what value results from
further applications. When you’ve written down at least some
of the answers and think you know what’s what, type the def-
initions into a file and load them in GHCi to test your answers.
-- Types not provided,
-- try filling them in yourself.
dodgyx y=x+y*10
oneIsOne =dodgy1
oneIsTwo =(flip dodgy) 2
1.For example, given the expression dodgy 1 0 , what do you
think will happen if we evaluate it? If you put the def-
initions in a file and load them in GHCi, you can do the
following to see the result.
Prelude&gt; dodgy 1 0
1
Now attempt to determine what the following expressions
reduce to. Do it in your head, verify in your REPL after
you think you have an answer.
2.dodgy 1 1</p>
<p>CHAPTER 7. MORE FUNCTIONAL PATTERNS 377
3.dodgy 2 2
4.dodgy 1 2
5.dodgy 2 1
6.oneIsOne 1
7.oneIsOne 2
8.oneIsTwo 1
9.oneIsTwo 2
10.oneIsOne 3
11.oneIsTwo 3
7.7 Guards
We have played around with booleans and expressions that
evaluate to their truth value including if-then-else expressions
which rely on boolean evaluation to decide between two out-
comes. In this section, we will look at another syntactic pattern
called guards that relies on truth values to decide between two
or more possible results.</p>
<p>CHAPTER 7. MORE FUNCTIONAL PATTERNS 378
if-then-else
Let’s begin with a quick review of what we learned about
if-then-else expressions in the Basic Datatypes chapter. Note,
if-then-else isnotguards! This is review, before moving on to
guards themselves. The pattern is this:
if <condition>
then <result if True>
else <result if False>
where the ifcondition is an expression that results in a Bool
value. We saw how this allows us to write functions like this:
Prelude&gt; let x = 0
Prelude&gt; let a = &quot;AWESOME&quot;
Prelude&gt; let w = &quot;wut&quot;
Prelude&gt; if (x + 1 == 1) then a else w
&quot;AWESOME&quot;
The next couple of examples will demonstrate how to use
the multiline block syntax for an ifexpression:
-- alternately
Prelude&gt; let x = 0
Prelude&gt; :{
Prelude| if (x + 1 == 1)
Prelude| then &quot;AWESOME&quot;</p>
<p>CHAPTER 7. MORE FUNCTIONAL PATTERNS 379
Prelude| else &quot;wut&quot;
Prelude| :}
&quot;AWESOME&quot;
The indentation isn’t required:
Prelude&gt; let x = 0
Prelude&gt; :{
Prelude| if (x + 1 == 1)
Prelude| then &quot;AWESOME&quot;
Prelude| else &quot;wut&quot;
Prelude| :}
&quot;AWESOME&quot;
In the exercises at the end of Chapter 4, you were asked to
write a function called myAbsthat returns the absolute value of
a real number. You would have implemented that function
with an if-then-else expression similar to the following:
myAbs::Integer -&gt;Integer
myAbsx= ifx&lt;0then(-x)elsex
We’re going to look at another way to write this using guards.
Writing guard blocks
Guard syntax allows us to write compact functions that allow
for two or more possible outcomes depending on the truth of</p>
<p>CHAPTER 7. MORE FUNCTIONAL PATTERNS 380
the conditions. Let’s start by looking at how we would write
myAbswith a guard block instead of with an if-then-else :
myAbs::Integer -&gt;Integer
myAbsx
|x&lt;0=(-x)
|otherwise =x
Notice that each guard has its own equals sign. We didn’t
put one after the argument in the first line of the function def-
inition because each case needs its own expression to return
if its branch succeeds. Now we’ll enumerate the components
for clarity:
myAbs::Integer -&gt;Integer
myAbs x
-- [1] [2]
|x&lt;0=(-x)
-- [3] [4] [5] [6]
|otherwise =x
-- [7] [8] [9] [10]
1.The name of our function, myAbsstill comes first.
2.There is one parameter named 𝑥.
3.Here’s where it gets diﬀerent. Rather than an =imme-
diately after the introduction of any parameter(s), we’re</p>
<p>CHAPTER 7. MORE FUNCTIONAL PATTERNS 381
starting a new line and using the pipe|to begin a guard
case.
4.This is the expression we’re using to test to see if this
branch should be evaluated or not. The guard case ex-
pression between the |and=must evaluate to Bool.
5.The=denotes that we’re declaring what expression to
return should our x &lt; 0beTrue.
6.Then after the =we have the expression (-x)which will
be returned if x &lt; 0.
7.Another new line and a |to begin a new guard case.
8.otherwise is another name for True, used here as a fallback
case in case x &lt; 0wasFalse.
9.Another =to begin declaring the expression to return if
we hit the otherwise case.
10.We kick 𝑥back out if it wasn’t less than 0.
Let’s see how this evaluates:
Prelude&gt; myAbs (-10)
10
Prelude&gt; myAbs 10
10</p>
<p>CHAPTER 7. MORE FUNCTIONAL PATTERNS 382
In the first example, when it is passed a negative number
as an argument, it looks at the first guard and sees that (-10)
is indeed less than 0, evaluates that as True, and so returns
the result of (-x), in this case, (-(-10)) or 10. In the second
example, it looks at the first guard, sees that 10 does not meet
that condition, so it is False, and goes to the next guard. The
otherwise is always True, so it returns 𝑥, in this case, 10. Guards
always evaluate sequentially, so your guards should be ordered
from the case that is most restrictive to the case that is least
restrictive.
Let’s look next at a function that will have more than two
possible outcomes, in this case the results of a test of sodium
(Na) levels in the blood. We want a function that looks at the
numbers (the numbers represent mEq/L or milliequivalents
per liter) and tells us if the blood sodium levels are normal or
not:
bloodNa ::Integer -&gt;String
bloodNa x
|x&lt;135=&quot;too low&quot;
|x&gt;145=&quot;too high&quot;
|otherwise =&quot;just right&quot;
We can incorporate diﬀerent types of expressions into the
guard block, as long as each guard can be evaluated to a Bool
value. For example, the following takes 3 numbers and tells</p>
<p>CHAPTER 7. MORE FUNCTIONAL PATTERNS 383
you if the triangle whose sides they measure is a right triangle
or not (using the Pythagorean theorem):
-- c is the hypotenuse of
-- the triangle.
isRight ::(Numa,Eqa)
=&gt;a-&gt;a-&gt;a-&gt;String
isRight a b c
|a^2+b^2==c^2=&quot;RIGHT ON&quot;
|otherwise =&quot;not right&quot;
And the following function will take your dog’s age and tell
you how old your dog is in human years:
dogYrs::Integer -&gt;Integer
dogYrsx
|x&lt;=0=0
|x&lt;=1=x<em>15
|x&lt;=2=x</em>12
|x&lt;=4=x<em>8
|otherwise =x</em>6
Why the diﬀerent numbers? Because puppies reach matu-
rity much faster than human babies do, so a year-old puppy
isn’t equivalent to a 6- or 7-year-old child (there is more com-
plexity to this conversion than this function uses, because</p>
<p>CHAPTER 7. MORE FUNCTIONAL PATTERNS 384
other factors such as the size of the dog play a role as well. You
can certainly experiment with that if you like).
We can also use wheredeclarations within guard blocks. Let’s
say you gave a test that had 100 questions and you wanted a
simple function for translating the number of questions the
student got right into a letter grade:
avgGrade ::(Fractional a,Orda)
=&gt;a-&gt;Char
avgGrade x
|y&gt;=0.9='A'
|y&gt;=0.8='B'
|y&gt;=0.7='C'
|y&gt;=0.59='D'
|y&lt;0.59='F'
wherey=x/100
No surprises there. Notice the variable 𝑦is introduced, not
as an argument to the named function but in the guard block
and is defined in the whereclause. By defining it there, it is in
scope for all the guards above it. There were 100 problems on
the hypothetical test, so any 𝑥we give it will be divided by 100
to return the letter grade.
Also notice we left out the otherwise ; we could have used it
for the final case but chose instead to use less than . That is fine
because in our guards we’ve handled all possible values. It is</p>
<p>CHAPTER 7. MORE FUNCTIONAL PATTERNS 385
important to note that GHCi cannot always tell you when you
haven’t accounted for all possible cases, and it can be difficult
to reason about it, so it is wise to use otherwise in your final
guard.
Remember: You can use :set -Wall in GHCi to turn on
warnings, and then it will tell you if you have non-exhaustive
patterns.
Exercises: Guard Duty
1.Itisprobablycleartoyouwhyyouwouldn’tputan otherwise
in your top-most guard, but try it with avgGrade anyway
and see what happens. It’ll be more clear if you rewrite
it as anotherwise match: | otherwise = 'F' . What happens
now if you pass a 90 as an argument? 75? 60?
2.What happens if you take avgGrade as it is written and
reorder the guards? Does it still typecheck and work the
same? Try moving | y &gt;= 0.7 = 'C' and passing it the
argument 90, which should be an ‘A.’ Does it return an ‘A’?
3.The following function returns
palxs
|xs==reverse xs =True
|otherwise =False
a)xswritten backwards when it’s True</p>
<p>CHAPTER 7. MORE FUNCTIONAL PATTERNS 386
b)Truewhenxsis a palindrome
c)Falsewhenxsis a palindrome
d)Falsewhenxsis reversed
4.What types of arguments can paltake?
5.What is the type of the function pal?
6.The following function returns
numbers x
|x&lt;0= -1
|x==0=0
|x&gt;0=1
a)the value of its argument plus or minus 1
b)the negation of its argument
c)an indication of whether its argument is a positive or
negative number or zero
d)binary machine language
7.What types of arguments can numbers take?
8.What is the type of the function numbers ?</p>
<p>CHAPTER 7. MORE FUNCTIONAL PATTERNS 387
7.8 Function composition
Function composition is a type of higher-order function that
allows us to combine functions such that the result of applying
one function gets passed to the next function as an argument.
It is a very concise style, in keeping with the terse functional
style Haskell is known for. At first, it seems complicated and
difficult to unpack, but once you get the hang of it, it’s fun!
Let’s begin by looking at the type signature and what it means:
(.)::(b-&gt;c)-&gt;(a-&gt;b)-&gt;a-&gt;c
-- [1] [2] [3] [4]
1.is a function from 𝑏to𝑐, passed as an argument (thus the
parentheses).
2.is a function from 𝑎to𝑏.
3.is a value of type 𝑎, the same as [2]expects as an argument.
4.is a value of type 𝑐, the same as [1]returns as a result.
Then with the addition of one set of parentheses:
(.)::(b-&gt;c)-&gt;(a-&gt;b)-&gt;(a-&gt;c)
-- [1] [2] [3]
In English:</p>
<p>CHAPTER 7. MORE FUNCTIONAL PATTERNS 388
1.given a function 𝑏to𝑐
2.given a function 𝑎to𝑏
3.return a function 𝑎to𝑐.
The result of (a -&gt; b) is the argument of (b -&gt; c) so this is
how we get from an 𝑎argument to a 𝑐result. We’ve stitched
the result of one function into being the argument of another.
Next let’s start looking at composed functions and how
to read and work with them. The basic syntax of function
composition looks like this:
(f.g) x=f (g x)
This composition operator, (.), takes two functions here,
named 𝑓and𝑔. The𝑓function corresponds to the (b -&gt; c) in
the type signature, while the 𝑔function corresponds to the
(a -&gt; b) . The𝑔function is applied to the (polymorphic) 𝑥
argument. The result of that application then passes to the 𝑓
function as its argument. The 𝑓function is in turn applied to
that argument and evaluated to reach the final result.
Let’s go step by step through this transformation. We can
think of the (.)or composition operator as being a way of
pipelining data through multiple functions. The following
composed functions will first add the values in the list together
and then negate the result of that:</p>
<p>CHAPTER 7. MORE FUNCTIONAL PATTERNS 389
Prelude&gt; negate . sum $ [1, 2, 3, 4, 5]
-15
-- which is evaluated like this
negate . sum $ [1, 2, 3, 4, 5]
-- note: this code works as well
negate (sum [1, 2, 3, 4, 5])
negate (15)
-15
Notice that we did this directly in our REPL, because the
composition operator is already in scope in Prelude . The sum
of the list is 15. That result gets passed to the negate function
and returns a result of (-15).
You may be wondering why we need the $operator. You
might remember way back when we talked about the prece-
dence of various operators that we said that operator has a
lower precedence than an ordinary function call (white space,
usually). Ordinary function application has a precedence of
10 (out of 10). The composition operator has a precedence of
9. If we left white space as our function application, this would
be evaluated like this:
negate . sum [1, 2, 3, 4, 5]
negate . 15</p>
<p>CHAPTER 7. MORE FUNCTIONAL PATTERNS 390
Because function application has a higher precedence than
the composition operator, that function application would
happen before the two functions composed. We’d be trying to
pass a numeric value where our composition operator needs a
function. By using the $we signal that application to the argu-
ments should happen afterthe functions are already composed.
We can also parenthesize it instead of using the $operator.
In that case, it looks like this:
Prelude&gt; (negate . sum) [1, 2, 3, 4, 5]
-15
The choice of whether to use parentheses or the dollar sign
isn’t important; it is a question of style and ease of writing and
reading.
The next example uses two functions, takeandreverse , and
is applied to an argument that is a list of numbers from 1 to 10.
What we expect to happen is that the list will first be reversed
(from 10 to 1) and then the first 5 elements of the new list will
be returned as the result.
Prelude&gt; take 5 . reverse $ [1..10]
[10,9,8,7,6]
Given the next bit of code, how could we rewrite it to use
function composition instead of parentheses?</p>
<p>CHAPTER 7. MORE FUNCTIONAL PATTERNS 391
Prelude&gt; take 5 (enumFrom 3)
[3,4,5,6,7]
We know that we will have to eliminate the parentheses,
add the composition operator, and add the $operator. It will
then look like this:
Prelude&gt; take 5 . enumFrom $ 3
[3,4,5,6,7]
You may also define it this way, which is more similar to
how composition is written in source files:
Prelude&gt; let f x = take 5 . enumFrom $ x
Prelude&gt; f 3
[3,4,5,6,7]
You may be wondering why bother with this if it simply
does the same thing as nesting functions in parentheses. One
reason is that it is quite easy to compose more than two func-
tions this way.
Thefilter odd function is new for us, but it simply filters the
odd numbers (you can change it to filter even if you wish) out
of the list that enumFrom builds for us. Finally, takewill return
as the result only the number of elements we have specified
as the argument of take. Feel free to experiment with varying
any of the arguments.</p>
<p>CHAPTER 7. MORE FUNCTIONAL PATTERNS 392
Prelude&gt; take 5 . filter odd . enumFrom $ 3
[3,5,7,9,11]
As you compose more functions, you can see that nesting
all the parentheses would become tiresome. This operator
allows us to do away with that. It also allows us to write in an
even more terse style known as “pointfree.”
7.9 Pointfree style
Pointfree refers to a style of composing functions without
specifying their arguments. The “point” in “pointfree” refers
to the arguments, not (as it may seem) to the function compo-
sition operator. In some sense, we add “points” (the operator)
to be able to drop points (arguments). Quite often, pointfree
code is tidier on the page and easier to read as it helps the
reader focus on the functions rather than the data that is being
shuffled around.
We said above that function composition looks like this:
(f . g) x = f (g x)
As you put more functions together, composition can make
them easier to read. For example, (f. g. h) x can be easier
to read than f (g (h x)) and it also brings the focus to the
functions rather than the arguments. Pointfree is an extension
of that idea but now we drop the argument altogether:</p>
<p>CHAPTER 7. MORE FUNCTIONAL PATTERNS 393
f . g = \x -&gt; f (g x)
f . g . h = \x -&gt; f (g (h x))
To see what this looks like in practice, we’ll start by rewriting
in pointfree style some of the functions we used in the section
above:
Prelude&gt; let f = negate . sum
Prelude&gt; f [1, 2, 3, 4, 5]
-15
Notice that when we define our function 𝑓we don’t spec-
ify that there will be any arguments. Yet when we apply the
function to an argument, the same thing happens as before.
How would we rewrite:
f::Int-&gt;[Int]-&gt;Int
fz xs=foldr (+) z xs
as a pointfree function?
Prelude&gt; let f = foldr (+)
Prelude&gt; f 0 [1..5]
15
And now because we named the function, it can be reused
with diﬀerent arguments.</p>
<p>CHAPTER 7. MORE FUNCTIONAL PATTERNS 394
Here is another example of a short pointfree function and
its result. It involves a new use of filter that uses the Bool
operator ==. Look at it carefully and, on paper or in your head,
walk through the evaluation process involved:
Prelude&gt; let f = length . filter (== 'a')
Prelude&gt; f &quot;abracadabra&quot;
5
Next, we’ll look at a set of functions that work together, in a
single module, and rely on both composition and pointfree
style:
-- arith2.hs
moduleArith2where
add::Int-&gt;Int-&gt;Int
addx y=x+y
addPF::Int-&gt;Int-&gt;Int
addPF=(+)
addOne::Int-&gt;Int
addOne=\x-&gt;x+1
addOnePF ::Int-&gt;Int
addOnePF =(+1)</p>
<p>CHAPTER 7. MORE FUNCTIONAL PATTERNS 395
main::IO()
main= do
print (0::Int)
print (add 10)
print (addOne 0)
print (addOnePF 0)
print ((addOne .addOne) 0)
print ((addOnePF .addOne) 0)
print ((addOne .addOnePF) 0)
print ((addOnePF .addOnePF) 0)
print (negate (addOne 0))
print ((negate .addOne) 0)
print ((addOne .addOne.addOne
.negate.addOne) 0)
Take your time and work through what each function is
doing, whether on paper or in your head. Then load this code
as a source file and run it in GHCi and see if your results were
accurate.
You should now have a good understanding of how you
can use (.)tocompose functions. It’s important to remember
that the functions in composition are applied from right to
left, like a Pacman munching from the right side, reducing the
expressions as he goes.</p>
<p>CHAPTER 7. MORE FUNCTIONAL PATTERNS 396
7.10 Demonstrating composition
You may recall back in Chapter 3 we mentioned that the func-
tionsprintandputStr seem similar on the surface but behave
diﬀerently because they have diﬀerent underlying types. Let’s
take a closer look at that now.
First,putStrLn andputStr have the same type:
putStr :: String -&gt; IO ()
putStrLn :: String -&gt; IO ()
But the type of printis diﬀerent:
print :: Show a =&gt; a -&gt; IO ()
They all return a result of IO ()for reasons we discussed
in the previous chapter. But the parameters here are quite
diﬀerent. The first two take String s as arguments, while print
has a constrained polymorphic parameter, Show a =&gt; a . The
first two work fine if we need to display values that are already
of type String . But how do we display numbers (or other non-
string values)? First we have to convert those numbers to
strings, then we can print the strings.
You may also recall a function from our discussion of the
Showtypeclass called show. Here’s the type of showagain:
show::Showa=&gt;a-&gt;String</p>
<p>CHAPTER 7. MORE FUNCTIONAL PATTERNS 397
Fortunately, it was understood that combining putStrLn and
showwould be a common pattern, so the function named print
is the composition of showandputStrLn . We do it this way
because it’s simpler . The printing function concerns itself only
with printing, while the stringification function concerns itself
only with that.
Here are two ways to implement printwithputStrLn and
show:
print::Showa=&gt;a-&gt;IO()
printa=putStrLn (show a)
-- using the . operator for
-- composing functions.
(.)::(b-&gt;c)-&gt;(a-&gt;b)-&gt;a-&gt;c
-- we can write print as:
print::Showa=&gt;a-&gt;IO()
printa=(putStrLn .show) a
Now let’s go step by step through this use of (.),putStrLn ,
andshow:</p>
<p>CHAPTER 7. MORE FUNCTIONAL PATTERNS 398
(.) ::(b-&gt;c)-&gt;(a-&gt;b)-&gt;a-&gt;c
putStrLn ::String-&gt;IO()
-- [1] [2]
show ::Showa=&gt;a-&gt;String
-- [3] [4]
putStrLn .show::Showa=&gt;a-&gt;IO()
-- [5] [6]
(.)::(b-&gt;c)-&gt;(a-&gt;b)-&gt;a-&gt;c
-- [1] [2] [3] [4] [5] [6]
-- If we replace the variables with
-- the specific types they take on
-- in this application of (.)
(.)::Showa=&gt;(String-&gt;IO())
-&gt;(a-&gt;String)
-&gt;a-&gt;IO()</p>
<p>CHAPTER 7. MORE FUNCTIONAL PATTERNS 399
(.)::(b -&gt;c)
-- (String -&gt; IO ())
-&gt;(a-&gt;b)
-- (a -&gt; String)
-&gt;a-&gt;c
-- a -&gt; IO ()
1.is the string that putStrLn accepts as an argument.
2.is theIO ()thatputStrLn returns, that is, performing the
side eﬀect of printing and returning unit.
3.is𝑎that must implement the Showtypeclass; this is the Show
a =&gt; a from the showfunction which is a method on the
Showtypeclass.
4.is the string that showreturns. This is what the Show a =&gt; a
value got stringified into.
5.is theShow a =&gt; a the final composed function expects.
6.is theIO ()the final composed function returns.
We can now make it pointfree. When we are working with
functions primarily in terms of composition rather than appli-
cation, the pointfree version can sometimes (not always) be
more elegant.</p>
<p>CHAPTER 7. MORE FUNCTIONAL PATTERNS 400
Here’s the previous version of the function:
print::Showa=&gt;a-&gt;IO()
printa=(putStrLn .show) a
And here’s the pointfree version of print:
print::Showa=&gt;a-&gt;IO()
print=putStrLn .show
The point of printis to compose putStrLn andshowso that
we don’t have to call showon its argument ourselves. That is,
printis principally about the composition of two functions,
so it comes out nicely as a pointfree function. Saying that
we could apply putStrLn . show to an argument in this case is
redundant.
7.11 Chapter Exercises
Multiple choice
1.A polymorphic function
a)changes things into sheep when invoked
b)has multiple arguments
c)has a concrete type</p>
<p>CHAPTER 7. MORE FUNCTIONAL PATTERNS 401
d)may resolve to values of diﬀerent types, depending
on inputs
2.Two functions named fandghave types Char -&gt; String
andString -&gt; [String] respectively. The composed func-
tiong . fhas the type
a)Char -&gt; String
b)Char -&gt; [String]
c)[[String]]
d)Char -&gt; String -&gt; [String]
3.A function fhas the type Ord a =&gt; a -&gt; a -&gt; Bool and we
apply it to one numeric value. What is the type now?
a)Ord a =&gt; a -&gt; Bool
b)Num -&gt; Num -&gt; Bool
c)Ord a =&gt; a -&gt; a -&gt; Integer
d)(Ord a, Num a) =&gt; a -&gt; Bool
4.A function with the type (a -&gt; b) -&gt; c
a)requires values of three diﬀerent types
b)is a higher-order function
c)must take a tuple as its first argument
d)has its parameters in alphabetical order</p>
<p>CHAPTER 7. MORE FUNCTIONAL PATTERNS 402
5.Given the following definition of f, what is the type of f
True?
f::a-&gt;a
fx=x
a)f True :: Bool
b)f True :: String
c)f True :: Bool -&gt; Bool
d)f True :: a
Let’s write code
1.The following function returns the tens digit of an integral
argument.
tensDigit ::Integral a=&gt;a-&gt;a
tensDigit x=d
wherexLast=x <code>div</code> 10
d=xLast <code>mod</code> 10
a)First, rewrite it using divMod .
b)Does the divMod version have the same type as the
original version?
c)Next, let’s change it so that we’re getting the hundreds
digit instead. You could start it like this (though that
may not be the only possibility):</p>
<p>CHAPTER 7. MORE FUNCTIONAL PATTERNS 403
hunsDx=d2
whered=undefined
...
2.Implement the function of the type a -&gt; a -&gt; Bool -&gt; a
once each using a case expression and once with a guard.
foldBool ::a-&gt;a-&gt;Bool-&gt;a
foldBool =
error
&quot;Error: Need to implement foldBool!&quot;
The result is semantically similar to if-then-else expres-
sions but syntactically quite diﬀerent. Here is the pattern
matching version to get you started:
foldBool3 ::a-&gt;a-&gt;Bool-&gt;a
foldBool3 x_False=x
foldBool3 _yTrue=y
3.Fill in the definition. Note that the first argument to our
function is alsoa function which can be applied to values.
Your second argument is a tuple, which can be used for
pattern matching:
g::(a-&gt;b)-&gt;(a, c)-&gt;(b, c)
g=undefined</p>
<p>CHAPTER 7. MORE FUNCTIONAL PATTERNS 404
4.For this next exercise, you’ll experiment with writing
pointfree versions of existing code. This involves some
new information, so read the following explanation care-
fully.
Typeclasses are dispatched by type. Readis a typeclass like
Show, but it is the dual or “opposite” of Show. In general, the
Readtypeclass isn’t something you should plan to use a
lot, but this exercise is structured to teach you something
about the interaction between typeclasses and types.
The function readin theReadtypeclass has the type:
read::Reada=&gt;String-&gt;a
Notice a pattern?
read::Reada=&gt;String-&gt;a
show::Showa=&gt;a-&gt;String
Write the following code into a source file. Then load it
and run it in GHCi to make sure you understand why the
evaluation results in the answers you see.</p>
<p>CHAPTER 7. MORE FUNCTIONAL PATTERNS 405
-- arith4.hs
moduleArith4where
-- id :: a -&gt; a
-- id x = x
roundTrip ::(Showa,Reada)=&gt;a-&gt;a
roundTrip a=read (show a)
main= do
print (roundTrip 4)
print (id 4)
5.Next, write a pointfree version of roundTrip . (n.b., This
refers to the function definition, not to its application in
main.)
6.We will continue to use the code in module Arith4 for this
exercise as well.
When we apply showto a value such as (1 :: Int) , the𝑎that
implements Show is Int, so GHC will use the Int instance
of the Show typeclass to stringify our Int of 1.
However, readexpects a String argument in order to re-
turn an 𝑎. TheString argument that is the first argument
toreadtells the function nothing about what type the de-
stringified result should be. In the type signature roundTrip</p>
<p>CHAPTER 7. MORE FUNCTIONAL PATTERNS 406
currently has, it knows because the type variables are the
same, so the type that is the input to showhas to be the
same type as the output of read.
Your task now is to change the type of roundTrip inArith4 to
(Show a, Read b) =&gt; a -&gt; b . How might we tell GHC which
instance of Readto dispatch against the String now? Make
the expression print (roundTrip 4) work. You will only
need the has the type syntax of ::and parentheses for
scoping.
7.12 Chapter Definitions
1.Binding orbound is a common word used to indicate con-
nection, linkage, or association between two objects. In
Haskell we’ll use it to talk about what value a variable has,
e.g., a parameter variable is bound to an argument value,
meaning the value is passed into the parameter as input
and each occurrence of that named parameter will have
the same value. Bindings as a plurality will usually refer
to a collection of variables and functions which can be
referenced by name.
blah::Int
blah=10
Here the variable blahis bound to the value 10.</p>
<p>CHAPTER 7. MORE FUNCTIONAL PATTERNS 407
2.Ananonymous function is a function which is not bound to
an identifier and is instead passed as an argument to an-
other function and/or used to construct another function.
See the following examples.
\x-&gt;x
-- anonymous version of id
idx=x
-- not anonymous, it's bound to 'id'
3.Currying is the process of transforming a function that
takes multiple arguments into a series of functions which
each take one argument and return one result. This is ac-
complished through the nesting. In Haskell, all functions
are curried by default. You don’t need to do anything
special yourself.</p>
<p>CHAPTER 7. MORE FUNCTIONAL PATTERNS 408
-- curry and uncurry already
-- exist in Prelude
curry'::((a, b) -&gt;c)-&gt;a-&gt;b-&gt;c
curry'f a b=f (a, b)
uncurry' ::(a-&gt;b-&gt;c)-&gt;((a, b) -&gt;c)
uncurry' f (a, b) =f a b
-- uncurried function,
-- takes a tuple of its arguments
add::(Int,Int)-&gt;Int
add(x, y)=x+y
add'::Int-&gt;Int-&gt;Int
add'=curry' add
A function that appears to take two arguments is two func-
tions that each take one argument and return one result.
What makes this work is that a function can return another
function.</p>
<p>CHAPTER 7. MORE FUNCTIONAL PATTERNS 409
fa b=a+b
-- is equivalent to
f=\a-&gt;(\b-&gt;a+b)
4.Pattern matching is a syntactic way of deconstructing prod-
uct and sum types to get at their inhabitants. With re-
spect to products, pattern matching gives you the means
for destructuring and exposing the contents of products,
binding one or more values contained therein to names.
With sums, pattern matching lets you discriminate which
inhabitant of a sum you mean to handle in that match.
It’s best to explain pattern matching in terms of how
datatypes work, so we’re going to use terminology that
you may not fully understand yet. We’ll cover this more
deeply soon.
-- nullary data constructor,
-- not a sum or product.
-- Just a single value.
dataBlah=Blah
Pattern matching on Blahcan only do one thing.
blahFunc ::Blah-&gt;Bool
blahFunc Blah=True</p>
<p>CHAPTER 7. MORE FUNCTIONAL PATTERNS 410
dataIdentity a=
Identity a
deriving (Eq,Show)
Identity is a unary data constructor. Still not a product,
only contains one value.
-- when you pattern match on Identity
-- you can unpack and expose the 'a'
unpackIdentity ::Identity a-&gt;a
unpackIdentity (Identity x)=x
-- But you can choose to ignore
-- the contents of Identity
ignoreIdentity ::Identity a-&gt;Bool
ignoreIdentity (Identity _)=True
-- or ignore it completely since
-- matching on a non-sum data constructor
-- changes nothing.
ignoreIdentity' ::Identity a-&gt;Bool
ignoreIdentity' _ =True</p>
<p>CHAPTER 7. MORE FUNCTIONAL PATTERNS 411
dataProduct a b=
Product a b
deriving (Eq,Show)
Now we can choose to use none, one, or both of the values
in the product of 𝑎and𝑏:
productUnpackOnlyA ::Product a b-&gt;a
productUnpackOnlyA (Product x_)=x
productUnpackOnlyB ::Product a b-&gt;b
productUnpackOnlyB (Product _y)=y
Or we can bind them both to a diﬀerent name:
productUnpack ::Product a b-&gt;(a, b)
productUnpack (Product x y)=(x, y)
What happens if you try to bind the values in the product
to the same name?
dataSumOfThree a b c=
FirstPossible a
|SecondPossible b
|ThirdPossible c
deriving (Eq,Show)</p>
<p>CHAPTER 7. MORE FUNCTIONAL PATTERNS 412
Now we can discriminate by the inhabitants of the sum
and choose to do diﬀerent things based on which con-
structor in the sum they were.
sumToInt ::SumOfThree a b c-&gt;Integer
sumToInt (FirstPossible _)=0
sumToInt (SecondPossible _)=1
sumToInt (ThirdPossible _)=2
-- We can selectively ignore
-- inhabitants of the sum
sumToInt ::SumOfThree a b c-&gt;Integer
sumToInt (FirstPossible _)=0
sumToInt _ = 1
-- We still need to handle
-- every possible value
Pattern matching is about your data.
5.Bottom is a non-value used to denote that the program
cannot return a value or result. The most elemental
manifestation of this is a program that loops infinitely.
Other forms can involve things like writing a function</p>
<p>CHAPTER 7. MORE FUNCTIONAL PATTERNS 413
that doesn’t handle all of its inputs and fails on a pattern
match. The following are examples of bottom:
-- If you apply this to any values,
-- it'll recurse indefinitely.
fx=f x
-- It'll a'splode if you pass a False value
dontDoThis ::Bool-&gt;Int
dontDoThis True=1
-- morally equivalent to
definitelyDontDoThis ::Bool-&gt;Int
definitelyDontDoThis True=1
definitelyDontDoThis False=error&quot;oops&quot;
-- don't use error.
-- We'll show you a better way soon.
Bottom can be useful as a canary for signaling when code
paths arebeing evaluated. We usually do this to determine
how lazy a program is or isn’t. You’ll see a lotof this in
our chapter on non-strictness later on.
6.Higher-orderfunctions are functions which themselves take</p>
<p>CHAPTER 7. MORE FUNCTIONAL PATTERNS 414
functions as arguments or return functions as results. Due
to currying, technically any function that appears to take
more than one argument is higher order in Haskell.</p>
<p>CHAPTER 7. MORE FUNCTIONAL PATTERNS 415
-- Technically higher order
-- because of currying
Int-&gt;Int-&gt;Int
-- See? Returns another function
-- after applying the first argument
Int-&gt;(Int-&gt;Int)
-- The rest of the following examples are
-- types of higher order functions
(a-&gt;b)-&gt;a-&gt;b
(a-&gt;b)-&gt;[a]-&gt;[b]
(Int-&gt;Bool)-&gt;[Int]-&gt;[Bool]
-- also higher order, this one
-- takes a function argument which itself
-- is higher order as well.
((a-&gt;b)-&gt;c)-&gt;[a]-&gt;[c]
7.Composition is the application of a function to the result
of having applied another function. The composition op-</p>
<p>CHAPTER 7. MORE FUNCTIONAL PATTERNS 416
erator is a higher-order function as it takes the functions
it composes as arguments and then returns a function of
the composition:
(.)::(b-&gt;c)-&gt;(a-&gt;b)-&gt;a-&gt;c
-- is
(.)::(b-&gt;c)-&gt;(a-&gt;b)-&gt;(a-&gt;c)
-- or
(.)::(b-&gt;c)-&gt;((a-&gt;b)-&gt;(a-&gt;c))
-- can be implemented as
comp::(b-&gt;c)-&gt;((a-&gt;b)-&gt;(a-&gt;c))
compf g x=f (g x)
The function 𝑔is applied to 𝑥,𝑓is applied to the result of
g x.
8.Pointfree is programming tacitly, or without mentioning
arguments by name. This tends to look like “plumby”
code where you’re routing data around implicitly or leav-
ing oﬀ unnecessary arguments thanks to currying. The
“point” referred to in the term pointfree is an argument.</p>
<p>CHAPTER 7. MORE FUNCTIONAL PATTERNS 417
-- not pointfree
blahx=x
addAndDrop x y=x+1
reverseMkTuple a b=(b, a)
reverseTuple (a, b)=(b, a)
-- pointfree versions of the above
blah=id
addAndDrop =const.(1+)
reverseMkTuple =flip (,)
reverseTuple =uncurry (flip (,))
To see more examples like this, check out the Haskell Wiki
page on Pointfree at https://wiki.haskell.org/Pointfree .
7.13 Follow-up resources
1.Paul Hudak; John Peterson; Joseph Fasel. A Gentle In-
troduction to Haskell, chapter on case expressions and
pattern matching.
https://www.haskell.org/tutorial/patterns.html
2.Simon Peyton Jones. The Implementation of Functional
Programming Languages, pages 53-103.
http://research.microsoft.com/en-us/um/people/simonpj/papers/
slpj-book-1987/index.htm</p>
<p>CHAPTER 7. MORE FUNCTIONAL PATTERNS 418
3.Christopher Strachey. Fundamental Concepts in Pro-
gramming Languages, page 11 for explanation of curry-
ing.
http://www.cs.cmu.edu/~crary/819-f09/Strachey67.pdf
4.J.N. Oliveira. An introduction to pointfree programming.
http://www.di.uminho.pt/~jno/ps/iscalc_1.ps.gz
5.Manuel Alcino Pereira da Cunha. Point-free Program
Calculation.
http://www4.di.uminho.pt/~mac/Publications/phd.pdf</p>
<p>Chapter 8
Recursion
Imagine a portion of the
territory of England has
been perfectly levelled,
and a cartographer traces
a map of England. The
work is perfect. There is
no particular of the
territory of England,
small as it can be, that has
not been recorded in the
map. Everything has its
own correspondence.
The map, then, must
contain a map of the map,
that must contain a map
of the map of the map,
and so on to infinity.
Jorge Luis Borges, citing
Josiah Royce
419</p>
<p>CHAPTER 8. FUNCTIONS THAT CALL THEMSELVES 420
8.1 Recursion
Recursion is defining a function in terms of itself via self-
referential expressions. It means that the function will con-
tinue to call itself and repeat its behavior until some condition
is met to return a result. It’s an important concept in Haskell
and in mathematics because it gives us a means of express-
ingindefinite or incremental computation without forcing us
to explicitly repeat ourselves and allowing the data we are
processing to decide when we are done computing.
Recursion is a natural property of many logical and math-
ematical systems, including human language. That there is
no limit on the number of expressible, valid sentences in hu-
man language is due to recursion. A sentence in English can
have another sentence nested within it. Sentences can be
roughly described as structures which have a noun phrase, a
verb phrase, and optionally another sentence. This possibility
for unlimited nested sentences is recursive and enables the
limitless expressibility therein. Recursion is a means of ex-
pressing code that must take an indefinite number of steps to
return a result.
But the lambda calculus does not appear on the surface
to have any means of recursion, because of the anonymity
of expressions. How do you call something without a name?
Being able to write recursive functions, though, is essential to
Turing completeness. We use a combinator — known as the Y</p>
<p>CHAPTER 8. FUNCTIONS THAT CALL THEMSELVES 421
combinator or fixed-point combinator — to write recursive
functions in the lambda calculus. Haskell has native recursion
ability based on the same principle as the Y combinator.
It is important to have a solid understanding of the behavior
of recursive functions. In later chapters, we will see that, in
fact, it is not often necessary to write our own recursive func-
tions, as many standard higher-order functions have built-in
recursion. But without understanding the systematic behav-
ior of recursion itself, it can be difficult to reason about those
HOFs. In this chapter, we will
•explore what recursion is and how recursive functions
evaluate;
•go step-by-step through the process of writing recursive
functions;
•have fun with bottom .
8.2 Factorial!
One of the classic introductory functions for demonstrating
recursion in functional languages is factorial. In arithmetic,
you might’ve seen expressions like 4!. Thebangyou’re seeing
next to the number 4 is the notation for the factorial function.
Let’s evaluate 4!:</p>
<p>CHAPTER 8. FUNCTIONS THAT CALL THEMSELVES 422
4! = 4 * 3 * 2 * 1
12 * 2 * 1
24 * 1
24
4! = 24
Now let’s do it the silly way in Haskell:
fourFactorial ::Integer
fourFactorial =4<em>3</em>2<em>1
This will return the correct result, but it only covers one
possible result for factorial . This is less than ideal. We want
to express the general idea of the function, not encode specific
inputs and outputs manually.
Now we’ll look at some broken code to introduce the con-
cept of a base case :
-- This won't work. It never stops.
brokenFact1 ::Integer -&gt;Integer
brokenFact1 n=n</em>brokenFact1 (n -1)
Let’s apply this to 4 and see what happens:</p>
<p>CHAPTER 8. FUNCTIONS THAT CALL THEMSELVES 423
brokenFact1 4=
4*(4-1)
<em>((4-1)-1)
<em>(((4-1)-1)-1)
...this series never stops
The way we can stop a recursive expression is by having a
base case that stops the self-application to further arguments.
Understanding this is critical for writing functions which are
correct and terminate properly. Here’s what this looks like for
factorial :
moduleFactorial where
factorial ::Integer -&gt;Integer
factorial 0=1
factorial n=n</em>factorial (n -1)
brokenFact1 4=
4</em>(4-1)
*((4-1)-1)
*(((4-1)-1)-1)
*((((4-1)-1)-1)-1)
*(((((4-1)-1)-1)-1)-1)
...never stops</p>
<p>CHAPTER 8. FUNCTIONS THAT CALL THEMSELVES 424
But the base case factorial 0 = 1 in the fixed version gives
our function a stopping point, so the reduction changes:
-- Changes to
-- n = n * factorial (n - 1)
factorial 4=
4<em>factorial ( 4-1)
-- evaluate (-) applied to 4 and 1
4</em>factorial 3
-- evaluate factorial applied to 3
-- expands to 3 * factorial (3 - 1)
4<em>3</em>factorial ( 3-1)
-- beta reduce (-) applied to 3 and 1
4<em>3</em>factorial 2
-- evaluate factorial applied to 2
4<em>3</em>2<em>factorial ( 2-1)
-- evaluate (-) applied to 2 and 1
4</em>3<em>2</em>factorial 1
-- evaluate factorial applied to 1
4<em>3</em>2<em>1</em>factorial ( 1-1)</p>
<p>CHAPTER 8. FUNCTIONS THAT CALL THEMSELVES 425
-- evaluate (-) applied to 1 and 1
-- we know factorial 0 = 1
-- so we evaluate that to 1
4<em>3</em>2<em>1</em>1
-- And when we evaluate
-- our multiplications
24
Making our base case an identity value for the function
(multiplication in this case) means that applying the function
to that case doesn’t change the result of previous applications.
Another way to look at recursion
In the last chapter, we looked at a higher-order function called
composition. Function composition is a way of tying two (or
more) functions together such that the result of applying the
first function gets passed as an argument to the next function.
This is the same thing recursive functions are doing — taking
the result of the first application of the function and passing it
to the next function — except in the case of recursive functions,
thefirstresultgetspassedbacktothesamefunctionratherthan
a diﬀerent one, until it reaches the base case and terminates.</p>
<p>CHAPTER 8. FUNCTIONS THAT CALL THEMSELVES 426
Where function composition as we normally think of it is
static and definite, recursive compositions are indefinite. The
number of times the function may be applied depends on the
arguments to the function, and the applications can be infinite
if a stopping point is not clearly defined.
Let’s recall that function composition has the following
type:
(.) :: (b -&gt; c) -&gt; (a -&gt; b) -&gt; a -&gt; c
And when we use it like this:
take5.filter odd .enumFrom $3
we know that the first result will be a list generated by
enumFrom which will be passed to filter odd , giving us a list of
only the odd results, and that list will be passed to take 5 and
our final result will be the first five members of that list. Thus,
results get piped through a series of functions.
Recursion is self-referential composition.1We apply a func-
tion to an argument, then pass that result on as an argument
to a second application of the same function and so on.
Now look again at how the compose function (.)is written:
(.)::(b-&gt;c)-&gt;(a-&gt;b)-&gt;a-&gt;c
(.) f g=\x-&gt;f (g x)
1Many thanks to George Makrydakis for discussing this with us.</p>
<p>CHAPTER 8. FUNCTIONS THAT CALL THEMSELVES 427
A programming language, such as Haskell, that is built
purely on lambda calculus has one verb for expressing compu-
tations that can be evaluated: apply. We apply a function to an
argument. Applying a function to an argument and potentially
doing something with the result is all we can do, no matter
what syntactic conveniences we construct to make it seem that
we are doing more than that. While we give function compo-
sition a special name and operator to point up the pattern and
make it convenient to use, it’s only a way of saying:
•given two functions, 𝑓and𝑔, as arguments to (.),
•when we get an argument 𝑥, apply𝑔to𝑥,
•then apply 𝑓to the result of (g x); or,
•to rephrase, in code:
(.) f g=\x-&gt;f (g x)
With function recursion, you might notice that it is func-
tion application in the same way that composition is. The
diﬀerence is that instead of a fixed number of applications,
recursive functions rely on inputs to determine when to stop
applying functions to successive results. Without a specified</p>
<p>CHAPTER 8. FUNCTIONS THAT CALL THEMSELVES 428
stopping point, the result of (g x)will keep being passed back
to𝑔indefinitely.2
Let’s look at some code to see the similarity in patterns:
inc::Numa=&gt;a-&gt;a
inc=(+1)
three=inc.inc.inc$0
-- different syntax, same thing
three'=(inc.inc.inc)0
Our composition of incbakes the number of applications
into the source code. We don’t presently have a means of
changing how many times we want it to apply incwithout
writing a new function.
So, we might want to make a general function that can apply
incan indefinite number of times and allow us to specify as
an argument how many times it should be applied:
incTimes ::(Eqa,Numa)=&gt;a-&gt;a-&gt;a
incTimes 0n=
n
incTimes times n =
1+(incTimes (times -1) n)
2Because Haskell is built on pure lambda calculus, recursion is implemented in the
language through the Y, or fixed-point combinator. You can read a very good explanation
of that at http://mvanier.livejournal.com/2897.html if you are interested in knowing how it
works.</p>
<p>CHAPTER 8. FUNCTIONS THAT CALL THEMSELVES 429
Here,𝑡𝑖𝑚𝑒𝑠is a variable representing the number of times
the incrementing function (not called inchere but written as
1 +in the function body) should be applied to the argument
𝑛. If we want to apply it zero times, it will return our 𝑛to us.
Otherwise, the incrementing function will be applied as many
times as we’ve declared:
Prelude&gt; incTimes 10 0
10
Prelude&gt; incTimes 5 0
5
Prelude&gt; incTimes 5 5
10
--does this look familiar?
In a function such as this, the looming threat of unending
recursion is minimized because the number of times to apply
the function is an argument to the function itself, and we’ve
defined a stopping point: when (times - 1) is equal to zero, it
returns 𝑛and that’s all the applications we get.
We can abstract the recursion out of incTimes , too:</p>
<p>CHAPTER 8. FUNCTIONS THAT CALL THEMSELVES 430
applyTimes ::(Eqa,Numa)=&gt;
a-&gt;(b-&gt;b)-&gt;b-&gt;b
applyTimes 0f b=b
applyTimes n f b=f (applyTimes (n -1) f b)
incTimes' ::(Eqa,Numa)=&gt;a-&gt;a-&gt;a
incTimes' times n =applyTimes times ( +1) n
When we do, we can make the composition more obvious
inapplyTimes :
applyTimes ::(Eqa,Numa)=&gt;
a-&gt;(b-&gt;b)-&gt;b-&gt;b
applyTimes 0f b=
b
applyTimes n f b=
f.applyTimes (n -1) f$b
We’re recursively composing our function 𝑓withapplyTimes
(n-1) f however many subtractions it takes to get nto 0!
Intermission: Exercise
Write out the evaluation of the following. It might be a little
less noisy if you do so with the form that didn’t use (.).
applyTimes 5(+1)5</p>
<p>CHAPTER 8. FUNCTIONS THAT CALL THEMSELVES 431
8.3 Bottom
⊥orbottom is a term used in Haskell to refer to computations
that do not successfully result in a value. The two main vari-
eties of bottom are computations that failed with an error or
those that failed to terminate. In logic, ⊥corresponds to false.
Let us examine a few ways by which we can have bottom in
our programs:
Prelude&gt; let x = x in x
*** Exception: &lt;<loop>&gt;
Here GHCi detected that let x = x in x was never going
to return and short-circuited the never-ending computation.
This is an example of bottom because it was never going to
return a result. Note that if you’re using a Windows com-
puter, this example may freeze your GHCi and not throw an
exception.
Next let’s define a function that will return an exception:
f::Bool-&gt;Int
fTrue=error&quot;blah&quot;
fFalse=0
And let’s try that out in GHCi:
Prelude&gt; f False</p>
<p>CHAPTER 8. FUNCTIONS THAT CALL THEMSELVES 432
0
Prelude&gt; f True
*** Exception: blah
In the first case, when we evaluated f False and got 0, that
didn’t result in a bottom value. But, when we evaluated f True ,
we got an exception which is a means of expressing that a
computation failed. We got an exception because we specified
that this value should return an error. But this, too, is an
example of bottom.
Another example of a bottom would be a partial function.
Let’s consider a rewrite of the previous function:
f::Bool-&gt;Int
fFalse=0
This has the same type and returns the same output. What
we’ve done is elided the f True = error &quot;blah&quot; case from the
function definition. This is nota solution to the problem with
the previous function, but it will give us a diﬀerent exception.
We can observe this for ourselves in GHCi:
Prelude&gt; let f :: Bool -&gt; Int; f False = 0
Prelude&gt; f False
0
Prelude&gt; f True
*** Exception: 6:23-33:</p>
<p>CHAPTER 8. FUNCTIONS THAT CALL THEMSELVES 433
Non-exhaustive patterns in function f
Theerrorvalue is still there, but our language implemen-
tation is making it the fallback case because we didn’t write
atotalfunction, that is, a function which handles all of its in-
puts. Because we failed to define ways to handle all potential
inputs, for example through an “otherwise” case, the previous
function was really:
f::Bool-&gt;Int
fFalse=0
f_ =error$&quot;*** Exception: &quot;
++&quot;Non-exhaustive&quot;
++&quot;patterns in function f&quot;
A partial function is one which does not handle all of its
inputs. A total function is one that does. How do we make our
𝑓into a total function? One way is with the use of the datatype
Maybe.
dataMaybea=Nothing |Justa
TheMaybedatatype can take an argument. In the first case,
Nothing , there is no argument; this is our way to say that there
is no result or data from the function without hitting bottom.
The second case, Just a takes an argument and allows us to
return the data we’re wanting. Maybemakes all uses of nilvalues</p>
<p>CHAPTER 8. FUNCTIONS THAT CALL THEMSELVES 434
and most uses of bottom unnecessary. Here’s how we’d use it
with𝑓:
f::Bool-&gt;MaybeInt
fFalse=Just0
f_ =Nothing
Note that the type and both cases all change. Not only do
we replace the errorwith the Nothing value from Maybe, but we
also have to wrap 0 in the Justconstructor from Maybe. If we
don’t do so, we’ll get a type error when we try to load the code,
as you can see:
f::Bool-&gt;MaybeInt
fFalse=0
f_ =Nothing
Prelude&gt; :l code/brokenMaybe1.hs
[1 of 1] Compiling Main
code/brokenMaybe1.hs:3:11:
No instance for (Num (Maybe Int))
arising from the literal ‘0’
In the expression: 0
In an equation for ‘f’: f False = 0</p>
<p>CHAPTER 8. FUNCTIONS THAT CALL THEMSELVES 435
This type error is because, as before, 0 has the type Num a
=&gt; a, so it’s trying to get an instance of NumforMaybe Int . We
can clarify our intent a bit:
f::Bool-&gt;MaybeInt
fFalse=0::Int
f_ =Nothing
And then get a better type error in the bargain:
Prelude&gt; :l code/brokenMaybe2.hs
[1 of 1] Compiling Main
code/brokenMaybe2.hs:3:11:
Couldn't match expected type
‘Maybe Int’ with actual type ‘Int’
In the expression: 0 :: Int
In an equation for ‘f’: f False = 0 :: Int
We’ll explain Maybein more detail a bit later.
8.4 Fibonacci numbers
Another classic demonstration of recursion in functional pro-
gramming is a function that calculates the 𝑛th number in a
Fibonacci sequence. The Fibonacci sequence is a sequence
of numbers in which each number is the sum of the previous</p>
<p>CHAPTER 8. FUNCTIONS THAT CALL THEMSELVES 436
two: 1, 1, 2, 3, 5, 8, 13, 21, 34... and so on. It’s an indefinite
computation that relies on adding two of its own members, so
it’s a perfect candidate for a recursive function. We’re going to
walk through the steps of how we would write such a function
for ourselves to get a better understanding of the reasoning
process.
1.Consider the types
The first thing we’ll consider is the possible type signature
for our function. The Fibonacci sequence only involves
positive whole numbers. The argument to our Fibonacci
function is going to be a positive whole number, because
we’re trying to return the value that is the 𝑛th member of
the Fibonacci sequence. Our result will also be a positive
whole number, since that’s what Fibonacci numbers are.
We would be looking, then, for values that are of the Intor
Integer types. We could use one of those concrete types
or use a typeclass for constrained polymorphism. Specif-
ically, we want a type signature that takes one integral
argument and returns one integral result. So, our type
signature will look something like this:
fibonacci ::Integer -&gt;Integer
-- or
fibonacci ::Integral a=&gt;a-&gt;a
2.Consider the base case</p>
<p>CHAPTER 8. FUNCTIONS THAT CALL THEMSELVES 437
It may sometimes be difficult to determine your base case
up front, but it’s worth thinking about. For one thing,
you do want to ensure that your function will terminate.
For another thing, giving serious consideration to your
base case is a valuable part of understanding how your
function works. Fibonacci numbers are positive integers,
so a reasonable base case is zero. When the recursive
process hits zero, it should terminate.
The Fibonacci sequence is a bit trickier than some, though,
because it needs two base cases. The sequence has to start
oﬀ with two numbers, since two numbers are involved in
computing the next. The next number after zero is 1, and
we add zero to 1 to start the sequence so those will be our
base cases:
fibonacci ::Integral a=&gt;a-&gt;a
fibonacci 0=0
fibonacci 1=1
3.Consider the arguments
We’ve already determined that the argument to our func-
tion, the value to which the function is applied, is an inte-
gral number and represents the member of the sequence
we want to be evaluated. That is, we want to pass a value
such as 10 to this function and have it calculate the 10th</p>
<p>CHAPTER 8. FUNCTIONS THAT CALL THEMSELVES 438
number in the Fibonacci sequence. We only need to have
one variable as a parameter to this function then.
But that argument will also be used as arguments within
the function due to the recursive process. Every Fibonacci
number is the result of adding the preceding two numbers.
So, in addition to a variable 𝑥, we will need to use (x - 1)
and(x - 2) to get both the numbers before our argument.
fibonacci ::Integral a=&gt;a-&gt;a
fibonacci 0=0
fibonacci 1=1
fibonacci x=(x-1) (x-2)
-- note: this doesn't work yet.
4.Consider the recursion
All right, now we come to the heart of the matter. In what
way will this function refer to itself and call itself? Look at
what we’ve worked out so far: what needs to happen next
to produce a Fibonacci number? One thing that needs
to happen is that (x - 1) and(x - 2) need to be added
together to produce a result. Try simply adding those two
together and running the function that way.</p>
<p>CHAPTER 8. FUNCTIONS THAT CALL THEMSELVES 439
fibonacci ::Integral a=&gt;a-&gt;a
fibonacci 0=0
fibonacci 1=1
fibonacci x=(x-1)+(x-2)
If you pass the value 6 to that function, what will happen?
Prelude&gt; fibonacci 6
9
Why? Because ((6 - 1) + (6 - 2)) equals 9. But this isn’t how
we calculate Fibonacci numbers! The sixth member of the
Fibonacci sequence is not ((6 - 1) + (6 - 2)). What we want is
to add the fifth member of the Fibonacci sequence to the
fourth member. That result will be the sixth member of
the sequence. We do this by making the function refer to
itself. In this case, we have to specify that both (x - 1) and
(x - 2) are themselves Fibonacci numbers, so we have to
call the function to itself twice.
fibonacci ::Integral a=&gt;a-&gt;a
fibonacci 0=0
fibonacci 1=1
fibonacci x=
fibonacci (x -1)+fibonacci (x -2)</p>
<p>CHAPTER 8. FUNCTIONS THAT CALL THEMSELVES 440
Now, if we apply this function to the value 6, we will get a
diﬀerent result:
Prelude&gt; fibonacci 6
8
Why? Because it evaluates this recursively:
fibonacci 6=fibonacci 5+fibonacci 4
fibonacci 5=fibonacci 4+fibonacci 3
fibonacci 4=fibonacci 3+fibonacci 2
fibonacci 3=fibonacci 2+fibonacci 1
fibonacci 2=fibonacci 1+fibonacci 0
Zero and 1 have been defined as being equal to zero and</p>
<ol>
<li>So here our recursion stops, and it starts adding the
result:
fibonacci 0 + 0
fibonacci 1 + 1
fibonacci 2 + (1 + 0 =) 1
fibonacci 3 + (1 + 1 =) 2</li>
</ol>
<p>CHAPTER 8. FUNCTIONS THAT CALL THEMSELVES 441
fibonacci 4 + (1 + 2 =) 3
fibonacci 5 = (2 + 3 =) 5
fibonacci 6 = (3 + 5 =) 8
It can be daunting at first to think how you would write a
recursive function and what it means for a function to call
itself. But as you can see, it’s useful when the function will
make reference to its own results in a repeated fashion.
8.5 Integral division from scratch
Many people learned multiplication by memorizing multi-
plication tables, usually up to 10x10 or 12x12 (dozen). In fact,
one can perform multiplication in terms of addition, repeated
over and over. Similarly, one can define integral division in
terms of subtraction.
Let’s think through our recursive division function one step
at a time. First, let’s consider the types we would want to use for
such a function and see if we can construct a reasonable type
signature. When we divide numbers, we have a numerator
and a denominator. When we evaluate 10 / 5 to get the answer
2, 10 is the numerator, 5 the denominator, and 2 the quotient.
So we have at least three numbers here. So, perhaps a type
likeInteger -&gt; Integer -&gt; Integer would be suitable. You could</p>
<p>CHAPTER 8. FUNCTIONS THAT CALL THEMSELVES 442
even add some type synonyms to make it more obvious if you
wished:
dividedBy ::Integer -&gt;Integer -&gt;Integer
dividedBy =div
-- changes to
typeNumerator =Integer
typeDenominator =Integer
typeQuotient =Integer
dividedBy ::Numerator
-&gt;Denominator
-&gt;Quotient
dividedBy =div
Thetypekeyword, instead of the more familiar dataor
newtype , declares a type synonym, or type alias. Those are all
Integer types, but we can give them diﬀerent names to make
them easier for human eyes to distinguish in type signatures.
For this example, we didn’t write out the recursive imple-
mentation of dividedBy we had in mind. As it turns out, when
we write the function, we will want to change the final type sig-
nature a bit, for reasons we’ll see in a minute. Sometimes the</p>
<p>CHAPTER 8. FUNCTIONS THAT CALL THEMSELVES 443
use of type synonyms can improve the clarity and purpose of
your type signatures, so this is something you’ll see, especially
in more complex code. For our relatively simple function, it
may not be necessary.
Next, let’s think through our base case. The way we divide in
terms of subtraction is by stopping when our result of having
subtracted repeatedly is lower than the divisor. If it divides
evenly, it’ll stop at 0:
Solve 20 divided by 4
-- [1] [2]
-- [1]: Dividend or numerator
-- [2]: Divisor or denominator
-- Result is quotient
20 divided by 4 == 20 - 4, 16</p>
<ul>
<li>4, 12</li>
<li>4, 8</li>
<li>4, 4</li>
<li>4, 0
-- 0 is less than 4, so we stopped.
-- We subtracted 5 times, so 20 / 4 == 5
Otherwise, we’ll have a remainder. Let’s look at a case where
it doesn’t divide evenly:
Solve 25 divided by 4</li>
</ul>
<p>CHAPTER 8. FUNCTIONS THAT CALL THEMSELVES 444
25 divided by 4 == 25 - 4, 21</p>
<ul>
<li>4, 17</li>
<li>4, 13</li>
<li>4, 9</li>
<li>4, 5</li>
<li>4, 1
-- we stop at 1, because it's less than 4
In the case of 25 divided by 4, we subtracted 4 six times and
had 1 as our remainder. We can generalize this process of di-
viding whole numbers, returning the quotient and remainder,
into a recursive function which does the repeated subtraction
and counting for us. Since we’d like to return the quotient
andthe remainder, we’re going to return the 2-tuple (,)as the
result of our recursive function.
dividedBy ::Integral a=&gt;a-&gt;a-&gt;(a, a)
dividedBy num denom =go num denom 0
wherego n d count
|n&lt;d=(count, n)
|otherwise =
go (n-d) d (count +1)
We’ve changed the type signature from the one we had
originally worked out, both to make it more polymorphic</li>
</ul>
<p>CHAPTER 8. FUNCTIONS THAT CALL THEMSELVES 445
(Integral a =&gt; a versusInteger ) and also to return the tuple
instead of just an integer.
Here we used a common Haskell idiom called a gofunction.
This allows us to define a function via a where-clause that can
accept more arguments than the top-level function dividedBy
does. In this case, the top-level function takes two arguments,
numanddenom, but we need a third argument in order to keep
trackofhowmanytimeswedothesubtraction. Thatargument
is called countand is defined with a starting value of zero and
is incremented by 1 every time the otherwise case is invoked.
There are two branches in our gofunction. The first case
is the most specific; when the numerator 𝑛is less than the
denominator 𝑑, the recursion stops and returns a result. It
is not significant that we changed the argument names from
𝑛𝑢𝑚and𝑑𝑒𝑛𝑜𝑚 to𝑛and𝑑. Thegofunction has already been
applied to those arguments in the definition of dividedBy so
the𝑛𝑢𝑚,𝑑𝑒𝑛𝑜𝑚 , and 0 are bound to 𝑛,𝑑, and𝑐𝑜𝑢𝑛𝑡in thewhere
clause.
The result is a tuple of 𝑐𝑜𝑢𝑛𝑡and the last value for 𝑛. This is
our base case that stops the recursion and gives a final result.
Here’s an example of how dividedBy expands but with the
code inlined:</p>
<p>CHAPTER 8. FUNCTIONS THAT CALL THEMSELVES 446
dividedBy 102
-- first we'll do this the previous way,
-- but we'll keep track of how many
-- times we subtracted.
10divided by 2==
10-2,8(subtracted 1time)
-2,6(subtracted 2times)
-2,4(subtracted 3times)
-2,2(subtracted 4times)
-2,0(subtracted 5times)
Since the final number was 0, there’s no remainder. We
subtracted five times. So 10 / 2 == 5 .
Now we’ll expand the code:
dividedBy 102=
go1020
|10&lt;2= ...
-- false, skip this branch
|otherwise =go (10-2)2(0+1)
-- otherwise is literally the value True
-- so if first branch fails,
-- this always succeeds</p>
<p>CHAPTER 8. FUNCTIONS THAT CALL THEMSELVES 447
go821
-- 8 isn't &lt; 2, so the otherwise branch
go (8-2)2(1+1)
-- n == 6, d == 2, count == 2
go622
go (6-2)2(2+1)
-- 6 isn't &lt; 2, so the otherwise branch
-- n == 4, d == 2, count == 3
go423
go (4-2)2(3+1)
-- 4 isn't &lt; 2, so the otherwise branch
-- n == 2, d == 2, count == 4
go224
go (2-2)2(4+1)
-- 2 isn't &lt; 2, so the otherwise branch
-- n == 0, d == 2, count == 5</p>
<p>CHAPTER 8. FUNCTIONS THAT CALL THEMSELVES 448
go025
-- the n &lt; d branch is finally evaluated
-- because 0 &lt; 2 is true
-- n == 0, d == 2, count == 5
|0&lt;2=(5,0)
(5,0)
The result of 𝑐𝑜𝑢𝑛𝑡is the quotient, that is, how many times
you can subtract 2 from 10. In a case where there was a remain-
der, that number would be the final value for your numerator
and would be returned as the remainder.
8.6 Chapter Exercises
Review of types
1.What is the type of [[True, False], [True, True], [False,
True]] ?
a)Bool
b)mostly True
c)[a]
d)[[Bool]]
2.Whichofthefollowinghasthesametypeas [[True, False],
[True, True], [False, True]] ?</p>
<p>CHAPTER 8. FUNCTIONS THAT CALL THEMSELVES 449
a)[(True, False), (True, True), (False, True)]
b)[[3 == 3], [6 &gt; 5], [3 &lt; 4]]
c)[3 == 3, 6 &gt; 5, 3 &lt; 4]
d)[&quot;Bool&quot;, &quot;more Bool&quot;, &quot;Booly Bool!&quot;]
3.For the following function
func ::[a]-&gt;[a]-&gt;[a]
funcx y=x++y
which of the following is true?
a)xandymust be of the same type
b)xandymust both be lists
c)ifxis aString thenymust be a String
d)all of the above
4.For the funccode above, which is a valid application of
functo both of its arguments?
a)func &quot;Hello World&quot;
b)func &quot;Hello&quot; &quot;World&quot;
c)func [1, 2, 3] &quot;a, b, c&quot;
d)func [&quot;Hello&quot;, &quot;World&quot;]</p>
<p>CHAPTER 8. FUNCTIONS THAT CALL THEMSELVES 450
Reviewing currying
Given the following definitions, tell us what value results from
further applications.
cattyConny ::String-&gt;String-&gt;String
cattyConny x y=x++&quot; mrow &quot; ++y
-- fill in the types
flippy=flip cattyConny
appedCatty =cattyConny &quot;woops&quot;
frappe=flippy&quot;haha&quot;
1.What is the value of appedCatty &quot;woohoo!&quot; ? Try to deter-
mine the answer for yourself, then test in the REPL.
2.frappe &quot;1&quot;
3.frappe (appedCatty &quot;2&quot;)
4.appedCatty (frappe &quot;blue&quot;)
5.cattyConny (frappe &quot;pink&quot;)
(cattyConny &quot;green&quot; (appedCatty &quot;blue&quot;))
6.cattyConny (flippy &quot;Pugs&quot; &quot;are&quot;) &quot;awesome&quot;</p>
<p>CHAPTER 8. FUNCTIONS THAT CALL THEMSELVES 451
Recursion
1.Write out the steps for reducing dividedBy 15 2 to its final
answer according to the Haskell code.
2.Write a function that recursively sums all numbers from
1 to n, n being the argument. So that if n was 5, you’d add
1 + 2 + 3 + 4 + 5 to get 15. The type should be (Eq a, Num a)
=&gt; a -&gt; a .
3.Write a function that multiplies two integral numbers
using recursive summation. The type should be (Integral
a) =&gt; a -&gt; a -&gt; a .
Fixing dividedBy
Our dividedBy function wasn’t quite ideal. For one thing. It
was a partial function and doesn’t return a result (bottom)
when given a divisor that is 0 or less.
Using the pre-existing divfunction we can see how negative
numbers should be handled:
Prelude&gt; div 10 2
5
Prelude&gt; div 10 (-2)
-5
Prelude&gt; div (-10) (-2)
5</p>
<p>CHAPTER 8. FUNCTIONS THAT CALL THEMSELVES 452
Prelude&gt; div (-10) (2)
-5
The next issue is how to handle zero. Zero is undefined for
division in math, so we ought to use a datatype that lets us say
there was no sensible result when the user divides by zero. If
you need inspiration, consider using the following datatype
to handle this.
dataDividedResult =
ResultInteger
|DividedByZero
McCarthy 91 function
We’re going to describe a function in English, then in math
notation, then show you what your function should return for
some test inputs. Your task is to write the function in Haskell.
The McCarthy 91 function yields 𝑥−10 when𝑥 &gt; 100 and91
otherwise. The function is recursive.
𝑀𝐶(𝑛) =⎧{
⎨{⎩𝑛−10 if𝑛 &gt;100
𝑀𝐶(𝑀𝐶(𝑛+11)) if𝑛 ≤100
mc91=undefined
You haven’t seen mapyet, but all you need to know right
now is that it applies a function to each member of a list and</p>
<p>CHAPTER 8. FUNCTIONS THAT CALL THEMSELVES 453
returns the resulting list. It’ll be explained in more detail in
the next chapter.
Prelude&gt; map mc91 [95..110]
[91,91,91,91,91,91,91,92,93,94,95,96,97,98,99,100]
Numbers into words
moduleWordNumber where
importData.List (intersperse )
digitToWord ::Int-&gt;String
digitToWord n=undefined
digits::Int-&gt;[Int]
digitsn=undefined
wordNumber ::Int-&gt;String
wordNumber n=undefined
Hereundefined is a placeholder to show you where you need
to fill in the functions. The nto the right of the function names
is the argument which will be an integer.
Fill in the implementations of the functions above so that
wordNumber returns the English word version of the Int value.</p>
<p>CHAPTER 8. FUNCTIONS THAT CALL THEMSELVES 454
You will first write a function that turns integers from 0-9 into
their corresponding English words, ”one,” ”two,” and so on.
Then you will write a function that takes the integer, separates
the digits, and returns it as a list of integers. Finally you will
need to apply the first function to the list produced by the sec-
ond function and turn it into a single string with interspersed
hyphens.
We’ve laid out multiple functions for you to consider as you
tackle the problem. You may not need all of them, depend-
ing on how you solve it — these are suggestions. Play with
them and look up their documentation to understand them
in deeper detail.
You will probably find this difficult.
div ::Integral a=&gt;a-&gt;a-&gt;a
mod ::Integral a=&gt;a-&gt;a-&gt;a
map ::(a-&gt;b)-&gt;[a]-&gt;[b]
concat ::[[a]]-&gt;[a]
intersperse ::a-&gt;[a]-&gt;[a]
(++) ::[a]-&gt;[a]-&gt;[a]
(:[]) ::a-&gt;[a]
Also consider:
Prelude&gt; div 135 10
13</p>
<p>CHAPTER 8. FUNCTIONS THAT CALL THEMSELVES 455
Prelude&gt; mod 135 10
5
Prelude&gt; div 13 10
1
Prelude&gt; mod 13 10
3
Here is what your output should look in the REPL when it’s
working:
Prelude&gt; wordNumber 12324546
&quot;one-two-three-two-four-five-four-six&quot;
Prelude&gt;
8.7 Definitions
1.Recursion is a means of computing results that may require
an indefinite amount of work to obtain through the use of
repeated function application. Most recursive functions
that terminate or otherwise do useful work will often have
a case that calls itself and a base case that acts as a backstop
of sorts for the recursion.</p>
<p>CHAPTER 8. FUNCTIONS THAT CALL THEMSELVES 456
-- not recursive
lessOne ::Int-&gt;Int
lessOne n=n-1
-- recursive
zero::Int-&gt;Int
zero0=0
zeron=zero (n -1)</p>
<p>Chapter 9
Lists
If the doors of perception
were cleansed, everything
would appear to man as it
is - infinite.
William Blake
457</p>
<p>CHAPTER 9. THIS THING AND SOME MORE STUFF 458
9.1 Lists
Lists do double duty in Haskell. The first purpose lists serve
is as a way to refer to and process a collection or plurality of
values. The second is as an infinite series of values, usually
generated by a function, which allows them to act as a stream
datatype.
In this chapter, we will:
•explain list’s datatype and how to pattern match on lists;
•practice many standard library functions for operating
on lists;
•learn about the underlying representations of lists;
•see what that representation means for their evaluation;
•and do a whole bunch of exercises!
9.2 The list datatype
The list datatype in Haskell is defined like this:
data[]a=[]|a:[a]
Here[]is the type constructor for lists as well as the data
constructor for the empty list. The []data constructor is a</p>
<p>CHAPTER 9. THIS THING AND SOME MORE STUFF 459
nullary constructor because it takes no arguments. The second
data constructor, in contrast, has arguments. (:)is an infix
operator usually called ‘cons’ (short for construct ). Here cons
takes a value of type 𝑎and a list of type [a]and evaluates to
[a].
Whereas the list datatype as a whole is a sum type, as we can
tell from the |in the definition, the second data constructor (:)
<code>cons</code> is aproduct because it takes two arguments. Remember,
a sum type can be read as an “or” as in the Booldatatype where
you get FalseorTrue. A product is like an “and.” We’re going
to talk more about sum and product types in another chapter,
but for now it will suffice to recognize that a : [a] constructs
a value from two arguments, by adding the 𝑎to the front of
the list [a]. The list datatype is a sum type, though, because
it iseitheran empty list ora single value with more list — not
both.
In English, one can read this as:
data[]a=[]|a:[a]
-- [1] [2] [3] [4] [5] [6]
1.The datatype with the type constructor []
2.takes a single type constructor argument ‘a’
3.at the term level can be constructed via
4.nullary constructor []</p>
<p>CHAPTER 9. THIS THING AND SOME MORE STUFF 460
5.orit can be constructed by
6.data constructor (:)which is a product of a value of the
typeawe mentioned in the type constructor anda value
of type [a], that is, “more list.”
The cons constructor (:)is an infix data constructor and
goes between the two arguments 𝑎and[a]that it accepts. Since
it takes two arguments, it is a product of those two arguments,
like the tuple type (a, b). Unlike a tuple, however, this con-
structor is recursive because it mentions its own type [a]as
one of the members of the product.
If you’re an experienced programmer or took a CS class at
some point, you may be familiar with singly-linked lists. This
is a fair description of the list datatype in Haskell, although
average case performance in some situations changes due
to nonstrict evaluation; however, it can contain infinite data
which makes it also work as a stream datatype, but one that has
the option of ending the stream with the []data constructor.
9.3 Pattern matching on lists
We know we can pattern match on data constructors, and the
data constructors for lists are no exceptions. Here we match
on the first argument to the infix (:)constructor, ignoring the
rest of the list, and return that value:</p>
<p>CHAPTER 9. THIS THING AND SOME MORE STUFF 461
Prelude&gt; let myHead (x : <em>) = x
Prelude&gt; :t myHead
myHead :: [t] -&gt; t
Prelude&gt; myHead [1, 2, 3]
1
We can do the opposite as well:
Prelude&gt; let myTail (</em> : xs) = xs
Prelude&gt; :t myTail
myTail :: [t] -&gt; [t]
Prelude&gt; myTail [1, 2, 3]
[2,3]
We do need to be careful with functions like these. Neither
myHead normyTail has a case to handle an empty list — if we try
to pass them an empty list as an argument, they can’t pattern
match:
Prelude&gt; myHead []
*** Exception:
Non-exhaustive patterns
in function myHead
Prelude&gt; myTail []
*** Exception:
Non-exhaustive patterns</p>
<p>CHAPTER 9. THIS THING AND SOME MORE STUFF 462
in function myTail
The problem is that the type [a] -&gt; a ofmyHead is deceptive
because the [a]type doesn’t guarantee that it’ll have an 𝑎value.
It’s not guaranteed that the list will have at least one value, so
myTail can fail as well. One possibility is putting in a base case:
myTail ::[a]-&gt;[a]
myTail[] =[]
myTail(_:xs)=xs
In that case, our function now evaluates like this:
Prelude&gt; myTail [1..5]
[2,3,4,5]
Prelude&gt; myTail []
[]
Using Maybe A better way to handle this situation is with a
datatype called Maybe. We’ll save a full treatment of Maybefor
a later chapter, but this should give you some idea of how it
works. The idea here is that it makes your failure case explicit,
and as programs get longer and more complex that can be
quite useful.
Let’s try an example using MaybewithmyTail. Instead of
having a base case that returns an empty list, the function</p>
<p>CHAPTER 9. THIS THING AND SOME MORE STUFF 463
written with Maybewould return a result of Nothing . As we can
see below, the Maybedatatype has two potential values, Nothing
orJust a :
Prelude&gt; :info Maybe
data Maybe a = Nothing | Just a
Rewriting myTail to useMaybeis fairly straightforward:
safeTail ::[a]-&gt;Maybe[a]
safeTail []=Nothing
safeTail (x:[])=Nothing
safeTail (_:xs)=Justxs
Notice that our function is still pattern matching on the list.
We’ve made the second base case safeTail (x:[]) = Nothing to
reflect the fact that if your list has only one value inside it, its
tail is an empty list. If you leave this case out, then this function
will return Just [] for lists that have only a head value. Take
a few minutes to play around with this and see how it works.
Then see if you can rewrite the myHead function above using
Maybe.
Later in the book, we’ll also cover a datatype called NonEmpty
which always has at least one value and avoids the empty list
problem.</p>
<p>CHAPTER 9. THIS THING AND SOME MORE STUFF 464
9.4 List’s syntactic sugar
Haskell has some syntactic sugar to accommodate the use of
lists, so that you can write:
Prelude&gt; [1, 2, 3] ++ [4]
[1, 2, 3, 4]
Rather than:
Prelude&gt; (1 : 2 : 3 : []) ++ 4 : []
[1,2,3,4]
The syntactic sugar is here to allow building lists in terms
of the successive applications of ‘cons‘ (:)to a value without
having to tediously type it all out.
When we talk about lists, we often talk about them in terms
of “cons cells” and spines. The syntactic sugar obscures this
underlying construction, but looking at the desugared ver-
sion above may make it more clear. The cons cells are the
list datatype’s second data constructor, a : [a] , the result of
recursively prepending a value to “more list.” The cons cell is
aconceptual space that values may inhabit.
The spine is the connective structure that holds the cons
cells together and in place. As we will soon see, this structure
nests the cons cells rather than ordering them in a right-to-
left row. Because diﬀerent functions may treat the spine and</p>
<p>CHAPTER 9. THIS THING AND SOME MORE STUFF 465
the cons cells diﬀerently, it is important to understand this
underlying structure.
9.5 Using ranges to construct lists
There are several ways we can construct lists. One of the
simplest is with ranges. The basic syntax is to make a list that
has the element you want to start the list from followed by
two dots followed by the value you want as the final element
in the list. Here are some examples using the range syntax,
followed by the desugared equivalents using functions from
theEnumtypeclass:
Prelude&gt; [1..10]
[1,2,3,4,5,6,7,8,9,10]
Prelude&gt; enumFromTo 1 10
[1,2,3,4,5,6,7,8,9,10]
Prelude&gt; [1,2..10]
[1,2,3,4,5,6,7,8,9,10]
Prelude&gt; enumFromThenTo 1 2 10
[1,2,3,4,5,6,7,8,9,10]
Prelude&gt; [1,3..10]
[1,3,5,7,9]
Prelude&gt; enumFromThenTo 1 3 10</p>
<p>CHAPTER 9. THIS THING AND SOME MORE STUFF 466
[1,3,5,7,9]
Prelude&gt; [2,4..10]
[2,4,6,8,10]
Prelude&gt; enumFromThenTo 2 4 10
[2,4,6,8,10]
Prelude&gt; ['t'..'z']
&quot;tuvwxyz&quot;
Prelude&gt; enumFromTo 't' 'z'
&quot;tuvwxyz&quot;
The types of the functions underlying the range syntax are:
enumFrom ::Enuma
=&gt;a-&gt;[a]
enumFromThen ::Enuma
=&gt;a-&gt;a-&gt;[a]
enumFromTo ::Enuma
=&gt;a-&gt;a-&gt;[a]
enumFromThenTo ::Enuma
=&gt;a-&gt;a-&gt;a-&gt;[a]
All of these functions require that the type being “ranged”
have an instance of the Enumtypeclass. The first two functions,</p>
<p>CHAPTER 9. THIS THING AND SOME MORE STUFF 467
enumFrom andenumFromThen , generate lists of indefinite, possibly
infinite, length. For it to create an infinitely long list, you
must be ranging over a type that has no upper bound in its
enumeration. Integer is such a type. You can make Integer
values as large as you have memory to describe.
Be aware that enumFromTo must have its first argument be
lower than the second argument:
Prelude&gt; enumFromTo 3 1
[]
Prelude&gt; enumFromTo 1 3
[1,2,3]
Otherwise you’ll get an empty list.
Exercise: EnumFromTo
Some things you’ll want to know about the Enumtypeclass:
Prelude&gt; :info Enum
class Enum a where
succ :: a -&gt; a
pred :: a -&gt; a
toEnum :: Int -&gt; a
fromEnum :: a -&gt; Int
enumFrom :: a -&gt; [a]
enumFromThen :: a -&gt; a -&gt; [a]</p>
<p>CHAPTER 9. THIS THING AND SOME MORE STUFF 468
enumFromTo :: a -&gt; a -&gt; [a]
enumFromThenTo :: a -&gt; a -&gt; a -&gt; [a]
Prelude&gt; succ 0
1
Prelude&gt; succ 1
2
Prelude&gt; succ 'a'
'b'
Write your own enumFromTo definitions for the types pro-
vided. Do not use range syntax to do so. It should return the
same results as if you did [start..stop] . Replace the undefined ,
an value which results in an error when evaluated, with your
own definition.</p>
<p>CHAPTER 9. THIS THING AND SOME MORE STUFF 469
eftBool ::Bool-&gt;Bool-&gt;[Bool]
eftBool =undefined
eftOrd::Ordering
-&gt;Ordering
-&gt;[Ordering ]
eftOrd=undefined
eftInt::Int-&gt;Int-&gt;[Int]
eftInt=undefined
eftChar ::Char-&gt;Char-&gt;[Char]
eftChar =undefined
9.6 Extracting portions of lists
In this section, we’ll take a look at some useful functions for
extracting portions of a list and dividing lists into parts. The
first three functions have similar type signatures, taking Int
arguments and applying them to a list argument:
take::Int-&gt;[a]-&gt;[a]
drop::Int-&gt;[a]-&gt;[a]
splitAt ::Int-&gt;[a]-&gt;([a], [a])</p>
<p>CHAPTER 9. THIS THING AND SOME MORE STUFF 470
We have seen examples of some of the above functions in
previous chapters, but they are common and useful enough
they deserve review.
Thetakefunction takes the specified number of elements
out of a list and returns a list containing just those elements.
As you can see it takes one argument that is an Intand applies
that to a list argument. Here’s how it works:
Prelude&gt; take 7 ['a'..'z']
&quot;abcdefg&quot;
Prelude&gt; take 3 [1..10]
[1,2,3]
Prelude&gt; take 3 []
[]
Notice that when we pass it an empty list as an argument,
it returns an empty list. These lists use the syntactic sugar
for building lists with ranges. We can also use takewith a list-
building function, such as enumFrom . Reminder: enumFrom can
generate an infinite list if the type of list inhabitants is, such
asInteger , an infinite set. But as long as we’re only taking a
certain number of elements from that, it won’t generate an
infinite list:
Prelude&gt; take 10 (enumFrom 10)</p>
<p>CHAPTER 9. THIS THING AND SOME MORE STUFF 471
[10,11,12,13,14,15,16,17,18,19]
Thedropfunction is similar to takebut drops the specified
number of elements oﬀ the beginning of the list. Again, we
can use it with ranges or list-building functions:
Prelude&gt; drop 5 [1..10]
[6,7,8,9,10]
Prelude&gt; drop 8 ['a'..'z']
&quot;ijklmnopqrstuvwxyz&quot;
Prelude&gt; drop 4 []
[]
Prelude&gt; drop 2 (enumFromTo 10 20)
[12,13,14,15,16,17,18,19,20]
ThesplitAt function cuts a list into two parts at the element
specified by the Intand makes a tuple of two lists:
Prelude&gt; splitAt 5 [1..10]
([1,2,3,4,5],[6,7,8,9,10])
Prelude&gt; splitAt 10 ['a'..'z']
(&quot;abcdefghij&quot;,&quot;klmnopqrstuvwxyz&quot;)</p>
<p>CHAPTER 9. THIS THING AND SOME MORE STUFF 472
Prelude&gt; splitAt 5 []
([],[])
Prelude&gt; splitAt 3 (enumFromTo 5 15)
([5,6,7],[8,9,10,11,12,13,14,15])
The higher-order functions takeWhile anddropWhile are a bit
diﬀerent, as you can see from the type signatures:
takeWhile ::(a-&gt;Bool)-&gt;[a]-&gt;[a]
dropWhile ::(a-&gt;Bool)-&gt;[a]-&gt;[a]
So these take and drop items out of a list that meet some
condition, as we can see from the presence of Bool.takeWhile
will take elements out of a list that meet that condition and
then stop when it meets the first element that doesn’t satisfy
the condition:
Take the elements that are less than 3:
Prelude&gt; takeWhile (&lt;3) [1..10]
[1,2]
Take the elements that are less than 8:
Prelude&gt; takeWhile (&lt;8) (enumFromTo 5 15)
[5,6,7]</p>
<p>CHAPTER 9. THIS THING AND SOME MORE STUFF 473
The next example returns an empty list because it stops
taking as soon as the condition isn’t met, which in this case is
the first element:
Prelude&gt; takeWhile (&gt;6) [1..10]
[]
In the final example above, why does it only return a single
𝑎?
Prelude&gt; takeWhile (=='a') &quot;abracadabra&quot;
&quot;a&quot;
Now, we’ll look at dropWhile whose behavior is probably
predictable based on the functions and type signatures we’ve
already seen in this section. We will use the same arguments
as we used with takeWhile so the diﬀerence between them is
easy to see:
Prelude&gt; dropWhile (&lt;3) [1..10]
[3,4,5,6,7,8,9,10]
Prelude&gt; dropWhile (&lt;8) (enumFromTo 5 15)
[8,9,10,11,12,13,14,15]
Prelude&gt; dropWhile (&gt;6) [1..10]
[1,2,3,4,5,6,7,8,9,10]</p>
<p>CHAPTER 9. THIS THING AND SOME MORE STUFF 474
Prelude&gt; dropWhile (=='a') &quot;abracadabra&quot;
&quot;bracadabra&quot;
Exercises: Thy Fearful Symmetry
1.UsingtakeWhile anddropWhile , write a function that takes a
string and returns a list of strings, using spaces to separate
the elements of the string into words, as in the following
sample:
Prelude&gt; myWords &quot;sheryl wants fun&quot;
[&quot;wallfish&quot;, &quot;wants&quot;, &quot;fun&quot;]
2.Next, write a function that takes a string and returns a list
of strings, using newline separators to break up the string
as in the following (your job is to fill in the undefined
function):</p>
<p>CHAPTER 9. THIS THING AND SOME MORE STUFF 475
modulePoemLines where
firstSen =&quot;Tyger Tyger, burning bright \n&quot;
secondSen =&quot;In the forests of the night \n&quot;
thirdSen =&quot;What immortal hand or eye \n&quot;
fourthSen =&quot;Could frame thy fearful <br />
\symmetry?&quot;
sentences =firstSen ++secondSen
++thirdSen ++fourthSen
-- putStrLn sentences -- should print
-- Tyger Tyger, burning bright
-- In the forests of the night
-- What immortal hand or eye
-- Could frame thy fearful symmetry?
-- Implement this
myLines ::String-&gt;[String]
myLines =undefined</p>
<p>CHAPTER 9. THIS THING AND SOME MORE STUFF 476
-- What we want 'myLines sentences'
-- to equal
shouldEqual =
[&quot;Tyger Tyger, burning bright&quot;
,&quot;In the forests of the night&quot;
,&quot;What immortal hand or eye&quot;
,&quot;Could frame thy fearful symmetry?&quot;
]
-- The main function here is a small test
-- to ensure you've written your function
-- correctly.
main::IO()
main=
print$
&quot;Are they equal? &quot;
++show (myLines sentences
==shouldEqual)
3.Now let’s look at what those two functions have in com-
mon. Try writing a new function that parameterizes the
character you’re breaking the string argument on and
rewrite myWords andmyLines using it.</p>
<p>CHAPTER 9. THIS THING AND SOME MORE STUFF 477
9.7 List comprehensions
List comprehensions are a means of generating a new list
from a list or lists. They come directly from the concept of
set comprehensions in mathematics, including similar syntax.
They must have at least one list, called the generator, that gives
the input for the comprehension, that is, provides the set of
items from which the new list will be constructed. They may
have conditions to determine which elements are drawn from
the list and/or functions applied to those elements.
Let’s start by looking at a very simple example:
[ x^2|x&lt;-[1..10]]
-- [1] [2] [ 3 ]
1.This is the output function that will apply to the members
of the list we indicate.
2.The pipe here designates the separation between the out-
put function and the input.
3.This is the input set: a generator list and a variable that
represents the elements that will be drawn from that list.
This says, “from a list of numbers from 1-10, take (&lt;-)
each element as an input to the output function.”
In plain English, that list comprehension will produce a
new list that includes the square of every number from 1 to 10:</p>
<p>CHAPTER 9. THIS THING AND SOME MORE STUFF 478
Prelude&gt; [x^2 | x &lt;- [1..10]]
[1,4,9,16,25,36,49,64,81,100]
Now we’ll look at some ways to vary what elements are
drawn from the generator list(s).
Adding predicates
List comprehensions can optionally take predicates that limit
the elements drawn from the generator list. The predicates
must evaluate to Boolvalues, as in other condition-placing
function types we’ve looked at (for example, guards). Then the
items drawn from the list and passed to the output function
will only be those that met the Truecase in the predicate.
For example, let’s say we wanted a similar list comprehen-
sion as we used above, but this time we wanted our new list to
contain the squares of only the even numbers while ignoring
the odds. In that case, we put a comma after our generator list
and add the condition:
Prelude&gt; [x^2 | x &lt;- [1..10], rem x 2 == 0]
[4,16,36,64,100]
Here we’ve specified that the only elements to take from
the generator list as 𝑥are those that, when divided by 2, have
a remainder of zero — that is, even numbers.</p>
<p>CHAPTER 9. THIS THING AND SOME MORE STUFF 479
We can also write list comprehensions that have multiple
generators. One thing to note is that the rightmost generator
will be exhausted first, then the second rightmost, and so on.
For example, let’s say you wanted to make a list of 𝑥to
the𝑦power, instead of squaring all of them as we did above.
Separate the two inputs with a comma as below:
Prelude&gt; [x^y | x &lt;- [1..5], y &lt;- [2, 3]]
[1,1,4,8,9,27,16,64,25,125]
When we examine the resulting list, we see that it is each
𝑥value first to the second power and then to the third power,
followed by the next 𝑥value to the second and then to the
third and so on, ending with the result of 5^2and5^3. We are
applying the function to each possible pairing of values from
the two lists we’re binding values out of. It begins by trying
to get a value out of the leftmost generator, from which we’re
getting 𝑥.
We could put a condition on that, too. Let’s say we only
want to return the list of values that are less than 200. We add
another comma and write our predicate:
Prelude&gt; :{
Prelude| [x ^ y |
Prelude| x &lt;- [1..10],
Prelude| y &lt;- [2, 3],
Prelude| x ^ y &lt; 200]</p>
<p>CHAPTER 9. THIS THING AND SOME MORE STUFF 480
Prelude| :}
[1,1,4,8,9,27,16,64,25,125,36,49,64,81,100]
We can use multiple generators to turn two lists into a list
of tuples containing those elements as well. The generator
lists don’t even have to be of the same length or, due to the
nature of the tuple type, even the same type:
Prelude&gt; :{
Prelude| [(x, y) |
Prelude| x &lt;- [1, 2, 3],
Prelude| y &lt;- [6, 7]]
Prelude| :}
[(1,6),(1,7),(2,6),(2,7),(3,6),(3,7)]
Prelude&gt; :{
Prelude| [(x, y) |
Prelude| x &lt;- [1, 2, 3],
Prelude| y &lt;- ['a', 'b']]
Prelude| :}
[(1,'a'),(1,'b'),(2,'a'),
(2,'b'),(3,'a'),(3,'b')]
Again the pattern is that it generates every possible tuple
for the first 𝑥value, then it moves to the next 𝑥value and so
on.</p>
<p>CHAPTER 9. THIS THING AND SOME MORE STUFF 481
Recall that the first list comprehension we looked at gen-
erated a list of all the values of 𝑥^2when𝑥is a number from
1-10. Let’s say you wanted to use that list in another list com-
prehension. First, you’d want to give that list a name. Let’s call
itmySqr:
Prelude&gt; let mySqr = [x^2 | x &lt;- [1..10]]
Now we can use that list as the generator for another list
comprehension. Here, we will limit our input values to those
that are less than 4 for the sake of brevity:
Prelude&gt; let mySqr = [x^2 | x &lt;- [1..10]]
Prelude&gt; :{
Prelude| [(x, y) |
Prelude| x &lt;- mySqr,
Prelude| y &lt;- [1..3], x &lt; 4]
Prelude| :}
[(1,1),(1,2),(1,3)]
Exercises: Comprehend Thy Lists
Take a look at the following functions, figure what you think
the output lists will be, and then run them in your REPL to
verify (note that you will need the mySqrlist from above in
scope to do this):
[x | x &lt;- mySqr, rem x 2 == 0]</p>
<p>CHAPTER 9. THIS THING AND SOME MORE STUFF 482
[(x, y) | x &lt;- mySqr,
y &lt;- mySqr,
x &lt; 50, y &gt; 50]
take 5 [ (x, y) | x &lt;- mySqr,
y &lt;- mySqr,
x &lt; 50, y &gt; 50 ]
List comprehensions with Strings
It’s worth remembering that strings are lists, so list comprehen-
sions can also be used with strings. We’re going to introduce
a standard function called elem1that tells you whether an el-
ement is in a list or not. It evaluates to a Boolvalue, so it is
useful as a predicate in list comprehensions:
Prelude&gt; :t elem
elem :: Eq a =&gt; a -&gt; [a] -&gt; Bool
Prelude&gt; elem 'a' &quot;abracadabra&quot;
True
Prelude&gt; elem 'a' &quot;Julie&quot;
False
In the first case, ‘a’ is an element of “abracadabra” so that
evaluates to True, but in the second case, there is no ‘a’ in
1Reminder, pretend Foldable in the type of elemmeans it’s a list until we cover Foldable
later.</p>
<p>CHAPTER 9. THIS THING AND SOME MORE STUFF 483
“Julie” so we get a Falseresult. As you can see from the type
signature, elemdoesn’t only work with characters and strings,
but that’s what we’ll use it for here. Let’s see if we can write a
list comprehension to remove all the lowercase letters from
a string. Here our condition is that we only want to take 𝑥
from our generator list when it meets the condition that it is
an element of the list of capital letters:
Prelude&gt; :{
Prelude| [x |
Prelude| x &lt;- &quot;Three Letter Acronym&quot;,
Prelude| elem x ['A'..'Z']]
Prelude| :}
&quot;TLA&quot;
Let’s see if we can now generalize this into an acronym
generator that will accept diﬀerent strings as inputs, instead of
forcing us to rewrite the whole list comprehension for every
string we might want to feed it. We will do this by naming
a function that will take one argument and use that as the
generator string for our list comprehension. So the function
argument and the generator string will need to be the same
thing:
Prelude&gt; :{
Prelude| let acro xs =
Prelude| [x | x &lt;- xs,</p>
<p>CHAPTER 9. THIS THING AND SOME MORE STUFF 484
Prelude| elem x ['A'..'Z']]
Prelude| :}
We use𝑥𝑠for our function argument to indicate to ourselves
that it’s a list, that the 𝑥is plural. It doesn’t have to be; you
could use a diﬀerent variable there and obtain the same result.
It is idiomatic to use a “plural” variable for list arguments, but
it is not necessary.
All right, so we have our acrofunction with which we can
generate acronyms from any string:
Prelude&gt; acro &quot;Self Contained Underwater Breathing Apparatus&quot;
&quot;SCUBA&quot;
Prelude&gt; acro &quot;National Aeronautics and Space Administration&quot;
&quot;NASA&quot;
Given the above, what do you think this function would do:
Prelude&gt; let myString xs = [x | x &lt;- xs, elem x &quot;aeiou&quot;]
Exercises: Square Cube
Given the following:
Prelude&gt; let mySqr = [x^2 | x &lt;- [1..5]]
Prelude&gt; let myCube = [y^3 | y &lt;- [1..5]]
1.First write an expression that will make tuples of the out-
puts of mySqrandmyCube .</p>
<p>CHAPTER 9. THIS THING AND SOME MORE STUFF 485
2.Now alter that expression so that it only uses the x and y
values that are less than 50.
3.Apply another function to that list comprehension to
determine how many tuples inhabit your output list.
9.8 Spines and nonstrict evaluation
As we have seen, lists are a recursive series of cons cells a : [a]
terminated by the empty list [], but we want a way to visu-
alize this structure in order to understand the ways lists get
processed. When we talk about data structures in Haskell, par-
ticularly lists, sequences, and trees, we talk about them having
aspine. This is the connective structure that ties the collection
of values together. In the case of a list, the spine is usually tex-
tually represented by the recursive cons (:)operators. Given
the data: [1, 2, 3] , we get a list that looks like:
1 : 2 : 3 : []
or
1 : (2 : (3 : []))
:
/ <br />
1 :
/ </p>
<p>CHAPTER 9. THIS THING AND SOME MORE STUFF 486
2 :
/ <br />
3 []
The problem with the 1 : (2 : (3 : [])) representation we
used earlier is that it makes it seem like the value 1 exists
“before” the cons (:)cell that contains it, but actually, the cons
cells contain the values. Because of this and the way nonstrict
evaluation works, you can evaluate cons cells independently
of what they contain. It is possible to evaluate only the spine of
the list without evaluating individual values. It is also possible
to evaluate only part of the spine of a list and not the rest of it.
Evaluation of the list in this representation proceeds down
the spine. However, constructing the list (when that is neces-
sary) proceeds upthe spine. In the example above, then, we
start with an infix operator, evaluate the arguments 1 and a
new cons cell, and proceed downward to the 3 and empty list.
But when we need to build the list, to print it in the REPL for
example, it proceeds from the bottom of the list up the spine,
first putting the 3 into the empty list, then adding the 2 to
the front of that list, then, finally, putting the 1 in the front of
that. Because Haskell’s evaluation is nonstrict, the list isn’t con-
structed until it’s consumed — indeed, nothing is evaluated
until it must be. Until it’s consumed or you force strictness
in some way, there are a series of placeholders as a blueprint</p>
<p>CHAPTER 9. THIS THING AND SOME MORE STUFF 487
of the list that can be constructed when it’s needed. We’ll talk
more about nonstrictness soon.
We’regoingtobring ⊥orbottom backintheformof undefined
in order to demonstrate some of the eﬀects of nonstrict evalu-
ation. Here we’re going to use _to syntactically signify values
we are ignoring and not evaluating. The underscores repre-
sent the values contained by the cons cells. The spine is the
recursive series of cons constructors signified by (:)as you
can see below:
: &lt;------|
/ \ |
_ : &lt;----| This is the &quot;spine&quot;
/ \ |
_ : &lt;--|
/ <br />
_ []
You’ll see the term ‘spine’ used in reference to data struc-
tures, such as trees, that aren’t lists. In the case of a list, the
spine is a linear succession of one cons cell wrapping another
cons cell. With data structures like trees, which we will cover
later, you’ll see that the spine can be nodes that contain 2 or
more nodes.</p>
<p>CHAPTER 9. THIS THING AND SOME MORE STUFF 488
Using GHCi’s :sprint command
We can use a special command in GHCi called sprint to print
variables and see what has been evaluated already, with the un-
derscore representing expressions that haven’t been evaluated
yet.
A warning : We always encourage you to experiment and
explore for yourself after seeing the examples in this book, but
:sprint has some behavioral quirks that can be a bit frustrating.
GHC Haskell has some opportunistic optimizations which
introduce strictness to make code faster when it won’t change
how your code evaluates. Additionally polymorphism means
values like Num a =&gt; a are really waiting for a sort of argument
which will make it concrete (this will be covered in more detail
in a later chapter). To avoid this, you have to assign a more
concrete type such as IntorDouble, otherwise it stays uneval-
uated,_, in:sprint ’s output. If you can keep these caveats to
:sprint ’s behavior in mind, it can be useful. Otherwise if you
find it confusing, don’t sweat it and wait for us to elaborate
more deeply in the chapter on nonstrictness.
Let’s define a list using enumFromTo , which is tantamount to
using syntax like ['a'..'z'] , then ask for the state of blahwith
respect to whether it has been evaluated:
Prelude&gt; let blah = enumFromTo 'a' 'z'
Prelude&gt; :sprint blah</p>
<p>CHAPTER 9. THIS THING AND SOME MORE STUFF 489
blah = _
Theblah = _ indicates that blahis totally unevaluated.
Next we’ll take one value from blahand then evaluate it by
forcing GHCi to print the expression:
Prelude&gt; take 1 blah
&quot;a&quot;
Prelude&gt; :sprint blah
blah = 'a' : _
So we’ve evaluated a cons cell :and the first value 'a'.
Then we take two values and print them — which forces
evaluation of the second cons cell and the second value:
Prelude&gt; take 2 blah
&quot;ab&quot;
Prelude&gt; :sprint blah
blah = 'a' : 'b' : _
Assuming this is a contiguous GHCi session, the first cons
cell and value were already forced.
We can keep going with this, evaluating the list one value
at a time:
Prelude&gt; take 3 blah
&quot;abc&quot;
Prelude&gt; :sprint blah</p>
<p>CHAPTER 9. THIS THING AND SOME MORE STUFF 490
blah = 'a' : 'b' : 'c' : _
Thelength function is only strict in the spine, meaning it
only forces evaluation of the spine of a list, not the values,
something we can see if we try to find the length of a list of
undefined values. But when we use length onblah,:sprint will
behave as though we had forced evaluation of the values as
well:
Prelude&gt; length blah
26
Prelude&gt; :sprint blah
blah = &quot;abcdefghijklmnopqrstuvwxyz&quot;
That the individual characters were shown as evaluated
and not exclusively the spine after getting the length of blahis
one of the unfortunate aforementioned quirks of how GHCi
evaluates code.
Spines are evaluated independently of values
Values in Haskell get reduced to weak head normal form by
default. By ‘normal form’ we mean that the expression is fully
evaluated. ‘Weak head normal form’ means the expression is
only evaluated as far as is necessary to reach a data constructor.</p>
<p>CHAPTER 9. THIS THING AND SOME MORE STUFF 491
Weak head normal form (WHNF) is a larger set and con-
tains both the possibility that the expression is fully evalu-
ated (normal form) and the possibility that the expression has
been evaluated to the point of arriving at a data constructor
or lambda awaiting an argument. For an expression in weak
head normal form, further evaluation may be possible once
another argument is provided. If no further inputs are pos-
sible, then it is still in WHNF but also in normal form (NF).
We’re going to explain this more fully later in the book in the
chapter on nonstrictness when we show you how call-by-need
works and the implications for Haskell. For now, we’ll look at
a few examples to get a sense for what might be going on.
Below we list some expressions and whether they are in
WHNF, NF, both, or neither:
(1,2)-- WHNF &amp; NF
This first example is in normal form and is fully evaluated.
Anything in normal form is by definition also in weak head
normal form, because weak head is an expression which is
evaluated up to at least the first data constructor. Normal
form exceeds that by requiring that all subexpressions be fully
evaluated. Here the components of the value are the tuple
data constructor and the values 1 and 2.
(1,1+1)</p>
<p>CHAPTER 9. THIS THING AND SOME MORE STUFF 492
This example is in WHNF, but not NF. The (+)applied to
its arguments could be evaluated but hasn’t been yet.
\x-&gt;x<em>10-- WHNF &amp; NF
This anonymous function is in normal form because while
(</em>)has been applied to two arguments of a sort, it cannot be
reduced further until the outer x -&gt; ... has been applied.
With nothing further to reduce, it is in normal form.
&quot;Papu&quot;++&quot;chon&quot;
This string concatenation is in neither WHNF nor NF, this
is because the outermost component of the expression is a
function, (++), whose arguments are fully applied but it hasn’t
been evaluated. Whereas, the following would be in WHNF
but not NF:
(1,&quot;Papu&quot;++&quot;chon&quot;)
When we define a list and define all its values, it is in NF
and all its values are known. There’s nothing left to evaluate
at that point, such as in the following example:
Prelude&gt; let num :: [Int]; num = [1, 2, 3]
Prelude&gt; :sprint num
num = [1,2,3]</p>
<p>CHAPTER 9. THIS THING AND SOME MORE STUFF 493
We can also construct a list through ranges or functions.
In this case, the list is in WHNF but not NF. The compiler
only evaluates the head or first node of the graph, but just the
cons constructor, not the value or rest of the list it contains.
We know there’s a value of type 𝑎in the cons cell we haven’t
evaluated and a “rest of list” which might either be the empty
list[]which ends the list or another cons cell — we don’t know
which because we haven’t evaluated the next [a]value yet. We
saw that above in the :sprint section, and you can see that
evaluation of the first values does not force evaluation of the
rest of the list:
Prelude&gt; let myNum :: [Int]; myNum = [1..10]
Prelude&gt; :sprint myNum
myNum = _
Prelude&gt; take 2 myNum
[1,2]
Prelude&gt; :sprint myNum
myNum = 1 : 2 : _
This is an example of WHNF evaluation. It’s weak head
normal form because the list has to be constructed by the
range and it’s only going to evaluate as far as it has to. With
take 2, we only need to evaluate the first two cons cells and
the values they contain, which is why when we used :sprint
we only saw 1 : 2 : _ . Evaluating to normal form would’ve</p>
<p>CHAPTER 9. THIS THING AND SOME MORE STUFF 494
meant recursing through the entire list, forcing not only the
entire spine but also the values each cons cell contained.
In these tree representations, evaluation or consumption of
the list goes downthe spine. The following is a representation
of a list that isn’t spine strict and is awaiting something to force
the evaluation:
:
/ <br />
_ _
By default, it stops here and never evaluates even the first
cons cell unless it’s forced to, as we saw.
However, functions that are spine strict can force complete
evaluation of the spine of the list even if they don’t force eval-
uation of each value. Pattern matching is strict by default, so
pattern matching on cons cells can mean forcing spine strict-
ness if your function doesn’t stop recursing the list. It can
evaluate the spine only or the spine as well as the values that
inhabit each cons cell, depending on context.
On the other hand, length is strict in the spine but not the
values. If we defined a list such as [1, 2, 3] , usinglength on it
would force evaluation of the entire spine without accompa-
nying strictness in the values:
:
/ </p>
<p>CHAPTER 9. THIS THING AND SOME MORE STUFF 495
_ :
/ <br />
_ :
/ <br />
_ []
We can see this if we use length but make one of the values
bottom with the undefined value, and see what happens:
Prelude&gt; let x = [1, undefined, 3]
Prelude&gt; length x
3
The first and third values in the list were numbers, but the
second value was undefined andlength didn’t make it crash.
Why? Because length measures the length of a list, which only
requires recursing the spine and counting how many cons cells
there are. We could define our own length function ourselves
like so:
-- <em>Not</em> identical to the length
-- function in Prelude
length::[a]-&gt;Integer
length[]=0
length(_:xs)=1+length xs</p>
<p>CHAPTER 9. THIS THING AND SOME MORE STUFF 496
One thing to note is that we use _to ignore the values in our
arguments or that are part of a pattern match. In this case, we
pattern-matched on the (:)data constructor, but wanted to
ignore the value which is the first argument. However, it’s not
a mere convention to bind references we don’t care about on
the left-hand side to <em>. You can’t bind arguments to the name
”</em>”; it’s part of the language. This is partly so the compiler
knows for a certainty you won’t ever evaluate something in
that particular case. Currently, if you try using _on the right-
hand side in the definition, it’ll think you’re trying to refer to
a hole.
We’re only forcing the (:)constructors and the []at the
end in order to count the number of values contained by the
list:
: &lt;-|
/ \ |
|-&gt; _ : &lt;-|
| / \ | These got evaluated (forced)
|-&gt; _ : &lt;-|
| / \ |
|-&gt; _ [] &lt;-|
|
| These did not
However, length will throw an error on a bottom value if
part of the spine itself is bottom:</p>
<p>CHAPTER 9. THIS THING AND SOME MORE STUFF 497
Prelude&gt; let x = [1] ++ undefined ++ [3]
Prelude&gt; x
[1*** Exception: Prelude.undefined
Prelude&gt; length x
*** Exception: Prelude.undefined
Printing the list fails, although it gets as far as printing the
first[and the first value, and attempting to get the length also
fails because it can’t count undefined spine values.
It’s possible to write functions which will force both the
spine and the values. sumis an example because in order to
return a result at all, it must return the sum of all values in the
list.
We’ll write our own sumfunction for the sake of demonstra-
tion:
mySum::Numa=&gt;[a]-&gt;a
mySum[]=0
mySum(x:xs)=x+mySum xs
First, the +operator is strict in both of its arguments, so that
will force evaluation of the values and the mySum xs . Therefore
mySumwill keep recursing until it hits the empty list and must
stop. Then it will start going back up the spine of the list,
summing the inhabitants as it goes. It looks something like
this (the zero represents our empty list):</p>
<p>CHAPTER 9. THIS THING AND SOME MORE STUFF 498
Prelude&gt; mySum [1..5]
1 + (2 + (3 + (4 + (5 + 0))))
1 + (2 + (3 + (4 + 5)))
1 + (2 + (3 + 9))
1 + (2 + 12)
1 + 14
15
We will be returning to this topic at various points in the
book because developing intuition for Haskell’s evaluation
strategies takes time and practice. If you don’t feel like you
fully understand it at this point, that’s OK. It’s a complex topic,
and it’s better to approach it in stages.
Exercises: Bottom Madness
Will it blow up?
Will the following expressions return a value or be ⊥?
1.[x^y|x&lt;-[1..5], y&lt;-[2, undefined]]
2.take1$
[x^y|x&lt;-[1..5], y&lt;-[2, undefined]]
3.sum[1, undefined, 3]
4.length[1,2, undefined]</p>
<p>CHAPTER 9. THIS THING AND SOME MORE STUFF 499
5.length$[1,2,3]++undefined
6.take1$filter even [ 1,2,3, undefined]
7.take1$filter even [ 1,3, undefined]
8.take1$filter odd [ 1,3, undefined]
9.take2$filter odd [ 1,3, undefined]
10.take3$filter odd [ 1,3, undefined]
Intermission: Is it in normal form?
For each expression below, determine whether it’s in:
1.normal form, which implies weak head normal form;
2.weak head normal form only; or,
3.neither.
Remember that an expression cannot be in normal form or
weak head normal form if the outermost part of the expression
isn’t a data constructor. It can’t be in normal form if any part
of the expression is unevaluated.
1.[1,2,3,4,5]
2.1:2:3:4:_</p>
<p>CHAPTER 9. THIS THING AND SOME MORE STUFF 500
3.enumFromTo 110
4.length[1,2,3,4,5]
5.sum(enumFromTo 110)
6.['a'..'m']++['n'..'z']
7.(_,'b')
9.9 Transforming lists of values
We have already seen how we can make recursive functions
with self-referential expressions. It’s a useful tool and a core
part of the logic of Haskell. In truth, in part because Haskell
uses nonstrict evaluation, we tend to use higher-order func-
tions for transforming data rather than manually recursing
over and over.
For example, one common thing you would want to do is
return a list with a function applied uniformly to all values
within the list. To do so, you need a function that is inherently
recursive and can apply that function to each member of the
list. For this purpose we can use either the maporfmapfunctions.
mapcan only be used with [].fmapis defined in a typeclass
named Functor and can be applied to data other than lists. We
will learn more about Functor later; for now, we’ll focus on the
list usage. Here are some examples using mapandfmap:</p>
<p>CHAPTER 9. THIS THING AND SOME MORE STUFF 501
Prelude&gt; map (+1) [1, 2, 3, 4]
[2,3,4,5]
Prelude&gt; map (1-) [1, 2, 3, 4]
[0,-1,-2,-3]
Prelude&gt; fmap (+1) [1, 2, 3, 4]
[2,3,4,5]
Prelude&gt; fmap (2*) [1, 2, 3, 4]
[2,4,6,8]
Prelude&gt; fmap id [1, 2, 3]
[1,2,3]
Prelude&gt; map id [1, 2, 3]
[1,2,3]
The types of mapandfmaprespectively are:
map:: (a-&gt;b)-&gt;[a]-&gt;[b]
fmap::Functor f=&gt;(a-&gt;b)-&gt;f a-&gt;f b
Let’s look at how the types line up with a program, starting
withmap:
map::(a-&gt;b)-&gt;[a]-&gt;[b]
map(+1)
The(a -&gt; b) becomes more specific and resolves to Num a
=&gt; a -&gt; a :</p>
<p>CHAPTER 9. THIS THING AND SOME MORE STUFF 502
Prelude&gt;:t map (+1)
map(+1)::Numb=&gt;[b]-&gt;[b]
Now we see it will take one list of Numas an argument and
return a list of Numas a result.
The type of fmapwill behave similarly:
fmap::Functor f=&gt;(a-&gt;b)-&gt;f a-&gt;f b
-- notice the Functor typeclass constraint
fmap(+1)
-- again, (a -&gt; b) is now more specific
It’s a bit diﬀerent from mapbecause the Functor typeclass
includes more than lists:
Prelude&gt; :t fmap (+1)
fmap (+1) :: (Num b, Functor f) =&gt; f b -&gt; f b
Here’s how mapis defined in base:
map::(a-&gt;b)-&gt;[a]-&gt;[b]
map_[]=[]
-- [1] [2] [3]
mapf (x:xs)=f x:map f xs
-- [4] [5] [6] [7] [8]</p>
<p>CHAPTER 9. THIS THING AND SOME MORE STUFF 503
1._is used here to ignore the function argument because
we don’t need it.
2.We are pattern matching on the []empty list case because
List is a sum type with two cases and we must handle both
every time we pattern match or case on a list value.
3.We return the []empty list value because when there are
no values, it’s the only correct thing we can do. If you
attempt to do anything else, the typechecker will swat
you.
4.We bind the function argument to the name 𝑓as it merits
no name more specific than this. 𝑓and𝑔are common
names for nonspecific function values in Haskell. This is
the function we are mapping over the list value with map
5.We do not leave the entire list argument bound as a single
name. Since we’ve already pattern-matched the []empty
list case, we know there must be at least one value in
the list. Here we pattern match into the (:)second data
constructor of the list, which is a product. 𝑥is the single
value of the cons product. 𝑥𝑠is the rest of the list.
6.We apply our function 𝑓to the single value 𝑥. This part
of themapfunction is what applies the function argument
to the contents of the list.</p>
<p>CHAPTER 9. THIS THING AND SOME MORE STUFF 504
7.We(:)cons the value returned by the expression f xonto
the head of the result of map’ing the rest of the list. Data is
immutable in Haskell. When we map, we do not mutate
the existing list, but build a new list with the values that
result from applying the function.
8.We call mapitself applied to 𝑓and𝑥𝑠. This expression is the
rest of the list with the function 𝑓applied to each value.
How do we write out what map fdoes? Note, this order of
evaluation doesn’t represent the proper nonstrict evaluation
order, but does give an idea of what’s going on:
map(+1) [1,2,3]
-- desugared, (:) is infixr 5,
-- so it's right-associative
map(+1) (1:(2:(3:[])))
-- Not an empty list, so second
-- pattern-match in map fires.
-- Apply (+1) to value, then map
(+1)1:
map (+1)
(2:(3:[]))</p>
<p>CHAPTER 9. THIS THING AND SOME MORE STUFF 505
-- Apply (+1) to the next value, cons onto
-- the result of mapping over the rest
(+1)1:
((+1)2:
(map (+1)
(3:[])))
-- Last time we'll trigger the
-- second-case of map
(+1)1:
((+1)2:
((+1)3:
(map (+1)[])))
-- Now we trigger the base-case that
-- handles empty list and return the
-- empty list.
(+1)1:
((+1)2:
((+1)3:[]))
-- Now we reduce
2:((+1)2:((+1)3:[]))
2:3:(+1)3:[]
2:3:4:[]==[2,3,4]</p>
<p>CHAPTER 9. THIS THING AND SOME MORE STUFF 506
Using the syntactic sugar of list, here’s an approximation of
whatmapis doing for us:
mapf [1,2,3]==[f1, f2, f3]
map(+1) [1,2,3]
[(+1)1, (+1)2, (+1)3]
[2,3,4]
Or using the spine syntax we introduced earlier:
:
/ <br />
1 :
/ <br />
2 :
/ <br />
3 []
map (+1) [1, 2, 3]
:
/ <br />
(+1) 1 :
/ <br />
(+1) 2 :
/ </p>
<p>CHAPTER 9. THIS THING AND SOME MORE STUFF 507
(+1) 3 []
As we mentioned above, these representations do not ac-
count for nonstrict evaluation. Crucially, mapdoesn’t traverse
the whole list and apply the function immediately. The func-
tion is applied to the values you force out of the list one by one.
We can see this by selectively leaving some values undefined:
Prelude&gt; map (+1) [1, 2, 3]
[2,3,4]
-- the whole list was forced because
-- GHCi printed the list that resulted
Prelude&gt; (+1) undefined
*** Exception: Prelude.undefined
Prelude&gt; (1, undefined)
(1,*** Exception: Prelude.undefined
Prelude&gt; fst (1, undefined)
1
Prelude&gt; map (+1) [1, 2, undefined]
[2,3,*** Exception: Prelude.undefined</p>
<p>CHAPTER 9. THIS THING AND SOME MORE STUFF 508
Prelude&gt; take 2 $ map (+1) [1, 2, undefined]
[2,3]
In the final example, the undefined value was never forced
and there was no error because we used take 2 to request only
the first two elements. With map (+1) we only force as many
values as cons cells we forced. We’ll only force the values if
we evaluate the result value in the list that the map function
returns.
The significant part here is that strictness doesn’t proceed
only outside-in. We can have lazily evaluated code (e.g., map)
wrapped around a strict core (e.g., +). In fact, we can choose to
apply laziness and strictness in how we evaluate the spine or
the leaves independently. A common mantra for performance
sensitive code in Haskell is, “lazy in the spine, strict in the
leaves.” We’ll cover this properly later when we talk about
nonstrictness and data structures, although many Haskell users
rarely worry about this.
You can use mapandfmapwith other functions and list types
as well. In this example, we use the fstfunction to return a
list of the first element of each tuple in a list of tuples:
Prelude&gt; map fst [(2, 3), (4, 5), (6, 7), (8, 9)]
[2,4,6,8]
Prelude&gt; fmap fst [(2, 3), (4, 5), (6, 7), (8, 9)]
[2,4,6,8]</p>
<p>CHAPTER 9. THIS THING AND SOME MORE STUFF 509
In this example we map a partially applied takefunction:
Prelude&gt; map (take 3) [[1..5], [1..5], [1..5]]
[[1,2,3],[1,2,3],[1,2,3]]
Next, we’ll map an if-then-else over a list using an anony-
mous function. This list will find any value equal to 3, negate
it, and then return the list:
Prelude&gt; map (\x -&gt; if x == 3 then (-x) else (x)) [1..10]
[1,2,-3,4,5,6,7,8,9,10]
At this point, you can try your hand at mapping diﬀerent
functions using this as a model. We recommend getting com-
fortable with mapping before moving on to the Folds chapter.
Exercises: More Bottoms
As always, we encourage you to try figuring out the answers
before you enter them into your REPL.
1.Will the following expression return a value or be ⊥?
take1$map (+1) [undefined, 2,3]
2.Will the following expression return a value?
take1$map (+1) [1, undefined, 3]
3.Will the following expression return a value?</p>
<p>CHAPTER 9. THIS THING AND SOME MORE STUFF 510
take2$map (+1) [1, undefined, 3]
4.What does the following mystery function do? What is its
type? Describe it (to yourself or a loved one) in standard
English and then test it out in the REPL to make sure you
were correct.
itIsMystery xs=
map (\x-&gt;elem x&quot;aeiou&quot;) xs
5.What will be the result of the following functions:
a)map(^2) [1..10]
b)mapminimum [[ 1..10], [10..20], [20..30]]
-- n.b. <code>minimum</code> is not the same function
-- as the <code>min</code> that we used before
c)mapsum [[1..5], [1..5], [1..5]]
6.Back in chapter 7, you wrote a function called foldBool .
That function exists in a module known as Data.Bool and
is called bool. Write a function that does the same (or
similar, if you wish) as the map (if-then-else) function you
saw above but uses boolinstead of the if-then-else syntax.
Your first step should be bringing the boolfunction into
scope by typing import Data.Bool at your Prelude prompt.</p>
<p>CHAPTER 9. THIS THING AND SOME MORE STUFF 511
9.10 Filtering lists of values
When we talked about function composition in Chapter 7,
we used a function called filter that takes a list as input and
returns a new list consisting solely of the values in the input
list that meet a certain condition, as in this example which
finds the even numbers of a list and returns a new list of those
values:
Prelude&gt; filter even [1..10]
[2,4,6,8,10]
Let’s now take a closer look at filter .filter has the follow-
ing definition:
filter::(a-&gt;Bool)-&gt;[a]-&gt;[a]
filter_[]=[]
filterpred (x:xs)
|pred x =x:filter pred xs
|otherwise =filter pred xs
Filtering takes a function that returns a Boolvalue, maps
that function over a list, and returns a new list of all the values
that met the condition. It’s important to remind ourselves that
this function, as we can see in the definition, builds a new list
including values that meet the condition and excluding the
ones that do not — it does not mutate the existing list.</p>
<p>CHAPTER 9. THIS THING AND SOME MORE STUFF 512
We have seen how filter works with oddandevenalready.
We have also seen one example along the lines of this:
Prelude&gt; filter (== 'a') &quot;abracadabra&quot;
&quot;aaaaa&quot;
Asyoumightsuspectfromwhatwe’veseenofHOFs, though,
filter can handle many types of arguments. The following ex-
ample does the same thing as filter even but with anonymous
function syntax:
Prelude&gt; filter (\x -&gt; (rem x 2) == 0) [1..20]
[2,4,6,8,10,12,14,16,18,20]
We covered list comprehensions earlier as a way of filtering
lists as well. Compare the following:
Prelude&gt; filter (\x -&gt; elem x &quot;aeiou&quot;) &quot;abracadabra&quot;
&quot;aaaaa&quot;
Prelude&gt; [x | x &lt;- &quot;abracadabra&quot;, elem x &quot;aeiou&quot;]
&quot;aaaaa&quot;
As they say, there’s more than one way to skin a cat.
Again, we recommend at this point you try writing some
filter functions of your own to get comfortable with the pat-
tern.</p>
<p>CHAPTER 9. THIS THING AND SOME MORE STUFF 513
Exercises: Filtering
1.Given the above, how might we write a filter function that
would give us all the multiples of 3 out of a list from 1-30?
2.Recalling what we learned about function composition,
how could we compose the above function with the length
function to tell us <em>how many</em> multiples of 3 there are
between 1 and 30?
3.Next we’re going to work on removing all articles (’the’, ’a’,
and ’an’) from sentences. You want to get to something
that works like this:
Prelude&gt; myFilter &quot;the brown dog was a goof&quot;
[&quot;brown&quot;,&quot;dog&quot;,&quot;was&quot;,&quot;goof&quot;]
You may recall that earlier in this chapter we asked you
to write a function that separates a string into a list of
strings by separating them at spaces. That is a standard
library function called words. You may consider starting
this exercise by using words(or your version, of course).
9.11 Zipping lists
Zipping lists together is a means of combining values from
multiple lists into a single list. Related functions like zipWith</p>
<p>CHAPTER 9. THIS THING AND SOME MORE STUFF 514
allow you to use a combining function to produce a list of
results from two lists.
First let’s look at zip:
Prelude&gt; :t zip
zip :: [a] -&gt; [b] -&gt; [(a, b)]
Prelude&gt; zip [1, 2, 3] [4, 5, 6]
[(1,4),(2,5),(3,6)]
One thing to note is that zipstops as soon as one of the lists
runs out of values:
Prelude&gt; zip [1, 2] [4, 5, 6]
[(1,4),(2,5)]
Prelude&gt; zip [1, 2, 3] [4]
[(1,4)]
And will return an empty list if either of the lists is empty:
Prelude&gt; zip [] [1..1000000000000000000]
[]
zipproceeds until the shortest list ends.
Prelude&gt; zip ['a'] [1..1000000000000000000]
[('a',1)]
Prelude&gt; zip [1..100] ['a'..'c']
[(1,'a'),(2,'b'),(3,'c')]</p>
<p>CHAPTER 9. THIS THING AND SOME MORE STUFF 515
We can use unzipto recover the lists as they were before
they were zipped:
Prelude&gt; zip [1, 2, 3] [4, 5, 6]
[(1,4),(2,5),(3,6)]
Prelude&gt; unzip $ zip [1, 2, 3] [4, 5, 6]
([1,2,3],[4,5,6])
Prelude&gt; fst $ unzip $ zip [1, 2, 3] [4, 5, 6]
[1,2,3]
Prelude&gt; snd $ unzip $ zip [1, 2, 3] [4, 5, 6]
[4,5,6]
Be aware that information can be lost in this process because
zipmust stop on the shortest list:
Prelude&gt; snd $ unzip $ zip [1, 2] [4, 5, 6]
[4,5]
We can also use zipWith to apply a function to the values of
two lists in parallel:
zipWith ::(a-&gt;b-&gt;c)
-- [1]
-&gt;[a]-&gt;[b]-&gt;[c]
-- [2] [3] [4]</p>
<p>CHAPTER 9. THIS THING AND SOME MORE STUFF 516
1.A function with two arguments. Notice how the type
variables of the arguments and result align with the type
variables in the lists.
2.The first input list.
3.The second input list.
4.The output list created from applying the function to the
values in the input lists.
A brief demonstration of how zipWith works:
Prelude&gt; zipWith (+) [1, 2, 3] [10, 11, 12]
[11,13,15]
Prelude&gt; zipWith (*) [1, 2, 3] [10, 11, 12]
[10,22,36]
Prelude&gt; zipWith (==) ['a'..'f'] ['a'..'m']
[True,True,True,True,True,True]
Prelude&gt; let xs = [10, 5, 34, 9]
Prelude&gt; let xs' = [6, 8, 12, 7]
Prelude&gt; zipWith max xs xs'
[10,8,34,9]</p>
<p>CHAPTER 9. THIS THING AND SOME MORE STUFF 517
Zipping exercises
1.Write your own version of zipand ensure it behaves the
same as the original.
zip::[a]-&gt;[b]-&gt;[(a, b)]
zip=undefined
2.Do what you did for zip, but now for zipWith :
zipWith ::(a-&gt;b-&gt;c)
-&gt;[a]-&gt;[b]-&gt;[c]
zipWith =undefined
3.Rewrite your zipin terms of the zipWith you wrote.
9.12 Chapter Exercises
The first set of exercises here will mostly be review but will
also introduce you to some new things. The second set is
more conceptually challenging but does not use any syntax or
concepts we haven’t already studied. If you get stuck, it may
help to flip back to a relevant section and review.
Data.Char
These first few exercises are straightforward but will introduce
you to some new library functions and review some of what</p>
<p>CHAPTER 9. THIS THING AND SOME MORE STUFF 518
we’ve learned so far. Some of the functions we will use here
are not standard in Prelude and so have to be imported from
a module called Data.Char . You may do so in a source file
(recommended) or at the Prelude prompt with the same phrase:
import Data.Char (write that at the top of your source file). This
brings into scope a bunch of new standard functions we can
play with that operate on CharandString types.
1.Query the types of isUpper andtoUpper .
2.Given the following behaviors, which would we use to
write a function that filters all the uppercase letters out
of aString ? Write that function such that, given the input
“HbEfLrLxO,” your function will return “HELLO.”
Prelude Data.Char&gt; isUpper 'J'
True
Prelude Data.Char&gt; toUpper 'j'
'J'
3.Write a function that will capitalize the first letter of a
string and return the entire string. For example, if given
the argument “julie,” it will return “Julie.”
4.Now make a new version of that function that is recursive
such that if you give it the input “woot” it will holler back
at you “WOOT.” The type signature won’t change, but
you will want to add a base case.</p>
<p>CHAPTER 9. THIS THING AND SOME MORE STUFF 519
5.To do the final exercise in this section, we’ll need another
standard function for lists called head. Query the type of
headand experiment with it to see what it does. Now write
a function that will capitalize the first letter of a String
and return only that letter as the result.
6.Cool. Good work. Now rewrite it as a composed function.
Then, for fun, rewrite it pointfree.
Ciphers
We’ll still be using Data.Char for this next exercise. You should
save these exercises in a module called Cipher because we’ll
be coming back to them in later chapters. You’ll be writing a
Caesar cipher for now, but we’ll suggest some variations on
the basic program in later chapters.
A Caesar cipher is a simple substitution cipher, in which
each letter is replaced by the letter that is a fixed number of
places down the alphabet from it. You will find variations on
this all over the place — you can shift leftward or rightward,
for any number of spaces. A rightward shift of 3 means that
’A’ will become ’D’ and ’B’ will become ’E,’ for example. If you
did a leftward shift of 5, then ’a’ would become ’v’ and so forth.
Your goal in this exercise is to write a basic Caesar cipher
that shifts rightward. You can start by having the number of
spaces to shift fixed, but it’s more challenging to write a cipher</p>
<p>CHAPTER 9. THIS THING AND SOME MORE STUFF 520
that allows you to vary the number of shifts so that you can
encode your secret messages diﬀerently each time.
There are Caesar ciphers written in Haskell all over the
internet, but to maximize the likelihood that you can write
yours without peeking at those, we’ll provide a couple of tips.
When yours is working the way you want it to, we would
encourage you to then look around and compare your solution
to others out there.
The first lines of your text file should look like this:
moduleCipherwhere
importData.Char
Data.Char includes two functions called ordandchrthat can
be used to associate a Charwith its Intrepresentation in the
Unicode system and vice versa:
*Cipher&gt;:t chr
chr::Int-&gt;Char
*Cipher&gt;:t ord
ord::Char-&gt;Int
Using these functions is optional; there are other ways you
can proceed with shifting, but using chrandordmight simplify
the process a bit.</p>
<p>CHAPTER 9. THIS THING AND SOME MORE STUFF 521
You want your shift to wrap back around to the beginning of
the alphabet, so that if you have a rightward shift of 3 from ’z,’
you end up back at ’c’ and not somewhere in the vast Unicode
hinterlands. Depending on how you’ve set things up, this
might be a bit tricky. Consider starting from a base character
(e.g., ’a’) and using modto ensure you’re only shifting over the
26 standard characters of the English alphabet.
You should include an unCaesar function that will decipher
your text as well. In a later chapter, we will test it.
Writing your own standard functions
Below are the outlines of some standard functions. The goal
here is to write your own versions of these to gain a deeper
understanding of recursion over lists and how to make func-
tions flexible enough to accept a variety of inputs. You could
figure out how to look up the answers, but you won’t do that
because you know you’d only be cheating yourself out of the
knowledge. Right?
Let’s look at an example of what we’re after here. The and2
function can take a list of Boolvalues and returns True if and
only if no values in the list are False. Here’s how you might
write your own version of it:
2Note that if you’re using GHC 7.10 or newer, the functions and,any, andallhave
been abstracted from being usable only with lists to being usable with any datatype that
has an instance of the typeclass Foldable . It still works with lists, the same as it did before.
Proceed assured that we’ll cover this later.</p>
<p>CHAPTER 9. THIS THING AND SOME MORE STUFF 522
-- direct recursion, not using (&amp;&amp;)
myAnd::[Bool]-&gt;Bool
myAnd[]=True
myAnd(x:xs)=
ifx==False
thenFalse
elsemyAnd xs
-- direct recursion, using (&amp;&amp;)
myAnd::[Bool]-&gt;Bool
myAnd[]=True
myAnd(x:xs)=x&amp;&amp;myAnd xs
And now the fun begins:
1.myOrreturns Trueif anyBoolin the list is True.
myOr::[Bool]-&gt;Bool
myOr=undefined
2.myAnyreturns Trueifa -&gt; Bool applied to any of the values
in the list returns True.
myAny::(a-&gt;Bool)-&gt;[a]-&gt;Bool
myAny=undefined
Example for validating myAny:</p>
<p>CHAPTER 9. THIS THING AND SOME MORE STUFF 523
Prelude&gt; myAny even [1, 3, 5]
False
Prelude&gt; myAny odd [1, 3, 5]
True
3.After you write the recursive myElem , write another version
that uses any. The built-in version of elemin GHC 7.10 and
newer has a type that uses Foldable instead of the list type
specifically. You can ignore that and write the concrete
version that works only for list.
myElem::Eqa=&gt;a-&gt;[a]-&gt;Bool
Prelude&gt; myElem 1 [1..10]
True
Prelude&gt; myElem 1 [2..10]
False
4.Implement myReverse .
myReverse ::[a]-&gt;[a]
myReverse =undefined
Prelude&gt; myReverse &quot;blah&quot;
&quot;halb&quot;
Prelude&gt; myReverse [1..5]
[5,4,3,2,1]</p>
<p>CHAPTER 9. THIS THING AND SOME MORE STUFF 524
5.squish flattens a list of lists into a list
squish::[[a]]-&gt;[a]
squish=undefined
6.squishMap maps a function over a list and concatenates the
results.
squishMap ::(a-&gt;[b])-&gt;[a]-&gt;[b]
squishMap =undefined
Prelude&gt; squishMap (\x -&gt; [1, x, 3]) [2]
[1,2,3]
Prelude&gt; squishMap (\x -&gt; &quot;WO &quot;++[x]++&quot; HOO &quot;) &quot;123&quot;
&quot;WO 1 HOO WO 2 HOO WO 3 HOO &quot;
7.squishAgain flattens a list of lists into a list. This time re-use
thesquishMap function.
squishAgain ::[[a]]-&gt;[a]
squishAgain =undefined
8.myMaximumBy takes a comparison function and a list and
returns the greatest element of the list based on the last
value that the comparison returned GTfor. If you import
maximumBy fromData.List , you’ll see the type is:
Foldable t
=&gt;(a-&gt;a-&gt;Ordering )-&gt;t a-&gt;a</p>
<p>CHAPTER 9. THIS THING AND SOME MORE STUFF 525
rather than
(a-&gt;a-&gt;Ordering )-&gt;[a]-&gt;a
myMaximumBy ::(a-&gt;a-&gt;Ordering )
-&gt;[a]-&gt;a
myMaximumBy =undefined
Prelude&gt; let xs = [1, 53, 9001, 10]
Prelude&gt; myMaximumBy compare xs
9001
9.myMinimumBy takes a comparison function and a list and
returns the least element of the list based on the last value
that the comparison returned LT for.
myMinimumBy ::(a-&gt;a-&gt;Ordering )
-&gt;[a]-&gt;a
myMinimumBy =undefined
Prelude&gt; let xs = [1, 53, 9001, 10]
Prelude&gt; myMinimumBy compare xs
1
10.Usingthe myMinimumBy andmyMaximumBy functions, writeyour
own versions of maximum andminimum . If you have GHC 7.10</p>
<p>CHAPTER 9. THIS THING AND SOME MORE STUFF 526
ornewer, you’llseeatypeconstructorthatwantsa Foldable
instance instead of a list as has been the case for many
functions so far.
myMaximum ::(Orda)=&gt;[a]-&gt;a
myMaximum =undefined
myMinimum ::(Orda)=&gt;[a]-&gt;a
myMinimum =undefined
9.13 Definitions
1.In type theory, a producttype is a type made of a set of types
compounded over each other. In Haskell we represent
products using tuples or data constructors with more than
one argument. The “compounding” is from each type
argument to the data constructor representing a value that
coexists with all the other values simultaneously. Products
of types represent a conjunction, “and,” of those types. If
you have a product of BoolandInt, your terms will each
contain a BoolandIntvalue.
2.In type theory, a sum type of two types is a type whose
terms are terms in either type, but not simultaneously. In
Haskell sum types are represented using the pipe, |, in a
datatype definition. Sums of types represent a disjunction,</p>
<p>CHAPTER 9. THIS THING AND SOME MORE STUFF 527
“or,” of those types. If you have a sum of BoolandInt, your
terms will be eitheraBoolvalueoranIntvalue.
3.Consis ordinarily used as a verb to signify that a list value
has been created by cons’ing a value onto the head of
another list value. In Haskell, (:)is the cons operator for
the list type. It is a data constructor defined in the list
datatype:
1:[2,3]
-- [a] [b]
[1,2,3]
-- [c]
(:)::a-&gt;[a]-&gt;[a]
-- [d] [e] [f]
a)The number 1, the value we are consing.
b)A list of the number 2 followed by the number 3.
c)The final result of consing 1onto[2, 3] .
d)The type variable 𝑎corresponds to 1, the value we
consed onto the list value.
e)The first occurrence of the type [a]in the cons oper-
ator’s type corresponds to the second and final argu-
ment(:)accepts, which was [2, 3] .</p>
<p>CHAPTER 9. THIS THING AND SOME MORE STUFF 528
f)The second and final occurrence of the type [a]in the
cons operator’s type corresponds to the final result
[1, 2, 3] .
4.Cons cell is a data constructor and a product of the types
aand[a]as defined in the list datatype. Because it refer-
ences the list type constructor itself in the second argu-
ment, it allows for nesting of multiple cons cells, possibly
indefinitely with the use of recursive functions, for repre-
senting an indefinite number of values in series:
data[]a=[]|a:[a]
-- ^ cons operator
-- Defining it ourselves
dataLista=Nil|Consa (Lista)
-- Creating a list using our list type
Cons1(Cons2(Cons3Nil))
Here(Cons 1 ...) ,(Cons 2 ...) and(Cons 3 Nil) are all
individual cons cells in the list [1, 2, 3] .
5.Thespineis a way to refer to the structure that glues a
collection of values together. In the list datatype it is</p>
<p>CHAPTER 9. THIS THING AND SOME MORE STUFF 529
formed by the recursive nesting of cons cells. The spine is,
in essence, the structure of collection that isn’tthe values
contained therein. Often spine will be used in reference
to lists, but it applies with tree data structures as well:
-- Given the list [1, 2, 3]
1:--------| The nested cons operators
(2:-----| here represent the spine.
(3:--|
[]))
-- Blanking the irrelevant values out
<em>:----------|
(</em>:-------|
(_:----&gt; Spine
[]))
9.14 Follow-up resources
1.Data.List documentation for the baselibrary.
http://hackage.haskell.org/package/base/docs/Data-List.html
2.Ninety-nine Haskell problems.
https://wiki.haskell.org/H-99:_Ninety-Nine_Haskell_Problems</p>
<p>Chapter 10
Folding lists
The explicit teaching of
thinking is no trivial task,
but who said that the
teaching of programming
is? In our terminology,
the more explicitly
thinking is taught, the
more of a scientist the
programmer will
become.
Edsger Dijkstra
530</p>
<p>CHAPTER 10. DATA STRUCTURE ORIGAMI 531
10.1 Folds
Folding is a concept that extends in usefulness and importance
beyond lists, but lists are often how they are introduced. Folds
as a general concept are called catamorphisms. You’re famil-
iar with the root, “morphism” from polymorphism. “Cata-”
means “down” or “against”, as in “catacombs.” Catamorphisms
are a means of deconstructing data. If the spine of a list is the
structure of a list, then a fold is what can reduce that structure.1
This chapter is a thorough look at the topic of folding lists
in Haskell. We will:
•explain what folds are and how they work;
•detail the evaluation processes of folds;
•walk through writing folding functions;
•introduce scans, functions that are related to folds.
10.2 Bringing you into the fold
Let’s start with a quick look at foldr, short for “fold right.” This
is the fold you’ll most often want to use with lists. The follow-
ing type signature may look a little hairy, but let’s compare it
1Note that a catamorphism canbreak down the structure but that structure might be
rebuilt, so to speak, during evaluation. That is, folds can return lists as results.</p>
<p>CHAPTER 10. DATA STRUCTURE ORIGAMI 532
to what we know about mapping. Note that the type of foldr
changed with GHC 7.10:
-- GHC 7.8 and older
foldr::(a-&gt;b-&gt;b)-&gt;b-&gt;[a]-&gt;b
-- GHC 7.10 and newer
foldr::Foldable t
=&gt;(a-&gt;b-&gt;b)
-&gt;b
-&gt;t a
-&gt;b
Lined up next to each other:
foldr::Foldable t=&gt;
(a-&gt;b-&gt;b)-&gt;b-&gt;t a-&gt;b
foldr::(a-&gt;b-&gt;b)-&gt;b-&gt;[]a-&gt;b
For now, all you need to know is that GHC 7.10 abstracted
out the list-specific part of folding into a typeclass that lets you
reuse the same folding functions for any datatype that can be
folded — not just lists. We can even recover the more concrete
type because we can always make a type more concrete, but
never more generic:
Prelude&gt; :{</p>
<p>CHAPTER 10. DATA STRUCTURE ORIGAMI 533
Prelude| let listFoldr :: (a -&gt; b -&gt; b)
Prelude| -&gt; b
Prelude| -&gt; [] a
Prelude| -&gt; b
Prelude| listFoldr = foldr
Prelude| :}
Prelude&gt; :t listFoldr
listFoldr :: (a -&gt; b -&gt; b) -&gt; b -&gt; [a] -&gt; b
Now let’s notice a parallel between mapandfoldr:
foldr::(a-&gt;b-&gt;b)-&gt;b-&gt;[a]-&gt;b
-- Remember how map worked?
map::(a-&gt;b)-&gt;[a]-&gt;[b]
map(+1)1: 2: 3:[]
(+1)1:(+1)2:(+1)3:[]
-- Given the list
foldr(+)0(1:2:3:[])
1+(2+(3+0))
Where mapapplies a function to each member of a list and
returns a list, a fold replaces the cons constructors with the
function and reduces the list.</p>
<p>CHAPTER 10. DATA STRUCTURE ORIGAMI 534
10.3 Recursive patterns
Let’s revisit sum:
Prelude&gt; sum [1, 5, 10]
16
As we’ve seen, it takes a list, adds the elements together,
and returns a single result. You might think of it as similar
to themapfunctions we’ve looked at, except that it’s mapping
(+)over the list, replacing the cons operators themselves, and
returning a single result, instead of mapping, for example, (+1)
into each cons cell and returning a whole list of results back
to us. This has the eﬀect of both mapping an operator over a
list and also reducing the list. In a previous section, we wrote
sumin terms of recursion:
sum::[Integer]-&gt;Integer
sum[]=0
sum(x:xs)=x+sum xs
And if we bring back our length function from earlier:
length::[a]-&gt;Integer
length[]=0
length(_:xs)=1+length xs</p>
<p>CHAPTER 10. DATA STRUCTURE ORIGAMI 535
Do you see some structural similiarity? What if you look at
product andconcat as well?
product ::[Integer]-&gt;Integer
product []=1
product (x:xs)=x*product xs
concat::[[a]]-&gt;[a]
concat[]=[]
concat(x:xs)=x++concat xs
In each case, the base case is the identity for that function.
So the identity for sum,length ,product , andconcat respectively
are 0, 0, 1, and []. When we do addition, adding zero gives us
the same result as our initial value: 1 + 0 = 1 . But when we do
multiplication, it’s multiplying by 1 that gives us the identity:
2 * 1 = 2 . With list concatenation, the identity is the empty
list, such that [1, 2, 3] ++ [] == [1, 2, 3] .
Also, each of them has a main function with a recursive
pattern that associates to the right. The head of the list gets
evaluated, set aside, and then the function moves to the right,
evaluates the next head, and so on.
10.4 Fold right
We call foldrthe “right fold” because the fold is right asso-
ciative; that is, it associates to the right. This is syntactically</p>
<p>CHAPTER 10. DATA STRUCTURE ORIGAMI 536
reflected in a straightforward definition of foldras well:
foldr::(a-&gt;b-&gt;b)-&gt;b-&gt;[a]-&gt;b
foldrf z[]=z
foldrf z (x:xs)=f x (foldr f z xs)
The similarities between this and the recursive patterns we
saw above should be clear. The “rest of the fold,” (foldr f z xs)
is an argument to the function 𝑓we’re folding with. The 𝑧is
the zero of our fold. It provides a fallback value for the empty
list case and a second argument to begin our fold with. The
zero is often the identity for whatever function we’re folding
with, such as 0 for (+)and 1 for (*).
How foldr evaluates
We’re going to rejigger our definition of foldra little bit. It
won’t change the semantics, but it’ll make it easier to write out
what’s happening:
foldr::(a-&gt;b-&gt;b)-&gt;b-&gt;[a]-&gt;b
foldrf z xs=
casexsof
[]-&gt;z
(x:xs)-&gt;f x (foldr f z xs)
Here we see how the right fold associates to the right. This
will reduce like the sumexample from earlier:</p>
<p>CHAPTER 10. DATA STRUCTURE ORIGAMI 537
foldr(+)0[1,2,3]
When we reduce that fold, the first step is substituting 𝑥𝑠in
our case expression:
foldr(+)0[1,2,3]=
case[1,2,3]of
...
Which case of the expression matches?
foldr(+)0[1,2,3]=
case[1,2,3]of
[]-&gt;0
(x:xs)-&gt;
f x (foldr f z xs) --&lt;---this one
What are f, x, xs, and z in that branch of the case?
foldr(+)0[1,2,3]=
case[1,2,3]of
[] -&gt;0
(1:[2,3])-&gt;
(+)1(foldr ( +)0[2,3])
Critically, we’re going to expand (foldr (+) 0 [2, 3]) only
because (+)is strict in both of its arguments, so it forces the</p>
<p>CHAPTER 10. DATA STRUCTURE ORIGAMI 538
next iteration. We could have a function which doesn’t contin-
ually force the rest of the fold. If it were to stop on the first case
here, then it would’ve returned the value 1. One such function
isconstwhich always returns the first argument. We’ll show
you how that behaves in a bit. Our next recursion is the (foldr
(+) 0 [2, 3]) :
foldr(+)0[2,3]=
case[2,3]of
[] -&gt;
0-- this didn't match again
(2:[3])-&gt;(+)2(foldr ( +)0[3])
There is (+) 1implicitly wrapped around this continuation
of the recursive fold. (+)is not only strict in both of its argu-
ments, but it’s unconditionally so, so we’re going to proceed to
the next recursion of foldr. Note that the function calls bounce
between our folding function 𝑓andfoldr. This bouncing back
and forth gives more control to the folding function. A hypo-
thetical folding function, such as const, which doesn’t need the
second argument has the opportunity to do less work by not
evaluating its second argument which is “more of the fold.”
There is (+) 1 ((+) 2 ...) implicitly wrapped around this
next step of the recursive fold:</p>
<p>CHAPTER 10. DATA STRUCTURE ORIGAMI 539
foldr(+)0[3]=
case[3]of
[] -&gt;
0-- this didn't match again
(3:[])-&gt;(+)3(foldr ( +)0[])
We’re going to ask for more foldrone last time. There is,
again,(+) 1 ((+) 2 ((+) 3 ...)) implicitly wrapped around
this final step of the recursive fold. Now we hit our base case
and and hit our base case:
foldr(+)0[]=
case[]of
[] -&gt;
0--&lt;--Thisone finally matches
-- ignore the other case, didn't happen
So one way to think about the way Haskell evaluates is that
it’s like a text rewriting system. Our expression has thus far
rewritten itself from:
foldr(+)0[1,2,3]
Into:
(+)1((+)2((+)30))</p>
<p>CHAPTER 10. DATA STRUCTURE ORIGAMI 540
If you wanted to clean it up a bit without changing how it
evaluates, you could make it the following:
1+(2+(3+0))
As in arithmetic, we evaluate innermost parentheses first:
1+(2+(3+0))
1+(2+3)
1+5
6
And now we’re done, with the result of 6.
We can also use a trick popularized by some helpful users
in the Haskell IRC community to see how the fold associates.2
xs=map show [ 1..5]
y=foldr (\x y-&gt;concat
[&quot;(&quot;,x,&quot;+&quot;,y,&quot;)&quot;])&quot;0&quot;xs
When we call 𝑦in the REPL, we can see how the foldreval-
uates:
2Idea borrowed from Cale Gibbard from the haskell Freenode IRC channel and on
the Haskell.org wiki https://wiki.haskell.org/Fold#Examples</p>
<p>CHAPTER 10. DATA STRUCTURE ORIGAMI 541
Prelude&gt; y
&quot;(1+(2+(3+(4+(5+0)))))&quot;
One initially nonobvious aspect of folding is that it happens
in two stages, traversal and folding. Traversal is the stage
in which the fold recurses over the spine. Folding refers to
the evaluation or reduction of the folding function applied
to the values. All folds recurse over the spine in the same
direction; the diﬀerence between left folds and right folds is
in the association, or parenthesization, of the folding function
and, thus, which direction the folding or reduction proceeds.
Withfoldr, the rest of our fold is an argument to the func-
tion we’re folding with:
foldrf z (x:xs)=f x (foldr f z xs)
-- ^--------------^
-- rest of the fold
Given this two-stage process and nonstrict evaluation, if
𝑓doesn’t evaluate its second argument (rest of the fold), no
more of the spine will be forced. One of the consequences of
this is that foldrcan avoid evaluating not only some or all of
the values in the list, but some or all of the list’s spineas well!
For this reason, foldrcan be used with lists that are potentially
infinite. For example, compare the following sets of results</p>
<p>CHAPTER 10. DATA STRUCTURE ORIGAMI 542
(recall that (+)will unconditionally evaluate the entire spine
and all of the values):
Prelude&gt; foldr (+) 0 [1..5]
15
While you cannot use foldrwith addition on an infinite list,
you can use functions that are not strict in both arguments and
therefore do not require evaluation of every value in order to
return a result. The function myAny, for example, can return a
Trueresult as soon as it finds one True:
myAny::(a-&gt;Bool)-&gt;[a]-&gt;Bool
myAnyf xs=
foldr (\x b-&gt;f x||b)Falsexs
The following should work despite being an infinite list:
Prelude&gt; myAny even [1..]
True
The following will never finish evaluating because it’s always
an odd number:
Prelude&gt; myAny even (repeat 1)
Another term we use for this never-ending evaluation is
bottom orundefined . There’s no guarantee that a fold of an</p>
<p>CHAPTER 10. DATA STRUCTURE ORIGAMI 543
infinite list will finish evaluating even if you used foldr, it of-
ten depends on the input data and the fold function. Let us
consider some more examples with a less inconvenient bottom :
Prelude&gt; let u = undefined
-- here, we give an undefined value
Prelude&gt; foldr (+) 0 [1, 2, 3, 4, u]
*** Exception: Prelude.undefined
Prelude&gt; let xs = take 4 [1, 2, 3, 4, u]
Prelude&gt; foldr (+) 0 xs
10
-- here, undefined is part of the spine
Prelude&gt; let xs = [1, 2, 3, 4] ++ u
Prelude&gt; foldr (+) 0 xs
*** Exception: Prelude.undefined
Prelude&gt; let xs = take 4 ([1, 2, 3, 4]++u)
Prelude&gt; foldr (+) 0 xs
10
By taking only the first four elements, we stop the recursive
folding process at the first four values so our addition function
does not run into bottom, and that works whether undefined is
one of the values or part of the spine.</p>
<p>CHAPTER 10. DATA STRUCTURE ORIGAMI 544
Thelength function behaves diﬀerently; it evaluates the
spine unconditionally, but not the values:
Prelude&gt; length [1, 2, 3, 4, undefined]
5
Prelude&gt; length ([1, 2, 3, 4] ++ undefined)
*** Exception: Prelude.undefined
However, if we drop the part of the spine that includes the
bottom before we use length, we can get an expression that
works:
Prelude&gt; let xs = [1, 2, 3, 4] ++ undefined
Prelude&gt; length (take 4 xs)
4
takeis nonstrict like everything else you’ve seen so far, and
in this case, it only returns as much list as you ask for. The dif-
ference in what it does, is it stopsreturning elements of the list
it was given when it hits the length limit you gave it. Consider
this:
Prelude&gt; let xs = [1, 2] ++ undefined
Prelude&gt; length $ take 2 $ take 4 xs
2
It doesn’t matter that take 4 could’ve hit the bottom! Noth-
ing forced it to because of the take 2 between it and length .</p>
<p>CHAPTER 10. DATA STRUCTURE ORIGAMI 545
Now that we’ve seen how the recursive second argument to
foldr’s folding function works, let’s consider the first argument:
foldr::(a-&gt;b-&gt;b)-&gt;b-&gt;[a]-&gt;b
foldrf z[]=z
foldrf z (x:xs)=f x (foldr f z xs)
-- [1]
The first argument, [1], involves a pattern match that is
strict by default — the 𝑓only applies to 𝑥if there is an 𝑥value
and not just an empty list. This means that foldrmust force
an initial cons cell in order to discriminate between the []and
the(x : xs) cases, so the first cons cell cannot be undefined.
Now we’re going to try something unusual to demonstrate
that the first bit of the spine must be evaluated by foldr. We
have a somewhat silly anonymous function that will ignore
all its arguments and return a value of 9001. We’re using it
withfoldrbecause it will never force evaluation of any of its
arguments, so we can have a bottom as a value or as part of
the spine, and it will not force an evaluation:
Prelude&gt; foldr (_ _ -&gt; 9001) 0 [1..5]
9001
Prelude&gt; let xs = [1, 2, 3, undefined]
Prelude&gt; foldr (_ _ -&gt; 9001) 0 xs
9001</p>
<p>CHAPTER 10. DATA STRUCTURE ORIGAMI 546
Prelude&gt; let xs = [1, 2, 3] ++ undefined
Prelude&gt; foldr (_ _ -&gt; 9001) 0 xs
9001
Everything is fine unless the first piece of the spine is bot-
tom:
Prelude&gt; foldr (_ _ -&gt; 9001) 0 undefined
*** Exception: Prelude.undefined
Prelude&gt; let xs = [1, undefined]
Prelude&gt; foldr (_ _ -&gt; 9001) 0 xs
9001
Prelude&gt; let xs = [undefined, undefined]
Prelude&gt; foldr (_ _ -&gt; 9001) 0 xs
9001
The final two examples work because it isn’t the first cons
cellthat is bottom — the undefined values are inside the cons
cells, not in the spine itself. Put diﬀerently, the cons cells
contain bottom values but are not themselves bottom. We will
experiment later with nonstrictness and strictness to see how
it aﬀects the way our programs evaluate.
Traversing the rest of the spine doesn’t occur unless the
function asks for the results of having folded the rest of the
list. In the following examples, we don’t force traversal of the</p>
<p>CHAPTER 10. DATA STRUCTURE ORIGAMI 547
spine because constthrows away its second argument, which
is the rest of the fold:
-- reminder:
-- const :: a -&gt; b -&gt; a
-- const x _ = x
Prelude&gt; const 1 2
1
Prelude&gt; const 2 1
2
Prelude&gt; foldr const 0 [1..5]
1
Prelude&gt; foldr const 0 [1,undefined]
1
Prelude&gt; foldr const 0 ([1,2] ++ undefined)
1
Prelude&gt; foldr const 0 [undefined,2]
*** Exception: Prelude.undefined
Now that we’ve seen how foldrevaluates, we’re going to
look atfoldlbefore we move on to learning how to write and
use folds.</p>
<p>CHAPTER 10. DATA STRUCTURE ORIGAMI 548
10.5 Fold left
Because of the way lists work, folds must first recurse over
the spine of the list from the beginning to the end. Left folds
traverse the spine in the same direction as right folds, but their
folding process is left associative and proceeds in the opposite
direction as that of foldr.
Here’s a simple definition of foldl. Note that to see the same
type for foldlin your GHCi REPL you will need to import
Data.List for the same reasons as with foldr:
-- again, different type in
-- GHC 7.10 and newer.
foldl::(b-&gt;a-&gt;b)-&gt;b-&gt;[a]-&gt;b
foldlf acc[]=acc
foldlf acc (x :xs)=foldl f (f acc x) xs
foldl::(b-&gt;a-&gt;b)-&gt;b-&gt;[a]-&gt;b
-- Given the list
foldl(+)0(1:2:3:[])
-- foldl associates like this
((0+1)+2)+3</p>
<p>CHAPTER 10. DATA STRUCTURE ORIGAMI 549
We can also use the same trick we used to see the associa-
tivity of foldrto see the associativity of foldl:
Prelude&gt; let conc = concat
Prelude&gt; let f x y = conc [&quot;(&quot;,x,&quot;+&quot;,y,&quot;)&quot;]
Prelude&gt; foldl f &quot;0&quot; (map show [1..5])
&quot;(((((0+1)+2)+3)+4)+5)&quot;
We can see from this that foldlbegins its reduction process
by adding the acc(accumulator) value to the head of the list,
whereas foldrhad added it to the final element of the list first.
We can also use functions called scansto see how folds eval-
uate. Scans are similar to folds but return a list of all the inter-
mediate stages of the fold. We can compare scanrandscanlto
their accompanying folds to see the diﬀerence in evaluation:
Prelude&gt; foldr (+) 0 [1..5]
15
Prelude&gt; scanr (+) 0 [1..5]
[15,14,12,9,5,0]
Prelude&gt; foldl (+) 0 [1..5]
15
Prelude&gt; scanl (+) 0 [1..5]
[0,1,3,6,10,15]</p>
<p>CHAPTER 10. DATA STRUCTURE ORIGAMI 550
The relationship between the scans and folds are as follows:
last(scanl f z xs) =foldl f z xs
head(scanr f z xs) =foldr f z xs
Each fold will return the same result for this operation, but
we can see from the scans that they arrive at that result in a
diﬀerent order, due to the diﬀerent associativity. We’ll talk
more about scans later.
Associativity and folding
Next we’ll take a closer look at some of the eﬀects of the asso-
ciativity of foldl. As we’ve said, both folds traverse the spine
in the same direction. What’s diﬀerent is the associativity of
the evaluation.
The fundamental way to think about evaluation in Haskell
is as substitution. When we use a right fold on a list with the
function 𝑓and start value 𝑧, we’re, in a sense, replacing the
cons constructors with our folding function and the empty list
constructor with our start value 𝑧:</p>
<p>CHAPTER 10. DATA STRUCTURE ORIGAMI 551
[1..3]==1:2:3:[]
foldrf z [1,2,3]
1<code>f</code> (foldr f z [ 2,3])
1<code>f</code> (2<code>f</code> (foldr f z [ 3]))
1<code>f</code> (2<code>f</code> (3<code>f</code> (foldr f z [])))
1<code>f</code> (2<code>f</code> (3<code>f</code> z))
Furthermore, lazy evaluation lets our functions, rather than
the ambient semantics of the language, dictate what order
things get evaluated in. Because of this, the parentheses are real .
In the above, the 3 <code>f</code> z pairing gets evaluated first because
it’s in the innermost parentheses. Right folds have to traverse
the list outside-in, but the folding itself starts from the end of
the list.
It’s hard to see this with arithmetic functions that are as-
sociative, such as addition, but it’s an important point to un-
derstand, so we’ll run through some diﬀerent examples. Let’s
start by using an arithmetic operation that isn’t associative:
Prelude&gt; foldr (^) 2 [1..3]
1
Prelude&gt; foldl (^) 2 [1..3]
64
This time we can see clearly that we got diﬀerent results, and</p>
<p>CHAPTER 10. DATA STRUCTURE ORIGAMI 552
that diﬀerence results from the way the functions associate.
Here’s a breakdown:
-- if you want to follow along,
-- use paper and not the REPL.
foldr(^)2[1..3]
(1^(2^(3^2)))
(1^(2^9))
1^512
1
Contrast that with this:
foldl(^)2[1..3]
((2^1)^2)^3
(2^2)^3
4^3
64
In this next set of comparisons, we will demonstrate the
eﬀect of associativity on argument order by folding the list
into a new list, like this:
Prelude&gt; foldr (:) [] [1..3]
[1,2,3]
Prelude&gt; foldl (flip (:)) [] [1..3]
[3,2,1]</p>
<p>CHAPTER 10. DATA STRUCTURE ORIGAMI 553
We must use flipwithfoldl. Let’s examine why.
Like a right fold, a left fold cannot perform magic and go to
the end of the list instantly; it must start from the beginning
of the list. However, the parentheses dictate how our code
evaluates. The type of the argument to the folding function
changes in addition to the associativity:
foldr::(a-&gt;b-&gt;b)-&gt;b-&gt;[a]-&gt;b
-- [1] [2] [3]
foldl::(b-&gt;a-&gt;b)-&gt;b-&gt;[a]-&gt;b
-- [4] [5] [6]
1.The parameter of type 𝑎represents one of the list element
arguments the folding function of foldris applied to.
2.The parameter of type 𝑏will either be the start value or
the result of the fold accumulated so far, depending on
how far you are into the fold.
3.The final result of having combined the list element and
the start value or fold so far to compute the fold.
4.The start value or fold accumulated so far is the first ar-
gument to foldl’s folding function.
5.The list element is the second argument to foldl’s folding
function.</p>
<p>CHAPTER 10. DATA STRUCTURE ORIGAMI 554
6.The final result of foldl’s fold function is of type 𝑏, like
that offoldr.
The type of (:)requires that a value be the first argument
and a list be the second argument:
(:) :: a -&gt; [a] -&gt; [a]
So the value is prepended, or “consed onto,” the front of
that list.
In the following examples, the tilde means “is equivalent or
equal to.” If we write a right fold that has the cons constructor
as our𝑓and the empty list as our 𝑧, we get:
-- foldr f z [1, 2, 3]
-- f ~ (:); z ~ []
-- Run it in your REPL. It'll return True.
foldr (:)<a href="HaskellProgramming/1:2:3:%5B%5D"></a>
==1:(2:(3:[]))
The consing process for foldrmatches the type signature
for(:). It also reproduces the same list because we’re replacing
the cons constructors with cons constructors and the null list
with null list. However, for it to be identical, it also has to be
right associative.
Doing the same with foldldoes not produce the same result.
When using foldl, the result we’ve accumulated so far is the</p>
<p>CHAPTER 10. DATA STRUCTURE ORIGAMI 555
first argument instead of the list element. This is opposite of
what(:)expects if we’re accumulating a list. Trying to fold
the identity of the list as above but with foldlwould give us a
type error because the reconstructing process for foldlwould
look like this:
foldlf z [1,2,3]
-- f ~ (:); z ~ []
-- (((z <code>f</code> 1) <code>f</code> 2) <code>f</code> 3)
((([]:1):2):3)
That won’t work because the 𝑧is an empty list and the 𝑓is
cons, so we have the order of arguments backwards for cons.
Enterflip, whose job is to take backwards arguments and turn
that frown upside down. It will flip each set of arguments
around for us, like this:
foldlf z [1,2,3]
-- f ~ (flip (:)); z ~ []
-- (((z <code>f</code> 1) <code>f</code> 2) <code>f</code> 3)
f=flip (:)
((([]<code>f</code>1) <code>f</code>2) <code>f</code>3)
(([1] <code>f</code>2) <code>f</code>3)
([2,1] <code>f</code>3)
[3,2,1]</p>
<p>CHAPTER 10. DATA STRUCTURE ORIGAMI 556
Evenwhenwe’vesatisfiedthetypesbyflippingthingsaround,
the left-associating nature of foldlleads to a diﬀerent result
from that of foldr.
For the next set of comparisons, we’re going to use a func-
tion called constthat takes two arguments and always returns
the first one. When we fold constover a list, it will take as its
first pair of arguments the accvalue and a value from the list
— which value it takes first depends on which type of fold it is.
We’ll show you how it evaluates for the first example:
Prelude&gt; foldr const 0 [1..5]
(const 1 _)
1
Sinceconstdoesn’t evaluate its second argument the rest
of the fold is never evaluated. The underscore represents the
rest of the unevaluated fold. Now, let’s look at the eﬀect of
flipping the arguments. The 0 result is because zero is our
accumulator value here, so it’s the first (or last) value of the
list:
Prelude&gt; foldr (flip const) 0 [1..5]
0
Next let’s look at what happens when we use the same func-
tions but this time with foldl. Take a few moments to under-
stand the evaluation process that leads to these results:</p>
<p>CHAPTER 10. DATA STRUCTURE ORIGAMI 557
Prelude&gt; foldl (flip const) 0 [1..5]
5
Prelude&gt; foldl const 0 [1..5]
0
This is the eﬀect of left associativity. The spine traversal
happens in the same order in a left or right fold — it must, be-
cause of the way lists are defined. Depending on your folding
function, a left fold can lead to a diﬀerent result than a right
fold of the same.
Exercises: Understanding Folds
1.foldr(<em>)1[1..5]
will return the same result as which of the following:
a)flip(</em>)1[1..5]
b)foldl(flip (<em>))1[1..5]
c)foldl(</em>)1[1..5]
2.Write out the evaluation steps for
foldl(flip (*))1[1..3]
3.One diﬀerence between foldrandfoldlis:
a)foldr, but not foldl, traverses the spine of a list from
right to left</p>
<p>CHAPTER 10. DATA STRUCTURE ORIGAMI 558
b)foldr, but not foldl, always forces the rest of the fold
c)foldr, but not foldl, associates to the right
d)foldr, but not foldl, is recursive
4.Folds are catamorphisms, which means they are generally
used to
a)reduce structure
b)expand structure
c)render you catatonic
d)generate infinite data structures
5.The following are simple folds very similar to what you’ve
already seen, but each has at least one error. Please fix
them and test in your REPL:
a)foldr(++) [&quot;woot&quot;,&quot;WOOT&quot;,&quot;woot&quot;]
b)foldrmax[]&quot;fear is the little death&quot;
c)foldrandTrue[False,True]
d)This one is more subtle than the previous. Can it ever
return a diﬀerent answer?
foldr(||)True[False,True]
e)foldl((++).show)&quot;&quot;[1..5]
f)foldrconst'a'[1..5]</p>
<p>CHAPTER 10. DATA STRUCTURE ORIGAMI 559
g)foldrconst0&quot;tacos&quot;
h)foldl(flip const) 0&quot;burritos&quot;
i)foldl(flip const) 'z'[1..5]
Unconditional spine recursion
An important diﬀerence between foldrandfoldlis that a left
fold has the successive steps of the fold as its first argument.
The next recursion of the spine isn’t intermediated by the
folding function as it is in foldr, which also means recursion of
the spine is unconditional. Having a function that doesn’t force
evaluation of either of its arguments won’t change anything.
Let’s review const:
Prelude&gt; const 1 undefined
1
Prelude&gt; (flip const) 1 undefined
*** Exception: Prelude.undefined
Prelude&gt; (flip const) undefined 1
1
Now compare:
Prelude&gt; let xs = [1..5] ++ undefined
Prelude&gt; foldr const 0 xs
1
Prelude&gt; foldr (flip const) 0 xs</p>
<p>CHAPTER 10. DATA STRUCTURE ORIGAMI 560
*** Exception: Prelude.undefined
Prelude&gt; foldl const 0 xs
*** Exception: Prelude.undefined
Prelude&gt; foldl (flip const) 0 xs
*** Exception: Prelude.undefined
However, while foldlunconditionally evaluates the spine
you can still selectively evaluate the values in the list. This will
throw an error because the bottom is part of the spine and
foldlmust evaluate the spine:
Prelude&gt; let xs = [1..5] ++ undefined
Prelude&gt; foldl (_ _ -&gt; 5) 0 xs
*** Exception: Prelude.undefined
But this is OK because bottom is a value here:
Prelude&gt; let xs = [1..5] ++ [undefined]
Prelude&gt; foldl (_ _ -&gt; 5) 0 xs
5
This feature means that foldlis generally inappropriate
with lists that are or could be infinite, but the combination of
the forced spine evaluation with nonstrictness means that it is
also usually inappropriate even for long lists, as the forced eval-
uation of the spine aﬀects performance negatively. Because</p>
<p>CHAPTER 10. DATA STRUCTURE ORIGAMI 561
foldlmust evaluate its whole spine before it starts evaluating
values in each cell, it accumulates a pile of unevaluated values
as it traverses the spine.
In most cases, when you need a left fold, you should use
foldl'. This function, called “fold-l-prime,” works the same
except it is strict. In other words, it forces evaluation of the
values inside cons cells as it traverses the spine, rather than
accumulating unevaluated expressions for each element of
the list. The strict evaluation here means it has less negative
eﬀect on performance over long lists.
10.6 How to write fold functions
When we write folds, we begin by thinking about what our
start value for the fold is. This is usually the identity value for
the function. When we sum the elements of a list, the identity
of summation is 0. When we multiply the elements of the list,
the identity is 1. This start value is also our fallback in case the
list is empty.
Next we consider our arguments. A folding function takes
two arguments, 𝑎and𝑏, where 𝑎is going to always be one of
the elements in the list and 𝑏is either the start value or the
value accumulated as the list is being processed.
Let’s say we want to write a function to take the first three
letters of each String value in a list of strings and concatenate</p>
<p>CHAPTER 10. DATA STRUCTURE ORIGAMI 562
that result into a final String . The type of the right fold for lists
is:
foldr::(a-&gt;b-&gt;b)-&gt;b-&gt;[a]-&gt;b
First, we’ll set up the beginnings of our expression:
foldr(\a b-&gt;undefined) []
[&quot;Pizza&quot;,&quot;Apple&quot;,&quot;Banana&quot; ]
We used an empty list as the start value, but since we plan to
return a String as our result, we could be a little more explicit
about our intent to build a String and make a small syntactic
change:
foldr(\a b-&gt;undefined) &quot;&quot;
[&quot;Pizza&quot;,&quot;Apple&quot;,&quot;Banana&quot; ]
Of course, because a String is a list, these are the same value:
Prelude&gt; &quot;&quot; == []
True
But&quot;&quot;signals intent with respect to the types involved:
Prelude&gt; :t &quot;&quot;
&quot;&quot; :: [Char]
Prelude&gt; :t []
[] :: [t]</p>
<p>CHAPTER 10. DATA STRUCTURE ORIGAMI 563
Moving along, we next want to work on the function. We
already know how to take the first three elements from a list
and we can reuse this for String :
foldr(\a b-&gt;take3a)&quot;&quot;
[&quot;Pizza&quot;,&quot;Apple&quot;,&quot;Banana&quot; ]
Now this will already typecheck and work, but it doesn’t
match the semantics we asked for:
Prelude&gt; :{
*Main| let pab =
*Main| [&quot;Pizza&quot;, &quot;Apple&quot;, &quot;Banana&quot;]
*Main| :}
Prelude&gt; foldr (\a b -&gt; take 3 a) &quot;&quot; pab
&quot;Piz&quot;
Prelude&gt; foldl (\b a -&gt; take 3 a) &quot;&quot; pab
&quot;Ban&quot;
We’re only getting the first three letters of the first or the
last string, depending on whether we did a right or left fold.
Note the argument naming order due to the diﬀerence in the
types of foldrandfoldl:
foldr::(a-&gt;b-&gt;b)-&gt;b-&gt;[a]-&gt;b
foldl::(b-&gt;a-&gt;b)-&gt;b-&gt;[a]-&gt;b</p>
<p>CHAPTER 10. DATA STRUCTURE ORIGAMI 564
The problem here is that right now we’re not folding the
list. We’re only mapping our take 3 over the list and selecting
the first or last result:
Prelude&gt; map (take 3) pab
[&quot;Piz&quot;,&quot;App&quot;,&quot;Ban&quot;]
Prelude&gt; head $ map (take 3) pab
&quot;Piz&quot;
Prelude&gt; last $ map (take 3) pab
&quot;Ban&quot;
So let us make this a proper fold and accumulate the result
by making use of the 𝑏argument. Remember the 𝑏is the
start value. Technically we could use concat on the result of
having mapped take 3 over the list (or its reverse, if we want
to simulate foldl):
Prelude&gt; concat $ map (take 3) pab
&quot;PizAppBan&quot;
Prelude&gt; let rpab = reverse pab
Prelude&gt; concat $ map (take 3) rpab
&quot;BanAppPiz&quot;
But we need an excuse to play with foldrandfoldl, so we’ll
pretend none of this happened!
Prelude&gt; let f = (\a b -&gt; take 3 a ++ b)</p>
<p>CHAPTER 10. DATA STRUCTURE ORIGAMI 565
Prelude&gt; foldr f &quot;&quot; pab
&quot;PizAppBan&quot;
Prelude&gt; let f' = (\b a -&gt; take 3 a ++ b)
Prelude&gt; foldl f' &quot;&quot; pab
&quot;BanAppPiz&quot;
Here we concatenated the result of having taken three el-
ements from the string value in our input list onto the front
of the string we’re accumulating. If we want to be explicit, we
can assert types for the values:
Prelude&gt; :{
*Prelude| let f a b = take 3
*Prelude| (a :: String) ++
*Prelude| (b :: String)
*Prelude| :}
Prelude&gt; foldr f &quot;&quot; pab
&quot;PizAppBan&quot;
Ifweassertsomethingthatisn’ttrue, thetypecheckercatches
us:
Prelude&gt; :{
*Prelude| let f a b = take 3 (a :: String)
*Prelude| ++ (b :: [String])
*Prelude| :}</p>
<p>CHAPTER 10. DATA STRUCTURE ORIGAMI 566
<interactive>:12:42:
Couldn't match type ‘Char’ with ‘[Char]’
Expected type: [String]
Actual type: [Char]
In the second argument of ‘(++)’,
namely ‘(b :: [String])’
In the expression:
take 3 (a :: String) ++ (b :: [String])
This can be useful for checking that your mental model of
the code is accurate.
Exercises: Database Processing
Write the following functions for processing this data.
importData.Time
dataDatabaseItem =DbString String
|DbNumber Integer
|DbDate UTCTime
deriving (Eq,Ord,Show)</p>
<p>CHAPTER 10. DATA STRUCTURE ORIGAMI 567
theDatabase ::[DatabaseItem ]
theDatabase =
[DbDate(UTCTime
(fromGregorian 191151)
(secondsToDiffTime 34123))
,DbNumber 9001
,DbString &quot;Hello, world!&quot;
,DbDate(UTCTime
(fromGregorian 192151)
(secondsToDiffTime 34123))
]
1.Write a function that filters for DbDate values and returns
a list of the UTCTime values inside them.
filterDbDate ::[DatabaseItem ]
-&gt;[UTCTime]
filterDbDate =undefined
2.Write a function that filters for DbNumber values and returns
a list of the Integer values inside them.
filterDbNumber ::[DatabaseItem ]
-&gt;[Integer]
filterDbNumber =undefined
3.Write a function that gets the most recent date.</p>
<p>CHAPTER 10. DATA STRUCTURE ORIGAMI 568
mostRecent ::[DatabaseItem ]
-&gt;UTCTime
mostRecent =undefined
4.Write a function that sums all of the DbNumber values.
sumDb::[DatabaseItem ]
-&gt;Integer
sumDb=undefined
5.Write a function that gets the average of the DbNumber val-
ues.
-- You'll probably need to use fromIntegral
-- to get from Integer to Double.
avgDb::[DatabaseItem ]
-&gt;Double
avgDb=undefined
10.7 Folding and evaluation
What diﬀerentiates foldrandfoldlis associativity. The right
associativity of foldrmeans the folding function evaluates
from the innermost cons cell to the outermost (the head). On
the other hand, foldlrecurses unconditionally to the end of the</p>
<p>CHAPTER 10. DATA STRUCTURE ORIGAMI 569
list through self-calls and then the folding function evaluates
from the outermost cons cell to the innermost:
Prelude&gt; let rcf = foldr (:) []
Prelude&gt; let xs = [1, 2, 3] ++ undefined
Prelude&gt; take 3 $ rcf xs
[1,2,3]
Prelude&gt; let lcf = foldl (flip (:)) []
Prelude&gt; take 3 $ lcf xs
*** Exception: Prelude.undefined
Let’s dive into our constexample a little more carefully:
foldr const 0 [1..5]
Withfoldr, you’ll evaluate const 1 (...) , butconstignores
the rest of the fold that would have occurred from the end of
the list up to the number 1, so this returns 1 without having
evaluated any more of the values or the spine. One way you
could examine this for yourself would be:
Prelude&gt; foldr const 0 ([1] ++ undefined)
1
Prelude&gt; head ([1] ++ undefined)
1
Prelude&gt; tail ([1] ++ undefined)
*** Exception: Prelude.undefined</p>
<p>CHAPTER 10. DATA STRUCTURE ORIGAMI 570
Similarly for foldl:
foldl (flip const) 0 [1..5]
Herefoldlwill recurse to the final cons cell, evaluate (flip
const) (...) 5 , ignore the rest of the fold that would occur
from the beginning up to the number 5, and return 5.
The relationship between foldrandfoldlis such that:
foldr f z xs =
foldl (flip f) z (reverse xs)
Butonlyfor finite lists! Consider:
Prelude&gt; let xs = repeat 0 ++ [1,2,3]
Prelude&gt; foldr const 0 xs
0
Prelude&gt; let xs' = repeat 1 ++ [1,2,3]
Prelude&gt; let rxs = reverse xs'
Prelude&gt; foldl (flip const) 0 rxs
^CInterrupted.
-- ^^ bottom.
If we flip our folding function 𝑓and reverse the list 𝑥𝑠,foldr
andfoldlwill return the same result:
Prelude&gt; let xs = [1..5]
Prelude&gt; foldr (:) [] xs</p>
<p>CHAPTER 10. DATA STRUCTURE ORIGAMI 571
[1,2,3,4,5]
Prelude&gt; foldl (flip (:)) [] xs
[5,4,3,2,1]
Prelude&gt; foldl (flip (:)) [] (reverse xs)
[1,2,3,4,5]
Prelude&gt; reverse $ foldl (flip (:)) [] xs
[1,2,3,4,5]
10.8 Summary
We presented a lot of material in this chapter. You might be
feeling a little weary of folds right now. So what’s the executive
summary?
foldr
1.The rest of the fold (recursive invocation of foldr) is an
argument to the folding function you passed to foldr. It
doesn’t directly self-call as a tail-call like foldl. You could
think of it as alternating between applications of foldrand
your folding function 𝑓. The next invocation of foldris
conditional on 𝑓having asked for more of the results of
having folded the list. That is:
foldr::(a-&gt;b-&gt;b)-&gt;b-&gt;[a]-&gt;b
-- ^</p>
<p>CHAPTER 10. DATA STRUCTURE ORIGAMI 572
That𝑏we’re pointing at in (a -&gt; b -&gt; b) istherestofthefold.
Evaluating that evaluates the next application of foldr.
2.Associates to the right.
3.Works with infinite lists. We know this because:
Prelude&gt; foldr const 0 [1..]
1
4.Is a good default choice whenever you want to transform
data structures, be they finite or infinite.
foldl
1.Self-calls (tail-call) through the list, only beginning to
produce values after reaching the end of the list.
2.Associates to the left.
3.Cannot be used with infinite lists. Try the infinite list
example earlier and your REPL will hang.
4.Is nearly useless and should almost always be replaced
withfoldl' for reasons we’ll explain later when we talk
about writing efficient Haskell programs.</p>
<p>CHAPTER 10. DATA STRUCTURE ORIGAMI 573
10.9 Scans
Scans, which we have mentioned above, work similarly to
maps and also to folds. Like folds, they accumulate values
instead of keeping the list’s individual values separate. Like
maps, they return a list of results. In this case, the list of results
shows the intermediate stages of evaluation, that is, the values
that accumulate as the function is doing its work.
Scans are not used as frequently as folds, and once you
understand the basic mechanics of folding, there isn’t a whole
lot new to understand. Still, it is useful to know about them
and get an idea of why you might need them.3
First, let’s take a look at the types. We’ll do a direct com-
parison of the types of folds and scans so the diﬀerence is
clear:
foldr :: (a -&gt; b -&gt; b) -&gt; b -&gt; [a] -&gt; b
scanr :: (a -&gt; b -&gt; b) -&gt; b -&gt; [a] -&gt; [b]
foldl :: (b -&gt; a -&gt; b) -&gt; b -&gt; [a] -&gt; b
scanl :: (b -&gt; a -&gt; b) -&gt; b -&gt; [a] -&gt; [b]
The primary diﬀerence is that the final result is a list (folds
canreturn a list as a result as well, but they don’t always). This
3The truth is scans are not used often, but there are times when you want to fold
a function over a list and return a list of the intermediate values that you can then use
as input to some other function. For a particularly elegant use of this, please see Chris
Done’s blog post about this solution to the waterfall problem at http://chrisdone.com/
posts/twitter-problem-loeb .</p>
<p>CHAPTER 10. DATA STRUCTURE ORIGAMI 574
means that they are not catamorphisms and, in an important
sense, aren’t folds at all. But no matter! The type signatures
are similar, and the routes of spine traversal and evaluation
are similar. This does mean that you can use scans in places
that you can’t use a fold, precisely because you return a list of
results rather than reducing the spine of the list.
The results that scans produce can be represented like this:
scanr (+) 0 [1..3]
[1 + (2 + (3 + 0)), 2 + (3 + 0), 3 + 0, 0]
[6, 5, 3, 0]
scanl (+) 0 [1..3]
[0, 0 + 1,0 + 1 + 2, 0 + 1 + 2 + 3]
[0, 1, 3, 6]</p>
<p>CHAPTER 10. DATA STRUCTURE ORIGAMI 575
scanl(+)1[1..3]
-- unfolding the
-- definition of scanl
=[1,1+1
, (1+1)+2
, ((1+1)+2)+3
]
-- evaluating addition
=[1,2,4,7]
Then to make this more explicit and properly equational,
we can follow along with how scanlexpands for this expression
based on the definition. First, we must see how scanlis defined.
We’re going to show you a version of it from a slightly older
baselibrary for GHC Haskell. The diﬀerences don’t change
anything important for us here:
scanl::(a-&gt;b-&gt;a)-&gt;a-&gt;[b]-&gt;[a]
scanlf q ls=
q:(caselsof
[]-&gt;[]
x:xs-&gt;scanl f (f q x) xs)
In an earlier chapter, we wrote a recursive function that</p>
<p>CHAPTER 10. DATA STRUCTURE ORIGAMI 576
returned the nth Fibonacci number to us. You can use a scan
function to return a list of Fibonacci numbers. We’re going
to do this in a source file because this will, in this state, return
an infinite list (feel free to try loading it into your REPL and
running it, but be quick with the ctrl-c):
fibs=1:scanl (+)1fibs
We start with a value of 1 and cons that onto the front of the
list generated by our scan. The list itself has to be recursive
because, as we saw previously, the idea of Fibonacci numbers
is that each one is the sum of the previous two in the sequence;
scanning the results of (+)over a nonrecursive list of numbers
whose start value is 1 would give us this:
scanl (+) 1 [1..3]
[1, 1 + 1, (1 + 1) + 2, ((1 + 1) + 2) + 3]
[1,2,4,7]
instead of the [1, 1, 2, 3, 5...] that we’re looking for.
Getting the fibonacci number we want
But we don’t really want an infinite list of Fibonacci numbers;
that isn’t very useful. We need a method to either take some
number of elements from that list or find the 𝑛th element as
we had done before. Fortunately, that’s the easy part. We’ll</p>
<p>CHAPTER 10. DATA STRUCTURE ORIGAMI 577
use the “bang bang” operator, !!, to find the 𝑛th element. This
operator is a way to index into a list, and indexing in Haskell
starts from zero. That is, the first value in your list is indexed
as zero. But otherwise the operator is straightforward:
(!!) :: [a] -&gt; Int -&gt; a
It needs a list as its first argument, an Intas its second argu-
ment and it returns one element from the list. Which item it
returns is the value that is in the 𝑛th spot where 𝑛is ourInt.
We will modify our source file:
fibs =1:scanl (+)1fibs
fibsNx=fibs!!x
Once we load the file into our REPL, we can use fibsNto
return the 𝑛th element of our scan:
Prelude&gt; fibsN 0
1
Prelude&gt; fibsN 2
2
Prelude&gt; fibsN 6
13
Now you can modify your source code to use the takeor
takeWhile functions or to filter it in any way you like. One</p>
<p>CHAPTER 10. DATA STRUCTURE ORIGAMI 578
note: filtering without also taking won’t work too well, because
you’re still getting an infinite list. It’s a filtered infinite list, sure,
but still infinite.
Scans Exercises
1.Modify your fibsfunction to only return the first 20 Fi-
bonacci numbers.
2.Modify fibsto return the Fibonacci numbers that are less
than 100.
3.Try to write the factorial function from Recursion as a
scan. You’ll want scanlagain, and your start value will be</p>
<ol>
<li>Warning: this will also generate an infinite list, so you
may want to pass it through a takefunction or similar.
10.10 Chapter Exercises
Warm-up and review
For the following set of exercises, you are not expected to use
folds. These are intended to review material from previous
chapters. Feelfreetouseanysyntaxorstructurefromprevious
chapters that seems appropriate.
1.Given the following sets of consonants and vowels:</li>
</ol>
<p>CHAPTER 10. DATA STRUCTURE ORIGAMI 579
stops = &quot;pbtdkg&quot;
vowels = &quot;aeiou&quot;
a)Write a function that takes inputs from stopsand
vowels and makes 3-tuples of all possible stop-vowel-
stop combinations. These will not all correspond to
real words in English, although the stop-vowel-stop
pattern is common enough that many of them will.
b)Modify that function so that it only returns the com-
binations that begin with a p.
c)Now set up lists of nouns and verbs (instead of stops
and vowels) and modify the function to make tuples
representing possible noun-verb-noun sentences.
2.What does the following mystery function do? What is
its type? Try to get a good sense of what it does before
you test it in the REPL to verify it.
seekritFunc x=
div (sum (map length (words x)))
(length (words x))
3.We’d really like the answer to be more precise. Can you
rewrite that using fractional division?</p>
<p>CHAPTER 10. DATA STRUCTURE ORIGAMI 580
Rewriting functions using folds
In the previous chapter, you wrote these functions using direct
recursion over lists. The goal now is to rewrite them using
folds. Where possible, to gain a deeper understanding of
folding, try rewriting the fold version so that it is point-free.
Point-free versions of these functions written with a fold
should look like:
myFunc=foldr f z
So for example with the andfunction:
-- Again, this type will be less
-- reusable than the one in GHC 7.10
-- and newer. Don't worry.
-- direct recursion, not using (&amp;&amp;)
myAnd::[Bool]-&gt;Bool
myAnd[]=True
myAnd(x:xs)=
ifx==False
thenFalse
elsemyAnd xs</p>
<p>CHAPTER 10. DATA STRUCTURE ORIGAMI 581
-- direct recursion, using (&amp;&amp;)
myAnd::[Bool]-&gt;Bool
myAnd[]=True
myAnd(x:xs)=x&amp;&amp;myAnd xs
-- fold, not point-free
-- in the folding function
myAnd::[Bool]-&gt;Bool
myAnd=foldr
(\a b-&gt;
ifa==False
thenFalse
elseb)True
-- fold, both myAnd and the folding
-- function are point-free now
myAnd::[Bool]-&gt;Bool
myAnd=foldr (&amp;&amp;)True
The goal here is to converge on the final version where
possible. You don’t need to write all variations for each ex-
ample, but the more variations you write, the deeper your
understanding of these functions will become.
1.myOrreturns Trueif anyBoolin the list is True.</p>
<p>CHAPTER 10. DATA STRUCTURE ORIGAMI 582
myOr::[Bool]-&gt;Bool
myOr=undefined
2.myAnyreturns Trueifa -&gt; Bool applied to any of the values
in the list returns True.
myAny::(a-&gt;Bool)-&gt;[a]-&gt;Bool
myAny=undefined
Example for validating myAny:
Prelude&gt; myAny even [1, 3, 5]
False
Prelude&gt; myAny odd [1, 3, 5]
True
3.Write two versions of myElem. One version should use
folding and the other should use any.
myElem::Eqa=&gt;a-&gt;[a]-&gt;Bool
Prelude&gt; myElem 1 [1..10]
True
Prelude&gt; myElem 1 [2..10]
False
4.Implement myReverse, don’t worry about trying to make
it lazy.</p>
<p>CHAPTER 10. DATA STRUCTURE ORIGAMI 583
myReverse ::[a]-&gt;[a]
myReverse =undefined
Prelude&gt; myReverse &quot;blah&quot;
&quot;halb&quot;
Prelude&gt; myReverse [1..5]
[5,4,3,2,1]
5.WritemyMapin terms of foldr. It should have the same
behavior as the built-in map.
myMap::(a-&gt;b)-&gt;[a]-&gt;[b]
myMap=undefined
6.WritemyFilter in terms of foldr. It should have the same
behavior as the built-in filter .
myFilter ::(a-&gt;Bool)-&gt;[a]-&gt;[a]
myFilter =undefined
7.squish flattens a list of lists into a list
squish::[[a]]-&gt;[a]
squish=undefined
8.squishMap maps a function over a list and concatenates the
results.</p>
<p>CHAPTER 10. DATA STRUCTURE ORIGAMI 584
squishMap ::(a-&gt;[b])-&gt;[a]-&gt;[b]
squishMap =undefined
Prelude&gt; squishMap (\x -&gt; [1, x, 3]) [2]
[1,2,3]
Prelude&gt; let f x = &quot;WO &quot; ++ [x] ++ &quot; OT &quot;
Prelude&gt; squishMap f &quot;blah&quot;
&quot;WO b OT WO l OT WO a OT WO h OT &quot;
9.squishAgain flattens a list of lists into a list. This time re-use
thesquishMap function.
squishAgain ::[[a]]-&gt;[a]
squishAgain =undefined
10.myMaximumBy takes a comparison function and a list and
returns the greatest element of the list based on the last
value that the comparison returned GTfor.
myMaximumBy ::(a-&gt;a-&gt;Ordering )
-&gt;[a]
-&gt;a
myMaximumBy =undefined
Prelude&gt; myMaximumBy (_ _ -&gt; GT) [1..10]
1
Prelude&gt; myMaximumBy (_ _ -&gt; LT) [1..10]</p>
<p>CHAPTER 10. DATA STRUCTURE ORIGAMI 585
10
Prelude&gt; myMaximumBy compare [1..10]
10
11.myMinimumBy takes a comparison function and a list and
returns the least element of the list based on the last value
that the comparison returned LTfor.
myMinimumBy ::(a-&gt;a-&gt;Ordering )
-&gt;[a]
-&gt;a
myMinimumBy =undefined
Prelude&gt; myMinimumBy (_ _ -&gt; GT) [1..10]
10
Prelude&gt; myMinimumBy (_ _ -&gt; LT) [1..10]
1
Prelude&gt; myMinimumBy compare [1..10]
1
10.11 Definitions
1.Afoldis a higher-order function which, given a function
to accumulate the results and a recursive data structure,
returns the built up value. Usually a “start value” for the
accumulation is provided along with a function that can</p>
<p>CHAPTER 10. DATA STRUCTURE ORIGAMI 586
combine the type of values in the data structure with the
accumulation. The term fold is typically used with ref-
erence to collections of values referenced by a recursive
datatype. For a generalization of “breaking down struc-
ture”, see catamorphism .
2.Acatamorphism is a generalization of folds to arbitrary
datatypes. Where a fold allows you to break down a list
into an arbitrary datatype, a catamorphism is a means of
breaking down the structure of any datatype. The bool
:: a -&gt; a -&gt; Bool -&gt; a function in Data.Bool is an example
of a simple catamorphism for a simple, non-collection
datatype. Similarly, maybe :: b -&gt; (a -&gt; b) -&gt; Maybe a -&gt;
bis the catamorphism for Maybe. See if you can notice a
pattern:</p>
<p>CHAPTER 10. DATA STRUCTURE ORIGAMI 587
dataBool=False|True
bool::a-&gt;a-&gt;Bool-&gt;a
dataMaybea=Nothing |Justa
maybe::b-&gt;(a-&gt;b)-&gt;Maybea-&gt;b
dataEithera b=Lefta|Rightb
either::(a-&gt;c)
-&gt;(b-&gt;c)
-&gt;Eithera b
-&gt;c
3.Atail call is the final result of a function. Some examples
of tail calls in Haskell functions:
fx y z=h (subFunction x y z)
wheresubFunction x y z =g x y z
-- the ``tail call'' is
-- h (subFunction x y z)
-- or more precisely, h.
4.Tail recursion is a function whose tail calls are recursive
invocations of itself. This is distinguished from functions
that call other functions in their tail call.
fx y z=h (subFunction x y z)
wheresubFunction x y z =g x y z</p>
<p>CHAPTER 10. DATA STRUCTURE ORIGAMI 588
The above is not tail recursive, calls ℎ, not itself.
fx y z=h (f (x -1) y z)
Still not tail recursive. 𝑓is invoked again but not in the
tail call of 𝑓; it’s an argument to the tail call, ℎ:
fx y z=f (x-1) y z
This is tail recursive. 𝑓is calling itself directly with no
intermediaries.
foldrf z[]=z
foldrf z (x:xs)=f x (foldr f z xs)
Not tail recursive, we give up control to the combining
function 𝑓before continuing through the list. foldr’s re-
cursive calls will bounce between foldrand𝑓.
foldlf z[]=z
foldlf z (x:xs)=foldl f (f z x) xs
Tail recursive. foldlinvokes itself recursively. The com-
bining function is only an argument to the recursive fold.</p>
<p>CHAPTER 10. DATA STRUCTURE ORIGAMI 589
10.12 Follow-up resources
1.Haskell Wiki. Fold.
https://wiki.haskell.org/Fold
2.Richard Bird. Sections 4.5 and 4.6 of Introduction to
Functional Programming using Haskell (1998).
3.Antoni Diller. Introduction to Haskell.
4.Graham Hutton. A tutorial on the universality and ex-
pressiveness of fold.
http://www.cs.nott.ac.uk/~gmh/fold.pdf</p>
<p>Chapter 11
Algebraic datatypes
The most depressing
thing about life as a
programmer, I think, is if
you’re faced with a chunk
of code that either
someone else wrote or,
worse still, you wrote
yourself but no longer
dare to modify. That’s
depressing.
Simon Peyton Jones
590</p>
<p>CHAPTER 11. RULE THE TYPES, RULE THE UNIVERSE 591
11.1 Algebraic datatypes
We have spent a lot of time talking about datatypes already, so
you may think we’ve covered everything that needs to be said
about those. This chapter’s purpose is ultimately to explain
how to construct your own datatypes in Haskell. Writing your
own datatypes can help you leverage some of Haskell’s most
powerful features — pattern matching, type checking, and
inference — in a way that makes your code more concise and
safer. But to understand that, first we need to explain the dif-
ferences among datatypes more fully and understand what it
means when we say datatypes are algebraic .
A type can be thought of as an enumeration of constructors
that have zero or more arguments.1We will return to this
description throughout the chapter, each time emphasizing a
diﬀerent portion of it.
Haskell oﬀers sum types, product types, product types with
record syntax, type aliases (for example, String is a type alias
for[Char] ), and a special datatype called a newtype that provides
for a diﬀerent set of options and constraints from either type
synonyms or data declarations. We will explain each of these
in detail in this chapter and show you how to exploit them for
maximum utility and type safety.
This chapter will:
1This description, slightly edited for our purposes, was proposed by Orah Kittrell in
the#haskell-beginners IRC channel.</p>
<p>CHAPTER 11. RULE THE TYPES, RULE THE UNIVERSE 592
•explain the “algebra” of algebraic datatypes;
•analyze the construction of data constructors;
•spell out when and how to write your own datatypes;
•clarify usage of type synonyms and newtype ;
•introduce kinds.
11.2 Data declarations review
We often want to create custom datatypes for structuring and
describing the data we are processing. Doing so can help you
analyze your problem by allowing you to focus first on how
youmodel the domain before you begin thinking about how
you write computations that solve your problem. It can also
make your code easier to read and use because it lays the
domain model out clearly.
In order to write your own types, though, you must under-
stand the way datatypes are constructed in more detail than
we’ve covered so far. Let’s begin with a review of the important
parts of datatypes, using the data declarations for Booland lists:</p>
<p>CHAPTER 11. RULE THE TYPES, RULE THE UNIVERSE 593
dataBool=False|True
-- [1] [2] [3] [4] [5] [6]
data[]a=[ ]|a:[a]
-- [ 7 ] [8] [9]
1.Keyword datato signal that what follows is a data declara-
tion, or a declaration of a datatype.
2.Type constructor (with no arguments).
3.Equals sign divides the type constructor from its data
constructors.
4.Data constructor. In this case, a data constructor that takes
no arguments and so is called a nullary constructor. This
is one of the possible values of this type that can show up
in term-level code.
5.The pipe denotes a sum type which indicates a logical
disjunction (colloquially, or) in what values can have that
type.
6.Constructor for the value True, another nullary construc-
tor.
7.Type constructor with an argument. An empty list has
to be applied to an argument in order to become a list</p>
<p>CHAPTER 11. RULE THE TYPES, RULE THE UNIVERSE 594
ofsomething . Here the argument is a polymorphic type
variable, so the list’s argument can be of diﬀerent types.
8.Data constructor for the empty list.
9.Data constructor that takes two arguments: an 𝑎and also
a[a].
When we talk about a data declaration, we are talking about
the definition of the entiretype. If wethink of a type as “anenu-
meration of constructors that have zeroor more arguments,”
thenBoolis an enumeration of two possible constructors, each
of which takes zeroarguments, while the type constructor []
enumerates two possible constructors and one of them takes
twoarguments. The pipe denotes what we call a sum type , a
type that has more than one constructor inhabiting it.
In addition to sum types, Haskell also has product types , and
we’ll talk more about those in a bit. The data constructors in
product types have more than one parameter. But first, let’s
turn our attention to the meaning of the word constructors .
11.3 Data and type constructors
There are two kinds of constructors in Haskell: type construc-
tors and data constructors. Type constructors are used only
at the type level, in type signatures and typeclass declarations
and instances. Types are static and resolve at compile time.</p>
<p>CHAPTER 11. RULE THE TYPES, RULE THE UNIVERSE 595
Data constructors construct the values at term level, values
you can interact with at runtime. We call them constructors
because they define a means of creating or building a type or
a value.
Although the term constructor is often used to describe all
type constructors and data constructors, we can make a dis-
tinction between constants andconstructors . Type and data con-
structors that take no arguments are constants. They can only
store a fixed type and amount of data. So, in the Booldatatype,
for example, Boolis a type constant, a concrete type that isn’t
waiting for any additional information in the form of an argu-
ment in order to be fully realized as a type. It enumerates two
values that are also constants, TrueandFalse, because they take
no arguments. While we call TrueandFalse“data constructors,”
in fact since they take no arguments, their value is already es-
tablished and not being constructed in any meaningful sense.
However, sometimes we need the flexibility of allowing dif-
ferent types or amounts of data to be stored in our datatypes.
For those times, type and data constructors may be parame-
terized. When a constructor takes an argument, then it’s like a
function in at least one sense — it must be applied to become a
concrete type or value. The following datatypes are pseudony-
mous versions of real datatypes in Haskell. We’ve given them
pseudonyms because we want to focus on the syntax, not the
semantics, for now.</p>
<p>CHAPTER 11. RULE THE TYPES, RULE THE UNIVERSE 596
dataTrivial =Trivial'
-- [1] [2]
dataUnaryTypeCon a=UnaryValueCon a
-- [3] [4]
1.Here the type constructor Trivial is like a constant value,
but at the type level. It takes no arguments and is thus
nullary . The Haskell Report calls these typeconstants to dis-
tinguish them from type constructors that take arguments.
2.The data constructor Trivial' is also like a constant value,
but it exists in value, term, or runtime space. These are
not three diﬀerent things, but three diﬀerent words for
the same space that types serve to describe.
3.UnaryTypeCon is a type constructor of one argument. It’s a
constructor awaiting a type constant to be applied to, but
it has no behavior in the sense that we think of functions
as having. Such type-level functions exist but are not
covered in this book.2
4.UnaryValueCon is a data constructor of one argument await-
ing a value to be applied to. Again, it doesn’t behave like
2If you’re interested in learning about this topic, Brent Yorgey’s blog posts about type
families and functional dependencies are a good place to start. https://byorgey.wordpress.
com/2010/06/29/typed-type-level-programming-in-haskell-part-i-functional-dependencies/</p>
<p>CHAPTER 11. RULE THE TYPES, RULE THE UNIVERSE 597
a term-level function in the sense of performing an oper-
ation on data. It’s more like a box to put values into. Be
careful with the box/container analogy as it will betray
you later — not all type arguments to constructors have
value-level witnesses! Some are phantom .
Each of these datatypes only enumerates one data construc-
tor. Whereas Trivial' is the only possible concrete value for
typeTrivial ,UnaryValueCon could show up as diﬀerent literal
values at runtime, depending on what type of 𝑎it is applied to.
Think back to the list datatype: at the type level, you have a :
[a]where the 𝑎is a variable. At the term level, in your code,
that will be applied to some type of values and become, for
example, [Char] or[Integer] (or list of whatever other concrete
type — obviously the set of possible lists is large).
11.4 Type constructors and kinds
Let’s look again at the list datatype:
data[]a=[]|a:[a]
This must be applied to a concrete type before you have a
list. We can see the parallel with functions when we look at
thekindsignature.
Kinds are the types of types, or types one level up. We
represent kinds in Haskell with *. We know something is a</p>
<p>CHAPTER 11. RULE THE TYPES, RULE THE UNIVERSE 598
fully applied, concrete type when it is represented as <em>. When
it is</em> -&gt; * , it, like a function, is still waiting to be applied.
Compare the following:
Prelude&gt; let f = not True
Prelude&gt; :t f
f :: Bool
Prelude&gt; let f x = x &gt; 3
Prelude&gt; :t f
f :: (Ord a, Num a) =&gt; a -&gt; Bool
The first 𝑓takes no arguments and is not awaiting appli-
cation to anything in order to produce a value, so its type
signature is a concrete type — note the lack of a function ar-
row. But the second 𝑓is awaiting application to an 𝑥so its type
signature has a function arrow. Once we apply it to a value, it
also has a concrete type:
Prelude&gt; let f x = x &gt; 3
Prelude&gt; :t f 5
f 5 :: Bool
We query the kind signature of a type constructor (not a
data constructor) in GHCi with a :kindor:k. We see that kind
signatures give us similar information about type constructors:</p>
<p>CHAPTER 11. RULE THE TYPES, RULE THE UNIVERSE 599
Prelude&gt; :k Bool
Bool :: *
Prelude&gt; :k [Int]
[Int] :: *
Prelude&gt; :k []
[] :: * -&gt; *
BothBooland[Int]are fully applied, concrete types, so their
kind signatures have no function arrows. That is, they are not
awaiting application to anything in order to be fully realized.
The kind of [], though, is * -&gt; * because it still needs to be
applied to a concrete type before it is itself a concrete type.
This is what the constructor of “type constructor” is referring
to.
11.5 Data constructors and values
We mentioned a bit ago that the Haskell Report draws a distinc-
tion between type constants and type constructors . We can draw
a similar distinction between data constructors and constant
values.</p>
<p>CHAPTER 11. RULE THE TYPES, RULE THE UNIVERSE 600
dataPugType =PugData
-- [1] [2]
dataHuskyType a=HuskyData
-- [3] [4]
dataDogueDeBordeaux doge=
-- [5]
DogueDeBordeaux doge
-- [6]
1.PugType is the type constructor, but it takes no arguments
so we can think of it as being a type constant . This is
how the Haskell Report refers to such types. This type
enumerates one constructor.
2.PugData is the only data constructor for the type PugType .
It also happens to be a constant value because it takes no
arguments and stands only for itself. For any function
that requires a value of type PugType , you know that value
will bePugData .
3.HuskyType is the type constructor and it takes a single para-
metrically polymorphic type variable as an argument. It
also enumerates one data constructor.</p>
<p>CHAPTER 11. RULE THE TYPES, RULE THE UNIVERSE 601
4.HuskyData is the data constructor for HuskyType . Note that
the type variable argument 𝑎doesnotoccur as an argu-
ment to HuskyData or anywhere else after the =. That means
our type argument 𝑎isphantom , or, “has no witness.” We
will elaborate on this later. Here HuskyData is a constant
value, like PugData .
5.DogueDeBordeaux is a type constructor and has a single type
variable argument like HuskyType , but called 𝑑𝑜𝑔𝑒instead
of𝑎. Why? Because the names of variables don’t matter.
At any rate, this type also enumerates one constructor.
6.DogueDeBordeaux is the lone data constructor. It has the
same name as the type constructor, but they are not the
same thing. The 𝑑𝑜𝑔𝑒type variable in the type construc-
tor occurs also in the data constructor. Remember that,
because they are the same type variable, these must agree
with each other: 𝑑𝑜𝑔𝑒must equal 𝑑𝑜𝑔𝑒. If your type is
DogueDeBordeaux [Person] , you must necessarily have a list
ofPerson values contained in the DogueDeBordeaux value.
But because DogueDeBordeaux must be applied before it’s a
concrete value, its literal value at runtime can change:
Prelude&gt; :t DogueDeBordeaux
DogueDeBordeaux :: doge
-&gt; DogueDeBordeaux doge</p>
<p>CHAPTER 11. RULE THE TYPES, RULE THE UNIVERSE 602
We can query the type of the value (not the type construc-
tor but the data constructor — it can be confusing when
the type constructor and the data constructor have the
same name, but it’s pretty common to do that in Haskell
because the compiler doesn’t confuse type names with
value names the way we mortals do). It tells us that once
𝑑𝑜𝑔𝑒is bound to a concrete type, then this will be a value
of type DogueDeBordeaux doge . It isn’t a value yet, but it’s a
definition for how to construct a value of that type.
Here’s how to make a value of the type of each:
myPug=PugData ::PugType
myHusky ::HuskyType a
myHusky =HuskyData
myOtherHusky ::Numa=&gt;HuskyType a
myOtherHusky =HuskyData
myOtherOtherHusky ::HuskyType [[[[Int]]]]
myOtherOtherHusky =HuskyData
-- no witness to the contrary ^
This will work because the value 10 agrees with the type
variable being bound to Int:</p>
<p>CHAPTER 11. RULE THE TYPES, RULE THE UNIVERSE 603
myDoge::DogueDeBordeaux Int
myDoge=DogueDeBordeaux 10
This will not work because 10 cannot be reconciled with
the type variable being bound to String :
badDoge ::DogueDeBordeaux String
badDoge =DogueDeBordeaux 10
Given this, we can see that constructors are how we create
values of types and refer to types in type signatures. There’s a
parallel here between type constructors and data constructors
that should be noted. We can illustrate this with a new canine-
oriented datatype:
dataDoggies a=
Huskya
|Mastiff a
deriving (Eq,Show)
-- type constructor awaiting an argument
Doggies
Note that the kind signature for the type constructor looks
like a function, and the type signature for either of its data
constructors looks similar.
This needs to be applied to become a concrete type:</p>
<p>CHAPTER 11. RULE THE TYPES, RULE THE UNIVERSE 604
Prelude&gt; :k Doggies
Doggies :: * -&gt; *
And this needs to be applied to become a concrete value:
Prelude&gt; :t Husky
Husky :: a -&gt; Doggies a
So the behavior of constructors is such that if they don’t take
any arguments, they behave like (type or value-level) constants.
If they do take arguments, they act like (type or value-level)
functions that don’t doanything except get applied.
Exercises: Dog Types
Given the datatypes defined in the above sections,
1.IsDoggies a type constructor or a data constructor?
2.What is the kind of Doggies ?
3.What is the kind of Doggies String ?
4.What is the type of Husky 10 ?
5.What is the type of Husky (10 :: Integer) ?
6.What is the type of Mastiff &quot;Scooby Doo&quot; ?
7.IsDogueDeBordeaux a type constructor or a data constructor?</p>
<p>CHAPTER 11. RULE THE TYPES, RULE THE UNIVERSE 605
8.What is the type of DogueDeBordeaux ?
9.What is the type of DogueDeBordeaux &quot;doggie!&quot;
11.6 What’s a type and what’s data?
As we’ve said, types are static and resolve at compile time.
Types are known before runtime, whether through explicit
declaration or type inference, and that’s what makes them
static types. Information about types does not persist through
to runtime. Data are what we’re working with at runtime.
Here compile time is literally when your program is getting
compiled by GHC or checked before execution in a REPL
like GHCi. Runtime is the actual execution of your program.
Types circumscribe values and in that way, they describe which
values are flowing through what parts of your program.
type constructors -- compile-time
-------------------- phase separation
data constructors -- runtime
Both data constructors and type constructors begin with
capital letters, but a constructor beforethe=in a datatype defini-
tion is a type constructor, while constructors afterthe=are data
constructors. Data constructors are usually generated by the</p>
<p>CHAPTER 11. RULE THE TYPES, RULE THE UNIVERSE 606
declaration. One tricky bit here is that when data constructors
take arguments, those arguments refer to other types . Because
of this, not everything referred to in a datatype declaration is
necessarily generated by that datatype itself. Let’s take a look at
a short example with diﬀerent datatypes to demonstrate what
we mean by this.
We start with a datatype Pricethat has one type construc-
tor, one data constructor, and one type argument in the data
constructor:
dataPrice=
-- (a)
PriceInteger deriving (Eq,Show)
-- (b) [1]
The type constructor is (a). The data constructor is (b), and
that takes one type argument, [1].
The value Pricedoes not depend solely on this datatype
definition. It depends on the type Integer as well. If, for some
reason, Integer wasn’t in scope, we’d be unable to generate
Pricevalues.
Next, we’ll define two datatypes, Manufacturer andAirline ,
that are each sum types with three data constructors. Each data
constructor in these is a possible value of that type, and since
none of them take arguments, all are generated by their decla-</p>
<p>CHAPTER 11. RULE THE TYPES, RULE THE UNIVERSE 607
rations and are more like constant values than constructors:
dataManufacturer =
-- (c)
Mini
-- (d)
|Mazda
-- (e)
|Tata
-- (f)
deriving (Eq,Show)
Manufacturer has the type constructor (c). Manufacturer has
three data constructors (d), (e), and (f).
dataAirline =
-- (g)
PapuAir
-- (h)
|CatapultsR'Us
-- (i)
|TakeYourChancesUnited
-- (j)
deriving (Eq,Show)</p>
<p>CHAPTER 11. RULE THE TYPES, RULE THE UNIVERSE 608
The type constructor is (g). Airline has three data construc-
tors (h), (i), and (j).
Next we’ll look at another sum type, but this one has data
constructors that take arguments. For the type Vehicle , the
data constructors are CarandPlane, so aVehicle is either a Car
value or a Planevalue. They each take types as arguments, just
asPriceitself took the type Integer as an argument:
dataVehicle =CarManufacturer Price
-- (k) (l) [2] [3]
|PlaneAirline
-- (m) [4]
deriving (Eq,Show)
The type constructor is (k). There are two data constructors,
(l) and (m). The type arguments are numbered [2], [3], and
[4]. [2] and [3] are type arguments to the data constructor Car,
while [4] is the type argument to the data constructor Plane. To
construct a Planevalue, therefore, we need a value from the
Airline type.
In the above, the datatypes are generating the constructors
marked with a letter. The type arguments marked with a
number existed prior to the declarations. Their definitions
exist outside of this declaration, and they must be in scope to
be used as part of this declaration.</p>
<p>CHAPTER 11. RULE THE TYPES, RULE THE UNIVERSE 609
Each of the above datatypes has a deriving clause. We have
seen this before, as it is usually true that you will want to de-
rive an instance of Showfor datatypes you write. The instance
allows your data to be printed to the screen as a string. De-
rivingEqis also common and allows you to derive equality
operations automatically for most datatypes where that would
make sense. There are other typeclasses that allow derivation
in this manner, and it obviates the need for manually writing
instances for each datatype and typeclass (reminder: you saw
an example of this in the Typeclasses chapter).
As we’ve seen, data constructors can take arguments. Those
arguments will be specific types, but not specific values. In
standard Haskell, we can’t choose specific values of types as
type arguments. We can’t say, for example, “ Boolwithout the
possibility of Falseas a value.” If you accept Boolas a valid type
for a function or as the component of a datatype, you must
accept all of Bool.
Exercises: Vehicles
For these exercises, we’ll use the datatypes defined in the above
section. It would be good if you’d typed them all into a source
file already, but if you hadn’t, please do so now. You can then
define some sample data on your own, or use these to get you
started:</p>
<p>CHAPTER 11. RULE THE TYPES, RULE THE UNIVERSE 610
myCar =CarMini(Price14000)
urCar =CarMazda(Price20000)
clownCar =CarTata(Price7000)
doge =PlanePapuAir
1.What is the type of myCar?
2.Given the following, define the functions:
isCar::Vehicle -&gt;Bool
isCar=undefined
isPlane ::Vehicle -&gt;Bool
isPlane =undefined
areCars ::[Vehicle]-&gt;[Bool]
areCars =undefined
3.Now we’re going to write a function to tell us the manu-
facturer of a piece of data:
getManu ::Vehicle -&gt;Manufacturer
getManu =undefined
4.Given that we’re returning the Manufacturer , what will hap-
pen if you use this on Planedata?</p>
<p>CHAPTER 11. RULE THE TYPES, RULE THE UNIVERSE 611
5.All right. Let’s say you’ve decided to add the size of the
plane as an argument to the Planeconstructor. Add that
to your datatypes in the appropriate places and change
your data and functions appropriately.
11.7 Data constructor arities
Now that we have a good understanding of the anatomy of
datatypes, we want to start demonstrating why we call them “al-
gebraic.” We’ll start by looking at something called arity. Arity
refers to the number of arguments a function or constructor
takes. A function that takes no arguments is called nullary ,
where nullary is a contraction of “null” and “-ary”. Null means
zero, the “-ary” suffix means “of or pertaining to”. “-ary” is a
common suffix used when talking about mathematical arity,
such as with nullary, unary, binary, and the like.
Data constructors which take no arguments are also called
nullary. Nullary data constructors, such as TrueandFalse, are
constant values at the term level and, since they have no argu-
ments, they can’t construct or represent any data other than
themselves. They are values which stand for themselves and
act as a witness of the datatype they were declared in.
We’ve said that “A type can be thought of as an enumeration
of constructors that have zero or morearguments.” We’ll look
next at constructors with arguments.
We’ve seen how data constructors may take arguments and</p>
<p>CHAPTER 11. RULE THE TYPES, RULE THE UNIVERSE 612
that makes them more like a function in that they must be
applied to something before you have a value. Data construc-
tors that take one argument are called unary. As we will see
later in this chapter, data constructors that take more than one
argument are called products .
All of the following are valid data declarations:
-- nullary
dataExample0 =
Example0
deriving (Eq,Show)
-- unary
dataExample1 =
Example1 Int
deriving (Eq,Show)
-- product of Int and String
dataExample2 =
Example2 IntString
deriving (Eq,Show)
Prelude&gt; Example0
Example0
Prelude&gt; Example1 10
Example1 10</p>
<p>CHAPTER 11. RULE THE TYPES, RULE THE UNIVERSE 613
Prelude&gt; Example1 10 == Example1 42
False
Prelude&gt; let nc = Example2 1 &quot;NC&quot;
Prelude&gt; Example2 10 &quot;FlappityBat&quot; == nc
False
OurExample2 is an example of a product , like tuples, which
we’ve seen before. Tuples can take several arguments — as
many as there are inhabitants of each tuple — and are consid-
ered the canonical product type; they are anonymous products
because they have no name. We’ll talk more about product
types soon.
Unary (one argument) data constructors contain a single
value of whatever type their argument was. The following is a
data declaration that contains the data constructor MyVal.MyVal
takes one Intargument and creates a type named MyType :
dataMyType=MyValInt
-- [1] [2] [3]
deriving (Eq,Show)
-- [4] [5]
1.Type constructor.
2.Data constructor. MyValtakes one type argument, so it is
called a unary data constructor.</p>
<p>CHAPTER 11. RULE THE TYPES, RULE THE UNIVERSE 614
3.Type argument to the definition of the data constructor
from [2].
4.Deriving clause.
5.Typeclass instances being derived. We’re getting equality
Eqand value stringification Showfor free.
Prelude&gt; :t MyVal
MyVal :: Int -&gt; MyType
Prelude&gt; MyVal 10
MyVal 10
Prelude&gt; MyVal 10 == MyVal 10
True
Prelude&gt; MyVal 10 == MyVal 9
False
Because MyValhas one Intargument, a value of type MyType
must contain one — only one — Intvalue.
11.8 What makes these datatypes
algebraic?
Algebraic datatypes in Haskell are algebraic because we can
describe the patterns of argument structures using two basic
operations: sum and product. The most direct way to explain
why they’re called sum and product is to demonstrate sum</p>
<p>CHAPTER 11. RULE THE TYPES, RULE THE UNIVERSE 615
and product in terms of cardinality . This can be understood in
terms of the cardinality you see with finite sets.3This doesn’t
map perfectly as we can have infinite data structures in Haskell,
but it’s a good way to begin understanding and appreciating
how datatypes work. When it comes to programming lan-
guages we are concerned with computable functions, not just
those which can generate a set.
The cardinality of a datatype is the number of possible
values it defines. That number can be as small as 0 or as large
as infinite (for example, numeric datatypes, lists). Knowing
how many possible values inhabit a type can help you reason
about your programs. In the following sections we’ll show
you how to calculate the cardinality of a given datatype based
solely on how it is defined. From there, we can determine
how many diﬀerent possible implementations there are of a
function for a given type signature.
Before we get into the specifics of how to calculate cardi-
nality in general, we’re going to take cursory glances at some
datatypes with easy to understand cardinalities: BoolandInt.
We’ve looked extensively at the Booltype already so you
already know it only has two inhabitants that are both nullary
data constructors, so Boolonly has two possible values. The
cardinality of Boolis, therefore, 2. Even without understanding
3Type theory was developed as an alternative mathematical foundation to set theory.
We won’t write formal proofs based on this, but the way we reason informally about types
as programmers derives in part from their origins as sets. Finite sets contain a number of
unique objects; that number is called cardinality.</p>
<p>CHAPTER 11. RULE THE TYPES, RULE THE UNIVERSE 616
the rules of cardinality of sum types, we can see why this is
true.
Another set of datatypes with cardinality that is reasonably
easy to understand are the Inttypes. In part this is because Int
and related types Int8,Int16, andInt32have clearly delineated
upper and lower bounds, defined by the amount of memory
they are permitted to use. We’ll use Int8here, even though it
isn’t very common in Haskell, because it has the smallest set
of possible inhabitants and thus the arithmetic is a bit easier
to do. Valid Int8values are whole numbers from (-128) to 127.
Int8is not included in the standard Prelude , unlike standard
Int, so we need to import it to see it in the REPL, but after
we do that we can use maxBound andminBound from the Bounded
typeclass to view the upper and lower values:
Prelude&gt; import Data.Int
Prelude Data.Int&gt; minBound :: Int8
-128
Prelude Data.Int&gt; maxBound :: Int8
127
Given that this range includes the value 0, we can easily
figure out the cardinality of Int8with some quick addition:
128 + 127 + 1 = 256. So the cardinality of Int8is 256. Anywhere
in your code where you’d have a value of type Int8, there are
256 possible runtime values.</p>
<p>CHAPTER 11. RULE THE TYPES, RULE THE UNIVERSE 617
Exercises: Cardinality
While we haven’t explicitly described the rules for calculating
the cardinality of datatypes yet, you might already have an idea
of how to do it for simple datatypes with nullary constructors.
Try not to overthink these exercises — follow your intuition
based on what you know.
1.dataPugType =PugData
2.For this one, recall that Boolis also defined with the |:
dataAirline =
PapuAir
|CatapultsR'Us
|TakeYourChancesUnited
3.Given what we know about Int8, what’s the cardinality of
Int16?
4.Use the REPL and maxBound andminBound to examine Int
andInteger . What can you say about the cardinality of
those types?
5.Extra credit (impress your friends!): What’s the connec-
tion between the 8 in Int8and that type’s cardinality of
256?</p>
<p>CHAPTER 11. RULE THE TYPES, RULE THE UNIVERSE 618
Simple datatypes with nullary data constructors
We’llstartourexplorationofcardinalitybylookingatdatatypes
with nullary data constructors:
dataExample =MakeExample deriving Show
Example is our type constructor, and MakeExample is our only
data constructor. Since MakeExample takes no type arguments, it
is a nullary constructor. We know that nullary data construc-
tors are constants and represent only themselves as values. It
is a single value whose only content is its name, not any other
data. Nullary constructors represent onevalue when reasoning
about the cardinality of the types they inhabit.
All you can say about MakeExample is that the constructor is
the value MakeExample and that it inhabits the type Example .
Theretheonlyinhabitantis MakeExample . Giventhat MakeExample
is a single nullary value, so the cardinality of the type Example is</p>
<ol>
<li>This is useful because it tells us that any time we see Example
in the type signature of a function, we only have to reason
about one possible value.
Exercises: For Example
1.You can query the type of a value in GHCi with the :type
command, also abbreviated :t.
Example:</li>
</ol>
<p>CHAPTER 11. RULE THE TYPES, RULE THE UNIVERSE 619
Prelude&gt; :t False
False :: Bool
What is the type of data constructor MakeExample ? What
happens when you request the type of Example ?
2.What if you try :infoonExample in GHCi? Can you deter-
mine what typeclass instances are defined for the Example
type using :infoin GHCi?
3.Try making a new datatype like Example but with a single
type argument added to MakeExample , such as Int. What has
changed when you query MakeExample with:typein GHCi?
Unary constructors
In the last section, we asked you to add a single type argument
to theMakeExample data constructor. In doing so, you changed
it from a nullary constructor to a unary one. A unary data con-
structor takes one argument. In the declaration of the datatype,
that parameter will be a type, not a value. Now, instead of your
data constructor being a constant, or a known value, the value
will be constructed at runtime from the argument we applied
it to.
Datatypes that only contain a unary constructor always have
the same cardinality as the type they contain. In the following,
Goatshas the same number of inhabitants as Int:</p>
<p>CHAPTER 11. RULE THE TYPES, RULE THE UNIVERSE 620
dataGoats=GoatsIntderiving (Eq,Show)
Anything that is a valid Int, must also be a valid argument
to theGoatsconstructor. Anything that isn’t a valid Intalso
isn’t a valid count of Goats.
For cardinality, this means unary constructors are the iden-
tity function.
11.9 newtype
We will now look at a way to define a type that can only ever
have a single unary data constructor. We use the newtype key-
word to mark these types, as they are diﬀerent from type
declarations marked with the datakeyword as well as from
type synonym definitions marked by the typekeyword. Like
other datatypes that have a single unary constructor, the car-
dinality of a newtype is the same as that of the type it contains.
Anewtype cannot be a product type, sum type, or contain
nullary constructors, but it has a few advantages over a vanilla
datadeclaration. One is that it has no runtime overhead, as
it reuses the representation of the type it contains. It can do
this because it’s not allowed to be a record (product type) or
tagged union (sum type). The diﬀerence between newtype and
the type it contains is gone by the time the compiler generates
the code.</p>
<p>CHAPTER 11. RULE THE TYPES, RULE THE UNIVERSE 621
To illustrate, let’s say we have a function from Int -&gt; Bool
for checking whether we have too many goats:
tooManyGoats ::Int-&gt;Bool
tooManyGoats n=n&gt;42
We might run into a problem here if we had diﬀerent limits
for diﬀerent sorts of livestock. What if we mixed up the Int
value of cows where we meant goats? Fortunately, there’s a
way to address this with unary constructors:
newtype Goats=
GoatsIntderiving (Eq,Show)
newtype Cows=
CowsIntderiving (Eq,Show)
Now we can rewrite our type to be safer, pattern matching
in order to access the Intinside our data constructor Goats:
tooManyGoats ::Goats-&gt;Bool
tooManyGoats (Goatsn)=n&gt;42
Now we can’t mix up our livestock counts:
Prelude&gt; tooManyGoats (Goats 43)
True
Prelude&gt; tooManyGoats (Cows 43)</p>
<p>CHAPTER 11. RULE THE TYPES, RULE THE UNIVERSE 622
Couldn't match expected type
‘Goats’ with actual type ‘Cows’
In the first argument of
‘tooManyGoats’, namely ‘(Cows 43)’
In the expression: tooManyGoats (Cows 43)
Usingnewtype can deliver other advantages related to type-
class instances. To see these, we need to compare newtypes to
type synonyms and regular data declarations. We’ll start with
a short comparison to type synonyms.
Anewtype is similar to a type synonym in that the represen-
tations of the named type and the type it contains are identical
and any distinction between them is gone at compile time. So,
aString really is a [Char] andGoatsabove is really an Int. On
the surface, for the human writers and readers of code, the
distinction can be helpful in tracking where data came from
and what it’s being used for, but the diﬀerence is irrelevant to
the compiler.
However, one key contrast between a newtype and a type
alias is that you can define typeclass instances for newtype s that
diﬀer from the instances for their underlying type. You can’t
do that for type synonyms. Let’s take a look at how that works.
We’ll first define a typeclass called TooMany and an instance for
Int:</p>
<p>CHAPTER 11. RULE THE TYPES, RULE THE UNIVERSE 623
classTooMany awhere
tooMany ::a-&gt;Bool
instance TooMany Intwhere
tooMany n =n&gt;42
We can use that instance in the REPL but only if we assign
the type Intto whatever numeric literal we’re passing as an
argument, because numeric literals are polymorphic. That
looks like this:
Prelude&gt; tooMany (42 :: Int)
Take a moment and play around with this — try leaving oﬀ
the type declaration and giving it diﬀerent arguments.
Now, let’s say for your goat counting you wanted a special
instance of TooMany that will have diﬀerent behavior from the
Intinstance. Under the hood, Goatsis stillIntbut the newtype
declaration will allow you to define a custom instance:
newtype Goats=GoatsIntderiving Show
instance TooMany Goatswhere
tooMany ( Goatsn)=n&gt;43
Try loading this and passing diﬀerent arguments to it. Does
it behave diﬀerently than the Intinstance above? Do you still</p>
<p>CHAPTER 11. RULE THE TYPES, RULE THE UNIVERSE 624
need to explicitly assign a type to your numeric literals? What
is the type of tooMany ?
Here we were able to make the Goatsnewtype have an in-
stance of TooMany which had diﬀerent behavior than the type
Intwhich it contains. We can’t do this if it’s a type synonym.
Don’t believe us? Try it.
On the other hand, what about the case where we want to
reuse the typeclass instances of the type our newtype contains?
For common typeclasses built into GHC like Eq,Ord,Enum, and
Showwe get this facility for free, as you’ve seen with the deriving
clauses in most datatypes.
For user-defined typeclasses, we can use a language exten-
sion called GeneralizedNewtypeDeriving . Language extensions,
enabled in GHC by the LANGUAGE pragma,4tell the compiler to
process input in ways beyond what the standard provides for.
In this case, this extension will tell the compiler to allow our
newtype to rely on a typeclass instance for the type it contains.
We can do this because the representations of the newtype and
the type it contains are the same. Still, it is outside of the
compiler’s standard behavior so we must give it the special
instruction to allow us to do this.
First, let’s take the case of what we must do without gener-
alized newtype deriving:
4Apragma is a special instruction to the compiler placed in source code. The LANGUAGE
pragma is perhaps more common in GHC Haskell than the other pragmas, but there are
other pragmas we will see later in the book.</p>
<p>CHAPTER 11. RULE THE TYPES, RULE THE UNIVERSE 625
classTooMany awhere
tooMany ::a-&gt;Bool
instance TooMany Intwhere
tooMany n =n&gt;42
newtype Goats=
GoatsIntderiving (Eq,Show)
instance TooMany Goatswhere
tooMany ( Goatsn)=tooMany n
TheGoatsinstance will do the same thing as the Intinstance,
but we still have to define it separately.
You can test this yourself to see that they’ll return the same
answers.
Now we’ll add the pragma at the top of our source file:</p>
<p>CHAPTER 11. RULE THE TYPES, RULE THE UNIVERSE 626
{-# LANGUAGE GeneralizedNewtypeDeriving #-}
classTooMany awhere
tooMany ::a-&gt;Bool
instance TooMany Intwhere
tooMany n =n&gt;42
newtype Goats=
GoatsIntderiving (Eq,Show,TooMany)
Now we don’t have to define an instance of TooMany forGoats
that’s identical to the Intinstance. We can reuse the instance
that we already have.
This is also nice for times when we want every typeclass
instance to be the same except for the one we want to change.
Exercises: Logic Goats
1.Reusing the TooMany typeclass, write an instance of the
typeclass for the type (Int, String) . This will require
adding a language pragma named FlexibleInstances5if
you do not use a newtype — GHC will tell you what to do.
5https://ghc.haskell.org/trac/haskell-prime/wiki/FlexibleInstances</p>
<p>CHAPTER 11. RULE THE TYPES, RULE THE UNIVERSE 627
2.Make another TooMany instance for (Int, Int) . Sum the
values together under the assumption this is a count of
goats from two fields.
3.Makeanother TooMany instance, thistimefor (Num a, TooMany
a) =&gt; (a, a) . This can mean whatever you want, such as
summing the two numbers together.
11.10 Sum types
Now that we’ve looked at data constructor arities, we’re ready
to define the algebra of algebraic datatypes. The first that we’ll
look at is the sum type such as Bool:
dataBool=False|True
We’ve mentioned previously that the |represents logical
disjunction—thatis, “or.” Thisisthe suminalgebraicdatatypes.
To know the cardinality of sum types, we addthe cardinalities
of their data constructors. TrueandFalsetake no type argu-
ments and thus are nullary constructors, each with a value of</p>
<ol>
<li></li>
</ol>
<p>Now we do some arithmetic. As we said earlier, nullary
constructors are 1, and sum types are +or addition, when we
are talking about cardinality:
dataBool=False|True</p>
<p>CHAPTER 11. RULE THE TYPES, RULE THE UNIVERSE 628
How many values inhabit Bool? There are two data con-
structors, each representing only one possible value. Given
that the |syntax represents (+)or addition:
-- ?? represents the cardinality
True|False= ??
True+False== ??
-- False and True both == 1
1+1== ??
We see that the cardinality of Boolis:
1+1==2
-- List of all possible values for Bool
[True,False]-- length is 2
You can check that in your REPL:
Prelude&gt; length (enumFrom False)
2
From this, we see that when working with a Boolvalue we
must reason about two possible values. Sum types are a way
of expressing alternate possibilities within a single datatype.</p>
<p>CHAPTER 11. RULE THE TYPES, RULE THE UNIVERSE 629
Signed 8-bit hardware integers in Haskell are defined using
the aforementioned Int8datatype with a range of values from
-128 to 127. It’s not defined this way, but you could think of
it as a sum type of the numbers in that range, leading to the
cardinality of 256 as we saw.
Exercises: Pity the Bool
1.Given a datatype
dataBigSmall =
BigBool
|SmallBool
deriving (Eq,Show)
What is the cardinality of this datatype? Hint: We already
knowBool’s cardinality. Show your work as demonstrated
earlier.
2.Given a datatype</p>
<p>CHAPTER 11. RULE THE TYPES, RULE THE UNIVERSE 630
-- bring Int8 in scope
importData.Int
dataNumberOrBool =
NumbaInt8
|BoolyBool Bool
deriving (Eq,Show)
-- parentheses due to syntactic
-- collision between (-) minus
-- and the negate function
letmyNumba =Numba(-128)
What is the cardinality of NumberOrBool ? What happens if
you try to create a Numbawith a numeric literal larger than
127? And with a numeric literal smaller than (-128)?
If you choose (-128) for a value precisely, you’ll notice
you get a spurious warning:
Prelude&gt; let n = Numba (-128)
Literal 128 is out of the
Int8 range -128..127
If you are trying to write a large negative
literal, use NegativeLiterals</p>
<p>CHAPTER 11. RULE THE TYPES, RULE THE UNIVERSE 631
Now, since -128 is a perfectly valid Int8value you could
choose to ignore this. What happens is that (-128) desug-
ars into (negate 128) . The compiler sees that you expect
the type Int8, butInt8’s max boundary is 127. So even
though you’re negating 128, it hasn’t done that step yet
andimmediately whines about 128 being larger than 127.
One way to avoid the warning is the following:
Prelude&gt; let n = (-128)
Prelude&gt; let x = Numba n
Or you can use the NegativeLiterals extension as it recom-
mends:
Prelude&gt; :set -XNegativeLiterals
Prelude&gt; let n = Numba (-128)
Note that the negative literals extension doesn’t prevent
the warning if you use negate .
11.11 Product types
What does it mean for a type to be a product? A product type’s
cardinality is the product of the cardinalities of its inhabitants.
Arithmetically, products are the result of multiplication . Where
a sum type was expressing or, a product type expresses and.</p>
<p>CHAPTER 11. RULE THE TYPES, RULE THE UNIVERSE 632
For those that have programmed in C-like languages before,
a product is like a struct. For those that haven’t, a product is a
way to carry multiple values around in a single data construc-
tor. Any data constructor with two or more type arguments is
a product.
We said previously that tuples are anonymous products.
The declaration of the tuple type looks like this:
( , ) :: a -&gt; b -&gt; (a, b)
This is a product, like a product type: it gives you a way
to encapsulate two pieces of data, of possibly (though not
necessarily) diﬀerent types, in a single value.
We’ll look next at a somewhat silly sum type:
dataQuantumBool =QuantumTrue
|QuantumFalse
|QuantumBoth
deriving (Eq,Show)
What is the cardinality of this sum type?
For reasons that will become obvious, a cardinality of 2
makes it harder to show the diﬀerence between sum and prod-
uct cardinality, so QuantumBool has a cardinality of 3. Now we’re
going to define a product type that contains two QuantumBool
values:</p>
<p>CHAPTER 11. RULE THE TYPES, RULE THE UNIVERSE 633
dataTwoQs=
MkTwoQs QuantumBool QuantumBool
deriving (Eq,Show)
The datatype TwoQshas one data constructor, MkTwoQs , that
takes two arguments, making it a product of the two types that
inhabit it. Each argument is of type QuantumBool , which has a
cardinality of 3.
You can write this out to help you visualize it if you like. A
MkTwoQs value could be:
MkTwoQs QuantumTrue QuantumTrue
MkTwoQs QuantumTrue QuantumFalse
MkTwoQs QuantumTrue QuantumBoth
MkTwoQs QuantumFalse QuantumFalse
-- ...... and so on
Note that there is no special syntax denoting product types
as there was with sums and |.MkTwoQs is a data constructor
taking two type arguments, which both happen to be the same
type. It is a product type, the product of two QuantumBool s. The
number of potential values that can manifest in this type is the
cardinality of one of its type arguments times the cardinality
of the other. So, what is the cardinality of TwoQs?
We could have also written the TwoQstype using a type alias
and the tuple data constructor. Type aliases create type con-
structors, not data constructors:</p>
<p>CHAPTER 11. RULE THE TYPES, RULE THE UNIVERSE 634
typeTwoQs=(QuantumBool ,QuantumBool )
The cardinality of this will be the same as it was previously.
The reason it’s important to understand cardinality is that
the cardinality of a datatype roughly equates to how difficult
it is to reason about.
Record syntax
Records in Haskell are product types with additional syntax to
provide convenient accessors to fields within the record. Let’s
begin by definining a simple product type:
dataPerson=
MkPerson StringInt
deriving (Eq,Show)
That is the familiar product type structure: the MkPerson
data constructor takes two type arguments in its definition, a
String value (a name) and an Intvalue (an age). The cardinality
of this is frankly terrifying.
As we’ve seen in previous examples, we can unpack the
contents of this type using functions that return the value we
want from our little box of values:</p>
<p>CHAPTER 11. RULE THE TYPES, RULE THE UNIVERSE 635
-- sample data
jm=MkPerson &quot;julie&quot; 108
ca=MkPerson &quot;chris&quot; 16
namae::Person-&gt;String
namae(MkPerson s_)=s
If you use the namaefunction in your REPL, it will return
theString value from your data.
Now let’s see how we could define a similar product type
but with record syntax:
dataPerson=
Person{ name::String
, age::Int}
deriving (Eq,Show)
You can see the similarity to the Person type defined above,
but defining it as a record means there are now named record
field accessors. They’re just functions that go from the product
type to a member of product:
Prelude&gt; :t name
name :: Person -&gt; String
Prelude&gt; :t age
age :: Person -&gt; Int</p>
<p>CHAPTER 11. RULE THE TYPES, RULE THE UNIVERSE 636
You can use this directly in GHCi:
Prelude&gt; Person &quot;Papu&quot; 5
Person {name = &quot;Papu&quot;, age = 5}
Prelude&gt; let papu = Person &quot;Papu&quot; 5
Prelude&gt; age papu
5
Prelude&gt; name papu
&quot;Papu&quot;
You can also do it from data that is in a file. Change the jm
andcadata above so that it is now of type Person , reload your
source file, and try using the record field accessors in GHCi to
query the values.
11.12 Normal form
We’velookedatthealgebrabehindHaskell’salgebraicdatatypes,
and explored how this is useful for understanding the cardi-
nality of datatypes. But the algebra doesn’t stop there. All the
existing algebraic rules for products and sums apply in type
systems, and that includes the distributive property. Let’s take
a look at how that works in arithmetic:
2 * (3 + 4)
2 * (7)</p>
<p>CHAPTER 11. RULE THE TYPES, RULE THE UNIVERSE 637
14
We can rewrite this with the multiplication distributed over
the addition and obtain the same result:
2 * 3 + 2 * 4
(6) + (8)
14
This is known as a “sum of products.” In normal arithmetic,
the expression is in normal form when it’s been reduced to
a final result. However, if you think of the numerals in the
above expressions as representations of set cardinality, then
the sum of products expression is in normal form, as there is
no computation to perform.
The distributive property can be generalized:
a * (b + c) -&gt; (a * b) + (a * c)
And this is true of Haskell’s types as well! Product types
distribute over sum types. To play with this, we’ll first define
some datatypes:</p>
<p>CHAPTER 11. RULE THE TYPES, RULE THE UNIVERSE 638
dataFiction =Fiction deriving Show
dataNonfiction =Nonfiction deriving Show
dataBookType =FictionBook Fiction
|NonfictionBook Nonfiction
deriving Show
We define two types with only single, nullary inhabitants:
Fiction andNonfiction . The reasons for doing that may not
be immediately clear but recall that we said you can’t use a
type while only permitting one of its inhabitants as a possible
value. You can’t ask for a value of type Boolwhile declaring
in your types that it must always be True— you must admit
the possibility of either Boolvalue. So, declaring the Fiction
andNonfiction types will allow us to factor out the book types
(below).
Then we have a sum type, BookType , with constructors that
take the Fiction andNonfiction typesas arguments. It’s impor-
tant to remember that, although the type constructors and data
constructors of Fiction andNonfiction have the same name,
they are not the same, and it is the type constructors that
are the arguments to FictionBook andNonfictionBook . Take a
moment and rename them to demonstrate this to yourself.
So, we have our sum type. Next we’re going to define a type
synonym called AuthorName and a product type called Author.
The type synonym doesn’t really do anything except help us</p>
<p>CHAPTER 11. RULE THE TYPES, RULE THE UNIVERSE 639
keep track of which String we’re using in the Author type:
typeAuthorName =String
dataAuthor=Author(AuthorName ,BookType )
This isn’t a sum of products, so it isn’t normal form. It
can, in some sense, be evaluated to tease apart the values that
are hiding in the sum type, BookType . Again, we can apply the
distributive property and rewrite Author in normal form:
typeAuthorName =String
-- If you have them in the same
-- file, you'll need to comment
-- out previous definitions of
-- Fiction and Nonfiction.
dataAuthor=
Fiction AuthorName
|Nonfiction AuthorName
deriving (Eq,Show)
Products distribute over sums. Just as we would do with
the expression a * (b + c) , where the inhabitants of the sum
typeBookType are the 𝑏and𝑐, we broke those values out and</p>
<p>CHAPTER 11. RULE THE TYPES, RULE THE UNIVERSE 640
made a sum of products. Now it’s in normal form because
no further evaluation can be done of these constructors until
some operation or computation is done using these types.
Another example of normal form can be found in the Expr
type which is very common to papers about type systems and
programming languages:
dataExpr=
NumberInt
|AddExprExpr
|MinusExpr
|MultExprExpr
|DivideExprExpr
This is in normal form because it’s a sum (type) of products:
(Number Int) + Add (Expr Expr) + …
A stricter interpretation of normal form or “sum of prod-
ucts” would require representing products with tuples and
sums with Either. The previous datatype in that form would
look like the following:</p>
<p>CHAPTER 11. RULE THE TYPES, RULE THE UNIVERSE 641
typeNumber=Int
typeAdd=(Expr,Expr)
typeMinus=Expr
typeMult=(Expr,Expr)
typeDivide=(Expr,Expr)
typeExpr=
EitherNumber
(EitherAdd
(EitherMinus
(EitherMultDivide)))
This representation finds applications in problems where
one is writing functions or foldsover the representations of
datatypes, such as with generics and metaprogramming. Some
of these methods have their application in Haskell but should
be used judiciously and aren’t always easy to use.
TheEither type will be explained in detail in the next chap-
ter.
Exercises: How Does Your Garden Grow?</p>
<ol>
<li>Given the type</li>
</ol>
<p>CHAPTER 11. RULE THE TYPES, RULE THE UNIVERSE 642
dataFlowerType =Gardenia
|Daisy
|Rose
|Lilac
deriving Show
typeGardener =String
dataGarden=
GardenGardener FlowerType
deriving Show
What is the sum of products normal form of Garden ?
11.13 Constructing and deconstructing
values
There are essentially two things we can do with a value: we can
generate or construct it or we can match on it and consume
it. We talked above about why data and type constructors are
calledconstructors , and this section will elaborate on that and
how to construct values of diﬀerent types. You have already
been doing this in previous chapters, but we hope this section
will lead you to a deeper understanding.</p>
<p>CHAPTER 11. RULE THE TYPES, RULE THE UNIVERSE 643
Construction and deconstruction of values form a duality.
Data is immutable in Haskell, so values carry with them the
information about how they were created. We can use that
information when we consume or deconstruct the value.
We’ll start by defining a collection of datatypes:
dataGuessWhat =
Chickenbutt deriving (Eq,Show)
dataIda=
MkIdaderiving (Eq,Show)
dataProduct a b=
Product a bderiving (Eq,Show)
dataSuma b=
Firsta
|Secondb
deriving (Eq,Show)
dataRecordProduct a b=
RecordProduct { pfirst ::a
, psecond ::b }
deriving (Eq,Show)
Now that we have diﬀerent sorts of datatypes to work with,</p>
<p>CHAPTER 11. RULE THE TYPES, RULE THE UNIVERSE 644
we’ll move on to constructing values of those types.
Sum and Product
HereSumandProduct are ways to represent arbitrary sums and
products in types. In ordinary Haskell code, it’s unlikely you’d
need or want nestable sums and products unless you were
doing something fairly advanced, but here we use them as a
means of demonstration.
If you have two values in a product, then the conversion
to using Product is straightforward (n.b.: The SumandProduct
declarations from above will need to be in scope for all the
following examples):</p>
<p>CHAPTER 11. RULE THE TYPES, RULE THE UNIVERSE 645
newtype NumCow=
NumCowInt
deriving (Eq,Show)
newtype NumPig=
NumPigInt
deriving (Eq,Show)
dataFarmhouse =
Farmhouse NumCowNumPig
deriving (Eq,Show)
typeFarmhouse' =Product NumCowNumPig
Farmhouse andFarmhouse' are the same.
For an example with three values in the product instead of
two, we must begin to take advantage of the fact that Product
takes two arguments, one of which can also be another Product
of values. In fact, you can nest them as far as you can stomach
or until the compiler chokes:</p>
<p>CHAPTER 11. RULE THE TYPES, RULE THE UNIVERSE 646
newtype NumSheep =
NumSheep Int
deriving (Eq,Show)
dataBigFarmhouse =
BigFarmhouse NumCowNumPigNumSheep
deriving (Eq,Show)
typeBigFarmhouse' =
Product NumCow(Product NumPigNumSheep )
We can perform a similar trick with Sum:
typeName=String
typeAge=Int
typeLovesMud =Bool
Sheep can produce between 2 and 30 pounds (0.9 and 13
kilos) of wool per year! Icelandic sheep don’t produce as much
wool per year as other breeds but the wool they do produce is
a finer wool.
typePoundsOfWool =Int
dataCowInfo =
CowInfo NameAge
deriving (Eq,Show)</p>
<p>CHAPTER 11. RULE THE TYPES, RULE THE UNIVERSE 647
dataPigInfo =
PigInfo NameAgeLovesMud
deriving (Eq,Show)
dataSheepInfo =
SheepInfo NameAgePoundsOfWool
deriving (Eq,Show)
dataAnimal=
CowCowInfo
|PigPigInfo
|SheepSheepInfo
deriving (Eq,Show)
-- Alternately
typeAnimal' =
SumCowInfo (SumPigInfo SheepInfo )
Again in the REPL, we use FirstandSecond to pattern match
on the data constructors of Sum:
-- Getting it right
Prelude&gt; let bess' = (CowInfo &quot;Bess&quot; 4)
Prelude&gt; let bess = First bess' :: Animal'</p>
<p>CHAPTER 11. RULE THE TYPES, RULE THE UNIVERSE 648
Prelude&gt; :{
*Main| let e' =
*Main| Second (SheepInfo &quot;Elmer&quot; 5 5)
*Main| :}
Prelude&gt; let elmer = Second e' :: Animal'
-- Making a mistake
Prelude&gt; :{
*Main| let elmo' =
*Main| Second (SheepInfo &quot;Elmo&quot; 5 5)
*Main| :}
Prelude&gt; let elmo = First elmo' :: Animal'
Couldn't match expected type ‘CowInfo’
with actual type ‘Sum a0 SheepInfo’
In the first argument of ‘First’, namely
‘(Second (SheepInfo &quot;Elmo&quot; 5 5))’
In the expression:
First (Second (SheepInfo &quot;Elmo&quot; 5 5))
:: Animal'
The first data constructor, First, has the argument CowInfo ,
butSheepInfo is nested within the Second constructor (it is the
Second of the Second). We can see how they don’t match and
the mistaken attempt nests in the wrong direction.
Prelude&gt; let sheep = SheepInfo &quot;Baaaaa&quot; 5 5</p>
<p>CHAPTER 11. RULE THE TYPES, RULE THE UNIVERSE 649
Prelude&gt; :t First (Second sheep)
First (Second (SheepInfo &quot;Baaaaa&quot; 5 5))
:: Sum (Sum a SheepInfo) b
Prelude&gt; :info Animal'
type Animal' =
Sum CowInfo (Sum PigInfo SheepInfo)
-- Defined at code/animalFarm1.hs:61:1
As we said, the actual types SumandProduct themselves aren’t
used very often in standard Haskell code, but it can be useful to
develop an intuition about this structure to sum and product
types.
Constructing values
Our first datatype, GuessWhat , is trivial, equivalent to the ()unit
type:
trivialValue ::GuessWhat
trivialValue =Chickenbutt
Types like this are sometimes used to signal discrete con-
cepts that you don’t want to flatten into the unit type. We’ll
elaborate on how this can make code easier to understand or
better abstracted later. There is nothing special in the syntax</p>
<p>CHAPTER 11. RULE THE TYPES, RULE THE UNIVERSE 650
here. We define trivialValue to be the nullary data constructor
Chickenbutt and we have a value of the type GuessWhat .
Next we look at a unary type constructor that contains one
unary data constructor:
dataIda=
MkIdaderiving (Eq,Show)
Because Idhas an argument, we have to apply it to some-
thing before we can construct a value of that type:
-- note:
-- MkId :: a -&gt; Id a
idInt::IdInteger
idInt=MkId10
We turn our attention to our product type with two argu-
ments. We’re going to define some type synonyms first to
make this more readable:
typeAwesome =Bool
typeName=String
person::Product NameAwesome
person=Product &quot;Simon&quot; True</p>
<p>CHAPTER 11. RULE THE TYPES, RULE THE UNIVERSE 651
The type synonyms Awesome andNamehere are for clarity.
They don’t obligate us to change our terms. We could have
used datatypes instead of type synonyms, as we will in the sum
type example below, but this is a quick and painless way to
construct the value that we need. Notice that we’re relying on
theProduct data constructor that we defined above. The Product
data constructor is a function of two arguments, the Nameand
Awesome . Notice, also, that Simons are invariably awesome.
Now we’ll use the Sumtype defined above:
dataSuma b=
Firsta
|Secondb
deriving (Eq,Show)
dataTwitter =
Twitter deriving (Eq,Show)
dataAskFm=
AskFmderiving (Eq,Show)
socialNetwork ::SumTwitter AskFm
socialNetwork =FirstTwitter
Here our type is a sum of Twitter orAskFm. We don’t have
both values at the same time without the use of a product</p>
<p>CHAPTER 11. RULE THE TYPES, RULE THE UNIVERSE 652
because sums are a means of expressing disjunction or the
ability to have one of several possible values. We have to use
one of the data constructors generated by the definition of Sum
in order to indicate which of the possibilities in the disjunction
we mean to express. Consider the case where we mix them
up:
Prelude&gt; type SN = Sum Twitter AskFm
Prelude&gt; Second Twitter :: SN
Couldn't match expected type ‘AskFm’ with
actual type ‘Twitter’
In the first argument of ‘Second’,
namely ‘Twitter’
In the expression:
Second Twitter :: Sum Twitter AskFm
Prelude&gt; First AskFm :: Sum Twitter AskFm
Couldn't match expected type ‘Twitter’ with
actual type ‘AskFm’
In the first argument of ‘First’,
namely ‘AskFm’
In the expression:</p>
<p>CHAPTER 11. RULE THE TYPES, RULE THE UNIVERSE 653
First AskFm :: Sum Twitter AskFm
The appropriate assignment of types to specific construc-
tors is dependent on the assertions in the type. The type signa-
tureSum Twitter AskFm tells you which goes with the data con-
structor Firstand which goes with the data constructor Second .
We can assert that ordering directly by writing a datatype like
this:
dataSocialNetwork =
Twitter
|AskFm
deriving (Eq,Show)
Now the data constructors for Twitter andAskFmare direct
inhabitants of the sum type SocialNetwork , where before they
inhabited the Sumtype. Now let’s consider how this might look
with type synonyms:
typeTwitter =String
typeAskFm=String
twitter ::SumTwitter AskFm
twitter =First&quot;Twitter&quot;
askfm::SumTwitter AskFm
askfm=First&quot;AskFm&quot;</p>
<p>CHAPTER 11. RULE THE TYPES, RULE THE UNIVERSE 654
There’s a problem with the above example. The name
ofaskfmimplies we meant Second &quot;AskFm&quot; , but we messed up.
Because we used type synonyms instead of defining datatypes,
the type system didn’t catch the mistake. The typechecker has
no way of knowing we made a mistake because both values are
Strings . Try to avoid using type synonyms with unstructured
data like text or binary. Type synonyms are best used when
you want something lighter weight than newtypes but also
want your type signatures to be more explicit.
Finally, we’ll consider the product that uses record syntax:
Prelude&gt; :t RecordProduct
RecordProduct :: a
-&gt; b
-&gt; RecordProduct a b
Prelude&gt; :t Product
Product :: a -&gt; b -&gt; Product a b
The first thing to notice is that you can construct values of
products that use record syntax in a manner identical to that
of non-record products. Records are just syntax to create field
references. They don’t do much heavy lifting in Haskell, but
they are convenient:
myRecord ::RecordProduct Integer Float
myRecord =RecordProduct 420.00001</p>
<p>CHAPTER 11. RULE THE TYPES, RULE THE UNIVERSE 655
We can take advantage of the fields that we defined on our
record to construct values in a slightly diﬀerent style. This can
be convenient for making things a little more obvious:
myRecord ::RecordProduct Integer Float
myRecord =
RecordProduct { pfirst =42
, psecond =0.00001 }
This is a bit more compelling when you have domain-
specific names for things:</p>
<p>CHAPTER 11. RULE THE TYPES, RULE THE UNIVERSE 656
dataOperatingSystem =
GnuPlusLinux
|OpenBSDPlusNevermindJustBSDStill
|Mac
|Windows
deriving (Eq,Show)
dataProgLang =
Haskell
|Agda
|Idris
|PureScript
deriving (Eq,Show)
dataProgrammer =
Programmer { os::OperatingSystem
, lang::ProgLang }
deriving (Eq,Show)
Then we can construct a value from the record product
Programmer :
Prelude&gt; :t Programmer
Programmer :: OperatingSystem
-&gt; ProgLang
-&gt; Programmer</p>
<p>CHAPTER 11. RULE THE TYPES, RULE THE UNIVERSE 657
nineToFive ::Programmer
nineToFive =Programmer { os=Mac
, lang=Haskell }
-- We can reorder stuff
-- when we use record syntax
feelingWizardly ::Programmer
feelingWizardly =
Programmer { lang=Agda
, os=GnuPlusLinux }
Exercise: Programmers
Write a function that generates all possible values of Programmer .
Use the provided lists of inhabitants of OperatingSystem and
ProgLang .</p>
<p>CHAPTER 11. RULE THE TYPES, RULE THE UNIVERSE 658
allOperatingSystems ::[OperatingSystem ]
allOperatingSystems =
[GnuPlusLinux
,OpenBSDPlusNevermindJustBSDStill
,Mac
,Windows
]
allLanguages ::[ProgLang ]
allLanguages =
[Haskell,Agda,Idris,PureScript ]
allProgrammers ::[Programmer ]
allProgrammers =undefined
Programmer is a product of two types, you can determine how
many inhabitants of Programmer you have by calculating:
length allOperatingSystems
*length allLanguages
This is the essence of how product types and the number
of inhabitants relate.
There are several ways you could write a function to do
that, and some may produce a list that has duplicate values
in it. If your resulting list has duplicate values in it, you can</p>
<p>CHAPTER 11. RULE THE TYPES, RULE THE UNIVERSE 659
usenubfromData.List to remove duplicate values over your
allProgrammers value. Either way, if your result (minus any
duplicate values) equals the number returned by multiplying
those lengths together, you’ve probably got it figured out. Try
to be clever and make it work without manually typing out
the values.
Accidental bottoms from records
We’re going to reuse the previous Programmer datatype to see
what happens if we construct a value using record syntax but
forget a field:
Prelude&gt; :{
*Main| let partialAf =
*Main| Programmer {os = GnuPlusLinux}
*Main| :}
Fields of ‘Programmer’
not initialised: lang
In the expression:
Programmer {os = GnuPlusLinux}
In an equation for ‘partialAf’:
partialAf =
Programmer {os = GnuPlusLinux}
-- and if we don't heed this warning...</p>
<p>CHAPTER 11. RULE THE TYPES, RULE THE UNIVERSE 660
Prelude&gt; partialAf
Programmer {os = GnuPlusLinux, lang =
*** Exception:
Missing field in
record construction lang
Donotdo this in your code! Either define the whole record
at once or not at all. If you think you need this, your code needs
to be refactored. Partial application of the data constructor
suffices to handle this:</p>
<p>CHAPTER 11. RULE THE TYPES, RULE THE UNIVERSE 661
-- Works the same as if
-- we'd used record syntax.
dataThereYet =
ThereFloatIntBool
deriving (Eq,Show)
-- who needs a &quot;builder pattern&quot;?
notYet::Int-&gt;Bool-&gt;ThereYet
notYet=nope25.5
notQuite ::Bool-&gt;ThereYet
notQuite =notYet10
yusssss ::ThereYet
yusssss =notQuite False
-- Not I, said the Haskell user.
Notice the way our types progressed.
There::Float-&gt;Int-&gt;Bool-&gt;ThereYet
notYet:: Int-&gt;Bool-&gt;ThereYet
notQuite :: Bool-&gt;ThereYet
yusssss :: ThereYet</p>
<p>CHAPTER 11. RULE THE TYPES, RULE THE UNIVERSE 662
Percolate values through your programs, not bottoms.6
Deconstructing values
When we discussed folds, we mentioned the idea of catamor-
phism. We explained that catamorphism was about deconstruct-
inglists. This idea is generally applicable to any datatype that
has values. Now that we’ve thoroughly explored constructing
values, the time has come to destroy what we have built. Wait,
no — we mean deconstruct.
We begin, as always, with some datatypes:
6A favorite snack of the North American Yeti is bottom-propagating Haskellers.</p>
<p>CHAPTER 11. RULE THE TYPES, RULE THE UNIVERSE 663
newtype Name =NameStringderiving Show
newtype Acres =AcresIntderiving Show
-- FarmerType is a Sum
dataFarmerType =DairyFarmer
|WheatFarmer
|SoybeanFarmer
deriving Show
-- Farmer is a plain ole product of
-- Name, Acres, and FarmerType
dataFarmer=
FarmerNameAcresFarmerType
deriving Show
Now we’re going to write a very basic function that breaks
down and unpacks the data inside our constructors:
isDairyFarmer ::Farmer-&gt;Bool
isDairyFarmer (Farmer_ _DairyFarmer )=
True
isDairyFarmer _ =
False
DairyFarmer is one value of the FarmerType type that is packed
up inside our Farmer product type. But our function can pull</p>
<p>CHAPTER 11. RULE THE TYPES, RULE THE UNIVERSE 664
that value out, pattern match on it, and tell us just what we’re
looking for.
Now an alternate formulation with a product that uses
record syntax:
dataFarmerRec =
FarmerRec { name ::Name
, acres ::Acres
, farmerType ::FarmerType }
deriving Show
isDairyFarmerRec ::FarmerRec -&gt;Bool
isDairyFarmerRec farmer=
casefarmerType farmer of
DairyFarmer -&gt;True
_ -&gt; False
This is just another way of unpacking or deconstructing the
contents of a product type.
Accidental bottoms from records
We take bottoms very seriously. You can easily propagate bottoms
through record types, and we implore you not to do so. Please,
do not do this:</p>
<p>CHAPTER 11. RULE THE TYPES, RULE THE UNIVERSE 665
dataAutomobile =Null
|Car{ make::String
, model ::String
, year::Integer }
deriving (Eq,Show)
This is a terrible thing to do, for a couple of reasons. One
is thisNullnonsense. Haskell oﬀers you the perfectly lovely
datatype Maybe, which you should use instead. Secondly, con-
sider the case where one has a Nullvalue, but you’ve used one
of the record accessors:
Prelude&gt; make Null
&quot;*** Exception: No match in
record selector make
-- Don't.
How do we fix this? Well, first, whenever we have a product
that uses record accessors, keep it separate of any sum type
that is wrapping it. To do this, split out the product into an
independent type with its own type constructor instead of
only as an inline data constructor product:</p>
<p>CHAPTER 11. RULE THE TYPES, RULE THE UNIVERSE 666
-- Split out the record/product
dataCar=Car{ make::String
, model ::String
, year::Integer }
deriving (Eq,Show)
-- The Null is still not great, but
-- we're leaving it in to make a point
dataAutomobile =Null
|Automobile Car
deriving (Eq,Show)
Now if we attempt to do something silly, the type system
catches us:
Prelude&gt; make Null
Couldn't match expected type ‘Car’
with actual type ‘Automobile’
In the first argument of ‘make’,
namely ‘Null’
In the expression: make Null
In Haskell, we want the typechecker to catch us doing things
wrong, so we can fix it before problems multiply and things go</p>
<p>CHAPTER 11. RULE THE TYPES, RULE THE UNIVERSE 667
wrong at runtime. But the typechecker can best help those
who help themselves.
11.14 Function type is exponential
In the arithmetic of calculating inhabitants of types, function
type is the exponent operator. Given a function a -&gt; b , we can
calculate the inhabitants with the formula 𝑏u?.
So if𝑏and𝑎areBool, then22is how you could express the
number of inhabitants in a function of Bool -&gt; Bool . Similarly,
a function of Boolto something of 3 inhabitants would be 32
and thus have nine possible implementations.
a-&gt;b-&gt;c
(c^b)^a
-- given arithmetic laws,
-- can be rewritten as
c^(b*a)
Earlier we identified the type (Bool, Bool) as having four
inhabitants. This can be determined by either writing out all
the possible unique inhabitants or, more easily, by doing the
arithmetic of (1 + 1) * (1 + 1) . Next we’ll see that the type of
functions (-&gt;)is, in the algebra of types, the exponentiation</p>
<p>CHAPTER 11. RULE THE TYPES, RULE THE UNIVERSE 668
operator. We’ll use a datatype with three cases because Bool
has one difficulty: two plus two, two times two, and two to
the power of two all equal the same thing. Let’s review the
arithmetic of sum types:
dataQuantum =
Yes
|No
|Both
deriving (Eq,Show)
-- 3 + 3
quantSum1 ::EitherQuantum Quantum
quantSum1 =RightYes
quantSum2 ::EitherQuantum Quantum
quantSum2 =RightNo
quantSum3 ::EitherQuantum Quantum
quantSum3 =RightBoth
quantSum4 ::EitherQuantum Quantum
quantSum4 =LeftYes
-- You can fill in the next two.
And now the arithmetic of product types:</p>
<p>CHAPTER 11. RULE THE TYPES, RULE THE UNIVERSE 669
-- 3 * 3
quantProd1 ::(Quantum,Quantum)
quantProd1 =(Yes,Yes)
quantProd2 ::(Quantum,Quantum)
quantProd2 =(Yes,No)
quantProd3 ::(Quantum,Quantum)
quantProd3 =(Yes,Both)
quantProd4 ::(Quantum,Quantum)
quantProd4 =(No,Yes)
quantProd5 ::(Quantum,Quantum)
quantProd5 =(No,No)
quantProd6 ::(Quantum,Quantum)
quantProd6 =(No,Both)
quantProd7 ::(Quantum,Quantum)
quantProd7 =(Both,Yes)
-- You can determine the final two.
And now a function type. Each possible unique implemen-
tation of the function is an inhabitant:</p>
<p>CHAPTER 11. RULE THE TYPES, RULE THE UNIVERSE 670
-- 3 ^ 3
quantFlip1 ::Quantum -&gt;Quantum
quantFlip1 Yes=Yes
quantFlip1 No=Yes
quantFlip1 Both=Yes
quantFlip2 ::Quantum -&gt;Quantum
quantFlip2 Yes=Yes
quantFlip2 No=Yes
quantFlip2 Both=No
quantFlip3 ::Quantum -&gt;Quantum
quantFlip3 Yes=Yes
quantFlip3 No=Yes
quantFlip3 Both=Both</p>
<p>CHAPTER 11. RULE THE TYPES, RULE THE UNIVERSE 671
quantFlip4 ::Quantum -&gt;Quantum
quantFlip4 Yes=Yes
quantFlip4 No=No
quantFlip4 Both=Yes
quantFlip5 ::Quantum -&gt;Quantum
quantFlip5 Yes=Yes
quantFlip5 No=Both
quantFlip5 Both=Yes
quantFlip6 ::Quantum -&gt;Quantum
quantFlip6 Yes=No
quantFlip6 No=Yes
quantFlip6 Both=Yes</p>
<p>CHAPTER 11. RULE THE TYPES, RULE THE UNIVERSE 672
quantFlip7 ::Quantum -&gt;Quantum
quantFlip7 Yes=Both
quantFlip7 No=Yes
quantFlip7 Both=Yes
quantFlip8 ::Quantum -&gt;Quantum
quantFlip8 Yes=Both
quantFlip8 No=Yes
quantFlip8 Both=No
quantFlip9 ::Quantum -&gt;Quantum
quantFlip9 Yes=Both
quantFlip9 No=No
quantFlip9 Both=No
quantFlip10 ::Quantum -&gt;Quantum
quantFlip10 Yes=Both
quantFlip10 No=No
quantFlip10 Both=Both
-- You can figure out the remaining
-- possibilities yourself.</p>
<p>CHAPTER 11. RULE THE TYPES, RULE THE UNIVERSE 673
Exponentiation in what order?
Consider the following function:
convert ::Quantum -&gt;Bool
convert =undefined
According to the equality of a -&gt; b and𝑏u?there should be 23
or 8 implementations of this function. Does this hold? Write
it out and prove it for yourself.
Exercises: The Quad
Determine how many unique inhabitants each type has.
Suggestion: do the arithmetic unless you want to verify.
Writing them out gets tedious quickly.
1.dataQuad=
One
|Two
|Three
|Four
deriving (Eq,Show)
-- how many different forms can this take?
eQuad::EitherQuadQuad
eQuad= ???</p>
<p>CHAPTER 11. RULE THE TYPES, RULE THE UNIVERSE 674
2.prodQuad ::(Quad,Quad)
3.funcQuad ::Quad-&gt;Quad
4.prodTBool ::(Bool,Bool,Bool)
5.gTwo::Bool-&gt;Bool-&gt;Bool
6.Hint: 5 digit number
fTwo::Bool-&gt;Quad-&gt;Quad
11.15 Higher-kinded datatypes
You may recall we discussed kinds earlier in this chapter. Kinds
are the types of type constructors, primarily encoding the
number of arguments they take. The default kind in Haskell is
*. Kind signatures work like type signatures, using the same ::
and-&gt;syntax, but there are only a few kinds and you’ll most
often see *.
Kinds are not types until they are fully applied. Only types
have inhabitants at the term level. The kind * -&gt; * is waiting
for a single *before it is fully applied. The kind * -&gt; * -&gt; *
must be applied twice before it will be a real type. This is
known as a higher-kinded type . Lists, for example, are higher-
kinded datatypes in Haskell.
Because types can be generically polymorphic by taking
type arguments, they can be applied at the type level:</p>
<p>CHAPTER 11. RULE THE TYPES, RULE THE UNIVERSE 675
-- identical to (a, b, c, d)
dataSillya b c d =
MkSilly a b c d deriving Show
-- in GHCi
Prelude&gt;:kindSilly
Silly:: * -&gt; * -&gt; * -&gt; * -&gt; *
Prelude&gt;:kindSillyInt
SillyInt:: * -&gt; * -&gt; * -&gt; *
Prelude&gt;:kindSillyIntString
SillyIntString:: * -&gt; * -&gt; *
Prelude&gt;:kindSillyIntStringBool
SillyIntStringBool:: * -&gt; *
Prelude&gt;:kindSillyIntStringBoolString
SillyIntStringBoolString:: *
-- Identical to (a, b, c, d)
Prelude&gt;:kind (,,,)
(,,,):: * -&gt; * -&gt; * -&gt; * -&gt; *
Prelude&gt;:kind (Int,String,Bool,String)
(Int,String,Bool,String):: *</p>
<p>CHAPTER 11. RULE THE TYPES, RULE THE UNIVERSE 676
Getting comfortable with higher-kinded types is important
as type arguments provide a generic way to express a “hole”
to be filled by consumers of your datatype later. Take the
following as an example from a library one of the authors
maintains called Bloodhound.7
dataEsResultFound a=
EsResultFound { _version ::DocVersion
, _source ::a
}deriving (Eq,Show)
We know that this particular kind of response from Elastic-
search will include a DocVersion value, so that’s been assigned a
type. On the other hand, _source has type 𝑎because we have
no idea what the structure of the documents they’re pulling
from Elasticsearch look like. In practice, we do need to be able
to dosomething with that value of type 𝑎. The thing we will
want to do with it — the way we will consume or use that data
— will usually be a FromJSON typeclass instance for deserializing
JSON data into a Haskell datatype. But in Haskell, we do not
conventionally put constraints on datatypes. That is, we don’t
want to constrain that polymorphic 𝑎in the datatype. The
FromJSON typeclass will likely (assuming that’s what is needed in
7http://hackage.haskell.org/package/bloodhound If you are not a programmer and do
not know what Elasticsearch and JSON are, try not to worry too much about the specifics.
Elasticsearch is a search engine and JSON is a format for transmitting data, especially
between servers and web applications.</p>
<p>CHAPTER 11. RULE THE TYPES, RULE THE UNIVERSE 677
a given context) constrain the variable in the type signature(s)
for the function(s) that will process this data.
Accordingly, the FromJSON typeclassinstancefor EsResultFound
requires a FromJSON instance for that 𝑎:
instance (FromJSON a)=&gt;
FromJSON (EsResultFound a)where
parseJSON ( Objectv)=
EsResultFound
&lt;$&gt;v.:&quot;_version&quot;
&lt;*&gt;v.:&quot;_source&quot;
parseJSON _ =empty
As you can hopefully see from this, by not fully applying
the type — by leaving it higher-kinded — space is left for the
type of the response to vary, for the “hole” to be filled in by
the end user.
11.16 Lists are polymorphic
What makes a list polymorphic? In what way can it take many
forms? What makes them polymorphic is that lists in Haskell
can contain values of any type. You do not have an 𝑎until the
list type’s type argument has been fully applied:
data[]a=[]|a:[a]
-- [1] [2] [3] [4] [5] [6]</p>
<p>CHAPTER 11. RULE THE TYPES, RULE THE UNIVERSE 678
1.Type constructor for list has special []syntax.
2.Single type argument to []. This is the type of value our
list contains.
3.Nil / empty list value constructor, again with the special
[] syntax. [] marks the end of the list.
4.A single value of type 𝑎.
5.:is an infix data constructor. It is a product of 𝑎[4] and
[a][6]
6.The rest of our list.
Infix type and data constructors When we give an operator
a nonalphanumeric name, it is infix by default. For example,
all the nonalphanumeric arithmetic functions are infix opera-
tors, while we have some alphanumeric arithmetic functions,
such as divandmodthat are prefix by default. So far, we’ve only
seen alphanumeric data constructors, except for this cons con-
structor in the list type, but the same rule applies to them.
Any operator that starts with a colon ( :) must be an in-
fix type or data constructor. All infix data constructors must
start with a colon. The type constructor of functions, (-&gt;), is
the only infix type constructor that doesn’t start with a colon.</p>
<p>CHAPTER 11. RULE THE TYPES, RULE THE UNIVERSE 679
Another exception is that they cannot be ::as this syntax is
reserved for type assertions.
In the following example, we’ll define the list type without
using an infix constructor:
-- Same type, redefined
-- with different syntax
dataLista=Nil|Consa (Lista)
-- [1] [2] [3] [5] [4] [6]
1.TheListtype constructor.
2.The𝑎type parameter to List.
3.Nil / empty list value, which also marks the end of a list.
4.A single value of type 𝑎in theConsproduct.
5.TheConsconstructor, product of 𝑎andList a .
6.The rest of our list.
How do we use our Listtype?
Prelude&gt; let nil = Nil
Prelude&gt; :t nil
nil :: List a</p>
<p>CHAPTER 11. RULE THE TYPES, RULE THE UNIVERSE 680
Thetypeparameterisn’tappliedbecause Nilbyitselfdoesn’t
tell the type inference what the Listcontains. But if we give it
some information, then the 𝑎can be assigned a concrete type:
Prelude&gt; let oneItem = (Cons &quot;woohoo!&quot; Nil)
Prelude&gt; :t oneItem
oneItem :: List [Char]
And how are our list types kinded?
Prelude&gt; :kind List
List :: * -&gt; *
Prelude&gt; :kind []
[] :: * -&gt; *
Prelude&gt; :kind List Int
List Int :: *
Prelude&gt; :kind [Int]
[Int] :: *
Much as we can refer to the function notbefore we’ve ap-
plied its argument, we can refer to the list type constructor, [],
before we’ve applied it to a type argument:
Prelude&gt; :t not
not :: Bool -&gt; Bool</p>
<p>CHAPTER 11. RULE THE TYPES, RULE THE UNIVERSE 681
Prelude&gt; :t not True
not True :: Bool
Prelude&gt; :k []
[] :: * -&gt; *
Prelude&gt; :k [Int]
[Int] :: *
The diﬀerence is that the argument of notis any value of
typeBool, and the argument of []is any type of kind *. So,
they’re similar, but type constructors are functions one level
up, structuring things that cannot exist at runtime — it’s purely
static and describes the structure of your types.
11.17 Binary Tree
Now we turn our attention to a type similar to list. The type
constructor for binary trees can take an argument, and it is
also recursive like lists:
dataBinaryTree a=
Leaf
|Node(BinaryTree a) a (BinaryTree a)
deriving (Eq,Ord,Show)
This tree has a value of type 𝑎at each node. Each node
could be a terminal node, called a leaf, or it could branch and</p>
<p>CHAPTER 11. RULE THE TYPES, RULE THE UNIVERSE 682
have two subtrees. The subtrees are also of type BinaryTree a ,
so this type is recursive. Each binary tree can store yet another
binary tree, which allows for trees of arbitrary depth.
In some cases, binary trees can be more efficient for struc-
turing and accessing data than a list, especially if you know
how to order your values in a way that lets you know whether
to look “left” or “right” to find what you want. On the other
hand, a tree that only branches to the right is indistinguishable
from an ordinary list. For now, we won’t concern ourselves
too much with this as we’ll talk about the proper application
of data structures later. Instead, you’re going to write some
functions for processing BinaryTree values.
Inserting into trees
The first thing to be aware of is that we need Ordin order to have
enough information about our values to know how to arrange
them in our tree. Accordingly, if something is lower, we want
to insert it somewhere on the left-hand part of our tree. If it’s
greater than the current node value, it should go somewhere
to the right. Left lesser, right greater is a common convention
for arranging binary trees — it could be the opposite and not
really change anything, but this matches our usual intuitions
of ordering as we do with, say, number lines. The point is you
want to be able to know where to look in the tree for values
greater or less than the current one you’re looking at.</p>
<p>CHAPTER 11. RULE THE TYPES, RULE THE UNIVERSE 683
Ourinsert function will insert a value into a tree or, if no
tree exists yet, give us a means of building a tree by inserting
values. It’s important to remember that data is immutable in
Haskell. We do not insert a value into an existing tree; each
time we want to insert a value into the data structure, we build
a whole new tree:
insert' ::Orda
=&gt;a
-&gt;BinaryTree a
-&gt;BinaryTree a
insert' bLeaf=NodeLeafbLeaf
insert' b (Nodeleft a right)
|b==a=Nodeleft a right
|b&lt;a=Node(insert' b left) a right
|b&gt;a=Nodeleft a (insert' b right)
The base case in our insert' function serves a couple pur-
poses. It handles inserting into an empty tree ( Leaf) and begin-
ning the construction of a new tree and also the case of having
reached the bottom of a much larger tree. The simplicity here
lets us ignore any inessential diﬀerences between those two
cases.
-- Leaf being the &quot;empty tree&quot; case</p>
<p>CHAPTER 11. RULE THE TYPES, RULE THE UNIVERSE 684
Prelude&gt; let t1 = insert' 0 Leaf
Prelude&gt; t1
Node Leaf 0 Leaf
Prelude&gt; let t2 = insert' 3 t1
Prelude&gt; t2
Node Leaf 0 (Node Leaf 3 Leaf)
Prelude&gt; let t3 = insert' 5 t2
Prelude&gt; t3
Node Leaf 0
(Node Leaf 3
(Node Leaf 5 Leaf))
We will examine binary trees and their properties later in
the book. For now, we want to focus not on the properties
of binary trees themselves, but on the structure of their type.
You might find the following exercises tricky or tedious, but
they will deepen your intuition for how recursive types work.
Write map for BinaryTree
Given the definition of BinaryTree above, write a map function
for the data structure. You don’t really need to know anything
about binary trees to write these functions. The structure
inherent in the definition of the type is all you need. All you
need to do is write the recursive functions.</p>
<p>CHAPTER 11. RULE THE TYPES, RULE THE UNIVERSE 685
No special algorithms are needed, and we don’t expect you
to keep the tree balanced or ordered. Also, remember that
we’ve never once mutated anything. We’ve only built new
values from input data. Given that, when you go to implement
mapTree , you’re not changing an existing tree — you’re building
a new one based on an existing one (as when you are mapping
functions over lists).
Note, you do notneed to use insert' for this. Retain the
original structure of the tree.</p>
<p>CHAPTER 11. RULE THE TYPES, RULE THE UNIVERSE 686
mapTree ::(a-&gt;b)
-&gt;BinaryTree a
-&gt;BinaryTree b
mapTree _Leaf=Leaf
mapTree f (Nodeleft a right) =
Nodeundefined undefined undefined
testTree' ::BinaryTree Integer
testTree' =
Node(NodeLeaf3Leaf)
1
(NodeLeaf4Leaf)
mapExpected =
Node(NodeLeaf4Leaf)
2
(NodeLeaf5Leaf)
-- acceptance test for mapTree
mapOkay =
ifmapTree ( +1) testTree' ==mapExpected
thenprint&quot;yup okay!&quot;
elseerror&quot;test failed!&quot;
Some hints for implementing mapTree follow.</p>
<p>CHAPTER 11. RULE THE TYPES, RULE THE UNIVERSE 687
The first pattern match in our mapTree function is the base
case, where we have a Leafvalue. We can’t apply the 𝑓there
because we don’t have an 𝑎, so we ignored it. Since we have
to return a value of type BinaryTree b whatever happens, we
return a Leafvalue.
We return a Nodein the second pattern match of our mapTree
function. Note that the Nodedata constructor takes three argu-
ments:
Prelude&gt; :t Node
Node :: BinaryTree a
-&gt; a
-&gt; BinaryTree a
-&gt; BinaryTree a
So you need to pass it more BinaryTree , a single value, and
moreBinaryTree . You have the following terms available to
you:
1.f::(a-&gt;b)
2.left::BinaryTree a
3.a::a
4.right::BinaryTree a</p>
<p>CHAPTER 11. RULE THE TYPES, RULE THE UNIVERSE 688
5.mapTree ::(a-&gt;b)
-&gt;BinaryTree a
-&gt;BinaryTree b
Now the Nodereturn needs to have a value of type 𝑏and
BinaryTree values with type 𝑏inside them. You have two func-
tions at your disposal. One gets you (a -&gt; b) , the other maps
BinaryTree s of type 𝑎intoBinaryTree s of type 𝑏. Get ’em tiger.
A few suggestions that might help you with this exercise.
1.Split out the patterns your function should match on first.
2.Implement the base case first.
3.Try manually writing out the steps of recursion at first,
then collapse them into a single step that is recursive.
Convert binary trees to lists
Write functions to convert BinaryTree values to lists. Make
certain your implementation passes the tests.</p>
<p>CHAPTER 11. RULE THE TYPES, RULE THE UNIVERSE 689
preorder ::BinaryTree a-&gt;[a]
preorder =undefined
inorder ::BinaryTree a-&gt;[a]
inorder =undefined
postorder ::BinaryTree a-&gt;[a]
postorder =undefined
testTree ::BinaryTree Integer
testTree =
Node(NodeLeaf1Leaf)
2
(NodeLeaf3Leaf)
testPreorder ::IO()
testPreorder =
ifpreorder testTree ==[2,1,3]
thenputStrLn &quot;Preorder fine!&quot;
elseputStrLn &quot;Bad news bears.&quot;
testInorder ::IO()
testInorder =
ifinorder testTree ==[1,2,3]
thenputStrLn &quot;Inorder fine!&quot;
elseputStrLn &quot;Bad news bears.&quot;
testPostorder ::IO()
testPostorder =
ifpostorder testTree ==[1,3,2]
thenputStrLn &quot;Postorder fine!&quot;
elseputStrLn &quot;postorder failed check&quot;
main::IO()
main= do
testPreorder
testInorder
testPostorder</p>
<p>CHAPTER 11. RULE THE TYPES, RULE THE UNIVERSE 690
Write foldr for BinaryTree
Given the definition of BinaryTree we have provided, write a
catamorphism for the binary trees.
-- any traversal order is fine
foldTree ::(a-&gt;b-&gt;b)
-&gt;b
-&gt;BinaryTree a
-&gt;b
11.18 Chapter Exercises
Multiple choice
1.Given the following datatype:
dataWeekday =
Monday
|Tuesday
|Wednesday
|Thursday
|Friday
we can say:
a)Weekday is a type with five data constructors</p>
<p>CHAPTER 11. RULE THE TYPES, RULE THE UNIVERSE 691
b)Weekday is a tree with five branches
c)Weekday is a product type
d)Weekday takes five arguments
2.and with the same datatype definition in mind, what is
the type of the following function, f?
fFriday=&quot;Miller Time&quot;
a)f :: [Char]
b)f :: String -&gt; String
c)f :: Weekday -&gt; String
d)f :: Day -&gt; Beer
3.Types defined with the datakeyword
a)must have at least one argument
b)must begin with a capital letter
c)must be polymorphic
d)cannot be imported from modules
4.The function g xs = xs !! (length xs - 1)
a)is recursive and may not terminate
b)delivers the head of xs
c)delivers the final element of xs
d)has the same type as xs</p>
<p>CHAPTER 11. RULE THE TYPES, RULE THE UNIVERSE 692
Ciphers
In the Lists chapter, you wrote a Caesar cipher. Now, we want
to expand on that idea by writing a Vigenère cipher. A Vi-
genère cipher is another substitution cipher, based on a Caesar
cipher, but it uses a series of Caesar ciphers for polyalphabetic
substitution. The substitution for each letter in the plaintext
is determined by a fixed keyword.
So, for example, if you want to encode the message “meet
at dawn,” the first step is to pick a keyword that will determine
which Caesar cipher to use. We’ll use the keyword “ALLY”
here. You repeat the keyword for as many characters as there
are in your original message:
MEET AT DAWN
ALLY AL LYAL
Now the number of rightward shifts to make to encode each
character is set by the character of the keyword that lines up
with it. The ’A’ means a shift of 0, so the initial M will remain
M. But the ’L’ for our second character sets a rightward shift
of 11, so ’E’ becomes ’P’. And so on, so “meet at dawn” encoded
with the keyword “ALLY” becomes “MPPR AE OYWY.”
Like the Caesar cipher, you can find all kinds of resources to
help you understand the cipher and also many examples writ-
ten in Haskell. Consider using a combination of chr,ord, and</p>
<p>CHAPTER 11. RULE THE TYPES, RULE THE UNIVERSE 693
modagain, possibly very similar to what you used for writing
the original Caesar cipher.
As-patterns
As-patterns in Haskell are a nifty way to be able to pattern match
on part of something and still refer to the entire original value.
Some examples:
f::Showa=&gt;(a, b)-&gt;IO(a, b)
ft@(a,_)= do
print a
return t
Here we pattern-matched on a tuple so we could get at the
first value for printing, but used the @symbol to introduce a
binding named 𝑡in order to refer to the whole tuple rather
than just a part.
Prelude&gt; f (1, 2)
1
(1,2)
We can use as-patterns with pattern matching on arbitrary
data constructors, which includes lists:</p>
<p>CHAPTER 11. RULE THE TYPES, RULE THE UNIVERSE 694
doubleUp ::[a]-&gt;[a]
doubleUp []=[]
doubleUp xs@(x:_)=x:xs
Prelude&gt; doubleUp []
[]
Prelude&gt; doubleUp [1]
[1,1]
Prelude&gt; doubleUp [1, 2]
[1,1,2]
Prelude&gt; doubleUp [1, 2, 3]
[1,1,2,3]
Use as-patterns in implementing the following functions:
1.This should return Trueif (and only if) all the values in
the first list appear in the second list, though they need
not be contiguous.
isSubseqOf ::(Eqa)
=&gt;[a]
-&gt;[a]
-&gt;Bool
The following are examples of how this function should
work:</p>
<p>CHAPTER 11. RULE THE TYPES, RULE THE UNIVERSE 695
Prelude&gt; isSubseqOf &quot;blah&quot; &quot;blahwoot&quot;
True
Prelude&gt; isSubseqOf &quot;blah&quot; &quot;wootblah&quot;
True
Prelude&gt; isSubseqOf &quot;blah&quot; &quot;wboloath&quot;
True
Prelude&gt; isSubseqOf &quot;blah&quot; &quot;wootbla&quot;
False
Prelude&gt; isSubseqOf &quot;blah&quot; &quot;halbwoot&quot;
False
Prelude&gt; isSubseqOf &quot;blah&quot; &quot;blawhoot&quot;
True
Remember that the sub-sequence has to be in the original
order!
2.Split a sentence into words, then tuple each word with the
capitalized form of each.
capitalizeWords ::String
-&gt;[(String,String)]
Prelude&gt; capitalizeWords &quot;hello world&quot;
[(&quot;hello&quot;, &quot;Hello&quot;), (&quot;world&quot;, &quot;World&quot;)]
Language exercises
1.Write a function that capitalizes a word.</p>
<p>CHAPTER 11. RULE THE TYPES, RULE THE UNIVERSE 696
capitalizeWord ::String-&gt;String
capitalizeWord =undefined
Example output.
Prelude&gt; capitalizeWord &quot;Chortle&quot;
&quot;Chortle&quot;
Prelude&gt; capitalizeWord &quot;chortle&quot;
&quot;Chortle&quot;
2.Write a function that capitalizes sentences in a paragraph.
Recognize when a new sentence has begun by checking
for periods. Reuse the capitalizeWord function.
capitalizeParagraph ::String-&gt;String
capitalizeParagraph =undefined
Example result you should get from your function:
Prelude&gt; let s = &quot;blah. woot ha.&quot;
Prelude&gt; capitalizeParagraph s
&quot;Blah. Woot ha.&quot;
Phone exercise
This exercise by geophf8originally for 1HaskellADay.9Thank
you for letting us use this exercise!
8https://twitter.com/geophf
9https://twitter.com/1haskelladay</p>
<h2>CHAPTER 11. RULE THE TYPES, RULE THE UNIVERSE 697
Remember old-fashioned phone inputs for writing text
where you had to press a button multiple times to get diﬀerent
letters to come up? You may still have to do this when you try
to search for a movie to watch using your television remote
control. You’re going to write code to translate sequences of
button presses into strings and vice versa.
So! Here is the layout of the phone:</h2>
<p>| 1 | 2 ABC | 3 DEF |</p>
<hr />
<h2 id="-4-ghi--5-jkl--6-mno-"><a class="header" href="#-4-ghi--5-jkl--6-mno-">| 4 GHI | 5 JKL | 6 MNO |</a></h2>
<h2 id="-7-pqrs--8-tuv--9-wxyz-"><a class="header" href="#-7-pqrs--8-tuv--9-wxyz-">| 7 PQRS | 8 TUV | 9 WXYZ |</a></h2>
<h2 id="----0--_----"><a class="header" href="#----0--_----">| * ^ | 0 + _ | # ., |</a></h2>
<p>Where star (*) gives you capitalization of the letter you’re
writing to your friends, and 0 is your space bar. To represent
the digit itself, you press that digit once more than the letters it
represents. If you press a button one more than is required to
type the digit, it wraps around to the first letter. For example,
2 -&gt; 'A'
22 -&gt; 'B'
222 -&gt; 'C'</p>
<p>CHAPTER 11. RULE THE TYPES, RULE THE UNIVERSE 698
2222 -&gt; '2'
22222 -&gt; 'A'
So on and so forth. We’re going to kick this around.
1.Create a data structure that captures the phone layout
above. Thedatastructureshouldbeabletoexpressenough
of how the layout works that you can use it to dictate the
behavior of the functions in the following exercises.
-- fill in the rest.
dataDaPhone =DaPhone
2.Convert the following conversations into the keypresses
required to express them. We’re going to suggest types
and functions to fill in order to accomplish the goal, but
they’re not obligatory. If you want to do it diﬀerently, go
right ahead.</p>
<p>CHAPTER 11. RULE THE TYPES, RULE THE UNIVERSE 699
convo::[String]
convo=
[&quot;Wanna play 20 questions&quot; ,
&quot;Ya&quot;,
&quot;U 1st haha&quot; ,
&quot;Lol ok. Have u ever tasted alcohol&quot; ,
&quot;Lol ya&quot; ,
&quot;Wow ur cool haha. Ur turn&quot; ,
&quot;Ok. Do u think I am pretty Lol&quot; ,
&quot;Lol ya&quot; ,
&quot;Just making sure rofl ur turn&quot; ]</p>
<p>CHAPTER 11. RULE THE TYPES, RULE THE UNIVERSE 700
-- validButtons = &quot;1234567890*#&quot;
typeDigit=Char
-- Valid presses: 1 and up
typePresses =Int
reverseTaps ::DaPhone
-&gt;Char
-&gt;[(Digit,Presses)]
reverseTaps =undefined
-- assuming the default phone definition
-- 'a' -&gt; [('2', 1)]
-- 'A' -&gt; [('*', 1), ('2', 1)]
cellPhonesDead ::DaPhone
-&gt;String
-&gt;[(Digit,Presses)]
cellPhonesDead =undefined
3.How many times do digits need to be pressed for each
message?
fingerTaps ::[(Digit,Presses)]-&gt;Presses
fingerTaps =undefined
4.What was the most popular letter for each message? What</p>
<p>CHAPTER 11. RULE THE TYPES, RULE THE UNIVERSE 701
wasitscost? You’llwanttocombine reverseTaps andfingerTaps
to figure out what it cost in taps. reverseTaps is a list be-
cause you need to press a diﬀerent button in order to get
capitals.
mostPopularLetter ::String-&gt;Char
mostPopularLetter =undefined
5.What was the most popular letter overall? What was the
most popular word?
coolestLtr ::[String]-&gt;Char
coolestLtr =undefined
coolestWord ::[String]-&gt;String
coolestWord =undefined
Hutton’s Razor
Hutton’s Razor10is a very simple expression language that
expresses integer literals and addition of values in that expres-
sion language. The “trick” to it is that it’s recursive and the
two expressions you’re summing together could be literals or
themselves further addition operations. This sort of datatype
is stereotypical of expression languages used to motivate ideas
in research papers and functional pearls. Evaluating or folding</p>
<p>CHAPTER 11. RULE THE TYPES, RULE THE UNIVERSE 702
a datatype is also in some sense what you’re doing most of the
time while programming anyway.
1.Your first task is to write the “eval” function which reduces
an expression to a final sum.
dataExpr
=LitInteger
|AddExprExpr
eval::Expr-&gt;Integer
eval=error&quot;do it to it&quot;
Example of expected output:
Prelude&gt; eval (Add (Lit 1) (Lit 9001))
9002
2.Write a printer for the expressions.
printExpr ::Expr-&gt;String
printExpr =undefined
Expected output:
10http://www.cs.nott.ac.uk/~pszgmh/bib.html#semantics</p>
<p>CHAPTER 11. RULE THE TYPES, RULE THE UNIVERSE 703
Prelude&gt; printExpr (Add (Lit 1) (Lit 9001))
&quot;1 + 9001&quot;
Prelude&gt; let a1 = Add (Lit 9001) (Lit 1)
Prelude&gt; let a2 = Add a1 (Lit 20001)
Prelude&gt; let a3 = Add (Lit 1) a2
Prelude&gt; printExpr a3
&quot;1 + 9001 + 1 + 20001&quot;
11.19 Definitions
1.Adatatype is how we declare and create data for our func-
tions to receive as inputs. Datatype declarations begin
with the keyword data. A datatype is made up of a type
constructor and zero or more data constructors which
each have zero or more arguments.</p>
<p>Chapter 12
Signaling adversity
Thank goodness we don’t
have only serious
problems, but ridiculous
ones as well
Edsger W. Dijkstra
704</p>
<p>CHAPTER 12. SIGNALING ADVERSITY 705
12.1 Signaling adversity
Sometimes it’s not convenient or possible for every value
in a datatype to make sense for your programs. When that
happens in Haskell, we use explicit datatypes to signal when
our functions receiveda combination of inputs that don’t make
sense. Later, we’ll see how to defend against those adverse
inputs at the time we construct our datatypes, but the Maybe
andEither datatypes we will demonstrate here are common.
This chapter will include:
•Nothing , orJust Maybe ;
•Either left or right, but not both;
•higher-kindedness;
•anamorphisms, but not animorphs.
12.2 How I learned to stop worrying
and love Nothing
Let’s consider the definition of Maybeagain:
dataMaybea=Nothing |Justa
You don’t need to define this yourself, as it’s included in the
Prelude by default. It’s also a very common datatype in Haskell</p>
<p>CHAPTER 12. SIGNALING ADVERSITY 706
because it lets us return a default Nothing value when we don’t
have any sensible values to return for our intended type 𝑎.
In the following intentionally simplistic function, we could
do several things with the odd numbers — we could return
them unmodified, we could modify them in some way dif-
ferent from the evens, we could return a zero, or we could
write an explicit signal that nothing happened because the
number wasn’t even:
ifEvenAdd2 ::Integer -&gt;Integer
ifEvenAdd2 n=
ifeven nthenn+2else ???
What can we do to make it say, “hey, this number wasn’t
even so I have nothing for you, my friend?” Instead of promis-
ing anInteger result, we can return Maybe Integer :
ifEvenAdd2 ::Integer -&gt;MaybeInteger
ifEvenAdd2 n=
ifeven nthenn+2elseNothing
This isn’t quite complete or correct either. While Nothing
has the type Maybe a , and𝑎can be assumed to be any type the
Maybeconstructor could contain, n+2is still of the type Integer .
We need to wrap that value in the other constructor Maybe
provides: Just. Here’s the error you’d get if you tried to load
it:</p>
<p>CHAPTER 12. SIGNALING ADVERSITY 707
<interactive>:9:75:
Couldn't match expected type
‘Maybe Integer’
with actual type ‘Integer’
In the first argument of ‘(+)’, namely ‘n’
In the expression: n + 2
And here’s how we fix it:
ifEvenAdd2 ::Integer -&gt;MaybeInteger
ifEvenAdd2 n=
ifeven nthenJust(n+2)elseNothing
We had to parenthesize n+2because function application
binds the most tightly in Haskell (has the highest precedence),
so the compiler otherwise would’ve parsed it as (Just n) + 2 ,
which is wrong and throws a type error. Now our function
is correct and explicit about the possibility of not getting a
result!
Smart constructors for datatypes
Let’s consider a Person type which keeps track of two things,
their name and their age. We’ll write this up as a simple prod-
uct type (note that NameandAgeare type aliases):</p>
<p>CHAPTER 12. SIGNALING ADVERSITY 708
typeName=String
typeAge=Integer
dataPerson=PersonNameAgederiving Show
There are already a few problems here. One is that we could
construct a Person with an empty String for a name or make
a person who is negative years old. This is no problem to fix
withMaybe, though:
typeName=String
typeAge=Integer
dataPerson=PersonNameAgederiving Show
mkPerson ::Name-&gt;Age-&gt;MaybePerson
mkPerson name age
|name/=&quot;&quot;&amp;&amp;age&gt;=0=
Just$Personname age
|otherwise =Nothing
And if you load this into your REPL:
Prelude&gt; mkPerson &quot;John Browning&quot; 160
Just (Person &quot;John Browning&quot; 160)
Cool. What happens when we feed it adverse data?</p>
<p>CHAPTER 12. SIGNALING ADVERSITY 709
Prelude&gt; mkPerson &quot;&quot; 160
Nothing
Prelude&gt; mkPerson &quot;blah&quot; 0
Just (Person &quot;blah&quot; 0)
Prelude&gt; mkPerson &quot;blah&quot; (-9001)
Nothing
mkPerson is what we call a smart constructor . It allows us to
construct values of a type only when they meet certain criteria,
so that we know we have a valid value, and return an explicit
signal when we do not.
This is much better than our original, but what if we want
to know if it was the name, age, or both that was bad? We may
want to tell our user something was wrong with their input.
Fortunately, we have a datatype for that!
12.3 Bleating either
We want a way to express why we didn’t get a successful result
back from our mkPerson constructor. To handle that, we’ve got
theEither datatype which is defined as follows in the Prelude :
dataEithera b=Lefta|Rightb
What we want is a way to know whyour inputs were incor-
rectifthey were incorrect. So we’ll start by making a sum type
to enumerate our failure modes:</p>
<p>CHAPTER 12. SIGNALING ADVERSITY 710
dataPersonInvalid =NameEmpty
|AgeTooLow
deriving (Eq,Show)
By now, you know why we derived Show, but it’s important
that we derive Eqbecause otherwise we can’t equality check the
constructors. Pattern matching is a case expression, where the
data constructor is the condition. Case expressions and pattern
matching will work without an Eqinstance, but guards using
(==)will not. As we’ve shown you previously, you can write
your own Eqinstance for your datatype if you want a specific
behavior, but it’s usually not necessary to do, so we will usually
derive the Eqinstance. Here’s the diﬀerence demonstrated in
code:
moduleEqCaseGuard where
dataPersonInvalid =NameEmpty
|AgeTooLow
-- Compiles without Eq
toString ::PersonInvalid -&gt;String
toString NameEmpty =&quot;NameEmpty&quot;
toString AgeTooLow =&quot;AgeTooLow&quot;</p>
<p>CHAPTER 12. SIGNALING ADVERSITY 711
instance ShowPersonInvalid where
show=toString
-- This does not work without an
-- Eq instance
blah::PersonInvalid -&gt;String
blahpi
|pi==NameEmpty =&quot;NameEmpty&quot;
|pi==AgeTooLow =&quot;AgeTooLow&quot;
|otherwise =&quot;???&quot;
It’s worth considering that if you needed to have an Eqin-
stance to pattern match, how would you write the Eqinstances?
Next our constructor type is going to change to:
mkPerson ::Name
-&gt;Age
-&gt;EitherPersonInvalid Person
This signifies that we’re going to get a Person value if we suc-
ceed but a PersonInvalid if it fails. Now we need to change our
logic to return PersonInvalid values inside a Leftconstructor
when the data is invalid, discriminating by each case as we go:</p>
<p>CHAPTER 12. SIGNALING ADVERSITY 712
typeName=String
typeAge=Integer
dataPerson=PersonNameAgederiving Show
dataPersonInvalid =NameEmpty
|AgeTooLow
deriving (Eq,Show)
mkPerson ::Name
-&gt;Age
-&gt;EitherPersonInvalid Person
-- [1] [2] [3]
mkPerson name age
|name/=&quot;&quot;&amp;&amp;age&gt;=0=
Right$Personname age
-- [4]
|name==&quot;&quot;=LeftNameEmpty
-- [5]
|otherwise =LeftAgeTooLow
1.OurmkPerson type takes a NameandAgereturns an Either
result.</p>
<p>CHAPTER 12. SIGNALING ADVERSITY 713
2.TheLeftresult of the Either is an invalid person, when
either the name or age is an invalid input.
3.TheRightresult is a valid person.
4.The first case of our mkPerson function, then, matches on
theRightconstructor of the Either and returns a Person
result. We could have written
name/=&quot;&quot;&amp;&amp;age&gt;=0=
Right(Personname age)
instead of using the dollar sign.
5.The next two cases match on the Leftconstructor and
allow us to tailor our invalid results based on the failure
reasons. We can pattern match on Leftbecause it’s one of
the constructors of Either .
We use Leftas our invalid or error constructor for a couple
of reasons. It is conventional to do so in Haskell, but that con-
vention came about for a reason. The reason has to do with the
ordering of type arguments and application of functions. Nor-
mally it is your error or invalid result that is going to cause a
stop to whatever work is being done by your program. Functor
will not map over the left type argument because it has been
applied away. You may remember Functor from our introduc-
tion offmapback in the chapter about lists; don’t worry, a full</p>
<p>CHAPTER 12. SIGNALING ADVERSITY 714
explanation of Functor is coming soon. Since you normally
want to apply functions and map over the case that doesn’t
stop your program (that is, notthe error case), it has become
convention that the LeftofEither is used for whatever case is
going to cause the work to stop.
Let’s see what it looks like when we have good data, although
Djali isn’t a person.1
Prelude&gt; :t mkPerson &quot;Djali&quot; 5
mkPerson &quot;Djali&quot; 5 :: Either PersonInvalid Person
Prelude&gt; mkPerson &quot;Djali&quot; 5
Right (Person &quot;Djali&quot; 5)
Then we can see what this does for us when dealing with
bad data:
Prelude&gt; mkPerson &quot;&quot; 10
Left NameEmpty
Prelude&gt; mkPerson &quot;Djali&quot; (-1)
Left AgeTooLow
Prelude&gt; mkPerson &quot;&quot; (-1)
Left NameEmpty
Notice in the last example that when both the name and
the age are wrong, we’re only going to see the result of the first
failure case, not both.
1Don’t know what we mean? Check the name Djali on a search engine.</p>
<p>CHAPTER 12. SIGNALING ADVERSITY 715
This is imperfect in one respect, as it doesn’t let us express a
list of errors. We can fix this, too! One thing that will change is
that instead of validating all the data for a Person at once, we’re
going to make separate checking functions and then combine
the results. We’ll see means of abstracting patterns like this
out later. We’re adding a type alias that wasn’t in our previous
version; otherwise, these types are the same as above:
typeName=String
typeAge=Integer
typeValidatePerson a=
Either[PersonInvalid ] a
dataPerson=PersonNameAgederiving Show
dataPersonInvalid =NameEmpty
|AgeTooLow
deriving (Eq,Show)
Now we’ll write our checking functions. Although more
than one thing could hypothetically be wrong with the age
value, we’ll keep this simple and only check to make sure it’s a
positive Integer value:</p>
<p>CHAPTER 12. SIGNALING ADVERSITY 716
ageOkay ::Age
-&gt;Either[PersonInvalid ]Age
ageOkay age= caseage&gt;=0of
True-&gt;Rightage
False-&gt;Left[AgeTooLow ]
nameOkay ::Name
-&gt;Either[PersonInvalid ]Name
nameOkay name= casename/=&quot;&quot;of
True-&gt;Rightname
False-&gt;Left[NameEmpty ]
We can nest the PersonInvalid sum type right into the Left
position of Either, just as we saw in the previous chapter (al-
though we weren’t using Either there, but similar types).
A couple of things to note here:
•TheNamevalue will only return this invalid result when
it’s an empty String .
•SinceNameis only a String value, it can be any String with
characters inside it, so “42” is still going to be returned as
a valid name. Try it.
•If you try to put an Integer in for the name, you won’t get
aLeftresult, you’ll get a type error. Try it. You’ll get a</p>
<p>CHAPTER 12. SIGNALING ADVERSITY 717
similar result if you try to feed a string value to the ageOkay
function.
•We’re going to return a list of PersonInvalid results. That
will allow us to return bothNameEmpty andAgeTooLow in cases
where both of those are true.
Now that our functions rely on Either to validate that the
age and name values are independently valid, we can write a
mkPerson function that will use our type alias ValidatePerson :</p>
<p>CHAPTER 12. SIGNALING ADVERSITY 718
mkPerson ::Name
-&gt;Age
-&gt;ValidatePerson Person
-- [1] [2]
mkPerson name age =
mkPerson' (nameOkay name) (ageOkay age)
-- [3] [4] [5]
mkPerson' ::ValidatePerson Name
-&gt;ValidatePerson Age
-&gt;ValidatePerson Person
-- [6]
mkPerson' (RightnameOk) ( RightageOk)=
Right(PersonnameOk ageOk)
mkPerson' (LeftbadName) ( LeftbadAge) =
Left(badName ++badAge)
mkPerson' (LeftbadName) _ =LeftbadName
mkPerson' _(LeftbadAge) =LeftbadAge
1.A type alias for Either [PersonInvalid] a .
2.This is the 𝑎argument to ValidatePerson type.
3.Ourmainfunctionnowreliesonasimilarly-namedhelper</p>
<p>CHAPTER 12. SIGNALING ADVERSITY 719
function.
4.First argument to this function is the result of the nameOkay
function.
5.Second argument is the result of the ageOkay function.
6.The type relies on the synonym for Either .
The rest of our helper function mkPerson' consists of plain
old pattern matches.
Now let’s see what we get:
Prelude&gt; mkPerson &quot;&quot; (-1)
Left [NameEmpty,AgeTooLow]
Ahh, that’s more like it. Now we can tell the user what was
incorrect in one go without them having to round-trip each
mistake! Later in the book, we’ll be able to replace mkPerson
andmkPerson' with the following:
mkPerson
::Name
-&gt;Age
-&gt;Validation [PersonInvalid ]Person
mkPerson name age =
liftA2
Person(nameOkay name) (ageOkay age)</p>
<p>CHAPTER 12. SIGNALING ADVERSITY 720
12.4 Kinds, a thousand stars in your
types
Kinds are types one level up. They are used to describe the
types of type constructors. One noteworthy feature of Haskell
is that it has higher-kinded types . Here the term ‘higher-kinded’
derives from higher-order functions, functions that take more
functions as arguments. Type constructors (that is, higher-
kinded types) are types that take more types as arguments. The
Haskell Report uses the term type constant to refer to types that
take no arguments and are already types. In the Report, type
constructor is used to refer to types which must have arguments
applied to become a type.
As we discussed in the last chapter, these are examples of
type constants :
Prelude&gt; :kind Int
Int :: *
Prelude&gt; :k Bool
Bool :: *
Prelude&gt; :k Char
Char :: *
The::syntax usually means “has type of,” but it is used for
kind signatures as well as type signatures.</p>
<p>CHAPTER 12. SIGNALING ADVERSITY 721
The following is an example of a type that has a type con-
structor rather than a type constant :
dataExample a=Blah|RoofGoats |Woota
Example is a type constructor rather than a constant because
it takes a type argument 𝑎which is used with the Wootdata
constructor. In GHCi we can query kinds with :k:
Prelude&gt; data Example a = Blah | RoofGoats | Woot a
Prelude&gt; :k Example
Example :: * -&gt; *
Example has one parameter, so it must be applied to one type
in order to become a concrete type represented by a single *.
The two-tuple takes two arguments, so it must be applied to
two types to become a concrete type:
Prelude&gt; :k (,)
(,) :: * -&gt; * -&gt; *
Prelude&gt; :k (Int, Int)
(Int, Int) :: *
TheMaybeandEither datatypes we’ve just reviewed also have
type constructors rather than constants. They have to be ap-
plied to an argument before they become concrete types. As
with the eﬀect of currying in type signatures, applying Maybe</p>
<p>CHAPTER 12. SIGNALING ADVERSITY 722
to an𝑎type constructor relieves us of one arrow and makes it
a kind star:
Prelude&gt; :k Maybe
Maybe :: * -&gt; *
Prelude&gt; :k Maybe Int
Maybe Int :: *
On the other hand, Either has to be applied to two argu-
ments, an 𝑎and a𝑏, so the kind of Either is star to star to star:
Prelude&gt; :k Either
Either :: * -&gt; * -&gt; *
And, again, we can query the eﬀects of applying it to argu-
ments:
Prelude&gt; :k Either Int
Either Int :: * -&gt; *
Prelude&gt; :k Either Int String
Either Int String :: *
As we’ve said, the kind *represents a concrete type. There
is nothing left awaiting application.</p>
<p>CHAPTER 12. SIGNALING ADVERSITY 723
Lifted and unlifted types To be precise, kind <em>is the kind of
all standard lifted types, while types that have the kind #are
unlifted. A lifted type, which includes any datatype you could
define yourself, is any that can be inhabited by bottom . Lifted
types are represented by a pointer and include most of the
datatypes we’ve seen and most that you’re likely to encounter
and use. Unlifted types are any type which cannot be inhabited
by bottom. Types of kind #are often native machine types
and raw pointers. Newtypes are a special case in that they are
kind</em>, but are unlifted because their representation is identical
to that of the type they contain, so the newtype itself is not
creating any new pointer beyond that of the type it contains.
That fact means that the newtype itself cannot be inhabited
by bottom, only the thing it contains can be, so newtypes are
unlifted. The default kind of concrete, fully-applied datatypes
in GHC is kind *.
Now what happens if we let our type constructor take an
argument?
Prelude&gt; data Identity a = Identity a
Prelude&gt; :k Identity
Identity :: * -&gt; *
As we discussed in the previous chapter, the arrow in the
kind signature, like the function arrow in type signatures, sig-
nals a need for application. In this case, we construct the type
by applying it to another type.</p>
<p>CHAPTER 12. SIGNALING ADVERSITY 724
Let’s consider the case of Maybe, which is defined as follows:
dataMaybea=Nothing |Justa
The type Maybeis a type constructor because it takes one
argument before it becomes a concrete type:
Prelude&gt; :k Maybe
Maybe :: * -&gt; *
Prelude&gt; :k Maybe Int
Maybe Int :: *
Prelude&gt; :k Maybe Bool
Maybe Bool :: *
Prelude&gt; :k Int
Int :: *
Prelude&gt; :k Bool
Bool :: *
Whereas the following will not work, because the kinds
don’t match up:
Prelude&gt; :k Maybe Maybe
Expecting one more argument to ‘Maybe’</p>
<p>CHAPTER 12. SIGNALING ADVERSITY 725
The first argument of ‘Maybe’ should have kind ‘<em>’,
but ‘Maybe’ has kind ‘</em> -&gt; *’
In a type in a GHCi command: Maybe Maybe
Maybeexpects a single type argument of kind *, which Maybe
is not.
If we give Maybea type argument that is kind *, it also be-
comes kind <em>so then it can be an argument to another Maybe:
Prelude&gt; :k Maybe Char
Maybe Char :: *
Prelude&gt; :k Maybe (Maybe Char)
Maybe (Maybe Char) :: *
OurExample datatype from earlier also won’t work as an
argument for Maybeby itself:
Prelude&gt; data Example a = Blah | RoofGoats | Woot a
Prelude&gt; :k Maybe Example
Expecting one more argument to ‘Example’
The first argument of ‘Maybe’ should have kind ‘</em>’,</p>
<p>CHAPTER 12. SIGNALING ADVERSITY 726
but ‘Example’ has kind ‘* -&gt; *’
In a type in a GHCi command: Maybe Example
However, if we apply the Example type constructor, we can
make it work and create a value of that type:
Prelude&gt; :k Maybe (Example Int)
Maybe (Example Int) :: *
Prelude&gt; :t Just (Woot n)
Just (Woot n) :: Maybe (Example Int)
Note that the list type constructor []is also kind * -&gt; * and
otherwise unexceptional save for the bracket syntax that lets
you type [a]and[Int]instead of [] aand[] Int :
Prelude&gt; :k []
[] :: * -&gt; *
Prelude :k [] Int
[] Int :: *
Prelude&gt; :k [Int]
[Int] :: *
So, we can’t have a Maybe [] for the same reason we couldn’t
have aMaybe Maybe , but we can have a Maybe [Bool] :</p>
<p>CHAPTER 12. SIGNALING ADVERSITY 727
Prelude&gt; :k Maybe []
Expecting one more argument to ‘[]’
The first argument of ‘Maybe’ should have kind ‘<em>’,
but ‘[]’ has kind ‘</em> -&gt; *’
In a type in a GHCi command: Maybe []
Prelude&gt; :k Maybe [Bool]
Maybe [Bool] :: *
If you recall, one of the first times we used Maybein the book
was to write a safe version of a tailfunction back in the chapter
on lists:
safeTail ::[a]-&gt;Maybe[a]
safeTail []=Nothing
safeTail (x:[])=Nothing
safeTail (_:xs)=Justxs
As soon as we apply this to a value, the polymorphic type
variables become constrained or concrete types:
Prelude&gt; safeTail &quot;julie&quot;
Just &quot;ulie&quot;
Prelude&gt; :t safeTail &quot;julie&quot;
safeTail &quot;julie&quot; :: Maybe [Char]</p>
<p>CHAPTER 12. SIGNALING ADVERSITY 728
Prelude&gt; safeTail [1..10]
Just [2,3,4,5,6,7,8,9,10]
Prelude&gt; :t safeTail [1..10]
safeTail [1..10] :: (Num a, Enum a) =&gt; Maybe [a]
Prelude&gt; :t safeTail [1..10 :: Int]
safeTail [1..10 :: Int] :: Maybe [Int]
We can expand on type constructors that take a single argu-
ment and see how the kind changes as we go:
Prelude&gt; data Trivial = Trivial
Prelude&gt; :k Trivial
Trivial :: *
Prelude&gt; data Unary a = Unary a
Prelude&gt; :k Unary
Unary :: * -&gt; *
Prelude&gt; data TwoArgs a b = TwoArgs a b
Prelude&gt; :k TwoArgs
TwoArgs :: * -&gt; * -&gt; *
Prelude&gt; data ThreeArgs a b c = ThreeArgs a b c
Prelude&gt; :k ThreeArgs
ThreeArgs :: * -&gt; * -&gt; * -&gt; *</p>
<p>CHAPTER 12. SIGNALING ADVERSITY 729
It may not be clear why this is useful to know right now,
other than helping to understand when your type errors are
caused by things not being fully applied. The implications of
higher-kindedness will be clearer in a later chapter.
Data constructors are functions
In the previous chapter, we noted the diﬀerence between data
constants and data constructors and noted that data construc-
tors that haven’t been fully applied have function arrows in
them. Once you apply them to their arguments, they return a
value of the appropriate type. In other words, data construc-
tors are functions. As it happens, they behave like Haskell
functions in that they are curried as well.
First let’s observe that nullary data constructors, which are
values taking no arguments, are notlike functions:
Prelude&gt; data Trivial = Trivial deriving Show
Prelude&gt; Trivial 1
Couldn't match expected type ‘Integer -&gt; t’
with actual type ‘Trivial’
(... etc ...)
However, data constructors that take arguments dobehave
like functions:</p>
<p>CHAPTER 12. SIGNALING ADVERSITY 730
Prelude&gt; data UnaryC = UnaryC Int deriving Show
Prelude&gt; :t UnaryC
UnaryC :: Int -&gt; UnaryC
Prelude&gt; UnaryC 10
UnaryC 10
Prelude&gt; :t UnaryC 10
UnaryC 10 :: UnaryC
Like functions, their arguments are typechecked against
the specification in the type:
Prelude&gt; UnaryC &quot;blah&quot;
Couldn't match expected type ‘Int’
with actual type ‘[Char]’
If we wanted a unary data constructor which could contain
any type, we would parameterize the type like so:
Prelude&gt; data Unary a = Unary a deriving Show
Prelude&gt; :t Unary
Unary :: a -&gt; Unary a
Prelude&gt; :t Unary 10
Unary 10 :: Num a =&gt; Unary a
Prelude&gt; :t Unary &quot;blah&quot;
Unary &quot;blah&quot; :: Unary [Char]</p>
<p>CHAPTER 12. SIGNALING ADVERSITY 731
And again, this works just like a function, except the type
of the argument can be whatever we want.
Note that if we want to use a derived (GHC generated) Show
instance for Unary, it has to be able to also show the contents,
the type 𝑎value contained by Unary’s data constructor:
Prelude&gt; :info Unary
data Unary a = Unary a
instance Show a =&gt; Show (Unary a)
If we try to use a type for 𝑎that does not have a Showinstance,
it won’t cause a problem until we try to show the value:
Prelude&gt; :t (Unary id)
(Unary id) :: Unary (t -&gt; t)
-- id doesn't have a Show instance
Prelude&gt; show (Unary id)
<interactive>:53:1:
No instance for (Show (t0 -&gt; t0))
...
The only way to avoid this would be to write an instance that
did not show the value contained in the Unarydata constructor,
but that would be somewhat unusual.</p>
<p>CHAPTER 12. SIGNALING ADVERSITY 732
Another thing to keep in mind is that you can’t ordinarily
hide polymorphic types from your type constructor, so the
following is invalid:
Prelude&gt; data Unary = Unary a deriving Show
Not in scope: type variable ‘a’
In order for the type variable 𝑎to be in scope, we usually
need to introduce it with our type constructor. There are ways
around this, but they’re rarely necessary or a good idea and
not relevant to the beginning Haskeller.
Here’s an example using fmapand the Justdata constructor
fromMaybeto demonstrate how Justis also like a function:
Prelude&gt; fmap Just [1, 2, 3]
[Just 1,Just 2,Just 3]
The significance and utility of this may not be immediately
obvious but will be more clear in later chapters.
12.5 Chapter Exercises
Determine the kinds
1.Given
id::a-&gt;a</p>
<p>CHAPTER 12. SIGNALING ADVERSITY 733
What is the kind of a?
2.r::a-&gt;f a
What are the kinds of aandf?
String processing
Because this is the kind of thing linguists ahemenjoy doing in
their spare time.
1.Write a recursive function named replaceThe which takes a
text/string, breaks it into words and replaces each instance
of “the” with “a”. It’s intended only to replace exactly
the word “the”. notThe is a suggested helper function for
accomplishing this.
-- example GHCi session
-- above the functions
-- &gt;&gt;&gt; notThe &quot;the&quot;
-- Nothing
-- &gt;&gt;&gt; notThe &quot;blahtheblah&quot;
-- Just &quot;blahtheblah&quot;
-- &gt;&gt;&gt; notThe &quot;woot&quot;
-- Just &quot;woot&quot;
notThe::String-&gt;MaybeString
notThe=undefined</p>
<p>CHAPTER 12. SIGNALING ADVERSITY 734
-- &gt;&gt;&gt; replaceThe &quot;the cow loves us&quot;
-- &quot;a cow loves us&quot;
replaceThe ::String-&gt;String
replaceThe =undefined
2.Write a recursive function that takes a text/string, breaks
it into words, and counts the number of instances of ”the”
followed by a vowel-initial word.
-- &gt;&gt;&gt; countTheBeforeVowel &quot;the cow&quot;
-- 0
-- &gt;&gt;&gt; countTheBeforeVowel &quot;the evil cow&quot;
-- 1
countTheBeforeVowel ::String-&gt;Integer
countTheBeforeVowel =undefined
3.Return the number of letters that are vowels in a word.
Hint: it’s helpful to break this into steps. Add any helper
functions necessary to achieve your objectives.
a)Test for vowelhood
b)Return the vowels of a string
c)Count the number of elements returned</p>
<p>CHAPTER 12. SIGNALING ADVERSITY 735
-- &gt;&gt;&gt; countVowels &quot;the cow&quot;
-- 2
-- &gt;&gt;&gt; countVowels &quot;Mikolajczak&quot;
-- 4
countVowels ::String-&gt;Integer
countVowels =undefined
Validate the word
Use the Maybetype to write a function that counts the number
of vowels in a string and the number of consonants. If the
number of vowels exceeds the number of consonants, the
function returns Nothing . In many human languages, vowels
rarely exceed the number of consonants so when they do, it
mayindicate the input isn’t a word (that is, a valid input to your
dataset):
newtype Word'=
Word'String
deriving (Eq,Show)
vowels=&quot;aeiou&quot;
mkWord::String-&gt;MaybeWord'
mkWord=undefined</p>
<p>CHAPTER 12. SIGNALING ADVERSITY 736
It’s only Natural
You’ll be presented with a datatype to represent the natural
numbers. The only values representable with the naturals
are whole numbers from zero to infinity. Your task will be
to implement functions to convert Natural s toInteger s and
Integer s toNatural s. The conversion from Natural s toInteger s
won’t return Maybebecause Integer is a strict superset of Natural .
AnyNatural can be represented by an Integer , but the same is
nottrue of any Integer . Negative numbers are not valid natural
numbers.</p>
<p>CHAPTER 12. SIGNALING ADVERSITY 737
-- As natural as any
-- competitive bodybuilder
dataNat=
Zero
|SuccNat
deriving (Eq,Show)
-- &gt;&gt;&gt; natToInteger Zero
-- 0
-- &gt;&gt;&gt; natToInteger (Succ Zero)
-- 1
-- &gt;&gt;&gt; natToInteger (Succ (Succ Zero))
-- 2
natToInteger ::Nat-&gt;Integer
natToInteger =undefined
-- &gt;&gt;&gt; integerToNat 0
-- Just Zero
-- &gt;&gt;&gt; integerToNat 1
-- Just (Succ Zero)
-- &gt;&gt;&gt; integerToNat 2
-- Just (Succ (Succ Zero))
-- &gt;&gt;&gt; integerToNat (-1)
-- Nothing
integerToNat ::Integer -&gt;MaybeNat
integerToNat =undefined</p>
<p>CHAPTER 12. SIGNALING ADVERSITY 738
Small library for Maybe
Write the following functions. This may take some time.
1.Simple boolean checks for Maybevalues.
-- &gt;&gt;&gt; isJust (Just 1)
-- True
-- &gt;&gt;&gt; isJust Nothing
-- False
isJust::Maybea-&gt;Bool
-- &gt;&gt;&gt; isNothing (Just 1)
-- False
-- &gt;&gt;&gt; isNothing Nothing
-- True
isNothing ::Maybea-&gt;Bool
2.The following is the Maybecatamorphism. You can turn a
Maybevalue into anything else with this.
-- &gt;&gt;&gt; mayybee 0 (+1) Nothing
-- 0
-- &gt;&gt;&gt; mayybee 0 (+1) (Just 1)
-- 2
mayybee ::b-&gt;(a-&gt;b)-&gt;Maybea-&gt;b</p>
<p>CHAPTER 12. SIGNALING ADVERSITY 739
3.In case you just want to provide a fallback value.
-- &gt;&gt;&gt; fromMaybe 0 Nothing
-- 0
-- &gt;&gt;&gt; fromMaybe 0 (Just 1)
-- 1
fromMaybe ::a-&gt;Maybea-&gt;a
-- Try writing it in terms
-- of the maybe catamorphism
4.Converting between ListandMaybe.
-- &gt;&gt;&gt; listToMaybe [1, 2, 3]
-- Just 1
-- &gt;&gt;&gt; listToMaybe []
-- Nothing
listToMaybe ::[a]-&gt;Maybea
-- &gt;&gt;&gt; maybeToList (Just 1)
-- [1]
-- &gt;&gt;&gt; maybeToList Nothing
-- []
maybeToList ::Maybea-&gt;[a]
5.For when we want to drop the Nothing values from our list.</p>
<p>CHAPTER 12. SIGNALING ADVERSITY 740
-- &gt;&gt;&gt; catMaybes [Just 1, Nothing, Just 2]
-- [1, 2]
-- &gt;&gt;&gt; let xs = take 3 $ repeat Nothing
-- &gt;&gt;&gt; catMaybes xs
-- []
catMaybes ::[Maybea]-&gt;[a]
6.You’ll see this called “sequence” later.
-- &gt;&gt;&gt; flipMaybe [Just 1, Just 2, Just 3]
-- Just [1, 2, 3]
-- &gt;&gt;&gt; flipMaybe [Just 1, Nothing, Just 3]
-- Nothing
flipMaybe ::[Maybea]-&gt;Maybe[a]
Small library for Either
Write each of the following functions. If more than one possi-
ble unique function exists for the type, use common sense to
determine what it should do.
1.Try to eventually arrive at a solution that uses foldr, even
if earlier versions don’t use foldr.
lefts'::[Eithera b]-&gt;[a]
2.Same as the last one. Use foldreventually.</p>
<p>CHAPTER 12. SIGNALING ADVERSITY 741
rights' ::[Eithera b]-&gt;[b]
3.partitionEithers' ::[Eithera b]
-&gt;([a], [b])
4.eitherMaybe' ::(b-&gt;c)
-&gt;Eithera b
-&gt;Maybec
5.This is a general catamorphism for Either values.
either' ::(a-&gt;c)
-&gt;(b-&gt;c)
-&gt;Eithera b
-&gt;c
6.Same as before, but use the either' function you just
wrote.
eitherMaybe'' ::(b-&gt;c)
-&gt;Eithera b
-&gt;Maybec
Mostofthefunctionsyoujustsawareinthe Prelude ,Data.Maybe ,
orData.Either but you should strive to write them yourself
without looking at existing implementations. You will deprive
yourself if you cheat.</p>
<p>CHAPTER 12. SIGNALING ADVERSITY 742
Unfolds
While the idea of catamorphisms is still relatively fresh in our
minds, let’s turn our attention to their dual: anamorphisms . If
folds, or catamorphisms, let us break data structures down
then unfolds let us build them up. There are, as with folds, a
few diﬀerent ways to unfold a data structure. We can use them
to create finite and infinite data structures alike.
-- iterate is like a limited
-- unfold that never ends
Prelude&gt; :t iterate
iterate :: (a -&gt; a) -&gt; a -&gt; [a]
-- because it never ends, we must use
-- take to get a finite list
Prelude&gt; take 10 $ iterate (+1) 0
[0,1,2,3,4,5,6,7,8,9]
-- unfoldr is more general
Prelude&gt; :t unfoldr
unfoldr :: (b -&gt; Maybe (a, b)) -&gt; b -&gt; [a]
-- Using unfoldr to do
-- the same thing as iterate
Prelude&gt; take 10 $ unfoldr (\b -&gt; Just (b, b+1)) 0</p>
<p>CHAPTER 12. SIGNALING ADVERSITY 743
[0,1,2,3,4,5,6,7,8,9]
Why bother?
We bother with this for the same reason we abstracted direct
recursion into folds, such as with sum,product , andconcat .
importData.List
mehSum::Numa=&gt;[a]-&gt;a
mehSumxs=go0xs
wherego::Numa=&gt;a-&gt;[a]-&gt;a
go n[]=n
go n (x:xs)=(go (n+x) xs)
niceSum ::Numa=&gt;[a]-&gt;a
niceSum =foldl' ( +)0
mehProduct ::Numa=&gt;[a]-&gt;a
mehProduct xs=go1xs
wherego::Numa=&gt;a-&gt;[a]-&gt;a
go n[]=n</p>
<div style="break-before: page; page-break-before: always;"></div><p>go n (x:xs)=(go (n*x) xs)
niceProduct ::Numa=&gt;[a]-&gt;a
niceProduct =foldl' ( *)1</p>
<p>CHAPTER 12. SIGNALING ADVERSITY 744
Remember the redundant structure when we looked at
folds?
mehConcat ::[[a]]-&gt;[a]
mehConcat xs=go[]xs
wherego::[a]-&gt;[[a]]-&gt;[a]
go xs'[]=xs'
go xs' (x :xs)=(go (xs' ++x) xs)
niceConcat ::[[a]]-&gt;[a]
niceConcat =foldr (++)[]
This may have given you a mild headache, but you may
also see that this same principle of abstracting out common
patterns and giving them names applies as well to unfolds as
it does to folds.
Write your own iterate and unfoldr
1.Write the function myIterate using direct recursion. Com-
pare the behavior with the built-in iterate to gauge cor-
rectness. Do not look at the source or any examples of
iterate so that you are forced to do this yourself.
myIterate ::(a-&gt;a)-&gt;a-&gt;[a]
myIterate =undefined</p>
<p>CHAPTER 12. SIGNALING ADVERSITY 745
2.Write the function myUnfoldr using direct recursion. Com-
pare with the built-in unfoldr to check your implementa-
tion. Again, don’t look at implementations of unfoldr so
that you figure it out yourself.
myUnfoldr ::(b-&gt;Maybe(a, b))
-&gt;b
-&gt;[a]
myUnfoldr =undefined
3.Rewrite myIterate intobetterIterate usingmyUnfoldr . A
hint — we used unfoldr to produce the same results as
iterate earlier. Do this with diﬀerent functions and see if
you can abstract the structure out.
-- It helps to have the
-- types in front of you
-- myUnfoldr :: (b -&gt; Maybe (a, b))
-- -&gt; b
-- -&gt; [a]
betterIterate ::(a-&gt;a)-&gt;a-&gt;[a]
betterIterate f x=myUnfoldr ...?
Remember, your betterIterate should have the same re-
sults as iterate .</p>
<p>CHAPTER 12. SIGNALING ADVERSITY 746
Prelude&gt; take 10 $ iterate (+1) 0
[0,1,2,3,4,5,6,7,8,9]
Prelude&gt; take 10 $ betterIterate (+1) 0
[0,1,2,3,4,5,6,7,8,9]
Finally something other than a list!
Given the BinaryTree from last chapter, complete the following
exercises. Here’s that datatype again:
dataBinaryTree a=
Leaf
|Node(BinaryTree a) a (BinaryTree a)
deriving (Eq,Ord,Show)
1.Writeunfold forBinaryTree .
unfold::(a-&gt;Maybe(a,b,a))
-&gt;a
-&gt;BinaryTree b
unfold=undefined
2.Make a tree builder.
Usingthe unfold functionyou’vemadefor BinaryTree , write
the following function:</p>
<p>CHAPTER 12. SIGNALING ADVERSITY 747
treeBuild ::Integer -&gt;BinaryTree Integer
treeBuild n=undefined
You should be producing results that look like the following:
Prelude&gt; treeBuild 0
Leaf
Prelude&gt; treeBuild 1
Node Leaf 0 Leaf
Prelude&gt; treeBuild 2
Node (Node Leaf 1 Leaf)
0
(Node Leaf 1 Leaf)
Prelude&gt; treeBuild 3
Node (Node (Node Leaf 2 Leaf)
1
(Node Leaf 2 Leaf))
0
(Node (Node Leaf 2 Leaf)
1
(Node Leaf 2 Leaf))
Or in a slightly diﬀerent representation:
0
0</p>
<p>CHAPTER 12. SIGNALING ADVERSITY 748
/ <br />
1 1
0
/ <br />
1 1
/\ /<br />
2 2 2 2
Good work.
12.6 Definitions
1.Ahigher-kinded type type is any type whose kind has a
function arrow in it and which can be described as a type
constructor rather than a type constant. The following
types are of a higher kind than *:
Maybe:: * -&gt; *
[]:: * -&gt; *
Either:: * -&gt; * -&gt; *
(-&gt;):: * -&gt; * -&gt; *
The following are not:</p>
<p>CHAPTER 12. SIGNALING ADVERSITY 749
Int:: *
Char:: *
String:: *
[Char]:: *
This is not to be confused with higher kinded polymor-
phism, which we’ll discuss later.</p>
<p>Chapter 13
Building projects
Wherever there is
modularity there is the
potential for
misunderstanding:
Hiding information
implies a need to check
communication
Alan Perlis
750</p>
<p>CHAPTER 13. BUILDING PROJECTS 751
13.1 Modules
Haskell programs are organized into modules. Modules con-
tain the datatypes, type synonyms, typeclasses, typeclass in-
stances, and values you’ve defined at the top level. They oﬀer
a means to import other modules into the scope of your pro-
gram, and they also contain values that can be exported to
other modules. If you’ve ever used a language with names-
paces, it’s the same thing.
In this chapter, we will be building a small, interactive
hangman-style game. Students of Haskell often ask what kind
of project they should work on as a way to learn Haskell, and
they want to jump right into the kind of program they’re
used to building in the languages they already know. What
most often happens is the student realizes how much they still
don’t understand about Haskell, shakes their fist at the sky, and
curses Haskell’s very name and all the elitist jerks who write
Haskell and flees to relative safety. Nobody wants that. Haskell
is sufficiently diﬀerent from other languages that we think it’s
best to spend time getting comfortable with how Haskell itself
works before trying to build substantial projects.
This chapter’s primary focus is not so much on code but on
how to set up a project in Haskell, use the package manager
known as Cabal, build the project with Stack, and work with
Haskell modules as they are. There are a few times we ask
you to implement part of the hangman game yourself, but</p>
<p>CHAPTER 13. BUILDING PROJECTS 752
much of the code is already written for you, and we’ve tried to
explain the structure as well as we can at this point in the book.
Some of it you won’t properly understand until we’ve covered
at least monads and IO. But if you finish the chapter feeling
like you now know how to set up a project environment and
get things running, then this chapter will have accomplished
its goal and we’ll all go oﬀ and take a much needed mid-book
nap.
Try to relax and have fun with this. You’ve earned it after
those binary tree exercises.
In this chapter, we’ll cover:
•writing Haskell programs with modules;
•using the Cabal package manager;
•building our project with Stack;
•conventions around project organization;
•building a small interactive game.
Note that you’ll need to have Stack1and Git2to follow along
with the instructions in this chapter. We’ll be using gitto
download an example project. Depending on your level of
prior experience, some of this may not be new information
1http://haskellstack.org
2https://git-scm.com/</p>
<p>CHAPTER 13. BUILDING PROJECTS 753
for you. Feel free to move as quickly through this material as
feels comfortable.
13.2 Making packages with Stack
The Haskell Cabal, or Common Architecture for Building Ap-
plications and Libraries, is a package manager. A package is
a program you’re building, including all of its modules and
dependencies, whether you’ve written it or you’re building
someone else’s program. A package has dependencies which are
the interlinked elements of that program, the other packages
and libraries it may depend on and any tests and documenta-
tion associated with the project. Cabal exists to help organize
all this and make sure all dependencies are properly in scope.
Stack is a cross-platform program for developing Haskell
projects. It is aimed at Haskellers both new and experienced,
and it helps you manage both projects made up of multiple
packages as well as individual packages, whereas Cabal exists
primarily to describe a single package with a Cabal file that
has the .cabal file extension.
Stack is built on top of Cabal in some important senses,
so we will still be working with .cabal files. However, Stack
simplifies the process somewhat, especially in large projects
with multiple dependencies, by allowing you to build those
large libraries only once and use them across projects. Stack</p>
<p>CHAPTER 13. BUILDING PROJECTS 754
also relies on an LTS (long term support) snapshot of Haskell
packages from Stackage3that are guaranteed to work together,
unlike packages from Hackage which may have conflicting
dependencies.
While the Haskell community does not have a prescribed
project layout, we recommend the basic structure embodied
in the Stack templates.
13.3 Working with a basic project
We’re going to start learning Cabal and Stack by building a
sample project called hello. To make this less tedious, we’re
going to use gitto checkout the sample project. In an appro-
priate directory for storing your projects, you’ll want to git
clonethe repository https://github.com/haskellbook/hello .
Building the project
Change into the project directory that the git clone invocation
created.
$ cd hello
You could edit the hello.cabal file. There you can replace
“Your Name Here” with…your name. We’ll next build our
project:
3https://www.stackage.org/</p>
<p>CHAPTER 13. BUILDING PROJECTS 755
$ stack build
If it complains about needing GHC to be installed, don’t
panic! Part of the benefit of Stack is that it can manage your
GHC installs for you. Before re-attempting stack build , do the
following:
$ stack setup
Thesetupcommand for Stack determines what version of
GHC you need based on the LTS snapshot specified in the
stack.yaml file of your project. The stack.yaml file is used to
determine the versions of your packages and what version
of GHC they’ll work best with. If you didn’t need to do this,
it’s possible you had a compatible version of GHC already
installed or that you’d run setup for an LTS snapshot that
needed the same version of GHC in the past. To learn more
about this, check out the Stackage website.
Loading and running code from the REPL
Having done that, next we’ll fire up the REPL.
$ stack ghci
[... some other noise...]
Ok, modules loaded: Main.
Prelude&gt; :l Main</p>
<p>CHAPTER 13. BUILDING PROJECTS 756
[1 of 1] Compiling Main
Ok, modules loaded: Main.
Prelude&gt; main
hello world
Above, we successfully started a GHCi REPL that is aware
of our project, loaded our Mainmodule, and then ran the main
function. Using Stack’s GHCi integration to fire up a REPL
doesn’t just let us load and run code in our project, but also
enables us to make use of our project’s dependencies. We’ll
demonstrate this later. indexmain@ main
stack exec
When you ran buildearlier, you may have seen something
like:
Linking .stack-work/dist/{...noise...}/hello
This noise is Stack compiling an executable binary and
linking to it. You can type the full path that Stack mentioned
in order to run the binary, but there’s an easier way — exec!
From our project directory, consider the following:
$ hello
zsh: command not found: hello
$ stack exec -- hello
hello world</p>
<p>CHAPTER 13. BUILDING PROJECTS 757
Stack knows what paths any executables might be located in,
so using Stack’s execcommand saves you the hassle of typing
out a potentially verbose path.
Executable stanzas in Cabal files
Stack created an executable earlier because of the following
stanza in the hello.cabal file:
executable hello
-- [1]
hs-source-dirs: src
-- [2]
main-is: Main.hs
-- [3]
default-language: Haskell2010
-- [4]
build-depends: base &gt;= 4.7 &amp;&amp; &lt; 5
-- [5]
1.Thisnamefollowingthedeclarationofan executable stanza
tells Stack or Cabal what to name the binary or executable
it creates.
2.Tells this stanza where to look for source code — in this
case, the srcsubdirectory.</p>
<p>CHAPTER 13. BUILDING PROJECTS 758
3.Execution of this binary should begin by looking for a main
function inside a file named Mainwith the module name
Main. Note that module names have to match filenames.
Your compiler (not just Stack) will reject using a file that
isn’t aMainmodule as the entry point to executing the
program. Also note that it’ll look for the Main.hs file under
all directories you specified in hs-source-dirs . Since we
specified only one, it’ll find this in src/Main.hs , which is
our only source file right now anyway.
4.Defines the version of the Haskell standard to expect. Not
very interesting and doesn’t do much — mostly boiler-
plate, but necessary.
5.This is usually a meatier part of any Cabal stanza, whether
it’s an executable, library, or test suite. This example ( base)
is really the bare minimum or baseline dependency in
almost any Haskell project as you can’t really get anything
done without the baselibrary. We’ll show you how to add
and install dependencies later.
A sidebar about executables and libraries Our project here
only has an executable stanza, which is appropriate for mak-
ing a command-line application which will be run and used.
When we’re writing code we want people to be able to reuse
in other projects, we need a library stanza in the .cabal file
and to choose which modules we want to expose. Executables</p>
<p>CHAPTER 13. BUILDING PROJECTS 759
are applications that the operating system will run directly,
while software libraries are code arranged in a manner so that
they can be reused by the compiler in the building of other
libraries and programs.
13.4 Making our project a library
First we’re going to add a library stanza to hello.cabal :
library
hs-source-dirs: src
exposed-modules: Hello
build-depends: base &gt;= 4.7 &amp;&amp; &lt; 5
default-language: Haskell2010
Then we’re going to create a file located at src/Hello.hs :
moduleHellowhere
sayHello ::IO()
sayHello = do
putStrLn &quot;hello world&quot;
Then we’re going to change our Mainmodule to use this
library function:</p>
<p>CHAPTER 13. BUILDING PROJECTS 760
moduleMainwhere
importHello
main::IO()
main= do
sayHello
If we try to build and run this now, it’ll work.
$ stack build
$ stack exec hello
hello world
But what if we had made a separate exedirectory?
$ mkdir exe
$ mv src/Main.hs exe/Main.hs
Then we need to edit the .cabal file to let it know our hello
executable uses the exedirectory:
executable hello
hs-source-dirs: exe
main-is: Main.hs
default-language: Haskell2010
build-depends: base &gt;= 4.7 &amp;&amp; &lt; 5</p>
<p>CHAPTER 13. BUILDING PROJECTS 761
If you then attempt to build this, it will fail.
hello/exe/Main.hs:3:8:
Could not find module ‘Hello’
It is a member of the hidden package
‘hello-0.1.0.0@hello_IJIUuynUbgsHAquBKsAsb5’.
Perhaps you need to add ‘hello’ to the
build-depends in your .cabal file.
Use -v to see a list of the files searched for.
We have two paths for fixing this, one better than the other.
One way is to simply add srcto the source directories the
executable is permitted to search. But it turns out that Cabal’s
suggestion here is precisely right. The better way to fix this is
to respect the boundaries of the library and executable and
instead to add your own library as a dependency:
executable hello
hs-source-dirs: exe
main-is: Main.hs
default-language: Haskell2010
build-depends: base &gt;= 4.7 &amp;&amp; &lt; 5
, hello
The build will now succeed. This also makes it easier to
know when you need to change what is exposed or exported
in your library, because you’re using your own interface.</p>
<p>CHAPTER 13. BUILDING PROJECTS 762
13.5 Module exports
By default, when you don’t specify any exports in a module,
every top-level binding is exported and can be imported by
another module. This is the case in our Hellomodule:
moduleHellowhere
sayHello ::IO()
sayHello = do
putStrLn &quot;hello world&quot;
But what happens if we specify an empty export list?
moduleHello
()
where
sayHello ::IO()
sayHello = do
putStrLn &quot;hello world&quot;
We’ll get the following error if we attempt to build it:
Not in scope: ‘sayHello’
To fix that explicitly, we add the top-level binding to the
export list:</p>
<p>CHAPTER 13. BUILDING PROJECTS 763
moduleHello
(sayHello )
where
sayHello ::IO()
sayHello = do
putStrLn &quot;hello world&quot;
Now the sayHello function will be exported. It seems point-
less in a module like this, but in bigger projects, it sometimes
makes sense to specify your exports in this way.
Exposing modules
First we’ll add a new module with a new IO action for our main
action to run: indexmain@ main
-- src/DogsRule.hs
moduleDogsRule
(dogs)
where
dogs::IO()
dogs= do
putStrLn &quot;Who's a good puppy?!&quot;
putStrLn &quot;YOU ARE!!!!!&quot;</p>
<p>CHAPTER 13. BUILDING PROJECTS 764
Then we’ll change our Mainmodule to make use of this:
moduleMainwhere
importDogsRule
importHello
main::IO()
main= do
sayHello
dogs
But if we attempt to build this, we’ll get the following error:
Could not find module ‘DogsRule’
As we did earlier with our library stanza, we need to also
expose the DogsRule module:
library
hs-source-dirs: src
exposed-modules: DogsRule
, Hello
build-depends: base &gt;= 4.7 &amp;&amp; &lt; 5
default-language: Haskell2010
Now it should be able to find our very important dog prais-
ing.</p>
<p>CHAPTER 13. BUILDING PROJECTS 765
13.6 More on importing modules
Importing modules brings more functions into scope beyond
those available in the standard Prelude . Imported modules
are top-level declarations. The entities imported as part of
thosedeclarations, likeothertop-leveldeclarations, havescope
throughout the module, although they can be shadowed by
local bindings. The eﬀect of multiple import declarations is cu-
mulative, but the ordering of import declarations is irrelevant.
An entity is in scope for the entire module if it is imported by
any of the import declarations.
In previous chapters, we’ve brought functions like booland
toUpper into scope for exercises by importing the modules they
are part of, Data.Bool andData.Char , respectively.
Let’s refresh our memory of how to do this in GHCi. The
:browse command allows us to see what functions are included
in the named module, while importing the module allows
us to use those functions. You can browse modules that you
haven’t imported yet, which can be useful if you’re not sure
which module the function you’re looking for is in:
Prelude&gt; :browse Data.Bool
bool :: a -&gt; a -&gt; Bool -&gt; a
(&amp;&amp;) :: Bool -&gt; Bool -&gt; Bool
data Bool = False | True
not :: Bool -&gt; Bool</p>
<p>CHAPTER 13. BUILDING PROJECTS 766
otherwise :: Bool
(||) :: Bool -&gt; Bool -&gt; Bool
Prelude&gt; import Data.Bool
Prelude&gt; :t bool
bool :: a -&gt; a -&gt; Bool -&gt; a
In the example above, we used an unqualified import of
everything in Data.Bool . What if we only wanted boolfrom
Data.Bool ?
First, we’re going to turn oﬀ Prelude so that we don’t have
any of the default imports. We will use another extension
when we start GHCi to turn Prelude oﬀ. You’ve previously seen
how to use language extensions in source files, but now we’ll
enter-XNoImplicitPrelude right when we enter our REPL:
-- Do this outside of any projects
$ stack ghci --ghci-options -XNoImplicitPrelude
Prelude&gt;
We can check that boolandnotare not in scope yet:
Prelude&gt; :t bool
<interactive>:1:1: Not in scope: ‘bool’
Prelude&gt; :t not</p>
<p>CHAPTER 13. BUILDING PROJECTS 767
<interactive>:1:1: Not in scope: ‘not’
Next we’ll do a selective import from Data.Bool , specifying
that we only want to import bool:
Prelude&gt; import Data.Bool (bool)
Prelude&gt; :t bool
bool :: a -&gt; a -&gt; GHC.Types.Bool -&gt; a
Prelude&gt; :t not
<interactive>:1:1: Not in scope: ‘not’
Now, normally in the Prelude ,notis in scope already but
boolis not. So you can see that by turning oﬀ Prelude , taking
its standard functions out of scope, and then importing only
bool, we no longer have the standard notfunction in scope.
You can import one or more functions from a module or
library. The syntax is just as we demonstrated with GHCi,
but your import declarations have to be at the beginning of
a module. Putting import Data.Char (toUpper) in the import
declarations of a module will ensure that toUpper , but not any
of the other entities contained in Data.Char , is in scope for that
module.
For the examples in the next section, you’ll want Prelude
back on, so please restart GHCi before proceeding.</p>
<p>CHAPTER 13. BUILDING PROJECTS 768
Qualified imports
What if you wanted to know where something you imported
came from in the code that uses it? We can use qualified
imports to make the names more explicit.
We use the qualified keyword in our imports to do this.
Sometimes you’ll have stuﬀ with the same name imported
from two diﬀerent modules; qualifying your imports is a com-
mon way of dealing with this. We’ll go through an example of
how you might use a qualified import.
Prelude&gt; import qualified Data.Bool
Prelude&gt; :t bool
<interactive>:1:1:
Not in scope: ‘bool’
Perhaps you meant ‘Data.Bool.bool’
Prelude&gt; :t Data.Bool.bool
Data.Bool.bool :: a -&gt; a -&gt; Bool -&gt; a
Prelude&gt; :t Data.Bool.not
Data.Bool.not :: Bool -&gt; Bool
In the case of import qualified Data.Bool , everything from
Data.Bool is in scope, but only when accessed with the full</p>
<p>CHAPTER 13. BUILDING PROJECTS 769
Data.Bool namespace. Now we are marking where the func-
tions that we’re using came from, which can be useful.
We can also provide aliases or alternate names for our mod-
ules when we qualify them so we don’t have to type out the
full namespace:
Prelude&gt; import qualified Data.Bool as B
Prelude&gt; :t bool
<interactive>:1:1:
Not in scope: ‘bool’
Perhaps you meant ‘B.bool’
Prelude&gt; :t B.bool
B.bool :: a -&gt; a -&gt; Bool -&gt; a
Prelude&gt; :t B.not
B.not :: Bool -&gt; Bool
You can do qualified imports in the import declarations at
the beginning of your module in the same way.
Setting the Prelude prompt When you imported Data.Bool
asBabove, you may have seen your prompt change:
Prelude&gt; import qualified Data.Bool as B
Prelude B&gt;</p>
<p>CHAPTER 13. BUILDING PROJECTS 770
And if you don’t want to unload the imported modules
(because you want them all to stay in scope), your prompt
could keep growing:
Prelude B&gt; import Data.Char
Prelude B Data.Char&gt;
(Reminder: you can use :mto unload the modules, which
does, of course, prevent the prompt from growing ever larger,
but also, well, unloads the modules so they’re not in scope
anymore!)
If you want to prevent the ever-growing prompt, you can
use the :setcommand to set the prompt to whatever you
prefer:
Prelude&gt; :set prompt &quot;Lambda&gt; &quot;
Lambda&gt; import Data.Char
Lambda&gt; :t B.bool
B.bool :: a -&gt; a -&gt; Bool -&gt; a
As you can see, Data.Bool is still in scope as B, but it doesn’t
show up in our prompt. You can set your Prelude prompt
permanently, if you wish, by changing it in your GHCi config-
uration file, but instructions for doing that are somewhat out
of the scope of the current chapter.</p>
<p>CHAPTER 13. BUILDING PROJECTS 771
Intermission: Check your understanding
Here is the import list from one of the modules in Chris’s
library called blacktip :
import qualified Control.Concurrent
asCC
import qualified Control.Concurrent.MVar
asMV
import qualified Data.ByteString.Char8
asB
import qualified Data.Locator
asDL
import qualified Data.Time.Clock.POSIX
asPSX
import qualified Filesystem
asFS
import qualified Filesystem.Path.CurrentOS
asFPC
import qualified Network.Info
asNI</p>
<p>CHAPTER 13. BUILDING PROJECTS 772
import qualified Safe
importControl.Exception (mask,try)
importControl.Monad (forever,when)
importData.Bits
importData.Bits.Bitwise (fromListBE )
importData.List.Split (chunksOf )
importDatabase.Blacktip.Types
importSystem.IO.Unsafe (unsafePerformIO )
For our purposes right now, it does not matter whether you
are familiar with the modules referenced in the import list.
Look at the declarations and answer the questions below:
1.What functions are being imported from Control.Monad ?
2.Which imports are both unqualified and imported in their
entirety?
3.From the name, what do you suppose importing blacktip ’s
Typesmodule brings in?
4.Now let’s compare a small part of blacktip ’s code to the
above import list:</p>
<p>CHAPTER 13. BUILDING PROJECTS 773
writeTimestamp ::MV.MVarServerState
-&gt;FPC.FilePath
-&gt;IOCC.ThreadId
writeTimestamp s path= do
CC.forkIO go
wherego=forever $ do
ss&lt;-MV.readMVar s
mask$_ -&gt; do
FS.writeFile path
(B.pack (show (ssTime ss)))
-- sleep for 1 second
CC.threadDelay 1000000
a)The type signature refers to three aliased imports.
What modules are named in those aliases?
b)Which import does FS.writeFile refer to?
c)Which import did forever come from?
13.7 Making our program interactive
Now we’re going to make our program ask for your name, then
greet you by name. First, we’ll rewrite our sayHello function
to take an argument:</p>
<p>CHAPTER 13. BUILDING PROJECTS 774
sayHello ::String-&gt;IO()
sayHello name=
putStrLn ( &quot;Hi &quot;++name++&quot;!&quot;)
Note we parenthesized the appending (++)function of the
String argument to putStrLn .
Next we’ll change mainto get the user’s name:
-- src/Main.hs
main::IO()
main= do
name&lt;-getLine
sayHello name
dogs
There are a couple of new things here. We’re using some-
thing called dosyntax, which is syntactic sugar. We use do
inside functions that return IOin order to sequence side eﬀects
in a convenient syntax. Let’s decompose what’s going on here:</p>
<p>CHAPTER 13. BUILDING PROJECTS 775
main::IO()
main= do
-- [1]
name&lt;-getLine
-- [4] [3] [2]
sayHello name
-- [5]
dogs
-- [6]
1.Thedohere begins the block.
2.getLine has type IO String , because it must perform I/O
(input/output, side eﬀects) in order to obtain the String.
getLine is what will allow you to enter your name to be
used in the mainfunction.
3.&lt;-in adoblock is pronounced bind. We’ll explain what
this is and how it works in the chapters on MonadandIO.
4.The result of binding ( &lt;-) over the IO String isString . We
bound it to the variable name. Remember, getLine has type
IO String ,namehas type String .
5.sayHello expects an argument String , which is the type of
namebutnotgetLine .</p>
<p>CHAPTER 13. BUILDING PROJECTS 776
6.dogs4expects nothing and is an IOaction of type IO (),
which fits the overall type of main.
indexmain@ main
Now we’ll fire oﬀ a build:
$ stack build
And run the program:
$ stack exec hello
After you hit enter, the program is going to wait for your
input. You’ll just see the cursor blinking on the line, waiting
for you to enter your name. As soon as you do, and hit enter,
it should greet you and then rave about the wonderfulness of
a dog.
What if we tried to pass getLine to sayHello? If we tried to
writemainwithout the use of dosyntax, particularly without
using&lt;-such as in the following example:
main::IO()
main=sayHello getLine
We’d get the following type error:
4Much like actual dogs.</p>
<p>CHAPTER 13. BUILDING PROJECTS 777
$ stack build
[2 of 2] Compiling Main
src/Main.hs:8:17:
Couldn't match type ‘IO String’ with ‘[Char]’
Expected type: String
Actual type: IO String
In the first argument of ‘sayHello’, namely ‘getLine’
In the expression: sayHello getLine
This is because getLine is anIOaction with type IO String ,
whereas sayHello expects a value of type String. We have to
use&lt;-to bind over the IOto get the string that we want to
pass tosayHello . This will be explained in more detail — a bit
more detail later in the chapter, and a lot more detail in a later
chapter.
Adding a prompt
Let’s make our program a bit easier to use by adding a prompt
that tells us our program is expecting input! We need to change
main:</p>
<p>CHAPTER 13. BUILDING PROJECTS 778
moduleMainwhere
importDogsRule
importHello
importSystem.IO
main::IO()
main= do
hSetBuffering stdout NoBuffering
putStr&quot;Please input your name: &quot;
name&lt;-getLine
sayHello name
dogs
We did several things here. One is that we used putStr in-
stead of putStrLn so that our input could be on the same line as
our prompt. We also imported from System.IO so that we could
usehSetBuffering ,stdout, andNoBuffering . That line of code is
so thatputStr isn’t buﬀered (deferred) and prints immediately.
Rebuild and rerun your program, and it should now work like
this:
$ stack exec hello
Please input your name: julie
Hi julie!
Who's a good puppy?!</p>
<p>CHAPTER 13. BUILDING PROJECTS 779
YOU ARE!!!!!
You can try removing the NoBuffering line (that whole first
line) from mainand rebuilding and running your program
to see how it changes. We will be using this as part of our
hangman game in a bit, but it isn’t necessary at this point to
understand how the buﬀering functions work in any detail.
13.8 do syntax and IO
We touched on donotation a bit above, but we want to explain
a few more things about it. doblocks are convenient syntactic
sugar that allow for sequencing actions, but because they are
only syntactic sugar, they are not, strictly speaking, necessary.
They can make blocks of code more readable and also hide
the underlying nesting, and that can help you write eﬀectful
code before you understand monads and IO. So you’ll see it
a lot in this chapter (and, indeed, you’ll see it quite a bit in
idiomatic Haskell code).
Themainexecutable in a Haskell program must always have
the type IO (). Thedosyntax specifically allows us to sequence
monadic actions .Monadis a typeclass we’ll explain in great detail
in a later chapter; here, the instance of Monad we care about is
IO. That is why mainfunctions are often (not always) doblocks.
indexmain@ main</p>
<p>CHAPTER 13. BUILDING PROJECTS 780
This syntax also provides a way of naming values returned
by monadic IOactions so that they can be used as inputs to
actions that happen later in the program. Let’s look at a very
simpledoblock and try to get a feel for what’s happening here:
main= do
-- [1]
x1&lt;-getLine
-- [2] [3] [4]
x2&lt;-getLine
-- [5]
return (x1 ++x2)
-- [6] [7]
1.dointroduces the block of IO actions.
2.𝑥1is a variable representing the value obtained from the
IO action getLine .
3.&lt;-binds the variable on the left to the result of the IO
action on the right.
4.getLine has the type IO String and takes user input of a
string value. In this case, the string the user inputs will be
the value bound to the 𝑥1name.</p>
<p>CHAPTER 13. BUILDING PROJECTS 781
5.𝑥2is a variable representing the value obtained from our
second getLine . As above it is bound to that value by the
&lt;-.
6.return will be discussed in more detail shortly, but here it
is the concluding action of our doblock.
7.This is the value return , well, returns — the conjunction of
the two strings we obtained from our two getLine actions.
While&lt;-is used to bind a variable, it is diﬀerent from other
methods we’ve seen in earlier chapters for naming and binding
variables. This arrow is part of the special dosugar and specif-
ically binds a name to the 𝑎of anm avalue, where 𝑚is some
monadic structure, in this case IO. The&lt;-allows us to extract
that𝑎and name it within the limited scope of the doblock
and use that named value as an input to another expression
within that same scope. Each assignment using &lt;-creates a
new variable rather than mutating an existing variable because
data is immutable.
return
This function really doesn’t do a lot, but the purpose it serves
is important, given the way monads and IOwork. It does noth-
ing but return a value, but it returns a value inside monadic
structure:</p>
<p>CHAPTER 13. BUILDING PROJECTS 782
Prelude&gt; :t return
return :: Monad m =&gt; a -&gt; m a
For our purposes in this chapter, return returns a value in
IO. Because the obligatory type of mainisIO (), the final value
must also have an IO ()type, and return gives us a way to add
no extra function except putting the final value in IO. If the
final action of a doblock is return () , that means there is no
real value to return at the end of performing the I/O actions,
but since Haskell programs can’t return literally nothing, they
return this empty tuple called unit simply to have something
to return. That empty tuple will not print to the screen in the
REPL, but it’s there in the underlying representation.
Let’s take a look at return in action. Let’s say you want to get
user input of two characters and test them for equality. You
can’t do this:
twoo::IOBool
twoo= doc&lt;-getChar
c'&lt;-getChar
c==c'
Try it and see what your type error looks like. It should
tell you that it can’t match the expected type IO Bool with the
actual type of c == c' , which is Bool. So, our final line needs to
return that Boolvalue in IO:</p>
<p>CHAPTER 13. BUILDING PROJECTS 783
twoo::IOBool
twoo= doc&lt;-getChar
c'&lt;-getChar
return (c ==c')
We put the Boolvalue into IOby using return. Cool. How
about if we have cases where we want to return nothing? We’ll
reuse the same basic code from above but make an if-then-else
within our doblock:
main::IO()
main= doc&lt;-getChar
c'&lt;-getChar
ifc==c'
thenputStrLn &quot;True&quot;
elsereturn()
What happens when the two input characters are equal?
What happens when they aren’t?
Some people have noted that dosyntax makes it feel like
you’re doing imperative programming in Haskell. It’s impor-
tant to note that this eﬀectful imperative style requires having
IOin our result type. We cannot perform eﬀects without evi-
dence of having done so in the type. dois only syntactic sugar,
but the monadic syntax we’ll cover in a later chapter works in
a similar way for monads other than IO.</p>
<p>CHAPTER 13. BUILDING PROJECTS 784
Do notation considered harmful! Just kidding. But some-
times enthusiastic programmers overuse doblocks. It is not
necessary, and considered bad style, to use doin single-line
expressions. You will eventually learn to use &gt;&gt;=in single-
line expressions instead of do(there’s an example of that in
this chapter). Similarly, it is unnecessary to use dowith func-
tions like putStrLn andprintthat already have the eﬀects baked
in. In the function above, we could have put doin front of
bothputStrLn andreturn and it would have worked the same,
but things get messy and the Haskell ninjas will come and be
severely disappointed in you.
13.9 Hangman game
Now we’re ready to build a game. We’ll use Stack’s newcom-
mand to create this project:
$ stack new hangman simple
That will generate a directory named hangman for you and
some put some default files into the directory.
You need a wordsfile for getting words from. Most Unix-
based operating systems will have a words list located at a
directory like the following:
$ ls /usr/share/dict/
american-english british-english</p>
<p>CHAPTER 13. BUILDING PROJECTS 785
cracklib-small README.select-wordlist
words words.pre-dictionaries-common
In this case, we’ll use the wordsword list which should be
your operating system’s default. You may have one that is
diﬀerently located, or you may need to download one. We
put it in the working directory at data/dict.txt :
$ tree .
.
├── LICENSE
├── Setup.hs
├── data
│ └── dict.txt
├── hangman.cabal
├── src
│ └── Main.hs
└── stack.yaml
The file was newline separated and so looked like:
$ head data/dict.txt
A
a
aa
aal
aalii</p>
<p>CHAPTER 13. BUILDING PROJECTS 786
aam
Aani
aardvark
aardwolf
Aaron
Now edit the .cabal file as follows:
name: hangman
version: 0.1.0.0
synopsis: Playing Hangman
homepage: Chris N Julie
license: BSD3
license-file: LICENSE
author: Chris Allen and Julie Moronuki
maintainer: haskellbook.com
category: Game
build-type: Simple
extra-source-files: data/dict.txt
cabal-version: &gt;=1.10
executable hangman
main-is: Main.hs
hs-source-dirs: src
build-depends: base &gt;=4.7 &amp;&amp; &lt;5
, random
, split
default-language: Haskell2010</p>
<p>CHAPTER 13. BUILDING PROJECTS 787
The important bit here is that we used two libraries: random
andsplit. Normally you’d do version ranges for your depen-
dencies like you see with base, but we left the versions of random
andsplitunassigned because they do not change much. The
primary and only source file was in src/Main.hs .
13.10 Step One: Importing modules
-- src/Main.hs
moduleMainwhere
importControl.Monad (forever)-- [1]
importData.Char (toLower)-- [2]
importData.Maybe (isJust)-- [3]
importData.List (intersperse )-- [4]
importSystem.Exit (exitSuccess )-- [5]
importSystem.Random (randomRIO )-- [6]
Here the imports are enumerated in the source code. For
your version of this project, you don’t need to add the enumer-
ating comments. All modules listed below are part of the main
baselibrary that comes with your GHC install unless otherwise
noted.</p>
<p>CHAPTER 13. BUILDING PROJECTS 788
1.We’re using forever fromControl.Monad to make an infinite
loop. A couple points to note:
a)You don’t haveto useforever to do this, but we’re going
to.
b)You are not expected to understand what it does or
how it works exactly. Basically it allows us to execute
a function over and over again, infinitely, or until we
cause the program to exit or fail, instead of evaluating
once and then stopping.
2.We will use toLower fromData.Char to convert all characters
of our string to lowercase:
Prelude&gt; import Data.Char (toLower)
Prelude&gt; toLower 'A'
'a'
Be aware that if you pass a character that doesn’t have a
sensible lowercase, toLower will kick the same character
back out:
Prelude&gt; toLower ':'
':'</p>
<p>CHAPTER 13. BUILDING PROJECTS 789
3.We will use isJust fromData.Maybe to determine if every
character in our puzzle has been discovered already or
not:
Prelude&gt; import Data.Maybe (isJust)
Prelude&gt; isJust Nothing
False
Prelude&gt; isJust (Just 10)
True
We will combine this with all, a standard function in the
Prelude . Hereallis a function which answers the question,
“given a function that will return True or False for each
element, does it return True for allof them?”
Prelude&gt; all even [2, 4, 6]
True
Prelude&gt; all even [2, 4, 7]
False
Prelude&gt; all isJust [Just 'd', Nothing, Just 'g']
False
Prelude&gt; all isJust [Just 'd', Just 'o', Just 'g']
True
The function allhas the type:</p>
<p>CHAPTER 13. BUILDING PROJECTS 790
Foldable t=&gt;(a-&gt;Bool)-&gt;t a-&gt;Bool
We haven’t explained the Foldable typeclass. For your
purposes you can assume it’s a set of operations for types
that can be folded in a manner conceptually similar to
the list type but which don’t necessarily contain more than
one value (or any values at all) the way a list or similar
datatype does. We can make the type more specific by
asserting a type signature like so:
Prelude&gt; :t all :: (a -&gt; Bool) -&gt; [a] -&gt; Bool
all :: (a -&gt; Bool) -&gt; [a] -&gt; Bool
This will work for any type which has a Foldable instance:
Prelude&gt; :t all :: (a -&gt; Bool) -&gt; Maybe a -&gt; Bool
all :: (a -&gt; Bool) -&gt; Maybe a -&gt; Bool
-- note the type variables used and
-- experiment independently
Prelude&gt; :t all :: (a -&gt; Bool) -&gt; Either b a -&gt; Bool
all :: (a -&gt; Bool) -&gt; Either b a -&gt; Bool
But it will not work if the datatype doesn’t have an instance
ofFoldable :</p>
<p>CHAPTER 13. BUILDING PROJECTS 791
Prelude&gt; :t all :: (a -&gt; Bool) -&gt; (b -&gt; a) -&gt; Bool
No instance for (Foldable ((-&gt;) b1)) arising
from a use of ‘all’
In the expression:
all :: (a -&gt; Bool) -&gt; (b -&gt; a) -&gt; Bool
4.Weuseintersperse fromData.List to…intersperseelements
in a list. In this case, we’re putting spaces between the
characters guessed so far by the player. You may remem-
ber we used intersperse back in the Recursion chapter to
put hyphens in our Numbers Into Words exercise:
Prelude&gt; import Data.List (intersperse)
Prelude&gt; intersperse ' ' &quot;Blah&quot;
&quot;B l a h&quot;
Conveniently, the type of intersperse says nothing about
characters or strings, so we can use it with lists containing
elements of any type:
Prelude&gt; :t intersperse
intersperse :: a -&gt; [a] -&gt; [a]
Prelude&gt; intersperse 0 [1, 1, 1]</p>
<p>CHAPTER 13. BUILDING PROJECTS 792
[1,0,1,0,1]
5.We use exitSuccess fromSystem.Exit to exit successfully —
no errors, we’re simply done. We indicate whether it was
a success or not so our operating system knows whether
an error occurred. Note that if you evaluate exitSuccess
in the REPL, it’ll report that an exception occurred. In a
normal running program that doesn’t catch the exception,
it’ll end your whole program.
6.We use randomRIO fromSystem.Random to select a word from
our dictionary at random. System.Random is in the library
random . Once again, you’ll need to have the library in scope
for your REPL to be able to load it. Once it’s in scope,
we can use randomRIO to get a random number. You can
see from the type signature that it takes a tuple as an
argument, but it uses the tuple as a range from which to
select a random item:
Prelude&gt; import System.Random
Prelude System.Random&gt; :t randomRIO
randomRIO :: Random a =&gt; (a, a) -&gt; IO a
Prelude System.Random&gt; randomRIO (0, 5)
4
Prelude System.Random&gt; randomRIO (1, 100)
71</p>
<p>CHAPTER 13. BUILDING PROJECTS 793
Prelude System.Random&gt; randomRIO (1, 100)
12
We will later use this random number generation to pro-
duce a random index of a word list to provide a means of
selecting random words for our puzzle.
13.11 Step Two: Generating a word list
For clarity’s sake, we’re using a type synonym to declare what
we mean by [String] in our types. Later we’ll show you a
version that’s even more explicit using newtype . We also use do
syntax to read the contents of our dictionary into a variable
named dict. We use the linesfunction to split our big blob
string we read from the file into a list of string values each
representing a single line. Each line is a single word, so our
result is the WordList :
typeWordList =[String]
allWords ::IOWordList
allWords = do
dict&lt;-readFile &quot;data/dict.txt&quot;
return (lines dict)
Let’s take a moment to look at lines, which splits strings at
the newline marks and returns a list of strings:</p>
<p>CHAPTER 13. BUILDING PROJECTS 794
Prelude&gt; lines &quot;aardvark\naaron&quot;
[&quot;aardvark&quot;,&quot;aaron&quot;]
Prelude&gt; length $ lines &quot;aardvark\naaron&quot;
2
Prelude&gt; length $ lines &quot;aardvark\naaron\nwoot&quot;
3
Prelude&gt; lines &quot;aardvark aaron&quot;
[&quot;aardvark aaron&quot;]
Prelude&gt; length $ lines &quot;aardvark aaron&quot;
1
Note that this does something similar but diﬀerent from
wordswhich splits by spaces (ostensibly between words) and
newlines:
Prelude&gt; words &quot;aardvark aaron&quot;
[&quot;aardvark&quot;,&quot;aaron&quot;]
Prelude&gt; words &quot;aardvark\naaron&quot;
[&quot;aardvark&quot;,&quot;aaron&quot;]
The next part of building our word list for our puzzle is to
set upper and lower bounds for the size of words we’ll use in
the puzzles. Feel free to change them if you want:</p>
<p>CHAPTER 13. BUILDING PROJECTS 795
minWordLength ::Int
minWordLength =5
maxWordLength ::Int
maxWordLength =9
The next thing we’re going to do is take the output of
allWords and filter it to fit the length criteria we defined above.
That will give us a shorter list of words to use in the puzzles:
gameWords ::IOWordList
gameWords = do
aw&lt;-allWords
return (filter gameLength aw)
wheregameLength w =
letl=length (w ::String)
inl&gt;=minWordLength
&amp;&amp;l&lt;maxWordLength
We next need to write a pair of functions that will pull a
random word out of our word list for us, so that the puzzle
player doesn’t know what the word will be. We’re going to use
therandomRIO function we mentioned above to facilitate that.
We’ll pass randomRIO a tuple of zero (the first indexed position
in our word list) and the number that is the length of our word
list minus one. Why minus one?</p>
<p>CHAPTER 13. BUILDING PROJECTS 796
We have to subtract one from the length of the word list
in order to index it because length starts counting from 1 but
an index of the list starts from 0. A list of length 5 does not
have a member indexed at position 5 — it has inhabitants at
positions 0-4 instead:
Prelude&gt; [1..5] !! 4
5
Prelude&gt; [1..5] !! 5
*** Exception: Prelude.(!!): index too large
In order to get the last value in the list, then, we must ask
for the member in the position of the length of the list minus
one:
Prelude&gt; let myList = [1..5]
Prelude&gt; length myList
5
Prelude&gt; myList !! length myList
*** Exception: Prelude.!!: index too large
Prelude&gt; myList !! (length myList - 1)
5
The next two functions work together to pull a random
word out of the gameWords list we had created above. Roughly
speaking, randomWord generates a random index number based</p>
<p>CHAPTER 13. BUILDING PROJECTS 797
on the length of a word list, wl, and then selects the member
of that list that is at that indexed position and returns an IO
String. Given what you know about randomRIO and indexing,
you should be able to supply the tuple argument to randomRIO
yourself:
randomWord ::WordList -&gt;IOString
randomWord wl= do
randomIndex &lt;-randomRIO ( , )
-- fill this part in ^^^
return$wl!!randomIndex
The second function, randomWord' binds the gameWords list to
therandomWord function so that the random word we’re getting
is from that list. We’re going to delay a full discussion of the
&gt;&gt;=operator known as “bind” until we get to the Monadchapter.
For now, we can say that, as we said about dosyntax, bindallows
us to sequentially compose actions such that a value generated
by the first becomes an argument to the second:
randomWord' ::IOString
randomWord' =gameWords &gt;&gt;=randomWord
Now that we have a word list, we turn our attention to the
building of an interactive game using it.</p>
<p>CHAPTER 13. BUILDING PROJECTS 798
13.12 Step Three: Making a puzzle
Our next step is to formulate the core game play. We need a
way to hide the word from the player (while giving them an
indication of how many letters it has) and create a means of
asking for letter guesses, determining if the guessed letter is
in the word, putting it in the word if it is and putting it into
an “already guessed” list if it’s not, and determining when the
game ends.
We start with a datatype for our puzzle. The puzzle is a
product of aString , a list of Maybe Char , and a list of Char:
dataPuzzle=
PuzzleString[MaybeChar] [Char]
-- [1] [2] [3]
1.the word we’re trying to guess
2.the characters we’ve filled in so far
3.the letters we’ve guessed so far
Next we’re going to write an instance of the typeclass Show
for our datatype Puzzle. You may recall that showallows us to
print human-readable stringy things to the screen, which is
obviously something we have to do to interact with our game.
But we want it to print our puzzle a certain way, so we define
this instance.</p>
<p>CHAPTER 13. BUILDING PROJECTS 799
Notice how the argument to showlines up with our datatype
definition above. Now discovered refers to our list of Maybe Char
andguessed is what we’ve named our list of Char, but we’ve
done nothing with the String itself:
instance ShowPuzzlewhere
show (Puzzle_discovered guessed) =
(intersperse ' '$
fmap renderPuzzleChar discovered)
++&quot; Guessed so far: &quot; ++guessed
This is going to show us two things as part of our puzzle:
the list of Maybe Char which is the string of characters we have
correctly guessed and the rest of the characters of the puzzle
word represented by underscores, interspersed with spaces;
and a list of Charthat reminds us of which characters we’ve
already guessed. We’ll talk about renderPuzzleChar below.
First we’re going to write a function that will take our puzzle
word and turn it into a list of Nothing . This is the first step in
hiding the word from the player. We’re going to ask you to
write this one yourself, using the following information:
•We’ve given you a type signature. Your first argument is a
String , which will be the word that is in play. It will return
a value of type Puzzle . Remember that the Puzzle type is a
product of three things.</p>
<p>CHAPTER 13. BUILDING PROJECTS 800
•Your first value in the output will be the same string as
the argument to the function.
•The second value will be the result of mapping a function
over that String argument. Consider using constin the
mapped function, as it will always return its first argument,
no matter what its second argument is.
•For purposes of this function, the final argument of Puzzle
is an empty list.
Go for it:
freshPuzzle ::String-&gt;Puzzle
freshPuzzle =undefined
Now we need a function that looks at the Puzzle String and
determines whether the character you guessed is an element
of that string. Here are some hints:
•This is going to need two arguments, and one of those
is of type Puzzle which is a product of 3 types. But for
the purpose of this function, we only care about the first
argument to Puzzle .
•We can use underscores to signal that there are values
we don’t care about and tell the function to ignore them.
Whether you use underscores to represent the arguments</p>
<p>CHAPTER 13. BUILDING PROJECTS 801
you don’t care about or go ahead and put the names of
those in won’t aﬀect the result of the function. It does,
however, keep your code a bit cleaner and easier to read
by explicitly signaling which arguments you care about
in a given function.
•The standard function elemworks like this:
Prelude&gt; :t elem
elem :: Eq a =&gt; a -&gt; [a] -&gt; Bool
Prelude&gt; elem 'a' &quot;julie&quot;
False
Prelude&gt; elem 3 [1..5]
True
So, here you go:
charInWord ::Puzzle-&gt;Char-&gt;Bool
charInWord =undefined
The next function is very similar to the one you just wrote,
but this time we don’t care if the Charis part of the String
argument — this time we want to check and see if it is an
element of the guessed list.
You’ve totally got this:
alreadyGuessed ::Puzzle-&gt;Char-&gt;Bool
alreadyGuessed =undefined</p>
<p>CHAPTER 13. BUILDING PROJECTS 802
OK, so far we have ways to choose a word that we’re trying
to guess and determine if a guessed character is part of that
word or not. But we need a way to hide the rest of the word
from the player while they’re guessing. Computers are a bit
dumb, after all, and can’t figure out how to keep secrets on
their own. Back when we defined our Showinstance for this
puzzle, we fmapped a function called renderPuzzleChar over
our second Puzzle argument. Let’s work on that function next.
The goal here is to use Maybeto permit two diﬀerent out-
comes. It will be mapped over a string in the typeclass instance,
so this function works on only one character at a time. If that
character has not been correctly guessed yet, it’s a Nothing value
and should appear on the screen as an underscore. If the char-
acter has been guessed, we want to display that character so
the player can see which positions they’ve correctly filled:
Prelude&gt; renderPuzzleChar Nothing
'_'
Prelude&gt; renderPuzzleChar (Just 'c')
'c'
Prelude&gt; let n = Nothing
Prelude&gt; let daturr = [n, Just 'h', n, Just 'e', n]
Prelude&gt; fmap renderPuzzleChar daturr
&quot;<em>h_e</em>&quot;
Your turn. Remember, you don’t need to do the mapping
part of it here:</p>
<p>CHAPTER 13. BUILDING PROJECTS 803
renderPuzzleChar ::MaybeChar-&gt;Char
renderPuzzleChar =undefined
The next bit is a touch tricky. The point is to insert a cor-
rectly guessed character into the string. Although none of the
components here are new to you, they’re put together in a
somewhat dense manner, so we’re going to unpack it (obvi-
ously, when you type this into your own file, you do not need
to add the enumerations):</p>
<p>CHAPTER 13. BUILDING PROJECTS 804
fillInCharacter ::Puzzle-&gt;Char-&gt;Puzzle
fillInCharacter (Puzzleword
-- [1]
filledInSoFar s) c =
-- [2]
Puzzleword newFilledInSoFar (c :s)
-- [ 3 ]
wherezipper guessed wordChar guessChar =
-- [4] [5] [6] [7]
ifwordChar ==guessed
thenJustwordChar
elseguessChar
-- [ 8 ]
newFilledInSoFar =
-- [9]
zipWith (zipper c)
word filledInSoFar
-- [ 10 ]
1.The first argument is our Puzzle with its three arguments,
with𝑠representing the list of characters already guessed.
2.The𝑐is ourCharargument and is the character the player
guessed on this turn.
3.Our result is the Puzzle with the filledInSoFar replaced by</p>
<p>CHAPTER 13. BUILDING PROJECTS 805
newFilledInSoFar the𝑐consed onto the front of the 𝑠list.
4.zipper is a combining function for deciding how to handle
the character in the word, what’s been guessed already,
and the character that was just guessed. If the current
character in the word is equal to what the player guessed,
then we go ahead and return Just wordChar to fill in that
spot in the puzzle. Otherwise, we kick the guessChar back
out. We kick guessChar back out because it might either be
a previously correctly guessed character oraNothing that
has not been guessed correctly this time nor in the past.
5.guessed is the character they guessed.
6.wordChar is the characters in the puzzle word — not the
ones they’ve guessed or not guessed, but the characters
in the word that they’re supposed to be guessing.
7.guessChar is the list that keeps track of the characters the
player has guessed so far.
8.Thisif-then-else expression checks to see if the guessed
character is one of the word characters. If it is, it wraps it
in aJustbecause our puzzle word is a list of Maybevalues.
9.newFilledInSoFar is the new state of the puzzle which uses
zipWith and the zipper combining function to fill in char-
acters in the puzzle. The zipper function is first applied to</p>
<p>CHAPTER 13. BUILDING PROJECTS 806
the character the player just guessed because that doesn’t
change. Then it’s zipped across two lists. One list is word
which is the word the user is trying to guess. The second
list,filledInSoFar is the puzzle state we’re starting with of
type[Maybe Char] . That’s telling us which characters in
wordhave been guessed.
10.Now we’re going to make our newFilledInSoFar by using
zipWith . You may remember this from the Lists chapter.
It’s going to zip the wordwith the filledInSoFar values while
applying the zipper function from just above it to the
values as it does.
Next we have this big doblock with a case expression and
each case also has a doblock inside it. Why not, right?
First, it tells the player what you guessed. The case ex-
pression is to give diﬀerent responses based on whether the
guessed character:
•had already been guessed previously;
•is in the word and needs to be filled in;
•or, was not previously guessed but also isn’t in the puzzle
word.
Despite the initial appearance of complexity, most of this
is syntax you’ve seen before, and you can look through it step-
by-step and see what’s going on:</p>
<p>CHAPTER 13. BUILDING PROJECTS 807
handleGuess ::Puzzle-&gt;Char-&gt;IOPuzzle
handleGuess puzzle guess = do
putStrLn $&quot;Your guess was: &quot; ++[guess]
case(charInWord puzzle guess
, alreadyGuessed puzzle guess) of
(<em>,True)-&gt; do
putStrLn &quot;You already guessed that <br />
\character, pick <br />
\something else!&quot;
return puzzle
(True,</em>)-&gt; do
putStrLn &quot;This character was in the <br />
\word, filling in the word <br />
\accordingly&quot;
return (fillInCharacter puzzle guess)
(False,_)-&gt; do
putStrLn &quot;This character wasn't in <br />
\the word, try again.&quot;
return (fillInCharacter puzzle guess)
All right, next we need to devise a way to stop the game
after a certain number of guesses. Hangman games normally
stop only after a certain number of incorrect guesses, but for
the sake of simplicity here, we’re stopping after a set number
of guesses, whether they’re correct or not. Again, the syntax</p>
<p>CHAPTER 13. BUILDING PROJECTS 808
here should be comprehensible to you from what we’ve done
so far:
gameOver ::Puzzle-&gt;IO()
gameOver (PuzzlewordToGuess _guessed) =
if(length guessed) &gt;7then
doputStrLn &quot;You lose!&quot;
putStrLn $
&quot;The word was: &quot; ++wordToGuess
exitSuccess
elsereturn()
Notice the way it’s written says you lose and exits the game
once you’ve guessed seven characters, even if the final (seventh)
guess is the final letter to fill into the word. There are, of course,
ways to modify that to make it more the way you’d expect a
hangman game to go, and we encourage you to play with that.
Next we need to provide a way to exit after winning the
game. We showed you how the combination of isJust andall
works earlier in the chapter, and you can see that in action
here. Recall that our puzzle word is a list of Maybevalues, so
when each character is represented by a Just Char rather than
aNothing , you win the game and we exit:</p>
<p>CHAPTER 13. BUILDING PROJECTS 809
gameWin ::Puzzle-&gt;IO()
gameWin (Puzzle_filledInSoFar _)=
ifall isJust filledInSoFar then
doputStrLn &quot;You win!&quot;
exitSuccess
elsereturn()
Next is the instruction for running a game. Here we use
forever so that this will execute this series of actions indef-
initely:
runGame ::Puzzle-&gt;IO()
runGame puzzle=forever $ do
gameOver puzzle
gameWin puzzle
putStrLn $
&quot;Current puzzle is: &quot; ++show puzzle
putStr&quot;Guess a letter: &quot;
guess&lt;-getLine
caseguessof
[c]-&gt;handleGuess puzzle c &gt;&gt;=runGame
_ -&gt;
putStrLn &quot;Your guess must <br />
\be a single character&quot;
And, finally, here is mainbringing everything together: it</p>
<p>CHAPTER 13. BUILDING PROJECTS 810
gets a word from the word list we generated, generates a fresh
puzzle, and then executes the runGame actions we saw above,
until such time as you guess all the characters in the word
correctly or have made seven guesses, whichever comes first:
indexmain@ main
main::IO()
main= do
word&lt;-randomWord'
letpuzzle=
freshPuzzle (fmap toLower word)
runGame puzzle
13.13 Adding a newtype
Another way you could modify your code in the above and
gain, perhaps, more clarity in places is with the use of newtype:
-- replace this type synonym
-- type WordList = [String]
newtype WordList =
WordList [String]
deriving (Eq,Show)</p>
<p>CHAPTER 13. BUILDING PROJECTS 811
allWords ::IOWordList
allWords = do
dict&lt;-readFile &quot;data/dict.txt&quot;
return$WordList (lines dict)
gameWords ::IOWordList
gameWords = do
(WordList aw)&lt;-allWords
return$WordList (filter gameLength aw)
wheregameLength w =
letl=length (w ::String)
inl&gt;minWordLength
&amp;&amp;l&lt;maxWordLength
randomWord ::WordList -&gt;IOString
randomWord (WordList wl)= do
randomIndex &lt;-
randomRIO ( 0, (length wl) -1)
return$wl!!randomIndex
13.14 Chapter exercises
Hangman game logic
You may have noticed when you were playing with the hang-
man game, that there are some weird things about its game</p>
<p>CHAPTER 13. BUILDING PROJECTS 812
logic:
•although it can play with words up to 9 characters long,
you only get to guess 7 characters;
•it ends the game after 7 guesses, whether they were correct
or incorrect;
•if your 7th guess supplies the last letter in the word, it may
still tell you you lost;
•it picks some very strange words that you didn’t suspect
were even in the dictionary.
These make it unlike hangman as you might have played it
in the past. Ordinarily, only incorrect guesses count against
you, so you can make as many correct guesses as you need
to fill in the word. Modifying the game so that it either gives
you more guesses before the game ends or only uses shorter
words (or both) involves only a couple of uncomplicated steps.
A bit more complicated but worth attempting as an exercise
is changing the game so that, as with normal hangman, only
incorrect guesses count towards the guess limit.
Modifying code
1.Ciphers: Open your Ciphers module and modify it so that
the Caesar and Vigenère ciphers work with user input.</p>
<p>CHAPTER 13. BUILDING PROJECTS 813
2.Here is a very simple, short block of code. Notice it has
aforever that will make it keep running, over and over
again. Load it into your REPL and test it out. Then refer
back to the chapter and modify it to exit successfully after
a False result.
importControl.Monad
palindrome ::IO()
palindrome =forever $ do
line1&lt;-getLine
case(line1==reverse line1) of
True-&gt;putStrLn &quot;It's a palindrome!&quot;
False-&gt;putStrLn &quot;Nope!&quot;
3.If you tried using palindrome on a sentence such as “Madam
I’mAdam,”youmayhavenoticedthatpalindromechecker
doesn’t work on that. Modifying the above so that it works
on sentences, too, involves several steps. You may need
to refer back to previous examples in the chapter to get
ideas for proper ordering and nesting. You may wish to
import Data.Char to use the function toLower . Have fun.</p>
<p>CHAPTER 13. BUILDING PROJECTS 814
4.typeName=String
typeAge=Integer
dataPerson=PersonNameAgederiving Show
dataPersonInvalid =
NameEmpty
|AgeTooLow
|PersonInvalidUnknown String
deriving (Eq,Show)
mkPerson ::Name
-&gt;Age
-&gt;EitherPersonInvalid Person
mkPerson name age
|name/=&quot;&quot;&amp;&amp;age&gt;0=
Right$Personname age
|name==&quot;&quot;=LeftNameEmpty
|not (age &gt;0)=LeftAgeTooLow
|otherwise =
Left$PersonInvalidUnknown $
&quot;Name was: &quot; ++show name ++
&quot; Age was: &quot; ++show age
Your job is to write the following function without modi-</p>
<p>CHAPTER 13. BUILDING PROJECTS 815
fying the code above.
gimmePerson ::IO()
gimmePerson =undefined
SinceIO ()is about the least informative type imaginable,
we’ll tell what it should do.
a)It should prompt the user for a name and age input.
b)It should attempt to construct a Person value using
the name and age the user entered. You’ll need the
readfunction for Age because it’s an Integer rather
than a String.
c)If it constructed a successful person, it should print
”Yay! Successfully got a person:” followed by the Per-
son value.
d)If it got an error value, report that an error occurred
and print the error.
13.15 Follow-up resources
1.Stack
https://github.com/commercialhaskell/stack
2.How I Start: Haskell
http://bitemyapp.com/posts/2014-11-18-how-i-start-haskell.
html</p>
<p>CHAPTER 13. BUILDING PROJECTS 816
3.Cabal FAQ
https://www.haskell.org/cabal/FAQ.html
4.Cabal user’s guide
https://www.haskell.org/cabal/users-guide/
5.A Gentle Introduction to Haskell, Modules chapter.
https://www.haskell.org/tutorial/modules.html</p>
<p>Chapter 14
Testing
We’ve tended to forget
that no computer will
ever ask a new question.
Grace Murray Hopper
817</p>
<p>CHAPTER 14. TESTING 818
14.1 Testing
This chapter, likethe one before it, is more focused on practical
matters rather than writing Haskell code per se. We will be
covering two testing libraries (there are others) and how and
when to use them. You will not be writing much of the code
in the chapter on your own; instead, please follow along by
entering it into files as directed (you will learn more if you
type rather than copy and paste). At the end of the chapter,
there are a number of exercises that ask you to write your own
tests for practice.
Testing is a core part of the working programmer’s toolkit,
and Haskell is no exception. Well-specified types can enable
programmers to avoid many obvious and tedious tests that
mightotherwisebenecessarytomaintaininuntypedprogram-
ming languages, but there’s still a lot of value to be obtained
in executable specifications. This chapter will introduce you
to testing methods for Haskell.
This chapter will cover:
•the whats and whys of testing;
•using the testing libraries HspecandQuickCheck ;
•a bit of fun with Morse code.</p>
<p>CHAPTER 14. TESTING 819
14.2 A quick tour of testing for the
uninitiated
When we write Haskell, we rely on the compiler to judge for
us whether our code is well formed. That prevents a great
number of errors, but it does not prevent them all. It is still
possible to write well-typed code that doesn’t perform as ex-
pected, and runtime errors can still occur. That’s where testing
comes in.
In general, tests allow you to state an expectation and then
verify that the result of an operation meets that expectation.
They allow you to verify that your code will do what you want
when executed.
For the sake of simplicity, we’ll say there are two broad cate-
gories of testing: unit testing and property testing. Unit testing
tests the smallest atomic units of software independently of
one another. Unit testing allows the programmer to check that
each function is performing the task it is meant to do. You
assert that when the code runs with a specified input, the result
is equal to the result you want.
Spec testing is a somewhat newer version of unit testing.
Like unit testing, it tests specific functions independently and
asksyoutoassertthat, whengiventhedeclaredinput, theresult
of the operation will be equal to the desired result. When you
run the test, the computer checks that the expected result is</p>
<p>CHAPTER 14. TESTING 820
equal to the actual result and everyone moves on with their day.
Some people prefer spec testing to unit testing because spec
testing is more often written in terms of assertions that are in
human-readable language. This can be especially valuable if
nonprogrammers need to be able to read and interpret the
results of the tests — they can read the English-language results
of the tests and, in some cases, write tests themselves.
Haskell provides libraries for both unit and spec testing.
We’ll focus on specification testing with the hspeclibrary in
this chapter, but HUnitis also available. One limitation to unit
and spec testing is that they test atomic units of code indepen-
dently, so they do not verify that all the pieces work together
properly.
Property testing is a diﬀerent beast. This kind of testing
was pioneered in Haskell because the type system and straight-
forward logic of the language lend themselves to property
tests, but it has since been adopted by other languages as well.
Property tests test the formal properties of programs without
requiring formal proofs by allowing you to express a truth-
valued, universally quantified (that is, will apply to all cases)
function — usually equality — which will then be checked
against randomly generated inputs.
The inputs are generated randomly by the standard func-
tions inside the QuickCheck library we use for property testing.
This relies on the type system to know what kinds of data to
generate. The default setting is for 100 inputs to be generated,</p>
<p>CHAPTER 14. TESTING 821
giving you 100 results. If it fails any one of these, then you
know your program doesn’t have the specified property. If
it passes, you can’t be positive it will never fail because the
data are randomly generated — there could be a weird edge
case out there that will cause your software to fail. QuickCheck is
cleverly written to be as thorough as possible and will usually
check the most common edge cases (for example, empty lists
and the maxBound andminBound s of the types in question, where
appropriate). You can also change the setting so that it runs
more tests.
Property testing is fantastic for ensuring that you’ve met
the minimum requirements to satisfy laws, such as the laws
of monads or basic associativity. It is not appropriate for all
programs, though, as it is not useful for times when there are
no assertable, truth-valued properties of the software.
14.3 Conventional testing
We are going to use the library hspec1to demonstrate a test
case, but we’re not going to explain hspecdeeply. The current
chapter will equip you with a means of writing tests for your
code later, but it’s not necessary to understand the details of
how the library works to do that. Some of the concepts hspec
leans on, such as functor, applicative, and monad, are covered</p>
<p>CHAPTER 14. TESTING 822
later as independent concepts.
First, let’s come up with a test case for addition. Generally
we want to make a Cabal project, even for small experiments.
Having a permanent project for experiments can eliminate
some of this overhead, but we’ll assume you haven’t done this
yet and start a small project:
-- addition.cabal
name: addition
version: 0.1.0.0
license-file: LICENSE
author: Chicken Little
maintainer: sky@isfalling.org
category: Text
build-type: Simple
cabal-version: &gt;=1.10
library
exposed-modules: Addition
ghc-options: -Wall -fwarn-tabs
build-depends: base &gt;=4.7 &amp;&amp; &lt;5
, hspec
hs-source-dirs: .
default-language: Haskell2010
1http://hackage.haskell.org/package/hspec</p>
<p>CHAPTER 14. TESTING 823
Note we’ve specified the hspecdependency, but not a version
range for it. You’ll probably want whatever the newest version
of it is but can probably get away with not specifying it for
now.
Next we’ll make the Addition module (exposed-modules) in
thesamedirectoryasourCabalfile. Thisiswhythe hs-source-dirs
option in the library stanza was set to .— this is the convention
for referring to the current directory.
For now, we’ll write a simple placeholder function to make
sure everything’s working:
-- Addition.hs
moduleAddition where
sayHello ::IO()
sayHello =putStrLn &quot;hello!&quot;
Then you can create an empty LICENSE file so the build
doesn’t complain:
$ touch LICENSE
Your local project directory should look like this now, before
having run any Stack commands:
$ tree
.</p>
<p>CHAPTER 14. TESTING 824
├── Addition.hs
└── addition.cabal
└── LICENSE
The next steps are to initialize the Stack file for describing
what snapshot of Stackage we’ll use:
$ stack init
Then we’ll want to build our project which will also install
the dependencies we need:
$ stack build
If that succeeded, let’s fire up a REEEEEEEPL and see if we
can call sayHello :
$ stack ghci
[some noise about configuring, loading packages, etc.]
Ok, modules loaded: Addition.
Prelude&gt; sayHello
hello!
If you got here, you’ve got a working test bed for making a
simple test case in hspec!</p>
<p>CHAPTER 14. TESTING 825
Truth according to Hspec
Next we’ll add the import of hspec’s primary module:
moduleAddition where
importTest.Hspec
sayHello ::IO()
sayHello =putStrLn &quot;hello!&quot;
Note that allof your imports must occur after the module
has been declared and before any expressions have been de-
fined in your module. You may have encountered an error or
a mistake might’ve been made. Here are a couple of examples.
moduleAddition where
sayHello ::IO()
sayHello =putStrLn &quot;hello!&quot;
importTest.Hspec
Here we put an import after at least one declaration. The
compiler parser doesn’t have a means of recognizing this spe-
cific mistake, so it can’t tell you properly what the error is:</p>
<p>CHAPTER 14. TESTING 826
Prelude&gt; :r
[1 of 1] Compiling Addition
Addition.hs:7:1: parse error on input ‘import’
Failed, modules loaded: none.
What else may have gone wrong? Well, we might have the
package hspecinstalled, but not included in our build-depends
for our project. Note you’ll need to quit and reopen the REPL
if you’ve made any changes to your .cabal file to reproduce
this error or fixed a mistake:
$ stack build
{... noise ...}
Could not find module ‘Test.Hspec’
It is a member of the hidden package
‘hspec-2.2.3@hspec_JWyjr3DNMsw1kiPzf88M5w’.
Perhaps you need to add ‘hspec’ to the
build-depends in your .cabal file.
Use -v to see a list of the files searched for.
{... other noise ...}
Process exited with code: ExitFailure 1</p>
<p>CHAPTER 14. TESTING 827
If you changed anything in order to test these error modes,
you’ll need to add hspecback to your build-depends and reinstall
it. Ifhspecis listed in your dependencies, stack build will set
you right.
Assuming everything is in order and Test.Hspec is being
imported, we can do a little exploration. We can use the :browse
command to get a listing of types from a module and get a
thousand-foot-view of what it oﬀers:
Prelude&gt; :browse Test.Hspec
context :: String -&gt; SpecWith a -&gt; SpecWith a
example :: Expectation -&gt; Expectation
specify :: Example a =&gt; String -&gt; a -&gt; SpecWith (Arg a)
(... list goes on for awhile ..)
Prelude&gt;
:browse is more useful when you already have some famil-
iarity with the library and how it works. When you’re using
an unfamiliar library, documentation is easier to digest. Good
documentation explains how important pieces of the library
work and gives examples of their use. This is especially valu-
able when encountering new concepts. As it happens, hspec
has some pretty good documentation at their website.2
2http://hspec.github.io/</p>
<p>CHAPTER 14. TESTING 828
Our first Hspec test
Let’s add a test assertion to our module now. If you glance
at the documentation, you’ll see that our example isn’t very
interesting, but we’ll make it somewhat more interesting soon:
moduleAddition where
importTest.Hspec
main::IO()
main=hspec$ do
describe &quot;Addition&quot; $ do
it&quot;1 + 1 is greater than 1&quot; $ do
(1+1)&gt;1<code>shouldBe</code> True
We’ve asserted in both English and code that (1 + 1) should
be greater than 1, and that is what hspecwill test for us. You
may recognize the donotation from the previous chapter. As
we said then, this syntax allows us to sequence monadic actions.
In the previous chapter, the monad in question was IO.
Here, we’re nesting multiple doblocks. The types of the do
blocks passed to hspec,describe , anditaren’tIO ()but some-
thing more specific to hspec. They result in IO ()in the end, but
there are other monads involved. We haven’t covered monads
yet, and this works fine without understanding precisely how
it works, so let’s just roll with it for now.</p>
<p>CHAPTER 14. TESTING 829
Note that you’ll get warnings about the Num a =&gt; a literals
getting defaulted to Integer . You can ignore this or add explicit
type signatures, it is up to you. With the above code in place,
we can load or reload our module and run mainto see the test
results:
Prelude&gt; main
Addition
1 + 1 is greater than 1
Finished in 0.0041 seconds
1 example, 0 failures
OK, so what happened here? Basically, hspecruns your code
and verifies that the arguments you passed to shouldBe are
equal. Let’s look at the types:
shouldBe ::(Eqa,Showa)
=&gt;a-&gt;a-&gt;Expectation
-- contrast with
(==)::Eqa=&gt;a-&gt;a-&gt;Bool
In a sense, it’s an augmented ==embedded in hspec’s model
of the universe. It needs the Showinstance in order to render a</p>
<p>CHAPTER 14. TESTING 830
value. That is, the Showinstance allows hspecto show you the
result of the tests, not just return a Boolvalue.
Let’s add another test, one that reads a little diﬀerently:
main::IO()
main=hspec$ do
describe &quot;Addition&quot; $ do
it&quot;1 + 1 is greater than 1&quot; $ do
(1+1)&gt;1<code>shouldBe</code> True
it&quot;2 + 2 is equal to 4&quot; $ do
2+2<code>shouldBe</code> 4
Modify your describe block about Addition so that it looks
like the above and run it in the REPL:
Prelude&gt; main
Addition
1 + 1 is greater than 1
2 + 2 is equal to 4
Finished in 0.0004 seconds
2 examples, 0 failures
For fun, we’ll look back to something you wrote early in the
book and write a short hspectest for it. Back in the Recursion</p>
<p>CHAPTER 14. TESTING 831
chapter, we wrote our own division function that looked like
this:
dividedBy ::Integral a=&gt;a-&gt;a-&gt;(a, a)
dividedBy num denom =go num denom 0
wherego n d count
|n&lt;d=(count, n)
|otherwise =
go (n-d) d (count +1)
We want to test that to see that it works as it should. To keep
things simple, we added dividedBy to ourAddition.hs file and
then rewrote the hspectests that were already there. We want
to test that the function is both subtracting the correct number
of times and keeping an accurate count of that subtraction
and also that it’s telling us the correct remainder, so we’ll give
hspectwo things to test for:
main::IO()
main=hspec$ do
describe &quot;Addition&quot; $ do
it&quot;15 divided by 3 is 5&quot; $ do
dividedBy 153<code>shouldBe</code> ( 5,0)
it&quot;22 divided by 5 is <br />
\4 remainder 2&quot; $ do
dividedBy 225<code>shouldBe</code> ( 4,2)</p>
<p>CHAPTER 14. TESTING 832
That’s it. When we reload Addition.hs in our REPL, we can
test our division function:
*Addition&gt; main
Addition
15 divided by 3 is 5
22 divided by 5 is 4 remainder 2
Finished in 0.0012 seconds
2 examples, 0 failures
Hurrah! We can do arithmetic!
Intermission: Short Exercise
In the Chapter Exercises at the end of Recursion, you were
given this exercise:
Write a function that multiplies two numbers using recur-
sive summation. The type should be (Eq a, Num a) =&gt; a -&gt; a
-&gt; aalthough, depending on how you do it, you might also
consider adding an Ordconstraint.
If you still have your answer, great! If not, rewrite it and
then write hspectests for it.
The above examples demonstrate the basics of writing in-
dividual tests to test particular values. If you’d like to see a</p>
<p>CHAPTER 14. TESTING 833
more developed example, you could refer to Chris’s library,
Bloodhound.3
14.4 Enter QuickCheck
hspecdoes a nice job with spec testing, but we’re Haskell users
— we’re never satisfied!! hspeccan only prove something about
particular values. Can we get assurances that are stronger,
something closer to proofs? As it happens, we can.
QuickCheck was the first library to oﬀer what is today called
property testing. hspectesting is more like what is known
as unit testing — the testing of individual units of code —
whereas property testing is done with the assertion of laws or
properties.
First, we’ll need to add QuickCheck to ourbuild-depends . Open
your.cabal file and add it. Be sure to capitalize QuickCheck (un-
likehspec, which begins with a lowercase ℎ). It should already
be installed, as hspechasQuickCheck as a dependency, but you
may need to reinstall it ( stack build ). Then open a new stack
ghcisession.
hspechasQuickCheck integration out of the box, so once that
is done, add the following to your module:
3https://github.com/bitemyapp/bloodhound</p>
<p>CHAPTER 14. TESTING 834
-- with your imports
importTest.QuickCheck
-- to the same describe block as the others
it&quot;x + 1 is always <br />
\greater than x&quot; $ do
property $\x-&gt;x+1&gt;(x::Int)
If we had not asserted the type of 𝑥in the property test, the
compiler would not have known what concrete type to use,
and we’d see a message like this:
No instance for (Show a0) arising from a use of ‘property’
The type variable ‘a0’ is ambiguous
...
No instance for (Num a0) arising from a use of ‘+’
The type variable ‘a0’ is ambiguous
...
No instance for (Ord a0) arising from a use of ‘&gt;’
The type variable ‘a0’ is ambiguous
Avoid this by asserting a concrete type, for example, (x ::
Int), in the property.
Assuming all is well, when we run it, we’ll see something
like the following:
Prelude&gt; main</p>
<p>CHAPTER 14. TESTING 835
Addition
1 + 1 is greater than 1
2 + 2 is equal to 4
x + 1 is always greater than x
Finished in 0.0067 seconds
3 examples, 0 failures
What’s being hidden a bit by hspecis that QuickCheck tests
manyvalues to see if your assertions hold for all of them. It
does this by randomly generating values of the type you said
you expected. So, it’ll keep feeding our function random Int
values to see if the property is ever false. The number of tests
QuickCheck runs defaults to 100.
Arbitrary instances
QuickCheck relies on a typeclass called Arbitrary and anewtype
calledGenfor generating its random data.
arbitrary is a value of type Gen:
Prelude&gt; :t arbitrary
arbitrary :: Arbitrary a =&gt; Gen a
This is a way to set a default generator for a type. When
you use the arbitrary value, you have to specify the type to</p>
<p>CHAPTER 14. TESTING 836
dispatch the right typeclass instance, as types and typeclass
instances form unique pairings. But this is just a value. How
do we see a list of values of the correct type?
We can use sample andsample' from the Test.QuickCheck mod-
ule in order to see some random data:
-- this prints each value on a new line
Prelude&gt; :t sample
sample :: Show a =&gt; Gen a -&gt; IO ()
-- this one returns a list
Prelude&gt; :t sample'
sample' :: Gen a -&gt; IO [a]
TheIOis necessary because it’s using a global resource of
random values to generate the data. A common way to gener-
ate pseudorandom data is to have a function that, given some
input “seed” value, returns a value and another seed value for
generating a diﬀerent value. You can bind the two actions
together, as we explained in the last chapter, to pass a new seed
value each time and keep generating seemingly random data.
In this case, however, we’re not doing that. Here we’re using
IOso that our function that generates our data can return a
diﬀerent result each time (not something pure functions are
allowed to do) by pulling from a global resource of random
values. If this doesn’t make a great deal of sense at this point,</p>
<p>CHAPTER 14. TESTING 837
it will be more clear once we’ve covered monads, and even
more so once we cover IO.
We use the Arbitrary typeclass in order to provide a genera-
tor forsample. It isn’t a terribly principled typeclass, but it is
popular and useful for this. We say it is unprincipled because
it has no laws and nothing specific it’s supposed to do. It’s a
convenient way of plucking a canonical generator for Gen a
out of thin air without having to know where it comes from.
If it feels a bit like <em>MAGICK</em> at this point, that’s fine. It is, a
bit, and the inner workings of Arbitrary are not worth fussing
over right now.
As you’ll see later, this isn’t necessary if you have a Genvalue
ready to go already. Genis a newtype with a single type argu-
ment. It exists for wrapping up a function to generate pseudo-
random values. The function takes an argument that is usually
provided by some kind of random value generator to give you
a pseudorandom value of that type, assuming it’s a type that
has an instance of the Arbitrary typeclass.
And this is what we get when we use the sample functions.
We use the arbitrary value but specify the type, so that it gives
us a list of random values of that type:
Prelude&gt; sample (arbitrary :: Gen Int)
0
-2
-1</p>
<p>CHAPTER 14. TESTING 838
4
-3
4
2
4
-3
2
-4
Prelude&gt; sample (arbitrary :: Gen Double)
0.0
0.13712502861905426
2.9801894108743605
-8.960645064542609
4.494161946149201
7.903662448338119
-5.221729489254451
31.64874305324701
77.43118278366954
-539.7148886375935
26.87468214215407
If you run sample arbitrary directly in GHCi without speci-
fying a type, it will default the type to ()and give you a very
nice list of empty tuples. If you try loading an unspecified
sample arbitrary from a source file, though, you will get an af-
fectionate message from GHC about having an ambiguous</p>
<p>CHAPTER 14. TESTING 839
type. Try it if you like. GHCi has somewhat diﬀerent rules for
default types than GHC does.
We can specify our own data for generating Genvalues. In
this example, we’ll specify a trivial function that always returns
a1of type Int:
-- trivial generator of values
trivialInt ::GenInt
trivialInt =return1
You may remember return from the previous chapter as
well. Here, it providesan expedientwayto construct a function.
Inthelastchapter, wenotedthatitdoesn’tdoawholelotexcept
return a value inside of a monad. Before we were using it to
put a value into IObut it’s not limited to use with that monad:
return::Monadm=&gt;a-&gt;m a
-- when <code>m</code> is Gen:
return::a-&gt;Gena
Putting 1into the Genmonad constructs a generator that
always returns the same value, 1.
So, what happens when we sample data from this?</p>
<p>CHAPTER 14. TESTING 840
Prelude&gt; sample' trivialInt
[1,1,1,1,1,1,1,1,1,1,1]
Notice now our value isn’t arbitrary for some type, but the
trivialInt value we defined above. That generator always re-
turns1, so allsample' can return for us is a list of 1.
Let’s explore diﬀerent means of generating values:
oneThroughThree ::GenInt
oneThroughThree =elements [ 1,2,3]
Try loading that via your Addition module and asking for a
sample set of random oneThroughThree values:
*Addition&gt; sample' oneThroughThree
[2,3,3,2,2,1,2,1,1,3,3]
Yep, it gave us random values from only that limited set.
At this time, each number in that set has the same chance of
showing up in our random data set. We could tinker with
those odds by having a list with repeated elements to give
those elements a higher probability of showing up in each
generation:
oneThroughThree ::GenInt
oneThroughThree =
elements [ 1,2,2,2,2,3]</p>
<p>CHAPTER 14. TESTING 841
Try running sample' again with this set and see if you no-
tice the diﬀerence. You may not, of course, because due to
the nature of probability, there is at least some chance that
2wouldn’t show up any more than it did with the previous
sample.
Next we’ll use choose andelements from the QuickCheck library
as generators of values:
-- choose :: System.Random.Random a
-- =&gt; (a, a) -&gt; Gen a
-- elements :: [a] -&gt; Gen a
genBool ::GenBool
genBool =choose ( False,True)
genBool' ::GenBool
genBool' =elements [ False,True]
genOrdering ::GenOrdering
genOrdering =elements [ LT,EQ,GT]
genChar ::GenChar
genChar =elements [ 'a'..'z']
You should enter all these into your Addition module, load
them into your REPL, and play with getting lists of sample</p>
<p>CHAPTER 14. TESTING 842
data for each.
Our next examples are a bit more complex:
genTuple ::(Arbitrary a,Arbitrary b)
=&gt;Gen(a, b)
genTuple = do
a&lt;-arbitrary
b&lt;-arbitrary
return (a, b)
genThreeple ::(Arbitrary a,Arbitrary b,
Arbitrary c)
=&gt;Gen(a, b, c)
genThreeple = do
a&lt;-arbitrary
b&lt;-arbitrary
c&lt;-arbitrary
return (a, b, c)
Here’s how to use generators when they have polymor-
phic type arguments. Remember that if you leave the types
unspecified, the extended defaulting behavior of GHCi will
(helpfully?) pick ())for you. Outside of GHCi, you’ll get an
error about an ambiguous type — we covered some of this
when we explained typeclasses earlier:</p>
<p>CHAPTER 14. TESTING 843
Prelude&gt; sample genTuple
((),())
((),())
((),())
Here it’s defaulting the 𝑎and𝑏to(). We can get more
interesting output if we tell it what we expect 𝑎and𝑏to be.
Note it’ll always pick 0 and 0.0 for the first numeric values:
Prelude&gt; sample (genTuple :: Gen (Int, Float))
(0,0.0)
(-1,0.2516606)
(3,0.7800742)
(5,-61.62875)
We can ask for lists and characters, or anything with an
instance of the Arbitrary typeclass:
Prelude&gt; sample (genTuple :: Gen ([()], Char))
([],'\STX')
([()],'X')
([],'?')
([],'\137')
([(),()],'\DC1')
([(),()],'z')
You can use :info Arbitrary in your GHCi to see what in-
stances are available.</p>
<p>CHAPTER 14. TESTING 844
We can also generate arbitrary MaybeandEither values:
genEither ::(Arbitrary a,Arbitrary b)
=&gt;Gen(Eithera b)
genEither = do
a&lt;-arbitrary
b&lt;-arbitrary
elements [ Lefta,Rightb]
-- equal probability
genMaybe ::Arbitrary a=&gt;Gen(Maybea)
genMaybe = do
a&lt;-arbitrary
elements [ Nothing,Justa]
-- What QuickCheck does so
-- you get more Just values
genMaybe' ::Arbitrary a=&gt;Gen(Maybea)
genMaybe' = do
a&lt;-arbitrary
frequency [ ( 1, return Nothing)
, (3, return ( Justa))]
-- frequency :: [(Int, Gen a)] -&gt; Gen a
For now, you should play with this in the REPL; it will</p>
<p>CHAPTER 14. TESTING 845
become useful to know later on.
Using QuickCheck without Hspec
We can also use QuickCheck without hspec. In that case, we no
longer need to specify 𝑥in our expression, because the type
ofprop_additionGreater provides for it. Thus, we rewrite our
previous example as follows:
prop_additionGreater ::Int-&gt;Bool
prop_additionGreater x=x+1&gt;x
runQc::IO()
runQc=quickCheck prop_additionGreater
For now, we don’t need to worry about how runQcdoes its
work. It’s a generic function, like main, that signals that it’s time
to do stuﬀ. Specifically, in this case, it’s time to perform the
QuickCheck tests.
Now, when we run it in the REPL, instead of the mainwe were
calling with hspec, we’ll call runQc, which will call on QuickCheck
to test the property we defined. When we run QuickCheck di-
rectly, it reports how many tests it ran:
Prelude&gt; runQc
+++ OK, passed 100 tests.
What happens if we assert something untrue?</p>
<p>CHAPTER 14. TESTING 846
prop_additionGreater x=x+0&gt;x
Prelude&gt; :r
[1 of 1] Compiling Addition
Ok, modules loaded: Addition.
Prelude&gt; runQc
*** Failed! Falsifiable (after 1 test):
0
Conveniently, QuickCheck doesn’t only tell us that our test
failed, but it tells us the first input it encountered that it failed
on. If you try to keep running it, you may notice that the
value that it fails on is always 0. A while ago, we said that
QuickCheck has some built-in cleverness and tries to ensure that
common error boundaries will always get tested. The input 0
is a frequent point of failure, so QuickCheck tries to ensure that
it is always tested (when appropriate, given the types, etc etc).
14.5 Morse code
In the interest of playing with testing, we’ll work through an
example project where we translate text to and from Morse
code. We’re going to start a new project for this. When you
do usestack new project-name to start a new project instead of
stack init for an existing project, it automatically generates a
file called Setup.hs that looks like this:</p>
<p>CHAPTER 14. TESTING 847
importDistribution.Simple
main=defaultMain
This isn’t terribly important. You rarely need to modify
or do anything at all with the Setup.hs file, and usually you
shouldn’t touch it at all. Occasionally, you may need to edit it
for certain tasks, so it is good to recognize that it’s there.
Next, as always, let’s get our .cabal file configured properly.
Some of this will be automatically generated by your stack new
project-name , but you’ll have to add to what it generates, being
careful about things like capitalization and indentation:
name: morse
version: 0.1.0.0
license-file: LICENSE
author: Chris Allen
maintainer: cma@bitemyapp.com
category: Text
build-type: Simple
cabal-version: &gt;=1.10
library
exposed-modules: Morse
ghc-options: -Wall -fwarn-tabs
build-depends: base &gt;=4.7 &amp;&amp; &lt;5
, containers</p>
<p>CHAPTER 14. TESTING 848
, QuickCheck
hs-source-dirs: src
default-language: Haskell2010
executable morse
main-is: Main.hs
ghc-options: -Wall -fwarn-tabs
hs-source-dirs: src
build-depends: base &gt;=4.7 &amp;&amp; &lt;5
, containers
, morse
, QuickCheck
default-language: Haskell2010
test-suite tests
ghc-options: -Wall -fno-warn-orphans
type: exitcode-stdio-1.0
main-is: tests.hs
hs-source-dirs: tests
build-depends: base
, containers
, morse
, QuickCheck
default-language: Haskell2010
Don’t forget to capitalize the QuickCheck dependency prop-</p>
<p>CHAPTER 14. TESTING 849
erly! Now that is set up and ready for us, so the next step is
to make our srcdirectory and the file called Morse.hs as our
“exposed module:”
-- src/Morse.hs
moduleMorse
(Morse
,charToMorse
,morseToChar
,stringToMorse
,letterToMorse
,morseToLetter
)where
import qualified Data.Map asM
typeMorse=String
Whoa, there — what’s all that stuﬀ after the module name?
That is a list of everything this module will export. We talked
a bit about this in the previous chapter, but didn’t make use of
it. In the hangman game, we had all our functions in one file,
so nothing needed to be exported.</p>
<p>CHAPTER 14. TESTING 850
Nota bene You don’t have to specify exports in this manner.
By default, the entire module is exposed and can be imported
by any other module. If you want to export everything in a
module, then specifying exports is unnecessary. However, it
can help, when managing large projects, to specify what will
get used by another module (and, by exclusion, what will not)
as a way of documenting your intent. In this case, we have
exported here more than we imported into Main, as we realized
that we only needed the two specified functions for Main. We
could go back and remove the things we didn’t specifically
import from the above export list, but we haven’t now, to give
you an idea of the process we’re going through putting our
project together.
Turning words into code
We are also using a qualified import of Data.Map . We covered
this type of import somewhat in the previous chapter. We
qualify the import and name it 𝑀so that we can use that 𝑀
as a prefix for the functions we’re using from that package.
That will help us keep track of where the functions came from
and also avoid same-name clashes with Prelude functions, but
without requiring us to tediously type Data.Map as a prefix to
each function name.
We’ll talk more about Mapas a data structure later in the book.
For now, we can understand it as being a balanced binary tree,</p>
<p>CHAPTER 14. TESTING 851
where each node is a pairing of a key and a value. The key is
an index for the value — a marker of how to find the value
in the tree. The key must be orderable (that is, must have an
Ordinstance), much like our binary tree functions earlier, such
asinsert , needed an Ordinstance. Maps can be more efficient
than lists because you do not have to search linearly through
a bunch of data. Because the keys are ordered and the tree is
balanced, searching through the binary tree divides the search
space in half each time you go “left” or “right.” You compare
the key to the index of the current node to determine if you
need to go left (less), right (greater), or if you’ve arrived at the
node for your value (equal).
You can see below why we used a Mapinstead of a simple list.
We want to make a list of pairs, where each pair includes both
the English-language character and its Morse code represen-
tation. We define our transliteration table thus:
letterToMorse ::(M.MapCharMorse)
letterToMorse =M.fromList [
('a',&quot;.-&quot;)
, ('b',&quot;-...&quot;)
, ('c',&quot;-.-.&quot;)
, ('d',&quot;-..&quot;)
, ('e',&quot;.&quot;)</p>
<p>CHAPTER 14. TESTING 852
, ('f',&quot;..-.&quot;)
, ('g',&quot;--.&quot;)
, ('h',&quot;....&quot;)
, ('i',&quot;..&quot;)
, ('j',&quot;.---&quot;)
, ('k',&quot;-.-&quot;)
, ('l',&quot;.-..&quot;)
, ('m',&quot;--&quot;)
, ('n',&quot;-.&quot;)
, ('o',&quot;---&quot;)
, ('p',&quot;.--.&quot;)
, ('q',&quot;--.-&quot;)
, ('r',&quot;.-.&quot;)
, ('s',&quot;...&quot;)
, ('t',&quot;-&quot;)
, ('u',&quot;..-&quot;)
, ('v',&quot;...-&quot;)
, ('w',&quot;.--&quot;)
, ('x',&quot;-..-&quot;)
, ('y',&quot;-.--&quot;)
, ('z',&quot;--..&quot;)
, ('1',&quot;.----&quot;)
, ('2',&quot;..---&quot;)</p>
<p>CHAPTER 14. TESTING 853
, ('3',&quot;...--&quot;)
, ('4',&quot;....-&quot;)
, ('5',&quot;.....&quot;)
, ('6',&quot;-....&quot;)
, ('7',&quot;--...&quot;)
, ('8',&quot;---..&quot;)
, ('9',&quot;----.&quot;)
, ('0',&quot;-----&quot;)
]
Note that we used M.fromList — the𝑀prefix tells us this
comes from Data.Map . We’re using a Mapto associate characters
with their Morse code representations. letterToMorse is the def-
inition of the Mapwe’ll use to look up the codes for individual
characters.
Next we write a few functions that allow us to convert a
Morse character to an English character and vice versa, and
also functions to do the same for strings:</p>
<p>CHAPTER 14. TESTING 854
morseToLetter ::M.MapMorseChar
morseToLetter =
M.foldWithKey (flip M.insert) M.empty
letterToMorse
charToMorse ::Char-&gt;MaybeMorse
charToMorse c=
M.lookup c letterToMorse
stringToMorse ::String-&gt;Maybe[Morse]
stringToMorse s=
sequence $fmap charToMorse s
morseToChar ::Morse-&gt;MaybeChar
morseToChar m=
M.lookup m morseToLetter
Notice we used Maybein three of those: not every Charthat
could potentially occur in a String has a Morse representation.
The Main event
Next we want to set up a Mainmodule that will handle our
Morse code conversions. Note that it’s going to import a bunch
of things, some of which we covered in the last chapter and
some we have not. Since we will not be going into the specifics</p>
<p>CHAPTER 14. TESTING 855
of how this code works, we won’t discuss those imports here.
It is, however, important to note that one of our imports is our
Morse.hs module from above:
-- src/Main.hs
moduleMainwhere
importControl.Monad (forever,when)
importData.List (intercalate )
importData.Traversable (traverse )
importMorse(stringToMorse ,morseToChar )
importSystem.Environment (getArgs)
importSystem.Exit (exitFailure ,
exitSuccess )
importSystem.IO (hGetLine ,hIsEOF,stdin)
As we said, we’re not going to explain this part in detail.
We encourage you to do your best reading and interpreting
it, but it’s quite dense, and this chapter isn’t about this code
— it’s about the tests. We’re cargo-culting a bit here, which
we don’t like to do, but we’re doing it so that we can focus on
the testing. Type this all into your Mainmodule — first the
function to convert to Morse:</p>
<p>CHAPTER 14. TESTING 856
convertToMorse ::IO()
convertToMorse =forever $ do
weAreDone &lt;-hIsEOF stdin
when weAreDone exitSuccess
-- otherwise, proceed.
line&lt;-hGetLine stdin
convertLine line
where
convertLine line = do
letmorse=stringToMorse line
casemorseof
(Juststr)
-&gt;putStrLn
(intercalate &quot; &quot;str)
Nothing
-&gt; do
putStrLn $&quot;ERROR: &quot; ++line
exitFailure
Now add the function to convert from Morse:</p>
<p>CHAPTER 14. TESTING 857
convertFromMorse ::IO()
convertFromMorse =forever $ do
weAreDone &lt;-hIsEOF stdin
when weAreDone exitSuccess
-- otherwise, proceed.
line&lt;-hGetLine stdin
convertLine line
where
convertLine line = do
letdecoded ::MaybeString
decoded =
traverse morseToChar
(words line)
casedecoded of
(Justs)-&gt;putStrLn s
Nothing -&gt; do
putStrLn $&quot;ERROR: &quot; ++line
exitFailure
And now our obligatory main:</p>
<p>CHAPTER 14. TESTING 858
main::IO()
main= do
mode&lt;-getArgs
casemodeof
[arg]-&gt;
caseargof
&quot;from&quot;-&gt;convertFromMorse
&quot;to&quot;-&gt;convertToMorse
_ -&gt; argError
_ -&gt;argError
whereargError = do
putStrLn &quot;Please specify the <br />
\first argument <br />
\as being 'from' or <br />
'to' morse, <br />
\such as: morse to&quot;
exitFailure
Make sure it’s all working
One way we can make sure everything is working for us from
the command line is by using echo. If this is familiar to you
and you feel comfortable with this, go ahead and try this:
$ echo &quot;hi&quot; | stack exec morse to</p>
<p>CHAPTER 14. TESTING 859
.... ..
$ echo &quot;.... ..&quot; | stack exec morse from
hi
If you’d like to find out where Stack put the executable, you
can use stack exec which morse on Mac and Linux. You can also
usestack install to ask Stack to build (if needed) and copy the
binaries from your project into a common directory. On Mac
and Linux that will be .local/bin in your home directory. The
location was chosen partly to respect XDG4guidelines.
Otherwise, load this module into your GHCi REPL and
give it a try to ensure everything compiles and seems to be in
working order. It’ll be helpful to fix any type or syntax errors
now, before we start trying to run the tests.
Time to test!
Now we need to write our test suite. We have those in their
own directory and file. We will again call the module Main
but note the file name (the name per se isn’t important, but
it must agree with the test file you have named in your Cabal
configuration for this project):
4https://wiki.archlinux.org/index.php/Xdg_user_directories</p>
<p>CHAPTER 14. TESTING 860
-- tests/tests.hs
moduleMainwhere
import qualified Data.Map asM
importMorse
importTest.QuickCheck
We have many fewer imports for this, which should all
already be familiar to you.
Now we set up our generators for ensuring that the random
valuesQuickCheck uses to test our program are sensible for our
Morse code program:
allowedChars ::[Char]
allowedChars =M.keys letterToMorse
allowedMorse ::[Morse]
allowedMorse =M.elems letterToMorse
charGen ::GenChar
charGen =elements allowedChars
morseGen ::GenMorse
morseGen =elements allowedMorse</p>
<p>CHAPTER 14. TESTING 861
We saw elements briefly above. It takes a list of some type
— in these cases, our lists of allowed characters and Morse
characters — and chooses a Genvalue from the values in that
list. Because Charincludes thousands of characters that have
no legitimate equivalent in Morse code, we need to write our
own custom generators.
Now we write up the property we want to check. We want
to check that when we convert something to Morse code and
then back again, it comes out as the same string we started out
with:
prop_thereAndBackAgain ::Property
prop_thereAndBackAgain =
forAll charGen
(\c-&gt;((charToMorse c)
&gt;&gt;=morseToChar) ==Justc)
main::IO()
main=quickCheck prop_thereAndBackAgain
This is how your setup should look when you have all this
done:
$ tree
.
├── LICENSE</p>
<p>CHAPTER 14. TESTING 862
├── Setup.hs
├── morse.cabal
├── src
│ ├── Main.hs
│ └── Morse.hs
├── stack.yaml
└── tests
└── tests.hs
Testing the Morse code
Now that our conversions seem to be working, let’s run our
tests to make sure. The property we’re testing is that we get the
same string after we convert it to Morse and back again. Let’s
load up our tests by opening a REPL from our main project
directory:
$ stack ghci morse:tests
{... noise noise noise ...}
Ok, modules loaded: Main.
Prelude&gt;
Sweet. Stack loaded everything for us and even built our
dependencies if needs be. Let’s see what happens:</p>
<p>CHAPTER 14. TESTING 863
Prelude&gt; main
+++ OK, passed 100 tests.
The test generates 100 random Morse code conversions
(a bunch of random strings) and makes sure they are always
equal once you have converted to and then from Morse code.
This gives you a pretty strong assurance that your program is
correct and will perform as expected for any input value.
14.6 Arbitrary instances
One of the more important parts of QuickCheck is learning to
write instances of the Arbitrary typeclass for your datatypes.
It’s a somewhat unfortunate but still necessary convenience
for your code to integrate cleanly with QuickCheck code. It’s
initially a bit confusing for beginners because it compacts a
few diﬀerent concepts and solutions to problems into a single
typeclass.
Babby’s First Arbitrary
First, we’ll begin with a maximally simple Arbitrary instance
for theTrivial datatype:</p>
<p>CHAPTER 14. TESTING 864
moduleMainwhere
importTest.QuickCheck
dataTrivial =
Trivial
deriving (Eq,Show)
trivialGen ::GenTrivial
trivialGen =
returnTrivial
instance Arbitrary Trivial where
arbitrary =trivialGen
Thereturn is necessary to return Trivial in theGenmonad:
main::IO()
main= do
sample trivialGen
Let’s take a sample:
Prelude&gt; sample trivialGen
Trivial
Trivial
Trivial</p>
<p>CHAPTER 14. TESTING 865
Trivial
Trivial
Trivial
Trivial
Trivial
Trivial
Trivial
Trivial
Although it’s impossible to see the point with Trivial by it-
self,Genvalues are generators of random values that QuickCheck
uses to get test values from.
Identity Crisis
This one is a little diﬀerent. It will produce random values
even if the Identity structure itself doesn’t and cannot vary.
dataIdentity a=
Identity a
deriving (Eq,Show)
identityGen ::Arbitrary a=&gt;
Gen(Identity a)
identityGen = do
a&lt;-arbitrary
return ( Identity a)</p>
<p>CHAPTER 14. TESTING 866
We’re using the Genmonad to pluck a single value of type
𝑎out of the air, embed it in Identity , then return as part of
theGenmonad. We know this is weird, but if you do it ten or
twenty times you might start to like it.
We’ll reuse the original identityGen we wrote. We can make
it the default generator for the Identity type by making it the
arbitrary value in the Arbitrary instance:
instance Arbitrary a=&gt;
Arbitrary (Identity a)where
arbitrary =identityGen
identityGenInt ::Gen(Identity Int)
identityGenInt =identityGen
We’re making a generator suitable for sampling by making
the type argument of Identity unambiguous for testing with
thesample function. Your output in the terminal could look
something like:
Prelude&gt; sample identityGenInt
Identity 0
Identity (-1)
Identity 2
Identity 4
Identity (-3)</p>
<p>CHAPTER 14. TESTING 867
Identity 5
Identity 3
Identity (-1)
Identity 12
Identity 16
Identity 0
You should be able to change the concrete type of Identity ’s
type argument and generate diﬀerent types of sample values.
Arbitrary Products
Arbitrary instances for product types get a teensy bit more
interesting, but they’re really an extension of what we did for
Identity :
dataPaira b=
Paira b
deriving (Eq,Show)
pairGen ::(Arbitrary a,
Arbitrary b)=&gt;
Gen(Paira b)
pairGen = do
a&lt;-arbitrary
b&lt;-arbitrary
return ( Paira b)</p>
<p>CHAPTER 14. TESTING 868
We will reuse our pairGen function as the arbitrary value in
the instance:
instance (Arbitrary a,
Arbitrary b)=&gt;
Arbitrary (Paira b)where
arbitrary =pairGen
pairGenIntString ::Gen(PairIntString)
pairGenIntString =pairGen
And now we can generate some sample values:
Pair 0 &quot;&quot;
Pair (-2) &quot;&quot;
Pair (-3) &quot;26&quot;
Pair (-5) &quot;B\NUL\143:\254\SO&quot;
Pair (-6) &quot;\184*\239\DC4&quot;
Pair 5 &quot;\238\213=J\NAK!&quot;
Pair 6 &quot;Pv$y&quot;
Pair (-10) &quot;G|J^&quot;
Pair 16 &quot;R&quot;
Pair (-7) &quot;(&quot;
Pair 19 &quot;i\ETX]\182\ENQ&quot;
Ah, the beauty of random String values.</p>
<p>CHAPTER 14. TESTING 869
Greater than the sum of its parts
Writing Arbitrary instances for sum types is a bit more inter-
esting still. First, make sure the following is included in your
imports:
importTest.QuickCheck.Gen (oneof)
Sum types represent disjunction, so with a sum type like
Sum, we need to represent the exclusive possibilities in our Gen.
One way to do that is to pull out as many arbitrary values
as you require for the cases of your sum type. We have two
data constructors in this sum type, so we’ll want two arbitrary
values. Then we’ll repack them into Genvalues, resulting in a
value of type [Gen a] that can be passed to oneof:</p>
<p>CHAPTER 14. TESTING 870
dataSuma b=
Firsta
|Secondb
deriving (Eq,Show)
-- equal odds for each
sumGenEqual ::(Arbitrary a,
Arbitrary b)=&gt;
Gen(Suma b)
sumGenEqual = do
a&lt;-arbitrary
b&lt;-arbitrary
oneof [return $Firsta,
return$Secondb]
Theoneoffunction will create a Gen afrom a list of Gen aby
giving each value an equal probability. From there, you’re
delegating to the Arbitrary instances of the types 𝑎and𝑏.
sumGenCharInt ::Gen(SumCharInt)
sumGenCharInt =sumGenEqual
We specify which Arbitrary instances to use for 𝑎and𝑏and
do a test run:
Prelude&gt; sample sumGenCharInt</p>
<p>CHAPTER 14. TESTING 871
First 'P'
First '\227'
First '\238'
First '.'
Second (-3)
First '\132'
Second (-12)
Second (-12)
First '\186'
Second (-11)
First '\v'
Where sum types get even more interesting is that you can
choose a diﬀerent weighting of probabilities than an equal dis-
tribution. Consider this snippet of the Maybe Arbitrary instance
from the QuickCheck library:
instance Arbitrary a=&gt;
Arbitrary (Maybea)where
arbitrary =
frequency [( 1, return Nothing),
(3, liftM Justarbitrary)]
It’s making an arbitrary Justvalue three times more likely
than aNothing value because the former is more likely to be
interesting and useful, but you still want to try shaking things
out with a Nothing from time to time.</p>
<p>CHAPTER 14. TESTING 872
Accordingly, we can assign a 10 times higher probability to
ourFirstdata constructor in a diﬀerent GenforSum:
sumGenFirstPls ::(Arbitrary a,
Arbitrary b)=&gt;
Gen(Suma b)
sumGenFirstPls = do
a&lt;-arbitrary
b&lt;-arbitrary
frequency [( 10, return $Firsta),
(1, return $Secondb)]
sumGenCharIntFirst ::Gen(SumCharInt)
sumGenCharIntFirst =sumGenFirstPls
With that modified version, you’ll find Second values are
much less common:
First '\208'
First '\242'
First '\159'
First 'v'
First '\159'
First '\232'
First '3'
First 'l'</p>
<p>CHAPTER 14. TESTING 873
Second (-16)
First 'x'
First 'Y'
One of the key insights here is that the Arbitrary instance
for a datatype doesn’t have to be the only way to generate or
provide random values of your datatype for QuickCheck tests.
You can oﬀer alternative Gens for your type with interesting or
useful behavior as well.
CoArbitrary
CoArbitrary is a counterpart to Arbitrary that enables the gener-
ation of functions fitting a particular type. Rather than talking
about random values you can get via Gen, it lets you provide
functions with a value of type 𝑎as an argument in order to
varyaGen:
arbitrary ::Arbitrary a=&gt;
Gena
coarbitrary ::CoArbitrary a=&gt;
a-&gt;Genb-&gt;Genb
-- [1] [ 2 ] [ 3 ]
Here[1]is used to return a modification or variant of [2]
which is the result [3]at the end.</p>
<p>CHAPTER 14. TESTING 874
It turns out, as long as your datatype has a Generic instance
derived, you can get these instances for free. The following
should work fine:
{-# LANGUAGE DeriveGeneric #-}
moduleCoArbitrary where
importGHC.Generics
importTest.QuickCheck
dataBool'=
True'
|False'
deriving (Generic)
instance CoArbitrary Bool'
This’ll then let you do things like the following:</p>
<p>CHAPTER 14. TESTING 875
importTest.QuickCheck
-- plus the above
trueGen ::GenInt
trueGen =coarbitrary True'arbitrary
falseGen ::GenInt
falseGen =coarbitrary False'arbitrary
Essentially this lets you randomly generate a function. It
might be a little hard to see why you’d care for now, but if
you ever find yourself wanting to randomly generate anything
with the (-&gt;)type inside it somewhere, it becomes salient in a
hurry.
14.7 Chapter Exercises
Now it’s time to write some tests of your own. You could write
tests for most of the exercises you’ve done in the book, but
whether you’d want to use hspecorQuickCheck depends on what
you’re trying to test. We’ve tried to simplify things a bit by
telling you which to use for these exercises, but, as always, we
encourage you to experiment on your own.</p>
<p>CHAPTER 14. TESTING 876
Validating numbers into words
Remember the “numbers into words” exercise in Recursion?
You’ll be writing tests to validate the functions you wrote.</p>
<p>CHAPTER 14. TESTING 877
moduleWordNumberTest where
importTest.Hspec
importWordNumber
(digitToWord ,digits,wordNumber )
main::IO()
main=hspec$ do
describe &quot;digitToWord&quot; $ do
it&quot;returns zero for 0&quot; $ do
digitToWord 0<code>shouldBe</code> &quot;zero&quot;
it&quot;returns one for 1&quot; $ do
print&quot;???&quot;
describe &quot;digits&quot; $ do
it&quot;returns [1] for 1&quot; $ do
digits1<code>shouldBe</code> [ 1]
it&quot;returns [1, 0, 0] for 100&quot; $ do
print&quot;???&quot;
describe &quot;wordNumber&quot; $ do
it&quot;one-zero-zero given 100&quot; $ do
wordNumber 100
<code>shouldBe</code> &quot;one-zero-zero&quot;
it&quot;nine-zero-zero-one for 9001&quot; $ do
print&quot;???&quot;</p>
<p>CHAPTER 14. TESTING 878
Fill in the test cases that print question marks. If you think
of additional tests you could perform, add them.
Using QuickCheck
Test some simple arithmetic properties using QuickCheck .
1.-- for a function
halfx=x/2
-- this property should hold
halfIdentity =(*2).half
2.importData.List (sort)
-- for any list you apply sort to
-- this property should hold
listOrdered ::(Orda)=&gt;[a]-&gt;Bool
listOrdered xs=
snd$foldr go ( Nothing,True) xs
wherego_status@(_,False)=status
go y (Nothing, t)=(Justy, t)
go y (Justx, t)=(Justy, x&gt;=y)
3.Now we’ll test the associative and commutative properties
of addition:</p>
<p>CHAPTER 14. TESTING 879
plusAssociative x y z=
x+(y+z)==(x+y)+z
plusCommutative x y=
x+y==y+x
Keep in mind these properties won’t hold for types based
on IEEE-754 floating point numbers, such as Floator
Double .
4.Now do the same for multiplication.
5.We mentioned in one of the first chapters that there are
some laws involving the relationship of quotandremand
divandmod. Write QuickCheck tests to prove them.
-- quot rem
(quot x y) *y+(rem x y) ==x
(div x y) *y+(mod x y) ==x
6.Is (^) associative? Is it commutative? Use QuickCheck to see
if the computer can contradict such an assertion.
7.Test that reversing a list twice is the same as the identity
of the list:
reverse .reverse ==id
8.Write a property for the definition of ($).</p>
<p>CHAPTER 14. TESTING 880
f$a=f a
f.g=\x-&gt;f (g x)
9.See if these two functions are equal:
foldr(:)==(++)
foldr(++)[]==concat
10.Hm. Is that so?
fn xs=length (take n xs) ==n
11.Finally, this is a fun one. You may remember we had you
compose readandshowone time to complete a “round
trip.” Well, now you can test that it works:
fx=(read (show x)) ==x
Failure
Find out why this property fails.</p>
<p>CHAPTER 14. TESTING 881
-- for a function
squarex=x*x
-- why does this property not hold?
-- Examine the type of sqrt.
squareIdentity =square.sqrt
Hint: Read about floating point arithmetic and precision if
you’re unfamiliar with it.
Idempotence
Idempotence refers to a property of some functions in which
the result value does not change beyond the initial application.
If you apply the function once, it returns a result, and applying
the same function to that value won’t ever change it. You might
think of a list that you sort: once you sort it, the sorted list will
remain the same after applying the same sorting function to
it. It’s already sorted, so new applications of the sort function
won’t change it.
UseQuickCheck and the following helper functions to demon-
strate idempotence for the following:
twicef=f.f
fourTimes =twice.twice</p>
<p>CHAPTER 14. TESTING 882
1.fx=
(capitalizeWord x
==twice capitalizeWord x)
&amp;&amp;
(capitalizeWord x
==fourTimes capitalizeWord x)
2.f'x=
(sort x
==twice sort x)
&amp;&amp;
(sort x
==fourTimes sort x)
Make a Gen random generator for the datatype
We demonstrated in the chapter how to make Gengenerators
for diﬀerent datatypes. We are so certain you enjoyed that, we
are going to ask you to do it for some new datatypes:
1.Equal probabilities for each.
dataFool=
Fulse
|Frue
deriving (Eq,Show)</p>
<p>CHAPTER 14. TESTING 883
2.2/3s chance of Fulse, 1/3 chance of Frue.
dataFool=
Fulse
|Frue
deriving (Eq,Show)
Hangman testing
Next, you should go back to the hangman project from the
previous chapter and write tests. The kinds of tests you can
write at this point will be limited due to the interactive nature
of the game. However, you can test the functions. Focus your
attention on testing the following:
fillInCharacter ::Puzzle-&gt;Char-&gt;Puzzle
fillInCharacter (Puzzleword
filledInSoFar s) c =
Puzzleword newFilledInSoFar (c :s)
wherezipper guessed wordChar guessChar =
ifwordChar ==guessed
thenJustwordChar
elseguessChar
newFilledInSoFar =
letzd=(zipper c)
inzipWith zd word filledInSoFar</p>
<p>CHAPTER 14. TESTING 884
and:
handleGuess ::Puzzle-&gt;Char-&gt;IOPuzzle
handleGuess puzzle guess = do
putStrLn $&quot;Your guess was: &quot; ++[guess]
case(charInWord puzzle guess
, alreadyGuessed puzzle guess) of
(<em>,True)-&gt; do
putStrLn &quot;You already guessed that <br />
\character, pick <br />
\something else!&quot;
return puzzle
(True,</em>)-&gt; do
putStrLn &quot;This character was in the <br />
\word, filling in the <br />
\word accordingly&quot;
return (fillInCharacter puzzle guess)
(False,_)-&gt; do
putStrLn &quot;This character wasn't in <br />
\the word, try again.&quot;
return (fillInCharacter puzzle guess)
Refresh your memory on what those are supposed to do
and then test to make sure they do.</p>
<p>CHAPTER 14. TESTING 885
Validating ciphers
As a final exercise, create QuickCheck properties that verify your
Caesar and Vigenère ciphers return the same data after encod-
ing and decoding a string.
14.8 Definitions
1.Unit testing is a method in which you test the smallest
parts of an application possible. These units are individu-
ally and independently scrutinized for desired behaviors.
Unit testing is better automated but it can also be done
manually via a human entering inputs and verifying out-
puts.
2.Property testing is a testing method where a subset of a
large input space is validated, usually against a property
or law some code should abide by. In Haskell, this is
usually done with QuickCheck which facilitates the random
generation of input and definition of properties to be veri-
fied. Common properties that are checked using property
testing are things like identity, associativity, isomorphism,
and idempotence.
3.When we say an operation or function is idempotent or
satisfies idempotence , we mean that applying it multiple
times doesn’t produce a diﬀerent result from the first time.</p>
<p>CHAPTER 14. TESTING 886
One example is multiplying by one or zero. You always
get the same result as the first time you multipled by one
or zero.
14.9 Follow-up resources
1.Pedro Vasconcelos; An introduction to QuickCheck
testing;
https://www.fpcomplete.com/user/pbv/
an-introduction-to-quickcheck-testing
2.Koen Claessen and John Hughes; (2000)
QuickCheck: A Lightweight Tool for Random Testing of
Haskell Programs
3.Pedro Vasconcelos; Verifying a Simple Compiler Using
Property-based Random Testing;
http://www.dcc.fc.up.pt/dcc/Pubs/TReports/TR13/
dcc-2013-06.pdf</p>
<p>Chapter 15
Monoid, Semigroup
Simplicity does not
precede complexity, but
follows it.
Alan Perlis
887</p>
<p>CHAPTER 15. MONOID, SEMIGROUP 888
15.1 Monoids and semigroups
One of the finer points of the Haskell community has been
its propensity for recognizing abstract patterns in code which
have well-defined, lawful representations in mathematics. A
word frequently used to describe these abstractions is algebra ,
by which we mean one or more operations and the setthey
operate over. Over the next few chapters, we’re going to be
looking at some of these. Some you may have heard of, such
as functor and monad. Some, such as monoid and the humble
semigroup, may seem new to you. One of the things that
Haskell is really good at is these algebras, and it’s important to
master them before we can do some of the exciting stuﬀ that’s
coming.
This chapter will include:
•Algebras!
•Laws!
•Monoids!
•Semigroups!</p>
<p>CHAPTER 15. MONOID, SEMIGROUP 889
15.2 What we talk about when we talk
about algebras
For some of us, talking about “an algebra” may sound some-
what foreign. So let’s take a second and talk about what we’re
talking about when we use this phrase, at least when we’re
talking about Haskell.
Algebra generally refers to one of the most important fields
of mathematics. In this usage, it means the study of mathe-
matical symbols and the rules governing their manipulation.
It is diﬀerentiated from arithmetic by its use of abstractions
such as variables. By the use of variables, we’re saying we don’t
care much what value will be put into that slot. We care about
the rules of how to manipulate this thing without reference to
its particular value.
And so, as we said above, an algebra refers to some opera-
tions and the set they operate over. Here again, we care less
about the particulars of the values or data we’re working with
and more about the general rules of their use.
In Haskell, these algebras can be implemented with type-
classes; the typeclasses define the set of operations. When we
talk about operations over a set, the set is the typethe opera-
tions are for. The instance defines how each operation will
perform for a given type or set. One of those algebras we use
ismonoid . If you’re a working programmer, you’ve probably</p>
<p>CHAPTER 15. MONOID, SEMIGROUP 890
had monoidal patterns in your code already, perhaps without
realizing it.
15.3 Monoid
A monoid is a binary associative operation with an identity.
This definition tells you a lot — if you’re accustomed to picking
apart mathematical definitions. Let us dissect this frog!
A monoid is a binary associative operation with an identity.
[1] [2] [3] [4] [5]
1.The thing we’re talking about — monoids. That’ll end up
being the name of our typeclass.
2.Binary, i.e., two. So, there will be two of something.
3.Associative — this is a property or law that must be satis-
fied. You’ve seen associativity with addition and multipli-
cation. We’ll explain it more in a moment.
4.Operation — so called because in mathematics, it’s usually
used as an infix operator. You can read this interchange-
ably as “function.” Note that given the mention of “binary”
earlier, we know that this is a function of two arguments.
5.Identity is one of those words in mathematics that pops
up a lot. In this context, we can take this to mean there’ll</p>
<p>CHAPTER 15. MONOID, SEMIGROUP 891
be some value which, when combined with any other
value, will always return that other value. This can be
seen most immediately with examples.
For lists, we have a binary operator, (++), that joins two
lists together. We can also use a function, mappend , from
theMonoid typeclass to do the same thing:
Prelude&gt; mappend [1, 2, 3] [4, 5, 6]
[1,2,3,4,5,6]
For lists, the empty list, [], is the identity value:
mappend [1..5][]=[1..5]
mappend [][1..5]=[1..5]
We can rewrite this as a more general rule, using mempty
from the Monoid typeclass as a generic identity value (more
on this later):
mappend x mempty =x
mappend mempty x =x
In plain English, a monoid is a function that takes two argu-
ments and follows two laws: associativity and identity. Asso-
ciativity means the arguments can be regrouped (or reparen-
thesized, or reassociated) in diﬀerent orders and give the same</p>
<p>CHAPTER 15. MONOID, SEMIGROUP 892
result, as in addition. Identity means there exists some value
such that when we pass it as input to our function, the opera-
tion is rendered moot and the other value is returned, such as
when we add zero or multiply by one. Monoid is the typeclass
that generalizes these laws across types.
15.4 How Monoid is defined in Haskell
Typeclasses give us a way to recognize, organize, and use com-
mon functionalities and patterns across types that diﬀer in
some ways but also have things in common. So, we recognize
that, although there are many types of numbers, all of them
can be arguments in addition, and then we make an addition
function as part of the Numclass that all numbers implement.
TheMonoid typeclass recognizes and orders a diﬀerent pat-
tern than Numbut the goal is similar. The pattern of Monoid is
outlined above: types that have binary functions that let you
join things together in accordance with the laws of associa-
tivity, along with an identity value that will return the other
argument unmodified. This is the pattern of summation, mul-
tiplication, and list concatenation, among other things. The
typeclass abstracts and generalizes the pattern so that you write
code in terms of anytype that can be monoidally combined.
The typeclass Monoid is defined:</p>
<p>CHAPTER 15. MONOID, SEMIGROUP 893
classMonoidmwhere
mempty ::m
mappend ::m-&gt;m-&gt;m
mconcat ::[m]-&gt;m
mconcat =foldr mappend mempty
mappend is howanytwo values that inhabit your type can be
joined together. mempty is the identity value for that mappend
operation. There are some laws that all Monoid instances must
abide, and we’ll get to those soon. Next, let’s look at some
examples of monoids in action!
15.5 Examples of using Monoid
The nice thing about monoids is that they are familiar; they’re
all over the place. The best way to understand them initially
is to look at examples of some common monoidal operations
and remember that this typeclass abstracts the pattern out,
giving you the ability to use the operations over a larger range
of types.
List
One common type with an instance of Monoid isList. Check
out how monoidal operations work with lists:
Prelude&gt; mappend [1, 2, 3] [4, 5, 6]</p>
<p>CHAPTER 15. MONOID, SEMIGROUP 894
[1,2,3,4,5,6]
Prelude&gt; mconcat [[1..3], [4..6]]
[1,2,3,4,5,6]
Prelude&gt; mappend &quot;Trout&quot; &quot; goes well with garlic&quot;
&quot;Trout goes well with garlic&quot;
This should look familiar, because we’ve certainly seen this
before:
Prelude&gt; (++) [1, 2, 3] [4, 5, 6]
[1,2,3,4,5,6]
Prelude&gt; (++) &quot;Trout&quot; &quot; goes well with garlic&quot;
&quot;Trout goes well with garlic&quot;
Prelude&gt; foldr (++) [] [[1..3], [4..6]]
[1,2,3,4,5,6]
Prelude&gt; foldr mappend mempty [[1..3], [4..6]]
[1,2,3,4,5,6]
Our old friend (++)! And if we look at the definition of
Monoid for lists, we can see how this all lines up:
instance Monoid[a]where
mempty =[]
mappend =(++)
For other types, the instances would be diﬀerent, but the
ideas behind them remain the same.</p>
<p>CHAPTER 15. MONOID, SEMIGROUP 895
15.6 Why Integer doesn’t have a
Monoid
The type Integer does not have a Monoid instance. None of the
numeric types do. Yet it’s clear that numbers have monoidal
operations, so what’s up with that, Haskell?
While in mathematics the monoid of numbers is summa-
tion, there’s not a clear reason why it can’t be multiplication.
Both operations are monoidal (binary, associative, having an
identity value), but each type should only have one unique
instance for a given typeclass, not two (one instance for a sum,
one for a product).
This won’t work:
Prelude&gt; let x = 1 :: Integer
Prelude&gt; let y = 3 :: Integer
Prelude&gt; mappend x y
<interactive>:6:1: error:
• No instance for (Monoid Integer)
arising from a use of ‘mappend’
• In the expression: mappend x y
In an equation for ‘it’:
it = mappend x y</p>
<p>CHAPTER 15. MONOID, SEMIGROUP 896
It isn’t clear if those should be added or multiplied as a
mappend operation. It says there’s no Monoid for those Integer s
for that reason. You get the idea.
To resolve the conflict, we have the SumandProduct newtypes
to wrap numeric values and signal which Monoid instance we
want. These newtypes are built into the Data.Monoid module.
While there are two possible instances of Monoid for numeric
values, we avoid using scoping tricks and abide by the rule that
typeclass instances are unique to the types they are for:
Prelude&gt; mappend (Sum 1) (Sum 5)
Sum {getSum = 6}
Prelude&gt; mappend (Product 5) (Product 5)
Product {getProduct = 25}
Prelude&gt; mappend (Sum 4.5) (Sum 3.4)
Sum {getSum = 7.9}
Note that we could use it with values that aren’t integral.
We can use these Monoid newtypes for all the types that have
instances of Num.
Integersformamonoidundersummationandmultiplication . We
can similarly say that lists form a monoid under concatenation.
It’s worth pointing out here that numbers aren’t the only
sets that have more than one possible monoid. Lists have
more than one possible monoid, although for now we’re only
working with concatenation (we’ll look at the other list monoid</p>
<p>CHAPTER 15. MONOID, SEMIGROUP 897
in another chapter). Several other types do as well. We usually
enforce the unique instance rule by using newtype to separate
the diﬀerent monoidal behaviors.
Why newtype?
Use of a newtype can be hard to justify or explain to people that
don’t yet have good intuitions for how Haskell code gets com-
piled and the representations of data used by your computer
in the course of executing your programs. With that in mind,
we’ll do our best and oﬀer two explanations intended for two
diﬀerent audiences. We will return to the topic of newtype in
more detail later in the book.
First, there’s not much semantic diﬀerence (except for cir-
cumstances involving bottom , explained later) between the fol-
lowing datatypes:
dataServer=ServerString
newtype Server' =Server' String
The main diﬀerences are that using newtype constrains the
datatype to having a single unary data constructor and newtype
guarantees no additional runtime overhead in “wrapping” the
original type. That is, the runtime representation of newtype
and what it wraps are always identical — no additional “boxing
up” of the data as is necessary for typical products and sums.</p>
<p>CHAPTER 15. MONOID, SEMIGROUP 898
Forveteranprogrammerswhounderstandpointers newtype
is like a single-member C union that avoids creating an extra
pointer, but still gives you a new type constructor and data
constructor so you don’t mix up the many many many things
that share a single representation.
In summary, why you might use newtype
1.To signal intent: using newtype makes it clear that you only
intend for it to be a wrapper for the underlying type. The
newtype cannot eventually grow into a more complicated
sum or product type, while a normal datatype can.
2.To improve type safety: avoid mixing up many values of
the same representation, such as TextorInteger .
3.To add diﬀerent typeclass instances to a type that is other-
wise unchanged representationally, such as with Sumand
Product .
More on Sum and Product
There’s more than one valid Monoid instance one can write for
numbers, so we use newtype wrappers to distinguish which we
want. If you import Data.Monoid you’ll see the SumandProduct
newtypes:</p>
<p>CHAPTER 15. MONOID, SEMIGROUP 899
Prelude&gt; import Data.Monoid
Prelude&gt; :info Sum
newtype Sum a = Sum {getSum :: a}
...some instances elided...
instance Num a =&gt; Monoid (Sum a)
Prelude&gt; :info Product
newtype Product a =
Product {getProduct :: a}
...some instances elided...
instance Num a =&gt; Monoid (Product a)
The instances say that we can use SumorProduct values as a
Monoid as long as they contain numeric values. We can prove
this is the case for ourselves. We’re going to be using the infix
operator for mappend in these examples. It has the same type
and does the same thing but saves some characters and will
make these examples a bit cleaner:
Prelude Data.Monoid&gt; :t (&lt;&gt;)
(&lt;&gt;) :: Monoid m =&gt; m -&gt; m -&gt; m
Prelude&gt; Sum &quot;Frank&quot; &lt;&gt; Sum &quot;Herbert&quot;
No instance for (Num [Char]) ...</p>
<p>CHAPTER 15. MONOID, SEMIGROUP 900
The example didn’t work because the 𝑎inSum awasString
which is not an instance of Num.
SumandProduct do what you’d expect with a bit of syntactic
surprise:
Prelude Data.Monoid&gt; (Sum 8) &lt;&gt; (Sum 9)
Sum {getSum = 17}
Prelude Data.Monoid&gt; mappend mempty Sum 9
Sum {getSum = 9}
Butmappend joins two things, so you can’t do this:
Prelude&gt; mappend (Sum 8) (Sum 9) (Sum 10)
You’ll get a big error message including this line:
Possible cause: ‘Sum’ is applied to too many arguments
In the first argument of ‘mappend’, namely ‘(Sum 8)’
So, that’s easy enough to fix by nesting:
Prelude&gt; mappend (Sum 1) (mappend (Sum 2) (Sum 3))
Sum {getSum = 6}
Or somewhat less tedious by infixing the mappend function:
Prelude&gt; Sum 1 &lt;&gt; Sum 1 &lt;&gt; Sum 1
Sum {getSum = 3}</p>
<p>CHAPTER 15. MONOID, SEMIGROUP 901
Or you could also put your Sums in a list and use mconcat :
Prelude&gt; mconcat [Sum 8, Sum 9, Sum 10]
Sum {getSum = 27}
Due to the special syntax of SumandProduct , we also have
built-in record field accessors we can use to unwrap the value:
Prelude&gt; getSum $ mappend (Sum 1) (Sum 1)
2
Prelude&gt; getProduct $ mappend (Product 5) (Product 5)
25
Prelude&gt; getSum $ mconcat [(Sum 5), (Sum 6), (Sum 7)]
18
Product is similar to Sumbut for multiplication.
15.7 Why bother?
Because monoids are common and they’re a nice abstraction
to work with when you have multiple monoidal things run-
ning around in a project. Knowing what a monoid is can help
you to recognize when you’ve encountered the pattern. Fur-
ther, having principled laws for it means you know you can
combine monoidal operations safely. When we say something</p>
<p>CHAPTER 15. MONOID, SEMIGROUP 902
is a monoid or can be described as monoidal , we mean you can
define at least one law-abiding Monoid instance for it.
A common use of monoids is to structure and describe com-
mon modes of processing data. Sometimes this is to describe
an API for incrementally processing a large dataset, sometimes
to describe guarantees needed to roll up aggregations (think
summation) in a parallel, concurrent, or distributed processing
framework.
One example of where things like the identity can be useful
is if you want to write a generic library for doing work in
parallel. You could choose to describe your work as being like
a tree, with each unit of work being a leaf. From there you
can partition the tree into as many chunks as are necessary to
saturate the number of processor cores or entire computers
you want to devote to the work. The problem is, if we have a
pair-wise operation and we need to combine an odd number
of leaves, how do we even out the count?
One straightforward way could be to simply provide mempty
(the identity value) to the odd leaves out so we get the same
result and pass it up to the next layer of aggregation!
A variant of monoid that provides more guarantees is the
Abelian or commutative monoid. Commutativity can be par-
ticularly helpful when doing concurrent or distributed pro-
cessing of data because it means the intermediate results being
computed in a diﬀerent order won’t change the eventual an-
swer.</p>
<p>CHAPTER 15. MONOID, SEMIGROUP 903
Monoids are even strongly associated with the concept of
folding or catamorphism — something we do all the time in
Haskell. You’ll see this more explicitly in the Foldable chapter,
but here’s a taste:
Prelude&gt; foldr mappend mempty ([2, 4, 6] :: [Product Int])
Product {getProduct = 48}
Prelude&gt; foldr mappend mempty ([2, 4, 6] :: [Sum Int])
Sum {getSum = 12}
Prelude&gt; foldr mappend mempty [&quot;blah&quot;, &quot;woot&quot;]
&quot;blahwoot&quot;
You’ll see monoidal structure come up when we explain
Applicative andMonadas well.
15.8 Laws
We’ll get to those laws in a moment. First, heed our little cri de
coeurabout why you should care about mathematical laws:
Laws circumscribe what constitutes a valid instance or con-
crete instance of the algebra or set of operations we’re working
with. We care about the laws a Monoid instance must adhere to
because we want our programs to be correct wherever possible.
Proofs are programs, and programs are proofs. We care about</p>
<p>CHAPTER 15. MONOID, SEMIGROUP 904
programs that compose well, that are easy to understand, and
which have predictable behavior. To that end, we should steal
prolifically from mathematics.
Algebras are defined by their laws and are useful principally
fortheir laws. Laws make up what algebras are.
Among other things, laws provide us guarantees that let
us build on solid foundations. Those guarantees give us pre-
dictable composition (or combination) of programs. Without
the ability to safely combine programs, everything must be
written from scratch, nothing could be reused. The physical
world has enjoyed the useful properties of stone stacked up
on top of stone since the Great Pyramid of Giza was built in
the pharaoh Sneferu’s reign in 2,600 BC. Similarly, if we want
to be able to stack up functions scalably, they need to obey
laws. Stones don’t evaporate into thin air or explode violently.
It’d be nice if our programs were similarly trustworthy.
There are more possible laws we can require for an algebra
than associativity or an identity, but these are simple examples
we are starting with for now, partly because Monoid is a good
place to start with algebras-as-typeclasses. We’ll see examples
of more later.
Monoid instances must abide by the following laws:</p>
<p>CHAPTER 15. MONOID, SEMIGROUP 905
-- left identity
mappend mempty x =x
-- right identity
mappend x mempty =x
-- associativity
mappend x (mappend y z) =
mappend (mappend x y) z
mconcat =foldr mappend mempty
Here is how the identity law looks in practice:
Prelude&gt; import Data.Monoid
-- left identity
Prelude&gt; mappend mempty (Sum 1)
Sum {getSum = 1}
-- right identity
Prelude&gt; mappend (Sum 1) mempty
Sum {getSum = 1}
We can demonstrate associativity more easily if we first
introduce the infix operator for mappend ,&lt;&gt;. Note the parenthe-
sization on the two examples:</p>
<p>CHAPTER 15. MONOID, SEMIGROUP 906
Prelude&gt; :t (&lt;&gt;)
(&lt;&gt;) :: Monoid m =&gt; m -&gt; m -&gt; m
-- associativity
Prelude&gt; (Sum 1) &lt;&gt; (Sum 2 &lt;&gt; Sum 3)
Sum {getSum = 6}
Prelude&gt; (Sum 1 &lt;&gt; Sum 2) &lt;&gt; (Sum 3)
Sum {getSum = 6}
Andmconcat should have the same result as foldr mappend
mempty :
Prelude&gt; mconcat [Sum 1, Sum 2, Sum 3]
Sum {getSum = 6}
Prelude&gt; foldr mappend mempty [Sum 1, Sum 2, Sum 3]
Sum {getSum = 6}
Now let’s see all of that again, but using the Monoid of lists:
-- mempty is []
-- mappend is (++)
-- left identity
Prelude&gt; mappend mempty [1, 2, 3]
[1,2,3]</p>
<p>CHAPTER 15. MONOID, SEMIGROUP 907
-- right identity
Prelude&gt; mappend [1, 2, 3] mempty
[1,2,3]
-- associativity
Prelude&gt; [1] &lt;&gt; ([2] &lt;&gt; [3])
[1,2,3]
Prelude&gt; ([1] &lt;&gt; [2]) &lt;&gt; [3]
[1,2,3]
-- mconcat ~ foldr mappend mempty
Prelude&gt; mconcat [[1], [2], [3]]
[1,2,3]
Prelude&gt; foldr mappend mempty [[1], [2], [3]]
[1,2,3]
Prelude&gt; concat [[1], [2], [3]]
[1,2,3]
The important part here is that you have these guarantees
even when you don’t know whatMonoid you’ll be working with.</p>
<p>CHAPTER 15. MONOID, SEMIGROUP 908
15.9 Diﬀerent instance, same
representation
Monoid is somewhat diﬀerent from other typeclasses in Haskell,
in that many datatypes have more than one valid monoid. We
saw that for numbers, both addition and multiplication are sen-
sible monoids with diﬀerent behaviors. When we have more
than one potential implementation for Monoid for a datatype,
it’s most convenient to use newtypes to tell them apart, as we
did with SumandProduct .
Addition is a classic appending operation, as is list concate-
nation. Referring to multiplication as an appending operation
may also seem intuitive enough, as it still follows the basic
pattern of combining two values of one type into one value.
But for other datatypes the meaning of append is less clear.
In these cases, the monoidal operation is less about combining
the values and more about finding a summary value for the set.
We mentioned above that monoids are important to folding
and catamorphisms more generally. Mappending is perhaps
bestthoughtofnotasawayofcombiningvaluesinthewaythat
addition or list concatenation does, but as a way to condense
any set of values to a summary value. We’ll start by looking at
theMonoid instances for Boolto see what we mean.
Boolean values have two possible monoids — a monoid of
conjunction and one of disjunction. As we do with numbers,</p>
<p>CHAPTER 15. MONOID, SEMIGROUP 909
we use newtypes to distinguish the two instances. AllandAny
are the newtypes for Bool’s monoids:
Prelude&gt; import Data.Monoid
Prelude&gt; All True &lt;&gt; All True
All {getAll = True}
Prelude&gt; All True &lt;&gt; All False
All {getAll = False}
Prelude&gt; Any True &lt;&gt; Any False
Any {getAny = True}
Prelude&gt; Any False &lt;&gt; Any False
Any {getAny = False}
Allrepresents boolean conjunction: it returns a Trueif and
only if all values it is “appending” are True.Anyis the monoid
of boolean disjunction: it returns a Trueif any value is True.
There is some sense in which it might feel strange to think of
this as a combining or mappending operation, unless we recall
that mappending is less about combining and more about
condensing or reducing.
TheMaybetype has more than two possible Monoids. We’ll
look at each in turn, but the two that have an obvious relation-
ship are FirstandLast. They are like boolean disjunction, but
with explicit preference for the leftmost or rightmost success</p>
<p>CHAPTER 15. MONOID, SEMIGROUP 910
in a series of Maybevalues. We have to choose because with
Bool, all you know is TrueorFalse— it doesn’t matter where
yourTrueorFalsevalues occurred. With Maybe, however, you
need to make a decision as to which Justvalue you’ll return
if there are multiple successes. FirstandLastencode these
diﬀerent possibilities.
Firstreturns the first or leftmost non- Nothing value:
Prelude&gt; First (Just 1) <code>mappend</code> First (Just 2)
First {getFirst = Just 1}
Lastreturns the last or rightmost non- Nothing value:
Prelude&gt; Last (Just 1) <code>mappend</code> Last (Just 2)
Last {getLast = Just 2}
Both will succeed in returning something in spite of Nothing
values as long as there’s at least one Just:
Prelude&gt; Last Nothing <code>mappend</code> Last (Just 2)
Last {getLast = Just 2}
Prelude&gt; First Nothing <code>mappend</code> First (Just 2)
First {getFirst = Just 2}
Neither can, for obvious reasons, return anything if all val-
ues are Nothing :</p>
<p>CHAPTER 15. MONOID, SEMIGROUP 911
Prelude&gt; First Nothing <code>mappend</code> First Nothing
First {getFirst = Nothing}
Prelude&gt; Last Nothing <code>mappend</code> Last Nothing
Last {getLast = Nothing}
To maintain the unique pairing of type and typeclass in-
stance, newtypes are used for all of those, the same as we saw
withSumandProduct .
Let’s look next at the third variety of Maybe Monoid .
15.10 Reusing algebras by asking for
algebras
We alluded to there being more possible Monoid s forMaybethan
justFirstandLast. Let’s write that other Monoid instance. We
will now be concerned not with choosing one value out of a
set of values but of combining the 𝑎values contained within
theMaybe a type.
First, try to notice a pattern:</p>
<p>CHAPTER 15. MONOID, SEMIGROUP 912
instance Monoidb=&gt;Monoid(a-&gt;b)
instance (Monoida,Monoidb)
=&gt;Monoid(a, b)
instance (Monoida,Monoidb,Monoidc)
=&gt;Monoid(a, b, c)
What these Monoids have in common is that they are giv-
ing you a new Monoid for a larger type by reusing the Monoid
instances of types that represent components of the larger
type.
This obligation to ask for a Monoid for an encapsulated type
(such as the 𝑎inMaybe a ) exists even when not all possible val-
ues of the larger type contain the value of the type argument.
For example, Nothing does not contain the 𝑎we’re trying to
get aMonoid for, but Just a does, so not all possible Maybevalues
contain the 𝑎type argument. For a Maybe Monoid that will have
amappend operation for the 𝑎values, we need a Monoid for what-
ever type 𝑎is.Monoids likeFirstandLastwrap the Maybe a but
do not require a Monoid for the𝑎value itself because they don’t
mappend the𝑎values or provide a mempty of them.
If you do have a datatype that has a type argument that
does not appear anywhere in the terms (a phantom type), the
typechecker does not demand that you have a Monoid instance
for that argument. For example:</p>
<p>CHAPTER 15. MONOID, SEMIGROUP 913
dataBoolya=
False'
|True'
deriving (Eq,Show)
-- conjunction
instance Monoid(Boolya)where
mappend False'_ =False'
mappend _False'=False'
mappend True'True'=True'
We didn’t need a Monoid constraint for 𝑎because we’re never
mappending 𝑎values (we can’t; none exist) and we’re never
asking for a mempty of type 𝑎. This is the fundamental reason
we don’t need the constraint, but it can happen that we don’t
do this even when the type doesoccur in the datatype.
Exercise: Optional Monoid
Writethe Monoid instanceforour Maybetyperenamedto Optional .</p>
<p>CHAPTER 15. MONOID, SEMIGROUP 914
dataOptional a=
Nada
|Onlya
deriving (Eq,Show)
instance Monoida
=&gt;Monoid(Optional a)where
mempty=undefined
mappend =undefined
Expected output:
Prelude&gt; Only (Sum 1) <code>mappend</code> Only (Sum 1)
Only (Sum {getSum = 2})
Prelude&gt; Only (Product 4) <code>mappend</code> Only (Product 2)
Only (Product {getProduct = 8})
Prelude&gt; Only (Sum 1) <code>mappend</code> Nada
Only (Sum {getSum = 1})
Prelude&gt; Only [1] <code>mappend</code> Nada
Only [1]
Prelude&gt; Nada <code>mappend</code> Only (Sum 1)
Only (Sum {getSum = 1})</p>
<p>CHAPTER 15. MONOID, SEMIGROUP 915
Associativity
This will be mostly review, but we want to be specific about
associativity. Associativitysaysthatyoucanassociate, orgroup,
the arguments of your operation diﬀerently and the result will
be the same.
Let’s review examples of some operations that can be reas-
sociated :
Prelude&gt; (1 + 9001) + 9001
18003
Prelude&gt; 1 + (9001 + 9001)
18003
Prelude&gt; (7 * 8) * 3
168
Prelude&gt; 7 * (8 * 3)
168
And some that cannot have the parentheses reassociated
without changing the result:
Prelude&gt; (1 - 10) - 100
-109
Prelude&gt; 1 - (10 - 100)
91
This isnotas strong a property as an operation that com-
mutes or is commutative . Commutative means you can reorder</p>
<p>CHAPTER 15. MONOID, SEMIGROUP 916
the arguments and still get the same result. Addition and mul-
tiplication are commutative, but (++)for the list type is only
associative.
Let’s demonstrate this by writing a mildly evil version of
addition that flips the order of its arguments:
Prelude&gt; let evilPlus = flip (+)
Prelude&gt; 76 + 67
143
Prelude&gt; 76 <code>evilPlus</code> 67
143
We have some evidence, but not proof, that(+)commutes.
However, we can’t do the same with (++):
Prelude&gt; let evilPlusPlus = flip (++)
Prelude&gt; let oneList = [1..3]
Prelude&gt; let otherList = [4..6]
Prelude&gt; oneList ++ otherList
[1,2,3,4,5,6]
Prelude&gt; oneList <code>evilPlusPlus</code> otherList
[4,5,6,1,2,3]
In this case, this serves as a proof by counterexample that
(++)doesnotcommute. It doesn’t matter if it commutes for all</p>
<p>CHAPTER 15. MONOID, SEMIGROUP 917
other inputs; that it doesn’t commute for one of them means
thelaw of commutativity does not hold.
Commutativity is a useful property and can be helpful in
circumstances when you might need to be able to reorder
evaluation of your data for efficiency purposes without need-
ing to worry about the result changing. Distributed systems
use commutative monoids in designing and thinking about
constraints, which are monoids that guarantee their operation
commutes.
But, for our purposes, Monoid abides by the law of associa-
tivity but not the law of commutativity, even though some
monoidal operations (addition and multiplication) are com-
mutative.
Identity
An identity is a value with a special relationship with an oper-
ation: it turns the operation into the identity function. There
are no identities without operations. The concept is defined in
terms of its relationship with a given operation. If you’ve done
grade school arithmetic, you’ve already seen some identities:
Prelude&gt; 1 + 0
1
Prelude&gt; 521 + 0
521
Prelude&gt; 1 * 1</p>
<p>CHAPTER 15. MONOID, SEMIGROUP 918
1
Prelude&gt; 521 * 1
521
Zero is the identity value for addition, while 1is the identity
value for multiplication. As we said, it doesn’t make sense to
talk about zero and one as identity values outside the context
of those operations. That is, zero is definitely not the identity
value for other operations. We can check this property with a
simple equality test as well:
Prelude&gt; let myList = [1..424242]
-- 0 serves as identity for addition
Prelude&gt; map (+0) myList == myList
True
-- but not for multiplication
Prelude&gt; map (*0) myList == myList
False
-- 1 serves as identity for multiplication
Prelude&gt; map (*1) myList == myList
True
-- but not for addition
Prelude&gt; map (+1) myList == myList
False</p>
<p>CHAPTER 15. MONOID, SEMIGROUP 919
This is the other law for Monoid : the binary operation must
be associative andit must have a sensible identity value.
The problem of orphan instances
We’ve said both in this chapter and in the earlier chapter de-
voted to Typeclasses that typeclasses have unique pairings of
the class and the instance for a particular type.
We do sometimes end up with multiple instances for a
single type when orphan instances are written. But writing
orphan instances should be avoided at all costs . If you get an
orphan instance warning from GHC, fix it.
An orphan instance is when an instance is defined for a
datatype and typeclass, but not in the same module as either
the declaration of the typeclass or the datatype. If you don’t
own the typeclass or the datatype, newtype it!
If you want an orphan instance so that you can have multi-
ple instances for the same type, you still want to use newtype .
We saw this earlier with SumandProduct which let us have two
diﬀerent Monoid instances for numbers without resorting to
orphans or messing up typeclass instance uniqueness.
Let’s see an example of an orphan instance and how to fix it.
First, make a project directory and change into that directory:
$ mkdir orphan-instance &amp;&amp; cd orphan-instance</p>
<p>CHAPTER 15. MONOID, SEMIGROUP 920
Then we’re going to make a couple of files, one module in
each:
moduleListywhere
newtype Listya=
Listy[a]
deriving (Eq,Show)
moduleListyInstances where
importData.Monoid
importListy
instance Monoid(Listya)where
mempty=Listy[]
mappend ( Listyl) (Listyl')=
Listy$mappend l l'
So our directory will look like:
$ tree
.
├── Listy.hs
└── ListyInstances.hs</p>
<p>CHAPTER 15. MONOID, SEMIGROUP 921
Then to build ListyInstances such that it can see Listy, we
must use the -Iflag to include the current directory and make
modules within it discoverable. The .after the Iis how we
say “this directory” in Unix-alikes. If you succeed, you should
see something like the following:
$ ghc -I. --make ListyInstances.hs
[2 of 2] Compiling ListyInstances
Note that the only output will be an object file, the result of
compiling a module that can be reused as a library by Haskell
code, because we didn’t define a mainsuitable for producing an
executable. We’re only using this approach to build this so that
we can avoid the hassle of initializing (via stack new or similar)
a project. For anything more complicated or long-lived than
this, use a dependency and build management tool like Cabal
(if you’re using Stack, you’re also using Cabal).
Now to provide one example of why orphan instances are
problematic. Ifwecopyour Monoid instancefrom ListyInstances
intoListy, then rebuild ListyInstances , we’ll get the following
error.
$ ghc -I. --make ListyInstances.hs
[1 of 2] Compiling Listy
[2 of 2] Compiling ListyInstances
Listy.hs:7:10:</p>
<p>CHAPTER 15. MONOID, SEMIGROUP 922
Duplicate instance declarations:
instance Monoid (Listy a)
-- Defined at Listy.hs:7:10
instance Monoid (Listy a)
-- Defined at ListyInstances.hs:5:10
These conflicting instance declarations could happen to
anybody who uses the previous version of our code. And
that’s a problem.
Orphan instances are stilla problem even if duplicate in-
stances aren’t both imported into a module because it means
your typeclass methods will start behaving diﬀerently depend-
ing on what modules are imported, which breaks the funda-
mental assumptions and niceties of typeclasses.
There are a few solutions for addressing orphan instances:
1.You defined the type but not the typeclass? Put the in-
stance in the same module as the type so that the type
cannot be imported without its instances.
2.You defined the typeclass but not the type? Put the in-
stance in the same module as the typeclass definition
so that the typeclass cannot be imported without its in-
stances.
3.Neither the type nor the typeclass are yours? Define your
own newtype wrapping the original type and now you’ve</p>
<p>CHAPTER 15. MONOID, SEMIGROUP 923
got a type that “belongs” to you for which you can rightly
define typeclass instances. There are means of making
this less annoying which we’ll discuss later.
These restrictions must be maintained in order for us to
reap the full benefit of typeclasses along with the reasoning
properties that are associated with them. A type musthave
a unique (singular) implementation of a typeclass in scope,
and avoiding orphan instances is how we prevent conflict-
ing instances. Be aware, however, that avoidance of orphan
instances is more strictly adhered to among library authors
rather than application developers, although it’s no less im-
portant in applications.
15.11 Madness
You may have seen mad libs before. The idea is to take a tem-
plate of phrases, fill them in with blindly selected categories
of words, and see if saying the final version is amusing.
Using a lightly edited example from the Wikipedia article
on Mad Libs:
&quot;___________! he said ______ as he
exclamation adverb
jumped into his car ____ and drove
noun</p>
<p>CHAPTER 15. MONOID, SEMIGROUP 924
off with his _________ wife.&quot;
adjective
We can make this into a function, like the following:
importData.Monoid
typeVerb=String
typeAdjective =String
typeAdverb=String
typeNoun=String
typeExclamation =String
madlibbin' ::Exclamation
-&gt;Adverb
-&gt;Noun
-&gt;Adjective
-&gt;String
madlibbin' e adv noun adj =
e&lt;&gt;&quot;! he said &quot; &lt;&gt;
adv&lt;&gt;&quot; as he jumped into his car &quot; &lt;&gt;
noun&lt;&gt;&quot; and drove off with his &quot; &lt;&gt;
adj&lt;&gt;&quot; wife.&quot;
Now you’re going to refactor this code a bit! Rewrite it using
mconcat .</p>
<p>CHAPTER 15. MONOID, SEMIGROUP 925
madlibbinBetter' ::Exclamation
-&gt;Adverb
-&gt;Noun
-&gt;Adjective
-&gt;String
madlibbinBetter' e adv noun adj =undefined
15.12 Better living through QuickCheck
Proving laws can be tedious, especially if the code we’re check-
ing is in the middle of changing frequently. Accordingly, hav-
ing a cheap way to get a sense of whether or not the laws are
likelyto be obeyed by an instance is pretty useful. QuickCheck
happens to be an excellent way to accomplish this.
Validating associativity with QuickCheck
You can check the associativity of some simple arithemetic
expressions by asserting equality between two versions with
diﬀerent parenthesization and checking them in the REPL:</p>
<p>CHAPTER 15. MONOID, SEMIGROUP 926
-- we're saying these are the same because
-- (+) and (<em>) are associative
1+(2+3)==(1+2)+3
4</em>(5<em>6)==(4</em>5)<em>6
This doesn’t tell us that associativity holds for anyinputs to
(+)and(</em>), though, and that is what we want to test. Our old
friend from the lambda calculus — abstraction! — suffices for
this:
\a b c-&gt;a+(b+c)==(a+b)+c
\a b c-&gt;a*(b<em>c)==(a</em>b)*c
But our arguments aren’t the only thing we can abstract.
What if we want to talk about the abstract property of associa-
tivity for some given function 𝑓?
\f a b c -&gt;
f a (f b c) ==f (f a b) c
-- or infix
(&lt;&gt;) a b c -&gt;
a&lt;&gt;(b&lt;&gt;c)==(a&lt;&gt;b)&lt;&gt;c</p>
<p>CHAPTER 15. MONOID, SEMIGROUP 927
Surprise! You can bind infix names for function arguments.
asc::Eqa
=&gt;(a-&gt;a-&gt;a)
-&gt;a-&gt;a-&gt;a
-&gt;Bool
asc(&lt;&gt;) a b c =
a&lt;&gt;(b&lt;&gt;c)==(a&lt;&gt;b)&lt;&gt;c
Now how do we turn this function into something we can
property test with QuickCheck ? The quickest and easiest way
would probably look something like the following:
importData.Monoid
importTest.QuickCheck
monoidAssoc ::(Eqm,Monoidm)
=&gt;m-&gt;m-&gt;m-&gt;Bool
monoidAssoc a b c=
(a&lt;&gt;(b&lt;&gt;c))==((a&lt;&gt;b)&lt;&gt;c)
We have to declare the types for the function in order to
run the tests, so that QuickCheck knows what types of data to
generate.
We can now use this to check associativity of functions:</p>
<p>CHAPTER 15. MONOID, SEMIGROUP 928
-- for brevity
Prelude&gt; type S = String
Prelude&gt; type B = Bool
Prelude&gt; quickCheck (monoidAssoc :: S -&gt; S -&gt; S -&gt; B)
+++ OK, passed 100 tests.
ThequickCheck function uses the Arbitrary typeclass to pro-
vide the randomly generated inputs for testing the function.
Although it’s common to do so, we may not want to rely on an
Arbitrary instance existing for the type of our inputs, for one
of a few reasons. It may be that we need a generator for a type
that doesn’t belong to us, so we’d rather not make an orphan
instance. Or it could be a type that already has an Arbitrary
instance, but we want to run tests with a diﬀerent random
distribution of values, or to make sure we check certain special
edge cases in addition to the random values.
You want to be careful to assert types so that QuickCheck
knows which Arbitrary instance to get random values for test-
ing from. You can use verboseCheck to see what values were
tested. If you try running the check verbosely and without
asserting a type for the arguments:
Prelude&gt; verboseCheck monoidAssoc
Passed:
()
()</p>
<p>CHAPTER 15. MONOID, SEMIGROUP 929
()
(repeated 100 times)
This is GHCi’s type-defaulting biting you, as we saw back in
the Testing chapter. GHCi has slightly more aggressive type-
defaulting which can be handy in an interactive session when
you want to fire oﬀ some code and have your REPL pick a
winner for the typeclasses it doesn’t know how to dispatch.
Compiled in a source file, GHC would’ve complained about
an ambiguous type.
Testing left and right identity
Following on from what we did with associativity, we can also
useQuickCheck to test left and right identity:
monoidLeftIdentity ::(Eqm,Monoidm)
=&gt;m
-&gt;Bool
monoidLeftIdentity a=(mempty &lt;&gt;a)==a
monoidRightIdentity ::(Eqm,Monoidm)
=&gt;m
-&gt;Bool
monoidRightIdentity a=(a&lt;&gt;mempty) ==a</p>
<p>CHAPTER 15. MONOID, SEMIGROUP 930
Then running these properties against a Monoid :
Prelude&gt; quickCheck (monoidLeftIdentity :: String -&gt; Bool)
+++ OK, passed 100 tests.
Prelude&gt; quickCheck (monoidRightIdentity :: String -&gt; Bool)
+++ OK, passed 100 tests.
Testing QuickCheck’s patience
Let us see an example of QuickCheck catching us out for having
an invalid Monoid . Here we’re going to demonstrate why a Bool
Monoid can’t have Falseas the identity, always returning the
valueFalse, and still be a valid Monoid :
-- associative, left identity, and right
-- identity properties have been elided.
-- Add them to your copy of this.
importControl.Monad
importData.Monoid
importTest.QuickCheck
dataBull=
Fools
|Twoo
deriving (Eq,Show)</p>
<p>CHAPTER 15. MONOID, SEMIGROUP 931
instance Arbitrary Bullwhere
arbitrary =
frequency [ ( 1, return Fools)
, (1, return Twoo) ]
instance MonoidBullwhere
mempty=Fools
mappend _ _ =Fools
typeBullMappend =
Bull-&gt;Bull-&gt;Bull-&gt;Bool
main::IO()
main= do
letma=monoidAssoc
mli=monoidLeftIdentity
mlr=monoidRightIdentity
quickCheck (ma ::BullMappend )
quickCheck (mli ::Bull-&gt;Bool)
quickCheck (mlr ::Bull-&gt;Bool)
If you load this up in GHCi and run main, you’ll get the
following output:
Prelude&gt; main
+++ OK, passed 100 tests.</p>
<p>CHAPTER 15. MONOID, SEMIGROUP 932
*** Failed! Falsifiable (after 1 test):
Twoo
*** Failed! Falsifiable (after 1 test):
Twoo
So this not-actually-a- Monoid forBoolturns out to pass asso-
ciativity, but fail on the right and left identity checks. To see
why, let’s line up the laws against what our mempty andmappend
are:
-- how the instance is defined
mempty=Fools
mappend _ _ =Fools
-- identity laws
mappend mempty x =x
mappend x mempty =x
-- Does it obey the laws?
-- because of how mappend is defined
mappend mempty x =Fools
mappend x mempty =Fools
-- Fools is not x, so it
-- fails the identity laws.</p>
<p>CHAPTER 15. MONOID, SEMIGROUP 933
It’s fine if your identity value is Fools, but if your mappend
always returns the identity, then it’s not an identity. It’s not
behaving like a zero as you’re not even checking if either argu-
ment is Foolsbefore returning Fools. It’s a black hole that spits
out one value, which is senseless. For an example of what is
meant by zero, consider multiplication which has an identity
anda zero:
-- Thus why the mempty for Sum is 0
0+x==x
x+0==x
-- Thus why the mempty for Product is 1
1<em>x==x
x</em>1==x
-- Thus why the mempty for
-- Product is <em>not</em> 0
0<em>x==0
x</em>0==0
UsingQuickCheck can be a great way to cheaply and easily
sanity check the validity of your instances against their laws.
You’ll see more of this.</p>
<p>CHAPTER 15. MONOID, SEMIGROUP 934
Exercise: Maybe Another Monoid
Write a Monoid instance for a Maybetype which doesn’t require
aMonoid for the contents. Reuse the Monoid lawQuickCheck prop-
erties and use them to validate the instance.
Don’t forget to write an Arbitrary instance for First'. We
won’t always stub that out explicitly for you. We suggest
learning how to use the frequency function from QuickCheck
forFirst' ’s instance.
newtype First'a=
First'{ getFirst' ::Optional a }
deriving (Eq,Show)
instance Monoid(First'a)where
mempty=undefined
mappend =undefined
firstMappend ::First'a
-&gt;First'a
-&gt;First'a
firstMappend =mappend</p>
<p>CHAPTER 15. MONOID, SEMIGROUP 935
typeFirstMappend =
First'String
-&gt;First'String
-&gt;First'String
-&gt;Bool
typeFstId=
First'String-&gt;Bool
main::IO()
main= do
quickCheck (monoidAssoc ::FirstMappend )
quickCheck (monoidLeftIdentity ::FstId)
quickCheck (monoidRightIdentity ::FstId)
Our expected output demonstrates a diﬀerent Monoid for
Optional /Maybewhich is getting the first success and holding
onto it, where any exist. This could be seen, with a bit of
hand-waving, as being a disjunctive (“or”) Monoid instance.
Prelude&gt; First' (Only 1) <code>mappend</code> First' Nada
First' {getFirst' = Only 1}
Prelude&gt; First' Nada <code>mappend</code> First' Nada
First' {getFirst' = Nada}
Prelude&gt; First' Nada <code>mappend</code> First' (Only 2)
First' {getFirst' = Only 2}</p>
<p>CHAPTER 15. MONOID, SEMIGROUP 936
Prelude&gt; First' (Only 1) <code>mappend</code> First' (Only 2)
First' {getFirst' = Only 1}
15.13 Semigroup
Mathematicians play with algebras like that creepy kid you
knew in grade school who would pull legs oﬀ of insects. Some-
times, they glue legs onto insects too, but in the case where
we’re going from Monoid toSemigroup , we’re pulling a leg oﬀ.
In this case, the leg is our identity. To get from a monoid
to a semigroup, we simply no longer furnish nor require an
identity. The core operation remains binary and associative.
With this, our definition of Semigroup is:
classSemigroup awhere
(&lt;&gt;)::a-&gt;a-&gt;a
And we’re left with one law:
(a&lt;&gt;b)&lt;&gt;c=a&lt;&gt;(b&lt;&gt;c)
Semigroup still provides a binary associative operation, one
that typically joins two things together (as in concatenation or
summation), but doesn’t have an identity value. In that sense,
it’s a weaker algebra.</p>
<p>CHAPTER 15. MONOID, SEMIGROUP 937
Not yet part of base As of GHC 8, the Semigroup typeclass
is part of basebut not part of Prelude . You need to import
Data.Semigroup to use its operations. Keep in mind that it de-
fines its own more general version of (&lt;&gt;)which only requires
aSemigroup constraint rather than a Monoid constraint.
You can import the NonEmpty datatype we are about to discuss
into your REPL by importing Data.List.NonEmpty .
NonEmpty, a useful datatype
One useful datatype that can’t have a Monoid instance but does
have a Semigroup instance is the NonEmpty list type. It is a list
datatype that can never be an empty list:
dataNonEmpty a=a:|[a]
deriving (Eq,Ord,Show)
-- some instances from the
-- real module elided
Here:|is an infix data constructor that takes two (type)
arguments. It’s a product of aand[a]. It guarantees that we
always have at least one value of type 𝑎, which [a]does not
guarantee as any list might be empty.
Note that although :|is not alphanumeric, as most of the
other data constructors you’re used to seeing are, it is a name
for an infix data constructor. Data constructors with only</p>
<p>CHAPTER 15. MONOID, SEMIGROUP 938
nonalphanumeric symbols and that begin with a colon are
infix by default; those with alphanumeric names are prefix by
default:
-- Prefix, works.
dataP=
PrefixIntString
-- Infix, works.
dataQ=
Int:!!:String
Since that data constructor is symbolic rather than alphanu-
meric, it can’t be used as a prefix:
dataR=
:!!:IntString
Using it as a prefix will cause a syntax error:
parse error on input ‘:!!:’
Failed, modules loaded: none.
On the other hand, an alphanumeric data constructor can’t
be used as an infix:
dataS=
IntPrefixString</p>
<p>CHAPTER 15. MONOID, SEMIGROUP 939
It will cause another error:
Not in scope: type constructor or class ‘Prefix’
A data constructor of that name is in scope;
did you mean DataKinds?
Failed, modules loaded: none.
Let’s return to the main point, which is NonEmpty . Because
NonEmpty isa productof twoarguments, wecould’vealso written
it as:
newtype NonEmpty a=
NonEmpty (a, [a])
deriving (Eq,Ord,Show)
We can’t write a Monoid forNonEmpty because it has no identity
value by design! There is no empty list to serve as an identity
for any operation over a NonEmpty list, yet there is still a binary
associative operation: two NonEmpty lists can still be concate-
nated. A type with a canonical binary associative operation but
no identity value is a natural fit for Semigroup . Here is a brief
example of using NonEmpty from the semigroups library with the
semigroup mappend (as of GHC 8.0.1, Semigroup andNonEmpty are
both in basebut not in Prelude ):
-- you may need to install <code>semigroups</code>
Prelude&gt; import Data.List.NonEmpty as N</p>
<p>CHAPTER 15. MONOID, SEMIGROUP 940
Prelude N&gt; import Data.Semigroup as S
Prelude N S&gt; 1 :| [2, 3]
1 :| [2,3]
Prelude N S&gt; :t 1 :| [2, 3]
1 :| [2, 3] :: Num a =&gt; NonEmpty a
Prelude N S&gt; :t (&lt;&gt;)
(&lt;&gt;) :: Semigroup a =&gt; a -&gt; a -&gt; a
Prelude N S&gt; let xs = 1 :| [2, 3]
Prelude N S&gt; let ys = 4 :| [5, 6]
Prelude N S&gt; xs &lt;&gt; ys
1 :| [2,3,4,5,6]
Prelude N S&gt; N.head xs
1
Prelude N S&gt; N.length (xs &lt;&gt; ys)
6
Beyond this, you use NonEmpty as you would a list, but what
you’ve gained is being explicit that having zero values is not
valid for your use-case. The datatype helps you enforce this
constraint by not letting you construct a NonEmpty unless you
have at least one value.</p>
<p>CHAPTER 15. MONOID, SEMIGROUP 941
15.14 Strength can be weakness
When Haskellers talk about the strength of an algebra, they
usually mean the number of operations it provides which in
turn expands what you can do with any given instance of that
algebra without needing to know specifically what type you
are working with.
The reason we cannot and do not want to make all of our
algebras as big as possible is that there are datatypes which
are very useful representationally, but which do not have the
ability to satisfy everything in a larger algebra that could work
fine if you removed an operation or law. This becomes a seri-
ous problem if NonEmpty is the right datatype for something in
the domain you’re representing. If you’re an experienced pro-
grammer, think carefully. How many times have you meant
for a list to never be empty? To guarantee this and make the
types more informative, we use types like NonEmpty .
The problem is that NonEmpty has no identity value for the
combining operation ( mappend ) inMonoid. So, we keep the as-
sociativity but drop the identity value and its laws of left and
right identity. This is what introduces the need for and idea
ofSemigroup from a datatype.
The most obvious way to see that a monoid is stronger than
a semigroup is to observe that it has a strict superset of the op-
erations and laws that Semigroup provides. Anything which is a
monoid is by definition alsoa semigroup. It is to be hoped that</p>
<p>CHAPTER 15. MONOID, SEMIGROUP 942
Semigroup will be made a superclass of Monoid in an upcoming
version of GHC.
classSemigroup a=&gt;Monoidawhere
...
Earlier we reasoned about the inverse relationship between
operations permitted over a type and the number of types that
can satisfy. We can see this relationship between the number
of operations and laws an algebra demands and the number
of datatypes that can provide a law abiding instance of that
algebra.
In the following example, 𝑎can be anything in the universe,
but there are no operations over it — we can only return the
same value.
id::a-&gt;a
•Number of types: Infinite — universally quantified so
it can be any type the expression applying the function
wants.
•Number of operations: one, if you can call it an operation,
referencing the value you were passed.
Withinc𝑎now has all the operations from Num, which lets
us do more. But that also means it’s now a finite set of types</p>
<p>CHAPTER 15. MONOID, SEMIGROUP 943
that can satisfy the Numconstraint rather than being strictly any
type in the universe:
inc::Numa=&gt;a-&gt;a
•Number of types: anything that implements Num. Zero to
many.
•Number of operations: 7 methods in Num
In the next example we know it’s an Integer , which gives us
many more operations than just a Numinstance:
somethingInt ::Int-&gt;Int
•Number of types: one — Int.
•Number of operations: considerably more than 7. In ad-
dition to Num,Inthas instances of Bounded ,Enum,Eq,Integral ,
Ord,Read,Real, andShow. On top of that, you can write ar-
bitrary functions that pattern match on concrete types
and return arbitrary values in that same type as the re-
sult. Polymorphism isn’t only useful for reusing code;
it’s also useful for expressing intent through parametricity
so that people reading the code know what we meant to
accomplish.
WhenMonoid is too strong or more than we need, we can use
Semigroup . If you’re wondering what’s weaker than Semigroup ,</p>
<p>CHAPTER 15. MONOID, SEMIGROUP 944
the usual next step is removing the associativity requirement,
giving you a magma. It’s not likely to come up in day to day
Haskell, but you can sound cool at programming conferences
for knowing what’s weaker than a semigroup so pocket that
one for the pub.
15.15 Chapter exercises
Semigroup exercises
Given a datatype, implement the Semigroup instance. Add
Semigroup constraints to type variables where needed. Use the
Semigroup class from the semigroups library (or from baseif you
are on GHC 8) or write your own. When we use (&lt;&gt;), we mean
the infix mappend from the Semigroup typeclass.
Note We’re not always going to derive every instance you
may want or need in the datatypes we provide for exercises.
We expect you to know what you need and to take care of it
yourself by this point.
1.Validate allof your instances with QuickCheck. Since
Semigroup ’s only law is associativity, that’s the only prop-
erty you need to reuse. Keep in mind that you’ll poten-
tially need to import the modules for Monoid andSemigroup
and to avoid naming conflicts for the (&lt;&gt;)depending on
your version of GHC.</p>
<p>CHAPTER 15. MONOID, SEMIGROUP 945
dataTrivial =Trivial deriving (Eq,Show)
instance Semigroup Trivial where
_ &lt;&gt; _ = undefined
instance Arbitrary Trivial where
arbitrary =returnTrivial
semigroupAssoc ::(Eqm,Semigroup m)
=&gt;m-&gt;m-&gt;m-&gt;Bool
semigroupAssoc a b c=
(a&lt;&gt;(b&lt;&gt;c))==((a&lt;&gt;b)&lt;&gt;c)
typeTrivAssoc =
Trivial -&gt;Trivial -&gt;Trivial -&gt;Bool
main::IO()
main=
quickCheck (semigroupAssoc ::TrivAssoc )
2.newtype Identity a=Identity a
3.dataTwoa b=Twoa b
Hint: Ask for another Semigroup instance.</p>
<p>CHAPTER 15. MONOID, SEMIGROUP 946
4.dataThreea b c=Threea b c
5.dataFoura b c d =Foura b c d
6.newtype BoolConj =
BoolConj Bool
What it should do:
Prelude&gt; (BoolConj True) &lt;&gt; (BoolConj True)
BoolConj True
Prelude&gt; (BoolConj True) &lt;&gt; (BoolConj False)
BoolConj False
7.newtype BoolDisj =
BoolDisj Bool
What it should do:
Prelude&gt; (BoolDisj True) &lt;&gt; (BoolDisj True)
BoolDisj True
Prelude&gt; (BoolDisj True) &lt;&gt; (BoolDisj False)
BoolDisj True
8.dataOra b=
Fsta
|Sndb</p>
<p>CHAPTER 15. MONOID, SEMIGROUP 947
TheSemigroup forOrshould have the following behavior.
We can think of this as having a “sticky” Sndvalue where
it’ll hold onto the first Sndvalue when and if one is passed
as an argument. This is similar to the First' Monoid you
wrote earlier.
Prelude&gt; Fst 1 &lt;&gt; Snd 2
Snd 2
Prelude&gt; Fst 1 &lt;&gt; Fst 2
Fst 2
Prelude&gt; Snd 1 &lt;&gt; Fst 2
Snd 1
Prelude&gt; Snd 1 &lt;&gt; Snd 2
Snd 1
9.newtype Combine a b=
Combine { unCombine ::(a-&gt;b) }
What it should do:
Prelude&gt; let f = Combine $ \n -&gt; Sum (n + 1)
Prelude&gt; let g = Combine $ \n -&gt; Sum (n - 1)
Prelude&gt; unCombine (f &lt;&gt; g) $ 0
Sum {getSum = 0}
Prelude&gt; unCombine (f &lt;&gt; g) $ 1
Sum {getSum = 2}</p>
<p>CHAPTER 15. MONOID, SEMIGROUP 948
Prelude&gt; unCombine (f &lt;&gt; f) $ 1
Sum {getSum = 4}
Prelude&gt; unCombine (g &lt;&gt; f) $ 1
Sum {getSum = 2}
Hint: This function will eventually be applied to a single
value of type 𝑎. But you’ll have multiple functions that can
produce a value of type 𝑏. How do we combine multiple
values so we have a single 𝑏? This one will probably be
tricky! Remember that the type of the value inside of
Combine is that of a function . The type of functions should
already have an Arbitrary instance that you can reuse for
testing this instance.
10.newtype Compa=
Comp{ unComp ::(a-&gt;a) }
Hint: We can do something that seems a little more spe-
cific and natural to functions now that the input and out-
put types are the same.</p>
<p>CHAPTER 15. MONOID, SEMIGROUP 949
11.-- Look familiar?
dataValidation a b=
Failure a|Success b
deriving (Eq,Show)
instance Semigroup a=&gt;
Semigroup (Validation a b)where
(&lt;&gt;)=undefined
Given this code:
main= do
letfailure ::String
-&gt;Validation StringInt
failure =Failure
success ::Int
-&gt;Validation StringInt
success =Success
print$success 1&lt;&gt;failure &quot;blah&quot;
print$failure &quot;woot&quot;&lt;&gt;failure &quot;blah&quot;
print$success 1&lt;&gt;success 2
print$failure &quot;woot&quot;&lt;&gt;success 2
You should get this output:</p>
<p>CHAPTER 15. MONOID, SEMIGROUP 950
Prelude&gt; main
Success 1
Failure &quot;wootblah&quot;
Success 1
Success 2
Monoid exercises
Given a datatype, implement the Monoid instance. Add Monoid
constraints to type variables where needed. For the datatypes
you’ve already implemented Semigroup instances for, you need
to figure out what the identity value is.
1.Again, validate allof your instances with QuickCheck.
Example scaﬀold is provided for the Trivial type.</p>
<p>CHAPTER 15. MONOID, SEMIGROUP 951
dataTrivial =Trivial deriving (Eq,Show)
instance Semigroup Trivial where
(&lt;&gt;)=undefined
instance MonoidTrivial where
mempty=undefined
mappend =(&lt;&gt;)
typeTrivAssoc =
Trivial -&gt;Trivial -&gt;Trivial -&gt;Bool
main::IO()
main= do
letsa=semigroupAssoc
mli=monoidLeftIdentity
mlr=monoidRightIdentity
quickCheck (sa ::TrivAssoc )
quickCheck (mli ::Trivial -&gt;Bool)
quickCheck (mlr ::Trivial -&gt;Bool)
2.newtype Identity a=
Identity aderiving Show
3.dataTwoa b=Twoa bderiving Show</p>
<p>CHAPTER 15. MONOID, SEMIGROUP 952
4.newtype BoolConj =
BoolConj Bool
What it should do:
Prelude&gt; (BoolConj True) <code>mappend</code> mempty
BoolConj True
Prelude&gt; mempty <code>mappend</code> (BoolConj False)
BoolConj False
5.newtype BoolDisj =
BoolDisj Bool
What it should do:
Prelude&gt; (BoolDisj True) <code>mappend</code> mempty
BoolDisj True
Prelude&gt; mempty <code>mappend</code> (BoolDisj False)
BoolDisj False
6.newtype Combine a b=
Combine { unCombine ::(a-&gt;b) }
What it should do:
Prelude&gt; let f = Combine $ \n -&gt; Sum (n + 1)
Prelude&gt; unCombine (mappend f mempty) $ 1
Sum {getSum = 2}</p>
<p>CHAPTER 15. MONOID, SEMIGROUP 953
7.Hint: We can do something that seems a little more spe-
cific and natural to functions now that the input and out-
put types are the same.
newtype Compa=
Comp(a-&gt;a)
8.This next exercise will involve doing something that will
feel a bit unnatural still and you may find it difficult. If you
get it and you haven’t done much FP or Haskell before,
get yourself a nice beverage. We’re going to toss you
the instance declaration so you don’t churn on a missing
Monoid constraint you didn’t know you needed.
newtype Mems a=
Mem{
runMem::s-&gt;(a,s)
}
instance Monoida=&gt;Monoid(Mems a)where
mempty=undefined
mappend =undefined
Given the following code:</p>
<p>CHAPTER 15. MONOID, SEMIGROUP 954
f'=Mem$\s-&gt;(&quot;hi&quot;, s+1)
main= do
letrmzero=runMem mempty 0
rmleft=runMem (f' &lt;&gt;mempty) 0
rmright =runMem (mempty &lt;&gt;f')0
print$rmleft
print$rmright
print$(rmzero ::(String,Int))
print$rmleft==runMem f' 0
print$rmright ==runMem f' 0
A correct Monoid forMemshould, given the above code, get
the following output:
Prelude&gt; main
(&quot;hi&quot;,1)
(&quot;hi&quot;,1)
(&quot;&quot;,0)
True
True
Make certain your instance has output like the above, this
is sanity-checking the Monoid identity laws for you! It’s not
a proof and it’s not even as good as property testing, but
it’ll catch the most common mistakes people make.</p>
<p>CHAPTER 15. MONOID, SEMIGROUP 955
It’s not a trick and you don’t need a Monoid for𝑠. Yes, such
aMonoid can and does exist. Hint: chain the 𝑠values from
one function to the other. You’ll want to check the identity
laws as a common first attempt will break them.
15.16 Definitions
1.Amonoid is a set that is closed under an associative binary
operation and has an identity element. Closed is the posh
mathematical way of saying its type is:
mappend ::m-&gt;m-&gt;m
Such that your arguments and output will always inhabit
the same type (set).
2.Asemigroup is a set that is closed under an associative
binary operation — and nothing else.
3.Laws are rules about how an algebra or structure should
behave. These are needed in part to make abstraction over
the commonalities of diﬀerent instantiations of the same
sort of algebra possible and practical . This is critical to
having abstractions which aren’t unpleasantly surprising.
4.Analgebra is variously:</p>
<p>CHAPTER 15. MONOID, SEMIGROUP 956
a)School algebra, such as that taught in primary and
secondary school. This usually entails the balancing
of polynomial equations and learning how functions
and graphs work.
b)The study of number systems and operations within
them. This will typically entail a particular area such
as groups or rings. This is what mathematicians com-
monly mean by “algebra.” This is sometimes disam-
biguated by being referred to as abstract algebra.
c)A third and final way algebra is used is to refer to a
vector space over a field with a multiplication.
When Haskellers refer to algebras, they’re usually talking
about a somewhat informal notion of operations over
a type and its laws, such as with semigroups, monoids,
groups, semirings, and rings.
15.17 Follow-up resources
1.Algebraic structure; Simple English Wikipedia
2.Haskell Monoids and Their Uses; Dan Piponi</p>
<p>Chapter 16
Functor
Lifting is the ”cheat
mode” of type tetris.
Michael Neale
957</p>
<p>CHAPTER 16. FUNCTOR 958
16.1 Functor
In the last chapter on Monoid , we saw what it means to talk about
an algebra and turn that into a typeclass. This chapter and
the two that follow, on Applicative andMonad, will be similar.
Each of these algebras is more powerful than the last, but the
general concept here will remain the same: we abstract out
a common pattern, make certain it follows some laws, give it
an awesome name, and wonder how we ever lived without it.
Monadsort of steals the Haskell spotlight, but you can do more
withFunctor andApplicative than many people realize. Also,
understanding Functor andApplicative is important to a deep
understanding of Monad.
This chapter is all about Functor , andFunctor is all about a
pattern of mapping over structure. We saw fmapway back in
the chapter on lists and noted that it worked just the same as
map, but we alsosaid back then that the diﬀerence is that you
can use fmapwith structures that aren’t lists . Now we will begin
to see what that means.
The great logician Rudolf Carnap appears to have been the
first person to use the word functor in the 1930s. He invented
the word to describe grammatical function words and logical
operations over sentences or phrases. Functors are combina-
tors: they take a sentence or phrase as input and produce a
sentence or phrase as an output, with some logical operation
applied to the whole. For example, negation is a functor in</p>
<p>CHAPTER 16. FUNCTOR 959
this sense because when negation is applied to a sentence, 𝐴,
it produces the negated version, ¬𝐴, as an output. It lifts the
concept of negation over the entire sentence or phrase struc-
ture without changing the internal structure. (Yes, in English
the negation word often appears inside the sentence, not on
the outside, but he was a logician and unconcerned with how
normal humans produced such pedestrian things as spoken
sentences. In logic, the negation operator is typically written
as a prefix, as above.)
This chapter will include:
•the return of the higher-kinded types;
•fmaps galore, and not only on lists;
•no more digressions about dusty logicians;
•words about typeclasses and constructor classes;
•puns based on George Clinton music, probably.
16.2 What’s a functor?
A functor is a way to apply a function over or around some
structure that we don’t want to alter. That is, we want to apply
the function to the value that is “inside” some structure and
leave the structure alone. That’s why it is most common to
introduce functor by way of fmapping over lists, as we did</p>
<p>CHAPTER 16. FUNCTOR 960
back in the lists chapter. The function gets applied to each
value inside the list, and the list structure remains. A good way
to relate “not altering the structure” to lists is that the length
of the list after mapping a function over it will always be the
same. No elements are removed or added, only transformed.
The typeclass Functor generalizes this pattern so that we can
use that basic idea with many types of structure, not just lists.
Functor is implemented in Haskell with a typeclass, just like
Monoid . Other means of implementing it are possible, but this
is the most convenient way to do so. The definition of the
Functor typeclass looks like this:
classFunctor fwhere
fmap::(a-&gt;b)-&gt;f a-&gt;f b
Now let’s dissect this a bit:
classFunctor fwhere
[1] [2] [3] [4]
fmap::(a-&gt;b)-&gt;f a-&gt;f b
[5] [ 6] [ 7] [8]
1.classis the keyword to begin the definition of a typeclass.
2.Functor is the name of the typeclass we are defining.</p>
<p>CHAPTER 16. FUNCTOR 961
3.Typeclasses in Haskell usually refer to a type. The letters
themselves, as with type variables in type signatures, do
not mean anything special. 𝑓is a conventional letter to
choose when referring to types that have functorial struc-
ture. The 𝑓must be the same 𝑓throughout the typeclass
definition.
4.Thewherekeyword ends the declaration of the typeclass
name and associated types. After the wherethe operations
provided by the typeclass are listed.
5.We begin the declaration of an operation named fmap.
6.The argument a -&gt; b is any Haskell function of that type
(remembering that it could be an (a -&gt; a) function for
this purpose).
7.The argument f ais aFunctor𝑓that takes a type argument
𝑎. That is, the 𝑓is a type that has an instance of the Functor
typeclass.
8.The return value is f b. It is the same𝑓fromf a, while
the type argument 𝑏possibly but not necessarily refers to a
diﬀerent type.
Before we delve into the details of how this typeclass works,
let’s see fmapin action so you get a feel for what’s going on first.</p>
<p>CHAPTER 16. FUNCTOR 962
16.3 There’s a whole lot of fmapgoin’
round
We have seen fmapbefore but we haven’t used it much except
for with lists. With lists, it seems to do the same thing as map:
Prelude&gt; map (\x -&gt; x &gt; 3) [1..6]
[False,False,False,True,True,True]
Prelude&gt; fmap (\x -&gt; x &gt; 3) [1..6]
[False,False,False,True,True,True]
Listis, of course, one type that implements the typeclass
Functor , but it seems unremarkable when it just does the same
thing as map. However, Listisn’t the only type that implements
Functor , andfmapcan apply a function over or around any of
those functorial structures, while mapcannot:
Prelude&gt; map (+1) (Just 1)
Couldn't match expected type ‘[b]’
with actual type ‘Maybe a0’
Relevant bindings include
it :: [b] (bound at 16:1)
In the second argument of ‘map’,
namely ‘(Just 1)’</p>
<p>CHAPTER 16. FUNCTOR 963
In the expression: map (+ 1) (Just 1)
Prelude&gt; fmap (+1) (Just 1)
Just 2
Intriguing! What else?
--with a tuple!
Prelude&gt; fmap (10/) (4, 5)
(4,2.0)
--with Either!
Prelude&gt; let rca = Right &quot;Chris Allen&quot;
Prelude&gt; fmap (++ &quot;, Esq.&quot;) rca
Right &quot;Chris Allen, Esq.&quot;
We can see how the type of fmapspecializes to diﬀerent
types here:</p>
<p>CHAPTER 16. FUNCTOR 964
typeEe=Eithere
typeCe=Constant e
typeI=Identity
-- Functor f =&gt;
fmap::(a-&gt;b)-&gt;f a-&gt;f b
::(a-&gt;b)-&gt;[ ] a-&gt;[ ] b
::(a-&gt;b)-&gt;Maybea-&gt;Maybeb
::(a-&gt;b)-&gt;Ee a-&gt;Ee b
::(a-&gt;b)-&gt;(e,) a-&gt;(e,) b
::(a-&gt;b)-&gt;Ia-&gt;Ib
::(a-&gt;b)-&gt;Ce a-&gt;Ce b
If you are using GHC 8 or newer, you can also see this for
yourself in your REPL by doing this:
Prelude&gt; :set -XTypeApplications
Prelude&gt; :type fmap @Maybe
fmap @Maybe ::
(a -&gt; b) -&gt; Maybe a -&gt; Maybe b
Prelude&gt; :type fmap @(Either _)
fmap @(Either _) ::
(a -&gt; b) -&gt; Either t a -&gt; Either t b
You may have noticed in the tuple and Either examples that
the first arguments (labeled 𝑒in the above chart) are ignored</p>
<p>CHAPTER 16. FUNCTOR 965
byfmap. We’ll talk about why that is in just a bit. Let’s first turn
our attention to what makes a functor. Later we’ll come back
to longer examples and expand on this considerably.
16.4 Let’s talk about 𝑓, baby
As we said above, the 𝑓in the typeclass definition for Functor
must be the same 𝑓throughout the entire definition, and it
must refer to a type that implements the typeclass. This sec-
tion details the practical ramifications of those facts.
The first thing we know is that our 𝑓here must have the kind</p>
<ul>
<li>-&gt; * . We talked about higher-kinded types in previous chap-
ters, and we recall that a type constant or a fully applied type
has the kind *. A type with kind * -&gt; * is awaiting application
to a type constant of kind *.
We know that the 𝑓in ourFunctor definition must be kind *
-&gt; *for a couple of reasons, which we will first describe and
then demonstrate:
1.Each argument (and result) in the type signature for a
function must be a fully applied (and inhabitable, modulo
Void, etc.) type. Each argument must have the kind *.
2.The type 𝑓was applied to a single argument in two dif-
ferent places: f aandf b. Since f aandf bmust each
have the kind *,𝑓by itself must be kind * -&gt; * .</li>
</ul>
<p>CHAPTER 16. FUNCTOR 966
It’s easier to see what these mean in practice by demonstrat-
ing with lots of code, so let’s tear the roof oﬀ this sucker.
Shining star come into view
Every argument to the type constructor of -&gt;must be of kind
*. We can verify this simply by querying kind of the function
type constructor for ourselves:
Prelude&gt; :k (-&gt;)
(-&gt;) :: * -&gt; * -&gt; *
Each argument and result of every function must be a type
constant, not a type constructor. Given that knowledge, we
can know something about Functor from the type of fmap:
classFunctor fwhere
fmap::(a-&gt;b)-&gt;f a-&gt;f b
--has kind: * -&gt; * -&gt; *
The type signature of fmaptells us that the 𝑓introduced
by the class definition for Functor mustaccept a single type
argument and thus be of kind * -&gt; *. We can determine this
even without knowing anything about the typeclass, which
we’ll demonstrate with some meaningless typeclasses:</p>
<p>CHAPTER 16. FUNCTOR 967
classSumthin awhere
s::a-&gt;a
classElsewhere
e::b-&gt;f (g a b c)
classBiffywhere
slayer::e a b
-&gt;(a-&gt;c)
-&gt;(b-&gt;d)
-&gt;e c d
Let’s deconstruct the previous couple of examples:
classSumthin awhere
s::a-&gt;a
-- [1] [1]
1.The argument and result type are both 𝑎. There’s nothing
else, so 𝑎has kind *.
classElsewhere
e::b-&gt;f (g a b c)
-- [1] [2] [3]
1.This𝑏, like𝑎in the previous example, stands alone as the
first argument to (-&gt;), so it is kind *.</p>
<p>CHAPTER 16. FUNCTOR 968
2.Here𝑓is the outermost type constructor for the second
argument (the result type) of (-&gt;). It takes a single argu-
ment, the type g a b c wrapped in parentheses. Thus, 𝑓
has kind * -&gt; * .
3.And𝑔is applied to three arguments 𝑎,𝑏, and𝑐. That means
it is kind * -&gt; * -&gt; * -&gt; * , where:
-- using :: to denote kind signature
g:: * -&gt; * -&gt; * -&gt; *
-- a, b, and c are each kind *
g:: * -&gt; * -&gt; * -&gt; *
ga b c (g a b c)
classBiffywhere
slayer::e a b
-- [1]
-&gt;(a-&gt;c)
-- [2] [3]
-&gt;(b-&gt;d)
-&gt;e c d
1.First,𝑒is an argument to (-&gt;)so the application of its
arguments must result in kind *. Given that, and knowing</p>
<p>CHAPTER 16. FUNCTOR 969
there are two arguments, 𝑎and𝑏, we can determine 𝑒is
kind* -&gt; * -&gt; * .
2.This𝑎is an argument to a function that takes no argu-
ments itself, so it’s kind *
3.The story for 𝑐is identical to 𝑎, just in another spot of the
same function.
The kind checker is going to fail on the next couple of
examples:
classImpishvwhere
impossibleKind ::v-&gt;v a
classAlsoImp vwhere
nope::v a-&gt;v
Remember that the name of the variable before the where
in a typeclass definition binds the occurrences of that name
throughout the definition. GHC will notice that our 𝑣some-
times has a type argument and sometimes not, and it will call
our bluﬀ if we attempt to feed it this nonsense:
‘v’ is applied to too many type arguments
In the type ‘v -&gt; v a’
In the class declaration for ‘Impish’</p>
<p>CHAPTER 16. FUNCTOR 970
Expecting one more argument to ‘v’
Expected a type, but ‘v’ has kind ‘k0 -&gt; *’
In the type ‘v a -&gt; v’
In the class declaration for ‘AlsoImp’
Just as GHC has type inference, it also has kind inference.
And just as it does with types, it can not only infer the kinds
but also validate that they’re consistent and make sense.
Exercises: Be Kind
Given a type signature, determine the kinds of each type vari-
able:
1.What’s the kind of 𝑎?
a-&gt;a
2.What are the kinds of 𝑏and𝑇? (The𝑇is capitalized on
purpose!)
a-&gt;b a-&gt;T(b a)
3.What’s the kind of 𝑐?
ca b-&gt;c b a</p>
<p>CHAPTER 16. FUNCTOR 971
A shining star for you to see
So, what if our type isn’t higher kinded? Let’s try it with a type
constant and see what happens:
-- functors1.hs
dataFixMePls =
FixMe
|Pls
deriving (Eq,Show)
instance Functor FixMePls where
fmap=
error
&quot;it doesn't matter, it won't compile&quot;
Notice there are no type arguments anywhere — everything
is one shining (kind) star! And if we load this file from GHCi,
we’ll get the following error:
Prelude&gt; :l functors1.hs
[1 of 1] Compiling Main
( functors1.hs, interpreted )
functors1.hs:8:18:
The first argument of ‘Functor’</p>
<p>CHAPTER 16. FUNCTOR 972
should have kind ‘* -&gt; <em>’,
but ‘FixMePls’ has kind ‘</em>’
In the instance declaration for
‘Functor FixMePls’
Failed, modules loaded: none.
In fact, asking for a Functor forFixMePls doesn’t really make
sense. To see why this doesn’t make sense, consider the types
involved:
-- Functor is:
fmap::Functor f=&gt;(a-&gt;b)-&gt;f a-&gt;f b
-- If we replace f with FixMePls
(a-&gt;b)-&gt;FixMePls a-&gt;FixMePls b
-- But FixMePls doesn't take
-- type arguments, so this is
-- really more like:
(FixMePls -&gt;FixMePls )
-&gt;FixMePls
-&gt;FixMePls
There’s no type constructor 𝑓in there! The maximally
polymorphic version of this is:
(a-&gt;b)-&gt;a-&gt;b</p>
<p>CHAPTER 16. FUNCTOR 973
So in fact, not having a type argument means this is:
($)::(a-&gt;b)-&gt;a-&gt;b
Without a type argument, this is mere function application.
Functor is function application
We just saw how trying to make a Functor instance for a type
constant means you have function application. But, in fact,
fmapis a specific sort of function application. Let’s look at the
types:
fmap::Functor f=&gt;(a-&gt;b)-&gt;f a-&gt;f b
There is also an infix operator for fmap. If you’re using an
older version of GHC, you may need to import Data.Functor
in order to use it in the REPL. Of course, it has the same type
as the prefix fmap:
-- &lt;$&gt; is the infix alias for fmap:
(&lt;$&gt;)::Functor f
=&gt;(a-&gt;b)
-&gt;f a
-&gt;f b
Notice something?</p>
<p>CHAPTER 16. FUNCTOR 974
(&lt;$&gt;)::Functor f
=&gt;(a-&gt;b)-&gt;f a-&gt;f b
($)::(a-&gt;b)-&gt;a-&gt;b
Functor is a typeclass for function application “over”, or
“through”, some structure fthat we want to ignore and leave
untouched. We’ll explain “leave untouched” in more detail
later when we talk about the Functor laws.
A shining star for you to see what your 𝑓can
truly be
Let’s resume our exploration of why we need a higher-kinded
𝑓.
If we add a type argument to the datatype from above, we
makeFixMePls into a type constructor, and this will work:</p>
<p>CHAPTER 16. FUNCTOR 975
-- functors2.hs
dataFixMePls a=
FixMe
|Plsa
deriving (Eq,Show)
instance Functor FixMePls where
fmap=
error
&quot;it doesn't matter, it won't compile&quot;
Now it’ll compile!
Prelude&gt; :l code/functors2.hs
[1 of 1] Compiling Main
Ok, modules loaded: Main.
But wait, we don’t need the error anymore! Let’s fix that
Functor instance:</p>
<p>CHAPTER 16. FUNCTOR 976
-- functors3.hs
dataFixMePls a=
FixMe
|Plsa
deriving (Eq,Show)
instance Functor FixMePls where
fmap_FixMe=FixMe
fmap f ( Plsa)=Pls(f a)
Let’s see how our instance lines up with the type of fmap:
fmap::Functor f
=&gt;(a-&gt;b)-&gt;f a-&gt;f b
fmap f ( Plsa)=Pls(f a)
-- (a -&gt; b) f a f b
While𝑓is used in the type of fmapto represent the Functor ,
by convention, it is also conventionally used in function def-
initions to name an argument that is itself a function. Don’t let
the names fool you into thinking the 𝑓in ourFixMePls instance
is the same 𝑓as in the Functor typeclass definition.
Now our code is happy-making!
Prelude&gt; :l code/functors3.hs</p>
<p>CHAPTER 16. FUNCTOR 977
[1 of 1] Compiling Main
Ok, modules loaded: Main.
Prelude&gt; fmap (+1) (Pls 1)
Pls 2
Notice the function gets applied over and inside of the
structure. This is how Haskell coders lift big heavy functions
over abstract structure!
Okay, let’s make another mistake for the sake of being ex-
plicit. What if we change the type of our Functor instance from
FixMePls toFixMePls a ?
-- functors4.hs
dataFixMePls a=
FixMe
|Plsa
deriving (Eq,Show)
instance Functor (FixMePls a)where
fmap_FixMe=FixMe
fmap f ( Plsa)=Pls(f a)
Notice we didn’t change the type; it still only takes one
argument. But now that argument is part of the 𝑓structure. If
we load this ill-conceived code:</p>
<p>CHAPTER 16. FUNCTOR 978
Prelude&gt; :l functors4.hs
[1 of 1] Compiling Main
functors4.hs:8:19:
The first argument of ‘Functor’
should have kind ‘* -&gt; <em>’,
but ‘FixMePls a’ has kind ‘</em>’
In the instance declaration for
‘Functor (FixMePls a)’
Failed, modules loaded: none.
We get the same error as earlier, because applying the type
constructor gave us something of kind <em>from the original kind
of</em> -&gt; * .
Typeclasses and constructor classes
You may have initially paused on the type constructor 𝑓in
the definition of Functor having kind * -&gt; * — this is quite
natural! In fact, earlier versions of Haskell didn’t have a facility
for expressing typeclasses in terms of higher-kinded types
at all. This was developed by Mark P. Jones1while he was
working on an implementation of Haskell called Gofer. This
work generalized typeclasses from being usable only with
types of kind *(also called type constants ) to being usable with</p>
<p>CHAPTER 16. FUNCTOR 979
higher-kinded types, called type constructors , as well.
In Haskell, the two use cases have been merged such that
we don’t call out constructor classes as being separate from
typeclasses, but we think it’s useful to highlight that something
significant has happened here. Now we have a means of talking
about the contents of types independently from the type that
structures those contents. That’s why we can have something
likefmapthat allows us to alter the contents of a value without
altering the structure (a list, or a Just) around the value.
16.5 Functor Laws
Instances of the Functor typeclass should abide by two basic
laws. Understanding these laws is critical for understanding
Functor and writing typeclass instances that are composable
and easy to reason about.
Identity
The first law is the law of identity:
fmapid==id
If wefmapthe identity function, it should have the same
result as passing our value to identity. We shouldn’t be chang-
1A system of constructor classes: overloading and implicit higher-order polymor-
phism
http://www.cs.tufts.edu/~nr/cs257/archive/mark-jones/fpca93.pdf</p>
<p>CHAPTER 16. FUNCTOR 980
ing any of the outer structure 𝑓that we’re mapping over by
mapping id. That’s why it’s the same as id. If we didn’t return
a new value in the a -&gt; b function mapped over the structure,
then nothing should’ve changed:
Prelude&gt; fmap id &quot;Hi Julie&quot;
&quot;Hi Julie&quot;
Prelude&gt; id &quot;Hi Julie&quot;
&quot;Hi Julie&quot;
Try it out on a few diﬀerent structures and check for your-
self.
Composition
The second law for Functor is the law of composition:
fmap(f.g)==fmap f.fmap g
This concerns the composability of fmap. If we compose
two functions, 𝑓and𝑔, andfmapthat over some structure, we
should get the same result as if we fmapped them and then
composed them:
Prelude&gt; fmap ((+1) . (*2)) [1..5]
[3,5,7,9,11]
Prelude&gt; fmap (+1) . fmap (*2) $ [1..5]
[3,5,7,9,11]</p>
<p>CHAPTER 16. FUNCTOR 981
If an implementation of fmapdoesn’t do that, it’s a broken
functor.
Structure preservation
Both of these laws touch on the essential rule that functors
must be structure preserving.
All we’re allowed to know in the type about our instance of
Functor implemented by 𝑓is that it implements Functor :
fmap::Functor f=&gt;(a-&gt;b)-&gt;f a-&gt;f b
The𝑓is constrained by the typeclass Functor , but that is all
we know about its type from this definition. As we’ve seen with
typeclass-constrained polymorphism, this still allows it to be
any type that has an instance of Functor . The core operation
that this typeclass provides for these types is fmap. Because the
𝑓persists through the type of fmap, whatever the type is, we
know it must be a type that can take an argument, as in f aand
f band that it will be the “structure” we’re lifting the function
over when we apply it to the value inside.
16.6 The Good, the Bad, and the Ugly
We’ll get a better picture of what it means for Functor instances
to be law-abiding or law-breaking by walking through some</p>
<p>CHAPTER 16. FUNCTOR 982
examples. We start by definining a type constructor with one
argument:
dataWhoCares a=
ItDoesnt
|Mattera
|WhatThisIsCalled
deriving (Eq,Show)
This datatype only has one data constructor containing a
value we could fmapover, and that is Matter. The others are
nullary so there is no value to work with inside the structure;
there is only structure.
Here we see a law-abiding instance:
instance Functor WhoCares where
fmap_ItDoesnt =ItDoesnt
fmap_WhatThisIsCalled =
WhatThisIsCalled
fmap f ( Mattera)=Matter(f a)
Our instance must follow the identity law or else it’s not a
valid functor. That law dictates that fmap id (Matter _) must
nottouchMatter — that is, it must be identical to id (Matter _) .
Functor is a way of lifting over structure (mapping) in such a
manner that you don’t have to care about the structure because
you’re not allowed to touch the structure anyway.</p>
<p>CHAPTER 16. FUNCTOR 983
Let us next consider a law-breaking instance:
instance Functor WhoCares where
fmap_ItDoesnt =WhatThisIsCalled
fmap fWhatThisIsCalled =ItDoesnt
fmap f ( Mattera)=Matter(f a)
Nowwecontemplatewhatitmeanstoleavethestructureun-
touched. In this instance, we’ve made our structure — not the
values wrapped or contained within the structure — change
by making ItDoesnt andWhatThisIsCalled do a little dosey-do.
It becomes rapidly apparent why this isn’t kosher at all.
Prelude&gt; fmap id ItDoesnt
WhatThisIsCalled
Prelude&gt; fmap id WhatThisIsCalled
ItDoesnt
Prelude&gt; fmap id ItDoesnt == id ItDoesnt
False
Prelude&gt; :{
*Main| fmap id WhatThisIsCalled ==
*Main| id WhatThisIsCalled
*Main| :}
False
This certainly does not abide by the identity law. It is not a
validFunctor instance.</p>
<p>CHAPTER 16. FUNCTOR 984
The law won But what if you dowant a function that can
change the value andthe structure?
We’ve got wonderful news for you: that exists! It’s a plain
old function. Write one. Write many! The point of Functor is
to reify and be able to talk about cases where we want to reuse
functions in the presence of more structure and be transpar-
entlyoblivious to that additional structure. We already saw that
Functor is in some sense a special sort of function application,
but since it is special , we want to preserve the things about
it that make it diﬀerent and more powerful than ordinary
function application. So, we stick to the laws.
Later in this chapter, we will talk about a sort of opposite,
where you can transform the structure but leave the type ar-
gument alone. This has a special name too, but there isn’t a
widely agreed upon typeclass.
Composition should just work
All right, now that we’ve seen how we can make a Functor in-
stance violate the identity law, let’s take a look at how we abide
by — and break! — the composition law. You may recall from
above that the law looks like this:
fmap (f . g) == fmap f . fmap g
Technically this follows from fmap id == id , but it’s worth
calling out so that we can talk about composition. This law</p>
<p>CHAPTER 16. FUNCTOR 985
says composing two functions lifted separately should pro-
duce the same result as if we composed the functions ahead
of time and then lifted the composed function all together.
Maintaining this property is about preserving composability
of our code and preventing our software from doing unpleas-
antly surprising things. We will now consider another invalid
Functor instance to see why this is bad news:
dataCountingBad a=
Heisenberg Inta
deriving (Eq,Show)
-- super NOT okay
instance Functor CountingBad where
fmap f ( Heisenberg n a)=
-- (a -&gt; b) f a =
Heisenberg (n+1) (f a)
-- f b
Well, what did we do here? CountingBad has one type argu-
ment, but Heisenberg has two arguments. If you look at how
that lines up with the type of fmap, you get a hint of why this
isn’t going to work out well. What part of our fmaptype does
the𝑛representing the Intargument to Heisenberg belong to?
We can load this horribleness up in the REPL and see that
composing two fmaps here does not produce the same results,</p>
<p>CHAPTER 16. FUNCTOR 986
so the composition law doesn’t hold:
Prelude&gt; let u = &quot;Uncle&quot;
Prelude&gt; let oneWhoKnocks = Heisenberg 0 u
Prelude&gt; fmap (++&quot; Jesse&quot;) oneWhoKnocks
Heisenberg 1 &quot;Uncle Jesse&quot;
Prelude&gt; let f = ((++&quot; Jesse&quot;).(++&quot; lol&quot;))
Prelude&gt; fmap f oneWhoKnocks
Heisenberg 1 &quot;Uncle lol Jesse&quot;
So far it seems OK, but what if we compose the two con-
catenation functions separately?
Prelude&gt; let j = (++ &quot; Jesse&quot;)
Prelude&gt; let l = (++ &quot; lol&quot;)
Prelude&gt; fmap j . fmap l $ oneWhoKnocks
Heisenberg 2 &quot;Uncle lol Jesse&quot;
Or to make it look more like the law:
Prelude&gt; let f = (++&quot; Jesse&quot;)
Prelude&gt; let g = (++&quot; lol&quot;)
Prelude&gt; fmap (f . g) oneWhoKnocks
Heisenberg 1 &quot;Uncle lol Jesse&quot;
Prelude&gt; fmap f . fmap g $ oneWhoKnocks
Heisenberg 2 &quot;Uncle lol Jesse&quot;
We can clearly see that</p>
<p>CHAPTER 16. FUNCTOR 987
fmap (f . g) == fmap f . fmap g
does not hold. So how do we fix it?
dataCountingGood a=
Heisenberg Inta
deriving (Eq,Show)
-- Totes cool.
instance Functor CountingGood where
fmap f ( Heisenberg n a)=
Heisenberg (n) (f a)
Stop messing with the IntinHeisenberg . Think of anything
that isn’t the final type argument of our 𝑓inFunctor as being
part of the structure that the functions being lifted should be
oblivious to.
16.7 Commonly used functors
Now that we have a sense of what Functor does for us and
how it’s meant to work, it’s time to start working through
some longer examples. This section is nearly all code and
examples with minimal prose explanation. Interacting with
these examples will help you develop an intuition for what’s
going on with a minimum of fuss.
We begin with a utility function:</p>
<p>CHAPTER 16. FUNCTOR 988
Prelude&gt; :t const
const :: a -&gt; b -&gt; a
Prelude&gt; let replaceWithP = const 'p'
Prelude&gt; replaceWithP 10000
'p'
Prelude&gt; replaceWithP &quot;woohoo&quot;
'p'
Prelude&gt; replaceWithP (Just 10)
'p'
We’ll use it with fmapnow for various datatypes that have
instances:
-- data Maybe a = Nothing | Just a
Prelude&gt; fmap replaceWithP (Just 10)
Just 'p'
Prelude&gt; fmap replaceWithP Nothing
Nothing
-- data [] a = [] | a : [a]
Prelude&gt; fmap replaceWithP [1, 2, 3, 4, 5]
&quot;ppppp&quot;
Prelude&gt; fmap replaceWithP &quot;Ave&quot;
&quot;ppp&quot;</p>
<p>CHAPTER 16. FUNCTOR 989
Prelude&gt; fmap (+1) []
[]
Prelude&gt; fmap replaceWithP []
&quot;&quot;
-- data (,) a b = (,) a b
Prelude&gt; fmap replaceWithP (10, 20)
(10,'p')
Prelude&gt; fmap replaceWithP (10, &quot;woo&quot;)
(10,'p')
Again, we’ll talk about why it skips the first value in the
tuple in a bit. It has to do with the kindedness of tuples and
the kindedness of the 𝑓inFunctor .
Now the instance for functions:
Prelude&gt; negate 10
-10
Prelude&gt; let tossEmOne = fmap (+1) negate
Prelude&gt; tossEmOne 10
-9
Prelude&gt; tossEmOne (-10)
11
The functor of functions won’t be discussed in great detail
until we get to the chapter on Reader, but it should look sort
of familiar:</p>
<p>CHAPTER 16. FUNCTOR 990
Prelude&gt; let tossEmOne' = (+1) . negate
Prelude&gt; tossEmOne' 10
-9
Prelude&gt; tossEmOne' (-10)
11
Now you’re starting to get into the groove; let’s see what
else we can do with our fancy new moves.
The functors are stacked and that’s a fact
We can combine datatypes, as we’ve seen, usually by nesting
them. We’ll be using the tilde character as a shorthand for “is
roughly equivalent to” throughout these examples:
-- lms ~ List (Maybe (String))
Prelude&gt; let n = Nothing
Prelude&gt; let w = Just &quot;woohoo&quot;
Prelude&gt; let ave = Just &quot;Ave&quot;
Prelude&gt; let lms = [ave, n, w]
Prelude&gt; let replaceWithP = const 'p'
Prelude&gt; replaceWithP lms
'p'
Prelude&gt; fmap replaceWithP lms
&quot;ppp&quot;</p>
<p>CHAPTER 16. FUNCTOR 991
Nothing unexpected there, but we notice that lmshas more
than one Functor type.Maybeand List (which includes String)
both have Functor instances. So, are we obligated to fmaponly
to the outermost datatype? No way, mate:
Prelude&gt; (fmap . fmap) replaceWithP lms
[Just 'p',Nothing,Just 'p']
Prelude&gt; let tripFmap = fmap . fmap . fmap
Prelude&gt; tripFmap replaceWithP lms
[Just &quot;ppp&quot;,Nothing,Just &quot;pppppp&quot;]
Let’s review in detail:
-- lms ~ List (Maybe String)
Prelude&gt; let ave = Just &quot;Ave&quot;
Prelude&gt; let n = Nothing
Prelude&gt; let w = Just &quot;woohoo&quot;
Prelude&gt; let lms = [ave, n, w]
Prelude&gt; replaceWithP lms
'p'
Prelude&gt; :t replaceWithP lms
replaceWithP lms :: Char
-- In:</p>
<p>CHAPTER 16. FUNCTOR 992
replaceWithP lms
-- replaceWithP's input type is:
List (Maybe String)
-- The output type is Char
-- So applying
replaceWithP
-- to
lms
-- accomplishes
List (Maybe String) -&gt; Char
The output type of replaceWithP is always the same.
If we do this:
Prelude&gt; fmap replaceWithP lms
&quot;ppp&quot;
-- fmap is going to leave the list
-- structure intact around our result:
Prelude&gt; :t fmap replaceWithP lms
fmap replaceWithP lms :: [Char]</p>
<p>CHAPTER 16. FUNCTOR 993
Here’s the X-ray view:
-- In:
fmap replaceWithP lms
-- replaceWithP's input type is:
Maybe String
-- The output type is Char
-- So applying
fmap replaceWithP
-- to
lms
-- accomplishes:
List (Maybe String) -&gt; List Char
-- List Char ~ String
What if we lift twice?
Keep on stacking them up:
Prelude&gt; (fmap . fmap) replaceWithP lms
[Just 'p',Nothing,Just 'p']</p>
<p>CHAPTER 16. FUNCTOR 994
Prelude&gt; :t (fmap . fmap) replaceWithP lms
(fmap . fmap) replaceWithP lms
:: [Maybe Char]
And the X-ray view:
-- In:
(fmap . fmap) replaceWithP lms
-- replaceWithP's input type is:
-- String aka List Char or [Char]
-- The output type is Char
-- So applying
(fmap . fmap) replaceWithP
-- to
lms
-- accomplishes
List (Maybe String) -&gt; List (Maybe Char)
Wait, how does that even typecheck? It may not seem obvi-
ous at first how (fmap . fmap) could typecheck. We’re going to</p>
<p>CHAPTER 16. FUNCTOR 995
ask you to work through the types. You might prefer to write
it out with pen and paper, as Julie does, or type it all out in a
text editor, as Chris does. We’ll help you out by providing the
type signatures. Since the two fmapfunctions being composed
could have diﬀerent types, we’ll make the type variables for
each function unique. Start by substituting the type of each
fmapfor each of the function types in the (.)signature:
(.)::(b-&gt;c)-&gt;(a-&gt;b)-&gt;a-&gt;c
-- fmap fmap
fmap::Functor f=&gt;(m-&gt;n)-&gt;f m-&gt;f n
fmap::Functor g=&gt;(x-&gt;y)-&gt;g x-&gt;g y
It might also be helpful to query the type of (fmap . fmap)
to get an idea of what your end type should look like (modulo
diﬀerent type variables).
Lift me baby one more time
We have another layer we can lift over if we wish:
Prelude&gt; let tripFmap = fmap . fmap . fmap
Prelude&gt; tripFmap replaceWithP lms
[Just &quot;ppp&quot;,Nothing,Just &quot;pppppp&quot;]
Prelude&gt; :t tripFmap replaceWithP lms
(fmap . fmap . fmap) replaceWithP lms</p>
<p>CHAPTER 16. FUNCTOR 996
:: [Maybe [Char]]
And the X-ray view:
-- In
(fmap . fmap . fmap) replaceWithP lms
-- replaceWithP's input type is:
-- Char
-- because we lifted over
-- the [] of [Char]
-- The output type is Char
-- So applying
(fmap . fmap . fmap) replaceWithP
-- to
lms
-- accomplishes
List (Maybe String) -&gt; List (Maybe String)
So, we see there’s a pattern.</p>
<p>CHAPTER 16. FUNCTOR 997
The real type of thing going down
We saw the pattern above, but for clarity we’ll summarize here
before moving on:
Prelude&gt; fmap replaceWithP lms
&quot;ppp&quot;
Prelude&gt; (fmap . fmap) replaceWithP lms
[Just 'p',Nothing,Just 'p']
Prelude&gt; let tripFmap = fmap . fmap . fmap
Prelude&gt; tripFmap replaceWithP lms
[Just &quot;ppp&quot;,Nothing,Just &quot;pppppp&quot;]
Let’s summarize the types, too, to validate our understand-
ing:</p>
<p>CHAPTER 16. FUNCTOR 998
-- replacing the type synonym String
-- with the underlying type [Char]
-- intentionally
replaceWithP' ::[Maybe[Char]]-&gt;Char
replaceWithP' =replaceWithP
[Maybe[Char]]-&gt;[Char]
[Maybe[Char]]-&gt;[MaybeChar]
[Maybe[Char]]-&gt;[Maybe[Char]]
Pause for a second and make sure you’re understanding
everything we’ve done so far. If not, play with it until it starts
to feel comfortable.
Get on up and get down
We’ll work through the same idea, but with more funky struc-
ture to lift over:
-- lmls ~ List (Maybe (List String))
Prelude&gt; let ha = Just [&quot;Ha&quot;, &quot;Ha&quot;]
Prelude&gt; let lmls = [ha, Nothing, Just []]
Prelude&gt; (fmap . fmap) replaceWithP lmls
[Just 'p',Nothing,Just 'p']</p>
<p>CHAPTER 16. FUNCTOR 999
Prelude&gt; let tripFmap = fmap . fmap . fmap
Prelude&gt; tripFmap replaceWithP lmls
[Just &quot;pp&quot;,Nothing,Just &quot;&quot;]
Prelude&gt; (tripFmap.fmap) replaceWithP lmls
[Just [&quot;pp&quot;,&quot;pp&quot;],Nothing,Just []]
See if you can trace the changing result types as we did
above.
One more round for the P-Funkshun
For those who like their funk uncut, here’s another look at the
changing types that result from lifting over multiple layers of
functorial structure, with a slightly higher resolution. We start
this time from a source file:</p>
<p>CHAPTER 16. FUNCTOR 1000
moduleReplaceExperiment where
replaceWithP ::b-&gt;Char
replaceWithP =const'p'
lms::[Maybe[Char]]
lms=[Just&quot;Ave&quot;,Nothing,Just&quot;woohoo&quot; ]
-- Just making the argument more specific
replaceWithP' ::[Maybe[Char]]-&gt;Char
replaceWithP' =replaceWithP
What happens if we lift it?
-- Prelude&gt; :t fmap replaceWithP
-- fmap replaceWithP :: Functor f
-- =&gt; f a -&gt; f Char
liftedReplace ::Functor f=&gt;f a-&gt;fChar
liftedReplace =fmap replaceWithP
But we can assert a more specific type for liftedReplace !
liftedReplace' ::[Maybe[Char]]-&gt;[Char]
liftedReplace' =liftedReplace</p>
<p>CHAPTER 16. FUNCTOR 1001
The[]around Char is the 𝑓off Char, or the structure we
lifted over. The 𝑓off ais the outermost []in [Maybe [Char]].
So,𝑓is instantiated to []when we make the type more specific,
whether by applying it to a value of type [Maybe [Char]] or by
means of explicitly writing liftedReplace' .
Stay on the scene like an fmapmachine
What if we lift it twice?
-- Prelude&gt; :t (fmap . fmap) replaceWithP
-- (fmap . fmap) replaceWithP
-- :: (Functor f1, Functor f)
-- =&gt; f (f1 a) -&gt; f (f1 Char)
twiceLifted ::(Functor f1,Functor f)=&gt;
f (f1 a) -&gt;f (f1Char)
twiceLifted =(fmap.fmap) replaceWithP
-- Making it more specific
twiceLifted' ::[Maybe[Char]]
-&gt;[MaybeChar]
twiceLifted' =twiceLifted
-- f ~ []
-- f1 ~ Maybe
Thrice?</p>
<p>CHAPTER 16. FUNCTOR 1002
-- Prelude&gt; let rWP = replaceWithP
-- Prelude&gt; :t (fmap . fmap . fmap) rWP
-- (fmap . fmap . fmap) replaceWithP
-- :: (Functor f2, Functor f1, Functor f)
-- =&gt; f (f1 (f2 a)) -&gt; f (f1 (f2 Char))
thriceLifted ::
(Functor f2,Functor f1,Functor f)
=&gt;f (f1 (f2 a)) -&gt;f (f1 (f2 Char))
thriceLifted =
(fmap.fmap.fmap) replaceWithP
-- More specific or &quot;concrete&quot;
thriceLifted' ::[Maybe[Char]]
-&gt;[Maybe[Char]]
thriceLifted' =thriceLifted
-- f ~ []
-- f1 ~ Maybe
-- f2 ~ []
Now we can print the results from our expressions and
compare them:</p>
<p>CHAPTER 16. FUNCTOR 1003
main::IO()
main= do
putStr&quot;replaceWithP' lms: &quot;
print (replaceWithP' lms)
putStr&quot;liftedReplace lms: &quot;
print (liftedReplace lms)
putStr&quot;liftedReplace' lms: &quot;
print (liftedReplace' lms)
putStr&quot;twiceLifted lms: &quot;
print (twiceLifted lms)
putStr&quot;twiceLifted' lms: &quot;
print (twiceLifted' lms)
putStr&quot;thriceLifted lms: &quot;
print (thriceLifted lms)
putStr&quot;thriceLifted' lms: &quot;
print (thriceLifted' lms)
Be sure to type all this into a file, load it in GHCi, run main
to see what output results. Then, modify the types and code-</p>
<p>CHAPTER 16. FUNCTOR 1004
based ideas and guesses of what should and shouldn’t work.
Forming hypotheses, creating experiments based on them
or modifying existing experiments, and validating them is a
critical part of becoming comfortable with abstractions like
Functor !
Exercises: Heavy Lifting
Addfmap, parentheses, and function composition to the expres-
sion as needed for the expression to typecheck and produce
the expected result. It may not always need to go in the same
place, so don’t get complacent.
1.a=(+1)$read&quot;[1]&quot;::[Int]
Expected result
Prelude&gt; a
[2]
2.b=(++&quot;lol&quot;) (Just[&quot;Hi,&quot;,&quot;Hello&quot;])
Prelude&gt; b
Just [&quot;Hi,lol&quot;,&quot;Hellolol&quot;]
3.c=(*2) (\x-&gt;x-2)</p>
<p>CHAPTER 16. FUNCTOR 1005
Prelude&gt; c 1
-2
4.d=
((return '1'++).show)
(\x-&gt;[x,1..3])
Prelude&gt; d 0
&quot;1[0,1,2,3]&quot;
5.e::IOInteger
e= letioi=readIO&quot;1&quot;::IOInteger
changed =read (&quot;123&quot;++) show ioi
in(*3) changed
Prelude&gt; e
3693
16.8 Transforming the unapplied type
argument
We’veseen that 𝑓mustbeahigher-kindedtypeandthat Functor
instances must abide by two laws, and we’ve played around
with some basic fmapping. We know that the goal of fmapping
is to leave the outer structure untouched while transforming
the type arguments inside.</p>
<p>CHAPTER 16. FUNCTOR 1006
Way back in the beginning, we noticed that when we fmap
over a tuple, it only transforms the second argument (the 𝑏).
We saw a similar thing when we fmapped over an Either value,
and we said we’d come back to this topic. Then we saw another
hint of it above in the Heisenberg example. Now the time has
come to talk about what happens to the other type arguments
(if any) when we can only tranform the innermost.
We’ll start with a couple of canonical types:
dataTwoa b=
Twoa b
deriving (Eq,Show)
dataOra b=
Firsta
|Secondb
deriving (Eq,Show)
You may recognize these as (,)andEither recapitulated, the
generic product and sum types, from which any combination
ofandandormay be made. But these are both kind * -&gt; *
-&gt; *, which isn’t compatible with Functor , so how do we write
Functor instances for them?
These wouldn’t work because TwoandOrhave the wrong
kind:</p>
<p>CHAPTER 16. FUNCTOR 1007
instance Functor Twowhere
fmap=undefined
instance Functor Orwhere
fmap=undefined
We know that we can partially apply functions, and we’ve
seen previously that we can do this:
Prelude&gt; :k Either
Either :: * -&gt; * -&gt; *
Prelude&gt; :k Either Integer
Either Integer :: * -&gt; *
Prelude&gt; :k Either Integer String
Either Integer String :: *
That has the eﬀect of applying out some of the arguments,
reducing the kindedness of the type. Previously, we’ve demon-
strated this by applying the type constructor to concrete types;
however, you can also apply it to a type variable that represents
a type constant to produce the same eﬀect.
So to fix the kind incompatibility for our TwoandOrtypes,
we apply one of the arguments of each type constructor, giving
us kind * -&gt; * :</p>
<p>CHAPTER 16. FUNCTOR 1008
-- we use 'a' for clarity, so you
-- can see more readily which type
-- was applied out but the letter
-- doesn't matter.
instance Functor (Twoa)where
fmap=undefined
instance Functor (Ora)where
fmap=undefined
These will pass the typechecker already, but we still need
to write the implementations of fmapfor both, so let’s proceed.
First we’ll turn our attention to Two:
instance Functor (Twoa)where
fmap f ( Twoa b)=Two$(f a) (f b)
This won’t fly, because the 𝑎is part of the functorial struc-
ture (the 𝑓). We’re not supposed to touch anything in the 𝑓
referenced in the type of fmap, so we can’t apply the function
(named 𝑓in ourfmapdefinition) to the 𝑎because the 𝑎is now
untouchable.</p>
<p>CHAPTER 16. FUNCTOR 1009
fmap::Functor f=&gt;(a-&gt;b)-&gt;f a-&gt;f b
-- here, f is (Two a) because
classFunctor fwhere
fmap::(a-&gt;b)-&gt;f a-&gt;f b
instance Functor (Twoa)where
-- remember, names don't mean
-- anything beyond their relationships
-- to each other.
::(a-&gt;b)-&gt;(Twoz) a-&gt;(Twoz) b
So to fix our Functor instance, we have to leave the left value
(it’s part of the structure of 𝑓) inTwoalone, and have our func-
tion only apply to the innermost value, in this case named
𝑏:
instance Functor (Twoa)where
fmap f ( Twoa b)=Twoa (f b)
Then with Or, we’re dealing with the independent possibility
of two diﬀerent values and types, but the same basic constraint
applies:</p>
<p>CHAPTER 16. FUNCTOR 1010
instance Functor (Ora)where
fmap_(Firsta)=Firsta
fmap f ( Secondb)=Second(f b)
We’ve applied out the first argument, so now it’s part of the
𝑓. The function we’re mapping around that structure can only
transform the innermost argument.
16.9 QuickChecking Functor instances
We know the Functor laws are the following:
fmapid =id
fmap(p.q)=(fmap p) .(fmap q)
We can turn those into the following QuickCheck properties:</p>
<p>CHAPTER 16. FUNCTOR 1011
functorIdentity ::(Functor f,Eq(f a))=&gt;
f a
-&gt;Bool
functorIdentity f=
fmap id f ==f
functorCompose ::(Eq(f c),Functor f)=&gt;
(a-&gt;b)
-&gt;(b-&gt;c)
-&gt;f a
-&gt;Bool
functorCompose f g x=
(fmap g (fmap f x)) ==(fmap (g .f) x)
As long as we provided concrete instances, we can now run
these to test them.
Prelude&gt; :{
*Main| let f :: [Int] -&gt; Bool
*Main| f x = functorIdentity x
*Main| :}
Prelude&gt; quickCheck f
+++ OK, passed 100 tests.
Prelude&gt; let c = functorCompose (+1) (*2)
Prelude&gt; let li x = c (x :: [Int])</p>
<p>CHAPTER 16. FUNCTOR 1012
Prelude&gt; quickCheck li
+++ OK, passed 100 tests.
Groovy.
Making QuickCheck generate functions too
QuickCheck happens to oﬀer the ability to generate functions.
There’s a typeclass called CoArbitrary that covers the function
argument type, whereas the (related) Arbitrary typeclass is used
for the function result type. If you’re curious about this, take
a look at the Function module in the QuickCheck library to see
how functions are generated from a datatype that represents
patterns in function construction.</p>
<p>CHAPTER 16. FUNCTOR 1013
{-# LANGUAGE ViewPatterns #-}
importTest.QuickCheck
importTest.QuickCheck.Function
functorCompose' ::(Eq(f c),Functor f)=&gt;
f a
-&gt;Funa b
-&gt;Funb c
-&gt;Bool
functorCompose' x (Fun_f) (Fun_g)=
(fmap (g .f) x)==(fmap g .fmap f$x)
There are a couple things going on here. One is that we
needed to import a new module from QuickCheck . Another
is that we’re pattern matching on the Funvalue that we’re
askingQuickCheck to generate. The underlying Funtype is es-
sentially a product of the weird function type and an ordi-
nary Haskell function generated from the weirdo. The weirdo
QuickCheck -specific concrete function is a function represented
by a datatype which can be inspected and recursed. We only
want the second part, the ordinary Haskell function, so we’re
pattern-matching that one out.
Prelude&gt; type IntToInt = Fun Int Int
Prelude&gt; :{</p>
<p>CHAPTER 16. FUNCTOR 1014
*Main| type IntFC =
*Main| [Int]
*Main| -&gt; IntToInt
*Main| -&gt; IntToInt
*Main| -&gt; Bool
*Main| :}
Prelude&gt; let fc' = functorCompose'
Prelude&gt; quickCheck (fc' :: IntFC)
+++ OK, passed 100 tests.
Noteofwarning, youcan’tprintthose Funvalues, so verboseCheck
will curse Socrates and spin in a circle if you try it.
16.10 Exercises: Instances of Func
Implement Functor instances for the following datatypes. Use
theQuickCheck properties we showed you to validate them.
1.newtype Identity a=Identity a
2.dataPaira=Paira a
3.dataTwoa b=Twoa b
4.dataThreea b c=Threea b c
5.dataThree'a b=Three'a b b</p>
<p>CHAPTER 16. FUNCTOR 1015
6.dataFoura b c d =Foura b c d
7.dataFour'a b=Four'a a a b
8.Can you implement one for this type? Why? Why not?
dataTrivial =Trivial
Doingtheseexercisesis critical tounderstandinghow Functor
works, do not skip past them!
16.11 Ignoring possibilities
We’ve already touched on the MaybeandEither functors. Now
we’ll examine in a bit more detail what those do for us. As
the title of this section suggests, the Functor instances for these
datatypes are handy for times you intend to ignore the left
cases, which are typically your error or failure cases. Because
fmapdoesn’t touch those cases, you can map your function
right to the values that you intend to work with and ignore
those failure cases.
Maybe
Let’s start with some ordinary pattern matching on Maybe:</p>
<p>CHAPTER 16. FUNCTOR 1016
incIfJust ::Numa=&gt;Maybea-&gt;Maybea
incIfJust (Justn)=Just$n+1
incIfJust Nothing =Nothing
showIfJust ::Showa
=&gt;Maybea
-&gt;MaybeString
showIfJust (Justs)=Just$show s
showIfJust Nothing =Nothing
Well, that’s boring, and there’s some redundant structure.
For one thing, they have the Nothing case in common:
someFunc Nothing =Nothing
Then they’re applying some function to the value if it’s a
Just:
someFunc (Justx)=Just$someOtherFunc x
What happens if we use fmap?</p>
<p>CHAPTER 16. FUNCTOR 1017
incMaybe ::Numa=&gt;Maybea-&gt;Maybea
incMaybe m=fmap (+1) m
showMaybe ::Showa
=&gt;Maybea
-&gt;MaybeString
showMaybe s=fmap show s
That appears to have cleaned things up a bit. Does it still
work?
Prelude&gt; incMaybe (Just 1)
Just 2
Prelude&gt; incMaybe Nothing
Nothing
Prelude&gt; showMaybe (Just 9001)
Just &quot;9001&quot;
Prelude&gt; showMaybe Nothing
Nothing
Yeah,fmaphas no reason to concern itself with the Nothing
— there’s no value there for it to operate on, so this all seems
to be working properly.
But we can abstract this a bit more. For one thing, we can
eta-reduce these functions. That is, we can rewrite them with-
out naming the arguments:</p>
<p>CHAPTER 16. FUNCTOR 1018
incMaybe'' ::Numa=&gt;Maybea-&gt;Maybea
incMaybe'' =fmap (+1)
showMaybe'' ::Showa
=&gt;Maybea
-&gt;MaybeString
showMaybe'' =fmap show
And they don’t even really have to be specific to Maybe!fmap
works for all datatypes with a Functor instance! We can query
the type of the expressions in GHCi and see for ourselves the
more generic type:
Prelude&gt; :t fmap (+1)
fmap (+1)
:: (Functor f, Num b) =&gt; f b -&gt; f b
Prelude&gt; :t fmap show
fmap show
:: (Functor f, Show a) =&gt; f a -&gt; f String
With that, we can rewrite them as much more generic func-
tions:</p>
<p>CHAPTER 16. FUNCTOR 1019
-- ``lifted'' because they've been
-- lifted over some structure f
liftedInc ::(Functor f,Numb)
=&gt;f b-&gt;f b
liftedInc =fmap (+1)
liftedShow ::(Functor f,Showa)
=&gt;f a-&gt;fString
liftedShow =fmap show
And they have the same behavior as always:
Prelude&gt; liftedInc (Just 1)
Just 2
Prelude&gt; liftedInc Nothing
Nothing
Prelude&gt; liftedShow (Just 1)
Just &quot;1&quot;
Prelude&gt; liftedShow Nothing
Nothing
Making them more polymorphic in the type of the functo-
rial structure means they’re more reusable now:
Prelude&gt; liftedInc [1..5]</p>
<p>CHAPTER 16. FUNCTOR 1020
[2,3,4,5,6]
Prelude&gt; liftedShow [1..5]
[&quot;1&quot;,&quot;2&quot;,&quot;3&quot;,&quot;4&quot;,&quot;5&quot;]
Exercise: Possibly
Write a Functor instance for a datatype identical to Maybe. We’ll
use our own datatype because Maybealready has a Functor in-
stance and we cannot make a duplicate one.
dataPossibly a=
LolNope
|Yeppers a
deriving (Eq,Show)
instance Functor Possibly where
fmap=undefined
If it helps, you’re basically writing the following function:
applyIfJust ::(a-&gt;b)
-&gt;Maybea
-&gt;Maybeb</p>
<p>CHAPTER 16. FUNCTOR 1021
Either
TheMaybetype solves some problems for Haskellers, but it
doesn’t solve all of them. As we saw in a previous chapter,
sometimes we want to preserve the reason whya computation
failed rather than only the information thatit failed. And for
that, we use Either .
By this point, you know that Either has aFunctor instance
inbasefor grateful programmers to use. So let’s put it to use.
We’ll stick to the same pattern we used for demonstrating
Maybe, for the sake of clarity:
incIfRight ::Numa
=&gt;Eithere a
-&gt;Eithere a
incIfRight (Rightn)=Right$n+1
incIfRight (Lefte)=Lefte
showIfRight ::Showa
=&gt;Eithere a
-&gt;EithereString
showIfRight (Rights)=Right$show s
showIfRight (Lefte)=Lefte
Once again we can simplify these using fmapso we don’t
have to address the case of leaving the error value alone:</p>
<p>CHAPTER 16. FUNCTOR 1022
incEither ::Numa
=&gt;Eithere a
-&gt;Eithere a
incEither m=fmap (+1) m
showEither ::Showa
=&gt;Eithere a
-&gt;EithereString
showEither s=fmap show s
And again we can eta-contract to drop the obvious argu-
ment:
incEither' ::Numa
=&gt;Eithere a
-&gt;Eithere a
incEither' =fmap (+1)
showEither' ::Showa
=&gt;Eithere a
-&gt;EithereString
showEither' =fmap show
And once againwe are confronted with functions that really
didn’t need to be specific to Either at all:</p>
<p>CHAPTER 16. FUNCTOR 1023
-- f ~ Either e
liftedInc ::(Functor f,Numb)
=&gt;f b-&gt;f b
liftedInc =fmap (+1)
liftedShow ::(Functor f,Showa)
=&gt;f a-&gt;fString
liftedShow =fmap show
Take a few moments to play around with this and note how
it works.
Short Exercise
1.Write a Functor instance for a datatype identical to Either .
We’ll use our own datatype because Either has aFunctor
instance.
dataSuma b=
Firsta
|Secondb
deriving (Eq,Show)
instance Functor (Suma)where
fmap=undefined</p>
<p>CHAPTER 16. FUNCTOR 1024
Your hint for this one is that you’re writing the following
function.
applyIfSecond ::(a-&gt;b)
-&gt;(Sume) a
-&gt;(Sume) b
2.Why is a Functor instance that applies the function only to
First,Either ’sLeft, impossible? We covered this earlier.
16.12 A somewhat surprising functor
There’s a datatype named ConstorConstant — you’ll see both
names depending on which library you use. Constant has a
validFunctor , but the behavior of the Functor instance may
surprise you a bit. First, let’s look at the constfunction , and
then we’ll look at the datatype:
Prelude&gt; :t const
const :: a -&gt; b -&gt; a
Prelude&gt; let a = const 1
Prelude&gt; a 1
1
Prelude&gt; a 2
1
Prelude&gt; a 3
1</p>
<p>CHAPTER 16. FUNCTOR 1025
Prelude&gt; a &quot;blah&quot;
1
Prelude&gt; a id
1
Withasimilarconceptinmind, thereisthe Constant datatype.
Constant looks like this:
newtype Constant a b=
Constant { getConstant ::a }
deriving (Eq,Show)
One thing we notice about this type is that the type param-
eter𝑏is aphantom type. It has no corresponding witness at
the value/term level. This is a concept and tactic we’ll explore
more later, but for now we can see how it echoes the function
const:
Prelude&gt; Constant 2
Constant {getConstant = 2}
Despite 𝑏being a phantom type, though, Constant is kind*
-&gt; * -&gt; * , and that is not a valid Functor . So how do we get one?
Well, there’s only one thing we can do with a type constructor,
just as with functions: apply it. So we dohave aFunctor for
Constant a , but not Constant alone. It has to be Constant a and
notConstant a b because Constant a b would be kind *.
Let’s look at the implementation of Functor forConstant :</p>
<p>CHAPTER 16. FUNCTOR 1026
instance Functor (Constant m)where
fmap_(Constant v)=Constant v
Looks like identity right? Let’s use this in the REPL and run
it through the Functor laws:
Prelude&gt; const 2 (getConstant (Constant 3))
2
Prelude&gt; fmap (const 2) (Constant 3)
Constant {getConstant = 3}
Prelude&gt; let gc = getConstant
Prelude&gt; let c = Constant 3
Prelude&gt; gc $ fmap (const 2) c
3
Prelude&gt; gc $ fmap (const &quot;blah&quot;) c
3
When you fmaptheconstfunction over the Constant type,
the first argument to constis never used because the partially
applied constis itself never used. The first type argument to
Constant ’s type constructor is in the part of the structure that
Functor skips over. The second argument to the Constant type
constructor is the phantom type variable 𝑏which has no value
or term-level witness in the datatype. Since there are no values
of the type the Functor is supposed to be mapping, we have</p>
<p>CHAPTER 16. FUNCTOR 1027
nothing we’re allowed to apply the function to, so we never
use the constexpressions.
But does this adhere to the Functor laws?
-- Testing identity
Prelude&gt; getConstant (id (Constant 3))
3
Prelude&gt; getConstant (fmap id (Constant 3))
3
-- Composition of the const function
Prelude&gt; ((const 3) . (const 5)) 10
3
Prelude&gt; ((const 5) . (const 3)) 10
5
-- Composition
Prelude&gt; let fc = fmap (const 3)
Prelude&gt; let fc' = fmap (const 5)
Prelude&gt; let separate = fc . fc'
Prelude&gt; let c = const 3
Prelude&gt; let c' = const 5
Prelude&gt; let fused = fmap (c . c')
Prelude&gt; let cw = Constant &quot;WOOHOO&quot;
Prelude&gt; getConstant $ separate $ cw
&quot;WOOHOO&quot;</p>
<p>CHAPTER 16. FUNCTOR 1028
Prelude&gt; let cdr = Constant &quot;Dogs rule&quot;
Prelude&gt; getConstant $ fused $ cdr
&quot;Dogs rule&quot;
(Constant a) is* -&gt; * which you need for the Functor , but
now you’re mapping over that 𝑏, and not the 𝑎.
This is a mere cursory check, not a proof that this is a valid
Functor . Most assurances of correctness that programmers use
exist on a gradient and aren’t proper proofs. Despite seeming
a bit pointless, Constant is a lawful Functor .
16.13 More structure, more functors
At times the structure of our types may require that we also
have aFunctor instance for an intermediate type layer. We’ll
demonstrate this using this datatype:
dataWrapf a=
Wrap(f a)
deriving (Eq,Show)
Notice that our 𝑎here is an argument to the 𝑓. So how are
we going to write a Functor instance for this?
instance Functor (Wrapf)where
fmap f ( Wrapfa)=Wrap(f fa)</p>
<p>CHAPTER 16. FUNCTOR 1029
This won’t work because there’s this 𝑓that we’re not hop-
ping over, and 𝑎(the value fmapshould be applying the function
to) is an argument to that 𝑓— the function can’t apply to that
𝑓that is wrapping 𝑎.
instance Functor (Wrapf)where
fmap f ( Wrapfa)=Wrap(fmap f fa)
Here we don’t know what type 𝑓is and it could be anything,
but it needs to be a type that has a Functor instance so that we
canfmapover it. So we add a constraint:
instance Functor f
=&gt;Functor (Wrapf)where
fmap f ( Wrapfa)=Wrap(fmap f fa)
And if we load up the final instance, we can use this wrapper
type:
Prelude&gt; fmap (+1) (Wrap (Just 1))
Wrap (Just 2)
Prelude&gt; fmap (+1) (Wrap [1, 2, 3])
Wrap [2,3,4]
It should work for any Functor . If we pass it something that
isn’t?</p>
<p>CHAPTER 16. FUNCTOR 1030
Prelude&gt; let n = 1 :: Integer
Prelude&gt; fmap (+1) (Wrap n)
Couldn't match expected type ‘f b’
with actual type ‘Integer’
Relevant bindings include
it :: Wrap f b (bound at <interactive>:8:1)
In the first argument of ‘Wrap’, namely ‘n’
In the second argument of ‘fmap’,
namely ‘(Wrap n)’
The number by itself doesn’t oﬀer the additional structure
needs for Wrapto work as a Functor . It’s expecting to be able to
fmapover some 𝑓independent of an 𝑎and this isn’t the case
with any type constant such as Integer .
16.14 IO Functor
We’ve seen the IOtype in the modules and testing chapters
already, but we weren’t doing much with it save to print text
or ask for string input from the user. The IOtype will get
a full chapter of its own later in the book. It is an abstract
datatype; there are no data constructors that you’re permitted
to pattern match on, so the typeclasses IOprovides are the</p>
<p>CHAPTER 16. FUNCTOR 1031
only way you can work with values of type IO a. One of the
simplest provided is Functor .
-- getLine :: IO String
-- read :: Read a =&gt; String -&gt; a
getInt::IOInt
getInt=fmap read getLine
Inthas aReadinstance, and fmapliftsreadover the IOtype. A
way you can read getLine here is that it’s not a String , but rather
away to obtain a string .IOdoesn’t guarantee that eﬀects will
be performed, but it does mean that they couldbe performed.
Here the side eﬀect is needing to block and wait for user input
via the standard input stream the OS provides:
Prelude&gt; getInt
10
10
We type 10 and hit enter. GHCi prints IOvalues unless the
type isIO (), in which case it hides the Unitvalue because it’s
meaningless:
Prelude&gt; fmap (const ()) getInt
10</p>
<p>CHAPTER 16. FUNCTOR 1032
The “10” in the GHCi session above is from typing 10 and
hitting enter. GHCi isn’t printing any result after that because
we’re replacing the Intvalue we read from a String. That
information is getting dropped on the floor because we applied
const () to the contents of the IO Int . If we ignore the presence
of IO, it’s as if we did this:
Prelude&gt; let getInt = 10 :: Int
Prelude&gt; const () getInt
()
GHCi as a matter of convenience and design, will not print
any value of type IO ()on the assumption that the IO action
you evaluated was evaluated for eﬀects and because the unit
value cannot communicate anything. We can use the return
function (seen earlier, explained later) to lift a unit value in IO
and reproduce this behavior of GHCi’s:
Prelude&gt; return 1 :: IO Int
1
Prelude&gt; ()
()
Prelude&gt; return () :: IO ()
Prelude&gt;
What if we want to do something more useful? We can fmap
any function we want over IO:</p>
<p>CHAPTER 16. FUNCTOR 1033
Prelude&gt; fmap (+1) getInt
10
11
Prelude&gt; fmap (++ &quot; and me too!&quot;) getLine
hello
&quot;hello and me too!&quot;
Wecanalso use dosyntaxtodo whatwe’redoingwith Functor
here:
meTooIsm ::IOString
meTooIsm = do
input&lt;-getLine
return (input ++&quot;and me too!&quot; )
bumpIt::IOInt
bumpIt= do
intVal&lt;-getInt
return (intVal +1)
But iffmap f suffices for what you’re doing, that’s usually
shorter and clearer. It’s perfectly all right and quite common
to start with a more verbose form of some expression and
then clean it up after you’ve got something that works.</p>
<p>CHAPTER 16. FUNCTOR 1034
16.15 What if we want to do something
diﬀerent?
We talked about Functor as a means of lifting functions over
structure so that we may transform only the contents, leaving
the structure alone. What if we wanted to transform only the
structure and leave the type argument to that structure or type
constructor alone? With this, we’ve arrived at natural transfor-
mations . We can attempt to put together a type to express what
we want:
nat::(f-&gt;g)-&gt;f a-&gt;g a
This type is impossible because we can’t have higher-kinded
types as argument types to the function type. What’s the
problem, though? It looks like the type signature for fmap,
doesn’t it? Yet 𝑓and𝑔inf -&gt; g are higher-kinded types. They
must be, because they are the same 𝑓and𝑔that, later in the
type signature, are taking arguments. But in those places they
are applied to their arguments and so have kind *.
So we make a modest change to fix it.
{-# LANGUAGE RankNTypes #-}
typeNatf g=forall a .f a-&gt;g a</p>
<p>CHAPTER 16. FUNCTOR 1035
So in a sense, we’re doing the opposite of what a Functor does.
We’re transforming the structure, preserving the values as they
were. We won’t explain it fully here, but the quantification of
𝑎in the right-hand side of the declaration allows us to obligate
all functions of this type to be oblivious to the contents of
the structures 𝑓and𝑔in much the same way that the identity
function cannot do anything but return the argument it was
given.
Syntactically, it lets us avoid talking about 𝑎in the type of
Nat— which is what we want, we shouldn’t haveany specific
information about the contents of 𝑓and𝑔because we’re sup-
posed to be only performing a structural transformation, not
a fold.
If you try to elide the 𝑎from the type arguments without
quantifying it separately, you’ll get an error:
Prelude&gt; type Nat f g = f a -&gt; g a
Not in scope: type variable ‘a’
Wecanaddthequantifier, butifweforgettoturnon RankNTypes
(orRank2Types ), it won’t work:
Prelude&gt; :{
*Main| type Nat f g =
*Main| forall a . f a -&gt; g a
*Main| :}</p>
<p>CHAPTER 16. FUNCTOR 1036
Illegal symbol '.' in type
Perhaps you intended to use RankNTypes or a
similar language extension to enable
explicit-forall syntax:
forall <tvs>. <type>
If we turn on RankNTypes , it works fine:
Prelude&gt; :set -XRank2Types
Prelude&gt; :{
*Main| type Nat f g =
*Main| forall a . f a -&gt; g a
*Main| :}
Prelude&gt;
To see an example of what the quantification prevents, con-
sider the following:</p>
<p>CHAPTER 16. FUNCTOR 1037
typeNatf g=forall a .f a-&gt;g a
-- This'll work
maybeToList ::NatMaybe[]
maybeToList Nothing =[]
maybeToList (Justa)=[a]
-- This will not work, not allowed.
degenerateMtl ::NatMaybe[]
degenerateMtl Nothing =[]
degenerateMtl (Justa)=[a+1]
What if we use a version of Natthat mentions 𝑎in the type?</p>
<p>CHAPTER 16. FUNCTOR 1038
moduleBadNatwhere
typeNatf g a=f a-&gt;g a
-- This'll work
maybeToList ::NatMaybe[]a
maybeToList Nothing =[]
maybeToList (Justa)=[a]
-- But this will too if we tell it
-- 'a' is Num a =&gt; a
degenerateMtl ::Numa=&gt;NatMaybe[]a
degenerateMtl Nothing =[]
degenerateMtl (Justa)=[a+1]
That last example should notwork and is not a good way to
think about natural transformation. Part of software is being
precise and when we talk about natural transformations we’re
saying as much about what we don’twant as we are about what
wedowant. In this case, the invariant we want to preserve is
that the function cannot do anything mischievous with the
values. If you want to transform the values, write a plain old
fold!
We’re going to return to the topic of natural transformations</p>
<p>CHAPTER 16. FUNCTOR 1039
in the next chapter, so cool your jets for now.
16.16 Functors are unique to a datatype
In Haskell, Functor instances will be unique for a given datatype.
We saw that this isn’t true for Monoid; however, we use new-
types to preserve the unique pairing of an instance to a type.
ButFunctor instances will be unique for a datatype, in part
because of parametricity, in part because arguments to type
constructors are applied in order of definition. In a hypothet-
ical not-Haskell language, the following might be possible:
dataTuplea b=
Tuplea b
deriving (Eq,Show)
-- this is impossible in Haskell
instance Functor (Tuple?b)where
fmap f ( Tuplea b)=Tuple(f a) b
There are essentially two ways to address this. One is to flip
the arguments to the type constructor; the other is to make a
new datatype using a Flipnewtype:</p>
<p>CHAPTER 16. FUNCTOR 1040
{-# LANGUAGE FlexibleInstances #-}
moduleFlipFunctor where
dataTuplea b=
Tuplea b
deriving (Eq,Show)
newtype Flipf a b=
Flip(f b a)
deriving (Eq,Show)
-- this works, goofy as it looks.
instance Functor (FlipTuplea)where
fmap f ( Flip(Tuplea b))=
Flip$Tuple(f a) b
Prelude&gt; fmap (+1) (Flip (Tuple 1 &quot;blah&quot;))
Flip (Tuple 2 &quot;blah&quot;)
However, Flip Tuple a b is a distinct type from Tuple a b
even if it’s only there to provide for diﬀerent Functor instance
behavior.</p>
<p>CHAPTER 16. FUNCTOR 1041
16.17 Chapter exercises
Determine if a valid Functor can be written for the datatype
provided.
1.dataBool=
False|True
2.dataBoolAndSomethingElse a=
False'a|True'a
3.dataBoolAndMaybeSomethingElse a=
Falsish |Truisha
4.Use the kinds to guide you on this one, don’t get too hung
up on the details.
newtype Muf=InF{ outF::f (Muf) }
5.Again, follow the kinds and ignore the unfamiliar parts
importGHC.Arr
dataD=
D(ArrayWordWord)IntInt
Rearrange the arguments to the type constructor of the
datatype so the Functor instance works.</p>
<p>CHAPTER 16. FUNCTOR 1042
1.dataSuma b=
Firsta
|Secondb
instance Functor (Sume)where
fmap f ( Firsta)=First(f a)
fmap f ( Secondb)=Secondb
2.dataCompany a b c=
DeepBlue a c
|Something b
instance Functor (Company e e')where
fmap f ( Something b)=Something (f b)
fmap_(DeepBlue a c)=DeepBlue a c
3.dataMorea b=
La b a
|Rb a b
deriving (Eq,Show)
instance Functor (Morex)where
fmap f ( La b a') =L(f a) b (f a')
fmap f ( Rb a b') =Rb (f a) b'</p>
<p>CHAPTER 16. FUNCTOR 1043
Keeping in mind that it should result in a Functor that does
the following:
Prelude&gt; fmap (+1) (L 1 2 3)
L 2 2 4
Prelude&gt; fmap (+1) (R 1 2 3)
R 1 3 3
WriteFunctor instances for the following datatypes.
1.dataQuanta b=
Finance
|Deska
|Bloorb
2.No, it’s not interesting by itself.
dataKa b=
Ka</p>
<p>CHAPTER 16. FUNCTOR 1044
3.{-# LANGUAGE FlexibleInstances #-}
newtype Flipf a b=
Flip(f b a)
deriving (Eq,Show)
newtype Ka b=
Ka
-- should remind you of an
-- instance you've written before
instance Functor (FlipKa)where
fmap=undefined
4.dataEvilGoateeConst a b=
GoatyConst b
-- You thought you'd escaped the goats
-- by now didn't you? Nope.
No, it doesn’t do anything interesting. No magic here or
in the previous exercise. If it works, you succeeded.
5.Do you need something extra to make the instance work?</p>
<p>CHAPTER 16. FUNCTOR 1045
dataLiftItOut f a=
LiftItOut (f a)
6.dataParappa f g a=
DaWrappa (f a) (g a)
7.Don’t ask for more typeclass instances than you need. You
can let GHC tell you what to do.
dataIgnoreOne f g a b =
IgnoringSomething (f a) (g b)
8.dataNotorious g o a t =
Notorious (g o) (g a) (g t)
9.You’ll need to use recursion.
dataLista=
Nil
|Consa (Lista)
10.A tree of goats forms a Goat-Lord, fearsome poly-creature.</p>
<p>CHAPTER 16. FUNCTOR 1046
dataGoatLord a=
NoGoat
|OneGoat a
|MoreGoats (GoatLord a)
(GoatLord a)
(GoatLord a)
-- A VERITABLE HYDRA OF GOATS
11.You’ll use an extra functor for this one, although your so-
lution might do it monomorphically without using fmap.
Keep in mind that you will probably not be able to vali-
date this one in the usual manner. Do your best to make
it work.2
dataTalkToMe a=
Halt
|PrintStringa
|Read(String-&gt;a)
16.18 Definitions
1.Higher-kinded polymorphism is polymorphism which has
a type variable abstracting over types of a higher kind.
Functor is an example of higher-kinded polymorphism
because the kind of the 𝑓parameter to Functor is* -&gt; *.
2Thanks to Andraz Bajt for inspiring this exercise.</p>
<p>CHAPTER 16. FUNCTOR 1047
Another example of higher-kinded polymorphism would
be a datatype having a parameter to the type constructor
which is of a higher kind, such as the following:
dataWeirdf a=Weird(f a)
Where the kinds of the types involved are:
a:: *
f:: * -&gt; *
Weird::(* -&gt; *)-&gt; * -&gt; *
Here both Weirdand𝑓are higher kinded, with Weirdbeing
an example of higher-kinded polymorphism.
2.Functor is a mapping between categories. In Haskell, this
manifests as a typeclass that generalizes the concept of map:
it takes a function (a -&gt; b) and lifts it into a diﬀerent type.
This conventionally implies some notion of a function
which can be applied to a value with more structure than
the unlifted function was originally designed for. The
additional structure is represented by the use of a higher-
kinded type 𝑓, introduced by the definition of the Functor
typeclass.</p>
<p>CHAPTER 16. FUNCTOR 1048
f ::a-&gt;b
-- ``more structure''
fmapf::f a-&gt;f b
-- f is applied to a single argument,
-- and so is kind * -&gt; *
One should be careful not to confuse this intuition for
it necessarily being exclusively about containers or data
structures. There’s a Functor of functions and many exotic
types will have a lawful Functor instance.
3.Let’s talk about lifting. Because most of the rest of the
book deals with applicatives and monads of various fla-
vors, we’re going to be lifting a lot, but what do we mean?
When Carnap first described functors in the context of
linguistics, he didn’t really talk about it as lifting anything,
and mathematicians have followed in his footsteps, fo-
cusing on mapping and the production of outputs from
certain types of inputs. Very mathematical of them, and
yet Haskellers use the lifting metaphor often (as we do, in
this book).
There are a couple of ways people commonly think about
it. One is that we can lift a function into a context. Another</p>
<p>CHAPTER 16. FUNCTOR 1049
is that we lift a function over some layer of structure to
apply it. The eﬀect is the same:
Prelude&gt; fmap (+1) $ Just 1
Just 2
Prelude&gt; fmap (+1) [1, 2, 3]
[2,3,4]
In the first case, we lift that function into a Maybecontext
in order to apply it; in the second case, into a list con-
text. It can be helpful to think of it in terms of lifting the
function into the context, because it’s the context we’ve
lifted the function into that determines how the function
will get applied (to one value or, recursively, to many, for
example). The context is the datatype, the definition of
the datatype, and the Functor instance we have for that
datatype. It’s also the contexts that determine what hap-
pens when we try to apply a function to an 𝑎that isn’t
there:
Prelude&gt; fmap (+1) []
[]
Prelude&gt; fmap (+1) Nothing
Nothing
But we often speak more casually about lifting over, as in
fmaplifts a function overa data constructor. This works,</p>
<p>CHAPTER 16. FUNCTOR 1050
too, if you think of the data constructor as a layer of struc-
ture. The function hops over that layer and applies to
what’s inside, if anything.
More precisely, lifting means applying a type constructor
to a type, as in taking an 𝑎type variable and applying an
𝑓type constructor to it to get an f a. Keeping this def-
inition in mind will be helpful. Remember to follow the
typesrather than getting too caught up in the web of a
metaphor.
4.George Clinton is one of the most important innovators of
funk music. Clinton headed up the bands Parliament and
Funkadelic, whose collective style of music is known as
P-Funk; the two bands have fused into a single apotheosis
of booty-shaking rhythm. The Parliament album Mother-
ship Connection is one of the most famous and influential
albums in rock history. Not a Functor , but you can pretend
the album is mapping your consciousness from the real
world into the category of funkiness if that helps.
16.19 Follow-up resources
1.Haskell Wikibook; The Functor class.
https://en.wikibooks.org/wiki/Haskell/The_Functor_class</p>
<p>CHAPTER 16. FUNCTOR 1051
2.Mark P. Jones; A system of constructor classes: overload-
ing and implicit higher-order polymorphism.
3.Gabriel Gonzalez; The functor design pattern.</p>
<p>Chapter 17
Applicative
…the images I most
connect to, historically
speaking, are in black and
white. I see more in black
and white – I like the
abstraction of it.
Mary Ellen Mark
1052</p>
<p>CHAPTER 17. APPLICATIVE 1053
17.1 Applicative
In the previous chapters, we’ve seen two common algebras
that are used as typeclasses. Monoid gives us a means of mashing
two values of the same type together. Functor , on the other
hand, is for function application oversome structure we don’t
want to have to think about. Monoid’s core operation, mappend ,
smashes the structures together — when you mappend two lists,
they become one list, so the structures themselves have been
joined. However, the core operation of Functor ,fmap, applies a
function to a value that is within some structure while leaving
that structure unaltered.
We come now to Applicative . Applicatives are monoidal
functors. No, no, stay with us. The Applicative typeclass allows
for function application lifted over structure (like Functor ). But
withApplicative the function we’re applying is also embed-
ded in some structure. Because the function andthe value
it’s being applied to both have structure, we have to smash
those structures together. So, Applicative involves monoids
and functors. And that’s a pretty powerful thing.
In this chapter, we will:
•define and explore the Applicative typeclass and its core
operations;
•demonstrate why applicatives are monoidal functors;</p>
<p>CHAPTER 17. APPLICATIVE 1054
•make the usual chitchat about laws and instances;
•do a lot of lifting;
•give you some Validation .
17.2 Defining Applicative
The first thing you’re going to notice about this typeclass dec-
laration is that the 𝑓that represents the structure, similar to
Functor , is itself constrained by the Functor typeclass:
classFunctor f=&gt;Applicative fwhere
pure::a-&gt;f a
(&lt;<em>&gt;)::f (a-&gt;b)-&gt;f a-&gt;f b
So, every type that can have an Applicative instance must
also have a Functor instance.
Thepurefunction does a simple and very boring thing:
it lifts something into functorial (applicative) structure. You
can think of this as being a bare minimum bit of structure or
structural identity . Identity for what, you’ll see later when we go
over the laws. The more interesting operation of this typeclass
is&lt;</em>&gt;. This is an infix function called ‘apply’ or sometimes
‘ap,’ or sometimes ‘tie fighter’ when we’re feeling particularly
zippy.
If we compare the types of &lt;*&gt;andfmap, we see the similar-
ity:</p>
<p>CHAPTER 17. APPLICATIVE 1055
-- fmap
(&lt;$&gt;)::Functor f
=&gt;(a-&gt;b)-&gt;f a-&gt;f b
(&lt;*&gt;)::Applicative f
=&gt;f (a-&gt;b)-&gt;f a-&gt;f b
The diﬀerence is the 𝑓representing functorial structure
that is on the outside of our function in the second definition.
We’ll see good examples of what that means in practice in a
moment.
Along with these core functions, the Control.Applicative li-
brary provides some other convenient functions: liftA,liftA2 ,
andliftA3 :</p>
<p>CHAPTER 17. APPLICATIVE 1056
liftA::Applicative f=&gt;
(a-&gt;b)
-&gt;f a
-&gt;f b
liftA2::Applicative f=&gt;
(a-&gt;b-&gt;c)
-&gt;f a
-&gt;f b
-&gt;f c
liftA3::Applicative f=&gt;
(a-&gt;b-&gt;c-&gt;d)
-&gt;f a
-&gt;f b
-&gt;f c
-&gt;f d
If you’re looking at the type of liftAand thinking, but that’s
fmap, you are correct. It is basically the same as fmaponly with
anApplicative typeclass constraint instead of a Functor one.
Since all applicatives are also functors, though, this is a distinc-
tion without much significance.
Similarly you can see that liftA2 andliftA3 arefmapbut with
functions involving more arguments. It can be a little difficult</p>
<p>CHAPTER 17. APPLICATIVE 1057
to wrap one’s head around how those will work in practice, so
we’ll want to look next at some examples to start developing a
sense of what applicatives can do for us.
17.3 Functor vs. Applicative
We’ve already said that applicatives are monoidal functors,
so what we’ve already learned about Monoid andFunctor is rele-
vant to our understanding of Applicative . We’ve already seen
some examples of what this means in practice, but we want to
develop a stronger intuition for the relationship.
Let’s review the diﬀerence between fmapand&lt;<em>&gt;:
fmap::(a-&gt;b)-&gt;f a-&gt;f b
(&lt;</em>&gt;)::f (a-&gt;b)-&gt;f a-&gt;f b
The diﬀerence is we now have an 𝑓in front of our function
(a -&gt; b) . The increase in power it introduces is profound. For
one thing, any Applicative also has a Functor and not merely
by definition — you can define a Functor in terms of a pro-
videdApplicative instance. Proving it is outside the scope of
the current book, but this follows from the laws of Functor
andApplicative (we’ll get to the applicative laws later in this
chapter):
fmapf x=pure f&lt;*&gt;x</p>
<p>CHAPTER 17. APPLICATIVE 1058
How might we demonstrate this? You’ll need to import
Control.Applicative if you’re using GHC 7.8 or older to test this
example:
Prelude&gt; fmap (+1) [1, 2, 3]
[2,3,4]
Prelude&gt; pure (+1) &lt;*&gt; [1..3]
[2,3,4]
Keeping in mind that pure has type Applicative f =&gt; a -&gt; f
a, we can think of it as a means of embedding a value of any
type in the structure we’re working with:
Prelude&gt; pure 1 :: [Int]
[1]
Prelude&gt; pure 1 :: Maybe Int
Just 1
Prelude&gt; pure 1 :: Either a Int
Right 1
Prelude&gt; pure 1 :: ([a], Int)
([],1)
The left type is handled diﬀerently from the right in the
final two examples for the same reason as here:
Prelude&gt; fmap (+1) (4, 5)
(4,6)</p>
<p>CHAPTER 17. APPLICATIVE 1059
The left type is part of the structure, and the structure is
not transformed by the function application.
17.4 Applicative functors are monoidal
functors
First let us notice something:
($)::(a-&gt;b)-&gt;a-&gt;b
(&lt;$&gt;)::(a-&gt;b)-&gt;f a-&gt;f b
(&lt;<em>&gt;)::f (a-&gt;b)-&gt;f a-&gt;f b
We already know $to be something of a do-nothing infix
function which exists to give the right-hand side more prece-
dence and thus avoid parentheses. For our present purposes
it acts as a nice proxy for ordinary function application in its
type.
When we get to &lt;$&gt;, the alias for fmap, we notice the first
change is that we’re now lifting our (a -&gt; b) over the 𝑓wrapped
around our value and applying the function to that value.
Then as we arrive at apor&lt;</em>&gt;, theApplicative apply method,
our function is now also embedded in the functorial structure.
Now we get to the monoidal in “monoidal functor”:</p>
<p>CHAPTER 17. APPLICATIVE 1060
::f (a-&gt;b)-&gt;f a-&gt;f b
-- The two arguments to our function are:
f(a-&gt;b)
-- and
fa
If we imagine that we can apply (a -&gt; b) to𝑎and get 𝑏,
ignoring the functorial structure, we still have a problem as we
need to return f b. When we were dealing with fmap, we had
only one bit of structure, so it was left unchanged. Now we
have two bits of structure of type 𝑓that we need to deal with
somehow before returning a value of type f b. We can’t simply
leave them unchanged; we must unite them somehow. Now,
they will be definitely the same type, because the 𝑓must be
the same type throughout. In fact, if we separate the structure
parts from the function parts, maybe we’ll see what we need:
::f (a-&gt;b)-&gt;f a-&gt;f b
f f f
(a-&gt;b) a b
Didn’t we have something earlier that can take two values
of one type and return one value of the same type? Provided</p>
<p>CHAPTER 17. APPLICATIVE 1061
the𝑓is a type with a Monoid instance, then we have a good way
to make them play nice together:
mappend ::Monoida=&gt;a-&gt;a-&gt;a
So, with Applicative , we have a Monoid for our structure and
function application for our values!
mappend ::f f f
$ :: (a-&gt;b) a b
(&lt;*&gt;)::f (a-&gt;b)-&gt;f a-&gt;f b
-- plus Functor fmap to be able to map
-- over the f to begin with.
So in a sense, we’re bolting a Monoid onto aFunctor to be able
to deal with functions embedded in additional structure. In
another sense, we’re enriching function application with the
very structure we were previously mapping over with Functor .
Let’s consider a few familiar examples to examine what this
means:</p>
<h1>CHAPTER 17. APPLICATIVE 1062
-- List
[(*2), (<em>3)]&lt;</em>&gt;[4,5]</h1>
<p>[2<em>4,2</em>5,3<em>4,3</em>5]
-- reduced
[8,10,12,15]
So what was (a -&gt; b) enriched with in f (a -&gt; b) -&gt; f a -&gt;
f b? In this case, “list-ness”. Although the actual application
of each (a -&gt; b) to a value of type 𝑎is quite ordinary, we now
have a list of functions rather than a single function as would
be the case if it was the list Functor .
But lists aren’t the only structure we can enrich our func-
tions with — not even close! The structure bit can also be Maybe:</p>
<h1>CHAPTER 17. APPLICATIVE 1063
Just(<em>2)&lt;</em>&gt;Just2</h1>
<h1>Just4
Just(<em>2)&lt;</em>&gt;Nothing</h1>
<h1>Nothing
Nothing &lt;*&gt;Just2</h1>
<h1>Nothing
Nothing &lt;*&gt;Nothing</h1>
<p>Nothing
WithMaybe, the ordinary functor is mapping over the pos-
sibility of a value’s nonexistence. With the Applicative , now
the function also might not be provided. We’ll see a couple
of nice, long examples of how this might happen — how you
could end up not even providing a function to apply — in a
bit, not just with Maybe, but with Either and a new type called
Validation as well.</p>
<p>CHAPTER 17. APPLICATIVE 1064
Show me the monoids
Recall that the Functor instance for the two-tuple ignores the
first value inside the tuple:
Prelude&gt; fmap (+1) (&quot;blah&quot;, 0)
(&quot;blah&quot;,1)
Butthe Applicative forthetwo-tupledemonstratesthemonoid
inApplicative nicely for us. In fact, if you call :infoon(,)in
your REPL you’ll notice something:
Prelude&gt; :info (,)
data (,) a b = (,) a b
-- Defined in ‘GHC.Tuple’
...
instance Monoid a
=&gt; Applicative ((,) a)
-- Defined in ‘GHC.Base’
...
instance (Monoid a, Monoid b)
=&gt; Monoid (a, b)
For the Applicative instance of two-tuple, we don’t need a
Monoid for the 𝑏because we’re using function application to
produce the 𝑏. However, for the first value in the tuple, we
still need the Monoid because we have two values and need to
somehow turn that into one value of the same type:</p>
<p>CHAPTER 17. APPLICATIVE 1065
Prelude&gt; (&quot;Woo&quot;, (+1)) &lt;<em>&gt; (&quot; Hoo!&quot;, 0)
(&quot;Woo Hoo!&quot;, 1)
Notice that for the 𝑎value, we didn’t apply any function,
but they have combined themselves as if by magic; that’s due
to theMonoid instance for the 𝑎values. The function in the 𝑏
position of the left tuple has been applied to the value in the 𝑏
position of the right tuple to produce a result. That function
application is why we don’t need a Monoid instance on the 𝑏.
Let’s look at more such examples. Pay careful attention to
how the 𝑎values in the tuples are combined:
Prelude&gt; import Data.Monoid
Prelude&gt; (Sum 2, (+1)) &lt;</em>&gt; (Sum 0, 0)
(Sum {getSum = 2},1)
Prelude&gt; (Product 3, (+9))&lt;<em>&gt;(Product 2, 8)
(Product {getProduct = 6},17)
Prelude&gt; (All True, (+1))&lt;</em>&gt;(All False, 0)
(All {getAll = False},1)
It doesn’t really matter whatMonoid , but we need some way
of combining or choosing our values.
Tuple Monoid and Applicative side by side
Squint if you can’t see it.</p>
<p>CHAPTER 17. APPLICATIVE 1066
instance (Monoida,Monoidb)
=&gt;Monoid(a,b)where
mempty=(mempty, mempty)
(a, b) <code>mappend</code> (a',b') =
(a <code>mappend</code> a', b <code>mappend</code> b')
instance Monoida
=&gt;Applicative ((,) a) where
pure x=(mempty, x)
(u, f)&lt;*&gt;(v, x)=
(u <code>mappend</code> v, f x)
Maybe Monoid and Applicative
While applicatives are monoidal functors, be careful about
making assumptions based on this. For one thing, Monoid and
Applicative instances aren’t required or guaranteed to have the
same monoid of structure, and the functorial part may change
the way it behaves. Nevertheless, you might be able to see the
implicit monoid in how the Applicative pattern matches on
theJustandNothing cases and compare that with this Monoid :</p>
<p>CHAPTER 17. APPLICATIVE 1067
instance Monoida=&gt;Monoid(Maybea)where
mempty=Nothing
mappend m Nothing =m
mappend Nothing m=m
mappend ( Justa) (Justa')=
Just(mappend a a')
instance Applicative Maybewhere
pure=Just
Nothing &lt;<em>&gt; _ = Nothing
_ &lt;</em>&gt;Nothing =Nothing
Justf&lt;*&gt;Justa=Just(f a)
Later we’ll see some examples of how diﬀerent Monoid in-
stances can give diﬀerent results for applicatives. For now,
recognize that the monoidal bit may not be what you recog-
nize as the canonical mappend of that type, because some types
can have multiple monoids.
17.5 Applicative in use
Bynowitshouldcomeasnosurprisethatmanyofthedatatypes
we’ve been working with in the past two chapters also have
Applicative instances. Since we are already so familiar with list
andMaybe, those examples will be a good place to start. Later</p>
<p>CHAPTER 17. APPLICATIVE 1068
in the chapter, we will be introducing some new types, so just
hang onto your hats.
List Applicative
We’ll start with the list Applicative because it’s a clear way to
get a sense of the pattern. Let’s start by specializing the types:
-- f ~ []
(&lt;<em>&gt;)::f (a-&gt;b)-&gt;f a-&gt;f b
(&lt;</em>&gt;)::[ ] (a-&gt;b)-&gt;[ ] a-&gt;[ ] b
-- more syntactically typical
(&lt;<em>&gt;)::[(a-&gt;b)]-&gt;[a]-&gt;[b]
pure::a-&gt;f a
pure::a-&gt;[ ] a
Or, again, if you have GHC 8 or newer, you can do this:
Prelude&gt; :set -XTypeApplications
Prelude&gt; :type (&lt;</em>&gt;) @[]
(&lt;*&gt;) @[] :: [a -&gt; b] -&gt; [a] -&gt; [b]
Prelude&gt; :type pure @[]
pure @[] :: a -&gt; [a]</p>
<p>CHAPTER 17. APPLICATIVE 1069
What’s the List applicative do?
Previously with list Functor , we were mapping a single function
over a plurality of values:
Prelude&gt; fmap (2^) [1, 2, 3]
[2,4,8]
Prelude&gt; fmap (^2) [1, 2, 3]
[1,4,9]
With the list Applicative , we are mapping a plurality of func-
tions over a plurality of values:
Prelude&gt; [(+1), (<em>2)] &lt;</em>&gt; [2, 4]
[3,5,4,8]
We can see how this makes sense given that:
(&lt;*&gt;)::Applicative f
=&gt;f (a-&gt;b)-&gt;f a-&gt;f b
f~[]
listApply ::[(a-&gt;b)]-&gt;[a]-&gt;[b]
listFmap ::(a-&gt;b)-&gt;[a]-&gt;[b]</p>
<p>CHAPTER 17. APPLICATIVE 1070
The𝑓structure that is wrapped around our function in the
listApply function is itself a list. Therefore, our a -&gt; b from
Functor has become a listofa -&gt; b .
Now what happened with that expression we tested? Some-
thing like this:
[(+1), (<em>2)]&lt;</em>&gt;[2,4]==[3,5,4,8]
[3,5,4,8]
-- [1] [2] [3] [4]
1.The first item in the list, 3, is the result of (+1) being applied
to 2.
2.5 is the result of applying (+1) to 4.
3.4 is the result of applying (*2) to 2.
4.8 is the result of applying (*2) to 4.
More visually:
[(+1), (<em>2)]&lt;</em>&gt;[2,4]
[ (+1)2, (+1)4, (*2)2, (*2)4]
It maps each function value from the first list over the sec-
ond list, applies the operations, and returns one list. The fact</p>
<p>CHAPTER 17. APPLICATIVE 1071
that it doesn’t return two lists or a nested list or some other
configuration in which both structures are preserved is the
monoidal part; the reason we don’t have a list of functions
concatenated with a list of values is the function application
part.
We can see this relationship more readily if we use the
tuple constructor with the list Applicative . We’ll use the infix
operator for fmapto map the tuple constructor over the first list.
This embeds an unapplied function (the tuple data constructor
in this case) into some structure (a list, in this case), and returns
a list of partially applied functions. The (infix) applicative will
then apply one list of operations to the second list, monoidally
appending the two lists:
Prelude&gt; (,) &lt;$&gt; [1, 2] &lt;<em>&gt; [3, 4]
[(1,3),(1,4),(2,3),(2,4)]
You might think of it this way:
Prelude&gt; (,) &lt;$&gt; [1, 2] &lt;</em>&gt; [3, 4]
-- fmap the (,) over the first list
[(1, ), (2, )] &lt;*&gt; [3, 4]
-- then we apply the first list
-- to the second
[(1,3),(1,4),(2,3),(2,4)]</p>
<p>CHAPTER 17. APPLICATIVE 1072
TheliftA2 function gives us another way to write this, too:
Prelude&gt; liftA2 (,) [1, 2] [3, 4]
[(1,3),(1,4),(2,3),(2,4)]
Let’s look at a few more examples of the same pattern:
Prelude&gt; (+) &lt;$&gt; [1, 2] &lt;<em>&gt; [3, 5]
[4,6,5,7]
Prelude&gt; liftA2 (+) [1, 2] [3, 5]
[4,6,5,7]
Prelude&gt; max &lt;$&gt; [1, 2] &lt;</em>&gt; [1, 4]
[1,4,2,4]
Prelude&gt; liftA2 max [1, 2] [1, 4]
[1,4,2,4]
If you’re familiar with Cartesian products1, this probably
looks a lot like one, but with functions.
We’re going to run through some more examples, to give
you a little more context for when these functions can become
useful. Thefollowingexampleswilluseafunctioncalled lookup
that we’ll briefly demonstrate:
Prelude&gt; :t lookup
lookup :: Eq a =&gt; a -&gt; [(a, b)] -&gt; Maybe b
1The Cartesian product is the product of two sets that results in all the ordered pairs
(tuples) of the elements of those sets.</p>
<p>CHAPTER 17. APPLICATIVE 1073
Prelude&gt; let l = lookup 3 [(3, &quot;hello&quot;)]
Prelude&gt; l
Just &quot;hello&quot;
Prelude&gt; fmap length $ l
Just 5
Prelude&gt; let c (x:xs) = toUpper x:xs
Prelude&gt; fmap c $ l
Just &quot;Hello&quot;
So,lookup searches inside a list of tuples for a value that
matches the input and returns the paired value wrapped inside
aMaybecontext.
It’s worth pointing out here that if you’re working with
Mapdata structures instead of lists of tuples, you can import
Data.Map and use a Mapversion of lookup along with fromList to
accomplish the same thing with that data structure:
Prelude&gt; let m = fromList [(3, &quot;hello&quot;)]
Prelude&gt; fmap c $ Data.Map.lookup 3 m
Just &quot;Hello&quot;
That may seem trivial at the moment, but Mapis a frequently
used data structure, so it’s worth mentioning.
Nowthatwehavevalueswrappedina Maybecontext, perhaps
we’d like to apply some functions to them. This is where we
want applicative operations. Although it’s more likely that
we’d have functions fetching data from somewhere else rather</p>
<p>CHAPTER 17. APPLICATIVE 1074
than having it all listed in our code file, we’ll go ahead and
define some values in a source file for convenience:
importControl.Applicative
fx=
lookup x [ ( 3,&quot;hello&quot;)
, (4,&quot;julie&quot;)
, (5,&quot;kbai&quot;)]
gy=
lookup y [ ( 7,&quot;sup?&quot;)
, (8,&quot;chris&quot;)
, (9,&quot;aloha&quot;)]
hz=
lookup z [( 2,3), (5,6), (7,8)]
mx=
lookup x [( 4,10), (8,13), (1,9001)]
Now we want to look things up and add them together. We’ll
start with some operations over these data:
Prelude&gt; f 3
Just &quot;hello&quot;
Prelude&gt; g 8
Just &quot;chris&quot;</p>
<p>CHAPTER 17. APPLICATIVE 1075
Prelude&gt; (++) &lt;$&gt; f 3 &lt;<em>&gt; g 7
Just &quot;hellosup?&quot;
Prelude&gt; (+) &lt;$&gt; h 5 &lt;</em>&gt; m 1
Just 9007
Prelude&gt; (+) &lt;$&gt; h 5 &lt;<em>&gt; m 6
Nothing
So we first fmapthose functions over the value inside the first
Maybecontext, if it’s a Justvalue, making it a partially applied
function wrapped in a Maybecontext. Then we use the tie-
fighter to apply that to the second value, again wrapped in a
Maybe. If either value is a Nothing , we get Nothing .
We can again do the same thing with liftA2 :
Prelude&gt; liftA2 (++) (g 9) (f 4)
Just &quot;alohajulie&quot;
Prelude&gt; liftA2 (^) (h 5) (m 4)
Just 60466176
Prelude&gt; liftA2 (</em>) (h 5) (m 4)
Just 60
Prelude&gt; liftA2 (<em>) (h 1) (m 1)
Nothing
Your applicative context can also sometimes be IO:
(++)&lt;$&gt;getLine &lt;</em>&gt;getLine
(,)&lt;$&gt;getLine &lt;*&gt;getLine</p>
<p>CHAPTER 17. APPLICATIVE 1076
Try it. Now try using fmapto get the length of the resulting
string of the first example.
Exercises: Lookups
In the following exercises you will need to use the following
terms to make the expressions typecheck:
1.pure
2.(&lt;$&gt;)
-- or fmap
3.(&lt;*&gt;)
Make the following expressions typecheck.
1.added::MaybeInteger
added=
(+3) (lookup 3$zip [1,2,3] [4,5,6])
2.y::MaybeInteger
y=lookup3$zip [1,2,3] [4,5,6]
z::MaybeInteger
z=lookup2$zip [1,2,3] [4,5,6]
tupled::Maybe(Integer,Integer)
tupled=(,) y z</p>
<p>CHAPTER 17. APPLICATIVE 1077
3.importData.List (elemIndex )
x::MaybeInt
x=elemIndex 3[1,2,3,4,5]
y::MaybeInt
y=elemIndex 4[1,2,3,4,5]
max'::Int-&gt;Int-&gt;Int
max'=max
maxed::MaybeInt
maxed=max' x y
4.xs=[1,2,3]
ys=[4,5,6]
x::MaybeInteger
x=lookup3$zip xs ys
y::MaybeInteger
y=lookup2$zip xs ys
summed::MaybeInteger
summed=sum$(,) x y</p>
<p>CHAPTER 17. APPLICATIVE 1078
Identity
TheIdentity type here is a way to introduce structure without
changing the semantics of what you’re doing. We’ll see it used
with these typeclasses that involve function application around
and over structure, but this type itself isn’t very interesting, as
it has no semantic flavor.
Specializing the types
Here is what the type will look like when our structure is
Identity :
-- f ~ Identity
-- Applicative f =&gt;
typeId=Identity
(&lt;<em>&gt;)::f (a-&gt;b)-&gt;f a-&gt;f b
(&lt;</em>&gt;)::Id(a-&gt;b)-&gt;Ida-&gt;Idb
pure::a-&gt;f a
pure::a-&gt;Ida
Why would we use Identity just to introduce some struc-
ture? What is the meaning of all this?
Prelude&gt; let xs = [1, 2, 3]</p>
<p>CHAPTER 17. APPLICATIVE 1079
Prelude&gt; let xs' = [9, 9, 9]
Prelude&gt; const &lt;$&gt; xs &lt;<em>&gt; xs'
[1,1,1,2,2,2,3,3,3]
Prelude&gt; let mkId = Identity
Prelude&gt; const &lt;$&gt; mkId xs &lt;</em>&gt; mkId xs'
Identity [1,2,3]
Having this extra bit of structure around our values lifts the
constfunction, from mapping over the lists to mapping over
theIdentity . We have to go over an 𝑓structure to apply the
function to the values inside. If our 𝑓is the list, constapplies to
the values inside the list, as we saw above. If the 𝑓isIdentity ,
thenconsttreats the lists inside the Identity structure as single
values, not structure containing values.
Exercise: Identity Instance
Write an Applicative instance for Identity .</p>
<p>CHAPTER 17. APPLICATIVE 1080
newtype Identity a=Identity a
deriving (Eq,Ord,Show)
instance Functor Identity where
fmap=undefined
instance Applicative Identity where
pure=undefined
(&lt;*&gt;)=undefined
Constant
This is not so diﬀerent from the Identity type, except this
not only provides structure it also acts like the constfunction.
It sort of throws away a function application. If this seems
confusing, it’s because it is. However, it is also something that,
likeIdentity has real use cases, and you will see it in other
people’s code. It can be difficult to get used to using it yourself,
but we keep trying.
This datatype is like the constfunction in that it takes two
arguments but one of them gets discarded. In the case of the
datatype, we have to map our function over the argument
that gets discarded. So there is no value to map over, and the
function application doesn’t happen.</p>
<p>CHAPTER 17. APPLICATIVE 1081
Specializing the types
All right, so here’s what the types will look like:
-- f ~ Constant e
typeC=Constant
(&lt;<em>&gt;)::f (a-&gt;b)-&gt;f a-&gt;f b
(&lt;</em>&gt;)::Ce (a-&gt;b)-&gt;Ce a-&gt;Ce b
pure::a-&gt;f a
pure::a-&gt;Ce a
And here are some examples of how it works. These are,
yes, a bit contrived, but showing you real code with this in it
would probably make it much harder for you to see what’s
going on:
Prelude&gt; let f = Constant (Sum 1)
Prelude&gt; let g = Constant (Sum 2)
Prelude&gt; f &lt;<em>&gt; g
Constant {getConstant = Sum {getSum = 3}
Prelude&gt; Constant undefined &lt;</em>&gt; g
Constant (Sum {getSum =
*** Exception: Prelude.undefined
Prelude&gt; pure 1
1</p>
<p>CHAPTER 17. APPLICATIVE 1082
Prelude&gt; pure 1 :: Constant String Int
Constant {getConstant = &quot;&quot;}
It can’t do anything because it can only hold onto the one
value. The function doesn’t exist, and the 𝑏is a ghost. So you
use this when whatever you want to do involves throwing away
a function application. We know it seems somewhat crazy, but
we promise there are really times real coders do this in real
code. Pinky swear.
Exercise: Constant Instance
Write an Applicative instance for Constant .
newtype Constant a b=
Constant { getConstant ::a }
deriving (Eq,Ord,Show)
instance Functor (Constant a)where
fmap=undefined
instance Monoida
=&gt;Applicative (Constant a)where
pure=undefined
(&lt;*&gt;)=undefined</p>
<p>CHAPTER 17. APPLICATIVE 1083
Maybe Applicative
WithMaybe, we’re doing something a bit diﬀerent from above.
We saw previously how to use fmapwithMaybe, but here our
function is also embedded in a Maybestructure. Therefore,
when𝑓isMaybe, we’re saying the function itself might not exist,
because we’re allowing the possibility of the function to be
applied being a Nothing case.
Specializing the types
Here’s what the type looks like when we’re using Maybeas our
𝑓structure:
-- f ~ Maybe
typeM=Maybe
(&lt;<em>&gt;)::f (a-&gt;b)-&gt;f a-&gt;f b
(&lt;</em>&gt;)::M(a-&gt;b)-&gt;Ma-&gt;Mb
pure::a-&gt;f a
pure::a-&gt;Ma
Are you ready to validate some persons? Yes. Yes, you are.</p>
<p>CHAPTER 17. APPLICATIVE 1084
Using the Maybe Applicative
Consider the following example where we validate our inputs
to create a value of type Maybe Person , where the Maybeis because
our inputs might be invalid:
validateLength ::Int
-&gt;String
-&gt;MaybeString
validateLength maxLen s =
if(length s) &gt;maxLen
thenNothing
elseJusts
newtype Name=
NameStringderiving (Eq,Show)
newtype Address =
Address Stringderiving (Eq,Show)
mkName::String-&gt;MaybeName
mkNames=
fmapName$validateLength 25s
mkAddress ::String-&gt;MaybeAddress
mkAddress a=
fmapAddress $validateLength 100a</p>
<p>CHAPTER 17. APPLICATIVE 1085
Now we’ll make a smart constructor for a Person :
dataPerson=
PersonNameAddress
deriving (Eq,Show)
mkPerson ::String
-&gt;String
-&gt;MaybePerson
mkPerson n a=
casemkName n of
Nothing -&gt;Nothing
Justn'-&gt;
casemkAddress a of
Nothing -&gt;Nothing
Justa'-&gt;
Just$Personn' a'
The problem here is while we’ve successfully leveraged fmap
fromFunctor in the simpler cases of mkName andmkAddress , we
can’t really make that work here with mkPerson . Let’s investigate
why:
Prelude&gt; :t fmap Person (mkName &quot;Babe&quot;)
fmap Person (mkName &quot;Babe&quot;)
:: Maybe (Address -&gt; Person)</p>
<p>CHAPTER 17. APPLICATIVE 1086
This has worked so far for the first argument to the Person
constructor that we’re validating, but we’ve hit a roadblock.
Can you see the problem?
Prelude&gt; :{
*Main| fmap (fmap Person (mkName &quot;Babe&quot;))
*Main| (mkAddress &quot;old macdonald's&quot;)
*Main| :}
Couldn't match expected type ‘Address -&gt; b’
with actual type
‘Maybe (Address -&gt; Person)’
Possible cause: ‘fmap’ is applied to too
many arguments
In the first argument of ‘fmap’, namely
‘(fmap Person (mkName &quot;Babe&quot;))’
In the expression:
fmap (fmap Person (mkName &quot;Babe&quot;)) v
The problem is that our (a -&gt; b) is now hiding inside Maybe.
Let’s look at the type of fmapagain:
fmap :: Functor f =&gt; (a -&gt; b) -&gt; f a -&gt; f b
Maybeis definitely a Functor , but that’s not really going to
help us here. We need to be able to map a function embedded
in our𝑓.Applicative gives us what we need here!</p>
<p>CHAPTER 17. APPLICATIVE 1087
(&lt;<em>&gt;)::Applicative f
=&gt;f (a-&gt;b)-&gt;f a-&gt;f b
Now let’s see if we can wield this new toy:
Prelude&gt; let s = &quot;old macdonald's&quot;
Prelude&gt; let addy = mkAddress s
Prelude&gt; let b = mkName &quot;Babe&quot;
Prelude&gt; let person = fmap Person b
Prelude&gt; person &lt;</em>&gt; addy
Just (Person (Name &quot;Babe&quot;)
(Address &quot;old macdonald's&quot;))
Nice, right? A little ugly though. Using the infix alias for
fmapcalled&lt;$&gt;cleans it up a bit, at least to Haskellers’ eyes:
Prelude&gt; Person &lt;$&gt; mkName &quot;Babe&quot; &lt;*&gt; addy
Just (Person (Name &quot;Babe&quot;)
(Address &quot;old macdonald's&quot;))
We still use fmap(via&lt;$&gt;) here for the first lifting over Maybe;
after that our (a -&gt; b) is hiding in the 𝑓where𝑓=Maybe, so we
have to start using Applicative to keep mapping over that.
We can now use a much shorter definition of mkPerson !</p>
<p>CHAPTER 17. APPLICATIVE 1088
mkPerson ::String
-&gt;String
-&gt;MaybePerson
mkPerson n a=
Person&lt;$&gt;mkName n &lt;*&gt;mkAddress a
As an additional bonus, this is now far less annoying to
extend if we added new fields as well.
Breaking down that example
We’re going to give the Functor andApplicative instances for
Maybethe same treatment we gave folds. This will be a bit long.
It is possible that some of this will seem like too much detail;
read it to whatever depth you feel you need to. It will sit here,
patiently waiting to see if you ever need to come back and
read it more closely.
Maybe Functor and the Name constructor</p>
<p>CHAPTER 17. APPLICATIVE 1089
instance Functor Maybewhere
fmap_Nothing =Nothing
fmap f ( Justa) =Just(f a)
instance Applicative Maybewhere
pure=Just
Nothing &lt;<em>&gt; _ = Nothing
_ &lt;</em>&gt;Nothing =Nothing
Justf&lt;*&gt;Justa=Just(f a)
TheApplicative instance is not exactly the same as the in-
stance in base, but that’s for simplification. For your purposes,
it produces the same results.
First the function and datatype definitions for our functor
write-up for how we’re using the validateLength function with
NameandAddress :</p>
<p>CHAPTER 17. APPLICATIVE 1090
validateLength ::Int
-&gt;String
-&gt;MaybeString
validateLength maxLen s =
if(length s) &gt;maxLen
thenNothing
elseJusts
newtype Name=
NameStringderiving (Eq,Show)
newtype Address =
Address Stringderiving (Eq,Show)
mkName::String-&gt;MaybeName
mkNames=fmapName$validateLength 25s
mkAddress ::String-&gt;MaybeAddress
mkAddress a=
fmapAddress $validateLength 100a
Now we’re going to start filling in the definitions and ex-
panding them equationally like we did in the chapter on folds.
First we apply mkName to the value &quot;babe&quot; so that𝑠is bound
to that string:</p>
<p>CHAPTER 17. APPLICATIVE 1091
mkNames=
fmapName$validateLength 25s
mkName&quot;babe&quot;=
fmapName$validateLength 25&quot;babe&quot;
Now we need to figure out what validateLength is about since
that has to be evaluated before we know what fmapis mapping
over. Here we’re applying it to 25 and ”babe”, evaluating the
length of the string ”babe”, and then determining which branch
in the if-then-else wins:
validateLength ::Int
-&gt;String
-&gt;MaybeString
validateLength 25&quot;babe&quot;=
if(length &quot;babe&quot;)&gt;25
thenNothing
elseJust&quot;babe&quot;
if4&gt;25
thenNothing
elseJust&quot;babe&quot;
-- 4 isn't greater than 25, so:
validateLength 25&quot;babe&quot;=
Just&quot;babe&quot;</p>
<p>CHAPTER 17. APPLICATIVE 1092
Now we’re going to replace validateLength applied to 25 and
”babe” with what it evaluated to, then figure out what the fmap
NameoverJust &quot;babe&quot; business is about:
mkName&quot;babe&quot;=
fmapName$Just&quot;babe&quot;
fmapName$Just&quot;babe&quot;
Keeping in mind the type of fmapfromFunctor , we see the
data constructor Nameis the function (a -&gt; b) we’re mapping
over some functorial 𝑓. In this case, 𝑓isMaybe. The𝑎in𝑓 𝑎is
String :
(a-&gt;b)-&gt;f a-&gt;f b
:tName ::(String-&gt;Name)
:tJust&quot;babe&quot;::MaybeString
typeM=Maybe
(a-&gt;b)-&gt;f a -&gt;f b
(String-&gt;Name)-&gt;MString-&gt;MName
Since we know we’re dealing with the Functor instance for
Maybe, we can inline thatfunction’s definition too!</p>
<p>CHAPTER 17. APPLICATIVE 1093
fmap_Nothing =Nothing
fmapf (Justa) =Just(f a)
-- We have (Just &quot;babe&quot;) so
-- skipping Nothing case
-- fmap _ Nothing = Nothing
fmapf (Justa) =
Just(f a)
fmapName(Just&quot;babe&quot;)=
Just(Name&quot;babe&quot;)
mkName&quot;babe&quot;=fmapName$Just&quot;babe&quot;
mkName&quot;babe&quot;=Just(Name&quot;babe&quot;)
-- f b
Maybe Applicative and Person
dataPerson=
PersonNameAddress
deriving (Eq,Show)
First we’ll be using the Functor to map the Person data con-
structor over the Maybe Name value. Unlike NameandAddress ,
Person takes two arguments rather than one.</p>
<p>CHAPTER 17. APPLICATIVE 1094
Person
&lt;$&gt;Just(Name&quot;babe&quot;)
&lt;*&gt;Just(Address &quot;farm&quot;)
fmapPerson(Just(Name&quot;babe&quot;))
:tPerson::Name-&gt;Address -&gt;Person
:tJust(Name&quot;babe&quot;)::MaybeName
(a-&gt;b)-&gt;f a-&gt;f b
(Name-&gt;Address -&gt;Person)
a-&gt;b
-&gt;MaybeName-&gt;Maybe(Address -&gt;Person)
f a f b</p>
<p>CHAPTER 17. APPLICATIVE 1095
fmap_Nothing =Nothing
fmapf (Justa) =Just(f a)
fmapPerson(Just(Name&quot;babe&quot;))
f::Person
a::Name&quot;babe&quot;
-- We skip this pattern match
-- because we have Just
-- fmap _ Nothing = Nothing
fmapf ( Justa) =
Just(f a)
fmapPerson(Just(Name&quot;babe&quot;))=
Just(Person(Name&quot;babe&quot;))
The problem is Person (Name &quot;babe&quot;) is awaiting another ar-
gument, the address, so it’s a partially applied function. That’s
our(a -&gt; b) in the type of Applicative ’s(&lt;*&gt;). The𝑓wrapping
our(a -&gt; b) is theMaybewhich results from us possibly not hav-
ing had an 𝑎to map over to begin with, resulting in a Nothing
value:</p>
<p>CHAPTER 17. APPLICATIVE 1096
-- Person is awaiting another argument
:tJust(Person(Name&quot;babe&quot;))
::Maybe(Address -&gt;Person)
:tJust(Address &quot;farm&quot;)::MaybeAddress
-- We want to apply the partially
-- applied (Person &quot;babe&quot;) inside the
-- 'Just' to the &quot;farm&quot; inside the Just.
Just(Person(Name&quot;babe&quot;))
&lt;<em>&gt;Just(Address &quot;farm&quot;)
So, since the function we want to map is inside the same
structure as the value we want to apply it to, we need the
Applicative (&lt;</em>&gt;) . In the following, we remind you of what the
type looks like and how the type specializes to this application:
f(a-&gt;b)-&gt;f a-&gt;f b
typeM=Maybe
typeAddy=Address
M(Addy-&gt;Person)-&gt;MAddy-&gt;MPerson
f( a-&gt;b )-&gt;f a-&gt;f b</p>
<p>CHAPTER 17. APPLICATIVE 1097
We know we’re using the Maybe Applicative , so we can go
ahead and inline the definition. Reminder that this version of
theApplicative instance is simplified from the one in GHC, so
please don’t email us to tell us our instance is wrong:
instance Applicative Maybewhere
pure=Just
Nothing &lt;<em>&gt; _ = Nothing
_ &lt;</em>&gt;Nothing =Nothing
Justf&lt;*&gt;Justa=Just(f a)
We know we can ignore the Nothing cases because our func-
tion isJust, our value is Just...and our cause is just! Just…kid-
ding.
If we fill in our partially applied Person constructor for 𝑓,
and our Address value for 𝑎, it’s not too hard to see how the
final result fits.</p>
<p>CHAPTER 17. APPLICATIVE 1098
-- Neither function nor value are Nothing,
-- so we skip these two cases
-- Nothing &lt;<em>&gt; _ = Nothing
-- _ &lt;</em>&gt; Nothing = Nothing
Justf&lt;<em>&gt;Justa=Just(f a)
Just(Person(Name&quot;babe&quot;))
&lt;</em>&gt;Just(Address &quot;farm&quot;)=
Just(Person(Name&quot;babe&quot;)
(Address &quot;farm&quot;))
Before we moooove on
dataCow=Cow{
name::String
, age ::Int
, weight ::Int
}deriving (Eq,Show)
noEmpty ::String-&gt;MaybeString
noEmpty &quot;&quot;=Nothing
noEmpty str=Juststr
noNegative ::Int-&gt;MaybeInt
noNegative n|n&gt;=0=Justn
|otherwise =Nothing</p>
<p>CHAPTER 17. APPLICATIVE 1099
-- Validating to get rid of empty
-- strings, negative numbers
cowFromString ::String
-&gt;Int
-&gt;Int
-&gt;MaybeCow
cowFromString name' age' weight' =
casenoEmpty name' of
Nothing -&gt;Nothing
Justnammy-&gt;
casenoNegative age' of
Nothing -&gt;Nothing
Justagey-&gt;
casenoNegative weight' of
Nothing -&gt;Nothing
Justweighty -&gt;
Just(Cownammy agey weighty)
cowFromString is…bad. You can probably tell. But by the use
of Applicative, it can be improved!</p>
<p>CHAPTER 17. APPLICATIVE 1100
-- you'll need to import this if
-- you have GHC &lt;7.10
importControl.Applicative
cowFromString' ::String
-&gt;Int
-&gt;Int
-&gt;MaybeCow
cowFromString' name' age' weight' =
Cow&lt;$&gt;noEmpty name'
&lt;<em>&gt;noNegative age'
&lt;</em>&gt;noNegative weight'
Or if we want other Haskellers to think we’re really cool
and hip:
cowFromString'' ::String
-&gt;Int
-&gt;Int
-&gt;MaybeCow
cowFromString'' name' age' weight' =
liftA3Cow(noEmpty name')
(noNegative age')
(noNegative weight')</p>
<p>CHAPTER 17. APPLICATIVE 1101
So, we’re taking advantage of the Maybe Applicative here.
What does that look like? First we’ll use the infix syntax for
fmap,&lt;$&gt;, and apply &lt;<em>&gt;:
Prelude&gt; let cow1 = Cow &lt;$&gt; noEmpty &quot;Bess&quot;
Prelude&gt; :t cow1
cow1 :: Maybe (Int -&gt; Int -&gt; Cow)
Prelude&gt; let cow2 = cow1 &lt;</em>&gt; noNegative 1
Prelude&gt; :t cow2
cow2 :: Maybe (Int -&gt; Cow)
Prelude&gt; let cow3 = cow2 &lt;*&gt; noNegative 2
Prelude&gt; :t cow3
cow3 :: Maybe Cow
Then with liftA3 :
Prelude&gt; let cow1 = liftA3 Cow
Prelude&gt; :t cow1
cow1 :: Applicative f
=&gt; f String -&gt; f Int -&gt; f Int -&gt; f Cow</p>
<p>CHAPTER 17. APPLICATIVE 1102
Prelude&gt; let cow2 = cow1 (noEmpty &quot;blah&quot;)
Prelude&gt; :t cow2
cow2 :: Maybe Int -&gt; Maybe Int -&gt; Maybe Cow
Prelude&gt; let cow3 = cow2 (noNegative 1)
Prelude&gt; :t cow3
cow3 :: Maybe Int -&gt; Maybe Cow
Prelude&gt; let cow4 = cow3 (noNegative 2)
Prelude&gt; :t cow4
cow4 :: Maybe Cow
So, from a simplified point of view, Applicative is really just
a way of saying:</p>
<p>CHAPTER 17. APPLICATIVE 1103
-- we fmap'd my function over some
-- functorial <code>f'' or it already -- was in </code>f'' somehow
-- f ~ Maybe
cow1::Maybe(Int-&gt;Int-&gt;Cow)
cow1=fmapCow(noEmpty &quot;Bess&quot;)
-- and we hit a situation where want to map
-- f (a -&gt; b)
-- not just (a -&gt; b)
(&lt;<em>&gt;)::Applicative f
=&gt;f (a-&gt;b)-&gt;f a-&gt;f b
-- over some f a
-- to get an f b
cow2::Maybe(Int-&gt;Cow)
cow2=cow1&lt;</em>&gt;noNegative 1
As a result, you may be able to imagine yourself saying, “I
want to do something kinda like an fmap, but my function is
embedded in the functorial structure too, not only the value I
want to apply my function to.” This is a basic motivation for
Applicative .
With the Applicative instance for Maybe, what we’re doing is</p>
<p>CHAPTER 17. APPLICATIVE 1104
enriching functorial application with the additional proviso
that, “I may not have a function at all.”
We can see this in the following specialization of the apply
function (&lt;<em>&gt;):
(&lt;</em>&gt;)::Applicative f
=&gt;f (a-&gt;b)-&gt;f a-&gt;f b
f~Maybe
typeM=Maybe
maybeApply ::M(a-&gt;b)-&gt;Ma-&gt;Mb
maybeFmap ::(a-&gt;b)-&gt;Ma-&gt;Mb
-- maybeFmap is just fmap's type
-- specialized to Maybe
You can test these specializations (more concrete versions)
of the types:</p>
<p>CHAPTER 17. APPLICATIVE 1105
maybeApply ::Maybe(a-&gt;b)
-&gt;Maybea
-&gt;Maybeb
maybeApply =(&lt;<em>&gt;)
maybeMap ::(a-&gt;b)
-&gt;Maybea
-&gt;Maybeb
maybeMap =fmap
If you make any mistakes, the compiler will let you know:
maybeMapBad ::(a-&gt;b)
-&gt;Maybea
-&gt;f b
maybeMapBad =fmap
Couldn't match type ‘f1’ with ‘Maybe’
‘f1’ is a rigid type variable bound by
an expression type signature:
(a1 -&gt; b1) -&gt; Maybe a1 -&gt; f1 b1
Exercise: Fixer Upper
Given the function and values provided, use (&lt;$&gt;)fromFunctor ,
(&lt;</em>&gt;)andpurefrom the Applicative typeclass to fill in missing
bits of the broken code to make it work.</p>
<p>CHAPTER 17. APPLICATIVE 1106
1.const&lt;$&gt;Just&quot;Hello&quot; &lt;<em>&gt;&quot;World&quot;
2.(,,,)Just90
&lt;</em>&gt;Just10Just&quot;Tierness&quot; [1,2,3]
17.6 Applicative laws
After examining the law, test each of the expressions in the
REPL.
1.Identity
Here is the definition of the identity law:
pureid&lt;*&gt;v=v
To see examples of this law, evaluate these expressions.</p>
<p>CHAPTER 17. APPLICATIVE 1107
pureid&lt;<em>&gt;[1..5]
pureid&lt;</em>&gt;Just&quot;Hello Applicative&quot;
pureid&lt;<em>&gt;Nothing
pureid&lt;</em>&gt;Left&quot;Error'ish&quot;
pureid&lt;<em>&gt;Right8001
-- ((-&gt;) a) has an instance
pureid&lt;</em>&gt;(+1)$2
As you may recall, Functor has a similar identity law, and
comparing them directly might help you see what’s hap-
pening:
id[1..5]
fmapid [1..5]
pureid&lt;*&gt;[1..5]
The identity law states that all three of those should be
equal. You can test them for equality in your REPL or you
could write a simple test to get the answer. So, what’s pure</p>
<p>CHAPTER 17. APPLICATIVE 1108
doing for us? It’s embedding our idfunction into some
structure so that we can use applyinstead of fmap.
2.Composition
Here is the definition of the composition law for applica-
tives:
pure(.)&lt;<em>&gt;u&lt;</em>&gt;v&lt;<em>&gt;w=
u&lt;</em>&gt;(v&lt;*&gt;w)
You may find the syntax a bit unusual and difficult to
read here. This is similar to the law of composition for
Functor . It is the law stating that the result of composing
our functions first and then applying them and the re-
sult of applying the functions first then composing them
should be the same. We’re using the composition opera-
tor as a prefix instead of the more usual infix, and using
purein order to embed that operator into the appropriate
structure so that it can work with apply.</p>
<p>CHAPTER 17. APPLICATIVE 1109
pure (.)
&lt;<em>&gt;[(+1)]
&lt;</em>&gt;[(<em>2)]
&lt;</em>&gt;[1,2,3]
[(+1)]&lt;<em>&gt;([(<em>2)]&lt;</em>&gt;[1,2,3])
pure (.)
&lt;</em>&gt;Just(+1)
&lt;<em>&gt;Just(<em>2)
&lt;</em>&gt;Just1
Just(+1)
&lt;</em>&gt;(Just(<em>2)&lt;</em>&gt;Just1)
This law is meant to ensure that there are no surprises
resulting from composing your function applications.
3.Homomorphism
Ahomomorphism is a structure-preserving map between
two algebraic structures. The eﬀect of applying a func-
tion that is embedded in some structure to a value that is
embedded in some structure should be the same as ap-
plying a function to a value without aﬀecting any outside
structure:</p>
<p>CHAPTER 17. APPLICATIVE 1110
puref&lt;<em>&gt;pure x=pure (f x)
That’s the statement of the law. Here’s how it looks in
practice:
pure(+1)&lt;</em>&gt;pure1
pure((+1)1)
Those two lines of code should give you the same result.
In fact, the result you see for those should be indistin-
guishable from the result of:
(+1)1
Because the structure that pureis providing there isn’t
meaningful. So you can think of this law as having to do
with the monoidal part of the applicative deal: the result
should be the result of the function application without
doing anything other than combining the structure bits.
Just as we saw how fmapis really just a special type of
function application that ignores a context or surround-
ing structure, applicative is also function application that
preserves structure. However, with applicative, since the
function being applied alsohas structure, the structures
have to be monoidal and come together in some fashion.</p>
<p>CHAPTER 17. APPLICATIVE 1111
pure(+1)&lt;<em>&gt;pure1::MaybeInt
pure((+1)1)::MaybeInt
Those two results should again be the same, but this time
the structure is being provided by Maybe, so will the result
of:
(+1)1
be equal this time around?
Here are a couple more examples to try out:
pure(+1)&lt;</em>&gt;pure1::[Int]
pure(+1)&lt;<em>&gt;pure1::EitheraInt
The general idea of the homomorphism law is that apply-
ing the function doesn’t change the structure around the
values.
4.Interchange
We begin again by looking at the definition of the inter-
change law:
u&lt;</em>&gt;pure y=pure ($y)&lt;*&gt;u</p>
<p>CHAPTER 17. APPLICATIVE 1112
It might help to break that down a bit. To the left of &lt;<em>&gt;
must always be a function embedded in some structure.
In the above definition, 𝑢represents a function embedded
in some structure:
Just(+2)&lt;</em>&gt;pure2
-- u &lt;<em>&gt; pure y
-- equals
Just4
The right side of the definition might be a bit less obvious.
By sectioning the $function application operator with
the𝑦, we create an environment in which the 𝑦is there,
awaiting a function to apply to it. Let’s try lining up the
types again and see if that clears this up:
pure($2)&lt;</em>&gt;Just(+2)
-- Remember, ($ 2) can become more concrete
($2)::Numa=&gt;(a-&gt;b)-&gt;b
Just(+2)::Numa=&gt;Maybe(a-&gt;a)
If you’re a bit confused by ($ 2), keep in mind that this
is sectioning the dollar-sign operator and applying the
second argument only, not the first. As a result, the type
changes in the following manner:</p>
<p>CHAPTER 17. APPLICATIVE 1113
-- These are the same
($2)
\f-&gt;f$2
($)::(a-&gt;b)-&gt;a-&gt;b
($2)::(a-&gt;b) -&gt;b
Then concreting the types of Applicative ’s methods:
mPure::a-&gt;Maybea
mPure=pure
embed::Numa=&gt;Maybe((a-&gt;b)-&gt;b)
embed=mPure ($2)
mApply::Maybe((a-&gt;b)-&gt;b)
-&gt;Maybe(a-&gt;b)
-&gt;Maybe b
mApply=(&lt;*&gt;)
myResult =pure ($2) <code>mApply</code> Just(+2)
-- myResult == Just 4
Then translating the types side by side, with diﬀerent
letters for some of the type variables to avoid confusion</p>
<p>CHAPTER 17. APPLICATIVE 1114
when comparing the original type with the more concrete
form:
(&lt;<em>&gt;)::Applicative f
=&gt;f (x-&gt;y)
-&gt;f x
-&gt;f y
mApply::Maybe((a-&gt;b)-&gt;b)
-&gt;Maybe(a-&gt;b)
-&gt;Maybe b
f ~Maybe
x <del>(a-&gt;b)
y ~ b
(x-&gt;y)</del>(a-&gt;b)-&gt;b
According to the interchange law, this should be true:
(Just(+2)&lt;</em>&gt;pure2)
==(pure ($2)&lt;*&gt;Just(+2))
And you can see why that should be true, because despite
the weird syntax, the two functions are doing the same
job. Here are some more examples for you to try out:</p>
<p>CHAPTER 17. APPLICATIVE 1115
[(+1), (<em>2)]&lt;</em>&gt;pure1
pure($1)&lt;<em>&gt;[(+1), (<em>2)]
Just(+3)&lt;</em>&gt;pure1
pure($1)&lt;</em>&gt;Just(+3)
EveryApplicative instance you write should obey those four
laws. This keeps your code composable and free of unpleasant
surprises.
17.7 You knew this was coming
Property testing the Applicative laws! You should have got the
gist of how to write properties based on laws, so we’re going to
use a library this time. Conal Elliott has a nice library called
checkers on Hackage and Github which provides some nice
properties and utilities for QuickCheck .
After installing checkers , we can reuse the existing proper-
ties for validating Monoids andFunctor s to revisit what we did
previously.</p>
<p>CHAPTER 17. APPLICATIVE 1116
moduleBadMonoid where
importData.Monoid
importTest.QuickCheck
importTest.QuickCheck.Checkers
importTest.QuickCheck.Classes
dataBull=
Fools
|Twoo
deriving (Eq,Show)
instance Arbitrary Bullwhere
arbitrary =
frequency [ ( 1, return Fools)
, (1, return Twoo) ]
instance MonoidBullwhere
mempty=Fools
mappend _ _ =Fools
instance EqPropBullwhere(=-=)=eq
main::IO()
main=quickBatch (monoid Twoo)</p>
<p>CHAPTER 17. APPLICATIVE 1117
There are some diﬀerences here worth noting. One is that
we don’t have to define the Monoid laws asQuickCheck properties
ourselves; they are already bundled into a TestBatch called
monoid . Another is that we need to define EqProp for our custom
datatype. This is straightforward because checkers exports a
function called eqwhich reuses the pre-existing Eqinstance
for the datatype. Finally, we’re passing a value of our type
tomonoid so it knows which Arbitrary instance to use to get
random values — note it doesn’t usethis value for anything.
Then we can run mainto kick it oﬀ and see how it goes:
Prelude&gt; main
monoid:
left identity:
*** Failed! Falsifiable (after 1 test):
Twoo
right identity:
*** Failed! Falsifiable (after 2 tests):
Twoo
associativity: +++ OK, passed 500 tests.
As we expect, it was able to falsify left and right identity for
Bull. Now let’s test a pre-existing Applicative instance, such
as list or Maybe. The type for the TestBatch which validates
Applicative instances is a bit gnarly, so please bear with us:</p>
<p>CHAPTER 17. APPLICATIVE 1118
applicative
::(Showa,Show(m a),Show(m (a-&gt;b))
,Show(m (b-&gt;c)),Applicative m
,CoArbitrary a,EqProp(m a)
,EqProp(m b),EqProp(m c)
,Arbitrary a,Arbitrary b
,Arbitrary (m a)
,Arbitrary (m (a-&gt;b))
,Arbitrary (m (b-&gt;c)))
=&gt;m (a, b, c) -&gt;TestBatch
First, a trick for managing functions like this. We know it’s
going to want Arbitrary instances for the Applicative structure,
functions (from 𝑎to𝑏,𝑏to𝑐) embedded in that structure, and
that it wants EqProp instances. That’s all well and good, but we
can ignore that.
m (a, b, c) -&gt;TestBatch
We just care about m (a, b, c) -&gt; TestBatch . We could pass
an actual value giving us our Applicative structure and three
values which could be of diﬀerent type, but don’t have to
be. We could also pass a bottom with a type assigned to let it
know what to randomly generate for validating the Applicative
instance.
Prelude&gt; let xs = [(&quot;b&quot;, &quot;w&quot;, 1)]</p>
<p>CHAPTER 17. APPLICATIVE 1119
Prelude&gt; quickBatch $ applicative xs
applicative:
identity: +++ OK, passed 500 tests.
composition: +++ OK, passed 500 tests.
homomorphism: +++ OK, passed 500 tests.
interchange: +++ OK, passed 500 tests.
functor: +++ OK, passed 500 tests.
Note that it defaulted the 1 :: Num a =&gt; a in order to not
have an ambiguous type. We would’ve had to specify that
outside of GHCi. In the following example we’ll use a bottom
to fire the typeclass dispatch:
Prelude&gt; type SSI = (String, String, Int)
Prelude&gt; :{
*Main| let trigger :: [SSI]
*Main| trigger = undefined
*Main| :}
Prelude&gt; quickBatch (applicative trigger)
applicative:
identity: +++ OK, passed 500 tests.
composition: +++ OK, passed 500 tests.
homomorphism: +++ OK, passed 500 tests.
interchange: +++ OK, passed 500 tests.
functor: +++ OK, passed 500 tests.</p>
<p>CHAPTER 17. APPLICATIVE 1120
Again, it’s not evaluating the value you pass it. That value
is just to let it know what types to use.
17.8 ZipList Monoid
The default monoid of lists in the GHC Prelude is concatena-
tion, but there is another way to monoidally combine lists.
Whereas the default list mappend ends up doing the following:
[1,2,3]&lt;&gt;[4,5,6]
-- changes to
[1,2,3]++[4,5,6]
[1,2,3,4,5,6]
TheZipList monoid combines the values of the two lists
as parallel sequences using a monoid provided by the values
themselves to get the job done:</p>
<p>CHAPTER 17. APPLICATIVE 1121
[1,2,3]&lt;&gt;[4,5,6]
-- changes to
[
1&lt;&gt;4
,2&lt;&gt;5
,3&lt;&gt;6
]
This should remind you of functions like zipandzipWith .
To make the above example work, you can assert a type like
Sum Integer for theNumvalues to get a Monoid .
Prelude&gt; import Data.Monoid
Prelude&gt; 1 &lt;&gt; 2
No instance for (Num a0) arising
from a use of ‘it’
The type variable ‘a0’ is ambiguous
Note: there are several potential
instances:
... some blather that mentions Num ...
Prelude&gt; 1 &lt;&gt; (2 :: Sum Integer)
Sum {getSum = 3}</p>
<p>CHAPTER 17. APPLICATIVE 1122
Prelude doesn’t provide this Monoid for us, so we must define
it ourselves.
moduleApl1where
importControl.Applicative
importData.Monoid
importTest.QuickCheck
importTest.QuickCheck.Checkers
importTest.QuickCheck.Classes
Some unfortunate orphan instances follow. Try to avoid
these in code you’re going to keep or release.</p>
<p>CHAPTER 17. APPLICATIVE 1123
-- this isn't going to work properly
instance Monoida
=&gt;Monoid(ZipList a)where
mempty =ZipList []
mappend =liftA2 mappend
instance Arbitrary a
=&gt;Arbitrary (ZipList a)where
arbitrary =ZipList &lt;$&gt;arbitrary
instance Arbitrary a
=&gt;Arbitrary (Suma)where
arbitrary =Sum&lt;$&gt;arbitrary
instance Eqa
=&gt;EqProp(ZipList a)where
(=-=)=eq
If we fire this up in the REPL, and test for its validity as a
Monoid , it’ll fail.
Prelude&gt; let zl = ZipList [1 :: Sum Int]
Prelude&gt; quickBatch $ monoid zl
monoid:
left identity:</p>
<p>CHAPTER 17. APPLICATIVE 1124
*** Failed! Falsifiable (after 3 tests):
ZipList [ Sum {getSum = -1} ]
right identity:
*** Failed! Falsifiable (after 4 tests):
ZipList [ Sum {getSum = -1}
, Sum {getSum = 3}
, Sum {getSum = 2} ]
associativity: +++ OK, passed 500 tests.
The problem is that the empty ZipList is thezeroand not
theidentity !
Zero vs. Identity
-- Zero
n<em>0==0
-- Identity
n</em>1==n
So how do we get an identity for ZipList ?
Sum1<code>mappend</code> ??? -&gt;Sum1
instance Monoida
=&gt;Monoid(ZipList a)where
mempty =pure mempty
mappend =liftA2 mappend</p>
<p>CHAPTER 17. APPLICATIVE 1125
You’ll find out what the puredoes here when you write the
Applicative forZipList yourself.
List Applicative Exercise
Implement the list Applicative . Writing a minimally complete
Applicative instance calls for writing the definitions of both
pureand&lt;<em>&gt;. We’re going to provide a hint as well. Use the
checkers library to validate your Applicative instance.
dataLista=
Nil
|Consa (Lista)
deriving (Eq,Show)
Remember what you wrote for the list Functor :
instance Functor Listwhere
fmap=undefined
Writing the list Applicative is similar.
instance Applicative Listwhere
pure=undefined
(&lt;</em>&gt;)=undefined
Expected result:</p>
<p>CHAPTER 17. APPLICATIVE 1126
Prelude&gt; let f = Cons (+1) (Cons (<em>2) Nil)
Prelude&gt; let v = Cons 1 (Cons 2 Nil)
Prelude&gt; f &lt;</em>&gt; v
Cons 2 (Cons 3 (Cons 2 (Cons 4 Nil)))
In case you get stuck, use the following functions and hints.
append::Lista-&gt;Lista-&gt;Lista
appendNilys=ys
append(Consx xs) ys =
Consx$xs <code>append</code> ys
fold::(a-&gt;b-&gt;b)-&gt;b-&gt;Lista-&gt;b
fold_bNil =b
foldf b (Consh t)=f h (fold f b t)
concat' ::List(Lista)-&gt;Lista
concat' =fold append Nil
-- write this one in terms
-- of concat' and fmap
flatMap ::(a-&gt;Listb)
-&gt;Lista
-&gt;Listb
flatMap f as=undefined</p>
<p>CHAPTER 17. APPLICATIVE 1127
Use the above and try using flatMap andfmapwithout explic-
itly pattern matching on cons cells. You’ll still need to handle
theNilcases.
flatMap is less strange than it would initially seem. It’s basi-
cally “fmap, then smush.”
Prelude&gt; fmap (\x -&gt; [x, 9]) [1, 2, 3]
[[1,9],[2,9],[3,9]]
Prelude&gt; let toMyList = foldr Cons Nil
Prelude&gt; let xs = toMyList [1, 2, 3]
Prelude&gt; let c = Cons
Prelude&gt; let f x = x <code>c</code> (9 <code>c</code> Nil)
Prelude&gt; flatMap f xs
Cons 1 (Cons 9 (Cons 2
(Cons 9 (Cons 3 (Cons 9 Nil)))))
Applicative instances, unlike Functor s, are not guaranteed to
have a unique implementation for a given datatype.
ZipList Applicative Exercise
Implement the ZipList Applicative . Use the checkers library to
validate your Applicative instance. We’re going to provide the
EqProp instance and explain the weirdness in a moment.</p>
<p>CHAPTER 17. APPLICATIVE 1128
dataLista=
Nil
|Consa (Lista)
deriving (Eq,Show)
take'::Int-&gt;Lista-&gt;Lista
take'=undefined
instance Functor Listwhere
fmap=undefined
instance Applicative Listwhere
pure=undefined
(&lt;*&gt;)=undefined
newtype ZipList' a=
ZipList' (Lista)
deriving (Eq,Show)
instance Eqa=&gt;EqProp(ZipList' a)where
xs=-=ys=xs' <code>eq</code> ys'
wherexs'= let(ZipList' l)=xs
intake'3000l
ys'= let(ZipList' l)=ys
intake'3000l</p>
<p>CHAPTER 17. APPLICATIVE 1129
instance Functor ZipList' where
fmap f ( ZipList' xs)=
ZipList' $fmap f xs
instance Applicative ZipList' where
pure=undefined
(&lt;<em>&gt;)=undefined
The idea is to align a list of functions with a list of values
and apply the first function to the first value and so on. The
instance should work with infinite lists. Some examples:
Prelude&gt; let zl' = ZipList'
Prelude&gt; let z = zl' [(+9), (<em>2), (+8)]
Prelude&gt; let z' = zl' [1..3]
Prelude&gt; z &lt;</em>&gt; z'
ZipList' [10,4,11]
Prelude&gt; let z' = zl' (repeat 1)
Prelude&gt; z &lt;</em>&gt; z'
ZipList' [10,2,9]
Note that the second z'was an infinite list. Check Prelude
for functions that can give you what you need. One starts
with the letter z, the other with the letter r. You’re looking
for inspiration from these functions, not to be able to directly
reusethemasyou’reusingacustom Listtype, nottheprovided
Prelude list type.</p>
<p>CHAPTER 17. APPLICATIVE 1130
Explaining and justifying the weird EqProp The good news is
it’sEqProp that has the weird “check only the first 3,000 values”
semantics instead of making the Eqinstance weird. The bad
news is this is a byproduct of testing for equality between infi-
nite lists…that is, you can’t. If you use a typical EqProp instance,
the test for homomorphism in your Applicative instance will
chase the infinite lists forever. Since QuickCheck is already an
exercise in “good enough” validity checking, we could choose
to feel justified in this. If you don’t believe us try running the
following in your REPL:
repeat 1 == repeat 1
Either and Validation Applicative
Yep, here we go again with the types:
Specializing the types
-- f ~ Either e
typeE=Either
(&lt;<em>&gt;)::f (a-&gt;b)-&gt;f a-&gt;f b
(&lt;</em>&gt;)::Ee (a-&gt;b)-&gt;Ee a-&gt;Ee b
pure::a-&gt;f a
pure::a-&gt;Ee a</p>
<p>CHAPTER 17. APPLICATIVE 1131
Either versus Validation
Often the interesting part of an Applicative is the monoid. One
byproduct of this is that just as you can have more than one
validMonoid for a given datatype, unlike Functor ,Applicative
can have more than one valid and lawful instance for a given
datatype.
The following is a brief demonstration of Either :
Prelude&gt; pure 1 :: Either e Int
Right 1
Prelude&gt; Right (+1) &lt;<em>&gt; Right 1
Right 2
Prelude&gt; Right (+1) &lt;</em>&gt; Left &quot;:(&quot;
Left &quot;:(&quot;
Prelude&gt; Left &quot;:(&quot; &lt;<em>&gt; Right 1
Left &quot;:(&quot;
Prelude&gt; Left &quot;:(&quot; &lt;</em>&gt; Left &quot;sadface.png&quot;
Left &quot;:(&quot;
We’vecoveredthebenefitsof Either alreadyandwe’veshown
you what the Maybe Applicative can clean up, so we won’t be-
labor those points. There’s an alternative to Either, called
Validation , that diﬀers only in the Applicative instance:</p>
<p>CHAPTER 17. APPLICATIVE 1132
dataValidation err a=
Failure err
|Success a
deriving (Eq,Show)
One thing to realize is that this is identical to theEither
datatype and there is even a pair of total functions which can
go between Validation andEither values interchangeably. Re-
member when we mentioned natural transformations? Both
of these functions are natural transformations:
validToEither ::Validation e a
-&gt;Eithere a
validToEither (Failure err)=Lefterr
validToEither (Success a)=Righta
eitherToValid ::Eithere a
-&gt;Validation e a
eitherToValid (Lefterr)=Failure err
eitherToValid (Righta)=Success a
eitherToValid .validToEither ==id
validToEither .eitherToValid ==id
Howdoes Validation diﬀer? Principallyinwhatthe Applicative
instance does with errors. Rather than just short-circuiting</p>
<p>CHAPTER 17. APPLICATIVE 1133
when it has two error values, it’ll use the Monoid typeclass to
combine them. Often this’ll just be a list or set of errors but
you can do whatever you want.
dataErrors=
DividedByZero
|StackOverflow
|MooglesChewedWires
deriving (Eq,Show)
success =Success (+1)
&lt;<em>&gt;Success 1
success ==Success 2
failure =Success (+1)
&lt;</em>&gt;Failure [StackOverflow ]
failure ==Failure [StackOverflow ]
failure' =Failure [StackOverflow ]
&lt;*&gt;Success (+1)
failure' ==Failure [StackOverflow ]</p>
<p>CHAPTER 17. APPLICATIVE 1134
failures =
Failure [MooglesChewedWires ]
&lt;*&gt;Failure [StackOverflow ]
failures ==
Failure [MooglesChewedWires
,StackOverflow ]
With the value failures , we see what distinguishes Either
andValidation : we can now preserve allfailures that occurred,
not just the first one.
Exercise: Variations on Either
Validation has the same representation as Either , but it can be
diﬀerent. The Functor will behave the same, but the Applicative
willbediﬀerent. Seeaboveforanideaofhow Validation should
behave. Use the checkers library.
dataValidation e a=
Failure e
|Success a
deriving (Eq,Show)</p>
<p>CHAPTER 17. APPLICATIVE 1135
-- same as Either
instance Functor (Validation e)where
fmap=undefined
-- This is different
instance Monoide=&gt;
Applicative (Validation e)where
pure=undefined
(&lt;<em>&gt;)=undefined
17.9 Chapter Exercises
Given a type that has an instance of Applicative , specialize the
types of the methods. Test your specialization in the REPL.
One way to do this is to bind aliases of the typeclass methods
to more concrete types that have the type we told you to fill
in.
1.-- Type
[]
-- Methods
pure::a-&gt; ?a
(&lt;</em>&gt;):: ?(a-&gt;b)-&gt; ?a-&gt; ?b</p>
<p>CHAPTER 17. APPLICATIVE 1136
2.-- Type
IO
-- Methods
pure::a-&gt; ?a
(&lt;<em>&gt;):: ?(a-&gt;b)-&gt; ?a-&gt; ?b
3.-- Type
(,) a
-- Methods
pure::a-&gt; ?a
(&lt;</em>&gt;):: ?(a-&gt;b)-&gt; ?a-&gt; ?b
4.-- Type
(-&gt;) e
-- Methods
pure::a-&gt; ?a
(&lt;*&gt;):: ?(a-&gt;b)-&gt; ?a-&gt; ?b
Write instances for the following datatypes. Confused?
Write out what the type should be. Use the checkers library
to validate the instances.
1.dataPaira=Paira aderiving Show</p>
<p>CHAPTER 17. APPLICATIVE 1137
2.This should look familiar.
dataTwoa b=Twoa b
3.dataThreea b c=Threea b c
4.dataThree'a b=Three'a b b
5.dataFoura b c d =Foura b c d
6.dataFour'a b=Four'a a a b
Combinations
Remember the vowels and stops exercise in the folds chapter?
Write the function to generate the possible combinations of
three input lists using liftA3 fromControl.Applicative .
importControl.Applicative (liftA3)
stops::String
stops=&quot;pbtdkg&quot;
vowels::String
vowels=&quot;aeiou&quot;
combos::[a]-&gt;[b]-&gt;[c]-&gt;[(a, b, c)]
combos=undefined</p>
<p>CHAPTER 17. APPLICATIVE 1138
17.10 Definitions
1.Applicative can be thought of characterizing monoidal
functors in Haskell. For a Haskeller’s purposes, it’s a way
to functorially apply a function which is embedded in
structure 𝑓of the same type as the value you’re mapping
it over.
fmap::(a-&gt;b)-&gt;f a-&gt;f b
(&lt;*&gt;)::f (a-&gt;b)-&gt;f a-&gt;f b
17.11 Follow-up resources
1.Tony Morris; Nick Partridge; Validation library
http://hackage.haskell.org/package/validation
2.Conor McBride; Ross Paterson; Applicative Programming
with Eﬀects
http://staff.city.ac.uk/~ross/papers/Applicative.html
3.Jeremy Gibbons; Bruno C. d. S. Oliveira; Essence of the
Iterator Pattern
4.Ross Paterson; Constructing Applicative Functors
http://staff.city.ac.uk/~ross/papers/Constructors.html</p>
<p>CHAPTER 17. APPLICATIVE 1139
5.Sam Lindley; Philip Wadler; Jeremy Yallop; Idioms are
oblivious, arrows are meticulous, monads are promiscu-
ous.
Note: Idiom means applicative functor and is a useful
search term for published work on applicative functors.</p>
<p>Chapter 18
Monad
There is nothing so
practical as a good theory
Phil Wadler, quoting Kurt
Lewin
1140</p>
<p>CHAPTER 18. MONAD 1141
18.1 Monad
Finally we come to one of the most talked about structures in
Haskell: the monad. Monads are not, strictly speaking, neces-
sary to Haskell. Although the current Haskell standard does
use monad for constructing and transforming IOactions, older
implementations of Haskell did not. Monads are powerful
and fun, but they do not define Haskell. Rather, monads are
defined in terms of Haskell.
Monads are applicative functors, but they have something
special about them that makes them diﬀerent from and more
powerful than either &lt;*&gt;orfmapalone. In this chapter, we
•defineMonad, its operations and laws;
•look at several examples of monads in practice;
•write the Monadinstances for various types;
•address some misinformation about monads.
18.2 Sorry — a monad is not a burrito
Well, then what the heck is a monad?1
1Section title with all due respect and gratitude to Mark Jason Dominus, whose
blog post, “Monads are like burritos” is a classic of its genre. http://blog.plover.com/prog/
burritos.html</p>
<p>CHAPTER 18. MONAD 1142
As we said above, a monad is an applicative functor with
some unique features that make it a bit more powerful than
either alone. A functor maps a function over some structure;
an applicative maps a function that is contained in some struc-
ture over some other structure and then combines the two
layers of structure like mappend . So you can think of monads
as another way of applying functions over structure, with a
couple of additional features. We’ll get to those features in a
moment. For now, let’s check out the typeclass definition and
core operations.
If you are using GHC 7.10 or newer, you’ll see an Applicative
constraint in the definition of Monad, as it should be:
classApplicative m=&gt;Monadmwhere
(&gt;&gt;=)::m a-&gt;(a-&gt;m b)-&gt;m b
(&gt;&gt;)::m a-&gt;m b-&gt;m b
return::a-&gt;m a
We’re going to explore this in some detail. Let’s start with
the typeclass constraint on 𝑚.
Applicative m
Older versions of GHC did not have Applicative as a superclass
ofMonad. Given that Monadis stronger than Applicative , and
Applicative is stronger than Functor , you can derive Applicative
andFunctor in terms of Monad, just as you can derive Functor in</p>
<p>CHAPTER 18. MONAD 1143
terms of Applicative . What does this mean? It means you can
writefmapusing monadic operations and it works:
fmapf xs=xs&gt;&gt;=return.f
Try it for yourself:
Prelude&gt; fmap (+1) [1..3]
[2,3,4]
Prelude&gt; [1..3] &gt;&gt;= return . (+1)
[2,3,4]
Thishappenstobealaw, notaconvenience. Functor ,Applicative ,
andMonadinstances over a given type should have the same
core behavior.
We’ll explore the relationship between these classes more
completely in a bit, but as part of understanding the typeclass
definition above, it’s important to understand this chain of
dependency:
Functor -&gt;Applicative -&gt;Monad
Whenever you’ve implemented an instance of Monadfor a
type you necessarily have an Applicative and aFunctor as well.</p>
<p>CHAPTER 18. MONAD 1144
Core operations
TheMonadtypeclass defines three core operations, although
you only need to define &gt;&gt;=for a minimally complete Monad
instance. Let’s look at all three:
(&gt;&gt;=)::m a-&gt;(a-&gt;m b)-&gt;m b
(&gt;&gt;)::m a-&gt;m b-&gt;m b
return::a-&gt;m a
We can dispense with the last of those, return: it’s just the
same as pure. All it does is take a value and return it inside
your structure, whether that structure is a list or JustorIO. We
talked about it a bit, and used it, back in the Modules chapter,
and we covered purein theApplicative chapter, so there isn’t
much else to say about it.
Thenextoperator, &gt;&gt;, doesn’thaveanofficialEnglish-language
name, but we like to call it Mr. Pointy. Some people do re-
fer to it as the sequencing operator, which we must admit is
more informative than Mr. Pointy. Mr. Pointy sequences
two actions while discarding any resulting value of the first
action. Applicative has a similar operator as well, although we
didn’t talk about it in that chapter. We will see examples of
this operator in the upcoming section on dosyntax.
Finally, the big bind! The&gt;&gt;=operator is called bindand is
— or, at least, comprises — what makes Monadspecial.</p>
<p>CHAPTER 18. MONAD 1145
The novel part of Monad
Conventionallywhenweusemonads, weusethebindfunction,
&gt;&gt;=. Sometimes we use it directly, sometimes indirectly via do
syntax. The question we should ask ourselves is, what’s unique
toMonad— at least from the point of view of types?
We already saw that it’s not return ; that’s another name for
purefromApplicative .
We also noted (and will see more clearly soon) that it also
isn’t&gt;&gt;which has a counterpart in Applicative .
And it also isn’t &gt;&gt;=, at least not in its entirety. The type of
&gt;&gt;=is visibly similar to that of fmapand&lt;<em>&gt;, which makes sense
since monads are applicative functors. For the sake of making
this maximally similar, we’re going to change the 𝑚ofMonad
to𝑓:
fmap::Functor f
=&gt;(a-&gt;b)-&gt;f a-&gt;f b
&lt;</em>&gt; :: Applicative f
=&gt;f (a-&gt;b)-&gt;f a-&gt;f b
&gt;&gt;= :: Monadf
=&gt;f a-&gt;(a-&gt;f b)-&gt;f b
OK, so bind is quite similar to &lt;*&gt;andfmapbut with the first
two arguments flipped. Still, the idea of mapping a function
over a value while bypassing its surrounding structure is not
unique to Monad.</p>
<p>CHAPTER 18. MONAD 1146
We can demonstrate this by fmapping a function of type (a
-&gt; m b) to make it more like &gt;&gt;=, and it will work. Nothing will
stop us. We will continue using the tilde to represent rough
equivalence between two things:
-- If b == f b
fmap::Functor f
=&gt;(a-&gt;f b)-&gt;f a-&gt;f (f b)
Let’s demonstrate this idea with list as our structure:
Prelude&gt; let andOne x = [x, 1]
Prelude&gt; andOne 10
[10,1]
Prelude&gt; :t fmap andOne [4, 5, 6]
fmap andOne [4, 5, 6] :: Num t =&gt; [[t]]
Prelude&gt; fmap andOne [4, 5, 6]
[[4,1],[5,1],[6,1]]
But, lo! We knew from our types that we’d end up with
anf (f b) — that is, an extra layer of structure, and now we
have a result of nested lists. What if we wanted Num a =&gt; [a]
instead of nested lists? We want a single layer of 𝑓structure,
but our mapped function has itself generated more structure !</p>
<p>CHAPTER 18. MONAD 1147
After mapping a function that generates additional monadic
structure in its return type, we want a way to discard one layer
of that structure.
So how do we accomplish that? Well, we saw how to do
what we want with lists very early on in this book:
Prelude&gt; concat $ fmap andOne [4, 5, 6]
[4,1,5,1,6,1]
The type of concat , fully generalized:
concat::Foldable t=&gt;t [a]-&gt;[a]
-- we can assert a less general type
-- for our purposes here
concat::[[a]]-&gt;[a]
Monad, in a sense, is a generalization of concat! The unique
part ofMonadis the following function:
importControl.Monad (join)
join::Monadm=&gt;m (m a) -&gt;m a
-- compare
concat:: [[a]] -&gt;[a]</p>
<p>CHAPTER 18. MONAD 1148
It’s somewhat novel that we can inject more structure via
our function application, where applicatives and fmaps have to
leave the structure untouched. Allowing the function itself to
alter the structure is something we’ve not seen in Functor and
Applicative , and we’ll explore the ramifications of that ability
more, especially when we start talking about the Maybemonad.
But we caninject more structure with a standard fmapif we
wish, as we saw above. However, the ability to flatten those two
layers of structure into one is what makes Monadspecial. And
it’s by putting that joinfunction together with the mapping
function that we get bind, also known as &gt;&gt;=.
So how do we get bind?
The answer is the exercise Writebindin terms of fmapand
join.
Fear is the mind-killer, friend. You can do it.
-- keep in mind this is (&gt;&gt;=) flipped
bind::Monadm=&gt;(a-&gt;m b)-&gt;m a-&gt;m b
bind=undefined
WhatMonadis not
SinceMonadis somewhat abstract and a little slippery, many
people talk about it from one or two perspectives that they feel
most comfortable with. Quite often, they address what Monad</p>
<p>CHAPTER 18. MONAD 1149
is from the perspective of the IO Monad .IOdoes have a Monad
instance, and it is a very common use of monads. However,
understanding monads only through that instance leads to
limited intuitions for what monads are and can do, and to a
lesser extent, a wrong notion of what IOis all about.
A monad is not:
1.Impure. Monadic functions are pure functions. IOis an ab-
stract datatype that allows for impure, or eﬀectful, actions,
and it has a Monadinstance. But there’s nothing impure
about monads.
2.An embedded language for imperative programming. Si-
mon Peyton-Jones, one of the lead developers and re-
searchers of Haskell and its implementation in GHC, has
famously said, “Haskell is the world’s finest imperative
programming language,” and he was talking about the
way monads handle eﬀectful programming. While mon-
ads are often used for sequencing actions in a way that
looks like imperative programming, there are commuta-
tive monads that do not order actions. We’ll see one a few
chapters down the line when we talk about Reader .
3.A value. The typeclass describes a specific relationship be-
tween elements in a domain and defines some operations
over them. When we refer to something as “a monad,”</p>
<p>CHAPTER 18. MONAD 1150
we’re using that the same way we talk about “a monoid,”
or “a functor.” None of those are values.
4.About strictness. The monadic operations of bindand
return are nonstrict. Some operations can be made strict
within a specific instance. We’ll talk more about this later
in the book.
Using monads also doesn’t require knowing math. Or cate-
gory theory. It does not require mystical trips to the tops of
mountains or starving oneself in a desert somewhere.
TheMonadtypeclass is generalized structure manipulation
withsomelawstomakeitsensible. Justlike Functor andApplicative .
We sort of hate to diminish the mystique, but that’s all there is
to it.
Monadalso lifts!
TheMonadclass also includes a set of liftfunctions that are the
same as the ones we already saw in Applicative . They don’t
do anything diﬀerent, but they are still around because some
libraries used them before applicatives were discovered, so
theliftMset of functions still exists to maintain compatibil-
ity. So, you may still see them sometimes. We’ll take a short
tour of them, comparing them directly to their applicative
counterparts:</p>
<p>CHAPTER 18. MONAD 1151
liftA::Applicative f
=&gt;(a-&gt;b)-&gt;f a-&gt;f b
liftM::Monadm
=&gt;(a1-&gt;r)-&gt;m a1-&gt;m r
As you may recall, that is fmapwith a diﬀerent typeclass
constraint. If you’d like to see examples of how it works, we
encourage you to write fmapfunctions in your REPL and take
turns replacing the fmapwithliftAorliftM.
But that’s not all we have:
liftA2::Applicative f
=&gt;(a-&gt;b-&gt;c)
-&gt;f a
-&gt;f b
-&gt;f c
liftM2::Monadm
=&gt;(a1-&gt;a2-&gt;r)
-&gt;m a1
-&gt;m a2
-&gt;m r
Aside from the numbering these appear the same. Let’s try
them out:
Prelude&gt; liftA2 (,) (Just 3) (Just 5)</p>
<p>CHAPTER 18. MONAD 1152
Just (3,5)
Prelude&gt; liftM2 (,) (Just 3) (Just 5)
Just (3,5)
You may remember way back in Lists, we talked about a
function called zipWith .zipWith isliftA2 orliftM2 specialized
to lists:
Prelude&gt; :t zipWith
zipWith :: (a -&gt; b -&gt; c)
-&gt; [a] -&gt; [b] -&gt; [c]
Prelude&gt; zipWith (+) [3, 4] [5, 6]
[8,10]
Prelude&gt; liftA2 (+) [3, 4] [5, 6]
[8,9,9,10]
Well, the types are the same, but the behavior diﬀers. The
diﬀering behavior has to do with which list monoid is being
used.
All right. Then we have the threes:</p>
<p>CHAPTER 18. MONAD 1153
liftA3::Applicative f
=&gt;(a-&gt;b-&gt;c-&gt;d)
-&gt;f a-&gt;f b
-&gt;f c-&gt;f d
liftM3::Monadm
=&gt;(a1-&gt;a2-&gt;a3-&gt;r)
-&gt;m a1-&gt;m a2
-&gt;m a3-&gt;m r
And, coincidentally, there is also a zipWith3 function. Let’s
see what happens:
Prelude&gt; :t zipWith3
zipWith3 :: (a -&gt; b -&gt; c -&gt; d) -&gt;
[a] -&gt; [b] -&gt; [c] -&gt; [d]
Prelude&gt; liftM3 (,,) [1, 2] [3] [5, 6]
[(1,3,5),(1,3,6),(2,3,5),(2,3,6)]
Prelude&gt; zipWith3 (,,) [1, 2] [3] [5, 6]
[(1,3,5)]
Again, using a diﬀerent monoid gives us a diﬀerent set of
results.
We wanted to introduce these functions here because they
will come up in some later examples in the chapter, but they
aren’t especially pertinent to Monad, and we saw the gist of them</p>
<p>CHAPTER 18. MONAD 1154
in the previous chapter. So, let’s turn our attention back to
monads, shall we?
18.3 Do syntax and monads
We introduced dosyntax in the Modules chapter. We were
using it within the context of IOas syntactic sugar that allowed
us to easily sequence actions by feeding the result of one action</p>
<div style="break-before: page; page-break-before: always;"></div><p>as the input value to the next. While dosyntax works with
any monad — not just IO— it is most commonly seen when
usingIO. This section is going to talk about why dois sugar and
demonstrate what the joinofMonadcan do for us. We will be
using the IO Monad to demonstrate here, but later on we’ll see
some examples of dosyntax without IO.
To begin, let’s look at some correspondences:
(<em>&gt;) :: Applicative f =&gt; f a -&gt; f b -&gt; f b
(&gt;&gt;) :: Monad m =&gt; m a -&gt; m b -&gt; m b
For our purposes, (</em>&gt;)and(&gt;&gt;)are the same thing: sequenc-
ing functions, but with two diﬀerent constraints. They should
in all cases do the same thing:
Prelude&gt; putStrLn &quot;Hello, &quot; &gt;&gt; putStrLn &quot;World!&quot;
Hello,
World!</p>
<p>CHAPTER 18. MONAD 1155
Prelude&gt; putStrLn &quot;Hello, &quot; <em>&gt; putStrLn &quot;World!&quot;
Hello,
World!
Not observably diﬀerent. Good enough for government
work!
We can see what dosyntax looks like after the compiler
desugars it for us by manually transforming it ourselves:
importControl.Applicative ((</em>&gt;))
sequencing ::IO()
sequencing = do
putStrLn &quot;blah&quot;
putStrLn &quot;another thing&quot;
sequencing' ::IO()
sequencing' =
putStrLn &quot;blah&quot;&gt;&gt;
putStrLn &quot;another thing&quot;
sequencing'' ::IO()
sequencing'' =
putStrLn &quot;blah&quot;*&gt;
putStrLn &quot;another thing&quot;</p>
<p>CHAPTER 18. MONAD 1156
You should have had the same results for each of the above.
We can do the same with the variable binding that dosyntax
includes:
binding ::IO()
binding = do
name&lt;-getLine
putStrLn name
binding' ::IO()
binding' =
getLine &gt;&gt;=putStrLn
Instead of naming the variable and passing that as an argu-
ment to the next function, we use &gt;&gt;=which passes it directly.
Whenfmapalone isn’t enough
Note that if you try to fmap putStrLn overgetLine , it won’t do
anything. Try typing this into your REPL:
Prelude&gt; putStrLn &lt;$&gt; getLine
You’ve used getLine , so when you hit enter it should await
your input. Type something in, hit enter again and see what
happens.</p>
<p>CHAPTER 18. MONAD 1157
Whatever input you gave it didn’t print, although it seems
like it should have due to the putStrLn being mapped over the
getLine . We evaluated the IOaction that requests input, but not
the one that prints it. So, what happened?
Well, let’s start with the types. The type of what you tried
to do is this:
Prelude&gt; :t putStrLn &lt;$&gt; getLine
putStrLn &lt;$&gt; getLine :: IO (IO ())
We’re going to break it down a little bit so that we’ll under-
stand why this didn’t work. First, getLine performs I/O to get
aString :
getLine ::IOString
AndputStrLn takes a String argument, performs I/O, and
returns nothing interesting — parents of children with an
allowance can sympathize:
putStrLn ::String-&gt;IO()
What is the type of fmapas it concerns putStrLn andgetLine ?</p>
<p>CHAPTER 18. MONAD 1158
-- The type we start with
&lt;$&gt; ::Functor f=&gt;(a-&gt;b)-&gt;f a-&gt;f b
-- Our (a -&gt; b) is putStrLn
(a -&gt;b )
putStrLn ::String-&gt;IO()
That𝑏gets specialized to the type IO (), which is going to
jam another IOactioninside of the I/O that getLine performs.
Perhaps this looks familiar from our demonstration of what
happens when you use fmapto map a function with type (a -&gt;
m b)instead of (a -&gt; b) — that is what’s happening here. This
is what is happening with our types:
f::Functor f=&gt;fString-&gt;f (IO())
fx=putStrLn &lt;$&gt;x
g::(String-&gt;b)-&gt;IOb
gx=x&lt;$&gt;getLine
putStrLn &lt;$&gt;getLine ::IO(IO())
Okay...so, which IOis which, and why does it ask for input
but not print what we typed in?</p>
<p>CHAPTER 18. MONAD 1159
-- [1] [2] [3]
h::IO(IO())
h=putStrLn &lt;$&gt;getLine
1.This outermost IOstructure represents the eﬀects getLine
must perform to get you a String that the user typed in.
2.This inner IOstructure represents the eﬀects that would
be performed ifputStrLn was evaluated.
3.The unit here is the unit that putStrLn returns.
One of the strengths of Haskell is that we can refer to, com-
pose, and map over eﬀectful computations without perform-
ing them or bending over backwards to make that pattern
work. For a simpler example of how we can wait to evalu-
ateIOactions (or any computation in general), consider the
following:
Prelude&gt; let printOne = putStrLn &quot;1&quot;
Prelude&gt; let printTwo = putStrLn &quot;2&quot;
Prelude&gt; let twoActions = (printOne, printTwo)
Prelude&gt; :t twoActions
twoActions :: (IO (), IO ())
With that tuple of two IOactions defined, we can now grab
one and evaluate it:</p>
<p>CHAPTER 18. MONAD 1160
Prelude&gt; fst twoActions
1
Prelude&gt; snd twoActions
2
Prelude&gt; fst twoActions
1
Note that we are able to evaluate IOactions multiple times.
This will be significant later.
Back to our conundrum of why we can’t fmap putStrLn over
getLine . Perhaps you’ve already figured out what we need to do.
We need to join those two IOlayers together. To get what we
want, we need the unique thing that Monadoﬀers:join. Watch
it work:
Prelude&gt; import Control.Monad (join)
Prelude&gt; join $ putStrLn &lt;$&gt; getLine
blah
blah
Prelude&gt; :t join $ putStrLn &lt;$&gt; getLine
join $ putStrLn &lt;$&gt; getLine :: IO ()
Whatjoindidhereis mergetheeﬀectsof getLine andputStrLn
into a single IOaction. This merged IOaction performs the
eﬀects in the order determined by the nesting of the IOactions.
As it happens, the cleanest way to express ordering in a lambda</p>
<p>CHAPTER 18. MONAD 1161
calculus without bolting on something unpleasant is through
nesting of expressions or lambdas.
That’s right. We still haven’t left the lambda calculus behind.
Monadic sequencing and dosyntax seem on the surface to
be very far removed from that. But they aren’t. As we said,
monadic actions are still pure, and the sequencing operations
we use here are ways of nesting lambdas. Now, IOis a bit dif-
ferent, as it does allow for side eﬀects, but since those eﬀects
are constrained within the IOtype, all the rest of it is still a
pure lambda calculus.
Sometimes it is valuable to suspend or otherwise not per-
form an I/O action until some determination is made, so types
likeIO (IO ()) aren’t necessarily invalid, but you should be
aware of what’s needed to make this example work.
Let’sgetbacktodesugaring dosyntaxwithournow-enriched
understanding of what monads do for us:</p>
<p>CHAPTER 18. MONAD 1162
bindingAndSequencing ::IO()
bindingAndSequencing = do
putStrLn &quot;name pls:&quot;
name&lt;-getLine
putStrLn ( &quot;y helo thar: &quot; ++name)
bindingAndSequencing' ::IO()
bindingAndSequencing' =
putStrLn &quot;name pls:&quot; &gt;&gt;
getLine &gt;&gt;=
\name-&gt;
putStrLn ( &quot;y helo thar: &quot; ++name)
As the nesting intensifies, you can see how dosyntax can
make things a bit cleaner and easier to read:
twoBinds ::IO()
twoBinds = do
putStrLn &quot;name pls:&quot;
name&lt;-getLine
putStrLn &quot;age pls:&quot;
age&lt;-getLine
putStrLn ( &quot;y helo thar: &quot;
++name++&quot; who is: &quot;
++age++&quot; years old.&quot; )</p>
<p>CHAPTER 18. MONAD 1163
twoBinds' ::IO()
twoBinds' =
putStrLn &quot;name pls:&quot; &gt;&gt;
getLine &gt;&gt;=
\name-&gt;
putStrLn &quot;age pls:&quot; &gt;&gt;
getLine &gt;&gt;=
\age-&gt;
putStrLn ( &quot;y helo thar: &quot;
++name++&quot; who is: &quot;
++age++&quot; years old.&quot; )
18.4 Examples of Monaduse
All right, we’ve seen what is diﬀerent about Monadand seen a
small demonstration of what that does for us. What we need
now is to see how monads work in code, with Monads other than
IO.
List
We’ve been starting oﬀ our examples of these typeclasses in
use with list examples because they can be quite easy to see
and understand. We will keep this section brief, though, as we
have more exciting things to show you.</p>
<p>CHAPTER 18. MONAD 1164
Specializing the types
This process should be familiar to you by now:
(&gt;&gt;=)::Monadm
=&gt;m a-&gt;(a-&gt;m b)-&gt;m b
(&gt;&gt;=)::[ ] a-&gt;(a-&gt;[ ] b)-&gt;[ ] b
-- or more syntactically common
(&gt;&gt;=)::[a]-&gt;(a-&gt;[b])-&gt;[b]
-- same as pure
return::Monadm=&gt;a-&gt;m a
return:: a-&gt;[ ] a
return:: a-&gt;[a]
Excellent. It’s like fmapexcept the order of arguments is
flipped and we can now generate more list (or an empty list)
inside of our mapped function. Let’s take it for a spin.
Example of the List Monadin use
Let’s start with a function and identify how the parts fit with
our monadic types:</p>
<p>CHAPTER 18. MONAD 1165
twiceWhenEven ::[Integer]-&gt;[Integer]
twiceWhenEven xs= do
x&lt;-xs
ifeven x
then[x<em>x, x</em>x]
else[x<em>x]
Thex &lt;- xs line binds individual values out of the list input,
like a list comprehension, giving us an 𝑎. Theif-then-else is
oura -&gt; m b . It takes the individual 𝑎values that have been
bound out of our m aand can generate more values, thereby
increasing the size of the list.
Them athat is our first input will be the argument we pass
to it below:
Prelude&gt; twiceWhenEven [1..3]
[1,4,4,9]
Now try this:
twiceWhenEven ::[Integer]-&gt;[Integer]
twiceWhenEven xs= do
x&lt;-xs
ifeven x
then[x</em>x, x*x]
else[]</p>
<p>CHAPTER 18. MONAD 1166
And try giving it the same input as above (for easy compar-
ison). Was the result what you expected? Keep playing around
with this, forming hypotheses about what will happen and
why and testing them in the REPL to develop an intuition for
how monads are working on a simple example. The examples
in the next sections are longer and more complex.
Maybe Monad
Now we come to a more exciting demonstration of what we
can do with our newfound power.
Specializing the types
It is the season for examining the types:
-- type M = Maybe
-- m ~ Maybe
(&gt;&gt;=)::Monadm
=&gt;m a-&gt;(a-&gt;m b)-&gt;m b
(&gt;&gt;=)::
Maybea-&gt;(a-&gt;Maybeb)-&gt;Maybeb
-- same as pure
return::Monadm=&gt;a-&gt;m a
return:: a-&gt;Maybea</p>
<p>CHAPTER 18. MONAD 1167
There should have been nothing surprising there, so let’s
get to the meat of the matter.
Using the Maybe Monad
This example looks like the one from the Applicative chapter,
but it’s diﬀerent. We encourage you to compare the two, al-
though we’ve been explicit about what exactly is happening
here. You developed some intutions above for dosyntax and
the listMonad; here we’ll be quite explicit about what’s happen-
ing, and by the time we get to the Either demonstration below,
it should be clear. Let’s get started:
dataCow=Cow{
name::String
, age ::Int
, weight ::Int
}deriving (Eq,Show)
noEmpty ::String-&gt;MaybeString
noEmpty &quot;&quot;=Nothing
noEmpty str=Juststr
noNegative ::Int-&gt;MaybeInt
noNegative n|n&gt;=0=Justn
|otherwise =Nothing</p>
<p>CHAPTER 18. MONAD 1168
-- if Cow's name is Bess, must be under 500
weightCheck ::Cow-&gt;MaybeCow
weightCheck c=
letw=weight c
n=name c
in ifn==&quot;Bess&quot;&amp;&amp;w&gt;499
thenNothing
elseJustc
mkSphericalCow ::String
-&gt;Int
-&gt;Int
-&gt;MaybeCow
mkSphericalCow name' age' weight' =
casenoEmpty name' of
Nothing -&gt;Nothing
Justnammy-&gt;
casenoNegative age' of
Nothing -&gt;Nothing
Justagey-&gt;
casenoNegative weight' of
Nothing -&gt;Nothing
Justweighty -&gt;
weightCheck
(Cownammy agey weighty)</p>
<p>CHAPTER 18. MONAD 1169
Prelude&gt; mkSphericalCow &quot;Bess&quot; 5 499
Just (Cow {name = &quot;Bess&quot;, age = 5, weight = 499})
Prelude&gt; mkSphericalCow &quot;Bess&quot; 5 500
Nothing
First, we’ll clean it up with dosyntax, then we’ll see why we
can’t do this with Applicative :
-- Do syntax isn't just for IO.
mkSphericalCow' ::String
-&gt;Int
-&gt;Int
-&gt;MaybeCow
mkSphericalCow' name' age' weight' = do
nammy&lt;-noEmpty name'
agey&lt;-noNegative age'
weighty &lt;-noNegative weight'
weightCheck ( Cownammy agey weighty)
And this works as expected.
Prelude&gt; mkSphericalCow' &quot;Bess&quot; 5 500
Nothing
Prelude&gt; mkSphericalCow' &quot;Bess&quot; 5 499
Just (Cow {name = &quot;Bess&quot;, age = 5, weight = 499})</p>
<p>CHAPTER 18. MONAD 1170
Can we write it with (&gt;&gt;=)? Sure!
-- Stack up the nested lambdas.
mkSphericalCow'' ::String
-&gt;Int
-&gt;Int
-&gt;MaybeCow
mkSphericalCow'' name' age' weight' =
noEmpty name' &gt;&gt;=
\nammy-&gt;
noNegative age' &gt;&gt;=
\agey-&gt;
noNegative weight' &gt;&gt;=
\weighty -&gt;
weightCheck ( Cownammy agey weighty)
So why can’t we do this with Applicative ? Because our
weightCheck function depends on the prior existence of a Cow
value and returns more monadic structure in its return type
Maybe Cow .
If your dosyntax looks like this:</p>
<p>CHAPTER 18. MONAD 1171
doSomething = do
a&lt;-f
b&lt;-g
c&lt;-h
pure (a, b, c)
You can rewrite it using Applicative . On the other hand, if
you have something like this:
doSomething' n= do
a&lt;-f n
b&lt;-g a
c&lt;-h b
pure (a, b, c)
You’re going to need Monadbecause 𝑔andℎare producing
monadic structure based on values that can only be obtained
by depending on values generated from monadic structure.
You’ll need jointo crunch the nesting of monadic structure
back down. If you don’t believe us, try translating doSomething'
toApplicative : so no resorting to &gt;&gt;=orjoin.
Here’s some code to kick that around:
f::Integer -&gt;MaybeInteger
f0=Nothing
fn=Justn</p>
<p>CHAPTER 18. MONAD 1172
g::Integer -&gt;MaybeInteger
gi=
ifeven i
thenJust(i+1)
elseNothing
h::Integer -&gt;MaybeString
hi=Just(&quot;10191&quot; ++show i)
doSomething' n= do
a&lt;-f n
b&lt;-g a
c&lt;-h b
pure (a, b, c)
The long and short of it:
1.With the Maybe Applicative , eachMaybecomputation fails
or succeeds independently of each other. You’re lifting
functions that are also JustorNothing overMaybevalues.
2.With the Maybe Monad , computations contributing to the
final result can choose to return Nothing based on previous
computations.</p>
<p>CHAPTER 18. MONAD 1173
Exploding a spherical cow
We said we’d be quite explicit about what’s happening in the
above, so let’s do this thing. Let’s get in the guts of this code
and how binding over Maybevalues works.
For once, this example instance is what’s in GHC’s base
library at time of writing:
instance MonadMaybewhere
return x =Justx
(Justx)&gt;&gt;=k=k x
Nothing &gt;&gt;= _ = Nothing
mkSphericalCow'' ::String
-&gt;Int
-&gt;Int
-&gt;MaybeCow
mkSphericalCow'' name' age' weight' =
noEmpty name' &gt;&gt;=
\nammy-&gt;
noNegative age' &gt;&gt;=
\agey-&gt;
noNegative weight' &gt;&gt;=
\weighty -&gt;
weightCheck ( Cownammy agey weighty)</p>
<p>CHAPTER 18. MONAD 1174
And what happens if we pass it some arguments?
-- Proceeding outermost to innermost.
mkSphericalCow'' &quot;Bess&quot;5499=
noEmpty &quot;Bess&quot;&gt;&gt;=
\nammy-&gt;
noNegative 5&gt;&gt;=
\agey-&gt;
noNegative 499&gt;&gt;=
\weighty -&gt;
weightCheck ( Cownammy agey weighty)
-- &quot;Bess&quot; /= &quot;&quot;, so skipping this pattern
-- noEmpty &quot;&quot; = Nothing
noEmpty &quot;Bess&quot;=Just&quot;Bess&quot;
So we produced the value Just &quot;Bess&quot; ; however, nammywill
be theString and not also the Maybestructure because &gt;&gt;=passes
𝑎to the function it binds over the monadic value, not 𝑚𝑎. Here
we’ll use the Maybe Monad instance to examine why:</p>
<p>CHAPTER 18. MONAD 1175
instance MonadMaybewhere
return x =Justx
(Justx)&gt;&gt;=k=k x
Nothing &gt;&gt;= _ = Nothing
noEmpty &quot;Bess&quot;&gt;&gt;=\nammy-&gt;
(restofthe computation)
-- noEmpty &quot;Bess&quot; evaluated
-- to Just &quot;Bess&quot;. So the first
-- Just case matches.
(Just&quot;Bess&quot;)&gt;&gt;=\nammy-&gt; ...
(Justx)&gt;&gt;=k=k x
-- k is \nammy et al.
-- x is &quot;Bess&quot; by itself.
Sonammyis bound to ”Bess”, and the following is the whole 𝑘:
&quot;Bess&quot;-&gt;
noNegative 5&gt;&gt;=
\agey-&gt;
noNegative 499&gt;&gt;=
\weighty -&gt;
weightCheck ( Cownammy agey weighty)</p>
<p>CHAPTER 18. MONAD 1176
Then how does the age check go?
mkSphericalCow'' &quot;Bess&quot;5499=
noEmpty &quot;Bess&quot;&gt;&gt;=
&quot;Bess&quot;-&gt;
noNegative 5&gt;&gt;=
\agey-&gt;
noNegative 499&gt;&gt;=
\weighty -&gt;
weightCheck ( Cow&quot;Bess&quot;agey weighty)
-- 5 &gt;= 0 is true, so we get Just 5
noNegative 5|5&gt;=0=Just5
|otherwise =Nothing
Again, although noNegative returns Just 5 , thebindfunction
will pass 5 on:</p>
<p>CHAPTER 18. MONAD 1177
mkSphericalCow'' &quot;Bess&quot;5499=
noEmpty &quot;Bess&quot;&gt;&gt;=
&quot;Bess&quot;-&gt;
noNegative 5&gt;&gt;=
\5-&gt;
noNegative 499&gt;&gt;=
\weighty -&gt;
weightCheck ( Cow&quot;Bess&quot;5weighty)
-- 499 &gt;= 0 is true, so we get Just 499
noNegative 499|499&gt;=0=Just499
|otherwise =Nothing
Passing 499 on:
mkSphericalCow'' &quot;Bess&quot;5499=
noEmpty &quot;Bess&quot;&gt;&gt;=
&quot;Bess&quot;-&gt;
noNegative 5&gt;&gt;=
\5-&gt;
noNegative 499&gt;&gt;=
\499-&gt;
weightCheck ( Cow&quot;Bess&quot;5499)</p>
<p>CHAPTER 18. MONAD 1178
weightCheck (Cow&quot;Bess&quot;5499)=
let499=weight ( Cow&quot;Bess&quot;5499)
&quot;Bess&quot;=name (Cow&quot;Bess&quot;5499)
-- fyi, 499 &gt; 499 is False.
in if&quot;Bess&quot;==&quot;Bess&quot;&amp;&amp;499&gt;499
thenNothing
elseJust(Cow&quot;Bess&quot;5499)
So in the end, we return Just (Cow &quot;Bess&quot; 5 499) .
Fail fast, like an overfunded startup
But what if we had failed? We’ll dissect the following compu-
tation:
Prelude&gt; mkSphericalCow'' &quot;&quot; 5 499
Nothing
And how do the guts fall when we explode this poor bovine?</p>
<p>CHAPTER 18. MONAD 1179
mkSphericalCow'' &quot;&quot;5499=
noEmpty &quot;&quot;&gt;&gt;=
\nammy-&gt;
noNegative 5&gt;&gt;=
\agey-&gt;
noNegative 499&gt;&gt;=
\weighty -&gt;
weightCheck ( Cownammy agey weighty)
-- &quot;&quot; == &quot;&quot;, so we get the Nothing case
noEmpty &quot;&quot;=Nothing
-- noEmpty str = Just str
After we’ve evaluated noEmpty &quot;&quot; and gotten a Nothing value,
we use(&gt;&gt;=). How does that go?</p>
<p>CHAPTER 18. MONAD 1180
instance MonadMaybewhere
return x =Justx
(Justx)&gt;&gt;=k=k x
Nothing &gt;&gt;= _ = Nothing
-- noEmpty &quot;&quot; := Nothing
Nothing &gt;&gt;=
\nammy-&gt;
-- Just case doesn't match, so skip it.
-- (Just x) &gt;&gt;= k = k x
-- This is what we're doing.
Nothing &gt;&gt;= _ = Nothing
So it turns out that the bindfunction will drop the entire
rest of the computation on the floor the moment anyof the
functions participating in the Maybe Monad actions produce a
Nothing value:
mkSphericalCow'' &quot;&quot;5499=
Nothing &gt;&gt;=-- NOPE.
In fact, you can demonstrate to yourself that that stuﬀ never
gets used with bottom , but does with a Justvalue:</p>
<p>CHAPTER 18. MONAD 1181
Prelude&gt; Nothing &gt;&gt;= undefined
Nothing
Prelude&gt; Just 1 &gt;&gt;= undefined
*** Exception: Prelude.undefined
But why do we use the Maybe Applicative andMonad? Because
this:
mkSphericalCow' ::String
-&gt;Int
-&gt;Int
-&gt;MaybeCow
mkSphericalCow' name' age' weight' = do
nammy&lt;-noEmpty name'
agey&lt;-noNegative age'
weighty &lt;-noNegative weight'
weightCheck ( Cownammy agey weighty)
is a lot nicer than case matching the Nothing case over and
over just so we can say Nothing -&gt; Nothing a million times. Life
is too short for repetition when computers lovetaking care of
repetition.
Either
Whew. Let’s all be thankful that cow was full of Maybevalues
and not tripe. Moving along, we’re going to demonstrate use</p>
<p>CHAPTER 18. MONAD 1182
of theEither Monad , step back a bit, and let your intuitions and
what you learned about Maybeguide you through.
Specializing the types
As always, we present the types:
-- m ~ Either e
(&gt;&gt;=)::Monadm
=&gt; m a
-&gt;(a-&gt; m b)
-&gt; m b
(&gt;&gt;=)::Eithere a
-&gt;(a-&gt;Eithere b)
-&gt;Eithere b
-- same as pure
return::Monadm=&gt;a-&gt; m aq
return:: a-&gt;Eithere a
Why do we keep doing this? To remind you that the types
always show you the way, once you’ve figured them out.
Using the Either Monad
Use what you know to go carefully through this code and
follow the types. First, we define our datatypes:</p>
<p>CHAPTER 18. MONAD 1183
moduleEitherMonad where
-- years ago
typeFounded =Int
-- number of programmers
typeCoders=Int
dataSoftwareShop =
Shop{
founded ::Founded
, programmers ::Coders
}deriving (Eq,Show)
dataFoundedError =
NegativeYears Founded
|TooManyYears Founded
|NegativeCoders Coders
|TooManyCoders Coders
|TooManyCodersForYears Founded Coders
deriving (Eq,Show)
Let’s bring some functions now:</p>
<p>CHAPTER 18. MONAD 1184
validateFounded
::Int
-&gt;EitherFoundedError Founded
validateFounded n
|n&lt;0=Left$NegativeYears n
|n&gt;500=Left$TooManyYears n
|otherwise =Rightn
-- Tho, many programmers <em>are</em> negative.
validateCoders
::Int
-&gt;EitherFoundedError Coders
validateCoders n
|n&lt;0=Left$NegativeCoders n
|n&gt;5000=Left$TooManyCoders n
|otherwise =Rightn</p>
<p>CHAPTER 18. MONAD 1185
mkSoftware
::Int
-&gt;Int
-&gt;EitherFoundedError SoftwareShop
mkSoftware years coders = do
founded &lt;-validateFounded years
programmers &lt;-validateCoders coders
ifprogrammers &gt;div founded 10
thenLeft$
TooManyCodersForYears
founded programmers
elseRight$Shopfounded programmers
Note that Either always short-circuits on the firstthing to
have failed. It mustbecause in the Monad, later values can depend
on previous ones:
Prelude&gt; mkSoftware 0 0
Right (Shop {founded = 0, programmers = 0})
Prelude&gt; mkSoftware (-1) 0
Left (NegativeYears (-1))
Prelude&gt; mkSoftware (-1) (-1)
Left (NegativeYears (-1))
Prelude&gt; mkSoftware 0 (-1)</p>
<p>CHAPTER 18. MONAD 1186
Left (NegativeCoders (-1))
Prelude&gt; mkSoftware 500 0
Right (Shop {founded = 500, programmers = 0})
Prelude&gt; mkSoftware 501 0
Left (TooManyYears 501)
Prelude&gt; mkSoftware 501 501
Left (TooManyYears 501)
Prelude&gt; mkSoftware 100 5001
Left (TooManyCoders 5001)
Prelude&gt; mkSoftware 0 500
Left (TooManyCodersForYears 0 500)
So, there is no MonadforValidation .Applicative andMonadin-
stances must have the same behavior. This is usually expressed
in the form:
importControl.Monad (ap)
(&lt;*&gt;)==ap
This is a way of saying the Applicative apply for a type must
not change behavior if derived from the Monadinstance’s bind</p>
<p>CHAPTER 18. MONAD 1187
operation.
-- Keeping in mind
(&lt;<em>&gt;)::Applicative f
=&gt;f (a-&gt;b)-&gt;f a-&gt;f b
ap::Monadm
=&gt;m (a-&gt;b)-&gt;m a-&gt;m b
Then deriving Applicative (&lt;</em>&gt;) from the stronger instance:
ap::(Monadm)=&gt;m (a-&gt;b)-&gt;m a-&gt;m b
apm m'= do
x&lt;-m
x'&lt;-m'
return (x x')
The problem is you can’t make a MonadforValidation that
accumulates the errors like the Applicative does. Instead, any
Monadinstance for Validation would be identical to Either ’sMonad
instance.
Short Exercise: Either Monad
Implement the Either Monad .</p>
<p>CHAPTER 18. MONAD 1188
dataSuma b=
Firsta
|Secondb
deriving (Eq,Show)
instance Functor (Suma)where
fmap=undefined
instance Applicative (Suma)where
pure=undefined
(&lt;*&gt;)=undefined
instance Monad(Suma)where
return=pure
(&gt;&gt;=)=undefined
18.5 Monad laws
TheMonadtypeclass has laws, as the other typeclasses do. These
laws exist, as with all the other typeclass laws, to ensure that
your code does nothing surprising or harmful. If the Monad
instance you write for your type abides by these laws, then
your monads should work as you want them to. To write your
own instance, you only have to define a &gt;&gt;=operation, but you
want your binding to be as predictable as possible.</p>
<p>CHAPTER 18. MONAD 1189
Identity laws
Monadhas two identity laws:
-- right identity
m&gt;&gt;=return =m
-- left identity
returnx&gt;&gt;=f=f x
Basically both of these laws are saying that return should be
neutral and not perform any computation. We’ll line them up
with the type of &gt;&gt;=to clarify what’s happening:
(&gt;&gt;=)::Monadm
=&gt;m a-&gt;(a-&gt;m b)-&gt;m b
-- [1] [2] [3]
First, right identity:
return::a-&gt;m a
m&gt;&gt;=return =m
-- [1] [2] [3]
The𝑚does represent an m aandm b, respectively, so the
structure is there even if it’s not apparent from the way the
law is written.</p>
<p>CHAPTER 18. MONAD 1190
And left identity:
-- applying return to x gives us an
-- m a value to start
return x &gt;&gt;=f=f x
-- [1] [2] [3]
Likepure,return shouldn’t change any of the behavior of the
rest of the function; it is only there to put things into structure
when we need to, and the existence of the structure should
not aﬀect the computation.
Associativity
The law of associativity is not so diﬀerent from other laws of
associativity we have seen. It does look a bit diﬀerent because
of the nature of &gt;&gt;=:
(m&gt;&gt;=f)&gt;&gt;=g=m&gt;&gt;=(\x-&gt;f x&gt;&gt;=g)
Regrouping the functions should not have any impact on
the final result, same as the associativity of Monoid . The syntax
there, in which, for the right side of the equals sign, we had to
pass in an 𝑥argument might seem confusing at first. So, let’s
look at it more carefully.
This side looks the way we expect it to:</p>
<p>CHAPTER 18. MONAD 1191
(m&gt;&gt;=f)&gt;&gt;=g
But remember that (&gt;&gt;=)allows the result value of one func-
tion to be passed as input to the next, like function application
but with our value at the left and successive functions proceed-
ing to the right. Remember this code?
getLine &gt;&gt;=putStrLn
The IO action for getLine is evaluated first, then putStrLn is
passed the input string that resulted from running getLine ’s
eﬀects. This left-to-right is partly down to the history of IOin
Haskell — it’s so the “order” of the code reads top to bottom.
We’ll explain this more later in the book.
When we reassociate them, we need to apply 𝑓so that𝑔has
an input value of type m ato start the whole thing oﬀ. So, we
pass in the argument 𝑥via an anonymous function:
m&gt;&gt;=(\x-&gt;f x&gt;&gt;=g)
And bada bing, now nothing can slow this roll.
We’re doing that thing again
Out of mercy, we’ll be using checkers (not Nixon’s dog) again.
The argument the Monad TestBatch wants is identical to the
Applicative , a tuple of three value types embedded in the struc-
tural type.</p>
<p>CHAPTER 18. MONAD 1192
Prelude&gt; quickBatch (monad [(1, 2, 3)])
monad laws:
left identity: +++ OK, passed 500 tests.
right identity: +++ OK, passed 500 tests.
associativity: +++ OK, passed 500 tests.
Going forward we’ll be using this to validate Monadinstances.
Let’s write a bad Monadto see what it can catch for us.
BadMonads and their denizens
We’re going to write an invalid Monad(andFunctor ). You could
pretend it’s Identity with an integer thrown in which gets in-
cremented on each fmapor bind.
moduleBadMonad where
importTest.QuickCheck
importTest.QuickCheck.Checkers
importTest.QuickCheck.Classes</p>
<p>CHAPTER 18. MONAD 1193
dataCountMe a=
CountMe Integer a
deriving (Eq,Show)
instance Functor CountMe where
fmap f ( CountMe i a)=
CountMe (i+1) (f a)
instance Applicative CountMe where
pure=CountMe 0
CountMe n f&lt;*&gt;CountMe n' a=
CountMe (n+n') (f a)
instance MonadCountMe where
return=pure
CountMe n a&gt;&gt;=f=
letCountMe _b=f a
inCountMe (n+1) b</p>
<p>CHAPTER 18. MONAD 1194
instance Arbitrary a
=&gt;Arbitrary (CountMe a)where
arbitrary =
CountMe &lt;$&gt;arbitrary &lt;*&gt;arbitrary
instance Eqa=&gt;EqProp(CountMe a)where
(=-=)=eq
main= do
lettrigger ::CountMe (Int,String,Int)
trigger =undefined
quickBatch $functor trigger
quickBatch $applicative trigger
quickBatch $monad trigger
When we run the tests, the Functor andMonadwill fail top
to bottom. The Applicative technically only failed the laws
because Functor did; in the Applicative instance we were using
a proper monoid-of-structure.
Prelude&gt; main
functor:
identity: *** Failed! Falsifiable (after 1 test):
CountMe 0 0
compose: *** Failed! Falsifiable (after 1 test):</p>
<p>CHAPTER 18. MONAD 1195
<function>
<function>
CountMe 0 0
applicative:
identity: +++ OK, passed 500 tests.
composition: +++ OK, passed 500 tests.
homomorphism: +++ OK, passed 500 tests.
interchange: +++ OK, passed 500 tests.
functor: *** Failed! Falsifiable (after 1 test):
<function>
CountMe 0 0
monad laws:
left identity: *** Failed! Falsifiable (after 1 test):
<function>
0
right identity: *** Failed! Falsifiable (after 1 test):
CountMe 0 0
associativity: *** Failed! Falsifiable (after 1 test):
CountMe 0 0
We can reapply the weird, broken increment semantics and
get a broken Applicative as well.</p>
<p>CHAPTER 18. MONAD 1196
instance Applicative CountMe where
pure=CountMe 0
CountMe n f&lt;*&gt;CountMe _a=
CountMe (n+1) (f a)
Now it’s allbroken.
applicative:
identity:
*** Failed! Falsifiable (after 1 test):
CountMe 0 0
composition:
*** Failed! Falsifiable (after 1 test):
CountMe 0 <function>
CountMe 0 <function>
CountMe 0 0
homomorphism:
*** Failed! Falsifiable (after 1 test):
<function>
0
interchange:
*** Failed! Falsifiable (after 3 tests):
CountMe (-1) <function>
0
Understanding what makes sense structurally for a Functor ,
Applicative , andMonoid can tell you what is potentially an in-</p>
<p>CHAPTER 18. MONAD 1197
valid instance before you’ve written any code. Incidentally,
even if you fix the Functor andApplicative instances, the Monad
instance is not yet fixed.
instance Functor CountMe where
fmap f ( CountMe i a)=CountMe i (f a)
instance Applicative CountMe where
pure=CountMe 0
CountMe n f&lt;*&gt;CountMe n' a=
CountMe (n+n') (f a)
instance MonadCountMe where
return=pure
CountMe _a&gt;&gt;=f=f a
This’ll pass as a valid Functor andApplicative , but it’s not a
validMonad. The problem is that while puresetting the integer
value to zero is fine for the purposes of the Applicative , but it
violates the right identity law of Monad.
Prelude&gt; CountMe 2 &quot;blah&quot; &gt;&gt;= return
CountMe 0 &quot;blah&quot;
So ourpureis too opinionated. Still a valid Applicative and
Functor , but what if puredidn’t agree with the Monoid of the</p>
<p>CHAPTER 18. MONAD 1198
structure? The following will pass the Functor laws but it isn’t a
validApplicative .
instance Functor CountMe where
fmap f ( CountMe i a)=CountMe i (f a)
instance Applicative CountMe where
pure=CountMe 1
CountMe n f&lt;<em>&gt;CountMe n' a=
CountMe (n+n') (f a)
As it happens, if we change the monoid-of-structure to
match the identity such that we have addition and the number
zero, it’s a valid Applicative again.
instance Applicative CountMe where
pure=CountMe 0
CountMe n f&lt;</em>&gt;CountMe n' a=
CountMe (n+n') (f a)
As you gain experience with these structures, you’ll learn to
identify what might have a valid Applicative but no valid Monad
instance. But how do we fix the Monadinstance? By fixing the
underlying Monoid !</p>
<p>CHAPTER 18. MONAD 1199
instance MonadCountMe where
return=pure
CountMe n a&gt;&gt;=f=
letCountMe n' b=f a
inCountMe (n+n') b
Once our Monadinstance starts summing the counts like the
Applicative did, it works fine! It can be easy at times to acciden-
tally write an invalid Monadthat typechecks, so it’s important to
useQuickCheck to validate your Monoid ,Functor ,Applicative , and
Monadinstances.
18.6 Application and composition
What we’ve seen so far has been primarily about function
application. We probably weren’t thinking too much about the
relationship between function application and composition
because with Functor andApplicative it hadn’t mattered much.
Both concerned functions that looked like the usual (a -&gt; b)
arrangement, so composition “just worked” and that this was
true was guaranteed by the laws of those typeclasses:</p>
<p>CHAPTER 18. MONAD 1200
fmapid=id
-- guarantees
fmapf.fmap g=fmap (f .g)
Which means composition under functors just works:
Prelude&gt; fmap ((+1) . (+2)) [1..5]
[4,5,6,7,8]
Prelude&gt; fmap (+1) . fmap (+2) $ [1..5]
[4,5,6,7,8]
WithMonadthe situation seems less neat at first. Let’s attempt
to define composition for monadic functions in a simple way:
mcomp::Monadm=&gt;
(b-&gt;m c)
-&gt;(a-&gt;m b)
-&gt;a-&gt;m c
mcompf g a=f (g a)
If we try to load this, we’ll get an error like this:
Couldn't match expected type ‘b’
with actual type ‘m b’
‘b’ is a rigid type variable bound</p>
<p>CHAPTER 18. MONAD 1201
by the type signature for
mcomp :: Monad m =&gt;
(b -&gt; m c)
-&gt; (a -&gt; m b)
-&gt; a -&gt; m c
at kleisli.hs:21:9
Relevant bindings include
g :: a -&gt; m b (bound at kleisli.hs:22:8)
f :: b -&gt; m c (bound at kleisli.hs:22:6)
mcomp :: (b -&gt; m c)
-&gt; (a -&gt; m b)
-&gt; a -&gt; m c
(bound at kleisli.hs:22:1)
In the first argument of ‘f’, namely ‘(g a)’
In the expression: f (g a)
Failed, modules loaded: none.
Well, that didn’t work. That error message is telling us that
𝑓is expecting a 𝑏for its first argument, but 𝑔is passing an m b
to𝑓. So, how do we apply a function in the presence of some
context that we want to ignore? We use fmap. That’s going to
give us an m (m c) instead of an m c, so we’ll want to jointhose
two monadic structures.</p>
<p>CHAPTER 18. MONAD 1202
mcomp::Monadm=&gt;
(b-&gt;m c)
-&gt;(a-&gt;m b)
-&gt;a-&gt;m c
mcompf g a=join (f &lt;$&gt;(g a))
But using joinandfmaptogether means we can go ahead
and use (&gt;&gt;=).
mcomp'' ::Monadm=&gt;
(b-&gt;m c)
-&gt;(a-&gt;m b)
-&gt;a-&gt;m c
mcomp'' f g a=g a&gt;&gt;=f
You don’t need to write anything special to make monadic
functions compose (as long as the monadic contexts are the
sameMonad) because Haskell has it covered: what you want is
Kleisli composition . Don’t sweat the strange name; it’s not as
weird as it sounds. As we saw above, what we need is function
composition written in terms of &gt;&gt;=to allow us to deal with
the extra structure, and that’s what the Kleisli fish gives us.
Let’s remind ourselves of the types of ordinary function
composition and &gt;&gt;=:</p>
<p>CHAPTER 18. MONAD 1203
(.)::(b-&gt;c)-&gt;(a-&gt;b)-&gt;a-&gt;c
(&gt;&gt;=)::Monadm
=&gt;m a-&gt;(a-&gt;m b)-&gt;m b
To get Kleisli composition oﬀ the ground, we have to flip
some arguments around to make the types work:
importControl.Monad
-- the order is flipped to match &gt;&gt;=
(&gt;=&gt;)
::Monadm
=&gt;(a-&gt;m b)-&gt;(b-&gt;m c)-&gt;a-&gt;m c
See any similarities to something you know yet?
(&gt;=&gt;)
::Monadm
=&gt;(a-&gt;m b)-&gt;(b-&gt;m c)-&gt;a-&gt;m c
flip(.)
::(a-&gt;b)-&gt;(b-&gt;c)-&gt;a-&gt;c
It’s function composition with monadic structure hanging
oﬀ the functions we’re composing. Let’s see an example!</p>
<p>CHAPTER 18. MONAD 1204
importControl.Monad ((&gt;=&gt;))
sayHi::String-&gt;IOString
sayHigreeting = do
putStrLn greeting
getLine
readM::Reada=&gt;String-&gt;IOa
readM=return.read
getAge::String-&gt;IOInt
getAge=sayHi&gt;=&gt;readM
askForAge ::IOInt
askForAge =
getAge&quot;Hello! How old are you? &quot;
We used return composed with readto turn it into some-
thing that provides monadic structure after being bound over
the output of sayHi. We needed the Kleisli composition opera-
tor to stitch sayHiandreadMtogether:</p>
<p>CHAPTER 18. MONAD 1205
sayHi::String-&gt;IOString
readM::Reada=&gt;String-&gt;IOa
-- [1] [2] [3]
(a-&gt;m b)
String-&gt;IOString
-- [4] [5] [6]
-&gt;(b-&gt;m c)
String-&gt;IOa
-- [7] [8] [9]
-&gt;a-&gt;m c
String IOa
1.The first type is the type of the input to sayHi,String .
2.TheIOthatsayHiperforms in order to present a greeting
and receive input.
3.TheString input from the user that sayHireturns.
4.TheString thatreadMexpects as an argument and which
sayHiwill produce.
5.TheIO readM returns into. Note that return/pure produce
IOvalues which perform no I/O.</p>
<p>CHAPTER 18. MONAD 1206
6.TheIntthatreadMreturns.
7.The original, initial String inputsayHiexpects so it knows
how to greet the user and ask for their age.
8.The final combined IOaction which performs all eﬀects
necessary to produce the final result.
9.The value inside of the final IOaction; in this case, this is
theIntvalue that readMreturned.
18.7 Chapter Exercises
WriteMonadinstancesforthefollowingtypes. Usethe QuickCheck
properties we showed you to validate your instances.
1.Welcome to the Nope Monad , where nothing happens and
nobody cares.
dataNopea=
NopeDotJpg
-- We're serious. Write it anyway.
2.dataPhhhbbtttEither b a=
Lefta
|Rightb</p>
<p>CHAPTER 18. MONAD 1207
3.Write a Monadinstance for Identity .
newtype Identity a=Identity a
deriving (Eq,Ord,Show)
instance Functor Identity where
fmap=undefined
instance Applicative Identity where
pure=undefined
(&lt;*&gt;)=undefined
instance MonadIdentity where
return=pure
(&gt;&gt;=)=undefined
4.This one should be easier than the Applicative instance
was. Remember to use the Functor thatMonadrequires, then
see where the chips fall.
dataLista=
Nil
|Consa (Lista)
Write the following functions using the methods provided
byMonadandFunctor . Using stuﬀ like identity and composition
is fine, but it has to typecheck with types provided.</p>
<p>CHAPTER 18. MONAD 1208
1.j::Monadm=&gt;m (m a) -&gt;m a
Expecting the following behavior:
Prelude&gt; j [[1, 2], [], [3]]
[1,2,3]
Prelude&gt; j (Just (Just 1))
Just 1
Prelude&gt; j (Just Nothing)
Nothing
Prelude&gt; j Nothing
Nothing
2.l1::Monadm=&gt;(a-&gt;b)-&gt;m a-&gt;m b
3.l2::Monadm
=&gt;(a-&gt;b-&gt;c)-&gt;m a-&gt;m b-&gt;m c
4.a::Monadm=&gt;m a-&gt;m (a-&gt;b)-&gt;m b
5.You’ll need recursion for this one.
meh::Monadm
=&gt;[a]-&gt;(a-&gt;m b)-&gt;m [b]
6.Hint: reuse “meh”
flipType ::(Monadm)=&gt;[m a]-&gt;m [a]</p>
<p>CHAPTER 18. MONAD 1209
18.8 Definition
1.Monad is a typeclass reifying an abstraction that is com-
monly used in Haskell. Instead of an ordinary function of
type𝑎to𝑏, you’re functorially applying a function which
produces more structure itself and using jointo reduce
the nested structure that results.
fmap::(a-&gt;b)-&gt;f a-&gt;f b
(&lt;*&gt;)::f (a-&gt;b)-&gt;f a-&gt;f b
(=&lt;&lt;)::(a-&gt;f b)-&gt;f a-&gt;f b
2.Amonadic function is one which generates more structure
after having been lifted over monadic structure. Contrast
the function arguments to fmapand(&gt;&gt;=)in:
fmap::(a-&gt;b)-&gt;f a-&gt;f b
(&gt;&gt;=)::m a-&gt;(a-&gt;m b)-&gt;m b
The significant diﬀerence is that the result is m band re-
quiresjoining the result after lifting the function over 𝑚.
What does this mean? That depends on the Monadinstance.
The distinction can be seen with ordinary function com-
position and Kleisli composition as well:</p>
<p>CHAPTER 18. MONAD 1210
(.)
::(b-&gt;c)-&gt;(a-&gt;b)-&gt;a-&gt;c
(&gt;=&gt;)
::Monadm
=&gt;(a-&gt;m b)-&gt;(b-&gt;m c)-&gt;a-&gt;m c
3.bindis unfortunately a somewhat overloaded term. You
first saw it used early in the book with respect to binding
variables to values, such as with the following:
letx=2inx+2
Where 𝑥is a variable bound to 2. However, when we’re
talking about a Monadinstance typically bind will refer
to having used &gt;&gt;=to lift a monadic function over the
structure. The distinction being:
-- lifting (a -&gt; b) over f in f a
fmap::(a-&gt;b)-&gt;f a-&gt;f b
-- binding (a -&gt; m b) over m in m a
(&gt;&gt;=)::m a-&gt;(a-&gt;m b)-&gt;m b
You’ll sometimes see us talk about the use of the bind
do-notation &lt;-or(&gt;&gt;=)as “binding over.” When we do, we</p>
<p>CHAPTER 18. MONAD 1211
mean that we lifted a monadic function and we’ll even-
tuallyjoinor smush the structure back down when we’re
done monkeying around in the Monad.Don’tpanic if we’re a
little casual about describing the use of &lt;-as having bound
over/out some 𝑎out ofm a.
18.9 Follow-up resources
1.What a Monad is not
https://wiki.haskell.org/What_a_Monad_is_not
2.Gabriel Gonzalez; How to desugar Haskell code
3.Stephen Diehl; What I wish I knew when Learning Haskell
http://dev.stephendiehl.com/hask/#monads
4.Stephen Diehl; Monads Made Difficult
http://www.stephendiehl.com/posts/monads.html
5.Brent Yorgey; Typeclassopedia
https://wiki.haskell.org/Typeclassopedia</p>
<p>Chapter 19
Applying structure
I often repeat repeat
myself, I often repeat
repeat. I don’t don’t know
why know why, I simply
know that I I I am am
inclined to say to say a lot
a lot this way this way- I
often repeat repeat
myself, I often repeat
repeat.
Jack Prelutsky
1212</p>
<p>CHAPTER 19. MONADS GONE WILD 1213
19.1 Applied structure
We thought you’d like to see Monoid,Functor ,Applicative , and
Monadin the wild as it were. Since we’d like to finish this book
before we have grandchildren, this will notbe accompanied
by the painstaking explanations and exercise regime you’ve
experienced up to this point. Don’t understand something?
Figure it out! We’ll do our best to leave a trail of breadcrumbs
for you to follow up on the code we show you. Consider this a
breezy survey of how Haskellers write code when they think
no one is looking and a pleasant break from your regularly
scheduled exercises. The code demonstrated will not always
include all necessary context to make it run, so don’t expect
to be able to load the snippets in GHCi and have them work.
If you don’t have a lot of previous programming experience
and some of the applications are difficult for you to follow,
you might prefer to return to this chapter at a later time, once
you start trying to read and use Haskell libraries for practical
projects.
19.2Monoid
Monoids are everywhere once you recognize the pattern and
start looking for them, but we’ve tried to choose a few good
examples to illustrate typical use-cases.</p>
<p>CHAPTER 19. MONADS GONE WILD 1214
Templating content in Scotty
Here the scotty web framework’s “Hello, World” example uses
mconcat to inject the parameter “word” into the HTML page
returned:
{-# LANGUAGE OverloadedStrings #-}
importWeb.Scotty
importData.Monoid (mconcat)
main=scotty3000$ do
get&quot;/:word&quot; $ do
beam&lt;-param&quot;word&quot;
html
(mconcat
[&quot;<h1>Scotty, &quot;
, beam
,&quot; me up!</h1>&quot; ])
If you’re interested in following up on this example, you
can find this example and a tutorial on the scotty Github repos-
itory.</p>
<p>CHAPTER 19. MONADS GONE WILD 1215
Concatenating connection parameters
The next example is from Aditya Bhargava’s “Making A Web-
site With Haskell,” a blog post that walks you through several
steps for, well, making a simple website in Haskell. It also uses
thescotty web framework.
Here we’re using foldrandMonoid to concatenate connection
parameters for connecting to the database:
runDb::SqlPersist (ResourceT IO) a
-&gt;IOa
runDbquery= do
letconnStr =
foldr ((k,v) t -&gt;
t&lt;&gt;(encodeUtf8 $
k&lt;&gt;&quot;=&quot;&lt;&gt;v&lt;&gt;&quot; &quot;))
&quot;&quot;params
runResourceT
.withPostgresqlConn connStr
$runSqlConn query
If you’re interested in following up on this, this blog post
is one of many that shows you step by step how to use scotty ,
although many of them breeze through each step without a
great deal of explanation. It will be easier to understand scotty
in detail once you’ve worked through monad transformers, but</p>
<p>CHAPTER 19. MONADS GONE WILD 1216
if you’d like to start playing around with some basic projects,
you may want to try them out.
Concatenating key configurations
The next example is going to be a bit meatier than the two
previous ones.
xmonad is a windowing system for X11 written in Haskell.
The configuration language is Haskell — the binary that runs
your WM is compiled from your personal configuration. The
following is an example of using mappend to combine the default
configuration’s key mappings and a modification of those keys:</p>
<p>CHAPTER 19. MONADS GONE WILD 1217
importXMonad
importXMonad.Actions.Volume
importData.Map.Lazy (fromList )
importData.Monoid (mappend)
main= do
xmonad def { keys =
\c-&gt;fromList [
((0, xK_F6),
lowerVolume 4&gt;&gt;return()),
((0, xK_F7),
raiseVolume 4&gt;&gt;return())
] <code>mappend</code> keys defaultConfig c
}
The type of keysis a function:
keys:: !(XConfig Layout
-&gt;Map(ButtonMask ,KeySym) (X()))
You don’t need to get too excited about the exclamation
point right now; it’s the syntax for a nifty thing called a strictness
annotation , which makes a field in a product strict. That is, you
won’t be able to construct the record or product that contains
the value without also forcing that field to weak head normal
form. We’ll explain this in more detail later in the book.</p>
<p>CHAPTER 19. MONADS GONE WILD 1218
The gist of the mainabove is that it allows your keymapping
to be based on the current configuration of your environment.
Whenever you type a key, xmonad will pass the current config to
yourkeysfunction in order to determine what (if any) action it
should take based on that. We’re using the Monoid here to add
new keyboard shortcuts for lowering and raising the volume
with F6 and F7. The monoid of the keysfunctions is combining
all of the key maps each function produces when applied to
theXConfig to produce a final canonical key map.
Say what?
This is a Monoid instance we hadn’t covered in the Monoid
chapter, so let’s take a look at it now:
instance Monoidb
=&gt;Monoid(a-&gt;b)
-- Defined in ‘GHC.Base’
This, friends, is the Monoid of functions.
But how does it work? First, let’s set up some very trivial
functions for demonstration:
Prelude&gt; import Data.Monoid
Prelude&gt; let f = const (Sum 1)
Prelude&gt; let g = const (Sum 2)
Prelude&gt; f 9001
Sum {getSum = 1}
Prelude&gt; g 9001</p>
<p>CHAPTER 19. MONADS GONE WILD 1219
Sum {getSum = 2}
Query the types of those functions and see how you think
they will match up to the Monoid instance above.
We know that whatever arguments we give to 𝑓and𝑔, they
will always return their first arguments, which are Summonoids.
So if we mappend 𝑓and𝑔, they’re going to ignore whatever argu-
ment we tried to apply them to and use the Monoid to combine
the results:
Prelude&gt; (f &lt;&gt; g) 9001
Sum {getSum = 3}
So this Monoid instance allows to mappend the results of two
function applications:
(a-&gt;b)&lt;&gt;(a-&gt;b)
Just as long as the 𝑏has aMonoid instance.
We’re going to oﬀer a few more examples that will get you
closer to what the particular use of mappend in thexmonad ex-
ample is doing. We mentioned Data.Map back in the Testing
chapter. It gives us ordered pairs of keys and values:
Prelude&gt; import qualified Data.Map as M
Prelude M&gt; :t M.fromList
M.fromList :: Ord k =&gt; [(k, a)] -&gt; Map k a</p>
<p>CHAPTER 19. MONADS GONE WILD 1220
Prelude M&gt; let f = M.fromList [('a', 1)]
Prelude M&gt; let g = M.fromList [('b', 2)]
Prelude M&gt; :t f
f :: Num a =&gt; Map Char a
Prelude M&gt; import Data.Monoid
Prelude M Data.Monoid&gt; f &lt;&gt; g
fromList [('a',1),('b',2)]
Prelude M Data.Monoid&gt; :t (f &lt;&gt; g)
(f &lt;&gt; g) :: Num a =&gt; Map Char a
Prelude M Data.Monoid&gt; mappend f g
fromList [('a',1),('b',2)]
Prelude M Data.Monoid&gt; f <code>mappend</code> g
fromList [('a',1),('b',2)]
-- but note what happens here:
Prelude&gt; f &lt;&gt; g
fromList [('a',1)]
So, returning to the xmonad configuration we started with.
Thekeysfield is a function which, given an XConfig , produces a
keymapping. It uses the monoid of functions to combine the
pre-existing function that generates the keymap to produce as
many maps as you have mappended functions, then combine
all the key maps into one.
This part:</p>
<p>CHAPTER 19. MONADS GONE WILD 1221
&gt;&gt;return()
says that the key assignment is performing some eﬀects and
only performing some eﬀects. Functions have to reduce to
some result, but sometimes their only purpose is to perform
someeﬀectsandyoudon’twanttodoanythingwiththe“result”
of evaluating the terms.
As we’ve said and other people have noted as well, monoids
areeverywhere — not just in Haskell but in all of programming.
19.3 Functor
There’s a reason we chose that Michael Neale quotation for
theFunctor chapter epigraph: lifting really is the cheat mode.
fmapis ubiquitous in Haskell, for all sorts of applications, but
we’ve picked a couple that we found especially demonstrative
of why it’s so handy.
Lifting over IO
Herewe’retakingafunctionthatdoesn’tperformI/O, addUTCTime ,
partially applying it to the oﬀset we’re going to add to the sec-
ond argument, then mapping it over the IOaction that gets us
the current time:</p>
<p>CHAPTER 19. MONADS GONE WILD 1222
importData.Time.Clock
offsetCurrentTime ::NominalDiffTime
-&gt;IOUTCTime
offsetCurrentTime offset=
fmap (addUTCTime (offset <em>24</em>3600))$
getCurrentTime
Context for the above:
1.NominalDiffTime is a newtype of Picoand has a Numinstance,
that’s why the arithmetic works.
addUTCTime ::NominalDiffTime
-&gt;UTCTime
-&gt;UTCTime
2.getCurrentTime ::IOUTCTime
3.fmap’s type got specialized.
fmap::(UTCTime -&gt;UTCTime)
-&gt;IOUTCTime
-&gt;IOUTCTime
Here we’re lifting some data conversion stuﬀ over the fact
that the UUID library has to touch an outside resource (ran-
dom number generation) to give us a random identifier. The</p>
<p>CHAPTER 19. MONADS GONE WILD 1223
UUID library used is named uuidon Hackage. The Textpack-
age used is named… text:
import Data.Text (Text)
import qualified Data.Text asT
import qualified Data.UUID asUUID
import qualified Data.UUID.V4 asUUIDv4
textUuid ::IOText
textUuid =
fmap (T.pack.UUID.toString)
UUIDv4.nextRandom
1.nextRandom ::IOUUID
2.toString ::UUID-&gt;String
3.pack::String-&gt;Text
4.fmap::(UUID-&gt;Text)
-&gt;IOUUID
-&gt;IOText
Lifting over web app monads
Frequently when you write web applications, you’ll have a
custom datatype to describe the web application which is also</p>
<p>CHAPTER 19. MONADS GONE WILD 1224
aMonad. It’s a Monadbecause your “app context” will have a
type parameter to describe what result was produced in the
course of a running web application. Often these types will
abstract out the availability of a request or other configuration
data with a Reader (explained in a later chapter), as well as the
performance of eﬀects via IO. In the following example, we’re
lifting over AppHandler andMaybe:
userAgent ::AppHandler (MaybeUserAgent )
userAgent =
(fmap.fmap) userAgent' getRequest
userAgent' ::Request -&gt;MaybeUserAgent
userAgent' req=
getHeader &quot;User-Agent&quot; req
We need the Functor here because while we can pattern
match on the Maybevalue, an AppHandler isn’t something we can
pattern match on. It’s a convention in this web framework
library, snap, to make a type alias for your web application type.
It usually looks like this:
typeAppHandler =Handler AppApp
The underlying infrastructure for snapis more complicated
than we can cover to any depth here, but suffice to say there
are a few things floating around:</p>
<p>CHAPTER 19. MONADS GONE WILD 1225
1.HTTP request which triggered the processing currently
occurring.
2.The current (possibly empty or default) response that will
be returned to the client when the handlers and middle-
ware are done.
3.A function for updating the request timeout.
4.A helper function for logging.
5.And a fair bit more than this.
The issue here is that your AppHandler is meant to be slotted
into a web application which requires the reading in of con-
figuration, initialization of a web server, and the sending of a
request to get everything in motion. This is essentially a bunch
of functions waiting for arguments — waiting for something
to do. It doesn’t make sense to do all that yourself every time
you want a value that can only be obtained in the course of
the web application doing its thing. Accordingly, our Functor
is letting us write functions over structure which handles all
this work. It’s like we’re saying, “here’s a function, apply it to a
thing that resulted from an HTTP request coming down the
pipe, if one comes along.”</p>
<p>CHAPTER 19. MONADS GONE WILD 1226
19.4Applicative
Applicative is somewhat new to Haskell, but it’s useful enough,
particularlywithparsers, thatit’seasytofindexamples. There’s
a whole chapter on parsers coming up later, but we thought
these examples were mostly comprehensible even without
that context.
hgrev
This is an example from Luke Hoersten’s hgrevproject. The
example in the README is a bit dense, but uses Monoid and
Applicative to combine parsers of command line arguments:
jsonSwitch ::Parser(a-&gt;a)
jsonSwitch =
infoOption $(hgRevStateTH jsonFormat)
$long&quot;json&quot;
&lt;&gt;short'J'
&lt;&gt;help
&quot;Display JSON version information&quot;
parserInfo ::ParserInfo (a-&gt;a)
parserInfo =
info (helper &lt;*&gt;verSwitch &lt;*jsonSwitch)
fullDesc</p>
<p>CHAPTER 19. MONADS GONE WILD 1227
You might be wondering what the &lt;<em>operator is. It’s an-
other operator from the Applicative typeclass. It allows you to
sequence actions, discarding the result of the second argument.
Does this look familiar?
Prelude&gt; :t (&lt;</em>)
(&lt;<em>) :: Applicative f =&gt; f a -&gt; f b -&gt; f a
Prelude&gt; :t const
const :: a -&gt; b -&gt; a
Basically the (&lt;</em>)operator (like its sibling, (*&gt;), and the
monadic operator, &gt;&gt;) is useful when you’re emitting eﬀects.
In this case, you’ve done something with eﬀects and want to
discard any value that resulted.
More parsing
Here we’re using Applicative to lift the data constructor for the
Payload type over the Parser returned by requesting a value by
key out of a JSON object, which is basically an association of
text keys to further more JSON values which may be strings,
numbers, arrays, or more JSON objects:</p>
<p>CHAPTER 19. MONADS GONE WILD 1228
parseJSON ::Value-&gt;Parsera
(.:)::FromJSON a
=&gt;Object
-&gt;Text
-&gt;Parsera
instance FromJSON Payload where
parseJSON ( Objectv)=
Payload &lt;$&gt;v.:&quot;from&quot;
&lt;<em>&gt;v.:&quot;to&quot;
&lt;</em>&gt;v.:&quot;subject&quot;
&lt;<em>&gt;v.:&quot;body&quot;
&lt;</em>&gt;v.:&quot;offset_seconds&quot;
parseJSON v =typeMismatch &quot;Payload&quot; v
This is the same as the JSON but for CSV1data:
parseRecord ::Record-&gt;Parsera
1CSV stands for comma-separated values, a common, though not entirely standard-
ized file format.</p>
<p>CHAPTER 19. MONADS GONE WILD 1229
instance FromRecord Release where
parseRecord v
|V.length v ==5=Release &lt;$&gt;v.!0
&lt;<em>&gt;v.!1
&lt;</em>&gt;v.!2
&lt;<em>&gt;v.!3
&lt;</em>&gt;v.!4
|otherwise =mzero
This one uses liftA2 to lift the tuple data constructor over
parseKey andparseValue to give key-value pairings. You can see
the(&lt;*)operator in there again as well, along with the infix
operator for fmapand=&lt;&lt;as well:</p>
<p>CHAPTER 19. MONADS GONE WILD 1230
instance Deserializeable ShowInfoResp where
parser=
e2err=&lt;&lt;convertPairs
.HM.fromList &lt;$&gt;parsePairs
where
parsePairs ::Parser[(Text,Text)]
parsePairs =
parsePair <code>sepBy</code> endOfLine
parsePair =
liftA2 (,) parseKey parseValue
parseKey =
takeTill ( ==':')&lt;*kvSep
kvSep=string&quot;: &quot;
parseValue =takeTill isEndOfLine
This one instance is a virtual cornucopia of applications
of the previous chapters and we believe it demonstrates how
much cleaner and more readable these can make your code.
And now for something diﬀerent
This next example is also using an applicative, but this is a bit
diﬀerent than the above examples. We’ll spend more time</p>
<p>CHAPTER 19. MONADS GONE WILD 1231
explaining this one, as this pattern for writing utility functions
is common:
moduleWeb.Shipping.Utils ((&lt;||&gt;))where
importControl.Applicative (liftA2)
(&lt;||&gt;)::(a-&gt;Bool)
-&gt;(a-&gt;Bool)
-&gt;a
-&gt;Bool
(&lt;||&gt;)=liftA2 ( ||)
At first glance, this doesn’t seem too hard to understand,
but some examples will help you develop an understanding
of what’s going on. We start with the operator for boolean
disjunction, (||), which is an or:
Prelude&gt; True || False
True
Prelude&gt; False || False
False
Prelude&gt; (2 &gt; 3) || (3 == 3)
True
And now we want to be able to keep that as an infix operator
but lift it over some context, so we use liftA2 :</p>
<p>CHAPTER 19. MONADS GONE WILD 1232
Prelude&gt; import Control.Applicative
Prelude&gt; let (&lt;||&gt;) = liftA2 (||)
Andwe’llmakesometrivialfunctionsagainforthepurposes
of demonstration:
Prelude&gt; let f 9001 = True; f _ = False
Prelude&gt; let g 42 = True; g _ = False
Prelude&gt; :t f
f :: (Eq a, Num a) =&gt; a -&gt; Bool
Prelude&gt; f 42
False
Prelude&gt; f 9001
True
Prelude&gt; g 42
True
Prelude&gt; g 9001
False
We can compose the two functions 𝑓and𝑔to take one input
and give one summary result like this:
Prelude&gt; (\n -&gt; f n || g n) 0
False
Prelude&gt; (\n -&gt; f n || g n) 9001
True
Prelude&gt; :t (\n -&gt; f n || g n)</p>
<p>CHAPTER 19. MONADS GONE WILD 1233
(\n -&gt; f n || g n)
:: (Eq a, Num a) =&gt; a -&gt; Bool
But we have to pass in that argument 𝑛in order to do it that
way. Our utility function gives us a cleaner way:
Prelude&gt; (f &lt;||&gt; g) 0
False
Prelude&gt; (f &lt;||&gt; g) 9001
True
It’s parallel application of the functions against an argument.
That application produces two values, so we monoidally com-
bine the two values so that we have a single value to return.
We’ve set up an environment so that two (a -&gt; Bool) functions
that don’t have an 𝑎argument yet can return a result based on
those two Boolvalues when the combined function is eventu-
ally applied against an 𝑎.
19.5Monad
BecauseeﬀectfulprogrammingisconstrainedinHaskellthrough
the use of IO, andIOhas an instance of Monad, examples of Monad
in practical Haskell code are everywhere. We tried to find
some examples that illustrate diﬀerent interesting use cases.</p>
<p>CHAPTER 19. MONADS GONE WILD 1234
Opening a network socket
Here we’re using dosyntax for IO’sMonadin order to bind a
socket handle from the socket smart constructor, connect it
to an address, then return the handle for reading and writing.
This example is from haproxy-haskell by Michael Xavier. See
thenetwork library on Hackage for use and documentation:
importNetwork.Socket
openSocket ::FilePath -&gt;IOSocket
openSocket p= do
sock&lt;-socketAF_UNIX
Stream
defaultProtocol
connect sock sockAddr
return sock
wheresockAddr =
SockAddrUnix .encodeString $p
This isn’t too unlike anything you saw in previous chapters,
at least since we built the hangman game. The next example
is a bit richer.</p>
<p>CHAPTER 19. MONADS GONE WILD 1235
Binding over failure in initialization
Michael Xavier’s Seraph is a process monitor and has a main
entry point which is typical of more developed libraries and
applications. The outermost MonadisIO, but the monad trans-
former variant of Either , calledEitherT , is used to bind over the
possibility of failure in constructing an initialization function.
This possibility of failure centers on being able to pull up a
correct configuration:</p>
<p>CHAPTER 19. MONADS GONE WILD 1236
main::IO()
main= do
initAndFp &lt;-runEitherT $ do
fp&lt;-tryHead NoConfig =&lt;&lt;lift getArgs
initCfg &lt;-load' fp
return (initCfg, fp)
either bail (uncurry boot) initAndFp
where
boot initCfg fp =
void$runMVC mempty
oracleModel (core initCfg fp)
bailNoConfig =
errorExit &quot;Please pass a config&quot;
bail (InvalidConfig e)=
errorExit
(&quot;Invalid config &quot; ++show e)
load' fp =
hoistEither
.fmapLInvalidConfig
=&lt;&lt;lift (load fp)
If you found that very dense and difficult to follow at this
point, we’d encourage you to have another look at it after we’ve
covered monad transformers.</p>
<p>CHAPTER 19. MONADS GONE WILD 1237
19.6 An end-to-end example: URL
shortener
In this section, we’re going to walk through an entire program,
beginning to end.2There are some pieces we are not going to
explain thoroughly; however, this is something you can build
and work with if you’re interested in doing so.
First, the .cabal file for the project:
name: shawty
version: 0.1.0.0
synopsis: URI shortener
description: Please see README.md
homepage: http://github.com/
license: BSD3
license-file: LICENSE
author: Chris Allen
maintainer: cma@bitemyapp.com
copyright: 2015, Chris Allen
category: Web
build-type: Simple
cabal-version: &gt;=1.10
executable shawty
2The code in this example can be found here: https://github.com/bitemyapp/
shawty-prime/blob/master/app/Main.hs</p>
<p>CHAPTER 19. MONADS GONE WILD 1238
hs-source-dirs: app
main-is: Main.hs
ghc-options: -threaded
build-depends: base
, bytestring
, hedis
, mtl
, network-uri
, random
, scotty
, semigroups
, text
, transformers
default-language: Haskell2010
And the project layout:
$ tree
.
├── LICENSE
├── Setup.hs
├── app
│   └── Main.hs
├── shawty.cabal
└── stack.yaml</p>
<p>CHAPTER 19. MONADS GONE WILD 1239
You may choose to use Stack or not. That is how we got the
template for the project in place. If you’d like to learn more,
check out Stack’s Github repo3and the Stack video tutorial4
we worked on together. The code following from this point is
inMain.hs .
We need to start our program oﬀ with a language extension:
{-# LANGUAGE OverloadedStrings #-}
OverloadedStrings is a way to make String literals polymor-
phic, the way numeric literals are polymorphic over the Num
typeclass. String literals are not ordinarily polymorphic; String
is a concrete type. Using OverloadedStrings allows us to use
String literals as TextandByteString values.
Brief aside about polymorphic literals
We mentioned that the integral number literals in Haskell
are typed Num a =&gt; a by default. Now that we have another
example to work with, it’s worth examining how they work
under the hood, so to speak. First, let’s look at a typeclass from
a module in base:
Prelude&gt; import Data.String
3Stack Github repo https://github.com/commercialhaskell/stack
4The video Stack mega-tutorial! The whole video is long, but covers a lot of abnormal
use cases. Use the time stamps to jump to what you need to learn. https://www.youtube.
com/watch?v=sRonIB8ZStw&amp;feature=youtu.be</p>
<p>CHAPTER 19. MONADS GONE WILD 1240
Prelude&gt; :info IsString
class IsString a where
fromString :: String -&gt; a
-- Defined in ‘Data.String’
instance IsString [Char]
-- Defined in ‘Data.String’
Then we may notice something in NumandFractional :
classNumawhere
-- irrelevant bits elided
fromInteger ::Integer -&gt;a
classNuma=&gt;Fractional awhere
-- elision again
fromRational ::Rational -&gt;a
OK, and what about our literals?
Prelude&gt; :set -XOverloadedStrings
Prelude&gt; :t 1
1 :: Num a =&gt; a
Prelude&gt; :t 1.0
1.0 :: Fractional a =&gt; a
Prelude&gt; :t &quot;blah&quot;
&quot;blah&quot; :: IsString a =&gt; a</p>
<p>CHAPTER 19. MONADS GONE WILD 1241
The basic design is that the underlying representation is
concrete, butGHCautomaticallywrapsitin fromString /fromInteger /fromRational .
So it’s as if:
{-# LANGUAGE OverloadedStrings #-}
&quot;blah&quot;::Text
==fromString ( &quot;blah&quot;::String)
1::Int
==fromInteger ( 1::Integer)
2.5::Double
==fromRational ( 2.5::Rational )
Librarieslike textandbytestring provideinstancesfor IsString
in order to perform the conversion. Assuming you have those
libraries installed, you can kick it around a little. Note that,
due to the monomorphism restriction, the following will work
in the REPL but would not work if we loaded it from a source
file (because it would default to a concrete type; we’ve seen
this a couple times earlier in the book):
Prelude&gt; :set -XOverloadedStrings
Prelude&gt; let a = &quot;blah&quot;
Prelude&gt; a</p>
<p>CHAPTER 19. MONADS GONE WILD 1242
&quot;blah&quot;
Prelude&gt; :t a
a :: Data.String.IsString a =&gt; a
Then you can make it a TextorByteString value:
Prelude&gt; import Data.Text (Text)
Prelude&gt; :{
*Main| import Data.ByteString (ByteString)
*Main| :}
Prelude&gt; let t = &quot;blah&quot; :: Text
Prelude&gt; let bs = &quot;blah&quot; :: ByteString
Prelude&gt; t == bs
Couldn't match expected type ‘Text’ with
actual type ‘ByteString’
In the second argument of ‘(==)’,
namely ‘bs’
In the expression: t == bs
OverloadedStrings is a convenience that originated in the
desire of working Haskell programmers to use String literals
forTextandByteString values. It’s not too big a deal, but it
can be nice and saves you manually wrapping each literal in
fromString .</p>
<p>CHAPTER 19. MONADS GONE WILD 1243
Back to the show
Next, the module name must be Mainas that is required for
anything exporting a mainexecutable to be invoked when the
program runs. We follow the OverloadedStrings extension with
our imports:
moduleMainwhere
importControl.Monad (replicateM )
importControl.Monad.IO.Class (liftIO)
import qualified Data.ByteString.Char8
asBC
importData.Text.Encoding
(decodeUtf8 ,encodeUtf8 )
import qualified Data.Text.Lazy asTL
import qualified Database.Redis asR
importNetwork.URI (URI,parseURI )
import qualified System.Random asSR
importWeb.Scotty
Where we import something “qualified (…) as (…)” we are
doing two things. Qualifying the import means that we can
only refer to values in the module with the full module path,
and we use asto give the module that we want in scope a name.
For example, Data.ByteString.Char8.pack is a fully qualified ref-</p>
<p>CHAPTER 19. MONADS GONE WILD 1244
erence to pack. We qualify the import so that we don’t import
declarations that would conflict with bindings that already
exist in Prelude . By specifying a name using as, we can give the
value a shorter, more convenient name. Where we import the
module name followed by parentheses, such as with replicateM
orliftIO , we are saying we only want to import the functions
or values of that name and nothing else. In the case of import
Web.Scotty , we are importing everything Web.Scotty exports. An
unqualified and unspecific import should be avoided except in
those cases where the provenance of the imported functions
will be obvious, or when the import is a toolkit you must use
all together, such as scotty .
Next we need to generate our shortened URLs that will refer
to the links people post to the service. We will make a String
of the characters we want to select from:
alphaNum ::String
alphaNum =['A'..'Z']++['0'..'9']
Now we need to pick random elements from alphaNum . The
general idea here should be familiar from the hangman game.
First, we find the length of the list to determine a range to
select from, then get a random number in that range, using IO
to handle the randomness:</p>
<p>CHAPTER 19. MONADS GONE WILD 1245
randomElement ::String-&gt;IOChar
randomElement xs= do
letmaxIndex ::Int
maxIndex =length xs -1
-- Right of arrow is IO Int,
-- so randomDigit is Int
randomDigit &lt;-SR.randomRIO ( 0, maxIndex)
return (xs !!randomDigit)
Next, we apply randomElement toalphaNum to get a single ran-
domletterornumberfromouralphabet. Thenweuse replicateM
7to repeat this action 7 times, giving a list of 7 random letters
or numbers:
shortyGen ::IO[Char]
shortyGen =
replicateM 7(randomElement alphaNum)
For additional fun, see what replicateM 2 [1, 3] does and
whether you can figure out why. Compare it to the Prelude
function, replicate .
You may have noticed a mention of Redis in our imports
and wondered what was up. If you’re not already familiar with
it, Redis is in-memory, key-value data storage. The details
of how Redis works are well beyond the scope of this book
and they’re not very important here. Redis can be convenient</p>
<p>CHAPTER 19. MONADS GONE WILD 1246
for some common use cases like caching, or when you want
persistence without a lot of ceremony, as was the case here.
You will need to install and have Redis running in order for
the project to work; otherwise, the web server will throw an
error upon failing to connect to Redis.
This next bit is a function whose arguments are our con-
nection to Redis ( R.Connection ), the key we are setting in Redis,
and the value we are setting the key to. We also perform side
eﬀects in IOto getEither R.Reply R.Status as a result. The key
in this case is the randomly generated URI we created, and
the value is the URL the user wants the shortener to provide
at that address:
saveURI ::R.Connection
-&gt;BC.ByteString
-&gt;BC.ByteString
-&gt;IO(EitherR.ReplyR.Status)
saveURI conn shortURI uri =
R.runRedis conn $R.set shortURI uri
The next function, getURI , takes the connection to Redis and
the shortened URI key in order to get the URI associated with
that short URL and show users where they’re headed:</p>
<p>CHAPTER 19. MONADS GONE WILD 1247
getURI ::R.Connection
-&gt;BC.ByteString
-&gt;IO(EitherR.Reply
(MaybeBC.ByteString ))
getURIconn shortURI =
R.runRedis conn $R.get shortURI
Next some basic templating functions for returning output
to the web browser:
linkShorty ::String-&gt;String
linkShorty shorty=
concat
[&quot;&lt;a href= &quot;&quot;
, shorty
,&quot;&quot;&gt;Copy and paste your short URL</a>&quot;
]
The final output to scotty has to be a Textvalue, so we’re
concatenating lists of Textvalues to produce responses to the
browser:</p>
<p>CHAPTER 19. MONADS GONE WILD 1248
-- TL.concat :: [TL.Text] -&gt; TL.Text
shortyCreated ::Showa
=&gt;a
-&gt;String
-&gt;TL.Text
shortyCreated resp shawty =
TL.concat [ TL.pack (show resp)
,&quot; shorty is: &quot;
,TL.pack (linkShorty shawty)
]
shortyAintUri ::TL.Text-&gt;TL.Text
shortyAintUri uri=
TL.concat
[ uri
,&quot; wasn't a url,&quot;
,&quot; did you forget http://?&quot;
]
shortyFound ::TL.Text-&gt;TL.Text
shortyFound tbs=
TL.concat
[&quot;&lt;a href= &quot;&quot;
, tbs,&quot;&quot;&gt;&quot;
, tbs,&quot;</a>&quot;]</p>
<p>CHAPTER 19. MONADS GONE WILD 1249
Now we get to the bulk of web-appy bits in the form of our
application. We’ll enumerate the application in chunks, but
they’re all in one appfunction:
app::R.Connection
-&gt;ScottyM ()
apprConn= do
-- [1]
get&quot;/&quot;$ do
-- [2]
uri&lt;-param&quot;uri&quot;
-- [3]
1.Redis connection that should’ve been fired up before the
web server started.
2.getis a function that takes a RoutePattern , an action that
returns an HTTP response, and adds the route to the
Scotty server it’s embedded in. As you might suspect,
RoutePattern has anIsString instance so that the pattern
can be a String literal. The top-level route is expressed as
”/”, i.e., likegoingto https://google.com/ orhttps://bitemyapp.
com/. That final /character is what’s being expressed.
3.Theparamfunction is a means of getting…parameters.</p>
<p>CHAPTER 19. MONADS GONE WILD 1250
param::Parsable a
=&gt;Data.Text.Internal .Lazy.Text
-&gt;ActionM a
It’s sort of like Read, but it’s parsing a value of the type
you ask for. The paramfunction can find arguments via
URL path captures (see below with :short), HTML form
inputs, or query parameters. The first argument to param
is the “name” or key for the input. We cannot explain the
entirety of HTTP and HTML here, but the following are
means of getting a param with the key name:
a)URL path capture
get&quot;/user/:name/view&quot; $ do
-- requesting the URL /user/Blah/view
-- would make name = &quot;Blah&quot;
-- such as:
-- http://localhost:3000/user/Blah/view
b)HTML input form. Here the name attribute for the
input field is ”name”.</p>
<form>
Name:<br>
<input type="text" name="name">
</form>
<p>CHAPTER 19. MONADS GONE WILD 1251
c)Query parameters for URIs are pairings of keys and
values following a question mark.
http://localhost:3000/?name=Blah
You can define more than one by using ampersand to
separate the key value pairs.
/?name=Blah&amp;state=Texas
Now for the next chunk of the appfunction:
letparsedUri ::MaybeURI
parsedUri =
parseURI ( TL.unpack uri)
caseparsedUri of
-- [1]
Just_ -&gt; do
shawty&lt;-liftIO shortyGen
-- [2]
letshorty=BC.pack shawty
-- [3]</p>
<p>CHAPTER 19. MONADS GONE WILD 1252
uri'=
encodeUtf8 ( TL.toStrict uri)
-- [4]
resp&lt;-
liftIO (saveURI rConn shorty uri')
-- [5]
html (shortyCreated resp shawty)
-- [6]
Nothing -&gt;text (shortyAintUri uri)
-- [7]
1.We test that the user gave us a valid URI by using the
network-uri library’s parseURI function. We don’t really
care about the datatype it got wrapped in, so when we
check if it’s JustorNothing , we drop it on the floor.
2.TheMonadhere is ActionM (an alias of ActionT ), which is a
datatype representing code that handles web requests
and returns responses. You can perform IOactions in
thisMonad, but you have to lift the IOaction over the addi-
tional structure. Conventionally, one uses MonadIO as a sort
of auto-lift for IOactions, but you could do it manually.
We won’t demonstrate this here. We will explain monad
transformers in a later chapter so that ActionT will be less
mysterious.</p>
<p>CHAPTER 19. MONADS GONE WILD 1253
3.ConvertingtheshortcodefortheURIintoa Char8ByteString
for storage in Redis.
4.Converting the URI the user provided from a lazy Text
value into a strict Textvalue, then encoding as a UTF-8 (a
common Unicode format) ByteString for storage in Redis.
5.Again using liftIO so that we can perform an IOaction
inside a scotty ActionM . In this case, we’re saving the short
code and the URI in Redis so that we can look things up
with the short code as a key, then get the URI back as a
value if it has been stored in the past.
6.The templated response we return when we successfully
saved the short code for the URI. This gives the user a
shortened URI to share.
7.Error response in case the user gave us a URI that wasn’t
valid.
The second handler handles requests to a shortened URI
and returns the unshortened URL to follow:</p>
<p>CHAPTER 19. MONADS GONE WILD 1254
get&quot;/:short&quot; $ do
-- [1]
short&lt;-param&quot;short&quot;
-- [2]
uri&lt;-liftIO (getURI rConn short)
-- [3]
caseuriof
Leftreply-&gt;
text (TL.pack (show reply))
-- [4] [5]
RightmbBS-&gt; case mbBSof
-- [6]
Nothing -&gt;text&quot;uri not found&quot;
-- [7]
Justbs-&gt;html (shortyFound tbs)
-- [8]
wheretbs::TL.Text
tbs=
TL.fromStrict
(decodeUtf8 bs)
-- [9]
1.This is the URL path capture we mentioned earlier, such
that requesting /blahfrom the server will cause it to get
the key “blah” from Redis and, if there’s a value stored in</p>
<p>CHAPTER 19. MONADS GONE WILD 1255
that key, return that URI in the response. To do that in a
web browser or with curl/wget, you’d point your client at
http://localhost:3000/blah to test it.
2.Same parameter fetching as before. This time we expect
it to be part of the path capture rather than a query argu-
ment.
3.Lifting an IOaction inside ActionM again, this time to get
the short code as the lookup key from Redis.
4.Lefthere (in the Either we get back from Redis) signifies
some kind of failure, usually an error.
5.Textresponse returning an error in case we got Leftso that
the user knows what the error was, taking advantage of
Redis having Showable errors to render it in the response.
6.Happy path.
7.Just because an error didn’t happen doesn’t mean the key
was in the database.
8.Wefetchakeythatexistsinthedatabase, getthe ByteString
out of the Justdata constructor and render the URI in the
success template to show the user the URI we stored.
9.Going in the opposite direction we went in before — de-
coding the ByteString on the assumption it’s encoded as</p>
<p>CHAPTER 19. MONADS GONE WILD 1256
UTF-8, then converting from a strict Textvalue to a lazy
Textvalue.
Now we come to the mainevent.mainreturns IO ()and acts as
the entry point for our web server when we start the executable.
We begin by invoking scotty 3000 , a helper function from the
scotty framework which, given a port to run on and a scotty
application, will listen for requests and respond to them:
main::IO()
main= do
rConn&lt;-R.connect R.defaultConnectInfo
scotty3000(app rConn)
And that is the entirety of this URL shortener. We have a
couple of exercises based on this code, and we encourage you
to come back to it after we’ve covered monad transformers as
well and see how your comprehension is growing.
Exercise
In the URL shortener, an important step was omitted. We’re
not checking if we’re overwriting an existing short code, which
is entirely possible despite them being randomly generated.
We can calculate the odds of this by examining the cardinality
of the values.</p>
<p>CHAPTER 19. MONADS GONE WILD 1257
-- alphaNum = ['A'..'Z'] ++ ['0'..'9']
-- shortyGen =
-- replicateM 7 (randomElement alphaNum)
lengthalphaNum ^7==78364164096
So, the problem is, what if we accidentally clobber a previ-
ously generated short URI? There are a few ways of solving
this. One is to check to see if the short URI already exists in the
database before saving it and throwing an error if it does. This
is going to be vanishingly unlikely to happen unless you’ve
suddenly become a very popular URI shortening service, but
it’d prevent the loss of any data. Your exercise is to devise
some means of making this less likely. The easiest way would
be to simply make the short codes long enough that you’d
need to run a computer until the heat death of the universe
to get a collision, but you should try throwing an error in the
first handler we showed you first.
19.7 That’s a wrap!
We hope this chapter gave you some idea of how Haskellers
use the typeclasses we’ve been talking about in real code, to
handle various types of problems. In the next two chapters,
we’ll be looking at Foldable andTraversable , two typeclasses
with some interesting properties that rely on these four alge-</p>
<p>CHAPTER 19. MONADS GONE WILD 1258
braic structures (monoid, functor, applicative, and monad),
so we encourage you to take some time to explore some of
the uses we’ve demonstrated here. Consider going back to
anything you didn’t understand very well the first time you
went through those chapters.
19.8 Follow-up resources
1.The case of the mysterious explosion in space; Bryan
O’Sullivan; Explains how GHC handles string literals.</p>
<p>Chapter 20
Foldable
You gotta know when to
hold ’em, know when to
fold ’em, know when to
walk away, know when to
run.
Kenny Rogers
1259</p>
<p>CHAPTER 20. FOLDABLE 1260
20.1 Foldable
This typeclass has been appearing in type signatures at least
since Chapter 3, but for your purposes in those early chapters,
we said you could think of a Foldable thing as a list. As you
saw in the chapter on folds, lists are certainly foldable data
structures. But it is also true that lists are not the only foldable
data structures, so this chapter will expand on the idea of
catamorphisms and generalize it to many datatypes.
A list fold is a way to reduce the values inside a list to one
summary value by recursively applying some function. It is
sometimes difficult to appreciate that, as filtering and mapping
functions may be implemented in terms of a fold and yet
return an entirely new list! The new list is the summary value
of the old list after being reduced, or transformed, by function
application.
The folding function is always dependent on some Monoid
instance. The folds we wrote previously mostly relied on
implicit monoidal operations. As we’ll see in this chapter,
generalizing catamorphisms to other datatypes depends on
understanding the monoids for those structures and, in some
cases, making them explicit.
This chapter will cover:
•theFoldable class and its core operations;
•the monoidal nature of folding;</p>
<p>CHAPTER 20. FOLDABLE 1261
•standard operations derived from folding.
20.2 The Foldable class
The Hackage documentation for the Foldable typeclass de-
scribes it as being a, “class of data structures that can be folded
to a summary value.” The folding operations that we’ve seen
previously fit neatly into that definition, but this typeclass in-
cludes many operations. We’re going to go through the full
definition a little at a time. The definition in the library begins:
classFoldable twhere
{-# MINIMAL foldMap | foldr #-}
TheMINIMAL annotation on the typeclass tells you that a
minimally complete definition of the typeclass will define
foldMap orfoldrfor a datatype. As it happens, foldMap andfoldr
can each be implemented in terms of the other, and the other
operations included in the typeclass can be implemented in
terms of either of them. As long as at least one is defined,
you have a working instance of Foldable . Some methods in the
typeclass have default implementations that can be overridden
when needed. This is in case there’s a more efficient way to
do something that’s specific to your datatype.
If you query the info about the typeclass in GHCi, the first
line of the definition includes the kind signature for 𝑡:</p>
<p>CHAPTER 20. FOLDABLE 1262
class Foldable (t :: * -&gt; *) where
That𝑡should be a higher-kinded type is not surprising: lists
are higher-kinded types. We need 𝑡to be a type constructor
for the same reasons we did with Functor , and we will see that
the eﬀects are very similar. Types that take more than one
type argument, such as tuples and Either , will necessarily have
their first type argument included as part of their structure.
Please note that you will need to use GHC 7.10 or later ver-
sions for all the examples in this chapter to work. Also, while
thePrelude as of GHCi 7.10 includes many changes related to
theFoldable typeclass, not all of Foldable is in the Prelude . To
follow along with the examples in the chapter, you may need
to import Data.Foldable andData.Monoid (for some of the Monoid
newtypes).
20.3 Revenge of the monoids
One thing we did not talk about when we covered folds pre-
viously is the importance of monoids. Folding necessarily
implies a binary associative operation that has an identity
value. The first two operations defined in Foldable make this
explicit:</p>
<p>CHAPTER 20. FOLDABLE 1263
classFoldable (t:: * -&gt; * )where
fold::Monoidm=&gt;t m-&gt;m
foldMap ::Monoidm
=&gt;(a-&gt;m)-&gt;t a-&gt;m
Whilefoldallows you to combine elements inside a Foldable
structure using the Monoid defined for those elements, foldMap
first maps each element of the structure to a Monoid and then
combines the results using that instance of Monoid .
Thesemightseemalittleweirduntilyourealizethat Foldable
is requiring that you make the implicit Monoid visible in folding
operations. Let’s take a look at a very basic foldroperation
and see how it compares to foldandfoldMap :
Prelude&gt; foldr (+) 0 [1..5]
15
The binary associative operation for that fold is (+), so we’ve
specified it without thinking of it as a monoid. The fact that
the numbers in our list have other possible monoids is not
relevant once we’ve specified which operation to use.
We can already see from the type of foldthat it’s not going to
work the same as foldr, because it doesn’t take a function for its
first argument. But we also can’t just fold up a list of numbers,
because the foldfunction doesn’t have a Monoid specified:
Prelude&gt; fold (+) [1, 2, 3, 4, 5]</p>
<p>CHAPTER 20. FOLDABLE 1264
-- error message resulting from incorrect
-- number of arguments
Prelude&gt; fold [1, 2, 3, 4, 5]
-- error message resulting from not having
-- an instance of Monoid
So, what we need to do to make foldwork is specify a Monoid
instance:
Prelude&gt; let xs = map Sum [1..5]
Prelude&gt; fold xs
Sum {getSum = 15}
Or, less tediously:
Prelude&gt; :{
*Main| let xs :: Sum Integer
*Main| xs = [1, 2, 3, 4, 5]
*Main| :}
Prelude&gt; fold xs
Sum {getSum = 15}
Prelude&gt; :{
*Main| let xs :: Product Integer
*Main| xs = [1, 2, 3, 4, 5]
*Main| :}
Prelude&gt; fold xs</p>
<p>CHAPTER 20. FOLDABLE 1265
Product {getProduct = 120}
In some cases, the compiler can identify and use the stan-
dardMonoid for a type, without us being explicit:
Prelude&gt; foldr (++) &quot;&quot; [&quot;hello&quot;, &quot; julie&quot;]
&quot;hello julie&quot;
Prelude&gt; fold [&quot;hello&quot;, &quot; julie&quot;]
&quot;hello julie&quot;
The default Monoid instance for lists gives us what we need
without having to specify it.
And now for something diﬀerent
Let’s turn our attention now to foldMap . Unlike fold,foldMap has
a function as its first argument. Unlike foldr, the first (function)
argument of foldMap must explicitly map each element of the
structure to a Monoid :
Prelude&gt; foldMap Sum [1, 2, 3, 4]
Sum {getSum = 10}
Prelude&gt; foldMap Product [1, 2, 3, 4]
Product {getProduct = 24}
Prelude&gt; foldMap All [True, False, True]
All {getAll = False}</p>
<p>CHAPTER 20. FOLDABLE 1266
Prelude&gt; foldMap Any [(3 == 4), (9 &gt; 5)]
Any {getAny = True}
Prelude&gt; let xs = [Just 1, Nothing, Just 5]
Prelude&gt; foldMap First xs
First {getFirst = Just 1}
Prelude&gt; foldMap Last xs
Last {getLast = Just 5}
In the above examples, the function being applied is a data
constructor. The data constructor identifies the Monoid instance
— themappend — for those types. It already contains enough
information to allow foldMap to reduce the collection of values
to one summary value.
However, foldMap can also have a function to map that is
diﬀerent from the Monoid it’s using:
Prelude&gt; let xs = map Product [1..3]
Prelude&gt; foldMap (*5) xs
Product {getProduct = 750}
-- 5 * 10 * 15
750
Prelude&gt; let xs = map Sum [1..3]
Prelude&gt; foldMap (*5) xs
Sum {getSum = 30}</p>
<p>CHAPTER 20. FOLDABLE 1267
-- 5 + 10 + 15
30
It can map the function to each value first and then use the
Monoid instance to reduce them to one value. Compare this to
foldrin which the function has the Monoid instance baked in:
Prelude&gt; foldr (<em>) 5 [1, 2, 3]
-- (1 * (2 * (3 * 5)))
30
In fact, due to the way foldrworks, declaring a Monoid in-
stance that is diﬀerent from what is implied in the folding
function doesn’t change the final result:
Prelude&gt; let sumXs = map Sum [2..4]
Prelude&gt; foldr (</em>) 3 sumXs
Sum {getSum = 72}
Prelude&gt; let productXs = map Product [2..4]
Prelude&gt; foldr (*) 3 productXs
Product {getProduct = 72}
However, it is worth pointing out that if what you’re trying
to fold only contains one value, declaring a Monoid instance
won’t change the behavior of foldMap either:
Prelude&gt; let fm = foldMap (*5)
Prelude&gt; fm (Just 100) :: Product Integer</p>
<p>CHAPTER 20. FOLDABLE 1268
Product {getProduct = 500}
Prelude&gt; fm (Just 5) :: Sum Integer
Sum {getSum = 25}
With only one value, it doesn’t need the Monoid instance.
Specifying the Monoid instance is necessary to satisfy the type-
checker, but with only one value, there is nothing to mappend .
It just applies the function. It will use the mempty value from
the declared Monoid instance, though, in cases where what you
are trying to fold is empty:
Prelude&gt; fm Nothing :: Sum Integer
Sum {getSum = 0}
Prelude&gt; fm Nothing :: Product Integer
Product {getProduct = 1}
So, what we’ve seen so far is that Foldable is a way of general-
izing catamorphisms — folding — to diﬀerent datatypes, and
at least in some cases, it forces you to think about the monoid
you’re using to combine values.
20.4 Demonstrating Foldable instances
As we said above, a minimal Foldable instance must have either
foldrorfoldMap . Any of the other functions in this typeclass
can be derived from one or the other of those. With that said,</p>
<p>CHAPTER 20. FOLDABLE 1269
let’s turn our attention to implementing Foldable instances for
diﬀerent types.
Identity
We’ll kick things oﬀ by writing a Foldable instance for Identity :
dataIdentity a=
Identity a
We’re only obligated to write foldrorfoldMap , but we’ll write
both plus foldlso you have the gist of it.
instance Foldable Identity where
foldr f z ( Identity x)=f x z
foldl f z ( Identity x)=f z x
foldMap f ( Identity x)=f x
Withfoldrandfoldl, we’re doing basically the same thing,
but with the arguments swapped. We didn’t need to do any-
thing special for foldMap .
It may seem strange to think of folding one value. When
we’ve talked about catamorphisms previously, we’ve focused
on how they can reduce a bunch of values down to one sum-
mary value. In the case of this Identity catamorphism, though,</p>
<p>CHAPTER 20. FOLDABLE 1270
the point is less to reduce the values inside the structure to
one value and more to consume, or use, the value:
Prelude&gt; foldr (<em>) 1 (Identity 5)
5
Prelude&gt; foldl (</em>) 5 (Identity 5)
25
Prelude&gt; let fm = foldMap (*5)
Prelude&gt; type PI = Product Integer
Prelude&gt; fm (Identity 100) :: PI
Product {getProduct = 500}
Maybe
Thisoneisalittlemoreinterestingbecause, unlikewith Identity ,
we have to account for the Nothing cases. When the Maybevalue
that we’re folding is Nothing , we need to be able to return some
“zero” value, while doing nothing with the folding function
but also disposing of the Maybestructure. For foldrandfoldl,
that zero value is the start value provided:
Prelude&gt; foldr (+) 1 Nothing
1
On the other hand, for foldMap we use the Monoid’s identity
value as our zero:</p>
<p>CHAPTER 20. FOLDABLE 1271
Prelude&gt; let fm = foldMap (+1)
Prelude&gt; fm Nothing :: Sum Integer
Sum {getSum = 0}
When the value is a Justvalue, though, we need to apply
the folding function to the value and, again, dispose of the
structure:
Prelude&gt; foldr (+) 1 (Just 3)
4
Prelude&gt; fm $ Just 3 :: Sum Integer
Sum {getSum = 4}
So, let’s look at the instance. We’ll use a fake Maybetype
again, to avoid conflict with the Maybe instance that already
exists:
instance Foldable Optional where
foldr_zNada=z
foldr f z ( Yepx)=f x z
foldl_zNada=z
foldl f z ( Yepx)=f z x
foldMap _Nada=mempty
foldMap f ( Yepa)=f a</p>
<p>CHAPTER 20. FOLDABLE 1272
Note that if you don’t tell it what Monoid you mean, it will
complain about the type being ambiguous:
Prelude&gt; foldMap (+1) Nada
No instance for (Num a0) arising
from a use of ‘it’
The type variable ‘a0’ is ambiguous
(... blah blah who cares ...)
So, we need to assert a type that has a Monoid for this to work:
Prelude&gt; import Data.Monoid
Prelude&gt; foldMap (+1) Nada :: Sum Int
Sum {getSum = 0}
Prelude&gt; foldMap (+1) Nada :: Product Int
Product {getProduct = 1}
Prelude&gt; foldMap (+1) (Just 1) :: Sum Int
Sum {getSum = 2}
With aNadavalue and a declared type of Sum Int (giving us
ourMonoid ),foldMap gave us Sum 0because that was the mempty or
identity for Sum. Similarly with NadaandProduct , we got Product
1because that was the identity for Product .</p>
<p>CHAPTER 20. FOLDABLE 1273
20.5 Some basic derived operations
TheFoldable typeclass includes some other operations that
we haven’t covered in this context yet. Some of these, such
aslength, were previously defined for use with lists, but their
types have been generalized now to make them useful with
other types of data structures. Below are descriptions, type
signatures, and examples for several of these:
-- | List of elements of a structure,
-- from left to right.
toList::t a-&gt;[a]
Prelude&gt; toList (Just 1)
[1]
Prelude&gt; let xs = [Just 1, Just 2, Just 3]
Prelude&gt; map toList xs
[[1],[2],[3]]
Prelude&gt; concatMap toList xs
[1,2,3]
Prelude&gt; let xs = [Just 1, Just 2, Nothing]
Prelude&gt; concatMap toList xs
[1,2]
Prelude&gt; toList (1, 2)
[2]</p>
<p>CHAPTER 20. FOLDABLE 1274
Why doesn’t it put the 1 in the list? For the same reason that
fmapdoesn’t apply a function to the 1.
-- | Test whether the structure is empty.
null::t a-&gt;Bool
Notice that nullreturns TrueonLeftandNothing values, just
as it does on empty lists and so forth:
Prelude&gt; null (Left 3)
True
Prelude&gt; null []
True
Prelude&gt; null Nothing
True
Prelude&gt; null (1, 2)
False
Prelude&gt; let xs = [Just 1, Just 2, Nothing]
Prelude&gt; fmap null xs
[False,False,True]
The next one, length , returns a count of how many 𝑎values
inhabit the t a. In a list, that could be multiple 𝑎values due to
the definition of that datatype. It’s important to note, though,
that for tuples, the first argument (as well as the leftmost, or
outermost, type arguments of datatypes such as Maybeand
Either ) is part of the 𝑡here, not part of the 𝑎.</p>
<p>CHAPTER 20. FOLDABLE 1275
-- | Returns the size/length of a finite
-- structure as an 'Int'.
length::t a-&gt;Int
Prelude&gt; length (1, 2)
1
Prelude&gt; let xs = [(1, 2), (3, 4), (5, 6)]
Prelude&gt; length xs
3
Prelude&gt; fmap length xs
[1,1,1]
Prelude&gt; fmap length Just [1, 2, 3]
1
The last example looks strange, we know. But if you run
it in your REPL, you’ll see it returns the result we promised.
Why? And why does this
Prelude&gt; length $ Just [1, 2, 3]
1
return the same result?
The𝑎ofJust a in the last case above is a list. There is only
one list.</p>
<p>CHAPTER 20. FOLDABLE 1276
Prelude&gt; let xs = [Just 1, Just 2, Just 3]
Prelude&gt; fmap length xs
[1,1,1]
Prelude&gt; let xs = [Just 1, Just 2, Nothing]
Prelude&gt; fmap length xs
[1,1,0]
-- | Does the element occur
-- in the structure?
elem::Eqa=&gt;a-&gt;t a-&gt;Bool
We’ve used Either in the following example set to demon-
strate the behavior of Foldable functions with Leftvalues. As
we saw with Functor , you can’t map over the left data construc-
tor, because the left type argument is part of the structure. In
the following example set, that means that elemcan’t see inside
theLeftconstructor to whatever the value is, so the result will
beFalse, even if the value matches:
Prelude&gt; elem 2 (Just 3)
False
Prelude&gt; elem True (Left False)
False
Prelude&gt; elem True (Left True)
False
Prelude&gt; elem True (Right False)
False</p>
<p>CHAPTER 20. FOLDABLE 1277
Prelude&gt; elem True (Right True)
True
Prelude&gt; let xs = [Right 1,Right 2,Right 3]
Prelude&gt; fmap (elem 3) xs
[False,False,True]
-- | The largest element
-- of a non-empty structure.
maximum ::Orda=&gt;t a-&gt;a
-- | The least element
-- of a non-empty structure.
minimum ::Orda=&gt;t a-&gt;a
Here, notice that LeftandNothing (and similar) values are
empty for the purposes of these functions:
Prelude&gt; maximum [10, 12, 33, 5]
33
Prelude&gt; let xs = [Just 2, Just 10, Just 4]
Prelude&gt; fmap maximum xs
[2,10,4]
Prelude&gt; fmap maximum (Just [3, 7, 10, 2])
Just 10</p>
<p>CHAPTER 20. FOLDABLE 1278
Prelude&gt; minimum &quot;julie&quot;
'e'
Prelude&gt; fmap minimum (Just &quot;julie&quot;)
Just 'e'
Prelude&gt; let xs = map Just &quot;jul&quot;
Prelude&gt; xs
[Just 'j',Just 'u',Just 'l']
Prelude&gt; fmap minimum xs
&quot;jul&quot;
Prelude&gt; let xs = [Just 4, Just 3, Nothing]
Prelude&gt; fmap minimum xs
[4,3,*** Exception:
minimum: empty structure
Prelude&gt; minimum (Left 3)
*** Exception: minimum: empty structure
We’ve seen sumandproduct before, and they do what their
names suggest: return the sum and product of the members
of a structure:
sum::(Foldable t,Numa)=&gt;t a-&gt;a
product ::(Foldable t,Numa)=&gt;t a-&gt;a
And now for some examples:</p>
<p>CHAPTER 20. FOLDABLE 1279
Prelude&gt; sum (7, 5)
5
Prelude&gt; fmap sum [(7, 5), (3, 4)]
[5,4]
Prelude&gt; fmap sum (Just [1, 2, 3, 4, 5])
Just 15
Prelude&gt; product Nothing
1
Prelude&gt; fmap product (Just [])
Just 1
Prelude&gt; fmap product (Right [1, 2, 3])
Right 6
Exercises: Library Functions
Implement the functions in terms of foldMap orfoldrfrom
Foldable , then try them out with multiple types that have
Foldable instances.
1.This and the next one are nicer with foldMap , butfoldris
fine too.
sum::(Foldable t,Numa)=&gt;t a-&gt;a
2.product ::(Foldable t,Numa)=&gt;t a-&gt;a
3.elem::(Foldable t,Eqa)
=&gt;a-&gt;t a-&gt;Bool</p>
<p>CHAPTER 20. FOLDABLE 1280
4.minimum ::(Foldable t,Orda)
=&gt;t a-&gt;Maybea
5.maximum ::(Foldable t,Orda)
=&gt;t a-&gt;Maybea
6.null::(Foldable t)=&gt;t a-&gt;Bool
7.length::(Foldable t)=&gt;t a-&gt;Int
8.Some say this is all Foldable amounts to.
toList::(Foldable t)=&gt;t a-&gt;[a]
9.Hint: use foldMap .
-- | Combine the elements
-- of a structure using a monoid.
fold::(Foldable t,Monoidm)=&gt;t m-&gt;m
10.DefinefoldMap in terms of foldr.
foldMap ::(Foldable t,Monoidm)
=&gt;(a-&gt;m)-&gt;t a-&gt;m
20.6 Chapter Exercises
WriteFoldable instances for the following datatypes.</p>
<p>CHAPTER 20. FOLDABLE 1281
1.dataConstant a b=
Constant b
2.dataTwoa b=
Twoa b
3.dataThreea b c=
Threea b c
4.dataThree'a b=
Three'a b b
5.dataFour'a b=
Four'a b b b
Thinking cap time. Write a filter function for Foldable types
usingfoldMap .
filterF ::(Applicative f
,Foldable t
,Monoid(f a))
=&gt;(a-&gt;Bool)-&gt;t a-&gt;f a
filterF =undefined
20.7 Follow-up resources
1.Jakub Arnold; Foldable and Traversable.</p>
<p>Chapter 21
Traversable
O, Thou hast damnable
iteration; and art, indeed,
able to corrupt a saint.
Shakespeare
1282</p>
<p>CHAPTER 21. TRAVERSABLE 1283
21.1 Traversable
Functor gives us a way to transform any values embedded in
structure. Applicative gives us a way to transform any val-
ues contained within a structure using a function that is also
embedded in structure. This means that each application pro-
duces the eﬀect of adding structure which is then applicatively
combined. Foldable gives us a way to process values embedded
in a structure as if they existed in a sequential order, as we’ve
seen ever since we learned about list folding.
Traversable was introduced in the same paper as Applicative
and its introduction to Prelude didn’t come until the release
of GHC 7.10. However, it was available as part of the base
library before that. Traversable depends on Applicative , and
thusFunctor , and is also superclassed by Foldable .
Traversable allows you to transform elements inside the
structure like a functor, producing applicative eﬀects along the
way, and lift those potentially multiple instances of applicative
structure outside of the traversable structure. It is commonly
described as a way to traverse a data structure, mapping a
function inside a structure while accumulating the applicative
contexts along the way. This is easiest to see, perhaps, through
liberal demonstration of examples, so let’s get to it.
In this chapter, we will:
•explain the Traversable typeclass and its canonical func-</p>
<p>CHAPTER 21. TRAVERSABLE 1284
tions;
•explore examples of Traversable in practical use;
•tidy up some code using this typeclass;
•and, of course, write some Traversable instances.
21.2 The Traversable typeclass definition
This is the typeclass definition as it appears in the library
Data.Traversable :
class(Functor t,Foldable t)
=&gt;Traversable twhere
traverse ::Applicative f=&gt;
(a-&gt;f b)
-&gt;t a
-&gt;f (t b)
traverse f =sequenceA .fmap f
traverse maps each element of a structure to an action, eval-
uates the actions from left to right, and collects the results. So,
for example, if you have a list (structure) of IOactions, at the
end you’d have</p>
<p>CHAPTER 21. TRAVERSABLE 1285
-- | Evaluate each action in the
-- structure from left to right,
-- and collect the results.
sequenceA ::Applicative f
=&gt;t (f a) -&gt;f (t a)
sequenceA =traverse id
{-# MINIMAL traverse | sequenceA #-}
A minimal instance for this typeclass provides an imple-
mentation of either traverse orsequenceA , because as you can
see they can be defined in terms of each other. As with Foldable ,
we can define more than the bare minimum if we can leverage
information specific to our datatype to make the behavior
more efficient. We’re going to focus on these two functions in
this chapter, though, as those are the most typically useful.
21.3sequenceA
We will start with some examples of sequenceA in action, as it is
the simpler of the two. You can see from the type signature
above that the eﬀect of sequenceA is flipping two contexts or
structures. It doesn’t by itself allow you to apply any function
to the𝑎value inside the structure; it only flips the layers of
structure around. Compare these:
Prelude&gt; sum [1, 2, 3]</p>
<p>CHAPTER 21. TRAVERSABLE 1286
6
Prelude&gt; fmap sum [Just 1, Just 2, Just 3]
[1,2,3]
Prelude&gt; (fmap . fmap) sum Just [1, 2, 3]
Just 6
Prelude&gt; fmap product [Just 1, Just 2, Nothing]
[1,2,1]
To these:
Prelude&gt; fmap Just [1, 2, 3]
[Just 1,Just 2,Just 3]
Prelude&gt; sequenceA $ fmap Just [1, 2, 3]
Just [1,2,3]
Prelude&gt; sequenceA [Just 1, Just 2, Just 3]
Just [1,2,3]
Prelude&gt; sequenceA [Just 1, Just 2, Nothing]
Nothing
Prelude&gt; fmap sum $ sequenceA [Just 1, Just 2, Just 3]
Just 6
Prelude&gt; let xs = [Just 3, Just 4, Nothing]
Prelude&gt; fmap product (sequenceA xs)
Nothing
In the first example, using sequenceA flips the structures
around — instead of a list of Maybevalues, you get a Maybeof a
list value. In the next two examples, we can lift a function over</p>
<p>CHAPTER 21. TRAVERSABLE 1287
theMaybestructure and apply it to the list that is inside, if we
have aJust a value after applying the sequenceA .
It’s worth mentioning here that the Data.Maybe module has a
function called catMaybes that oﬀers a diﬀerent way of handling
a list of Maybevalues:
Prelude&gt; import Data.Maybe
Prelude&gt; catMaybes [Just 1, Just 2, Just 3]
[1,2,3]
Prelude&gt; catMaybes [Just 1, Just 2, Nothing]
[1,2]
Prelude&gt; let xs = [Just 1, Just 2, Just 3, Nothing]
Prelude&gt; sum $ catMaybes xs
6
Prelude&gt; fmap sum $ sequenceA xs
Nothing
UsingcatMaybes allows you to sum (or otherwise process) the
list ofMaybevalues even if there’s potentially a Nothing value
lurking within.
21.4traverse
Let’s look next at the type of traverse :</p>
<p>CHAPTER 21. TRAVERSABLE 1288
traverse
::(Applicative f,Traversable t)
=&gt;(a-&gt;f b)-&gt;t a-&gt;f (t b)
You might notice a similarity between that and the types of
fmapand(=&lt;&lt;)(flip bind):
fmap ::(a-&gt;b)-&gt;f a-&gt;f b
(=&lt;&lt;)::(a-&gt;m b)-&gt;m a-&gt;m b
traverse ::(a-&gt;f b)-&gt;t a-&gt;f (t b)
We’re still mapping a function over some embedded value(s), like
fmap, but similar to flip bind, that function is itself generating
more structure. However, unlike flip bind, that structure can
be of a diﬀerent type than the structure we lifted over to apply
the function. And at the end, it will flip the two structures
around, as sequenceA did.
In fact, as we saw in the typeclass definition, traverse isfmap
composed with sequenceA :
traverse f=sequenceA .fmap f
Let’s look at how that works in practice:
Prelude&gt; fmap Just [1, 2, 3]
[Just 1,Just 2,Just 3]
Prelude&gt; sequenceA $ fmap Just [1, 2, 3]</p>
<p>CHAPTER 21. TRAVERSABLE 1289
Just [1,2,3]
Prelude&gt; sequenceA . fmap Just $ [1, 2, 3]
Just [1,2,3]
Prelude&gt; traverse Just [1, 2, 3]
Just [1,2,3]
We’ll run through some longer examples in a moment, but
the general idea is that anytime you’re using sequenceA . fmap
f, you can use traverse to achieve the same result in one step.
mapMistraverse
Youmayhaveseenaslightlydiﬀerentwayofexpressing traverse
before, in the form of mapM.
In versions of GHC prior to 7.10, the type of mapMwas the
following:
mapM::Monadm
=&gt;(a-&gt;m b)-&gt;[a]-&gt;m [b]
-- contrast with
traverse ::Applicative f
=&gt;(a-&gt;f b)-&gt;t a-&gt;f (t b)
We can think of traverse inTraversable as abstracting the []
inmapMto being any traversable data structure and generalizing</p>
<p>CHAPTER 21. TRAVERSABLE 1290
theMonadrequirement to only need an Applicative . This is
valuable as it means we can use this pattern more widely and
with more code. For example, the list datatype is fine for
small pluralities of values but in more performance-sensitive
code, you may want to use a Vector from the vector1library.
Withtraverse , you won’t have to change your code because
the primary Vector datatype has a Traversable instance and so
should work.
Similarly, the type for sequence in GHC versions prior to
7.10 is a less useful sequenceA :
sequence ::Monadm
=&gt;[m a]
-&gt;m [a]
-- contrast with
sequenceA ::(Applicative f,Traversable t)
=&gt;t (f a)
-&gt;f (t a)
Again we’re generalizing the list to any Traversable and weak-
ening the Monadrequirement to Applicative .
1http://hackage.haskell.org/package/vector</p>
<p>CHAPTER 21. TRAVERSABLE 1291
21.5 So, what’s Traversable for?
In a literal sense, anytime you need to flip two type construc-
tors around, or map something and then flip them around,
that’s probably Traversable :
sequenceA ::Applicative f
=&gt;t (f a) -&gt;f (t a)
traverse ::Applicative f
=&gt;(a-&gt;f b)-&gt;t a-&gt;f (t b)
We’ll kick around some hypothetical functions and values
without bothering to implement them in the REPL to see when
we may want traverse orsequenceA :
Prelude&gt; let f = undefined :: a -&gt; Maybe b
Prelude&gt; let xs = undefined :: [a]
Prelude&gt; :t map f xs
map f xs :: [Maybe b]
But what if we want a value of type Maybe [b] ? The following
will work, but we can do better:
Prelude&gt; :t sequenceA $ map f xs
sequenceA $ map f xs :: Maybe [a]</p>
<p>CHAPTER 21. TRAVERSABLE 1292
It’s usually better to use traverse whenever we see a sequence
orsequenceA combined with a maporfmap:
Prelude&gt; :t traverse f xs
traverse f xs :: Maybe [b]
Next we’ll start looking at real examples of when you’d want
to do this.
21.6 Morse code revisited
We’re going to call back to what we did in the Testing chapter
with the Morse code to look at a nice example of how to use
traverse . Let’s recall what we had done there:
stringToMorse ::String-&gt;Maybe[Morse]
stringToMorse s=
sequence $fmap charToMorse s
-- what we want to do:
stringToMorse ::String-&gt;Maybe[Morse]
stringToMorse =traverse charToMorse
Normally, you might expect that if you mapped an (a -&gt;
f b)over at a, you’d end up with t (f b) buttraverse flips
that around. Remember, we had each character conversion
wrapped in a Maybedue to the possibility of getting characters in</p>
<p>CHAPTER 21. TRAVERSABLE 1293
a string that aren’t translatable into Morse (or, in the opposite
conversion, aren’t Morse characters):
Prelude&gt; morseToChar &quot;gobbledegook&quot;
Nothing
Prelude&gt; morseToChar &quot;-.-.&quot;
Just 'c'
We can use fromMaybe to remove the Maybelayer:
Prelude&gt; import Data.Maybe
Prelude Data.Maybe&gt; fromMaybe ' ' (morseToChar &quot;-.-.&quot;)
'c'
Prelude&gt; stringToMorse &quot;chris&quot;
Just [&quot;-.-.&quot;,&quot;....&quot;,&quot;.-.&quot;,&quot;..&quot;,&quot;...&quot;]
Prelude&gt; import Data.Maybe
Prelude&gt; fromMaybe [] (stringToMorse &quot;chris&quot;)
[&quot;-.-.&quot;,&quot;....&quot;,&quot;.-.&quot;,&quot;..&quot;,&quot;...&quot;]
We’ll define a little helper for use in the following examples:
Prelude&gt; let morse s = fromMaybe [] (stringToMorse s)
Prelude&gt; :t morse
morse :: String -&gt; [Morse]
Now, if we fmap morseToChar , we get a list of Maybevalues:</p>
<p>CHAPTER 21. TRAVERSABLE 1294
Prelude&gt; fmap morseToChar (morse &quot;chris&quot;)
[Just 'c',Just 'h',Just 'r',Just 'i',Just 's']
We don’t want catMaybes here because it drops the Nothing
values. What we want here is for any Nothing values to make the
final result Nothing . The function that gives us what we want for
this issequence . We did use sequence in the original version of
thestringToMorse function. sequence is useful for flipping your
types around as well (note the positions of the 𝑡and𝑚). There
is asequence inPrelude and another, more generic, version in
Data.Traversable :
Prelude&gt; :t sequence
sequence :: (Monad m, Traversable t) =&gt;
t (m a) -&gt; m (t a)
-- more general, can be used with types
-- other than List
Prelude&gt; import Data.Traversable as T
Prelude T&gt; :t T.sequence
T.sequence :: (Traversable t, Monad m)
=&gt; t (m a) -&gt; m (t a)
To use this over a list of Maybe(or other monadic) values, we
need to compose it with fmap:
Prelude&gt; :t (sequence .) . fmap</p>
<p>CHAPTER 21. TRAVERSABLE 1295
(sequence .) . fmap
:: (Monad m, Traversable t) =&gt;
(a1 -&gt; m a) -&gt; t a1 -&gt; m (t a)
Prelude&gt; sequence $ fmap morseToChar (morse &quot;chris&quot;)
Just &quot;chris&quot;
Prelude&gt; sequence $ fmap morseToChar (morse &quot;julie&quot;)
Just &quot;julie&quot;
The weird looking composition, which you’ve possibly also
seen in the form of (join .) . fmap is because fmaptakes two
(not one) arguments, so the expressions aren’t proper unless
we compose twice to await a second argument for fmapto get
applied to.
-- we want this
(sequence .).fmap=
\f xs-&gt;sequence (fmap f xs)
-- not this
sequence .fmap=
\f-&gt;sequence (fmap f)
This composition of sequence andfmapis so common that
traverse is now a standard Prelude function. Compare the
above to the following:</p>
<p>CHAPTER 21. TRAVERSABLE 1296
*Morse T&gt; traverse morseToChar (morse &quot;chris&quot;)
Just &quot;chris&quot;
*Morse T&gt; traverse morseToChar (morse &quot;julie&quot;)
Just &quot;julie&quot;
So,traverse isjustfmapandthe Traversable versionof sequence
bolted together into one convenient function. sequence is the
unique bit, but you need to do the fmapfirst most of the time,
so you end up using traverse . This is very similar to the way
&gt;&gt;=is justjoincomposed with fmapwherejoinis the bit that is
unique to Monad.
21.7 Axing tedious code
Try to bear with us for a moment and realize that the following
is real but also intentionally fake code. That is, one of the
authors helped somebody with refactoring their code, and
this simplified version is what your author was given. One of
the strengths of Haskell is that we can work in terms of types
without worry about code that actually runs sometimes. This
code is from Alex Petrov:</p>
<p>CHAPTER 21. TRAVERSABLE 1297
-- Thanks for the great example, Alex
dataQuery =Query
dataSomeObj =SomeObj
dataIoOnlyObj =IoOnlyObj
dataErr =Err
-- There's a decoder function that makes
-- some object from String
decodeFn ::String-&gt;EitherErrSomeObj
decodeFn =undefined
-- There's a query, that runs against the
-- DB and returns array of strings
fetchFn ::Query-&gt;IO[String]
fetchFn =undefined
-- an additional &quot;context initializer&quot;,
-- that also has IO
makeIoOnlyObj ::[SomeObj]
-&gt;IO[(SomeObj,IoOnlyObj )]
makeIoOnlyObj =undefined</p>
<p>CHAPTER 21. TRAVERSABLE 1298
-- before
pipelineFn
::Query
-&gt;IO(EitherErr[(SomeObj,IoOnlyObj )])
pipelineFn query= do
a&lt;-fetchFn query
casesequence (map decodeFn a) of
(Lefterr)-&gt;return$Left$err
(Rightres)-&gt; do
a&lt;-makeIoOnlyObj res
return$Righta
The objective was to clean up this code. A few things made
them suspicious:
1.The use of sequence andmap.
2.Manually casing on the result of the sequence and the map.
3.Binding monadically over the Either only to perform an-
other monadic ( IO) action inside of that.
We pared the pipeline function down to this:</p>
<p>CHAPTER 21. TRAVERSABLE 1299
pipelineFn
::Query
-&gt;IO(EitherErr[(SomeObj,IoOnlyObj )])
pipelineFn query= do
a&lt;-fetchFn query
traverse makeIoOnlyObj (mapM decodeFn a)
Thanks to merijn on the IRC channel for helping with this.
We can make it pointfree if we want to:
pipelineFn
::Query
-&gt;IO(EitherErr[(SomeObj,IoOnlyObj )])
pipelineFn =
(traverse makeIoOnlyObj
.mapM decodeFn =&lt;&lt;).fetchFn
And since mapMis justtraverse with a slightly diﬀerent type:
pipelineFn
::Query
-&gt;IO(EitherErr[(SomeObj,IoOnlyObj )])
pipelineFn =
(traverse makeIoOnlyObj
.traverse decodeFn =&lt;&lt;).fetchFn</p>
<p>CHAPTER 21. TRAVERSABLE 1300
This is the terse, clean style many Haskellers prefer. As we
said back when we first introduced it, pointfree style can help
focus the attention on the functions, rather than the specifics
of the data that are being passed around as arguments. Using
functions like traverse cleans up code by drawing attention to
the ways the types are changing and signaling the program-
mer’s intent.
21.8 Do all the things
We’re going to use an HTTP client library named wreq2for
this demonstration so we can make calls to a handy-dandy
website for testing HTTP clients at http://httpbin.org/ . Feel
free to experiment and substitute your own ideas for HTTP
services or websites you could poke and prod.
2http://hackage.haskell.org/package/wreq</p>
<p>CHAPTER 21. TRAVERSABLE 1301
moduleHttpStuff where
importData.ByteString.Lazy hiding(map)
importNetwork.Wreq
-- replace with other websites
-- if desired or needed
urls::[String]
urls=[&quot;http://httpbin.org/ip&quot;
,&quot;http://httpbin.org/bytes/5&quot;
]
mappingGet ::[IO(Response ByteString )]
mappingGet =map get urls
But what if we don’t want a list of IOactions we can perform
to get a response, but rather one big IOaction that produces a
list of responses? This is where Traversable can be helpful:
traversedUrls ::IO[Response ByteString ]
traversedUrls =traverse get urls
We hope that these examples have helped demonstrate that
Traversable is a useful typeclass. While Foldable seems trivial,
it is a necessary superclass of Traversable , andTraversable , like</p>
<p>CHAPTER 21. TRAVERSABLE 1302
Functor andMonad, is now widely used in everyday Haskell code,
due to its practicality.
Strength for understanding
Traversable is stronger than Functor andFoldable . Because of
this, we can recover the Functor andFoldable instance for a
type from the Traversable , just as we can recover the Functor
andApplicative from the Monad. Here we can use the Identity
type to get something that is essentially just fmapall over again:
Prelude&gt; import Data.Functor.Identity
Prelude&gt; traverse (Identity . (+1)) [1, 2]
Identity [2,3]
Prelude&gt; runIdentity $ traverse (Identity . (+1)) [1, 2]
[2,3]
Prelude&gt; :{
Prelude| let edgeMap f t =
Prelude| runIdentity $ traverse (Identity . f) t
Prelude| :}
Prelude&gt; :t edgeMap
edgeMap :: Traversable t =&gt; (a -&gt; b) -&gt; t a -&gt; t b
Prelude&gt; edgeMap (+1) [1..5]
[2,3,4,5,6]</p>
<p>CHAPTER 21. TRAVERSABLE 1303
UsingConstorConstant , wecanrecoverafoldMappy-looking
Foldable as well:
Prelude&gt; import Data.Monoid
-- from <code>transformers</code>
Prelude&gt; import Data.Functor.Constant
Prelude&gt; let xs = [1, 2, 3, 4, 5] :: [Sum Integer]
Prelude&gt; traverse (Constant . (+1)) xs
Constant (Sum {getSum = 20})
Prelude&gt; :{
Prelude| let foldMap' f t =
Prelude| getConstant $ traverse (Constant . f) t
Prelude| :}
Prelude&gt; :t foldMap'
foldMap' :: (Traversable t, Monoid a)
=&gt; (a1 -&gt; a) -&gt; t a1 -&gt; a
Prelude&gt; :t foldMap
foldMap :: (Foldable t, Monoid m) =&gt; (a -&gt; m) -&gt; t a -&gt; m
Doing exercises like this can help strengthen your intuitions
for the relationships of these typeclasses and their canonical
functions. We know it sometimes feels like these things are
pure intellectual exercise, but getting comfortable with ma-
nipulating functions like these is ultimately the key to getting</p>
<p>CHAPTER 21. TRAVERSABLE 1304
comfortable with Haskell. This is how you learn to play type
Tetris with the pros.
21.9 Traversable instances
You knew this was coming.
Either
TheTraversable instance that follows here is identical to the
one in the Data.Traversable module in base, but we’ve added
aFunctor ,Foldable , andApplicative so that you might see a
progression:</p>
<p>CHAPTER 21. TRAVERSABLE 1305
dataEithera b=
Lefta
|Rightb
deriving (Eq,Ord,Show)
instance Functor (Eithera)where
fmap_(Leftx)=Leftx
fmap f ( Righty)=Right(f y)
instance Applicative (Eithere)where
pure =Right
Lefte&lt;<em>&gt; _ = Lefte
Rightf&lt;</em>&gt;r=fmap f r
instance Foldable (Eithera)where
foldMap <em>(Left</em>)=mempty
foldMap f ( Righty)=f y
foldr_z (Left_)=z
foldr f z ( Righty)=f y z
instance Traversable (Eithera)where
traverse _(Leftx)=pure (Leftx)
traverse f ( Righty)=Right&lt;$&gt;f y</p>
<p>CHAPTER 21. TRAVERSABLE 1306
Given what you’ve seen above, this hopefully isn’t too sur-
prising. We have function application and type-flipping, in an
Either context.
Tuple
As above, we’ve provided a progression of instances, but for
the two-tuple or anonymous product:
instance Functor ((,) a) where
fmap f (x,y) =(x, f y)
instance Monoida
=&gt;Applicative ((,) a) where
pure x=(mempty, x)
(u, f)&lt;*&gt;(v, x)=
(u <code>mappend</code> v, f x)
instance Foldable ((,) a) where
foldMap f ( _, y)=f y
foldr f z ( _, y)=f y z
instance Traversable ((,) a) where
traverse f (x, y) =(,) x&lt;$&gt;f y
Here, we have much the same, but for a tuple context.</p>
<p>CHAPTER 21. TRAVERSABLE 1307
21.10Traversable Laws
Thetraverse function must satisfy the following laws:
1.Naturality
t.traverse f =traverse (t .f)
This law tells us that function composition behaves in
unsurprising ways with respect to a traversed function.
Since a traversed function 𝑓is generating the structure
that appears on the “outside” of the traverse operation,
there’s no reason we shouldn’t be able to float a function
over the structure into the traversal itself.
2.Identity
traverse Identity =Identity
This law says that traversing the data constructor of the
Identity type over a value will produce the same result
as just putting the value in Identity . This tells us Identity
represents a structural identity for traversing data. This is
another way of saying that a Traversable instance cannot
add or inject any structure or eﬀects.
3.Composition
traverse (Compose .fmap g.f)=
Compose .fmap (traverse g) .traverse f</p>
<p>CHAPTER 21. TRAVERSABLE 1308
This law demonstrates how we can collapse sequential
traversals into a single traversal, by taking advantage of
theCompose datatype, which combines structure.
ThesequenceA function must satisfy the following laws:
1.Naturality
t.sequenceA =sequenceA .fmap t
2.Identity
sequenceA .fmapIdentity =Identity
3.Composition
sequenceA .fmapCompose =
Compose .fmap sequenceA .sequenceA
Noneofthisshould’vebeentoosurprisinggivenwhatyou’ve
seen with traverse .
21.11 Quality Control
Great news! You can QuickCheck your Traversable instances as
well, since they have laws. Conveniently, the checkers library
we’ve been using already has the laws for us. You can add the
following to a module and change the type alias to change
what instances are being tested:</p>
<p>CHAPTER 21. TRAVERSABLE 1309
typeTI=[]
main= do
lettrigger ::TI(Int,Int, [Int])
trigger =undefined
quickBatch (traversable trigger)
21.12 Chapter Exercises
Traversable instances
Write a Traversable instance for the datatype provided, filling
in any required superclasses. Use QuickCheck to validate your
instances.
Identity
Write a Traversable instance for Identity .
newtype Identity a=Identity a
deriving (Eq,Ord,Show)
instance Traversable Identity where
traverse =undefined</p>
<p>CHAPTER 21. TRAVERSABLE 1310
Constant
newtype Constant a b=
Constant { getConstant ::a }
Maybe
dataOptional a=
Nada
|Yepa
List
dataLista=
Nil
|Consa (Lista)
Three
dataThreea b c=
Threea b c
Pair
dataPaira b=
Paira b</p>
<p>CHAPTER 21. TRAVERSABLE 1311
Big
When you have more than one value of type 𝑏, you’ll want
to useMonoid andApplicative for the Foldable andTraversable
instances respectively.
dataBiga b=
Biga b b
Bigger
Same as for Big.
dataBiggera b=
Biggera b b b
S
This may be difficult. To make it easier, we’ll give you the
constraints and QuickCheck instances:
{-# LANGUAGE FlexibleContexts #-}
moduleSkiFree where
importTest.QuickCheck
importTest.QuickCheck.Checkers</p>
<p>CHAPTER 21. TRAVERSABLE 1312
dataSn a=S(n a) a deriving (Eq,Show)
instance (Functor n
,Arbitrary (n a)
,Arbitrary a )
=&gt;Arbitrary (Sn a)where
arbitrary =
S&lt;$&gt;arbitrary &lt;<em>&gt;arbitrary
instance (Applicative n
,Testable (nProperty )
,EqPropa )
=&gt;EqProp(Sn a)where
(Sx y)=-=(Sp q)=
(property $(=-=)&lt;$&gt;x&lt;</em>&gt;p)
.&amp;.(y=-=q)
instance Traversable n
=&gt;Traversable (Sn)where
traverse =undefined
main=
sample' (arbitrary ::Gen(S[]Int))</p>
<p>CHAPTER 21. TRAVERSABLE 1313
Instances for Tree
This might be hard. Write the following instances for Tree.
dataTreea=
Empty
|Leafa
|Node(Treea) a (Treea)
deriving (Eq,Show)
instance Functor Treewhere
fmap=undefined
-- foldMap is a bit easier
-- and looks more natural,
-- but you can do foldr too
-- for extra credit.
instance Foldable Treewhere
foldMap =undefined
instance Traversable Treewhere
traverse =undefined
Hints:
1.ForfoldMap , thinkFunctor but with some Monoid thrown in.</p>
<p>CHAPTER 21. TRAVERSABLE 1314
2.Fortraverse , thinkFunctor but with some Functor3thrown
in.
21.13 Follow-up resources
1.Foldable and Traversable; Jakub Arnold.
2.The Essence of the Iterator Pattern; Jeremy Gibbons and
Bruno Oliveira.
3.Applicative Programming with Eﬀects; Conor McBride
and Ross Paterson.
3Not a typo.</p>
<p>Chapter 22
Reader
The tears of the world are
a constant quantity. For
each one who begins to
weep somewhere else
another stops. The same
is true of the laugh.
Samuel Beckett
1315</p>
<p>CHAPTER 22. FUNCTIONS WAITING FOR INPUT 1316
22.1 Reader
The last two chapters were focused on some typeclasses that
might still seem strange and difficult to you. The next three
chapters are going to focus on some patterns that might still
seem strange and difficult. Foldable ,Traversable ,Reader,State,
and parser combinators are not strictly necessary to under-
standing and using Haskell. We do have reasons for introduc-
ing them now, but those reasons might not seem clear to you
for a while. If you don’t quite grasp all of it on the first pass,
that’s completely fine. Read it through, do your best with the
exercises, come back when you feel like you’re ready.
Whenwritingapplications, programmersoftenneedtopass
around some information that may be needed intermittently
or universally throughout an entire application. We don’t want
to simply pass this information as arguments because it would
be present in the type of almost every function. This can make
the code harder to read and harder to maintain. To address
this, we use Reader .
In this chapter, we will:
•examine the Functor ,Applicative , andMonadinstances for
functions;
•learn about the Reader newtype;
•see some examples of using Reader .</p>
<p>CHAPTER 22. FUNCTIONS WAITING FOR INPUT 1317
22.2 A new beginning
We’re going to set this chapter up a bit diﬀerently from previ-
ous chapters, because we’re hoping that this will help demon-
strate that what we’re doing here is not that diﬀerent from
things you’ve done before. So, we’re going to start with some
examples. Start a file like this:
importControl.Applicative
boop=(*2)
doop=(+10)
bip::Integer -&gt;Integer
bip=boop.doop
We know that the bipfunction will take one argument be-
cause of the types of boop,doop, and(.). Note that if you do not
specify the types and load it from a file, it will be monomor-
phic by default; if you wish to make bippolymorphic, you
may change its signature but you also need to specify a poly-
morphic type for the two functions it’s built from. The rest of
the chapter will wait while you verify these things.
When we apply bipto an argument, doopwill be applied to
that argument first, and then the result of that will be passed
as input to boop. So far, nothing new.</p>
<p>CHAPTER 22. FUNCTIONS WAITING FOR INPUT 1318
We can also write that function composition this way:
bloop::Integer -&gt;Integer
bloop=fmap boop doop
We aren’t accustomed to fmapping a function over another
function, and you may be wondering what the functorial con-
text here is. By “functorial context” we mean the structure
(datatype) that the function is being lifted over in order to
apply to the value inside. For example, a list is a functorial
context we can lift functions over. We say that the function gets
lifted over the structure of the list and applied to or mapped
over the values that are inside the list.
Inbloop, the context is a partially applied function. As in
function composition, fmapcomposes the two functions before
applying them to the argument. The result of the one can then
get passed to the next as input. Using fmaphere lifts the one
partially applied function over the next, in a sense setting up
something like this:
fmapboop doop x ==(*2) ((+10) x)
-- when this x comes along, it's the
-- first necessary argument to (+10)
-- then the result for that is the
-- first necessary argument to (*2)</p>
<p>CHAPTER 22. FUNCTIONS WAITING FOR INPUT 1319
This is the Functor of functions. We’re going to go into more
detail about this soon.
For now, let’s turn to another set of examples. Put these in
the same file so boopanddoopare still in scope:
bbop::Integer -&gt;Integer
bbop=(+)&lt;$&gt;boop&lt;*&gt;doop
duwop::Integer -&gt;Integer
duwop=liftA2 ( +) boop doop
Now we’re in an Applicative context. We’ve added another
function to lift over the contexts of our partially applied func-
tions. This time, we still have partially applied functions that
are awaiting application to an argument, but this will work
diﬀerently than fmapping did. This time, the argument will
get passed to both boopanddoopin parallel, and the results will
be added together.
boopanddoopare each waiting for an input. We can apply
them both at once like this:
Prelude&gt; bbop 3
19
That does something like this:</p>
<p>CHAPTER 22. FUNCTIONS WAITING FOR INPUT 1320
((+)&lt;$&gt;(<em>2)&lt;</em>&gt;(+10))3
-- First the fmap
(*2)::Numa=&gt;a-&gt;a
(+)::Numa=&gt;a-&gt;a-&gt;a
(+)&lt;$&gt;(*2)::Numa=&gt;a-&gt;a-&gt;a
Mapping a function awaiting two arguments over a function
awaiting one produces a two argument function.
Remember, this is identical to function composition:
(+).(*2)::Numa=&gt;a-&gt;a-&gt;a
With the same result:
Prelude&gt; ((+) . (*2)) 5 3
13
Prelude&gt; ((+) &lt;$&gt; (*2)) 5 3
13
So what’s happening?</p>
<p>CHAPTER 22. FUNCTIONS WAITING FOR INPUT 1321
((+)&lt;$&gt;(*2))53
-- Keeping in mind that this
-- is (.) under the hood
((+).(*2))53
-- f . g = \ x -&gt; f (g x)
((+).(<em>2))==\x-&gt;(+) (2</em>x)
The tricky part here is that even after we apply 𝑥, we’ve got
(+)partially applied to the first argument which was doubled
by(*2). There’s a second argument, and that’s what will get
added to the first argument that got doubled:</p>
<p>CHAPTER 22. FUNCTIONS WAITING FOR INPUT 1322
-- The first function to get
-- applied is (<em>2), and the
-- first argument is 5. (<em>2)
-- takes one argument, so we get:
((+).(<em>2))53
(\x-&gt;(+) (2</em>x))53
(\5-&gt;(+) (2</em>5))3
((+)10)3
-- Then it adds 10 and 3
13
Okay, but what about the second bit?
((+)&lt;$&gt;(<em>2)&lt;</em>&gt;(+10))3
-- Wait, what? What happened to the
-- first argument?
((+)&lt;$&gt;(<em>2)&lt;</em>&gt;(+10))::Numb=&gt;b-&gt;b
One of the nice things about Haskell is we can assert a more
concrete type for functions like (&lt;</em>&gt;)and see if the compiler
agrees we’re putting forth something hypothetically possible.
Let’s remind ourselves of the type of (&lt;<em>&gt;):
Prelude&gt; :t (&lt;</em>&gt;)</p>
<p>CHAPTER 22. FUNCTIONS WAITING FOR INPUT 1323
(&lt;<em>&gt;) :: Applicative f =&gt; f (a -&gt; b) -&gt; f a -&gt; f b
-- in this case, we know f is ((-&gt;) a)
-- so we concretize it thusly
Prelude&gt; :t (&lt;</em>&gt;) :: (a -&gt; a -&gt; b) -&gt; (a -&gt; a) -&gt; (a -&gt; b)
(&lt;<em>&gt;) :: (a -&gt; a -&gt; b) -&gt; (a -&gt; a) -&gt; (a -&gt; b)
The compiler agrees that this is a possible type for (&lt;</em>&gt;).
So how does that work? What’s happening is we’re feeding
a single argument to the (<em>2)and(+10)and the two results
form the two arguments to (+):
((+)&lt;$&gt;(<em>2)&lt;</em>&gt;(+10))3
(3</em>2)+(3+10)
6+13
19
We’d use this when two functions would share the same
input and we want to apply some other function to the result
of those to reach a final result. This happens more than you
might think, and we saw an example of it back in the Abstract
Structure Applied chapter:</p>
<p>CHAPTER 22. FUNCTIONS WAITING FOR INPUT 1324
moduleWeb.Shipping.Utils ((&lt;||&gt;))where
importControl.Applicative (liftA2)
(&lt;||&gt;)::(a-&gt;Bool)
-&gt;(a-&gt;Bool)
-&gt;a
-&gt;Bool
(&lt;||&gt;)=liftA2 ( ||)
That is the same idea as duwopabove.
Finally, another example:
boopDoop ::Integer -&gt;Integer
boopDoop = do
a&lt;-boop
b&lt;-doop
return (a +b)
This will do precisely the same thing as the Applicative ex-
ample, but this time the context is monadic. This distinction
doesn’t much matter with this particular function. We assign
the variable 𝑎to the partially applied function boop, and𝑏to
doop. As soon as we receive an input, it will fill the empty slots
inboopanddoop. The results will be bound to the variables 𝑎
and𝑏and passed into return .</p>
<p>CHAPTER 22. FUNCTIONS WAITING FOR INPUT 1325
So, we’ve seen here that we can have a Functor ,Applicative ,
andMonadfor partially applied functions. In all cases, these
are awaiting application to one argument that will allow both
functions to be evaluated. The Functor of functions is function
composition. The Applicative andMonadchain the argument
forward in addition to the composition (applicatives and mon-
ads are both varieties of functors, so they retain that core
functorial behavior).
This is the idea of Reader. It is a way of stringing functions
together when all those functions are awaiting one input from
a shared environment. We’re going to get into the details of
how it works, but the important intuition here is that it’s an-
other way of abstracting out function application and gives us
a way to do computation in terms of an argument that hasn’t
been supplied yet. We use this most often when we have a con-
stant value that we will obtain from somewhere outside our
program that will be an argument to a whole bunch of func-
tions. Using Reader allows us to avoid passing that argument
around explicitly.
Short Exercise: Warming Up
We’ll be doing something here very similar to what you saw
above, to give you practice and try to develop a feel or intuition
for what is to come. These are similar enough to what you just
saw that you can almost copy and paste, so try not to overthink</p>
<p>CHAPTER 22. FUNCTIONS WAITING FOR INPUT 1326
them too much.
First, start a file oﬀ like this:
importData.Char
cap::[Char]-&gt;[Char]
capxs=map toUpper xs
rev::[Char]-&gt;[Char]
revxs=reverse xs
Two simple functions with the same type, taking the same
type of input. We could compose them, using (.)orfmap:
composed ::[Char]-&gt;[Char]
composed =undefined
fmapped ::[Char]-&gt;[Char]
fmapped =undefined
The output of those two should be identical: one string that
is made all uppercase and reversed, like this:
Prelude&gt; composed &quot;Julie&quot;
&quot;EILUJ&quot;
Prelude&gt; fmapped &quot;Chris&quot;
&quot;SIRHC&quot;</p>
<p>CHAPTER 22. FUNCTIONS WAITING FOR INPUT 1327
Now we want to return the results of capandrevboth, as a
tuple, like this:
Prelude&gt; tupled &quot;Julie&quot;
(&quot;JULIE&quot;,&quot;eiluJ&quot;)
-- or
Prelude&gt; tupled' &quot;Julie&quot;
(&quot;eiluJ&quot;,&quot;JULIE&quot;)
We will want to use an Applicative here. The type will look
like this:
tupled::[Char]-&gt;([Char], [Char])
There is no special reason such a function needs to be
monadic, but let’s do that, too, to get some practice. Do it
one time using dosyntax; then try writing a new version using
(&gt;&gt;=). The types will be the same as the type for tupled .
22.3 This is Reader
As we saw above, functions have Functor ,Applicative , andMonad
instances. Usually when you see or hear the term Reader, it’ll
be referring to the Monadinstance.
We use function composition because it lets us compose two
functions without explicitly having to recognize the argument
that will eventually arrive; the Functor of functions is function</p>
<p>CHAPTER 22. FUNCTIONS WAITING FOR INPUT 1328
composition. With the Functor of functions, we are able to map
an ordinary function over another to create a new function
awaiting a final argument. The Applicative andMonadinstances
for the function type give us a way to map a function that is
awaiting an 𝑎over another function that is also awaiting an 𝑎.
Giving it a name helps us know the what and why of what
we’re doing: reading an argument from the environment into
functions. It’ll be especially nice for clarity’s sake later when
we make the ReaderT monad transformer.
Exciting, right? Let’s back up here and go into more detail
about how Reader works.
22.4 Breaking down the Functor of
functions
If you type :info Functor in your REPL, one of the instances
you might notice is the one for the partially applied type
constructor of functions ((-&gt;) r) :
instance Functor ((-&gt;) r)
This can be a little confusing, so we’re going to unwind it
until hopefully it’s a bit more comfortable. First, let’s see what
we can accomplish with this:
Prelude&gt; fmap (+1) (*2) 3</p>
<p>CHAPTER 22. FUNCTIONS WAITING FOR INPUT 1329
7
-- Rearranging a little bit
Prelude&gt; fmap (+1) (*2) $ 3
7
Prelude&gt; (fmap (+1) (*2)) 3
7
This should look familiar:
Prelude&gt; (+1) . (*2) $ 3
7
Prelude&gt; (+2) . (*1) $ 2
4
Prelude&gt; fmap (+2) (*1) $ 2
4
Prelude&gt; (+2) <code>fmap</code> (*1) $ 2
4
Fortunately, there’s nothing weird going on here. If you
check the implementation of the instance in base, you’ll find
the following:</p>
<p>CHAPTER 22. FUNCTIONS WAITING FOR INPUT 1330
instance Functor ((-&gt;) r)where
fmap=(.)
Let’s unravel the types. Remember that (-&gt;)takes two argu-
ments and therefore has kind * -&gt; * -&gt; * . So, we know upfront
that we have to apply one of the type arguments before we
can have a Functor . With the Either Functor , we know that we
will lift over the Either a and if our function will be applied, it
will be applied to the 𝑏value. With the function type:
data(-&gt;) a b
the same rule applies: you have to lift over the (-&gt;) a and
only transform the 𝑏value. The 𝑎is conventionally called 𝑟
forReader in these instances, but a type variable of any other
name smells as sweet. Here, 𝑟is the first argument of (a -&gt; b) :</p>
<p>CHAPTER 22. FUNCTIONS WAITING FOR INPUT 1331
-- Type constructor of functions
(-&gt;)
-- Fully applied
a-&gt;b
((-&gt;) r)
-- is
r-&gt;
-- so r is the type of the
-- argument to the function
From this, we can determine that 𝑟, the argument type for
functions, is part of the structure being lifted over when we lift
over a function, not the value being transformed or mapped
over.
This leaves the result of the function as the value being
transformed. This happens to line up neatly with what func-
tion composition is about:
(.)::(b-&gt;c)-&gt;(a-&gt;b)-&gt;a-&gt;c
-- or perhaps
(.)::(b-&gt;c)-&gt;(a-&gt;b)-&gt;(a-&gt;c)
Now how does this line up with Functor ?</p>
<p>CHAPTER 22. FUNCTIONS WAITING FOR INPUT 1332
(.)::(b-&gt;c)-&gt;(a-&gt;b)-&gt;(a-&gt;c)
fmap::Functor f=&gt;(a-&gt;b)-&gt;f a-&gt;f b
We’ll remove the names of the functions and the typeclass
constraint as we can take them for granted from here on out:</p>
<p>CHAPTER 22. FUNCTIONS WAITING FOR INPUT 1333
::(b-&gt;c)-&gt;(a-&gt;b)-&gt;(a-&gt;c)
::(a-&gt;b)-&gt;f a-&gt;f b
-- Changing up the letters
-- without changing the meaning
::(b-&gt;c)-&gt;(a-&gt;b)-&gt;(a-&gt;c)
::(b-&gt;c)-&gt;f b-&gt;f c
-- f is ((-&gt;) a)
::(b-&gt;c)
-&gt; (a-&gt;b)
-&gt; (a-&gt;c)
::(b-&gt;c)
-&gt;((-&gt;) a) b
-&gt;((-&gt;) a) c
-- Unroll the prefix notation into infix
::(b-&gt;c)-&gt;(a-&gt;b)-&gt;(a-&gt;c)
::(b-&gt;c)-&gt;(a-&gt;b)-&gt;(a-&gt;c)
Bada bing. Functorial lifting for functions.</p>
<p>CHAPTER 22. FUNCTIONS WAITING FOR INPUT 1334
22.5 But uh, Reader?
Ah yes, right. Reader is a newtype wrapper for the function
type:
newtype Readerr a=
Reader{ runReader ::r-&gt;a }
The𝑟is the type we’re reading in and 𝑎is the result type of
our function.
TheReader newtype has a handy runReader accessor to get
the function out of Reader. Let us prove for ourselves that
this is the same thing, but with a touch of data constructor
jiggery-pokery mixed in. What does the Functor for this look
like compared to function composition?</p>
<p>CHAPTER 22. FUNCTIONS WAITING FOR INPUT 1335
instance Functor (Readerr)where
fmap::(a-&gt;b)
-&gt;Readerr a
-&gt;Readerr b
fmap f ( Readerra)=
Reader$\r-&gt;f (ra r)
-- same as (.)
compose ::(b-&gt;c)-&gt;(a-&gt;b)-&gt;(a-&gt;c)
compose f g=\x-&gt;f (g x)
-- see it?
\r-&gt;f (ra r)
\x-&gt;f (g x)
Basically the same thing right? In the Reader functor, rahas
the type r -&gt; a, andfhas the type a -&gt; b. Applying rato the
valueryields a value of type a, which fis then applied to,
yielding a value of type b. Function composition!
We can use the fact that we recognize this as function com-
position to make a slightly diﬀerent instance for Reader :</p>
<p>CHAPTER 22. FUNCTIONS WAITING FOR INPUT 1336
instance Functor (Readerr)where
fmap::(a-&gt;b)
-&gt;Readerr a
-&gt;Readerr b
fmap f ( Readerra)=
Reader$(f.ra)
So what we’re doing here is basically:
1.Unpack r -&gt; a out ofReader
2.Compose 𝑓with the function we unpacked out of Reader .
3.Put the new function made from the composition back
intoReader .
Without the Reader newtype, we drop steps 1 and 3 and have
function composition.
Exercise: Ask
Implement the following function. If you get stuck, remem-
ber it’s less complicated than it looks. Write down what you
know. What do you know about the type 𝑎? What does the
type simplify to? How many inhabitants does that type have?
You’ve seen the type before.
ask::Readera a
ask=Reader???</p>
<p>CHAPTER 22. FUNCTIONS WAITING FOR INPUT 1337
22.6 Functions have an Applicative too
We’ve seen a couple of examples already of the Applicative of
functions and how it works. Now we’ll get into the details.
The first thing we want to do is notice how the types spe-
cialize:
-- Applicative f =&gt;
-- f ~ (-&gt;) r
pure::a-&gt;f a
pure::a-&gt;(r-&gt;a)
(&lt;<em>&gt;)::f (a-&gt;b)
-&gt;f a
-&gt;f b
(&lt;</em>&gt;)::(r-&gt;a-&gt;b)
-&gt;(r-&gt;a)
-&gt;(r-&gt;b)
As we saw in the Functor instance, the 𝑟ofReader is part of
the𝑓structure. We have two arguments in this function, and
both of them are functions waiting for the 𝑟input. When that
comes, both functions will be applied to return a final result
of𝑏.</p>
<p>CHAPTER 22. FUNCTIONS WAITING FOR INPUT 1338
Demonstrating the function Applicative
This example is similar to other demonstrations we’ve done
previously in the book, but this time we’ll be aiming to show
you what specific use the Applicative of functions typically has.
We start with some newtypes for tracking our diﬀerent String
values:
newtype HumanName =
HumanName String
deriving (Eq,Show)
newtype DogName =
DogName String
deriving (Eq,Show)
newtype Address =
Address String
deriving (Eq,Show)
We do this so that our types are more self-explanatory, to
express intent, and so we don’t accidentally mix up our inputs.
A type like this:
String-&gt;String-&gt;String
is difficult when:</p>
<p>CHAPTER 22. FUNCTIONS WAITING FOR INPUT 1339
1.They aren’t strictly any string value.
2.They aren’t processed in an identical fashion. You don’t
handle addresses the same as names.
So make the diﬀerence explicit.
We’ll make two record types:
dataPerson=
Person{
humanName ::HumanName
, dogName ::DogName
, address ::Address
}deriving (Eq,Show)
dataDog=
Dog{
dogsName ::DogName
, dogsAddress ::Address
}deriving (Eq,Show)
The following are sample data to use. You can modify them
as you’d like:</p>
<p>CHAPTER 22. FUNCTIONS WAITING FOR INPUT 1340
pers::Person
pers=
Person(HumanName &quot;Big Bird&quot; )
(DogName &quot;Barkley&quot; )
(Address &quot;Sesame Street&quot; )
chris::Person
chris=Person(HumanName &quot;Chris Allen&quot; )
(DogName &quot;Papu&quot;)
(Address &quot;Austin&quot; )
And here is how we’d write it with and without Reader :
-- without Reader
getDog::Person-&gt;Dog
getDogp=
Dog(dogName p) (address p)
-- with Reader
getDogR ::Person-&gt;Dog
getDogR =
Dog&lt;$&gt;dogName &lt;*&gt;address
Can’t see the Reader ? What if we concrete the types a bit?</p>
<p>CHAPTER 22. FUNCTIONS WAITING FOR INPUT 1341
(&lt;$-&gt;&gt;)::(a-&gt;b)
-&gt;(r-&gt;a)
-&gt;(r-&gt;b)
(&lt;$-&gt;&gt;)=(&lt;$&gt;)
(&lt;<em>-&gt;&gt;)::(r-&gt;a-&gt;b)
-&gt;(r-&gt;a)
-&gt;(r-&gt;b)
(&lt;</em>-&gt;&gt;)=(&lt;<em>&gt;)
-- with Reader
getDogR' ::Person-&gt;Dog
getDogR' =
Dog&lt;$-&gt;&gt;dogName &lt;</em>-&gt;&gt;address
What we’re trying to highlight here is that Reader is not
alwaysReader , sometimes it’s the ambient Applicative orMonad
associated with the partially applied function type, here that
isr -&gt;.
The pattern of using Applicative in this manner is common,
so there’s an alternate way to do this using liftA2 :</p>
<p>CHAPTER 22. FUNCTIONS WAITING FOR INPUT 1342
importControl.Applicative (liftA2)
-- with Reader, alternate
getDogR' ::Person-&gt;Dog
getDogR' =
liftA2DogdogName address
Here’s the type of liftA2.
liftA2::Applicative f=&gt;
(a-&gt;b-&gt;c)
-&gt;f a-&gt;f b-&gt;f c
Again, we’re waiting for an input from elsewhere. Rather
than having to thread the argument through our functions,
we elide it and let the types manage it for us.
Exercise: Reading Comprehension
1.WriteliftA2 yourself. Think about it in terms of abstract-
ing out the diﬀerence between getDogR andgetDogR' if that
helps.
myLiftA2 ::Applicative f=&gt;
(a-&gt;b-&gt;c)
-&gt;f a-&gt;f b-&gt;f c
myLiftA2 =undefined</p>
<p>CHAPTER 22. FUNCTIONS WAITING FOR INPUT 1343
2.Write the following function. Again, it is simpler than it
looks.
asks::(r-&gt;a)-&gt;Readerr a
asksf=Reader???
3.Implement the Applicative forReader .
To write the Applicative instance for Reader, we’ll use an
extension called InstanceSigs . It’s an extension we need
in order to assert a type for the typeclass methods. You
ordinarily cannot assert type signatures in instances. The
compiler already knows the type of the functions, so it’s
not usually necessary to assert the types in instances any-
way. We did this for the sake of clarity, to make the Reader
type explicit in our signatures.</p>
<p>CHAPTER 22. FUNCTIONS WAITING FOR INPUT 1344
-- you'll need this pragma
{-# LANGUAGE InstanceSigs #-}
instance Applicative (Readerr)where
pure::a-&gt;Readerr a
pure a=Reader$ ???
(&lt;<em>&gt;)::Readerr (a-&gt;b)
-&gt;Readerr a
-&gt;Readerr b
(Readerrab)&lt;</em>&gt;(Readerra)=
Reader$\r-&gt; ???
Some instructions and hints.
a)When writing the purefunction for Reader , remember
that what you’re trying to construct is a function that
takes a value of type 𝑟, which you know nothing about,
and return a value of type 𝑎. Given that you’re not
really doing anything with 𝑟, there’s really only one
thing you can do.
b)We got the definition of the apply function started for
you, we’ll describe what you need to do and you write
the code. If you unpack the type of Reader’s apply
above, you get the following:</p>
<p>CHAPTER 22. FUNCTIONS WAITING FOR INPUT 1345
&lt;*&gt; ::(r-&gt;a-&gt;b)
-&gt;(r-&gt;a)
-&gt;(r-&gt;b)
-- contrast this with the type of fmap
fmap::(a-&gt;b)
-&gt;(r-&gt;a)
-&gt;(r-&gt;b)
So what’s the diﬀerence? The diﬀerence is that apply,
unlikefmap, also takes an argument of type 𝑟.
Make it so.
22.7 The Monadof functions
Functions also have a Monadinstance. You saw this in the be-
ginning of this chapter, and you perhaps have some intuition
now for how this must work. We’re going to walk through a
simplified demonstration of how it works before we get to the
types and instance. Feel free to work through this section as
quickly or slowly as you think appropriate to your own grasp
of what we’ve presented so far.
Let’s start by supposing that we could write a couple of
functions like so:</p>
<p>CHAPTER 22. FUNCTIONS WAITING FOR INPUT 1346
foo::(Functor f,Numa)=&gt;f a-&gt;f a
foor=fmap (+1) r
bar::Foldable f=&gt;t-&gt;f a-&gt;(t,Int)
barr t=(r, length t)
Now, as it happens in our program, we want to make one
function that will do both — increment the values inside our
structure and also tell us the length of the value. We could
write that like this:
froot::Numa=&gt;[a]-&gt;([a],Int)
frootr=(map (+1) r, length r)
Or we could write the same function by combining the
two functions we already had. As it is written above, bartakes
two arguments. We could write a version that takes only one
argument, so that both parts of the tuple apply to the same
argument. That is easy enough to do (notice the change in the
type signature as well):
barOne::Foldable t=&gt;t a-&gt;(t a,Int)
barOner=(r, length r)
That gave us the reduction to one argument that we wanted
but didn’t increment the values in the list as our foofunction
does. We can add that this way:</p>
<p>CHAPTER 22. FUNCTIONS WAITING FOR INPUT 1347
barPlus r=(foo r, length r)
But we can also do that more compactly by making (foo r)
the first argument to bar:
frooty::Numa=&gt;[a]-&gt;([a],Int)
frootyr=bar (foo r) r
Now we have an environment in which two functions are
waiting for the same argument to come in. They’ll both apply
to that argument in order to produce a final result.
Let’s make a small change to make it look a little more
Reader -y:
frooty' ::Numa=&gt;[a]-&gt;([a],Int)
frooty' =\r-&gt;bar (foo r) r
Then we abstract this out so that it’s not specific to these
functions:
fooBind m k=\r-&gt;k (m r) r
In this very polymorphic version, the type signature will
look like this:
fooBind ::(t2-&gt;t1)
-&gt;(t1-&gt;t2-&gt;t)
-&gt;t2
-&gt;t</p>
<p>CHAPTER 22. FUNCTIONS WAITING FOR INPUT 1348
So many 𝑡types! That’s because we can’t know very much
about those types once our function is that abstract. We can
make it a little more clear by making some substitutions. We’ll
use the 𝑟to represent the argument that both of our functions
are waiting on — the Reader -y part:
fooBind ::(r-&gt;a)
-&gt;(a-&gt;r-&gt;b)
-&gt;(r-&gt;b)
If we could take the 𝑟parts out, we might notice that fooBind
itself looks like a very abstract and simplified version of some-
thing we’ve seen before (overparenthesizing a bit, for clarity):
(&gt;&gt;=) :: Monad m =&gt;
m a -&gt; (a -&gt; (m b)) -&gt; m b
(r -&gt; a) -&gt; (a -&gt; (r -&gt; b)) -&gt; (r -&gt; b)
This is how we get to the Monadof functions. Just as with the
Functor andApplicative instances, the ((-&gt;) r) is our structure
— the𝑚in the type of (&gt;&gt;=). In the next section, we’ll work
forward from the types.
TheMonadinstance
As we noted, the 𝑟argument remains part of our (monadic)
structure:</p>
<p>CHAPTER 22. FUNCTIONS WAITING FOR INPUT 1349
(&gt;&gt;=)::Monadm
=&gt;m a-&gt;(a-&gt;m b) -&gt; m b
(&gt;&gt;=)::
(-&gt;) r a-&gt;(a-&gt;(-&gt;) r b)-&gt;(-&gt;) r b
(&gt;&gt;=)::
(r-&gt;a)-&gt;(a-&gt;r-&gt;b)-&gt;r-&gt;b
return::Monadm=&gt;a-&gt; m a
return:: a-&gt;(-&gt;) r a
return:: a-&gt;r-&gt;a
You may notice that return looks like a function we’ve seen
a lot of in this book.
Let’s look at it side by side with the Applicative :
(&lt;*&gt;)::(r-&gt;a-&gt;b)
-&gt;(r-&gt;a)
-&gt;(r-&gt;b)
(&gt;&gt;=)::(r-&gt;a)
-&gt;(a-&gt;r-&gt;b)
-&gt;(r-&gt;b)
Or with the flipped bind:</p>
<p>CHAPTER 22. FUNCTIONS WAITING FOR INPUT 1350
(&lt;*&gt;)::(r-&gt;a-&gt;b)
-&gt;(r-&gt;a)
-&gt;(r-&gt;b)
(=&lt;&lt;)::(a-&gt;r-&gt;b)
-&gt;(r-&gt;a)
-&gt;(r-&gt;b)
So you’ve got this ever-present type 𝑟following your func-
tions around like a lonely puppy.
Example uses of the Reader type
Remember the earlier example with Person andDog? Here’s the
same but with the Reader Monad anddosyntax:
-- with Reader Monad
getDogRM ::Person-&gt;Dog
getDogRM = do
name&lt;-dogName
addy&lt;-address
return$Dogname addy
Exercise: Reader Monad
1.Implement the Reader Monad .</p>
<p>CHAPTER 22. FUNCTIONS WAITING FOR INPUT 1351
-- Don't forget instancesigs.
instance Monad(Readerr)where
return=pure
(&gt;&gt;=)::Readerr a
-&gt;(a-&gt;Readerr b)
-&gt;Readerr b
(Readerra)&gt;&gt;=aRb=
Reader$\r-&gt; ???
Hint: constrast the type with the Applicative instance and
perform the most obvious change you can imagine to
make it work.
2.Rewrite the monadic getDogRM to use your Reader datatype.
22.8Reader Monad by itself is boring
It can’t do anything the Applicative cannot.</p>
<p>CHAPTER 22. FUNCTIONS WAITING FOR INPUT 1352
{-# LANGUAGE NoImplicitPrelude #-}
modulePrettyReader where
flip::(a-&gt;b-&gt;c)-&gt;(b-&gt;a-&gt;c)
flipf a b=f b a
const::a-&gt;b-&gt;a
consta b=a
(.)::(b-&gt;c)-&gt;(a-&gt;b)-&gt;(a-&gt;c)
f.g=\a-&gt;f (g a)
classFunctor fwhere
fmap::(a-&gt;b)-&gt;f a-&gt;f b
classFunctor f=&gt;Applicative fwhere
pure::a-&gt;f a
(&lt;*&gt;)::f (a-&gt;b)-&gt;f a-&gt;f b
classApplicative f=&gt;Monadfwhere
return::a-&gt;f a
(&gt;&gt;=)::f a-&gt;(a-&gt;f b)-&gt;f b</p>
<p>CHAPTER 22. FUNCTIONS WAITING FOR INPUT 1353
instance Functor ((-&gt;) r)where
fmap=(.)
instance Applicative ((-&gt;) r)where
pure=const
f&lt;<em>&gt;a=\r-&gt;f r (a r)
instance Monad((-&gt;) r)where
return=pure
m&gt;&gt;=k=flip k&lt;</em>&gt;m
Speaking generally in terms of the algebras alone, you can-
not get a Monadinstance from the Applicative . You can get
anApplicative from the Monad. However, our instances above
aren’t in terms of an abstract datatype; we know it’s the type
of functions. Because it’s not hiding behind a Reader newtype,
we can use flipandapplyto make the Monadinstance. We need
specific type information to augment what the Applicative is
capable of before we can get our Monadinstance.</p>
<p>CHAPTER 22. FUNCTIONS WAITING FOR INPUT 1354
22.9 You can change what comes below,
but not above
The “read-only” nature of the type argument 𝑟means that you
can swap in a diﬀerent type or value of 𝑟for functions that
you call, but not for functions that call you. The best way to
demonstrate this is with the withReaderT function which lets
us start a new Reader context with a diﬀerent argument being
provided:
withReaderT
::(r'-&gt;r)
-- ^ The function to modify
-- the environment.
-&gt;ReaderT r m a
-- ^ Computation to run in the
-- modified environment.
-&gt;ReaderT r' m a
withReaderT f m=
ReaderT $runReaderT m .f
In the next chapter, we’ll see the Statemonad where we can
not only read in a value, but provide a new one which will
change the value carried by the functions that called us, not
only those we called.</p>
<p>CHAPTER 22. FUNCTIONS WAITING FOR INPUT 1355
22.10 You tend to see ReaderT , notReader
Reader rarely stands alone. Usually it’s one Monadin a stack of
multiple types providing a Monadinstance such as with a web
application that uses Reader to give you access to context about
the HTTP request. When used in that fashion, it’s a monad
transformer and we put a letter T after the type to indicate
when we’re using it as such, so you’ll usually see ReaderT in
production Haskell code rather than Reader .
Further, a Reader ofIntisn’t all that useful or compelling.
Usually if you have a Reader , it’s of a record of several (possibly
many) values that you’re getting out of the Reader .
22.11 Chapter Exercises
A warm-up stretch
These exercises are designed to be a warm-up and get you
using some of the stuﬀ we’ve learned in the last few chap-
ters. While these exercises comprise code fragments from
real code, they are simplified in order to be discrete exercises.
That will allow us to highlight and practice some of the type
manipulation from Traversable andReader, both of which are
tricky.
The first simplified part is that we’re going to set up some
toy data; in the real programs these are taken from, the data</p>
<p>CHAPTER 22. FUNCTIONS WAITING FOR INPUT 1356
is coming from somewhere else — a database, for example.
We just need some lists of numbers. We’re going to use some
functions from Control.Applicative andData.Maybe , so we’ll im-
port those at the top of our practice file. We’ll call our lists of
toy data by common variable names for simplicity.
moduleReaderPractice where
importControl.Applicative
importData.Maybe
x=[1,2,3]
y=[4,5,6]
z=[7,8,9]
The next thing we want to do is write some functions that
zip those lists together and use lookup to find the value associ-
ated with a specified key in our zipped lists. For demonstration
purposes, it’s nice to have the outputs be predictable, so we
recommend writing some that are concrete values, as well as
one that can be applied to a variable:
lookup::Eqa=&gt;a-&gt;[(a, b)] -&gt;Maybeb
-- zip x and y using 3 as the lookup key
xs::MaybeInteger
xs=undefined</p>
<p>CHAPTER 22. FUNCTIONS WAITING FOR INPUT 1357
-- zip y and z using 6 as the lookup key
ys::MaybeInteger
ys=undefined
-- it's also nice to have one that
-- will return Nothing, like this one
-- zip x and y using 4 as the lookup key
zs::MaybeInteger
zs=lookup4$zip x y
-- now zip x and z using a
-- variable lookup key
z'::Integer -&gt;MaybeInteger
z'n=undefined
Now we want to add the ability to make a Maybe (,) of values
usingApplicative . Have x1make a tuple of xsandys, andx2
make a tuple of of ysandzs. Also, write x3which takes one
input and makes a tuple of the results of two applications of
z'from above.</p>
<p>CHAPTER 22. FUNCTIONS WAITING FOR INPUT 1358
x1::Maybe(Integer,Integer)
x1=undefined
x2::Maybe(Integer,Integer)
x2=undefined
x3::Integer
-&gt;(MaybeInteger,MaybeInteger)
x3=undefined
Your outputs from those should look like this:
*ReaderPractice&gt; x1
Just (6,9)
*ReaderPractice&gt; x2
Nothing
*ReaderPractice&gt; x3 3
(Just 9,Just 9)
Next, we’re going to make some helper functions. Let’s use
uncurry to allow us to add the two values that are inside a tuple:</p>
<p>CHAPTER 22. FUNCTIONS WAITING FOR INPUT 1359
uncurry ::(a-&gt;b-&gt;c)-&gt;(a, b)-&gt;c
-- that first argument is a function
-- in this case, we want it to be addition
-- summed is uncurry with addition as
-- the first argument
summed::Numc=&gt;(c, c)-&gt;c
summed=undefined
And now we’ll make a function similar to some we’ve seen
before that lifts a boolean function over two partially applied
functions:
bolt::Integer -&gt;Bool
-- use &amp;&amp;, &gt;3, &lt;8
bolt=undefined
Finally, we’ll be using fromMaybe in themainexercise, so let’s
look at that:
fromMaybe ::a-&gt;Maybea-&gt;a
You give it a default value and a Maybevalue. If the Maybe
value is a Just a, it will return the 𝑎value. If the value is a
Nothing , it returns the default value instead:
*ReaderPractice&gt; fromMaybe 0 xs</p>
<p>CHAPTER 22. FUNCTIONS WAITING FOR INPUT 1360
6
<em>ReaderPractice&gt; fromMaybe 0 zs
0
Now we’ll cobble together a main, so that in one call we can
execute several things at once.
main::IO()
main= do
print$
sequenceA [ Just3,Just2,Just1]
print$sequenceA [x, y]
print$sequenceA [xs, ys]
print$summed&lt;$&gt;((,)&lt;$&gt;xs&lt;</em>&gt;ys)
print$fmap summed ((,) &lt;$&gt;xs&lt;*&gt;zs)
print$bolt7
print$fmap bolt z
When you run this in GHCi, your results should look like
this:
*ReaderPractice&gt; main
Just [3,2,1]
[[1,4],[1,5],[1,6],[2,4],[2,5],[2,6],[3,4],[3,5],[3,6]]
Just [6,9]
Just 15
Nothing</p>
<p>CHAPTER 22. FUNCTIONS WAITING FOR INPUT 1361
True
[True,False,False]
Next, we’re going to add one that combines sequenceA and
Reader in a somewhat surprising way (add this to main):
print$sequenceA [( &gt;3), (&lt;8), even] 7
The type of sequenceA is
sequenceA ::(Applicative f,Traversable t)
=&gt;t (f a) -&gt;f (t a)
-- so in this:
sequenceA [(&gt;3), (&lt;8), even] 7
-- f ~ (-&gt;) a and t ~ []
Wehavea Reader fortheApplicative (functions)andatraversable
for the list. Pretty handy. We’re going to call that function
sequAfor the purposes of the following exercises:
sequA::Integral a=&gt;a-&gt;[Bool]
sequAm=sequenceA [( &gt;3), (&lt;8), even] m
And henceforth let
summed&lt;$&gt;((,)&lt;$&gt;xs&lt;*&gt;ys)</p>
<p>CHAPTER 22. FUNCTIONS WAITING FOR INPUT 1362
be known as s'.
OK, your turn. Within the mainabove, write the following
(you can delete everything after donow if you prefer — just
remember to use printto be able to print the results of what
you’re adding):
1.fold the boolean conjunction operator over the list of
results of sequA(applied to some value).
2.applysequAtos'; you’ll need fromMaybe .
3.applybolttoys; you’ll need fromMaybe .
Rewriting Shawty
Remember the URL shortener? Instead of manually passing
the database connection rConnfrommainto the app function
that generates a Scotty app, use ReaderT to make the database
connection available. We know you haven’t seen the trans-
former variant yet and we’ll explain them soon, but you should
try to do the transformation mechanically. Research as neces-
sary using a search engine. Use this version of the app: https:
//github.com/bitemyapp/shawty-prime/blob/master/app/Main.hs
22.12 Definition
A monad transformer is a special type that takes a monad as
an argument and returns a monad as a result. It allows us to</p>
<p>CHAPTER 22. FUNCTIONS WAITING FOR INPUT 1363
combine two monads into one that shares the behaviors of
both, such as allowing us to add exception handling to a State
monad. It is somewhat common to create a stack of transform-
ers to create one large monad that has features from several
monads, for example, rolling Reader,Either, andIOtogether
to get a monad that captures the behavior of waiting for an
argument that will get passed around to multiple functions
but is likely to come in via some kind of I/O action and has the
possibility of failure we might like to catch. Often this stack
will be given a type alias for convenience.
22.13 Follow-up resources
1.Reader Monad; All About Monads
https://wiki.haskell.org/All_About_Monads
2.Reader Monad; Programming with Monads; Real World
Haskell</p>
<p>Chapter 23
State
Four centuries ago,
Descartes pondered the
mind-body problem:
how can incorporeal
minds interact with
physical bodies? Today,
computing scientists face
their own version of the
mind-body problem:
how can virtual software
interact with the real
world?
Philip Wadler
1364</p>
<p>CHAPTER 23. STATE 1365
23.1 State
What if I need state? In Haskell we have many means of repre-
senting, accessing, and modifying state. We can think of state
as data that exists in addition to the inputs and outputs of our
functions, data that can potentially change after each function
is evaluated.
In this chapter, we will:
•talk about what state means;
•explore some ways of handling state in Haskell;
•generate some more random numbers;
•and examine the Statenewtype and Monadinstance.
23.2 What is state?
The concept of state originates in the circuit and automata
theory that much of computer science and programming be-
gan with. The simplest form of state could be understood as a
light switch. A light switch has two possible states, on or oﬀ.
That disposition of the light switch, being on or oﬀ, could be
understood as its state. Similarly, transistors in computers
have binary states of being on or oﬀ. This is a very low-level
way of seeing it, but this maps onto the state that exists in
computer memory.</p>
<p>CHAPTER 23. STATE 1366
In most imperative programming languages, this stateful-
ness is pervasive, implicit, and not referenced in the types
of your functions. In Haskell, we’re not allowed to secretly
change some value; all we can do is accept arguments and
return a result. The Statetype in Haskell is a means of express-
ing state that may change in the course of evaluating code
without resort to mutation. The monadic interface for State
is, much as you’ve seen already, more of a convenience than a
strict necessity for working with State.
We have the option to capture the idea and convenience
of a value which potentially changes with each computation
without resorting to mutability. Statecaptures this idea and
cleans up the bookkeeping required. If you need in-place
mutation, then the STtype is what you want, and we address
that briefly in later chapters.
In Haskell, if we use the Statetype and its associated Monad
(for convenience, not strictly necessary), we can have state
which:
1.doesn’t require IO;
2.is limited only to the data in our Statecontainer;
3.maintains referential transparency;
4.is explicit in the types of our functions.</p>
<p>CHAPTER 23. STATE 1367
There are other means of sharing data within a program
that are designed for diﬀerent needs than the Statedatatype
itself.Stateis appropriate when you want to express your
program in terms of values that potentially vary with each
evaluation step, which can be read and modified, but don’t
otherwise have specific operational constraints.
23.3 Random numbers
As we did in the previous chapter, we’ll start with an extended
example. This will help you get an idea of the problem we’re
trying to solve with the Statedatatype.
We’ll be using the random1library, version 1.1, in this ex-
ample.
First, let’s give an overview of some of the functions we’ll
be using here. We used the System.Random library back in the
chapter where we built the hangman game, but we’ll be using
some diﬀerent functions for this example. This is in broad
strokes; it isn’t meant to go into great detail about how these
generators work.
System.Random is designed to generate pseudorandom values.
You can generate those values through providing a seed value
or by using the system-initialised generator. We’ll be using
the following from that library:
1https://hackage.haskell.org/package/random</p>
<p>CHAPTER 23. STATE 1368
1.One of the types we’ll be seeing here, StdGen , is a datatype
that is a product of two Int32values. So a value of type
StdGen always comprises two Int32values. They are the
seed values used to generate the next random number.
2.mkStdGen has the type:
mkStdGen ::Int-&gt;StdGen
We’ll ignore the implementation at this point because
those details aren’t important here. The idea is that it takes
anIntargument and maps it into a generator to return a
value of type StdGen , which is a pair of Int32values.
3.nexthas the type:
next::g-&gt;(Int, g)
where𝑔is a value of type StdGen . TheIntthat is first in the
tuple is the pseudorandom number generated from the
StdGen value; the second value is a new StdGen value.
4.random has the type:
random::(RandomGen g,Randoma)
=&gt;g-&gt;(a, g)
This is similar to nextbut allows us to generate random
values that aren’t numbers. The range generated will be
determined by the type.</p>
<p>CHAPTER 23. STATE 1369
Now, let’s have a little demonstration of these:
Prelude&gt; import System.Random
Prelude&gt; mkStdGen 0
1 1
Prelude&gt; :t mkStdGen 0
mkStdGen 0 :: StdGen
Prelude&gt; let sg = mkStdGen 0
Prelude&gt; :t next sg
next sg :: (Int, StdGen)
Prelude&gt; next sg
(2147482884,40014 40692)
Prelude&gt; next sg
(2147482884,40014 40692)
We get the same answer twice because the underlying func-
tionthat’sdecidingthevaluesreturnedispure; thetypedoesn’t
permit the performance of any eﬀects to get spooky action.
Define a new version of sgthat provides a diﬀerent input value
tomkStdGen and see what happens.
So, we have a value called next sg . Now, if we want to use
that to generate the next random number, we need to feed the
StdGen value from that tuple to nextagain. We can use sndto
extract that StdGen value and pass it as an input to next:
Prelude&gt; snd (next sg)
40014 40692</p>
<p>CHAPTER 23. STATE 1370
Prelude&gt; let newSg = snd (next sg)
Prelude&gt; :t newSg
newSg :: StdGen
Prelude&gt; next newSg
(2092764894,1601120196 1655838864)
You’ll keep getting the same results of nextthere, but you
can extract that StdGen value and pass it to nextagain to get a
new tuple:
Prelude&gt; next (snd (next newSg))
(1679949200,1635875901 2103410263)
Now we’ll look at a few examples using random. Because
random can generate values of diﬀerent types, we need to specify
the type to use:
Prelude&gt; :t random newSg
random newSg :: Random a =&gt; (a, StdGen)
Prelude&gt; random newSg :: (Int, StdGen)
(138890298504988632,439883729 1872071452)
Prelude&gt; random newSg :: (Double, StdGen)
(0.41992072972993366,439883729 1872071452)
Simple enough, but what if we want a number within a
range?</p>
<p>CHAPTER 23. STATE 1371
Prelude&gt; :t randomR
randomR :: (RandomGen g, Random a) =&gt; (a, a) -&gt; g -&gt; (a, g)
Prelude&gt; randomR (0, 3) newSg :: (Int, StdGen)
(1,1601120196 1655838864)
Prelude&gt; randomR (0, 3) newSg :: (Double, StdGen)
(1.259762189189801,439883729 1872071452)
We have to pass the new state of the random number gen-
erator to the nextfunction to get a new value:
Prelude&gt; let rx :: (Int, StdGen); rx = random (snd sg3)
Prelude&gt; rx
(2387576047905147892,1038587761 535353314)
Prelude&gt; snd rx
1038587761 535353314
This chaining of state can get tedious. Addressing this te-
dium is our aim in this chapter.
23.4 The Statenewtype
Stateis defined in a newtype, like Reader in the previous chap-
ter, and that type looks like this:
newtype States a=
State{ runState ::s-&gt;(a, s) }</p>
<p>CHAPTER 23. STATE 1372
It’s initially a bit strange looking, but you might notice some
similarity to the Reader newtype:
newtype Readerr a=
Reader{ runReader ::r-&gt;a }
We’ve seen several newtypes whose contents are a function,
particularly with our Monoid newtypes ( Sum,Product , etc.). New-
types must have the same underlying representation as the
type they wrap, as the newtype wrapper disappears at compile
time. So the function contained in the newtype must be iso-
morphic to the type it wraps. That is, there must be a way to go
from the newtype to the thing it wraps and back again without
losing information. For example, the following demonstrates
an isomorphism:
typeIsoa b=(a-&gt;b, b-&gt;a)
newtype Suma=Sum{ getSum ::a }
sumIsIsomorphicWithItsContents
::Isoa (Suma)
sumIsIsomorphicWithItsContents =
(Sum, getSum)
Whereas the following do not:</p>
<p>CHAPTER 23. STATE 1373
-- Not an isomorphism, because
-- it might not work.
(a-&gt;Maybeb, b-&gt;Maybea)
-- Not an isomorphism for two reasons.
-- You lose information whenever there
-- was more than one element in [a]. Also,
-- [a] -&gt; a is partial because there
-- might not be any elements.
[a]-&gt;a, a-&gt;[a]
With that in mind, let us look at the Statedata constructor
andrunState record accessor as our means of putting a value
in and taking a value out of the Statetype:
State::(s-&gt;(a, s)) -&gt;States a
runState ::States a-&gt;s-&gt;(a, s)
Stateis a function that takes input state and returns an out-
put value, 𝑎, tupled with the new state value. The key is that
the previous state value from each application is chained to
the next one, and this is not an uncommon pattern. Stateis
often used for things like random number generators, solvers,
games, and carrying working memory while traversing a data
structure. The polymorphism means you don’t have to make
a new state for each possible instantiation of 𝑠and𝑎.</p>
<p>CHAPTER 23. STATE 1374
Let’s get back to our random numbers:
Note that random looks an awful lot like Statehere:
random::(Randoma)
=&gt;StdGen-&gt;(a,StdGen)
State{ runState
::s-&gt;(a, s) }
If we look at the type of randomR , once partially applied, it
should also remind you of State:
randomR ::(...)=&gt;(a, a)-&gt;g-&gt;(a, g)
State{ runState :: s-&gt;(a, s) }
23.5 Throw down
Now let us use this kit to generate die such as for a game:</p>
<p>CHAPTER 23. STATE 1375
moduleRandomExample where
importSystem.Random
-- Six-sided die
dataDie=
DieOne
|DieTwo
|DieThree
|DieFour
|DieFive
|DieSix
deriving (Eq,Show)
As you might expect, we’ll be using the random library, and a
simpleDiedatatype to represent a six-sided die.</p>
<p>CHAPTER 23. STATE 1376
intToDie ::Int-&gt;Die
intToDie n=
casenof
1-&gt;DieOne
2-&gt;DieTwo
3-&gt;DieThree
4-&gt;DieFour
5-&gt;DieFive
6-&gt;DieSix
-- Use 'error'
-- <em>extremely</em> sparingly.
x-&gt;
error$
&quot;intToDie got non 1-6 integer: &quot;
++show x
Don’t use erroroutside of experiments like this, or in cases
where the branch you’re ignoring is provably impossible. We
do not use the word provably here lightly.2
Now we need to roll the dice:
2Because partial functions are a pain, you should only use an error like this when
the branch that would spawn the error can literally never happen. Unexpected software
failures are often due to things like this. It is also completely unnecessary in Haskell; we
have good alternatives, like using MaybeorEither. The only reason we didn’t here is to
keep it simple and focus attention on the State Monad .</p>
<p>CHAPTER 23. STATE 1377
rollDieThreeTimes ::(Die,Die,Die)
rollDieThreeTimes = do
lets=mkStdGen 0
(d1, s1) =randomR ( 1,6) s
(d2, s2) =randomR ( 1,6) s1
(d3,_)=randomR ( 1,6) s2
(intToDie d1, intToDie d2, intToDie d3)
This code isn’t optimal, but it does work. It will produce
the same results every time, because it is free of eﬀects, but
you can make it produce a new result on a new dice roll if you
modify the start value. Try it a couple of times to see what we
mean. It seems unlikely that this will develop into a gambling
addiction, but in the event it does, the authors disclaim liability
for such.
So, how can we improve our suboptimal code there. With
State, of course!
moduleRandomExample2 where
importControl.Applicative (liftA3)
importControl.Monad (replicateM )
importControl.Monad.Trans.State
importSystem.Random
importRandomExample</p>
<p>CHAPTER 23. STATE 1378
First, we’ll add some new imports. You’ll need transformers
to be installed for the Stateimport to work, but that should
have come with your GHC install, so you should be good to
go.
UsingStatewill allow us to factor out the generation of a
singleDie:
rollDie ::StateStdGenDie
rollDie =state$ do
(n, s)&lt;-randomR ( 1,6)
return (intToDie n, s)
For our purposes, the statefunction is a constructor that
takes aState-like function and embeds it in the Statemonad
transformer. Ignore the transformer part for now — we’ll get
there. The state function has the following type:
state::Monadm
=&gt;(s-&gt;(a, s))
-&gt;StateTs m a
Note that we’re binding the result of randomR out of the State
monad the doblock is in rather than using let. This is still more
verbose than is necessary. We can lift our intToDie function
over the State:</p>
<p>CHAPTER 23. STATE 1379
rollDie' ::StateStdGenDie
rollDie' =
intToDie &lt;$&gt;state (randomR ( 1,6))
State StdGen had a final type argument of Int. We lifted Int
-&gt; Die over it and transformed that final type argument to Die.
We’ll exercise more brevity upfront in the next function:
rollDieThreeTimes'
::StateStdGen(Die,Die,Die)
rollDieThreeTimes' =
liftA3 (,,) rollDie rollDie rollDie
Lifting the three-tuple constructor over three Stateactions
that produce Dievalues when given an initial state to work
with. How does this look in practice?
Prelude&gt; evalState rollDieThreeTimes' (mkStdGen 0)
(DieSix,DieSix,DieFour)
Prelude&gt; evalState rollDieThreeTimes' (mkStdGen 1)
(DieSix,DieFive,DieTwo)
Seems to work fine. Again, the same inputs give us the same
result. What if we want a list of Dieinstead of a tuple?</p>
<p>CHAPTER 23. STATE 1380
-- Seems appropriate?
repeat::a-&gt;[a]
infiniteDie ::StateStdGen[Die]
infiniteDie =repeat&lt;$&gt;rollDie
Does this infiniteDie function do what we want or expect?
What is it repeating?
Prelude&gt; take 6 $ evalState infiniteDie (mkStdGen 0)
[DieSix,DieSix,DieSix,DieSix,DieSix,DieSix]
We already know based on previous inputs that the first 3
values shouldn’t be identical for a seed value of 0. So what
happened? What happened is we repeated a single die value
— we didn’t repeat the state action that produces a die. This is
what we need:
replicateM ::Monadm
=&gt;Int-&gt;m a-&gt;m [a]
nDie::Int-&gt;StateStdGen[Die]
nDien=replicateM n rollDie
And when we use it?
Prelude&gt; evalState (nDie 5) (mkStdGen 0)</p>
<p>CHAPTER 23. STATE 1381
[DieSix,DieSix,DieFour,DieOne,DieFive]
Prelude&gt; evalState (nDie 5) (mkStdGen 1)
[DieSix,DieFive,DieTwo,DieSix,DieFive]
We get precisely what we wanted.
Keep on rolling
In the following example, we keep rolling a single die until we
reach or exceed a sum of 20.
rollsToGetTwenty ::StdGen-&gt;Int
rollsToGetTwenty g=go00g
where
go::Int-&gt;Int-&gt;StdGen-&gt;Int
go sum count gen
|sum&gt;=20=count
|otherwise =
let(die, nextGen) =
randomR ( 1,6) gen
ingo (sum +die)
(count+1) nextGen
Then seeing it in action:
Prelude&gt; rollsToGetTwenty (mkStdGen 0)
5</p>
<p>CHAPTER 23. STATE 1382
Prelude&gt; rollsToGetTwenty (mkStdGen 0)
5
We can also use randomIO , which uses IOto get a new value
each time without needing to create a unique value for the
StdGen :
Prelude&gt; :t randomIO
randomIO :: Random a =&gt; IO a
Prelude&gt; (rollsToGetTwenty . mkStdGen) &lt;$&gt; randomIO
6
Prelude&gt; (rollsToGetTwenty . mkStdGen) &lt;$&gt; randomIO
7
Under the hood, it’s the same interface and State Monad
driven mechanism, but it’s mutating a single globally used
StdGen to walk the generator forward on each use. See the
random library source code to see how this works.
Exercises: Roll Your Own
1.Refactor rollsToGetTwenty into having the limit be a func-
tion argument.
rollsToGetN ::Int-&gt;StdGen-&gt;Int
rollsToGetN =undefined
2.Change rollsToGetN to recording the series of die that oc-
curred in addition to the count.</p>
<p>CHAPTER 23. STATE 1383
rollsCountLogged ::Int
-&gt;StdGen
-&gt;(Int, [Die])
rollsCountLogged =undefined
23.6 Write Statefor yourself
Use the datatype definition from the beginning of this chapter,
with the name changed to avoid conflicts in case you have
Stateimported from the libraries transformers ormtl. We’re
calling it Moi, because we enjoy allusions to famous quotations3;
feel free to change the name if you wish to protest absolute
monarchy, but change them consistently throughout.
newtype Mois a=
Moi{ runMoi ::s-&gt;(a, s) }
State Functor
Implement the Functor instance for State.
instance Functor (Mois)where
fmap::(a-&gt;b)-&gt;Mois a-&gt;Mois b
fmap f ( Moig)= ???
3We are referring to the (possibly apocryphal) quotation attributed to the French
King Louis XIV, “L’Etat, c’est moi.” For those of you who do not speak French, it means,
“I am the State.” Cheers.</p>
<p>CHAPTER 23. STATE 1384
Prelude&gt; runMoi ((+1) &lt;$&gt; (Moi $ \s -&gt; (0, s))) 0
(1,0)
State Applicative
Write the Applicative instance for State.
instance Applicative (Mois)where
pure::a-&gt;Mois a
pure a= ???
(&lt;<em>&gt;)::Mois (a-&gt;b)
-&gt;Mois a
-&gt;Mois b
(Moif)&lt;</em>&gt;(Moig)=
???
State Monad
Write the Monadinstance for State.</p>
<p>CHAPTER 23. STATE 1385
instance Monad(Mois)where
return=pure
(&gt;&gt;=)::Mois a
-&gt;(a-&gt;Mois b)
-&gt;Mois b
(Moif)&gt;&gt;=g=
???
23.7 Get a coding job with one weird
trick
Some companies will use FizzBuzz4to screen (not so much
test) candidates applying to software positions. The problem
statement goes:
Write a program that prints the numbers from 1 to
100. But for multiples of three print “Fizz” instead of
the number and for the multiples of five print “Buzz”.
For numbers which are multiples of both three and
five print “FizzBuzz”.
A typical fizzbuzz solution in Haskell looks something like:
4http://c2.com/cgi/wiki?FizzBuzzTest</p>
<p>CHAPTER 23. STATE 1386
fizzBuzz ::Integer -&gt;String
fizzBuzz n|n <code>mod</code> 15==0=&quot;FizzBuzz&quot;
|n <code>mod</code> 5==0=&quot;Buzz&quot;
|n <code>mod</code> 3==0=&quot;Fizz&quot;
|otherwise =show n
main::IO()
main=
mapM_ (putStrLn .fizzBuzz) [ 1..100]
A fizzbuzz using Stateis a suitable punishment for asking
a software candidate to write this in person after presumably
getting through a couple phone screens. Let’s look at what a
version with Statemight look like:
importControl.Monad
importControl.Monad.Trans.State
fizzBuzz ::Integer -&gt;String
fizzBuzz n|n <code>mod</code> 15==0=&quot;FizzBuzz&quot;
|n <code>mod</code> 5==0=&quot;Buzz&quot;
|n <code>mod</code> 3==0=&quot;Fizz&quot;
|otherwise =show n</p>
<p>CHAPTER 23. STATE 1387
fizzbuzzList ::[Integer]-&gt;[String]
fizzbuzzList list=
execState (mapM_ addResult list) []
addResult ::Integer -&gt;State<a href="HaskellProgramming/">String</a>
addResult n= do
xs&lt;-get
letresult=fizzBuzz n
put (result :xs)
Note that Stateis a type alias of StateT you imported.
main::IO()
main=
mapM_ putStrLn $
reverse $fizzbuzzList [ 1..100]
The good part here is that we’re collecting data initially
before dumping the results to standard output via putStrLn .
The bad is that we’re reversing a list. Reversing singly-linked
lists is not great, even in Haskell, and won’t terminate on an
infinite list. One of the issues is that we’re accepting an input
that defines the numbers we’ll use fizzbuzz on linearly from
beginning to end.
There are a couple ways we could handle this. One is to
use a data structure with cheaper appending to the end. Using
(++)recursively can be very slow, so let’s use something that</p>
<p>CHAPTER 23. STATE 1388
can append in constant time. The counterpart to []which has
this property is the diﬀerence list5which has O(1) append.
importControl.Monad
importControl.Monad.Trans.State
-- http://hackage.haskell.org/package/dlist
import qualified Data.DList asDL
fizzBuzz ::Integer -&gt;String
fizzBuzz n|n <code>mod</code> 15==0=&quot;FizzBuzz&quot;
|n <code>mod</code> 5==0=&quot;Buzz&quot;
|n <code>mod</code> 3==0=&quot;Fizz&quot;
|otherwise =show n
5https://github.com/spl/dlist</p>
<p>CHAPTER 23. STATE 1389
fizzbuzzList ::[Integer]-&gt;[String]
fizzbuzzList list=
letdlist=
execState (mapM_ addResult list)
DL.empty
-- convert back to normal list
inDL.apply dlist []
addResult ::Integer
-&gt;State(DL.DListString)()
addResult n= do
xs&lt;-get
letresult=fizzBuzz n
-- snoc appends to the end, unlike
-- cons which adds to the front
put (DL.snoc xs result)
main::IO()
main=
mapM_ putStrLn $fizzbuzzList [ 1..100]
We can clean this up further. If you have GHC 7.10 or newer,
mapM_will specify a Foldable type, not only a list:
Prelude&gt; :t mapM_
mapM_ :: (Monad m, Foldable t) =&gt; (a -&gt; m b) -&gt; t a -&gt; m ()</p>
<p>CHAPTER 23. STATE 1390
By letting DList’sFoldable instance do the conversion to a
list for us, we can eliminate some code:
fizzbuzzList ::[Integer]
-&gt;DL.DListString
fizzbuzzList list=
execState (mapM_ addResult list) DL.empty
addResult ::Integer
-&gt;State(DL.DListString)()
addResult n= do
xs&lt;-get
letresult=fizzBuzz n
put (DL.snoc xs result)
main::IO()
main=
mapM_ putStrLn $fizzbuzzList [ 1..100]
DList’sFoldable instance converts to a list before folding
because of limitations specific to the datatype. You get cheap
appending, but you give up the ability to “see” what you’ve
built unless you’re willing to do all the work of building the
structure. We’ll discuss this in more detail in a forthcoming
chapter.</p>
<p>CHAPTER 23. STATE 1391
One thing that may strike you here is that the use of State
was superfluous. That’s good! It’s not common you need State
as such in Haskell. You might use a diﬀerent form of State
calledSTas a selective optimization, but Stateitself is a stylistic
choice that falls out of what the code is telling you. Don’t
feel compelled to use or not use State. Please frighten some
interviewers with a spooky fizzbuzz. Make something even
weirder than what we’ve shown you here!
Fizzbuzz Diﬀerently
It’s an exercise! Rather than changing the underlying data
structure, fix our reversing fizzbuzz by changing the code in
the following way:
fizzbuzzFromTo ::Integer
-&gt;Integer
-&gt;[String]
fizzbuzzFromTo =undefined
Continue to use consing in the construction of the result
list, but have it come out in the right order to begin with by
enumerating the sequence backwards. This sort of tactic is
more commonly how you’ll want to fix your code when you’re
quashing unnecessary reversals.</p>
<p>CHAPTER 23. STATE 1392
23.8 Chapter exercises
Write the following functions. You’ll want to use your own
Statetype for which you’ve defined the Functor ,Applicative ,
andMonad.
1.Construct a Statewhere the state is also the value you
return.
get::States s
get= ???
Expected output
Prelude&gt; runState get &quot;curryIsAmaze&quot;
(&quot;curryIsAmaze&quot;,&quot;curryIsAmaze&quot;)
2.Construct a Statewhere the resulting state is the argument
provided and the value is defaulted to unit.
put::s-&gt;States()
puts= ???
Prelude&gt; runState (put &quot;blah&quot;) &quot;woot&quot;
((),&quot;blah&quot;)
3.Run the Statewith𝑠and get the state that results.</p>
<p>CHAPTER 23. STATE 1393
exec::States a-&gt;s-&gt;s
exec(Statesa) s= ???
Prelude&gt; exec (put &quot;wilma&quot;) &quot;daphne&quot;
&quot;wilma&quot;
Prelude&gt; exec get &quot;scooby papu&quot;
&quot;scooby papu&quot;
4.Run the Statewith𝑠and get the value that results.
eval::States a-&gt;s-&gt;a
eval(Statesa)= ???
Prelude&gt; eval get &quot;bunnicula&quot;
&quot;bunnicula&quot;
Prelude&gt; eval get &quot;stake a bunny&quot;
&quot;stake a bunny&quot;
5.Write a function which applies a function to create a new
State.
modify::(s-&gt;s)-&gt;States()
modify=undefined
Should behave like the following:</p>
<p>CHAPTER 23. STATE 1394
Prelude&gt; runState (modify (+1)) 0
((),1)
Prelude&gt; runState (modify (+1) &gt;&gt; modify (+1)) 0
((),2)
You don’t need to compose them, you can throw away the
result because it returns unit for 𝑎anyway.
23.9 Follow-up resources
1.State Monad; All About Monads; Haskell Wiki
https://wiki.haskell.org/All_About_Monads
2.State Monad; Haskell Wiki
https://wiki.haskell.org/State_Monad
3.Understanding Monads; Haskell Wikibook</p>
<p>Chapter 24
Parser combinators
Within a computer,
natural language is
unnatural.
Alan Perlis
1395</p>
<p>CHAPTER 24. PARSER COMBINATORS 1396
24.1 Parser combinators
The word parse comes from the Latin word for parts, and
means to analyze a sentence and label the syntactic role, or part
of speech, of each component. Language teachers once em-
phasized this ability because it forced students to think about
the structure of sentences, the relationships among the parts,
and the connection between the structure and the meaning of
the whole. Diagramming sentences was common because it
made parsing visual and somewhat concrete.
It is now common to represent grammatical structures of
natural languages as trees, so that a sentence such as
Boy plays with dog.
might be thought to have an underlying representation
such as
S(entence)
/ <br />
Boy plays (verb)
(subject) <br />
with (preposition)
<br />
dog (object)
We are not here to become linguists, but parsing in com-
puter science is related to the parsing of natural language</p>
<p>CHAPTER 24. PARSER COMBINATORS 1397
sentences. The core idea of parsing in programming is to
accept serialized input in the form of a sequence of characters
(textual data) or bytes (raw binary data) and turn that into a
value of a structured datatype. Serialized data is data that has
been translated into a format, such as JSON or XML1, that can
be stored or transmitted across a network connection. Parsing
breaks up that chunk of data and allows you to find and process
the parts you care about.
If we wrote a computer program to parse a sentence into
a very simplified model of English grammar, it could look
something like the tree above. Often when we are parsing
things, the structured datatype that results will look something
like a tree. In Haskell, we can sometimes end up having a tree
because recursive types are so easy to express in Haskell.
In this chapter, we will
•use a parsing library to cover the basics of parsing;
•demonstrate the awesome power of parser combinators;
•marshall and unmarshall some JSON data;
•talk about tokenization.
1If you do not know what JSON and XML are yet, try not to get too hung up on that.
All that matters at this point is that they are standard data formats. We’ll look at JSON in
more detail later in the chapter.</p>
<p>CHAPTER 24. PARSER COMBINATORS 1398
24.2 A few more words of introduction
In this chapter, we will not look too deeply into the types of
the parsing libraries we’re using, learn every sort of parser
there is, or artisanally handcraft all of our parsing functions
ourselves.
These are thoroughly considered decisions. Parsing is a
huge field of research in its own right with connections that
span natural language processing, linguistics, and program-
ming language theory. This topic could easily fill a book in
itself (in fact, it has). The underlying types and typeclasses of
the libraries we’ll be using are complicated. To be sure, if you
enjoy parsing and expect to do it a lot, those are things you’d
want to learn; they are simply out of the scope of this book.
This chapter takes a diﬀerent approach than previous chap-
ters. The focus is on enabling you to use Haskell’s parsing
libraries — not to be a master of parsing and writing parsers
in general. This is not the bottom-up approach you may be
accustomed to; by necessity, we’re working outside-in and
trying to cover what you’re likely to need. Depending on your
specific interests, you may find this chapter too long or not
nearly long enough.</p>
<p>CHAPTER 24. PARSER COMBINATORS 1399
24.3 Understanding the parsing process
A parser is a function that takes some textual input (it could
be aString , or another datatype such as ByteString orText) and
returns some structure as an output. That structure might
be a tree, for example, or an indexed map of locations in the
parsed data. Parsers analyze structure in conformance with
rules specified in a grammar, whether it’s a grammar of a
human language, a programming language, or a format such
as JSON.
A parser combinator is a higher-order function that takes
parsers as input and returns a new parser as output. You may
remember our brief discussion of combinators way back in
the lambda calculus chapter. Combinators are expressions
with no free variables.
The standard for what constitutes a combinator with respect
to parser combinators is a little looser. Parsers are functions,
so parser combinators are higher-order functions that can take
parsers as arguments. Usually the argument passing is elided
because the interface of parsers will often be like the State
monad which permits Reader -style implicit argument passing.
Among other things, combinators allow for recursion and for
gluing together parsers in a modular fashion to parse data
according to complex rules.
For computers, parsing is something like reading when
you’re really young. Perhaps you were taught to trace the</p>
<p>CHAPTER 24. PARSER COMBINATORS 1400
letters with your finger for phonetic pronunciation. Later, you
were able to follow word by word, then you started scanning
with your eyes. Eventually, you learned how to read with
subvocalization.
Since we didn’t use an analogy for Monad
We’re going to run through some code now that will demon-
strate the idea of parsing. Let’s begin by installing the parsing
librarytrifecta ,2then work through a short demonstration of
what it does. We’ll talk more about the design of trifecta in
a while. For now, we’re going to use it in a state of somewhat
ignorant bliss.
Let’s put up some code:
moduleLearnParsers where
importText.Trifecta
stop::Parsera
stop=unexpected &quot;stop&quot;
unexpected is a means of throwing errors in parsers like
trifecta which are an instance of the Parsing typeclass. Here
2We’ll be using this version of trifecta
http://hackage.haskell.org/package/trifecta-1.5.2</p>
<p>CHAPTER 24. PARSER COMBINATORS 1401
we’re using it to make the parser fail for demonstration pur-
poses.
What demonstration purposes?
We’re glad you asked! The basic idea behind a parser is that
you’re moving a sort of cursor around a linear stream of text.
It’ssimplesttothinkoftheindividualunitswithinthestreamas
characters or ideographs, though you’ll want to start thinking
of your parsing problems in chunkier terms as you progress.
The idea is that this cursor is a bit like you’re reading the text
with your finger:
Julie bit Papuchon
^
Then let us say we parsed the word “Julie” — we’ve now
consumed that input, so the cursor will be at “bit”:
Julie bit Papuchon
^
If we weren’t expecting the word “bit,” our parser could fail
here, and we’d get an error at the word “bit” like that. However,
if we did parse the word “bit” successfully and thus consumed
that input, it might look something like this:
Julie bit Papuchon
^</p>
<p>CHAPTER 24. PARSER COMBINATORS 1402
The analogy we’re using here isn’t perfect. One of the hard-
est problems in writing parsers, especially the parser libraries
themselves, is making it easy to express things the way the
programmer would like, but still have the resulting parser be
fast.
Back to the code
With the cursor analogy in mind, let’s return to the module
we started.
We’ll first make a little function that only parses one charac-
ter, and then sequence that with stopto make it read that one
character and then die:
-- read a single character '1'
one=char'1'
-- read a single character '1', then die
one'=one&gt;&gt;stop
-- equivalent to char '1' &gt;&gt; stop
Forone', we’re using the sequencing operator from Monadto
combine two parsers, stopandchar '1' . Given the type of &gt;&gt;:
(&gt;&gt;)::Monadm=&gt;m a-&gt;m b-&gt;m b
it’s safe to assume that whatever char '1' returns in the
expression</p>
<p>CHAPTER 24. PARSER COMBINATORS 1403
char'1'&gt;&gt;stop
gets thrown away. Critically, any eﬀect the m aaction had
upon the monadic context remains. The result value of the
parse function gets thrown away, but the eﬀect of “moving the
cursor” remains. Another possible eﬀect is causing the parse
to fail.
A bit like…
State. Plus failure. No seriously, take a look at this definition
of theParser type:
typeParsera=String-&gt;Maybe(a,String)
You can read this as:
1.Await a string value
2.Produce a result which may or may not succeed. (A
Nothing value means the parse failed.)
3.Return a tuple of the value you wanted and whatever’s
left of the string that you didn’t consume to produce the
value of type 𝑎.
Then remind yourself of what Reader andStatelook like:</p>
<p>CHAPTER 24. PARSER COMBINATORS 1404
newtype Readerr a=
Reader{ runReader ::r-&gt;a }
newtype States a=
State{ runState ::s-&gt;(a, s) }
If you have convinced yourself that Stateis an elaboration
ofReader and that you can see how the Parser type looks sorta
likeState, we can move on.
The idea here with the Parser type is that the Stateis han-
dling the fact that you need to await an eventual text input and
that having parsed something out of that text input results in
a new state of the input stream. It also lets you return a value
independent of the state, while Maybehandles the possibility
of the parser failure.
If we were to look at the underlying pattern of a parsing
function such as char, you can see the State-ish pattern. Please
understand that while this should work as a character-parsing
function, we are simplifying here and this is not what the
source code of any modern parsing library will look like:</p>
<p>CHAPTER 24. PARSER COMBINATORS 1405
-- rudimentary char
-- demo only, this won't work as is.
char::Char-&gt;ParserChar
charc=
Parser$\s-&gt;
casesof
(x:xs)-&gt; ifc==x
then[(c, xs)]
else[]
_ -&gt;[]
We could encode the possibility of failure in that by adding
Maybebut at this point, that isn’t important because we’re using
a library that has encoded the possibility of failure for us. It has
also optimized the heck out of charfor us. But we wanted to
show you how the underlying function is the s -&gt;embedded
in theParser data constructor.
Consider the type of a Hutton-Meijer parser:</p>
<p>CHAPTER 24. PARSER COMBINATORS 1406
-- from Text.ParserCombinators.HuttonMeijer
-- polyparse-1.11
typeToken=Char
newtype Parsera=
P([Token]-&gt;[(a, [Token])])
-- Same thing, differently formatted:
typeParser' a=String-&gt;[(a,String)]
This changes things from the previous, less common but
simpler variant, by allowing you to express a range of possibly
valid parses starting from the input provided. This is more
powerful than the Maybevariant, but this design isn’t used in
popular Haskell parser combinator libraries any longer. Al-
though the underlying implementation has changed dramati-
cally with new discoveries and designs, most parsing libraries
in Haskell are going to have an interface that behaves a bit like
Statein that the act of parsing things has an observable eﬀect
on one or more bits of state.
If we were talking about State, this means any putto the
Statevalue would be observable to the next action in the same
Monad(you can verify what follows in your REPL by import-
ingControl.Monad.Trans.State ). These examples use the trans-
former variant of State, but if you ignore the T, you should be
able to get the basic idea:</p>
<p>CHAPTER 24. PARSER COMBINATORS 1407
get::Monadm=&gt;StateTs m s
put::Monadm=&gt;s-&gt;StateTs m()
runStateT ::StateTs m a-&gt;s-&gt;m (a, s)
Prelude&gt; runStateT (put 8) 7
((),8)
Prelude&gt; runStateT get 8
(8,8)
Prelude&gt; runStateT (put 1 &gt;&gt; get) 8
(1,1)
Prelude&gt; (runStateT $ put 1 &gt;&gt; get) 0
(1,1)
Prelude&gt; (runStateT $ put 2 &gt;&gt; get) 10021490234890
(2,2)
Prelude&gt; (runStateT $ put 2 &gt;&gt; return 9001) 0
(9001,2)
Nowputreturns a unit value, a throwaway value, so we’re
only evaluating it for eﬀect anyway. It modifies the state but
doesn’t have any value of its own. So when we throw away its
value, we’re left with its eﬀect on the state, although getputs
that value into both the 𝑎and𝑠slots in the tuple.
This is an awful lot like what happens when we sequence a
parsing function such as charwithstop, as above. There is no
real result of char, but it does change the state. The state here
is the location of the cursor in the input stream. In reality, a</p>
<p>CHAPTER 24. PARSER COMBINATORS 1408
modern and mature parser design in Haskell will often look
about as familiar to you as the alien hellscape underneath the
frozen crust of one of the moons of Jupiter. Don’t take the
idea of there being an actual cursor too literally, but there may
be some utility in imagining it this way.
Back to our regularly scheduled coding
Onward with the code:
-- read two characters, '1', and '2'
oneTwo=char'1'&gt;&gt;char'2'
-- read two characters,
-- '1' and '2', then die
oneTwo' =oneTwo&gt;&gt;stop
testParse ::ParserChar-&gt;IO()
testParse p=
print$parseString p mempty &quot;123&quot;
The𝑝argument is a parser. Specifically, it’s a character
parser. The functions oneandoneTwo have the type Parser Char .
You can check the types of one'andoneTwo' yourself.
We needed to declare the type of testParse in order to Show
what we parsed because of ambiguity.</p>
<p>CHAPTER 24. PARSER COMBINATORS 1409
The key thing to realize here is that we’re using parsers like
values and combining them using the same stuﬀ we use with
ordinary functions or operators from the Applicative andMonad
typeclasses. The structure that makes up the Applicative or
Monadin this case is the Parser itself.
Next we’ll write a function to print a string to standard
output (stdout) with a newline prefixed, and then use that
function as part of a mainthat will show us what we’ve got so
far:
pNLs=
putStrLn ( '\n':s)
main= do
pNL&quot;stop:&quot;
testParse stop
pNL&quot;one:&quot;
testParse one
pNL&quot;one':&quot;
testParse one'
pNL&quot;oneTwo:&quot;
testParse oneTwo
pNL&quot;oneTwo':&quot;
testParse oneTwo'
Let’s run it and interpret the results. Since it’s text on a</p>
<p>CHAPTER 24. PARSER COMBINATORS 1410
computer screen instead of tea leaves, we’ll call it science. If
you remain unconvinced, you have our permission to don a
white labcoat and print the output using a dot-matrix printer.
Some of you kids probably don’t even know what a dot-matrix
printer is.3
Runmainand see what happens:
Prelude&gt; main
stop:
Failure (interactive):1:1: error: unexpected
stop
123<EOF>
^
We failed immediately before consuming any input in the
above, so the caret in the error is at the beginning of our string
value.
Next result:
one:
Success '1'
We parsed a single character, the digit 1. The result is know-
ing we succeeded. But what about the rest of the input stream?
Well, the thing we used to run the parser dropped the rest of
3shakes fist at sky</p>
<p>CHAPTER 24. PARSER COMBINATORS 1411
the input on the floor. There are ways to change this behavior
which we’ll explain in the exercises.
Next up:
one':
Failure (interactive):1:2: error: unexpected
stop
123<EOF>
^
We parsed a single character successfully, then dropped it
because we used &gt;&gt;to sequence it with stop. This means the
cursor was one character forward due to the previous parser
succeeding. Helpfully, trifecta tells us where our parser failed.
And for our last result:
oneTwo:
Success '2'
oneTwo':
Failure (interactive):1:3: error: unexpected
stop
123<EOF>
^
It’s the same as before, but we parsed two characters indi-
vidually. What if we we don’t want to discard the first character
we parsed and instead parse “12?” See the exercises below!</p>
<p>CHAPTER 24. PARSER COMBINATORS 1412
Exercises: Parsing Practice
1.There’s a combinator that’ll let us mark that we expect
an input stream to be finished at a particular point in our
parser. In the parsers library this is simply called eof(end-
of-file) and is in the Text.Parser.Combinators module. See
if you can make the oneandoneTwo parsers fail because
they didn’t exhaust the input stream!
2.Usestring to make a Parser that parses “1”, “12”, and “123”
out of the example input respectively. Try combining it
withstoptoo. That is, a single parser should be able to
parse all three of those strings.
3.Try writing a Parser that does what string does, but using
char.
Intermission: parsing free jazz
Let us play with these parsers! We typically use the parseString
function to run parsers, but if you figure some other way that
works for you, so be it! Here’s some parsing free jazz, if you
will, meant only to help develop your intuition about what’s
going on:
Prelude&gt; import Text.Trifecta
Prelude&gt; :t char
char :: CharParsing m =&gt; Char -&gt; m Char</p>
<p>CHAPTER 24. PARSER COMBINATORS 1413
Prelude&gt; :t parseString
parseString
:: Parser a
-&gt; Text.Trifecta.Delta.Delta
-&gt; String
-&gt; Result a
Prelude&gt; let gimmeA = char 'a'
Prelude&gt; :t parseString gimmeA mempty
parseString gimmeA mempty :: String -&gt; Result Char
Prelude&gt; parseString gimmeA mempty &quot;a&quot;
Success 'a'
Prelude&gt; parseString gimmeA mempty &quot;b&quot;
Failure (interactive):1:1: error: expected: &quot;a&quot;
b<EOF>
^
Prelude&gt; parseString (char 'b') mempty &quot;b&quot;
Success 'b'
Prelude&gt; parseString (char 'b' &gt;&gt; char 'c') mempty &quot;b&quot;
Failure (interactive):1:2: error: unexpected
EOF, expected: &quot;c&quot;
b<EOF>
^
Prelude&gt; parseString (char 'b' &gt;&gt; char 'c') mempty &quot;bc&quot;
Success 'c'</p>
<p>CHAPTER 24. PARSER COMBINATORS 1414
Prelude&gt; parseString (char 'b' &gt;&gt; char 'c') mempty &quot;abc&quot;
Failure (interactive):1:1: error: expected: &quot;b&quot;
abc<EOF>
^
Seems like we ought to have a way to say, “parse this string”
rather than having to sequence the parsers of individual char-
acters bit by bit, right? Turns out, we do:
Prelude&gt; parseString (string &quot;abc&quot;) mempty &quot;abc&quot;
Success &quot;abc&quot;
Prelude&gt; parseString (string &quot;abc&quot;) mempty &quot;bc&quot;
Failure (interactive):1:1: error: expected: &quot;abc&quot;
bc<EOF>
^
Prelude&gt; parseString (string &quot;abc&quot;) mempty &quot;ab&quot;
Failure (interactive):1:1: error: expected: &quot;abc&quot;
ab<EOF>
^
Importantly, it’s not a given that a single parser exhausts all
of its input — they only consume as much text as they need
to produce the value of the type requested.
Prelude&gt; parseString (char 'a') mempty &quot;abcdef&quot;
Success 'a'
Prelude&gt; let stop = unexpected &quot;stop pls&quot;</p>
<p>CHAPTER 24. PARSER COMBINATORS 1415
Prelude&gt; parseString (char 'a' &gt;&gt; stop) mempty &quot;abcdef&quot;
Failure (interactive):1:2: error: unexpected
stop pls
abcdef<EOF>
^
Prelude&gt; parseString (string &quot;abc&quot;) mempty &quot;abcdef&quot;
Success &quot;abc&quot;
Prelude&gt; parseString (string &quot;abc&quot; &gt;&gt; stop) mempty &quot;abcdef&quot;
Failure (interactive):1:4: error: unexpected
stop pls
abcdef<EOF>
^
Note that we can also parse UTF-8 encoded ByteString s with
trifecta :
Prelude&gt; import Text.Trifecta
Prelude&gt; :t parseByteString
parseByteString
:: Parser a
-&gt; Text.Trifecta.Delta.Delta
-&gt; Data.ByteString.Internal.ByteString
-&gt; Result a
Prelude&gt; parseByteString (char 'a') mempty &quot;a&quot;
Success 'a'</p>
<p>CHAPTER 24. PARSER COMBINATORS 1416
This ends the free jazz session. We now return to serious
matters.
24.4 Parsing fractions
Now that we have some idea of what parsing is, what parser
combinators are, and what the monadic underpinnings of
parsing look like, let’s move on to parsing fractions. The top
of this module should look like this:
{-# LANGUAGE OverloadedStrings #-}
moduleText.Fractions where
importControl.Applicative
importData.Ratio ((%))
importText.Trifecta
We named the module Text.Fractions because we’re pars-
ing fractions out of text input, and there’s no need to be more
clever about it than that. We’re going to be using String in-
puts with trifecta at first, but you’ll see why we threw an
OverloadedStrings extension in there later.
Now, on to parsing fractions! We’ll start with some test
inputs:</p>
<p>CHAPTER 24. PARSER COMBINATORS 1417
badFraction =&quot;1/0&quot;
alsoBad =&quot;10&quot;
shouldWork =&quot;1/2&quot;
shouldAlsoWork =&quot;2/1&quot;
Then we’ll write our actual parser:
parseFraction ::ParserRational
parseFraction = do
numerator &lt;-decimal
-- [2] [1]
char'/'
-- [3]
denominator &lt;-decimal
-- [ 4 ]
return (numerator %denominator)
-- [5] [6]
1.decimal ::Integral a=&gt;Parsera
This is the type of decimal within the context of those
functions. If you use GHCi to query the type of decimal ,
you will see a more polymorphic type signature.
2.Herenumerator has the type Integral a =&gt; a .
3.char::Char-&gt;ParserChar</p>
<p>CHAPTER 24. PARSER COMBINATORS 1418
As with decimal , if you query the type of charin GHCi,
you’ll see a more polymorphic type, but this is the type
ofcharin context.
4.Same deal as numerator , but when we match an integral
number we’re binding the result to the name denominator .
5.The final result has to be a parser, so we embed our inte-
gral value in the Parser type by using return.
6.We construct ratios using the %infix operator:
(%)::Integral a
=&gt;a-&gt;a-&gt;GHC.Real.Ratioa
Then the fact that our final result is a Rational makes the
Integral a =&gt; a values into concrete Integer values.
typeRational =GHC.Real.RatioInteger
We’ll put together a quick shim main function to run the
parser against the test inputs and see the results:</p>
<p>CHAPTER 24. PARSER COMBINATORS 1419
main::IO()
main= do
letparseFraction' =
parseString parseFraction mempty
print$parseFraction' shouldWork
print$parseFraction' shouldAlsoWork
print$parseFraction' alsoBad
print$parseFraction' badFraction
Try not to worry about the mempty values; it might give you
a clue about what’s going on in trifecta under the hood, but
it’s not something we’re going to explore in this chapter.
We will briefly note the type of parseString , which is how
we’re running the parser we created:
parseString ::Parsera
-&gt;Text.Trifecta .Delta.Delta
-&gt;String
-&gt;Resulta
The first argument is the parser we’re going to run against
the input, the second is a Delta, the third is the String we’re
parsing, and then the final result is either the thing we wanted
of type 𝑎or an error string to let us know something went
wrong. You can ignore the Deltathing — use mempty to provide
the do-nothing input. We won’t be covering deltas in this book
so consider it extra credit if you get curious.</p>
<p>CHAPTER 24. PARSER COMBINATORS 1420
Anyway, when we run the code, the results look like this:
Prelude&gt; main
Success (1 % 2)
Success (2 % 1)
Failure (interactive):1:3: error: unexpected
EOF, expected: &quot;/&quot;, digit
10<EOF>
^
Success *** Exception: Ratio has zero denominator
The first two succeeded properly. The third failed because it
couldn’t parse a fraction out of the text “10”. The error is telling
us that it ran out of text in the input stream while still waiting
for the character '/'. The final error did not result from the
process of parsing; we know that because it is a Success data
constructor. The final error resulted from trying to construct
a ratio with a denominator that is zero — which makes no
sense. We can reproduce the issue in GHCi:
Prelude&gt; 1 % 0
*** Exception: Ratio has zero denominator
-- So the parser result is which is tantamount to
Prelude&gt; Success (1 % 0)
Success *** Exception: Ratio has zero denominator
This is sort of a problem because exceptions end our pro-
grams. Observe:</p>
<p>CHAPTER 24. PARSER COMBINATORS 1421
main::IO()
main= do
letparseFraction' =
parseString parseFraction mempty
print$parseFraction' badFraction
print$parseFraction' shouldWork
print$parseFraction' shouldAlsoWork
print$parseFraction' alsoBad
We’ve put the expression that throws an exception in the
first line this time, when we run it we get:
Prelude&gt; main
Success *** Exception: Ratio has zero denominator
So, our program halted on the error. This is not great. You
may be tempted to “handle” the error. Catching exceptions
is okay, but this is a particular class of exceptions that means
something is quite wrong with your program. You should elim-
inate the possibility of exceptions occurring in your programs
where possible.
We’ll talk more about error handling in a later chapter, but
the idea here is that a Parser type already explicitly encodes
the possibility of failure. It’s better for a value of type Parser
ato have only one vector for errors and that vector is the
parser’s ability to encode failure. There may be an edge case</p>
<p>CHAPTER 24. PARSER COMBINATORS 1422
that doesn’t suit this design preference, but it’s a very good
idea to not have exceptions or bottoms that aren’t explicitly
called out as a possibility in the types whenever possible.
We could modify our program to handle the 0 denominator
case and change it into a parse error:
virtuousFraction ::ParserRational
virtuousFraction = do
numerator &lt;-decimal
char'/'
denominator &lt;-decimal
casedenominator of
0-&gt;fail&quot;Denominator cannot be zero&quot;
_ -&gt;return (numerator %denominator)
Here is our first explicit use of fail, which by historical ac-
cident is part of the Monadtypeclass. Realistically, not all Monads
have a proper implementation of fail, so it will be moved out
into aMonadFail class eventually. For now, it suffices to know
that it is our means of returning an error for the Parser type
here.
Now for another run of our test inputs, but with our more
cautious parser:</p>
<p>CHAPTER 24. PARSER COMBINATORS 1423
testVirtuous ::IO()
testVirtuous = do
letvirtuousFraction' =
parseString virtuousFraction mempty
print$virtuousFraction' badFraction
print$virtuousFraction' alsoBad
print$virtuousFraction' shouldWork
print$virtuousFraction' shouldAlsoWork
When we run this, we’re going to get a slightly diﬀerent
result at the end:
Prelude&gt; testVirtuous
Failure (interactive):1:4: error: Denominator
cannot be zero, expected: digit
1/0<EOF>
^
Failure (interactive):1:3: error: unexpected
EOF, expected: &quot;/&quot;, digit
10<EOF>
^
Success (1 % 2)
Success (2 % 1)
Now we have no bottom causing the program to halt and
we get a Failure value which explains the cause for the failure.
Much better!</p>
<p>CHAPTER 24. PARSER COMBINATORS 1424
Exercise: Unit of Success
This should not be unfamiliar at this point, even if you do not
understand all the details:
Prelude&gt; parseString integer mempty &quot;123abc&quot;
Success 123
Prelude&gt; parseString (integer &gt;&gt; eof) mempty &quot;123abc&quot;
Failure (interactive):1:4: error: expected: digit,
end of input
123abc<EOF>
^
Prelude&gt; parseString (integer &gt;&gt; eof) mempty &quot;123&quot;
Success ()
You may have already deduced why it returns ()as aSuccess
result here; it’s consumed all the input but there is no result
to return from having done so. The result Success () tells you
the parse was successful and consumed the entire input, so
there’s nothing to return.
What we want you to try now is rewriting the final example
so it returns the integer that it parsed instead of Success () .
It should return the integer successfully when it receives an
input with an integer followed by an EOF and fail in all other
cases:
Prelude&gt; parseString (yourFuncHere) mempty &quot;123&quot;</p>
<p>CHAPTER 24. PARSER COMBINATORS 1425
Success 123
Prelude&gt; parseString (yourFuncHere) mempty &quot;123abc&quot;
Failure (interactive):1:4: error: expected: digit,
end of input
123abc<EOF>
^
24.5 Haskell’s parsing ecosystem
Haskell has several excellent parsing libraries available. parsec
andattoparsec are perhaps the two most well known parser
combinator libraries in Haskell, but there is also megaparsec
and others. aesonandcassava are among the libraries designed
for parsing specific types of data (JSON data and CSV data,
respectively).
For this chapter, we opted to use trifecta , as you’ve seen.
One reason for that decision is that trifecta has error messages
that are very easy to read and interpret, unlike some other
libraries. Also, trifecta does not seem likely to undergo major
changes in its fundamental design. Its design is somewhat
unusual and complex, but most of the things that make it
unusual will be irrelevant to you in this chapter. If you intend
to do a lot of parsing in production, you may need to get
comfortable using attoparsec , as it is particularly known for
very speedy parsing; you will see some attoparsec (andaeson)
later in the chapter.</p>
<p>CHAPTER 24. PARSER COMBINATORS 1426
The design of trifecta has evolved such that the API4is split
across two libraries, parsers5andtrifecta . The reason for this
is that the trifecta package itself provides the concrete im-
plementation of the trifecta parser as well as trifecta -specific
functionality, but the parsers API is a collection of typeclasses
that abstract over diﬀerent kinds of things parsers can do. The
Text.Trifecta module handles exporting what you need to get
started from each package, so this information is mostly so
you know where to look if you need to start spelunking.
Typeclasses of parsers
As we noted above, trifecta relies on the parsers library for
certain typeclasses. These typeclasses abstract over common
kinds of things parsers do. We’re only going to note a few
things here that we’ll be seeing in the chapter so that you have
a sense of their provenance.
Note that the following is a discussion of code provided for
you by the parsers library, you do not need to type this in!
1.The typeclass Parsing hasAlternative as a superclass. We’ll
talk more about Alternative in a bit. The Parsing typeclass
4API stands for application programming interface. When we write software that
relies on libraries or makes requests to a service such as Twitter — basically, software
that relies on other software — we rely on a set of defined functions. The API is that set
of functions that we use to interface with that software without having to write those
functions or worry too much about their source code. When you look at a library on
Hackage, (unless you click to view the source code), you’re looking at the API of that
library.
5http://hackage.haskell.org/package/parsers</p>
<p>CHAPTER 24. PARSER COMBINATORS 1427
provides for functionality needed to describe parsers in-
dependent of input type. A minimal complete instance of
this typeclass defines the following functions: try,(&lt;?&gt;),
andnotFollowedBy . Let’s start with try:
-- Text.Parser.Combinators
classAlternative m=&gt;Parsing mwhere
try::m a-&gt;m a
Thistakesaparserthatmayconsumeinputand, onfailure,
goes back to where it started and fails if we didn’t consume
input.
It also gives us the function notFollowedBy which does not
consume input but allows us to match on keywords by
matching on a string of characters that is not followed by
some thing we do not want to match:
notFollowedBy ::Showa=&gt;m a-&gt;m()
-- &gt; noAlpha = notFollowedBy alphaNum
-- &gt; keywordLet =
-- try $ string &quot;let&quot; &lt;* noAlpha
2.TheParsing typeclass also includes unexpected which is
used to emit an error on an unexpected token, as we saw
earlier, and eof. Theeoffunction only succeeds at the end
of input:</p>
<p>CHAPTER 24. PARSER COMBINATORS 1428
eof::m()
-- &gt; eof =
-- notFollowedBy anyChar
-- &lt;?&gt; &quot;end of input&quot;
We’ll be seeing more of this one in upcoming sections.
3.The library also defines the typeclass CharParsing , which
hasParsing as a superclass. This handles parsing individ-
ual characters.
-- Text.Parser.Char
classParsing m=&gt;CharParsing mwhere
We’ve already seen charfrom this class, but it also includes
these:
-- Parses any single character other
-- than the one provided. Returns
-- the character parsed.
notChar ::Char-&gt;mChar
-- Parser succeeds for any character.
-- Returns the character parsed.
anyChar ::mChar</p>
<p>CHAPTER 24. PARSER COMBINATORS 1429
-- Parses a sequence of characters, returns
-- the string parsed.
string::String-&gt;mString
-- Parses a sequence of characters
-- represented by a Text value,
-- returns the parsed Text fragment.
text::Text-&gt;mText
Theparsers library has much more than this, but for our
immediate purposes these will suffice. The important point is
that it defines for us some typeclasses and basic combinators
for common parsing tasks. We encourage you to explore the
documentation more on your own.
24.6 Alternative
Let’s say we had a parser for numbers and one for alphanu-
meric strings:
Prelude&gt; import Text.Trifecta
Prelude&gt; parseString (some letter) mempty &quot;blah&quot;
Success &quot;blah&quot;
Prelude&gt; parseString integer mempty &quot;123&quot;
Success 123
What if we had a type that could be an Integer or aString ?</p>
<p>CHAPTER 24. PARSER COMBINATORS 1430
moduleAltParsing where
importControl.Applicative
importText.Trifecta
typeNumberOrString =
EitherInteger String
a=&quot;blah&quot;
b=&quot;123&quot;
c=&quot;123blah789&quot;
parseNos ::ParserNumberOrString
parseNos =
(Left&lt;$&gt;integer)
&lt;|&gt;(Right&lt;$&gt;some letter)
main= do
letp f i=
parseString f mempty i
print$p (some letter) a
print$p integer b
print$p parseNos a
print$p parseNos b
print$p (many parseNos) c
print$p (some parseNos) c</p>
<p>CHAPTER 24. PARSER COMBINATORS 1431
We can read &lt;|&gt;as being an or, or disjunction, of our two
parsers; manyis zero or more and someis one or more.
Prelude&gt; parseString (some integer) mempty &quot;123&quot;
Success [123]
Prelude&gt; parseString (many integer) mempty &quot;123&quot;
Success [123]
Prelude&gt; parseString (many integer) mempty &quot;&quot;
Success []
Prelude&gt; parseString (some integer) mempty &quot;&quot;
Failure (interactive):1:1: error: unexpected
EOF, expected: integer
<EOF>
^
What we’re taking advantage of here with some,many, and
(&lt;|&gt;)is theAlternative typeclass:</p>
<p>CHAPTER 24. PARSER COMBINATORS 1432
classApplicative f=&gt;Alternative fwhere
-- | The identity of '&lt;|&gt;'
empty::f a
-- | An associative binary operation
(&lt;|&gt;)::f a-&gt;f a-&gt;f a
-- | One or more.
some::f a-&gt;f [a]
some v=some_v
where
many_v=some_v&lt;|&gt;pure[]
some_v=(fmap (:) v)&lt;<em>&gt;many_v
-- | Zero or more.
many::f a-&gt;f [a]
many v=many_v
where
many_v=some_v&lt;|&gt;pure[]
some_v=(fmap (:) v)&lt;</em>&gt;many_v
If you use the :infocommand in the REPL after importing
Text.Trifecta or loading the above module, you’ll find some
andmanyare defined in GHC.Base because they come from this
typeclass rather than being specific to a particular parser or to
theparsers library, or even to this particular problem domain.</p>
<p>CHAPTER 24. PARSER COMBINATORS 1433
What if we wanted to require that each value be separated
by newline? QuasiQuotes lets us have a multiline string without
the newline separators and use it as a single argument:
{-# LANGUAGE QuasiQuotes #-}
moduleAltParsing where
importControl.Applicative
importText.RawString.QQ
importText.Trifecta
typeNumberOrString =
EitherInteger String
eitherOr ::String
eitherOr =[r|
123
abc
456
def
|]</p>
<p>CHAPTER 24. PARSER COMBINATORS 1434
QuasiQuotes
Above, the [r|is beginning a quasiquoted6section, using the
quasiquoter named r. Note we had to enable the QuasiQuotes
language extension to use this syntax. At time of writing ris
defined in raw-strings-qq version 1.1 as follows:
r::QuasiQuoter
r=QuasiQuoter {
-- Extracted from dead-simple-json.
quoteExp =
return.LitE.StringL
.normaliseNewlines,
-- error messages elided
quotePat =
_ -&gt;fail&quot;some error message&quot;
quoteType =
_ -&gt;fail&quot;some error message&quot;
quoteDec =
_ -&gt;fail&quot;some error message&quot;
The idea here is that this is a macro that lets us write ar-
bitrary text inside of the block that begins with [r|and ends
6There’s a rather nice wiki page and tutorial example at: https://wiki.haskell.org/
Quasiquotation</p>
<p>CHAPTER 24. PARSER COMBINATORS 1435
with|]. This specific quasiquoter exists to allow writing mul-
tiline strings without manual escaping. The quasiquoter is
generating the following for us:
&quot;\n<br />
\123\n<br />
\abc\n<br />
\456\n<br />
\def\n&quot;
Not as nice right? As it happens, if you want to see what
a quasiquoter or Template Haskell7is generating at compile-
time, you can enable the -ddump-splices flag to see what it does.
Here’s an example using a minimal stub file:
7https://wiki.haskell.org/Template_Haskell</p>
<p>CHAPTER 24. PARSER COMBINATORS 1436
{-# LANGUAGE QuasiQuotes #-}
moduleQuasimodo where
importText.RawString.QQ
eitherOr ::String
eitherOr =[r|
123
abc
456
def
|]
Then in GHCi we use the :setcommand to turn on the
splice dumping flag so we can see what the quasiquoter gener-
ated:
Prelude&gt; :set -ddump-splices
Prelude&gt; :l code/quasi.hs
[1 of 1] Compiling Quasimodo
code/quasi.hs:(8,12)-(12,2): Splicing expression
&quot;\n<br />
\123\n<br />
\abc\n<br />
\456\n&quot;</p>
<p>CHAPTER 24. PARSER COMBINATORS 1437
======&gt;
&quot;\n<br />
\123\n<br />
\abc\n<br />
\456\n&quot;
Right, so back to the parser we were going to write!
Return to Alternative
All right, we return now to our AltParsing module. We’re going
to use this fantastic function:
parseNos ::ParserNumberOrString
parseNos =
(Left&lt;$&gt;integer)
&lt;|&gt;(Right&lt;$&gt;some letter)
and rewrite mainto apply that to the eitherOr value:
main= do
letp f i=parseString f mempty i
print$p parseNos eitherOr
Note that we lifted LeftandRightover their arguments.
This is because there is Parser structure between the (potential)
value obtained by running the parser and what the data con-
structor expects. A value of type Parser Char is a parser that</p>
<p>CHAPTER 24. PARSER COMBINATORS 1438
will possibly produce a Charvalue if it is given an input that
doesn’t cause it to fail. The type of some letter is the following:
Prelude&gt; import Text.Trifecta
Prelude&gt; :t some letter
some letter :: CharParsing f =&gt; f [Char]
However, for our purposes we can say that the type is specif-
icallytrifecta ’sParser type:
Prelude&gt; let someLetter = some letter :: Parser [Char]
Prelude&gt; let someLetter = some letter :: Parser String
If we try to mash a data constructor expecting a String and
our parser-of-string together like a kid playing with action
figures, we get a type error:
Prelude&gt; data MyName = MyName String deriving Show
Prelude&gt; MyName someLetter
Couldn't match type ‘Parser String’ with ‘[Char]’
Expected type: String
Actual type: Parser String
In the first argument of ‘MyName’, namely ‘someLetter’
In the expression: MyName someLetter
Unless we lift it over the Parser structure, since Parser is a
Functor !</p>
<p>CHAPTER 24. PARSER COMBINATORS 1439
Prelude&gt; :info Parser
{... content elided ...}
instance Monad Parser
instance Functor Parser
instance Applicative Parser
instance Monoid a =&gt; Monoid (Parser a)
instance Errable Parser
instance DeltaParsing Parser
instance TokenParsing Parser
instance Parsing Parser
instance CharParsing Parser
We should need an fmapright?
-- same deal
Prelude&gt; :t MyName &lt;$&gt; someLetter
MyName &lt;$&gt; someLetter :: Parser MyName
Prelude&gt; :t MyName <code>fmap</code> someLetter
MyName <code>fmap</code> someLetter :: Parser MyName
Then running either of them:
Prelude&gt; parseString someLetter mempty &quot;Chris&quot;
Success &quot;Chris&quot;
Prelude&gt; let mynameParser = MyName &lt;$&gt; someLetter
Prelude&gt; parseString mynameParser mempty &quot;Chris&quot;</p>
<p>CHAPTER 24. PARSER COMBINATORS 1440
Success (MyName &quot;Chris&quot;)
Cool.
Back to our original code, which will spit out an error:
Prelude&gt; main
Failure (interactive):1:1: error: expected: integer,
letter
It’s easier to see why if we look at the test string:
Prelude&gt; eitherOr
&quot;\n123\nabc\n456\ndef\n&quot;
One way to fix this is to amend the quasiquoted string:
eitherOr ::String
eitherOr =[r|123
abc
456
def
|]
What if we wanted to permit a newline before attempting
to parse strings or integers?</p>
<p>CHAPTER 24. PARSER COMBINATORS 1441
eitherOr ::String
eitherOr =[r|
123
abc
456
def
|]
parseNos ::ParserNumberOrString
parseNos =
skipMany (oneOf &quot;\n&quot;)
&gt;&gt;
(Left&lt;$&gt;integer)
&lt;|&gt;(Right&lt;$&gt;some letter)
main= do
letp f i=parseString f mempty i
print$p parseNos eitherOr
Prelude&gt; main
Success (Left 123)
OK, but we’d like to keep parsing after each line. If we try
the obvious thing and use someto ask for one-or-more results,
we’ll get a somewhat mysterious error:</p>
<p>CHAPTER 24. PARSER COMBINATORS 1442
Prelude&gt; parseString (some parseNos) mempty eitherOr
Failure (interactive):6:1: error: unexpected
EOF, expected: integer, letter
<EOF>
^
The issue here is that while skipMany lets us skip zero or more
times, it means we started the next run of the parser before we
hit EOF. This means it expects us to match an integer or some
letters after having seen the newline character after “def”. We
can simply amend the input:
eitherOr ::String
eitherOr =[r|
123
abc
456
def|]
Our previous attempt will now work fine:
Prelude&gt; parseString (some parseNos) mempty eitherOr
Success [Left 123,Right &quot;abc&quot;,Left 456,Right &quot;def&quot;]
If we’re dissatisfied with simply changing the rules of the
game, there are a couple ways we can make our parser cope
withspuriousterminalnewlines. Oneistoaddanother skipMany
rule after we parse our value:</p>
<p>CHAPTER 24. PARSER COMBINATORS 1443
parseNos ::ParserNumberOrString
parseNos = do
skipMany (oneOf &quot;\n&quot;)
v&lt;-(Left&lt;$&gt;integer)
&lt;|&gt;(Right&lt;$&gt;some letter)
skipMany (oneOf &quot;\n&quot;)
return v
Another option is to keep the previous version of the parser
which skips a potential leading newline:
parseNos ::ParserNumberOrString
parseNos =
skipMany (oneOf &quot;\n&quot;)
&gt;&gt;
(Left&lt;$&gt;integer)
&lt;|&gt;(Right&lt;$&gt;some letter)
But then tokenize it with the default tokenbehavior:
Prelude&gt; parseString (some (token parseNos)) mempty eitherOr
Success [Left 123,Right &quot;abc&quot;,Left 456,Right &quot;def&quot;]
We’ll explain soon what this token stuﬀ is about, but we
want to be a bit careful here as token parsers and character
parsers are diﬀerent sorts of things. What applying tokento
parseNos did for us here is make it optionally consume trailing</p>
<p>CHAPTER 24. PARSER COMBINATORS 1444
whitespace we don’t care about, where whitespace includes
newline characters.
Exercise: Try Try
Make a parser, using the existing fraction parser plus a new dec-
imal parser, that can parse either decimals or fractions. You’ll
want to use &lt;|&gt;fromAlternative to combine the…alternative
parsers. If you find this too difficult, write a parser that parses
straightforward integers or fractions. Make a datatype that
contains either an integer or a rational and use that datatype as
the result of the parser. Or use Either . Run free, grasshopper.
Hint: we’ve not explained it yet, but you may want to try
try.
24.7 Parsing configuration files
For our next examples, we’ll be using the INI8configuration
file format, partly because it’s an informal standard so we can
play fast and loose for learning and experimentation purposes.
We’re also using INI because it’s relatively uncomplicated.
Here’s a teensy example of an INI config file:
8INI is an informal standard for configuration files on some platforms. The name
comes from the file extension, .ini, short for “initialization.”</p>
<p>CHAPTER 24. PARSER COMBINATORS 1445
; comment
[section]
host=wikipedia .org
alias=claw
The above contains a comment, which contributes noth-
ing to the data parsed out of the configuration file but which
may provide context to the settings being configured. It’s fol-
lowed by a section header named &quot;section&quot; which contains
two settings: one named &quot;host&quot; with the value &quot;wikipedia.org&quot; ,
another named &quot;alias&quot; with the value &quot;claw&quot; .
We’ll begin this example with our pragmas, module decla-
ration, and imports:</p>
<p>CHAPTER 24. PARSER COMBINATORS 1446
{-# LANGUAGE OverloadedStrings #-}
{-# LANGUAGE QuasiQuotes #-}
moduleData.Ini where
importControl.Applicative
importData.ByteString (ByteString )
importData.Char (isAlpha)
importData.Map (Map)
import qualified Data.Map asM
importData.Text (Text)
import qualified Data.Text.IO asTIO
importTest.Hspec
importText.RawString.QQ
-- parsers 0.12.3, trifecta 1.5.2
importText.Trifecta
OverloadedStrings andQuasiQuotes should be familiar by now.
When writing parsers in Haskell, it’s often easiest to work in
terms of smaller parsers that deal with a sub-problem of the
overall parsing problem you’re solving, then combine them
into the final parser. This isn’t a perfect recipe for understand-
ing your parser, but being able to compose them straightfor-
wardly like functions is pretty nifty. Let’s start by creating a
test input for an INI header, a datatype, and then the parser</p>
<p>CHAPTER 24. PARSER COMBINATORS 1447
for it:
headerEx ::ByteString
headerEx =&quot;[blah]&quot;
-- &quot;[blah]&quot; -&gt; Section &quot;blah&quot;
newtype Header=
HeaderString
deriving (Eq,Ord,Show)
parseBracketPair ::Parsera-&gt;Parsera
parseBracketPair p=
char'['*&gt;p&lt;*char']'
-- these operators mean the brackets
-- will be parsed and then discarded
-- but the p will remain as our result
parseHeader ::ParserHeader
parseHeader =
parseBracketPair ( Header&lt;$&gt;some letter)
Here we’ve combined two parsers in order to parse a Header .
We can experiment with each of them in the REPL. First
we’ll examine the types of the some letter parser we passed to
parseBracketPair :</p>
<p>CHAPTER 24. PARSER COMBINATORS 1448
Prelude&gt; :t some letter
some letter :: CharParsing f =&gt; f [Char]
Prelude&gt; :t Header &lt;$&gt; some letter
Header &lt;$&gt; some letter :: CharParsing f =&gt; f Header
Prelude&gt; let slp = Header &lt;$&gt; some letter :: Parser Header
The first type is some parser that can understand characters
which will produce a String value if it succeeds. The second
type is the same, but produces a Header value instead of a String .
Parser types in Haskell almost always encode the possibility
of failure; we’ll cover how later in this chapter. The third type
gives us concrete Parser type from trifecta where there had
been the polymorphic type 𝑓.
Theletter function parses a single character, while some
letter parses one or more characters. We need to wrap the
Header constructor around that so that our result there — what-
everlettersmightbeinsidethebrackets, the 𝑝ofparseBracketPair
— will be labeled as the Header of the file in the final parse.
Next,assignmentEx is just some test input so we can begin
kicking around our parser. The type synonyms are to make
the types more readable as well. Nothing too special here:</p>
<p>CHAPTER 24. PARSER COMBINATORS 1449
assignmentEx ::ByteString
assignmentEx =&quot;woot=1&quot;
typeName=String
typeValue=String
typeAssignments =MapNameValue
parseAssignment ::Parser(Name,Value)
parseAssignment = do
name&lt;-some letter
_ &lt;-char'='
val&lt;-some (noneOf &quot;\n&quot;)
skipEOL -- important!
return (name, val)
-- | Skip end of line and
-- whitespace beyond.
skipEOL ::Parser()
skipEOL =skipMany (oneOf &quot;\n&quot;)
Let us explain parseAssignment step by step. For parsing the
initial key or name of an assignment, we parse one or more
letters:
name&lt;-some letter</p>
<p>CHAPTER 24. PARSER COMBINATORS 1450
Then we parse and throw away the “=” used to separate keys
and values:
_ &lt;-char'='
Then we parse one or more characters as long as they aren’t
a newline. This is so letters, numbers, and whitespace are
permitted:
val&lt;-some (noneOf &quot;\n&quot;)
We skip “end-of-line” until we stop getting newline charac-
ters:
skipEOL -- important!
This is so we can delineate the end of assignments and
parse more than one assignment in a straightforward manner.
Consider an alternative variant of this same parser that doesn’t
haveskipEOL :
parseAssignment' ::Parser(Name,Value)
parseAssignment' = do
name&lt;-some letter
_ &lt;-char'='
val&lt;-some (noneOf &quot;\n&quot;)
return (name, val)</p>
<p>CHAPTER 24. PARSER COMBINATORS 1451
Then trying out this variant of the parser:
Prelude&gt; let spa' = some parseAssignment'
Prelude&gt; let s = &quot;key=value\nblah=123&quot;
Prelude&gt; parseString spa' mempty s
Success [(&quot;key&quot;,&quot;value&quot;)]
Pity. Can’t parse the second assignment. But the first ver-
sion that includes the skipEOL should work:
Prelude&gt; let spa = some parseAssignment
Prelude&gt; parseString spa mempty s
Success [(&quot;key&quot;,&quot;value&quot;),(&quot;blah&quot;,&quot;123&quot;)]
Prelude&gt; let d = &quot;key=value\n\n\ntest=data&quot;
Prelude&gt; parseString spa mempty d
Success [(&quot;key&quot;,&quot;value&quot;),(&quot;test&quot;,&quot;data&quot;)]
We have to skip the one-or-more newline characters sepa-
rating the first and second assignment in order for the rerun
of the assignment parser to begin successfully parsing the
letters that make up the key of the second assignment. Happy-
making, right?
We finish things oﬀ for parseAssignment by tupling name and
value together and re-embedding the result in the Parser type:
return(name, val)</p>
<p>CHAPTER 24. PARSER COMBINATORS 1452
Then for dealing with INI comments, that is, skipping them
in the parser and discarding the data:
commentEx ::ByteString
commentEx =
&quot;; last modified 1 April <br />
\2001 by John Doe&quot;
commentEx' ::ByteString
commentEx' =
&quot;; blah\n; woot\n\n;hah&quot;
-- | Skip comments starting at the
-- beginning of the line.
skipComments ::Parser()
skipComments =
skipMany ( do _ &lt;- char';'&lt;|&gt;char'#'
skipMany (noneOf &quot;\n&quot;)
skipEOL)
We made a couple of comment examples for testing the
parser. Note that comments can begin with #or;.
Next, we need section parsing. We’ll make some data for
testing that out, as we did with comments above. This is also
where we’ll put that QuasiQuotes extension to use, allowing us
to make multiline strings nicer to write:</p>
<p>CHAPTER 24. PARSER COMBINATORS 1453
sectionEx ::ByteString
sectionEx =
&quot;; ignore me \n[states] \nChris=Texas&quot;
sectionEx' ::ByteString
sectionEx' =[r|
; ignore me
[states]
Chris=Texas
|]
sectionEx'' ::ByteString
sectionEx'' =[r|
; comment
[section]
host=wikipedia .org
alias=claw
[whatisit]
red=intoothandclaw
|]
Then we get into the section parsing:</p>
<p>CHAPTER 24. PARSER COMBINATORS 1454
dataSection =
Section HeaderAssignments
deriving (Eq,Show)
newtype Config=
Config(MapHeaderAssignments )
deriving (Eq,Show)
skipWhitespace ::Parser()
skipWhitespace =
skipMany (char ' '&lt;|&gt;char'\n')
parseSection ::ParserSection
parseSection = do
skipWhitespace
skipComments
h&lt;-parseHeader
skipEOL
assignments &lt;-some parseAssignment
return$
Section h (M.fromList assignments)
Above, we defined datatypes for a section and an entire INI
config. You’ll notice that parseSection skips both whitespace
and comments now. And it returns the parsed section with</p>
<p>CHAPTER 24. PARSER COMBINATORS 1455
the header (that’s the ℎ) and a map of assignments:
*Data.Ini&gt; parseByteString parseSection mempty sectionEx
Success (Section (Header &quot;states&quot;)
(fromList [(&quot;Chris&quot;,&quot;Texas&quot;)]))
So far, so good. Next, let’s roll the sections up into a Map
that keys section data by section name, with the values being
further more Maps of assignment names mapped to their
values. We use foldrto aggregate the list of sections into a
single Map value:
rollup::Section
-&gt;MapHeaderAssignments
-&gt;MapHeaderAssignments
rollup(Section h a) m=
M.insert h a m
parseIni ::ParserConfig
parseIni = do
sections &lt;-some parseSection
letmapOfSections =
foldr rollup M.empty sections
return ( ConfigmapOfSections)
After you load this code into your REPL, try running:</p>
<p>CHAPTER 24. PARSER COMBINATORS 1456
parseByteString parseIni mempty sectionEx
and comparing it to the output of:
parseByteString parseSection mempty sectionEx
that you saw above.
Now we’ll put these things together. We’re interested in
whether our parsers do what they should do rather than pars-
ing an actual INI file, so we’ll have mainrun some hspectests.
We’ll use a helper function, maybeSuccess , as part of the tests:
maybeSuccess ::Resulta-&gt;Maybea
maybeSuccess (Success a)=Justa
maybeSuccess _ =Nothing
main::IO()
main=hspec$ do
describe &quot;Assignment Parsing&quot; $
it&quot;can parse a simple assignment&quot; $ do
letm=parseByteString
parseAssignment
mempty assignmentEx
r'=maybeSuccess m
print m
r' <code>shouldBe</code> Just(&quot;woot&quot;,&quot;1&quot;)</p>
<p>CHAPTER 24. PARSER COMBINATORS 1457
describe &quot;Header Parsing&quot; $
it&quot;can parse a simple header&quot; $ do
letm=
parseByteString parseHeader
mempty headerEx
r'=maybeSuccess m
print m
r' <code>shouldBe</code> Just(Header&quot;blah&quot;)
describe &quot;Comment parsing&quot; $
it&quot;Skips comment before header&quot; $ do
letp=skipComments &gt;&gt;parseHeader
i=&quot;; woot\n[blah]&quot;
m=parseByteString p mempty i
r'=maybeSuccess m
print m
r' <code>shouldBe</code> Just(Header&quot;blah&quot;)</p>
<p>CHAPTER 24. PARSER COMBINATORS 1458
describe &quot;Section parsing&quot; $
it&quot;can parse a simple section&quot; $ do
letm=parseByteString parseSection
mempty sectionEx
r'=maybeSuccess m
states=
M.fromList [( &quot;Chris&quot;,&quot;Texas&quot;)]
expected' =
Just(Section (Header&quot;states&quot; )
states)
print m
r' <code>shouldBe</code> expected'</p>
<p>CHAPTER 24. PARSER COMBINATORS 1459
describe &quot;INI parsing&quot; $
it&quot;Can parse multiple sections&quot; $ do
letm=
parseByteString parseIni
mempty sectionEx''
r'=maybeSuccess m
sectionValues =
M.fromList
[ (&quot;alias&quot;,&quot;claw&quot;)
, (&quot;host&quot;,&quot;wikipedia.org&quot; )]
whatisitValues =
M.fromList
[(&quot;red&quot;,&quot;intoothandclaw&quot; )]
expected' =
Just(Config
(M.fromList
[ (Header&quot;section&quot;
, sectionValues)
, (Header&quot;whatisit&quot;
, whatisitValues)]))
print m
r' <code>shouldBe</code> expected'
We leave it to you to run this and experiment with it.</p>
<p>CHAPTER 24. PARSER COMBINATORS 1460
24.8 Character and token parsers
All right, that was a lot of code. Let’s all step back and take a
deep breath.
You probably have some idea by now of what we mean by
tokenizing, but the time has come for more detail. Tokeniza-
tion is a handy parsing tactic, so it’s baked into some of the
library functions we’ve been using. It’s worth diving in and
exploring what it means.
Traditionally, parsing has been done in two stages, lexing
and parsing. Characters from a stream will be fed into the
lexer, which will then emit tokens on demand to the parser
until it has no more to emit.9The parser then structures the
stream of tokens into a tree, commonly called an “abstract
syntax tree” or AST:
-- hand-wavy types: Stream because
-- production-grade parsers in Haskell
-- won't use [] for performance reasons
lexer::StreamChar-&gt;StreamToken
parser::StreamToken-&gt;AST
Lexers are simpler, typically performing parses that don’t
require looking ahead into the input stream by more than
9Lexers and tokenizers are similar, separating a stream of text into tokens based on
indicators such as whitespace or newlines; lexers often attach some context to the tokens,
where tokenizers typically do not.</p>
<p>CHAPTER 24. PARSER COMBINATORS 1461
one character or token at a time. Lexers are at times called
tokenizers. Lexing is sometimes done with regular expres-
sions, but a parsing library in Haskell will usually intend that
you do your lexing and parsing with the same API. Lexers (or
tokenizers) and parsers have a lot in common, being primarily
diﬀerentiated by their purpose and class of grammar.10
Insert tokens to play
Let’s play around with some things to see what tokenizing
does for us:
Prelude&gt; parseString (some digit) mempty &quot;123 456&quot;
Success &quot;123&quot;
Prelude&gt; parseString (some (some digit)) mempty &quot;123 456&quot;
Success [&quot;123&quot;]
Prelude&gt; parseString (some integer) mempty &quot;123&quot;
Success [123]
Prelude&gt; parseString (some integer) mempty &quot;123456&quot;
Success [123456]
The problem here is that if we wanted to recognize 123 and
456 as independent strings, we need some kind of separator.
Now we can go ahead and do that manually, but the tokenizers
10Formal grammars — rules for generating strings in a formal language — are placed
in a hierarchy, often called the Chomsky hierarchy after the linguist Noam Chomsky.</p>
<p>CHAPTER 24. PARSER COMBINATORS 1462
inparsers can do it for you too, also handling a mixture of
whitespace and newlines:
Prelude&gt; parseString (some integer) mempty &quot;123 456&quot;
Success [123,456]
Prelude&gt; parseString (some integer) mempty &quot;123\n\n 456&quot;
Success [123,456]
Or even space and newlines interleaved:
Prelude&gt; parseString (some integer) mempty &quot;123 \n \n 456&quot;
Success [123,456]
But simply applying tokentodigitdoesn’t do what you
think:
Prelude&gt; let s = &quot;123 \n \n 456&quot;
Prelude&gt; parseString (token (some digit)) mempty s
Success &quot;123&quot;
Prelude&gt; parseString (token (some (token digit))) mempty s
Success &quot;123456&quot;
Prelude&gt; parseString (some decimal) mempty s
Success [123]
Prelude&gt; parseString (some (token decimal)) mempty s
Success [123,456]
Compare that to the integer function, which is already a
tokenizer:</p>
<p>CHAPTER 24. PARSER COMBINATORS 1463
Prelude&gt; parseString (some integer) mempty &quot;1\n2\n 3\n&quot;
Success [1,2,3]
We can write a tokenizing parser like some integer like this:
p'::Parser[Integer]
p'=some$ do
i&lt;-token (some digit)
return (read i)
And we can compare the output of that to the output of
applying tokentodigit:
Prelude&gt; let s = &quot;1\n2\n3&quot;
Prelude&gt; parseString p' mempty s
Success [1,2,3]
Prelude&gt; parseString (token (some digit)) mempty s
Success &quot;1&quot;
Prelude&gt; parseString (some (token (some digit))) mempty s
Success [&quot;1&quot;,&quot;2&quot;,&quot;3&quot;]
You’ll want to think carefully about the scope at which
you’re tokenizing as well:
Prelude&gt; let tknWhole = token $ char 'a' &gt;&gt; char 'b'
Prelude&gt; parseString tknWhole mempty &quot;a b&quot;
Failure (interactive):1:2: error: expected: &quot;b&quot;</p>
<p>CHAPTER 24. PARSER COMBINATORS 1464
a b<EOF>
^
Prelude&gt; parseString tknWhole mempty &quot;ab ab&quot;
Success 'b'
Prelude&gt; parseString (some tknWhole) mempty &quot;ab ab&quot;
Success &quot;bb&quot;
If we wanted that first example to work, we need to tokenize
the parse of the first character, not the whole a-then-b parse:
Prelude&gt; let tknCharA = (token (char 'a')) &gt;&gt; char 'b'
Prelude&gt; parseString tknCharA mempty &quot;a b&quot;
Success 'b'
Prelude&gt; parseString (some tknCharA) mempty &quot;a ba b&quot;
Success &quot;bb&quot;
Prelude&gt; parseString (some tknCharA) mempty &quot;a b a b&quot;
Success &quot;b&quot;
The last example stops at the first 𝑎 𝑏parse because the
parser doesn’t say anything about a space after 𝑏and the tok-
enization behavior only applies to what followed 𝑎. We can
tokenize both character parsers though:
Prelude&gt; let tknBoth = token (char 'a') &gt;&gt; token (char 'b')
Prelude&gt; parseString (some tknBoth) mempty &quot;a b a b&quot;
Success &quot;bb&quot;</p>
<p>CHAPTER 24. PARSER COMBINATORS 1465
A mild warning: don’t get too tokenization happy. Try to
make it coarse-grained and selective. Overuse of tokenizing
parsersormixturewithcharacterparserscanmakeyourparser
slow or hard to understand. Use your judgment. Keep in mind
that tokenization isn’t exclusively about whitespace; it’s about
ignoring noise so you can focus on the structures you are
parsing.
24.9 Polymorphic parsers
If we take the time to assert polymorphic types for our parsers,
we can get parsers that can be run using attoparsec ,trifecta ,
parsec, or anything else that has implemented the necessary
typeclasses. Let’s give it a whirl, shall we?
{-# LANGUAGE OverloadedStrings #-}
moduleText.Fractions where
importControl.Applicative
importData.Attoparsec.Text (parseOnly )
importData.Ratio ((%))
importData.String (IsString )
importText.Trifecta</p>
<p>CHAPTER 24. PARSER COMBINATORS 1466
badFraction ::IsString s=&gt;s
badFraction =&quot;1/0&quot;
alsoBad ::IsString s=&gt;s
alsoBad =&quot;10&quot;
shouldWork ::IsString s=&gt;s
shouldWork =&quot;1/2&quot;
shouldAlsoWork ::IsString s=&gt;s
shouldAlsoWork =&quot;2/1&quot;
parseFraction ::(Monadm,TokenParsing m)
=&gt;mRational
parseFraction = do
numerator &lt;-decimal
_ &lt;-char'/'
denominator &lt;-decimal
casedenominator of
0-&gt;fail&quot;Denominator cannot be zero&quot;
_ -&gt;return (numerator %denominator)
We’ve left some typeclass-constrained polymorphism in
our type signatures for flexibility. Our mainwill run both
attoparsec andtrifecta versions for us so we can compare the
outputs directly:</p>
<p>CHAPTER 24. PARSER COMBINATORS 1467
main::IO()
main= do
-- parseOnly is Attoparsec
letattoP=parseOnly parseFraction
print$attoP badFraction
print$attoP shouldWork
print$attoP shouldAlsoWork
print$attoP alsoBad
-- parseString is Trifecta
letp f i=
parseString f mempty i
print$p parseFraction badFraction
print$p parseFraction shouldWork
print$p parseFraction shouldAlsoWork
print$p parseFraction alsoBad
Prelude&gt; main
Left &quot;Failed reading: Denominator cannot be zero&quot;
Right (1 % 2)
Right (2 % 1)
Left &quot;&quot;/&quot;: not enough input&quot;
Failure (interactive):1:4: error: Denominator
cannot be zero, expected: digit
1/0<EOF></p>
<p>CHAPTER 24. PARSER COMBINATORS 1468
^
Success (1 % 2)
Success (2 % 1)
Failure (interactive):1:3: error: unexpected
EOF, expected: &quot;/&quot;, digit
10<EOF>
^
See what we meant earlier about the error messages?
It’s not perfect and could bite you
While the polymorphic parser combinators in the parsers li-
brary enable you to write parsers which can then be run with
various parsing libraries, this doesn’t free you of understand-
ing the particularities of each. In general, trifecta tries to
matchparsec ’s behaviors in most respects, the latter of which
is more extensively documented.
Failure and backtracking
Returning to our cursor model of parsers, backtracking is
returning the cursor to where it was before a failing parser
consumed input. In some cases, it can be a little confusing
to debug the same error in two diﬀerent runs of the same
parser doing essentially the same things in trifecta ,parsec,</p>
<p>CHAPTER 24. PARSER COMBINATORS 1469
andattoparsec , but the errors themselves might be diﬀerent.
Let’s consider an example of this.
{-# LANGUAGE OverloadedStrings #-}
We use OverloadedStrings so that we can use string literals as
if they were ByteStrings when testing attoparsec :
moduleBTwhere
importControl.Applicative
import qualified Data.Attoparsec.ByteString
asA
importData.Attoparsec.ByteString
(parseOnly )
importData.ByteString (ByteString )
importText.Trifecta hiding(parseTest )
importText.Parsec (Parsec,parseTest )
trifP::Showa
=&gt;Parsera
-&gt;String-&gt;IO()
trifPp i=
print$parseString p mempty i
Helper function to run a trifecta parser and print the result:</p>
<p>CHAPTER 24. PARSER COMBINATORS 1470
parsecP ::(Showa)
=&gt;ParsecString()a
-&gt;String-&gt;IO()
parsecP =parseTest
Helper function to run a parsec parser and print the result:
attoP::Showa
=&gt;A.Parsera
-&gt;ByteString -&gt;IO()
attoPp i=
print$parseOnly p i
Helper function for attoparsec — same deal as before:
nobackParse ::(Monadf,CharParsing f)
=&gt;fChar
nobackParse =
(char'1'&gt;&gt;char'2')
&lt;|&gt;char'3'
Here’s our first parser. It attempts to parse '1'followed by
'2'or'3'. This parser does not backtrack:</p>
<p>CHAPTER 24. PARSER COMBINATORS 1471
tryParse ::(Monadf,CharParsing f)
=&gt;fChar
tryParse =
try (char '1'&gt;&gt;char'2')
&lt;|&gt;char'3'
This parser has similar behavior to the previous one, except
it backtracks if the first parse fails. Backtracking means that the
input cursor returns to where it was before the failed parser
consumed input.
main::IO()
main= do
-- trifecta
trifP nobackParse &quot;13&quot;
trifP tryParse &quot;13&quot;
-- parsec
parsecP nobackParse &quot;13&quot;
parsecP tryParse &quot;13&quot;
-- attoparsec
attoP nobackParse &quot;13&quot;
attoP tryParse &quot;13&quot;
The error messages you get from each parser are going
to vary a bit. This isn’t because they’re wildly diﬀerent, but</p>
<p>CHAPTER 24. PARSER COMBINATORS 1472
is mostly due to how they attribute errors. You should see
something like:
Prelude&gt; main
Failure (interactive):1:2:
error: expected: &quot;2&quot;
13<EOF>
^
Failure (interactive):1:1: error:
expected: &quot;3&quot;
13<EOF>
^
parse error at (line 1, column 2):
unexpected &quot;3&quot;
expecting &quot;2&quot;
parse error at (line 1, column 2):
unexpected &quot;3&quot;
expecting &quot;2&quot;
Left &quot;&quot;3&quot;: satisfyElem&quot;
Left &quot;&quot;3&quot;: satisfyElem&quot;
Conversely, if you try the valid inputs &quot;12&quot;and&quot;3&quot;with
nobackParse and each of the three parsers, you should see all of
them succeed.
This can be confusing. When you add backtracking to a
parser, error attribution can become more complicated at</p>
<p>CHAPTER 24. PARSER COMBINATORS 1473
times. To avoid this, consider using the &lt;?&gt;operator to anno-
tate parse rules any time you use try.11
tryAnnot ::(Monadf,CharParsing f)
=&gt;fChar
tryAnnot =
(try (char '1'&gt;&gt;char'2')</p>
<?>"Tried 12" )
<p>&lt;|&gt;(char'3'&lt;?&gt;&quot;Tried 3&quot; )
Then running this in the REPL:
Prelude&gt; trifP tryAnnot &quot;13&quot;
Failure (interactive):1:1: error: expected: Tried 12,
Tried 3
13<EOF>
^
Now the error will list the parses it attempted before it failed.
You’ll want to make the annotations more informative than
what we demonstrated in your own parsers.
11Parsec “try a &lt;|&gt; b” considered harmful; Edward Z. Yang</p>
<p>CHAPTER 24. PARSER COMBINATORS 1474
24.10 Marshalling from an AST to a
datatype
Fair warning: This section relies on a little more background
knowledge from you than previous sections have. If you are
not a person who already has some programming experience,
the following may not seem terribly useful to you, and there
may be some unfamiliar terminology and concepts.
The act of parsing, in a sense, is a means of necking down
the cardinality of our inputs to the set of things our programs
have a sensible answer for. It’s unlikely you can do some-
thing meaningful and domain-specific when your input type
isString ,Text, orByteString . However, if you can parse one of
those types into something structured, rejecting bad inputs,
then you might be able to write a proper program. One of the
mistakes programmers make in writing programs handling
text is in allowing their data to stay in the textual format, doing
mind-bending backflips to cope with the unstructured nature
of textual inputs.
In some cases, the act of parsing isn’t enough. You might
have a sort of AST or structured representation of what was
parsed, but from there, you might expect that AST or repre-
sentation to take a particular form. This means we want to
narrow the cardinality and get even more specific about how
our data looks. Often this second step is called unmarshalling</p>
<p>CHAPTER 24. PARSER COMBINATORS 1475
our data. Similarly, marshalling is the act of preparing data
for serialization, whether via memory alone (foreign function
interface boundary) or over a network interface.
The whole idea here is that you have two pipelines for your
data:
Text-&gt;Structure -&gt;Meaning
-- parse -&gt; unmarshall
Meaning -&gt;Structure -&gt;Text
-- marshall -&gt; serialize
There isn’t only one way to accomplish this, but we’ll show
you a commonly used library and how it has this two-stage
pipeline in the API.
Marshalling and unmarshalling JSON data
aesonis presently the most popular JSON12library in Haskell.
One of the things that’ll confuse programmers coming to
Haskell from Python, Ruby, Clojure, JavaScript, or similar
languages, is that there’s usually no unmarshall/marshall step.
Instead, the raw JSON AST will be represented directly as an
untyped blob of data. Users of typed languages are more likely
12JSON stands for JavaScript Object Notation. JSON is, for better or worse, a very
common open-standard data format used to transmit data, especially between browsers
and servers. As such, dealing with JSON is a common programming task, so you might
as well get used to it now.</p>
<p>CHAPTER 24. PARSER COMBINATORS 1476
to have encountered something like this. We’ll be using aeson
0.10.0.0 for the following examples.
{-# LANGUAGE OverloadedStrings #-}
{-# LANGUAGE QuasiQuotes #-}
moduleMarshalling where
importData.Aeson
importData.ByteString.Lazy (ByteString )
importText.RawString.QQ
sectionJson ::ByteString
sectionJson =[r|
{&quot;section&quot; :{&quot;host&quot;:&quot;wikipedia.org&quot; },
&quot;whatisit&quot; :{&quot;red&quot;:&quot;intoothandclaw&quot; }
}
|]
Notethatwe’resayingthetypeof sectionJson isalazy ByteString .
If you get strict and lazy ByteString types mixed up you’ll get
errors.
Provided a strict ByteString when a lazy one was expected:
<interactive>:10:8:
Couldn't match expected type</p>
<p>CHAPTER 24. PARSER COMBINATORS 1477
Data.ByteString.Lazy.Internal.ByteString
with actual type ByteString
NB:
Data.ByteString.Lazy.Internal.ByteString
is defined in
Data.ByteString.Lazy.Internal
ByteString
is defined in
Data.ByteString.Internal
The actual type is what we provided; the expected type is
what the types wanted. The NB:in the type error stands for
nota bene. Either we used the wrong code (so expected type
needs to change), or we provided the wrong values (actual
type, our types/values, need to change). You can reproduce
this error by making the following mistake in the marshalling
module:
-- Change the import of the ByteString
-- type constructor from:
importData.ByteString.Lazy (ByteString )
-- Into:
importData.ByteString (ByteString )</p>
<p>CHAPTER 24. PARSER COMBINATORS 1478
Provided a lazy ByteString when a strict one was expected:
{-# LANGUAGE OverloadedStrings #-}
{-# LANGUAGE QuasiQuotes #-}
moduleWantedStrict where
importData.Aeson
importData.ByteString.Lazy (ByteString )
importText.RawString.QQ
sectionJson ::ByteString
sectionJson =[r|
{&quot;section&quot; :{&quot;host&quot;:&quot;wikipedia.org&quot; },
&quot;whatisit&quot; :{&quot;red&quot;:&quot;intoothandclaw&quot; }
}
|]
main= do
letblah::MaybeValue
blah=decodeStrict sectionJson
print blah
You’ll get the following type error if you load that up:
code/wantedStrictGotLazy.hs:19:27:</p>
<p>CHAPTER 24. PARSER COMBINATORS 1479
Couldn't match expected type
‘Data.ByteString.Internal.ByteString’
with actual type ‘ByteString’
NB:
‘Data.ByteString.Internal.ByteString’
is defined in ‘Data.ByteString.Internal’
‘ByteString’ is defined in ‘Data.ByteString.Lazy.Internal’
In the first argument of ‘decodeStrict’,
namely ‘sectionJson’
In the expression: decodeStrict sectionJson
The more useful information is in the NB:or nota bene,
where the internal modules are mentioned. The key is to re-
member actual type means “your code”, expected type means
“what they expected,” and that the ByteString module that
doesn’t have Lazyin the name is the strict version. We can
modify our code a bit to get nicer type errors:</p>
<p>CHAPTER 24. PARSER COMBINATORS 1480
-- replace the (ByteString)
-- import with these
import qualified Data.ByteString asBS
import qualified Data.ByteString.Lazy
asLBS
-- edit the type sig for this one
sectionJson ::LBS.ByteString
Then we’ll get the following type error instead:
Couldn't match expected type ‘BS.ByteString’
with actual type ‘LBS.ByteString’
NB: ‘BS.ByteString’ is defined in
‘Data.ByteString.Internal’
‘LBS.ByteString’ is defined in
‘Data.ByteString.Lazy.Internal’
In the first argument of ‘decodeStrict’,
namely ‘sectionJson’
In the expression: decodeStrict sectionJson
This is helpful because we have both versions available as
qualified modules. You may not always be so fortunate and
will need to remember which is which.</p>
<p>CHAPTER 24. PARSER COMBINATORS 1481
Back to the JSON
Let’s get back to handling JSON. The most common functions
for using aesonare the following:
Prelude&gt; import Data.Aeson
Prelude&gt; :t encode
encode :: ToJSON a =&gt; a -&gt; LBS.ByteString
Prelude&gt; :t decode
decode :: FromJSON a =&gt; LBS.ByteString -&gt; Maybe a
These functions are sort of eliding the intermediate step
that passes through the Value type in aeson, which is a datatype
JSON AST — “sort of,” because you can decode the raw JSON
data into a Valueanyway:
Prelude&gt; decode sectionJson :: Maybe Value
Just (Object (fromList [
(&quot;whatisit&quot;,
Object (fromList [(&quot;red&quot;,
String &quot;intoothandclaw&quot;)])),
(&quot;section&quot;,
Object (fromList [(&quot;host&quot;,
String &quot;wikipedia.org&quot;)]))]))
Not, uh, super pretty. We’ll figure out something nicer in
a moment. Also do not forget to assert a type, or the type-
defaulting in GHCi will do silly things:</p>
<p>CHAPTER 24. PARSER COMBINATORS 1482
Prelude&gt; decode sectionJson
Nothing
Now what if we do want a nicer representation for this JSON
noise? Well, let’s define our datatypes and see if we can decode
the JSON into our type:</p>
<p>CHAPTER 24. PARSER COMBINATORS 1483
{-# LANGUAGE OverloadedStrings #-}
{-# LANGUAGE QuasiQuotes #-}
moduleMarshalling where
importControl.Applicative
importData.Aeson
importData.ByteString.Lazy (ByteString )
import qualified Data.Text asT
importData.Text (Text)
importText.RawString.QQ
sectionJson ::ByteString
sectionJson =[r|
{&quot;section&quot; :{&quot;host&quot;:&quot;wikipedia.org&quot; },
&quot;whatisit&quot; :{&quot;red&quot;:&quot;intoothandclaw&quot; }
}
|]
dataTestData =
TestData {
section ::Host
, what::Color
}deriving (Eq,Show)
newtype Host=
HostString
deriving (Eq,Show)
typeAnnotation =String
dataColor=
RedAnnotation
|BlueAnnotation
|YellowAnnotation
deriving (Eq,Show)
main= do
letd::MaybeTestData
d=decode sectionJson
print d</p>
<p>CHAPTER 24. PARSER COMBINATORS 1484
This will in fact net you a type error complaining about
there not being an instance of FromJSON forTestData . Which is
true! GHC has no idea how to unmarshall JSON data (in the
form of a Value) into a TestData value. Let’s add an instance so
it knows how:</p>
<p>CHAPTER 24. PARSER COMBINATORS 1485
instance FromJSON TestData where
parseJSON ( Objectv)=
TestData &lt;$&gt;v.:&quot;section&quot;
&lt;*&gt;v.:&quot;whatisit&quot;
parseJSON _ =
fail&quot;Expected an object for TestData&quot;
instance FromJSON Hostwhere
parseJSON ( Objectv)=
Host&lt;$&gt;v.:&quot;host&quot;
parseJSON _ =
fail&quot;Expected an object for Host&quot;
instance FromJSON Colorwhere
parseJSON ( Objectv)=
(Red&lt;$&gt;v.:&quot;red&quot;)
&lt;|&gt;(Blue&lt;$&gt;v.:&quot;blue&quot;)
&lt;|&gt;(Yellow&lt;$&gt;v.:&quot;yellow&quot; )
parseJSON _ =
fail&quot;Expected an object for Color&quot;
Also note that you can use quasiquotes to avoid having to
escape quotation marks in the REPL as well:
Prelude&gt; :set -XOverloadedStrings
Prelude&gt; decode &quot;{&quot;blue&quot;: &quot;123&quot;}&quot; :: Maybe Color</p>
<p>CHAPTER 24. PARSER COMBINATORS 1486
Just (Blue &quot;123&quot;)
Prelude&gt; :set -XQuasiQuotes
Prelude&gt; decode [r|{&quot;red&quot;: &quot;123&quot;}|] :: Maybe Color
Just (Red &quot;123&quot;)
To relate what we just did back to the relationship between
parsing and marshalling, the idea is that our FromJSON instance
is accepting the Valuetype and ToJSON instances generate the
Valuetype, closing the following loop:
-- FromJSON
ByteString -&gt;Value-&gt;yourType
-- parse -&gt; unmarshall
-- ToJSON
yourType -&gt;Value-&gt;ByteString
-- marshall -&gt; serialize
The definition of Valueat time of writing is the following:</p>
<p>CHAPTER 24. PARSER COMBINATORS 1487
-- | A JSON value represented
-- as a Haskell value.
dataValue=Object!Object
|Array!Array
|String!Text
|Number!Scientific
|Bool!Bool
|Null
deriving (Eq,Read,Show,
Typeable ,Data)
What if we want to unmarshall something that could be a
Number or aString ?
dataNumberOrString =
NumbaInteger
|Stringy Text
deriving (Eq,Show)
instance FromJSON NumberOrString where
parseJSON ( Numberi)=return$Numbai
parseJSON ( Strings)=return$Stringy s
parseJSON _ =
fail&quot;NumberOrString must <br />
\be number or string&quot;</p>
<p>CHAPTER 24. PARSER COMBINATORS 1488
This won’t quite work at first. The trouble is that JSON (and
JavaScript, as it happens) only has one numeric type and that
type is a IEEE-754 float. JSON (and JavaScript, terrifyingly)
have no integral types or integers, so aesonhas to pick one
representation that works for all possible JSON numbers. The
most precise way to do that is the Scientific type which is an
arbitrarily precise numerical type (you may remember this
from way back in Chapter 4, Basic Datatypes). So we need to
convert from a Scientific to anInteger :
importControl.Applicative
importData.Aeson
importData.ByteString.Lazy (ByteString )
import qualified Data.Text asT
importData.Text (Text)
importText.RawString.QQ
importData.Scientific (floatingOrInteger )
dataNumberOrString =
NumbaInteger
|Stringy Text
deriving (Eq,Show)</p>
<p>CHAPTER 24. PARSER COMBINATORS 1489
instance FromJSON NumberOrString where
parseJSON ( Numberi)=
casefloatingOrInteger i of
(Left_)-&gt;
fail&quot;Must be integral number&quot;
(Rightinteger) -&gt;
return$Numbainteger
parseJSON ( Strings)=return$Stringy s
parseJSON _ =
fail&quot;NumberOrString must <br />
\be number or string&quot;
-- so it knows what we want to parse
dec::ByteString
-&gt;MaybeNumberOrString
dec=decode
eitherDec ::ByteString
-&gt;EitherStringNumberOrString
eitherDec =eitherDecode
main= do
print$dec&quot;blah&quot;
Now let’s give it a whirl:</p>
<p>CHAPTER 24. PARSER COMBINATORS 1490
Prelude&gt; main
Nothing
Butwhathappened? Wecanrewritethecodetouse eitherDec
to get a slightly more helpful type error:
main= do
print$dec&quot;blah&quot;
print$eitherDec &quot;blah&quot;
Then reloading the code and trying again in the REPL:
Prelude&gt; main
Nothing
Left &quot;Error in $: Failed reading:
not a valid json value&quot;
By that means, we are able to get more informative errors
fromaeson. If we wanted some examples that worked, we
could try things like the following:
Prelude&gt; dec &quot;123&quot;
Just (Numba 123)
Prelude&gt; dec &quot;&quot;blah&quot;&quot;
Just (Stringy &quot;blah&quot;)
It’s worth getting comfortable with aesoneven if you don’t
plan to work with much JSON because many serialization</p>
<p>CHAPTER 24. PARSER COMBINATORS 1491
libraries in Haskell follow a similar API pattern. Play with the
example and see how you need to change the type of decto
be able to parse a list of numbers or strings.
24.11 Chapter Exercises
1.Write a parser for semantic versions as defined by http:
//semver.org/ . After making a working parser, write an Ord
instance for the SemVer type that obeys the specification
outlined on the SemVer website.</p>
<p>CHAPTER 24. PARSER COMBINATORS 1492
-- Relevant to precedence/ordering,
-- cannot sort numbers like strings.
dataNumberOrString =
NOSSString
|NOSIInteger
typeMajor=Integer
typeMinor=Integer
typePatch=Integer
typeRelease =[NumberOrString ]
typeMetadata =[NumberOrString ]
dataSemVer=
SemVerMajorMinorPatchRelease Metadata
parseSemVer ::ParserSemVer
parseSemVer =undefined
Expected results:
Prelude&gt; parseString parseSemVer mempty &quot;2.1.1&quot;
Success (SemVer 2 1 1 [] [])
Prelude&gt; parseString parseSemVer mempty &quot;1.0.0-x.7.z.92&quot;
Success (SemVer 1 0 0
[NOSS &quot;x&quot;, NOSI 7, NOSS &quot;z&quot;, NOSI 92] [])</p>
<p>CHAPTER 24. PARSER COMBINATORS 1493
Prelude&gt; SemVer 2 1 1 [] [] &gt; SemVer 2 1 0 [] []
True
2.Write a parser for positive integer values. Don’t reuse the
pre-existing digitorinteger functions, but you can use
the rest of the libraries we’ve shown you so far. You are
not expected to write a parsing library from scratch.
parseDigit ::ParserChar
parseDigit =undefined
base10Integer ::ParserInteger
base10Integer =undefined
Expected results:
Prelude&gt; parseString parseDigit mempty &quot;123&quot;
Success '1'
Prelude&gt; parseString parseDigit mempty &quot;abc&quot;
Failure (interactive):1:1: error: expected: parseDigit
abc<EOF>
^
Prelude&gt; parseString base10Integer mempty &quot;123abc&quot;
Success 123
Prelude&gt; parseString base10Integer mempty &quot;abc&quot;
Failure (interactive):1:1: error: expected: integer</p>
<p>CHAPTER 24. PARSER COMBINATORS 1494
abc<EOF>
^
Hint: Assume you’re parsing base-10 numbers. Use arith-
metic as a cheap “accumulator” for your final number as
you parse each digit left-to-right.
3.Extend the parser you wrote to handle negative and pos-
itive integers. Try writing a new parser in terms of the
one you already have to do this.
Prelude&gt; parseString base10Integer' mempty &quot;-123abc&quot;
Success (-123)
4.Write a parser for US/Canada phone numbers with vary-
ing formats.</p>
<p>CHAPTER 24. PARSER COMBINATORS 1495
-- aka area code
typeNumberingPlanArea =Int
typeExchange =Int
typeLineNumber =Int
dataPhoneNumber =
PhoneNumber NumberingPlanArea
Exchange LineNumber
deriving (Eq,Show)
parsePhone ::ParserPhoneNumber
parsePhone =undefined
With the following behavior:
Prelude&gt; parseString parsePhone mempty &quot;123-456-7890&quot;
Success (PhoneNumber 123 456 7890)
Prelude&gt; parseString parsePhone mempty &quot;1234567890&quot;
Success (PhoneNumber 123 456 7890)
Prelude&gt; parseString parsePhone mempty &quot;(123) 456-7890&quot;
Success (PhoneNumber 123 456 7890)
Prelude&gt; parseString parsePhone mempty &quot;1-123-456-7890&quot;
Success (PhoneNumber 123 456 7890)
Cf. Wikipedia’s article on “National conventions for writ-
ing telephone numbers”. You are encouraged to adapt the</p>
<p>CHAPTER 24. PARSER COMBINATORS 1496
exercise to your locality’s conventions if they are not part
of the NNAP scheme.
5.Write a parser for a log file format and sum the time
spent in each activity. Additionally, provide an alterna-
tive aggregation of the data that provides average time
spent per activity per day. The format supports the use
of comments which your parser will have to ignore. The
#characters followed by a date mark the beginning of a
particular day.
Log format example:
-- wheee a comment</p>
<h1 id="2025-02-05"><a class="header" href="#2025-02-05">2025-02-05</a></h1>
<p>08:00 Breakfast
09:00 Sanitizing moisture collector
11:00 Exercising in high-grav gym
12:00 Lunch
13:00 Programming
17:00 Commuting home in rover
17:30 R&amp;R
19:00 Dinner
21:00 Shower
21:15 Read
22:00 Sleep</p>
<p>CHAPTER 24. PARSER COMBINATORS 1497</p>
<h1 id="2025-02-07----dates-not-nececessarily-sequential"><a class="header" href="#2025-02-07----dates-not-nececessarily-sequential">2025-02-07 -- dates not nececessarily sequential</a></h1>
<p>08:00 Breakfast -- should I try skippin bfast?
09:00 Bumped head, passed out
13:36 Wake up, headache
13:37 Go to medbay
13:40 Patch self up
13:45 Commute home for rest
14:15 Read
21:00 Dinner
21:15 Read
22:00 Sleep
You are to derive a reasonable datatype for represent-
ing this data yourself. For bonus points, make this bi-
directionalbymakingaShowrepresentationforthedatatype
which matches the format you are parsing. Then write a
generator for this data using QuickCheck’s Gen and see if
you can break your parser with QuickCheck.
6.Write a parser for IPv4 addresses.
importData.Word
dataIPAddress =
IPAddress Word32
deriving (Eq,Ord,Show)</p>
<p>CHAPTER 24. PARSER COMBINATORS 1498
A 32-bit word is a 32-bit unsigned int. Lowest value is 0
rather than being capable of representing negative num-
bers, but the highest possible value in the same number
of bits is twice as high. Note:
Prelude&gt; import Data.Int
Prelude&gt; import Data.Word
Prelude&gt; maxBound :: Int32
2147483647
Prelude&gt; maxBound :: Word32
4294967295
Prelude&gt; div 4294967295 2147483647
2
Word32 is an appropriate and compact way to represent
IPv4 addresses. You are expected to figure out not only
how to parse the typical IP address format, but how IP
addresses work numerically insofar as is required to write
a working parser. This will require using a search engine
unless you have an appropriate book on internet network-
ing handy.
Example IPv4 addresses and their decimal representa-
tions:
172.16.254.1 -&gt; 2886794753
204.120.0.15 -&gt; 3430416399</p>
<p>CHAPTER 24. PARSER COMBINATORS 1499
7.Same as before, but IPv6.
importData.Word
dataIPAddress6 =
IPAddress6 Word64Word64
deriving (Eq,Ord,Show)
Example IPv6 addresses and their decimal representa-
tions:
0:0:0:0:0:ffff:ac10:fe01 -&gt; 281473568538113
0:0:0:0:0:ffff:cc78:f -&gt; 281474112159759
FE80:0000:0000:0000:0202:B3FF:FE1E:8329 -&gt;
338288524927261089654163772891438416681
2001:DB8::8:800:200C:417A -&gt;
42540766411282592856906245548098208122
One of the trickier parts about IPv6 will be full vs. col-
lapsed addresses and the abbrevations. See this Q&amp;A
thread13about IPv6 abbreviations for more.
Ensure you can parse abbreviated variations of the earlier
examples like:
13http://answers.google.com/answers/threadview/id/770645.html</p>
<p>CHAPTER 24. PARSER COMBINATORS 1500
FE80::0202:B3FF:FE1E:8329
2001:DB8::8:800:200C:417A
8.Removethederived ShowinstancesfromtheIPAddress/IPAd-
dress6 types, and write your own Showinstance for each
type that renders in the typical textual format appropriate
to each.
9.Write a function that converts between IPAddress and
IPAddress6.
10.Write a parser for the DOT language14that Graphviz uses
to express graphs in plain-text.
We suggest you look at the AST datatype in Haphviz15for
ideas on how to represent the graph in a Haskell datatype.
If you’re feeling especially robust, you can try using fgl16.
24.12 Definitions
1.A parser parses.
You read the chapter right?
2.A parser combinator combines two or more parsers to
produce a new parser. Good examples of this are things
14http://www.graphviz.org/doc/info/lang.html
15http://hackage.haskell.org/package/haphviz
16http://hackage.haskell.org/package/fgl</p>
<p>CHAPTER 24. PARSER COMBINATORS 1501
like using &lt;|&gt;fromAlternative to produce a new parser
from the disjunction of two parser arguments to &lt;|&gt;. Or
some. Ormany. Ormappend . Or(&gt;&gt;).
3.Marshalling is transforming a potentially nonlinear rep-
resentation of data in memory into a format that can be
stored on disk or transmitted over a network socket. Go-
ing in the opposite direction is called unmarshalling. Cf.
serialization and deserialization.
4.A token(izer) converts text, usually a stream of characters,
into more meaningful or “chunkier” structures such as
words, sentences, or symbols. The linesandwordsfunc-
tions you’ve used earlier in this book are like very unso-
phisticated tokenizers.
5.Lexer — see tokenizer.
24.13 Follow-up resources
1.Parsec try a-or-b considered harmful; Edward Z. Yang
2.Code case study: parsing a binary data format; Real World
Haskell
3.The Parsec parsing library; Real World Haskell</p>
<p>CHAPTER 24. PARSER COMBINATORS 1502
4.An introduction to parsing text in Haskell with Parsec;
James Wilson;
http://unbui.lt/#!/post/haskell-parsec-basics
5.Parsing CSS with Parsec; Jakub Arnold
6.Parsec: A practical parser library; Daan Leijen, Erik Mei-
jer;
http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.24.
5200
7.How to Replace Failure by a List of Successes; Philip
Wadler;
http://dl.acm.org/citation.cfm?id=5288
8.How to Replace Failure by a Heap of Successes; Edward
Kmett
9.Two kinds of backtracking; Samuel Gélineau (gelisam);
http://gelisam.blogspot.ca/2015/09/two-kinds-of-backtracking.
html
10.LL and LR in Context: Why Parsing Tools Are Hard; Josh
Haberman
http://blog.reverberate.org/2013/09/ll-and-lr-in-context-why-parsing-tools.
html
11.Parsing Techniques, a practical guide; second edition;
Grune &amp; Jacobs</p>
<p>CHAPTER 24. PARSER COMBINATORS 1503
12.Parsing JSON with Aeson; School of Haskell
13.aeson; 24 days of Hackage; Oliver Charles</p>
<p>Chapter 25
Composing types
The last thing one
discovers in composing a
work is what to put first.
T. S. Eliot
1504</p>
<p>CHAPTER 25. E PLURIBUS MONAD 1505
25.1 Composing types
This chapter and the next are about monad transformers, both
the principles behind them and the practicalities of using them.
For many programmers, monad transformers are indistin-
guishable from magick, so we want to approach them from
both angles and demonstrate that they are both comprehen-
sible via their types and practical in normal programming.
Functors and applicatives are both closed under composi-
tion: this means that you can compose two functors (or two
applicatives) and return another functor (or applicative, as the
case may be). This is not true of monads, however; when you
compose two monads, the result is not necessarily another
monad. We will see this soon.
However, there are times when composing monads is desir-
able. Diﬀerent monads allow us to work with diﬀerent eﬀects.
Composing monads allows you to build up computations with
multiple eﬀects. By stacking, for example, a Maybemonad with
anIO, you can be performing IOactions while also building up
computations that have a possibility of failure, handled by the
Maybemonad.
A monad transformer is a variant of an ordinary type that
takes an additional type argument which is assumed to have a
Monadinstance. For example, MaybeT is the transformer variant
of theMaybetype. The transformer variant of a type gives us</p>
<p>CHAPTER 25. E PLURIBUS MONAD 1506
aMonadinstance that binds over both bits of structure. This
allows us to compose monads and combine their eﬀects. Get-
ting comfortable with monad transformers is important to
becoming proficient in Haskell, so we’re going to take it pretty
slowly and go step by step. You won’t necessarily want to start
out early on defining a bunch of transformer stacks yourself,
but familiarity with them will help a great deal in using other
people’s libraries.
In this chapter, we will
•demonstrate why composing two monads does not give
you another monad;
•examine the Identity andCompose types;
•manipulate types until we can make monads compose;
•meet some common monad transformers;
•work through an Identity crisis.
25.2 Common functions as types
We’ll start in a place that may seem a little strange and point-
less at first, with newtypes that correspond to some very basic
functions. We can construct types that are like those func-
tions because we have types that can take arguments — that</p>
<p>CHAPTER 25. E PLURIBUS MONAD 1507
is, type constructors. In particular, we’ll be using types that
correspond to idand(.).
You’ve seen some of the types we’re going to use in the
following sections before, but we’ll be putting them to some
novel uses. The idea here is to use these datatypes as helpers in
order to demonstrate the problems with composing monads,
and we’ll see how these type constructors can also serve as
monad transformers, because a monad transformer is a type
constructor that takes a monad as an argument.
Identity is boring
You’ve seen this type in previous chapters, sometimes as a
datatype and sometimes as a newtype. We’ll construct the
type diﬀerently this time, as a newtype with a helper function
of the sort we saw in Reader andState:
newtype Identity a=
Identity { runIdentity ::a }
We’ll be using the newtype in this chapter because the
monad transformer version, IdentityT , is usually written as
a newtype. The use of the prefixes runorgetindicates that
these accessor functions are means of extracting the underly-
ing value from the type. There is no real diﬀerence in meaning
between runandget. You’ll see these accessor functions often,</p>
<p>CHAPTER 25. E PLURIBUS MONAD 1508
particularly with utility types like Identity or transformer vari-
ants reusing an original type.
Anoteaboutnewtypes Whilemonadtransformertypescould
be written using the datakeyword, they are most commonly
written as newtypes, and we’ll be sticking with that pattern
here. They are only newtyped to avoid unnecessary overhead,
as newtypes, as we recall, have an underlying representation
identical to the type they contain. The important thing is that
monad transformers are never sum or product types; they are
always a means of wrapping one extra layer of (monadic) struc-
ture around a type, so there is never a reason they couldn’t
be newtypes. Haskellers have a general tendency to avoid
adding additional runtime overhead if they can, so if they can
newtype it, they most often will.
Another thing we want to notice about Identity is the sim-
ilarity of the kind of our Identity type to the type of the id
function, although the fidelity of the comparison isn’t perfect
given the limitations of type-level computation in Haskell:
Prelude&gt; :t id
id :: a -&gt; a
Prelude&gt; :k Identity
Identity :: * -&gt; *
The kind signature of the type resembles the type signature</p>
<p>CHAPTER 25. E PLURIBUS MONAD 1509
of the function, which we hope isn’t too much of a surprise.
Fine, so far — not much new here. Yet.
Compose
We mentioned above that we can also construct a datatype
that corresponds to function composition.
Here is the Compose type. It should look to you much like
function composition, but in this case, the 𝑓and𝑔represent
type constructors, not term-level functions:
newtype Compose f g a=
Compose { getCompose ::f (g a) }
deriving (Eq,Show)
So, we have a type constructor that takes three type argu-
ments: 𝑓and𝑔must be type constructors themselves, while 𝑎
will be a concrete type (consider the relationship between type
constructors and term-level functions on the one hand, and
values and type constants on the other). As we did above, let’s
look at the kind of Compose — note the kinds of the arguments
to the type constructor:
Compose :: (* -&gt; <em>) -&gt; (</em> -&gt; *) -&gt; * -&gt; *
Does that remind you of anything?
(.)::(b-&gt;c)-&gt;(a-&gt;b)-&gt;a-&gt;c</p>
<p>CHAPTER 25. E PLURIBUS MONAD 1510
So, what does that look like in practice? Something like this:
Prelude&gt; Compose [Just 1, Nothing]
Compose {getCompose = [Just 1,Nothing]}
Prelude&gt; let xs = [Just (1::Int), Nothing]
Prelude&gt; :t Compose xs
Compose [Just (1 :: Int), Nothing]
:: Compose [] Maybe Int
Given the above value, the type variables get bound accord-
ingly:
Compose [Just(1::Int),Nothing]
Compose { getCompose ::f (g a) }
Compose []MaybeInt
f~[]
g~Maybe
a~Int
We have one bit of structure wrapped around another, then
a value type (the 𝑎) because the whole thing still has to be kind
*in the end.
We’ve made the point in previous chapters that type con-
structors are functions. Type constructors can take other type</p>
<p>CHAPTER 25. E PLURIBUS MONAD 1511
constructors as arguments, too, just as functions can take other
functions as arguments. This is what allows us to compose
types.
25.3 Two little functors sittin’ in a tree,
L-I-F-T-I-N-G
Let’s start with composing functors, using the types we saw
above. We know we can lift over Identity ; you’ve seen this
Functor before:
instance Functor Identity where
fmap f ( Identity a)=Identity (f a)
Identity here gives us a sort of vanilla Functor that doesn’t do
anything interesting but captures the essence of what Functor is
about. The function gets lifted into the context of the Identity
type and then mapped over the 𝑎value.
It turns out we can get a Functor instance for Compose , too, if
we ask that the 𝑓and𝑔both have Functor instances:
instance (Functor f,Functor g)=&gt;
Functor (Compose f g)where
fmap f ( Compose fga)=
Compose $(fmap.fmap) f fga</p>
<p>CHAPTER 25. E PLURIBUS MONAD 1512
Now the 𝑓and the 𝑔both have to be part of the structure
that we’re lifting over, so they both have to be Functor s them-
selves. We need to be able to jump over both those layers in
order to apply to the value that’s ultimately inside. We have
tofmaptwice to get to that value inside because of the layered
structures.
To return to the example we used above, we have this type:
newtype Compose f g a=
Compose { getCompose ::f (g a) }
deriving (Eq,Show)
Compose { getCompose ::f (g a) }
Compose []MaybeInt
And if we use our Functor instance, we can apply a function
to theIntvalue wrapped up in all that structure:
Prelude&gt; let xs = [Just 1, Nothing]
Prelude&gt; Compose xs
Compose {getCompose = [Just 1,Nothing]}
Prelude&gt; fmap (+1) (Compose xs)
Compose {getCompose = [Just 2,Nothing]}</p>
<p>CHAPTER 25. E PLURIBUS MONAD 1513
We can generalize this to diﬀerent amounts of structure,
such as with one less bit of structure. You may remember this
from a previous chapter:
newtype Onef a=
One(f a)
deriving (Eq,Show)
instance Functor f=&gt;
Functor (Onef)where
fmap f ( Onefa)=One$fmap f fa
Or one more layer of structure than Compose :
newtype Threef g h a =
Three(f (g (h a)))
deriving (Eq,Show)
instance (Functor f,Functor g,Functor h)
=&gt;Functor (Threef g h)where
fmap f ( Threefgha)=
Three$(fmap.fmap.fmap) f fgha
As with the anonymous product (,)and the anonymous
sumEither, theCompose type allows us to express arbitrarily
nested types:</p>
<p>CHAPTER 25. E PLURIBUS MONAD 1514
v::Compose []
Maybe
(Compose Maybe[]Integer)
v=Compose [Just(Compose $Just[1])]
The way to think about this is that the composition of two
datatypes that have a Functor instance gives rise to a new Functor
instance. You’ll sometimes see people refer to this as functors
being closed under composition which means that when you
compose two Functor s, you get another Functor .
25.4 Twinplicative
You probably guessed this was our next step in Compose -landia.
Applicatives, it turns out, are also closed under composition.
We can compose two types that have Applicative instances and
get a new Applicative instance. But you’re going to write it.</p>
<p>CHAPTER 25. E PLURIBUS MONAD 1515
GOTCHA! Exercise time
-- instance types provided as
-- they may help.
{-# LANGUAGE InstanceSigs #-}
instance (Applicative f,Applicative g)
=&gt;Applicative (Compose f g)where
pure::a-&gt;Compose f g a
pure=undefined
(&lt;<em>&gt;)::Compose f g (a-&gt;b)
-&gt;Compose f g a
-&gt;Compose f g b
(Compose f)&lt;</em>&gt;(Compose a)=undefined
We mentioned in an earlier chapter that Applicative is a
weaker algebra than Monad, and that sometimes there are bene-
fits to preferring an Applicative when you don’t need the full
power of the Monad. This is one of those benefits. To compose
Applicative s, you don’t need to do the legwork that Monads re-
quire in order to compose and still have a Monad. Oh, yes, right
— we still haven’t quite made it to monads composing, but
we’re about to.</p>
<p>CHAPTER 25. E PLURIBUS MONAD 1516
25.5 Twonad?
What about Monad? There’s no problem composing two arbi-
trary datatypes that have Monadinstances. We saw this already
when we used Compose withMaybeand list, which both have Monad
instances defined. However, the result of having done so does
not give you a Monad.
The issue comes down to a lack of information. Both types
Compose is working with are polymorphic, so when you try to
write bind for the Monad, you’re trying to combine two poly-
morphic binds into a single combined bind. This, it turns out,
is not possible:
{-# LANGUAGE InstanceSigs #-}
-- impossible.
instance (Monadf,Monadg)
=&gt;Monad(Compose f g)where
return=pure
(&gt;&gt;=)::Compose f g a
-&gt;(a-&gt;Compose f g b)
-&gt;Compose f g b
(&gt;&gt;=)= ???</p>
<p>CHAPTER 25. E PLURIBUS MONAD 1517
These are the types we’re trying to combine, because 𝑓and
𝑔are necessarily both monads with their own Monadinstances:
Monadf=&gt;f a-&gt;(a-&gt;f b)-&gt;f b
Monadg=&gt;g a-&gt;(a-&gt;g b)-&gt;g b
From those, we are trying to write this bind:
(Monadf,Monadg)
=&gt;f (g a) -&gt;(a-&gt;f (g b)) -&gt;f (g b)
Or formulated diﬀerently:
(Monadf,Monadg)
=&gt;f (g (f (g a))) -&gt;f (g a)
And this is not possible. There’s not a good way to jointhat
final𝑓and𝑔. It’s a great exercise to try to make it work, because
the barriers you’ll run into are instructive in their own right.
You can also read Composing monads1by Mark P. Jones and
Luc Duponcheel to see why it’s not possible.
No free burrito lunches
Since getting another Monadgiven the composition of two arbi-
trary types that have a Monadinstance is impossible, what can
we do to get a Monadinstance for combinations of types? The
1http://web.cecs.pdx.edu/~mpj/pubs/RR-1004.pdf</p>
<p>CHAPTER 25. E PLURIBUS MONAD 1518
answer is, monad transformers. We’ll get to that after a little
break for some exercises.
25.6 Exercises: Compose Instances
1.Write the Compose Foldable instance.
ThefoldMap = undefined bit is a hint to make it easier and
look more like what you’ve seen already.
instance (Foldable f,Foldable g)=&gt;
Foldable (Compose f g)where
foldMap =undefined
2.Write the Compose Traversable instance.
instance (Traversable f,Traversable g)=&gt;
Traversable (Compose f g)where
traverse =undefined
And now for something completely diﬀerent
This has nothing to do with anything else in this chapter, but
it makes for a fun exercise.</p>
<p>CHAPTER 25. E PLURIBUS MONAD 1519
classBifunctor pwhere
{-# MINIMAL bimap | first, second #-}
bimap::(a-&gt;b)
-&gt;(c-&gt;d)
-&gt;p a c
-&gt;p b d
bimap f g =first f .second g
first::(a-&gt;b)-&gt;p a c-&gt;p b c
first f =bimap f id
second::(b-&gt;c)-&gt;p a b-&gt;p a c
second=bimap id
It’s a functor that can map over two type arguments instead
of one. Write Bifunctor instances for the following types:
1.The less you think, the easier it’ll be.
dataDeuxa b=Deuxa b
2.dataConsta b=Consta
3.dataDreia b c=Dreia b c
4.dataSuperDrei a b c=SuperDrei a b</p>
<p>CHAPTER 25. E PLURIBUS MONAD 1520
5.dataSemiDrei a b c=SemiDrei a
6.dataQuadriceps a b c d =
Quadzzz a b c d
7.dataEithera b=
Lefta
|Rightb
25.7 Monad transformers
We’ve now seen what the problem with Monadis: you can put
two together but you don’t get a new Monadinstance out of it.
When we need to get a new Monadinstance, we need a monad
transformer. It’s not magic; the answer is in the types.
We said above that a monad transformer is a type construc-
tor that takes a Monadas an argument and returns a Monadas
a result. We also noted that the fundamental problem with
composing two monads lies in the impossibility of joining two
unknown monads. In order to make that joinhappen, we need
to reduce the polymorphism and get concrete information
about one of the monads that we’re working with. The other
monad remains polymorphic as a variable type argument to
our type constructor. Transformers help you make a monad
out of multiple (2, 3, 4...) types that each have a Monadinstance</p>
<p>CHAPTER 25. E PLURIBUS MONAD 1521
by wrapping around existing monads that provide each bit of
wanted functionality.
The types are tricky here, so we’re going to be walking
through writing monad transformers very slowly. Parts of
what follows may seem tedious, so work through it as slowly
or quickly as you need to.
Monadic stacking
Applicative allows us to apply functions of more than one ar-
gument in the presence of functorial structure, enabling us to
cope with this transition:
-- from this:
fmap(+1) (Just1)
-- to this:
(,,)
&lt;$&gt;Just1
&lt;<em>&gt;Just&quot;lol&quot;
&lt;</em>&gt;Just[1,2]
Sometimes we want a (&gt;&gt;=)which can address more than
oneMonadat once. You’ll often see this in applications that have
multiple things going on, such as a web app where combining
Reader andIOis common. You want IOso you can perform ef-
fectful actions like talking to a database and also Reader for the</p>
<p>CHAPTER 25. E PLURIBUS MONAD 1522
database connection(s) and/or HTTP request context. Some-
times you may even want multiple Readers (app-specific data
vs. what the framework provides by default), although usually
there’s a way to add only the data you want to a product type
of a single Reader .
So the question becomes, how do we get one big bind over
a type like the following?
IO(ReaderString[a])
-- where the Monad instances involved
-- are that of IO, Reader, and []
Doing it badly
We could make one-oﬀ types for each combination, but this
will get tiresome quickly. For example:
newtype MaybeIO a=
MaybeIO { runMaybeIO ::IO(Maybea) }
newtype MaybeList a=
MaybeList { runMaybeList ::[Maybea] }
We don’t need to resort to this; we can get a Monadfor two
types, as long as we know what one of the types is. Transform-</p>
<p>CHAPTER 25. E PLURIBUS MONAD 1523
ers are a means of avoiding making a one-oﬀ Monadfor every
possible combination of types.
25.8 IdentityT
Much as Identity helps show oﬀ the most basic essence of
Functor ,Applicative , andMonad,IdentityT is going to help you
begin to understand monad transformers. Using this type that
doesn’t have a lot of interesting stuﬀ going on with it will help
keep us focused on the types and the important fundamentals
of transformers. What we see here will be applicable to other
transformers as well, but types like Maybeand list introduce
other possibilities (failure cases, empty lists) that complicate
things a bit.
First, let’s compare the Identity type you’ve seen up to this
point and our new IdentityT datatype:
-- Plain old Identity. 'a' can be
-- something with more structure,
-- but it's not required and Identity
-- won't know anything about it.
newtype Identity a=
Identity { runIdentity ::a }
deriving (Eq,Show)</p>
<p>CHAPTER 25. E PLURIBUS MONAD 1524
-- The identity monad transformer, serving
-- only to to specify that additional
-- structure should exist.
newtype IdentityT f a=
IdentityT { runIdentityT ::f a }
deriving (Eq,Show)
What changed here is that we added an extra type argument.
Thenwewant Functor instancesforboth Identity andIdentityT :
instance Functor Identity where
fmap f ( Identity a)=Identity (f a)
instance (Functor m)
=&gt;Functor (IdentityT m)where
fmap f ( IdentityT fa)=
IdentityT (fmap f fa)
TheIdentityT instancehereshouldlooksimilartothe Functor
instance for the Onedatatype above — the 𝑓𝑎argument is the
value inside the IdentityT with the (untouchable) structure
wrapped around it. All we know about that additional layer of
structure wrapped around the 𝑎value is that it is a Functor .
We also want Applicative instances for each:</p>
<p>CHAPTER 25. E PLURIBUS MONAD 1525
instance Applicative Identity where
pure=Identity
(Identity f)&lt;<em>&gt;(Identity a)=
Identity (f a)
instance (Applicative m)
=&gt;Applicative (IdentityT m)where
pure x=IdentityT (pure x)
(IdentityT fab)&lt;</em>&gt;(IdentityT fa)=
IdentityT (fab&lt;<em>&gt;fa)
TheIdentity instance should be familiar. In the IdentityT
instance, the 𝑓𝑎𝑏variable represents the f (a -&gt; b) that is the
first argument of (&lt;</em>&gt;). Since this can rely on the Applicative
instance for 𝑚to handle that bit, this instance defines how
to applicatively apply in the presence of that outer IdentityT
layer.
Finally, we want some Monadinstances:</p>
<p>CHAPTER 25. E PLURIBUS MONAD 1526
instance MonadIdentity where
return=pure
(Identity a)&gt;&gt;=f=f a
instance (Monadm)
=&gt;Monad(IdentityT m)where
return=pure
(IdentityT ma)&gt;&gt;=f=
IdentityT $ma&gt;&gt;=runIdentityT .f
TheMonadinstance is tricky, so we’re going to do a few things
to break it down. Keep in mind that Monadis where we have to
really use concrete type information from IdentityT in order
to make the types fit.
The bind breakdown
We’ll start with a closer look at the instance as written above:</p>
<p>CHAPTER 25. E PLURIBUS MONAD 1527
instance (Monadm)
=&gt;Monad(IdentityT m)where
return=pure
(IdentityT ma)&gt;&gt;=f=
-- [ 1 ] [2] [3]
IdentityT $ma
-- [8] [4]
&gt;&gt;=runIdentityT .f
-- [5] [7] [6]
1.First we pattern match or unpack the m avalue of IdentityT
m avia the data constructor. Doing this has the type
IdentityT m a -&gt; m a and the type of maism a. This nomen-
clature doesn’t mean anything beyond mnemonic signal-
ing, but it is intended to be helpful.
2.The type of the bind we are implementing is the follow-
ing:
(&gt;&gt;=)::IdentityT m a
-&gt;(a-&gt;IdentityT m b)
-&gt;IdentityT m b
This is the instance we are defining.
3.The function we’re binding over is IdentityT m a . It has
the following type:</p>
<p>CHAPTER 25. E PLURIBUS MONAD 1528
(a-&gt;IdentityT m b)
4.Heremais the same one we unpacked out of the IdentityT
data constructor and has the type m a. Removed from its
IdentityT context, this is now the m athat this bind takes
as its first argument.
5.This is a diﬀerent bind! The first bind is the bind we’re
trying to implement; this bind is its definition or imple-
mentation. We’re now using the Monadwe asked for in the
instance declaration with the constraint Monad m =&gt; . This
will have the type:
(&gt;&gt;=)::m a-&gt;(a-&gt;m b)-&gt;m b
This is with respect to the 𝑚in the type IdentityT m a , not
the class of Monadinstances in general. In other words,
since we have already unpacked the IdentityT bit and, in
a sense, gotten it out of the way, this bind will be the bind
for the type 𝑚in the type IdentityT m . We don’t know
whatMonadthat is yet, and we don’t need to; since it has
theMonadtypeclass constraint on that variable, we know it
already has a Monadinstance defined for it, and this second
bind will be the bind defined for that type. All we’re doing
here is defining how to use that bind in the presence of
the additional IdentityT structure.</p>
<p>CHAPTER 25. E PLURIBUS MONAD 1529
6.This is the same 𝑓which was an argument to the Monad
instance we are defining, of type:
(a-&gt;IdentityT m b)
7.We need runIdentityT because 𝑓returns IdentityT m b , but
the&gt;&gt;=for the Monad m =&gt; has the type m a -&gt; (a -&gt; m b)
-&gt; m b. It’ll end up trying to join m (IdentityT m b) , which
won’t work because mandIdentityT m are not the same
type. We use runIdentityT to unpack the value. Doing
this has the type IdentityT m b -&gt; m b and the composition
runIdentityT . f in this context has the type a -&gt; m b . You
can use undefined in GHCi to demonstrate this for yourself:
Prelude&gt; :{
*Main| let f :: (a -&gt; IdentityT m b)
*Main| f = undefined
*Main| :}
Prelude&gt; :t f
f :: a -&gt; IdentityT m b
Prelude&gt; :t runIdentityT
runIdentityT :: IdentityT f a -&gt; f a
Prelude&gt; :t (runIdentityT . f)
(runIdentityT . f) :: a1 -&gt; f a</p>
<p>CHAPTER 25. E PLURIBUS MONAD 1530
OK, the type variables don’t have the same name, but you
can see how a1 -&gt; f a anda -&gt; m b are the same type.
8.To satisfy the type of the outer bind we are implementing
for the MonadofIdentityT m , which expects a final result
of the type IdentityT m b , we must take the m bwhich the
expression ma &gt;&gt;= runIdentityT . f returns and repack it
inIdentityT . Note:
Prelude&gt; :t IdentityT
IdentityT :: f a -&gt; IdentityT f a
Prelude&gt; :t runIdentityT
runIdentityT :: IdentityT f a -&gt; f a
Now we have a bind we can use with IdentityT and some
otherMonad— in this example, a list:
Prelude&gt; let sumR = return . (+1)
Prelude&gt; IdentityT [1, 2, 3] &gt;&gt;= sumR
IdentityT {runIdentityT = [2,3,4]}
Implementing the bind, step by step
Now we’re going to backtrack and go through implementing
that bind step by step. The goal here is to demystify what
we’ve done and enable you to write your own instances for
whatever monad transformer you might need to implement</p>
<p>CHAPTER 25. E PLURIBUS MONAD 1531
yourself. We’ll go ahead and start back at the beginning, but
withInstanceSigs turned on so we can see the type:
{-# LANGUAGE InstanceSigs #-}
instance (Monadm)
=&gt;Monad(IdentityT m)where
return=pure
(&gt;&gt;=)::IdentityT m a
-&gt;(a-&gt;IdentityT m b)
-&gt;IdentityT m b
(IdentityT ma)&gt;&gt;=f=
undefined
Let’s leave the undefined as our final return expression, then
useletbindings and contradiction to see the types of our
attempts at making a Monadinstance. We’re going to use the
bottom value ( undefined ) to defer the parts of the proof we’re
obligated to produce until we’re ready. First, let’s get a let
binding in place and see it load, even if the code doesn’t work:</p>
<p>CHAPTER 25. E PLURIBUS MONAD 1532
(&gt;&gt;=)::IdentityT m a
-&gt;(a-&gt;IdentityT m b)
-&gt;IdentityT m b
(IdentityT ma)&gt;&gt;=f=
letaimb=ma&gt;&gt;=f
inundefined
We’re using 𝑎𝑖𝑚𝑏as a mnemonic for the parts of the whole
thing that we’re trying to implement.
Here we get an error:
Couldn't match type ‘m’ with ‘IdentityT m’
That type error isn’t the most helpful thing in the world.
It’s hard to know what’s wrong from that. So, we’ll poke at this
a bit in order to get a more helpful type error.
First, we’ll do something we know should work. We’ll use
fmapinstead. Because that will typecheck (but not give us the
same result as (&gt;&gt;=)), we need to do something to give the
compiler a chance to contradict us and tell us the real type.
We force that type error by asserting a fully polymorphic type
for𝑎𝑖𝑚𝑏:</p>
<p>CHAPTER 25. E PLURIBUS MONAD 1533
(&gt;&gt;=)::IdentityT m a
-&gt;(a-&gt;IdentityT m b)
-&gt;IdentityT m b
(IdentityT ma)&gt;&gt;=f=
letaimb::a
aimb=fmap f ma
inundefined
The type we asserted for 𝑎𝑖𝑚𝑏is impossible; we’ve said it
could be every type, and it can’t. The only thing that can have
that type is bottom, as bottom inhabits all types.
Conveniently, GHC will let us know what 𝑎𝑖𝑚𝑏is:
Couldn't match expected type ‘a1’
with actual type ‘m (IdentityT m b)’
Withthecurrentimplementation, 𝑎𝑖𝑚𝑏hasthetype m (IdentityT
m b). Now we can see the real problem: there is an IdentityT
layer in between the two bits of 𝑚that we need to join in order
to have a monad.
Here’s a breakdown:
(&gt;&gt;=)::IdentityT m a
-&gt;(a-&gt;IdentityT m b)
-&gt;IdentityT m b
The pattern match on IdentityT comes from having lifted
over it:</p>
<p>CHAPTER 25. E PLURIBUS MONAD 1534
(a-&gt;IdentityT m b)
The problem is, we used &gt;&gt;=over
ma
-- and got
m(IdentityT m b)
It doesn’t typecheck because (&gt;&gt;=)merges structure of the
same type after lifting (remember: it’s fmapcomposed with
joinunder the hood). Had our type been m (m b) after binding
fovermait would’ve worked fine. As it is, we need to find a
way to get the two bits of 𝑚together without an intervening
IdentityT layer.
We’re going to continue with having separate fmapandjoin
instead of using (&gt;&gt;=)because it makes the step-wise manip-
ulation of structure easier to see. How do we get rid of the
IdentityT in the middle of the two 𝑚structures? Well, we know
𝑚is aMonad, which means it’s also a Functor . So, we can use
runIdentityT to get rid of the IdentityT structure in the middle
of the stack of types:</p>
<p>CHAPTER 25. E PLURIBUS MONAD 1535
-- Change m (IdentityT m b)
-- into m (m b)
-- Note:
runIdentityT ::IdentityT f a-&gt;f a
fmaprunIdentityT ::Functor f
=&gt;f (IdentityT f1 a)-&gt;f (f1 a)
(&gt;&gt;=)::IdentityT m a
-&gt;(a-&gt;IdentityT m b)
-&gt;IdentityT m b
(IdentityT ma)&gt;&gt;=f=
letaimb::a
aimb=fmap runIdentityT (fmap f ma)
inundefined
And when we load this code, we get an encouraging type
error:
Couldn't match expected type ‘a1’
with actual type ‘m (m b)’
It’s telling us we have achieved the type m (m b) , so now we
know how to get where we want. The 𝑎1here is the 𝑎we had
assigned to 𝑎𝑖𝑚𝑏, but it’s telling us that our actual type is not
what we asserted but this other type. Thus we have discovered</p>
<p>CHAPTER 25. E PLURIBUS MONAD 1536
what our actual type is, which gives us a clue about how to fix
it.
We’ll use joinfromControl.Monad to merge the nested 𝑚
structure:
(&gt;&gt;=)::IdentityT m a
-&gt;(a-&gt;IdentityT m b)
-&gt;IdentityT m b
(IdentityT ma)&gt;&gt;=f=
letaimb::a
aimb=
join (fmap runIdentityT (fmap f ma))
inundefined
And when we load it, the compiler tells us we finally have
anm bthat we can return:
Couldn't match expected type ‘a1’
with actual type ‘m b’
In fact, before we begin cleaning up our code, we can verify
this is the case real quick:</p>
<p>CHAPTER 25. E PLURIBUS MONAD 1537
(&gt;&gt;=)::IdentityT m a
-&gt;(a-&gt;IdentityT m b)
-&gt;IdentityT m b
(IdentityT ma)&gt;&gt;=f=
letaimb=
join (fmap runIdentityT (fmap f ma))
inaimb
We removed the type declaration for aimband also changed
thein undefined . But we know that 𝑎𝑖𝑚𝑏has the actual type m
b, so this won’t work. Why? If we take a look at the type error:
Couldn't match type ‘m’ with ‘IdentityT m’
The(&gt;&gt;=)we are implementing has a final result of type
IdentityT m b , so the type of 𝑎𝑖𝑚𝑏doesn’t match it yet. We need
to wrap m binIdentityT to make it typecheck:</p>
<p>CHAPTER 25. E PLURIBUS MONAD 1538
-- Remember:
IdentityT ::f a-&gt;IdentityT f a
instance (Monadm)
=&gt;Monad(IdentityT m)where
return=pure
(&gt;&gt;=)::IdentityT m a
-&gt;(a-&gt;IdentityT m b)
-&gt;IdentityT m b
(IdentityT ma)&gt;&gt;=f=
letaimb=
join (fmap runIdentityT
(fmap f ma))
inIdentityT aimb
This should compile. We rewrap m bback in the IdentityT
type and we should be good to go.
Refactoring
Now that we have something that works, let’s refactor. We’d
like to improve our implementation of (&gt;&gt;=). Taking things
one step at a time is usually more successful than trying to
rewrite all at once, especially once you have a baseline version
that you know should work. How should we improve this line?</p>
<p>CHAPTER 25. E PLURIBUS MONAD 1539
IdentityT $
join(fmap runIdentityT (fmap f ma))
Well, oneofthe Functor lawstellsussomethingabout fmapping
twice:
-- Functor law:
fmap(f.g)==fmap f.fmap g
Indeed! So we can change that line to the following and it
should be identical:
IdentityT $
join(fmap (runIdentityT .f) ma)
Now it seems suspicious that we’re fmapping and also us-
ingjoinon the result of having fmapped the two functions we
composed. Isn’t joincomposed with fmapjust(&gt;&gt;=)?
x&gt;&gt;=f=join (fmap f x)
Accordingly, we can change our Monadinstance to the fol-
lowing:</p>
<p>CHAPTER 25. E PLURIBUS MONAD 1540
instance (Monadm)
=&gt;Monad(IdentityT m)where
return=pure
(&gt;&gt;=)::IdentityT m a
-&gt;(a-&gt;IdentityT m b)
-&gt;IdentityT m b
(IdentityT ma)&gt;&gt;=f=
IdentityT $ma&gt;&gt;=runIdentityT .f
And that should work still! We have a type constructor now
(IdentityT ) that takes a monad as an argument and returns a
monad as a result.
This implementation can be written other ways. In the
transformers library, for example, it’s written like this:
m&gt;&gt;=k=
IdentityT $runIdentityT .k
=&lt;&lt;runIdentityT m
Take a moment and work out for yourself how that is func-
tionally equivalent to our implementation.
The essential extra of monad transformers
It may not seem like it, but the IdentityT monad transformer
captures the essence of transformers generally. We only em-</p>
<p>CHAPTER 25. E PLURIBUS MONAD 1541
barked on this quest because we couldn’t be guaranteed a Monad
instance given the composition of two types. Given that, we
know having Functor ,Applicative , andMonadat our disposal isn’t
enough to make that new Monadinstance. So what was novel in
the following code?
(&gt;&gt;=)::IdentityT m a
-&gt;(a-&gt;IdentityT m b)
-&gt;IdentityT m b
(IdentityT ma)&gt;&gt;=f=
IdentityT $ma&gt;&gt;=runIdentityT .f
It wasn’t the pattern match on IdentityT ; we get that from
theFunctor anyway:
-- Not this
(IdentityT ma)...
It wasn’t the ability to (&gt;&gt;=)functions over the mavalue of
type𝑚𝑎, we get that from the Monadconstraint on 𝑚anyway.
-- Not this
...ma&gt;&gt;= ...
We needed to know one of the types concretely so that
we could use runIdentityT (essentially fmapping a fold of the
IdentityT structure) and then repack the value in IdentityT :</p>
<p>CHAPTER 25. E PLURIBUS MONAD 1542
-- We need to know IdentityT
-- concretely to do this
IdentityT ..runIdentityT ...
As you’ll recall, until we used runIdentityT we couldn’t get
the types to fit because IdentityT was wedged in the middle of
two bits of 𝑚. It turns out to be impossible to fix that using
onlyFunctor ,Applicative , andMonad. This is an example of why
we can’t make a Monadinstance for the Compose type, but we
can make a transformer type like IdentityT where we leverage
information specific to the type and combine it with any other
type that has a Monadinstance. In general, in order to make the
types fit, we’ll need some way to fold and reconstruct the type
we have concrete information for.
25.9 Finding a pattern
Transformers are bearers of single-type concrete information
that let you create ever-bigger monads in a sense. Nesting
such as
(Monadm)=&gt;m (m a)
is addressed by joinalready. We use transformers when
we want a &gt;&gt;=operation over 𝑓and𝑔of diﬀerent types (but
both have Monadinstances). You have to create new types called</p>
<p>CHAPTER 25. E PLURIBUS MONAD 1543
monad transformers and write Monadinstances for those types
to have a way of dealing with the extra structure generated.
The general pattern is this: You want to compose two poly-
morphic types, 𝑓and𝑔, that each have a Monadinstance. But
you’ll end up with this pattern:
f(g (f b))
Monad’s bind can’t join those types, not with that intervening
𝑔. So you need to get to this:
f(f b)
You won’t be able to unless you have some way of folding
the𝑔in the middle. You can’t do that with Monad. The essence
ofMonadisjoin, but here you have only one bit of 𝑔structure,
notg (g ...) , so that’s not enough. The straightforward thing
to do is to make 𝑔concrete. With concrete type information
for the inner bit of structure, we can fold out the 𝑔and get on
with it. The good news is that transformers don’t require 𝑓be
concrete; 𝑓can remain polymorphic so long as it has a Monad
instance, so we only write a transformer once for each type.
We can see this pattern with IdentityT as well. You may
recall this step in our process of writing IdentityT ’sMonad:
(IdentityT ma)&gt;&gt;=f=
letaimb::m (IdentityT m b)
aimb=fmap f ma</p>
<p>CHAPTER 25. E PLURIBUS MONAD 1544
We have something that’ll typecheck, but it’s not quite in
the shape we would like. Of course, the underlying type once
we throw away the IdentityT data constructor is m (m b) which’ll
suit us just fine, but we have to fold out the IdentityT before we
can use the joinfromMonad m =&gt; m . That leads us to the next
step:
letaimb::m (m b)
aimb=fmap runIdentityT (fmap f ma)
Now we finally have something we can join because we lifted
the record accessor for IdentityT over the 𝑚! Since IdentityT
is so simple, the record accessor is sufficient to fold up the
structure. From there the following transitions become easy:
m(m b)-&gt;m b-&gt;IdentityT m b
The final type is what our definition of (&gt;&gt;=)forIdentityT
must result in.
The basic pattern that many monad transformers are en-
abling us to cope with is the following type transitions, where
𝑚is the polymorphic, outer structure and 𝑇is some concrete
type the transformer is for. For example, in the above, 𝑇would
beIdentityT .</p>
<p>CHAPTER 25. E PLURIBUS MONAD 1545
m (Tm b)
-&gt;m (m b)
-&gt;m b
-&gt;Tm b
Don’t consider this a hard and fast rule for what types you’ll
encounter in implementing transformers, but rather some
intuition for why transformers are necessary to begin with.</p>
<p>Chapter 26
Monad transformers
I do not say such things
except insofar as I
consider this to permit
some transformation of
things. Everything I do, I
do in order that it may be
of use.
Michel Foucault
1546</p>
<p>CHAPTER 26. STACK ‘EM UP 1547
26.1 Monad transformers
The last chapter demonstrated why we need monad transform-
ers and the basic type manipulation that’s going on to make
that bit of magick happen. Monad transformers are important
in a lot of everyday Haskell code, though, so we want to dive
deeper and make sure we have a good understanding of how
to use them in practice. Even after you know how to write all
the transformer instances, managing stacks of transformers
in an application can be tricky. The goal of this chapter is to
get comfortable with it.
In this chapter, we will
•work through more monad transformer types and in-
stances;
•look at the ordering and wrapping of monad transformer
stacks;
•lift, lift, lift, and lift some more.
26.2 MaybeT
In the last chapter, we worked through an extended break-
down of the IdentityT transformer. IdentityT is, as you might
imagine, not the most useful of the monad transformers, al-
though it is not without practical applications (more on this</p>
<p>CHAPTER 26. STACK ‘EM UP 1548
later). As we’ve seen, though, the Maybe Monad can be useful, and
so it is that the tranformer variant, MaybeT, finds its way into
the pantheon of important transformers.
TheMaybeT transformer is a bit more complex than IdentityT .
If you worked through all the exercises of the previous chapter,
then this section will not be too surprising, because this will
rely on things you’ve seen with IdentityT and the Compose type
already. However, to ensure that transformers are thoroughly
demystified for you, it’s worth working through them carefully.
We begin with the newtype for our transformer:
newtype MaybeTm a=
MaybeT{ runMaybeT ::m (Maybea) }
The structure of our MaybeT type and the Compose type are
similar so we can reuse the basic patterns of the Compose type
for theFunctor andApplicative instances:</p>
<p>CHAPTER 26. STACK ‘EM UP 1549
-- Remember the Functor for Compose?
instance (Functor f,Functor g)
=&gt;Functor (Compose f g)where
fmap f ( Compose fga)=
Compose $(fmap.fmap) f fga
-- compare to the instance for MaybeT
instance (Functor m)
=&gt;Functor (MaybeTm)where
fmap f ( MaybeTma)=
MaybeT$(fmap.fmap) f ma
We don’t need to do anything diﬀerent for the Functor in-
stance, because transformers are needed for the Monad, not the
Functor .
Spoiler alert!
If you haven’t yet written the Applicative instance for Compose
from the previous chapter, you may want to stop right here.
We’ll start with what might seem like an obvious way to
write the MaybeT Applicative and find out why it doesn’t work.
This does not compile:</p>
<p>CHAPTER 26. STACK ‘EM UP 1550
instance (Applicative m)
=&gt;Applicative (MaybeTm)where
pure x=MaybeT(pure (pure x))
(MaybeTfab)&lt;<em>&gt;(MaybeTmma)=
MaybeT$fab&lt;</em>&gt;mma
The𝑓𝑎𝑏represents the function m (Maybe (a -&gt; b)) and the
𝑚𝑚𝑎represents the m (Maybe a) .
You’ll get this error if you try it:
Couldn't match type ‘Maybe (a -&gt; b)’
with ‘Maybe a -&gt; Maybe b’
Here is the Applicative instance for Compose as a comparison
with the MaybeT instance we’re trying to write:
instance (Applicative f,Applicative g)
=&gt;Applicative (Compose f g)where
pure x=Compose (pure (pure x))
Compose f&lt;<em>&gt;Compose x=
Compose ((&lt;</em>&gt;)&lt;$&gt;f&lt;*&gt;x)
Let’s break this down a bit in case you felt confused when
you wrote this for the last chapter’s exercise. Because you did
that exercise…right?</p>
<p>CHAPTER 26. STACK ‘EM UP 1551
The idea here is that we have to lift an Applicative apply
over the outer structure 𝑓to get the g (a -&gt; b) intog a -&gt; g b
so that the Applicative instance for 𝑓can be leveraged. We can
stretch this idea a bit and use concrete types:
innerMost
::[Maybe(Identity (a-&gt;b))]
-&gt;[Maybe(Identity a-&gt;Identity b)]
innerMost =(fmap.fmap) (&lt;<em>&gt;)
second'
::[Maybe(Identity a-&gt;Identity b)]
-&gt;[Maybe(Identity a)
-&gt;Maybe(Identity b) ]
second' =fmap (&lt;</em>&gt;)
final'
::[Maybe(Identity a)
-&gt;Maybe(Identity b) ]
-&gt;[Maybe(Identity a)]
-&gt;[Maybe(Identity b)]
final'=(&lt;*&gt;)
The function that could be the Applicative instance for such
a hypothetical type would look like:</p>
<p>CHAPTER 26. STACK ‘EM UP 1552
lmiApply ::[Maybe(Identity (a-&gt;b))]
-&gt;[Maybe(Identity a)]
-&gt;[Maybe(Identity b)]
lmiApply f x=
final' (second' (innerMost f)) x
TheApplicative instance for our MaybeT type will employ this
same idea, because applicatives are closed under composition,
as we noted in the last chapter. We only need to do something
diﬀerent from the Compose instances once we get to Monad.
So, we took the long way around to this:
instance (Applicative m)
=&gt;Applicative (MaybeTm)where
pure x=MaybeT(pure (pure x))
(MaybeTfab)&lt;<em>&gt;(MaybeTmma)=
MaybeT$(&lt;</em>&gt;)&lt;$&gt;fab&lt;*&gt;mma
MaybeT Monad instance
At last, on to the Monadinstance! Note that we’ve given some of
the intermediate types:
instance (Monadm)
=&gt;Monad(MaybeTm)where
return=pure</p>
<p>CHAPTER 26. STACK ‘EM UP 1553
(&gt;&gt;=)::MaybeTm a
-&gt;(a-&gt;MaybeTm b)
-&gt;MaybeTm b
(MaybeTma)&gt;&gt;=f=
-- [2] [3]
MaybeT$ do
-- [ 1 ]
-- ma :: m (Maybe a)
-- v :: Maybe a
v&lt;-ma
-- [4]
casevof
-- [5]
Nothing -&gt;returnNothing
-- [ 6 ]
Justy-&gt;runMaybeT (f y)
-- [7] [8]
-- y :: a
-- f :: a -&gt; MaybeT m b
-- f y :: MaybeT m b
-- runMaybeT (f y) :: m (Maybe b)
Explaining it step by step:</p>
<p>CHAPTER 26. STACK ‘EM UP 1554
1.We have to return a MaybeT value at the end, so the doblock
has the MaybeT data constructor in front of it. This means
the final value of our doblock expression must be of type
m (Maybe b) in order to typecheck because our goal is to
go from MaybeT m a toMaybeT m b .
2.The first argument to bind here is MaybeT m a . We unbun-
dled that from MaybeT by pattern matching on the MaybeT
newtype data constructor.
3.The second argument to bind is (a -&gt; MaybeT m b) .
4.In the definition of MaybeT , notice something:
newtype MaybeTm a=
MaybeT{ runMaybeT ::m (Maybea) }
-- ^---------^
It’s aMaybevalue wrapped in some other type for which
all we know is that it has a Monadinstance. Accordingly, we
begin in our doblock by using the left arrow bind syntax.
This gives us a reference to the hypothetical Maybevalue
out of the 𝑚structure which is unknown.
5.Since using &lt;-to bind Maybe a out ofm (Maybe a) left us
with aMaybevalue, we use a case expression on the Maybe
value.</p>
<p>CHAPTER 26. STACK ‘EM UP 1555
6.If we get Nothing , we kick Nothing back out, but we have to
embed it in the 𝑚structure. We don’t know what 𝑚is, but
being a Monad(and thus also an Applicative ) means we can
usereturn (pure) to perform that embedding.
7.If we get Just, we now have a value of type 𝑎that we can
pass to our function fof type a -&gt; MaybeT m b .
8.We have to fold the m (Maybe b) value out of the MaybeT
since the MaybeT constructor is already wrapped around
the whole doblock, then we’re done.
Don’t be afraid to get a pen and paper and work all that out
until you understand how things are happening before you
move on.
26.3 EitherT
Just asMaybehas a transformer variant in the form of MaybeT , we
can make a transformer variant of Either . We’ll call it EitherT .
Your task is to implement the instances for the transformer
variant:
newtype EitherT e m a=
EitherT { runEitherT ::m (Eithere a) }</p>
<p>CHAPTER 26. STACK ‘EM UP 1556
Exercises: EitherT
1.Write the Functor instance for EitherT :
instance Functor m
=&gt;Functor (EitherT e m)where
fmap=undefined
2.Write the Applicative instance for EitherT :
instance Applicative m
=&gt;Applicative (EitherT e m)where
pure=undefined
f&lt;*&gt;a=undefined
3.Write the Monadinstance for EitherT :
instance Monadm
=&gt;Monad(EitherT e m)where
return=pure
v&gt;&gt;=f=undefined
4.Write the swapEitherT helper function for EitherT .</p>
<p>CHAPTER 26. STACK ‘EM UP 1557
-- transformer version of swapEither.
swapEitherT ::(Functor m)
=&gt;EitherT e m a
-&gt;EitherT a m e
swapEitherT =undefined
Hint: write swapEither first, then swapEitherT in terms of
the former.
5.Write the transformer variant of the either catamorphism.
eitherT ::Monadm=&gt;
(a-&gt;m c)
-&gt;(b-&gt;m c)
-&gt;EitherT a m b
-&gt;m c
eitherT =undefined
26.4 ReaderT
ReaderT is one of the most commonly used transformers in
conventional Haskell applications. It is like Reader, except in
the transformer variant we’re generating additional structure
in the return type of the function:
newtype ReaderT r m a=
ReaderT { runReaderT ::r-&gt;m a }</p>
<p>CHAPTER 26. STACK ‘EM UP 1558
The value inside the ReaderT is a function. Type constructors
such as Maybeare also functions in some senses, but we have
to handle this case a bit diﬀerently. The first argument to the
function inside ReaderT is part of the structure we’ll have to
bind over.
This time we’re going to give you the instances. If you want
to try writing them yourself, do not read on!</p>
<p>CHAPTER 26. STACK ‘EM UP 1559
instance (Functor m)
=&gt;Functor (ReaderT r m)where
fmap f ( ReaderT rma)=
ReaderT $(fmap.fmap) f rma
instance (Applicative m)
=&gt;Applicative (ReaderT r m)where
pure a=ReaderT (pure (pure a))
(ReaderT fmab)&lt;<em>&gt;(ReaderT rma)=
ReaderT $(&lt;</em>&gt;)&lt;$&gt;fmab&lt;*&gt;rma
instance (Monadm)
=&gt;Monad(ReaderT r m)where
return=pure
(&gt;&gt;=)::ReaderT r m a
-&gt;(a-&gt;ReaderT r m b)
-&gt;ReaderT r m b
(ReaderT rma)&gt;&gt;=f=
ReaderT $\r-&gt; do
-- [1]
a&lt;-rma r
-- [3] [ 2 ]
runReaderT (f a) r
-- [5] [ 4 ] [6]</p>
<p>CHAPTER 26. STACK ‘EM UP 1560
1.Again, the type of the value in a ReaderT must be a function,
so the act of binding a function over a ReaderT must itself
be a function awaiting the argument of type 𝑟, which we’ve
chosen to name 𝑟as a convenience in our terms. Also note
that we’re repacking our lambda inside the ReaderT data
constructor.
2.Wepattern-matchedthe r -&gt; m a (representedinourterms
by𝑟𝑚𝑎) out of the ReaderT data constructor. Now we’re ap-
plying it to the 𝑟that we’re expecting in the body of the
anonymous lambda.
3.The result of applying r -&gt; m a to a value of type 𝑟ism
a. We need a value of type 𝑎in order to apply our a -&gt;
ReaderT r m b function. To be able to write code in terms
of that hypothetical 𝑎, we bind ( &lt;-) the𝑎out of the 𝑚
structure. We’ve bound that value to the name 𝑎as a
mnemonic to remember the type.
4.Applying 𝑓, which has type a -&gt; ReaderT r m b , to the value
𝑎results in a value of type ReaderT r m b .
5.We unpack the r -&gt; m b out of the ReaderT structure.
6.Finally, we apply the resulting r -&gt; m b to the𝑟we had at
the beginning of our lambda, that eventual argument that
Reader abstracts for us. We have to return m bas the final
expression in this anonymous lambda or the function is</p>
<p>CHAPTER 26. STACK ‘EM UP 1561
not valid. To be valid, it must be of type r -&gt; m b which
expresses the constraint that if it is applied to an argument
of type 𝑟, it must produce a value of type m b.
No exercises this time. You deserve a break.
26.5 StateT
Similar to Reader andReaderT ,StateT isStatebut with additional
monadic structure wrapped around the result. StateT is some-
what more useful and common than the State Monad you saw
earlier. Like ReaderT , its value is a function:
newtype StateTs m a=
StateT{ runStateT ::s-&gt;m (a,s) }
Exercises: StateT
If you’re familiar with the distinction, you’ll be implementing
the strict variant of StateT here. To make the strict variant,
you don’t have to do anything special. Write the most obvious
thing that could work. The lazy (lazier, anyway) variant is
the one that involves adding a bit extra. We’ll explain the
diﬀerence in the chapter on nonstrictness.
1.You’ll have to do the Functor andApplicative instances
first, because there aren’t Functor andApplicative instances
ready to go for the type Monad m =&gt; s -&gt; m (a, s) .</p>
<p>CHAPTER 26. STACK ‘EM UP 1562
instance (Functor m)
=&gt;Functor (StateTs m)where
fmap f m =undefined
2.As with Functor , you can’t cheat and reuse an underlying
Applicative instance, so you’ll have to do the work with
thes -&gt; m (a, s) type yourself.
instance (Monadm)
=&gt;Applicative (StateTs m)where
pure=undefined
(&lt;*&gt;)=undefined
Also note that the constraint on 𝑚is notApplicative as you
expect, but rather Monad. This is because you can’t express
the order-dependent computation you’d expect the StateT
Applicative to have without having a Monad for 𝑚. To
learn more, see this Stack Overflow question1about this
issue. Also see this Github issue2on the NICTA Course
Github repository. Beware ! The NICTA course issue
gives away the answer. In essence, the issue is that without
Monad, you’re feeding the initial state to each computation
inStateT rather than threading it through as you go. This
1Is it possible to implement ‘(Applicative m) =&gt; Applica-
tive (StateT s m)‘? http://stackoverflow.com/questions/18673525/
is-it-possible-to-implement-applicative-m-applicative-statet-s-m
2https://github.com/NICTA/course/issues/134</p>
<p>CHAPTER 26. STACK ‘EM UP 1563
is a general pattern contrasting Applicative andMonadand
is worth contemplating.
3.TheMonadinstance should look fairly similar to the Monad
instance you wrote for ReaderT .
instance (Monadm)
=&gt;Monad(StateTs m)where
return=pure
sma&gt;&gt;=f=undefined
ReaderT, WriterT, StateT
We’d like to point something out about these three types:
newtype Readerr a=
Reader{ runReader ::r-&gt;a }
newtype Writerw a=
Writer{ runWriter ::(a, w) }
newtype States a=
State{ runState ::s-&gt;(a, s) }
and their transformer variants:</p>
<p>CHAPTER 26. STACK ‘EM UP 1564
newtype ReaderT r m a=
ReaderT { runReaderT ::r-&gt;m a }
newtype WriterT w m a=
WriterT { runWriterT ::m (a, w) }
newtype StateTs m a=
StateT{ runStateT ::s-&gt;m (a, s) }
You’re already familiar with Reader andState. We haven’t
shown you Writer orWriterT up to this point because, quite
frankly, you shouldn’t use it. We’ll explain why not in a section
later in this chapter.
For the purposes of the progression we’re trying to demon-
strate here, it suffices to know that the Writer Applicative and
Monadwork by combining the 𝑤values monoidally. With that
in mind, what we can see is that Reader lets us talk about val-
ues we need, Writer lets us deal with values we can emit and
combine (but not read), and Statelets us both read and write
values in any manner we desire — including monoidally, like
Writer . This is one reason you needn’t bother with Writer since
Statecan replace it anyway. That’s why you don’t need Writer ;
we’ll talk more about why you don’t want Writer later.
In fact, there is a type in the transformers library that com-
binesReader ,Writer , andStateinto one big type:</p>
<p>CHAPTER 26. STACK ‘EM UP 1565
newtype RWSTr w s m a =
RWST{ runRWST ::r-&gt;s-&gt;m (a, s, w) }
Because of the Writer component, you probably wouldn’t
want to use that in most applications either, but it’s good to
know it exists.
Correspondence between StateT and Parser
You may recall what a simple parser type looks like:
typeParsera=String-&gt;Maybe(a,String)
You may remember our discussion about the similarities
between parsers and Statein the Parsers chapter. Now, we
could choose to define a Parser type in the following manner:
newtype StateTs m a=
StateT{ runStateT ::s-&gt;m (a,s) }
typeParser=StateTStringMaybe
Nobody does this in practice, but it’s useful to consider the
similarity to get a feel for what StateT is all about.</p>
<p>CHAPTER 26. STACK ‘EM UP 1566
26.6 Types you probably don’t want to
use
Not every type will necessarily be performant or make sense.
ListTandWriter /WriterT are examples of this.
Why not use Writer or WriterT?
It’s a bit too easy to get into a situation where Writer is either
too lazy or too strict for the problem you’re solving, and then
it’ll use more memory than you’d like. Writer can accumulate
unevaluated thunks, causing memory leaks. It’s also inappro-
priate for logging long-running or ongoing programs due to
the fact that you can’t retrieve any of the logged values until
the computation is complete.3
Usually when Writer is used in an application, it’s not called
Writer . Instead a one-oﬀ is created for a specific type 𝑤. Given
that, it’s still useful to know when you’re looking at something
that’s a Reader,Writer, orState, even if the author didn’t use
the types by those names from the transformers library. Some-
times this is because they wanted a stricter Writer than the one
already available.
Determining and measuring when more strictness (more
eagerly evaluating your thunks) is needed in your programs is
3If you’d like to understand this better, Gabriel Gonzalez has a helpful blog post on
the subject. http://www.haskellforall.com/2014/02/streaming-logging.html</p>
<p>CHAPTER 26. STACK ‘EM UP 1567
the topic of the upcoming chapter on nonstrictness.
The ListT you want isn’t made from the List type
The most obvious way to implement ListTis generally not
recommended for a variety of reasons, including:
1.Most people’s first attempt won’t pass the associativity law.
We’re not going to show you a way to write it that does
pass that law because it’s not worth it for the reasons listed
below.
2.It’s not very fast.
3.Streaming libraries like pipes4andconduit5do it better for
most use cases.
Prior art for “ ListTdone right” also includes AmbT6by Conal</p>
<div style="break-before: page; page-break-before: always;"></div><p>Elliott, although you may find it challenging to understand if
you aren’t familiar with ContTand the motivation behind Amb.
Lists in Haskell are as much a control structure as a data
structure, so streaming libraries such as pipesgenerally suffice
if you need a transformer. This is less of a sticking point in
writing applications than you’d think.
4http://hackage.haskell.org/package/pipes
5http://hackage.haskell.org/package/conduit
6https://wiki.haskell.org/Amb</p>
<p>CHAPTER 26. STACK ‘EM UP 1568
26.7 Recovering an ordinary type from
a transformer
If you have a transformer variant of a type and want to use
it as if it was the non-transformer version, you need some 𝑚
structure that doesn’t do anything. Have we seen anything like
that? What about Identity ?
Prelude&gt; runMaybeT $ (+1) &lt;$&gt; MaybeT (Identity (Just 1))
Identity {runIdentity = Just 2}
Prelude&gt; runMaybeT $ (+1) &lt;$&gt; MaybeT (Identity Nothing)
Identity {runIdentity = Nothing}
Given that, we can get Identity fromIdentityT and so on in
the following manner:
typeMyIdentity a=IdentityT Identity a
typeMaybe a=MaybeTIdentity a
typeEithere a=EitherT eIdentity a
typeReaderr a=ReaderT eIdentity a
typeStates a=StateTsIdentity a
This works fine for recovering the non-transformer variant
of each type as the Identity type is acting as a bit of do-nothing
structural paste for filling in the gap.</p>
<p>CHAPTER 26. STACK ‘EM UP 1569
Yeah, but why? You don’t ordinarily need to do this if you’re
working with a transformer that has a corresponding non-
transformer type you can use. For example, it’s less common
to need ( ExceptT Identity ) because the Either type is already
there, so you don’t need to retrieve that type from the trans-
former. However, if you’re writing something with, say, scotty ,
where a ReaderT is part of the environment, you can’t easily
retrieve the Reader type out of that because Reader is not a type
that exists on its own and you can’t modify that ReaderT with-
out essentially rewriting all of scotty , and, wow, nobody wants
that for you. You might then have a situation where what
you’re doing only needs a Reader, not aReaderT , so you could
use (ReaderTIdentity ) to be compatible with scotty without hav-
ing to rewrite everything but still being able to keep your own
code a bit tighter and simpler.
Thetransformers library In general, don’t use hand-rolled
versions of these transformer types without good reason. You
can find many of them in base or the transformers library, and
that library should have come with your GHC installation.
A note on ExceptT Although a library called either exists on
Hackage and provides the EitherT type, most Haskellers are
moving to the identical ExceptT type in the transformers library.
Again, this has mostly to do with the fact that transformers</p>
<p>CHAPTER 26. STACK ‘EM UP 1570
comes packaged with GHC already, so ExceptT is ready-to-
hand; the underlying type is the same.
26.8 Lexically inner is structurally
outer
One of the trickier parts of monad transformers is that the
lexical representation of the types will violate your intuitions
with respect to the relationship it has with the structure of
your values. Let us note something in the definitions of the
following types:
-- definition in transformers may look
-- slightly different. It's not important.
newtype ExceptT e m a=
ExceptT { runExceptT ::m (Eithere a) }
newtype MaybeTm a=
MaybeT{ runMaybeT ::m (Maybea) }
newtype ReaderT r m a=
ReaderT { runReaderT ::r-&gt;m a }
A necessary byproduct of how transformers work is that
the additional structure 𝑚is always wrapped around our value.
One thing to note is that it’s only wrapped around things</p>
<p>CHAPTER 26. STACK ‘EM UP 1571
we can have, not things we need, such as with ReaderT . The
consequence of this is that a series of monad transformers in a
type will begin with the innermost type structurally speaking.
Consider the following:
moduleOuterInner where
importControl.Monad.Trans.Except
importControl.Monad.Trans.Maybe
importControl.Monad.Trans.Reader
-- We only need to use return once
-- because it's one big Monad
embedded ::MaybeT
(ExceptT String
(ReaderT ()IO))
Int
embedded =return1
We can sort of peel away the layers one by one:</p>
<p>CHAPTER 26. STACK ‘EM UP 1572
maybeUnwrap ::ExceptT String
(ReaderT ()IO) (MaybeInt)
maybeUnwrap =runMaybeT embedded
-- Next
eitherUnwrap ::ReaderT ()IO
(EitherString(MaybeInt))
eitherUnwrap =runExceptT maybeUnwrap
-- Lastly
readerUnwrap ::()
-&gt;IO(EitherString
(MaybeInt))
readerUnwrap =runReaderT eitherUnwrap
Then if we’d like to evaluate this code, we feed the unit
value to the function:
Prelude&gt; readerUnwrap ()
Right (Just 1)
Why is this the result? Consider that we used return for a
Monadcomprising Reader ,Either , andMaybe:</p>
<p>CHAPTER 26. STACK ‘EM UP 1573
instance Monad((-&gt;) r)where
return=const
instance Monad(Eithere)where
return=Right
instance MonadMaybewhere
return=Just
We can treat having used return for theReader /Either /Maybe
stack as composition, consider how we get the same result as
readerUnwrap () here:
Prelude&gt; (const . Right . Just $ 1) ()
Right (Just 1)
A terminological point to keep in mind when reading about
monad transformers is that when Haskellers say base monad
they usually mean what is structurally outermost.
typeMyTypea=IO[Maybea]
InMyType , the base monad is IO.
Exercise: Wrap It Up
TurnreaderUnwrap fromthepreviousexamplebackinto embedded
through the use of the data constructors for each transformer.</p>
<p>CHAPTER 26. STACK ‘EM UP 1574
-- Modify it to make it work.
embedded ::MaybeT
(ExceptT String
(ReaderT ()IO))
Int
embedded = ???(const ( Right(Just1)))
26.9 MonadTrans
We often want to lift functions into a larger context. We’ve
been doing this for a while with Functor , which lifts a function
into a context and applies it to the value inside. The facility
to do this also undergirds Applicative ,Monad, andTraversable .
However, fmapisn’t always enough, so we have some functions
that are essentially fmapfor diﬀerent contexts:
fmap :: Functor f
=&gt; (a -&gt; b) -&gt; f a -&gt; f b
liftA :: Applicative f
=&gt; (a -&gt; b) -&gt; f a -&gt; f b
liftM :: Monad m
=&gt; (a -&gt; r) -&gt; m a -&gt; m r</p>
<p>CHAPTER 26. STACK ‘EM UP 1575
You might notice the latter two examples have liftin the
function name. While we’ve encouraged you not to get too
excited about the meaning of function names, in this case they
do give you a clue of what they’re doing. They are lifting,
just asfmapdoes, a function into some larger context. The
underlying structure of the bind function from Monadis also a
lifting function — fmapagain! — composed with the crucial
joinfunction.
In some cases, we want to talk about more or diﬀerent
structure than these types permit. In other cases, we want
something that does as much lifting as is necessary to reach
some (structurally) outermost position in a stack of monad
transformers. Monad transformers can be nested in order
to compose various eﬀects into one monster function, but to
manage those stacks, we need to lift more.
The typeclass that lifts
MonadTrans is a typeclass with one core method: lift. Speaking
generally, it is about lifting actions in some Monadover a trans-
former type which wraps itself in the original Monad. Fancy!</p>
<p>CHAPTER 26. STACK ‘EM UP 1576
classMonadTrans twhere
-- | Lift a computation from
-- the argument monad to
-- the constructed monad.
lift::(Monadm)=&gt;m a-&gt;t m a
Here the 𝑡is a (constructed) monad transformer type that
has an instance of MonadTrans defined.
We’re going to work through a relatively uncomplicated
example from scotty now.
Motivating MonadTrans
You may remember from previous chapters that scotty is a
web framework for Haskell. One thing to know about scotty,
without getting into all the gritty details of how it works, is that
the monad transformers the framework relies on are them-
selves newtypes for monad transformer stacks. Wait, what?
Well, look:</p>
<p>CHAPTER 26. STACK ‘EM UP 1577
newtype ScottyT e m a=
ScottyT
{ runS::State(ScottyState e m) a }
deriving (Functor,Applicative ,Monad)
newtype ActionT e m a=
ActionT
{ runAM
::ExceptT
(ActionError e)
(ReaderT ActionEnv
(StateTScottyResponse m))
a
}
deriving (Functor,Applicative )
typeScottyM =ScottyT TextIO
typeActionM =ActionT TextIO
We’ll use ActionM andActionT andScottyM andScottyT as if
they were the same thing, but the Mvariants are type synonyms
for the transformers with the inner types already set. This
roughly translates to the errors (the left side of the ExceptT ) in
ScottyM orActionM being returned as Text, while the right side
of theExceptT , whatever it does, is IO.ExceptT is the transformer</p>
<p>CHAPTER 26. STACK ‘EM UP 1578
version of Either, and a ReaderT and aStateT are stacked up
inside that as well. These internal mechanics don’t matter that
much to you, as a user of the scotty API, but it’s useful to see
how much is packed up in there.
Now, back to our example. This is the “hello, world” exam-
ple using scotty , but the following will cause a type error:
-- scotty.hs
{-# LANGUAGE OverloadedStrings #-}
moduleScottywhere
importWeb.Scotty
importData.Monoid (mconcat)
main=scotty3000$ do
get&quot;/:word&quot; $ do
beam&lt;-param&quot;word&quot;
putStrLn &quot;hello&quot;
html$
mconcat [ &quot;<h1>Scotty, &quot; ,
beam,
&quot; me up!</h1>&quot; ]</p>
<p>CHAPTER 26. STACK ‘EM UP 1579
Reminder: in your terminal, you can follow along with this
like so:
$ stack build scotty
$ stack ghci
Prelude&gt; :l scotty.hs
When you try to load it, you should get a type error:
Couldn't match expected type
‘Web.Scotty.Internal.Types.ActionT
Data.Text.Internal.Lazy.Text IO a0’
with actual type ‘IO ()’
In a stmt of a 'do' block: putStrLn &quot;hello&quot;
In the second argument of ‘($)’, namely
‘do { beam &lt;- param &quot;word&quot;;
putStrLn &quot;hello&quot;;
html $ mconcat [&quot;<h1>Scotty, &quot;, beam, ....] }’
The reason for this type error is that putStrLn has the type
IO (), but it is inside a doblock inside our get, and the monad
that code is in is therefore ActionM /ActionT :
get::RoutePattern
-&gt;ActionM ()
-&gt;ScottyM ()</p>
<p>CHAPTER 26. STACK ‘EM UP 1580
OurActionT type eventually reaches IO, but there’s addi-
tional structure we need to lift over first. To fix this, we’ll start
by adding an import:
importControl.Monad.Trans.Class
And amend that line with putStrLn to the following:
lift(putStrLn &quot;hello&quot;)
It should work.
You can assert a type for the liftembedded in the scotty
action:
lethello=putStrLn &quot;hello&quot;
(lift::IOa-&gt;ActionM a) hello
Let’s see what it does. Load the file again and call the main
function. You should see this message:
Setting phasers to stun... (port 3000) (ctrl-c to quit)
In the address bar of your web browser, type localhost:3000 .
You should notice two things: one is that there is nothing in
thebeamslot of the text that prints to your screen, and the other
is that it prints ‘hello’ to your terminal where the program is
running. Try adding a word to the end of the address:
localhost:3000/beam</p>
<p>CHAPTER 26. STACK ‘EM UP 1581
The text on your screen should change, and ‘hello’ should
print in your terminal again. That /:word parameter is what has
been bound via the variable beaminto that html line at the end
of thedoblock, while the ‘hello’ has been lifted over the ActionM
so that it can print in your terminal. It will print another ‘hello’
to your terminal every time something happens on the web
page.
We can concretize our use of liftin the following steps.
Please follow along by asserting the types for the application
ofliftin thescotty application above:
lift::(Monadm,MonadTrans t)
=&gt;m a-&gt;t m a
lift::(MonadTrans t)
=&gt;IOa-&gt;tIOa
lift::IOa-&gt;ActionM a
lift::IO()-&gt;ActionM ()
We go from (t IO a) to(ActionM a) because the IOis inside
theActionM .
Let’s examine ActionM more carefully:
Prelude&gt; import Web.Scotty
Prelude&gt; import Web.Scotty.Trans
Prelude&gt; :info ActionM
type ActionM = ActionT Data.Text.Internal.Lazy.Text IO
-- Defined in ‘Web.Scotty’</p>
<p>CHAPTER 26. STACK ‘EM UP 1582
We can see for ourselves what this liftdid by looking at
theMonadTrans instance for ActionT , which is what ActionM is a
type alias of:
instance MonadTrans (ActionT e)where
lift=ActionT .lift.lift.lift
Part of the niceness here is that ActionT is itself defined in
terms of three more monad transformers. We can see this in
the definition of ActionT :
newtype ActionT e m a=
ActionT {
runAM
::ExceptT
(ActionError e)
(ReaderT ActionEnv
(StateTScottyResponse m))
a
}deriving (Functor,Applicative )
Let’s first replace the liftforActionT with its definition and
see if it still works:</p>
<p>CHAPTER 26. STACK ‘EM UP 1583
{-# LANGUAGE OverloadedStrings #-}
moduleScottywhere
importWeb.Scotty
importWeb.Scotty.Internal.Types
(ActionT(..))
importControl.Monad.Trans.Class
importData.Monoid (mconcat)
All the (..)means is that we want to import all the data
constructors of the ActionT type, rather than none or a partic-
ular list of them. You can look into the syntax in more detail
independently if you like. Now for the scotty application itself:
main=scotty3000$ do
get&quot;/:word&quot; $ do
beam&lt;-param&quot;word&quot;
(ActionT .lift.lift.lift)
(putStrLn &quot;hello&quot;)
html$
mconcat [ &quot;<h1>Scotty, &quot; ,
beam,
&quot; me up!</h1>&quot; ]</p>
<p>CHAPTER 26. STACK ‘EM UP 1584
This should still work! Note that we had to ask for the data
constructor for ActionT from an Internal module because the
implementation is hidden by default. We’ve got three lifts,
one each for ExceptT ,ReaderT , andStateT .
Next we’ll do ExceptT :
instance MonadTrans (ExceptT e)where
lift=ExceptT .liftMRight
To use that in our code, add the following import:
importControl.Monad.Trans.Except
And our app changes into the following:
main=scotty3000$ do
get&quot;/:word&quot; $ do
beam&lt;-param&quot;word&quot;
(ActionT
.(ExceptT .liftMRight)
.lift
.lift) (putStrLn &quot;hello&quot;)
html$
mconcat [ &quot;<h1>Scotty, &quot; ,
beam,
&quot; me up!</h1>&quot; ]</p>
<p>CHAPTER 26. STACK ‘EM UP 1585
Thenfor ReaderT ,wetakeaganderat Control.Monad.Trans.Reader
in thetransformers library and see the following:
instance MonadTrans (ReaderT r)where
lift=liftReaderT
liftReaderT ::m a-&gt;ReaderT r m a
liftReaderT m=ReaderT (const m)
For reasons, liftReaderT isn’t exported by transformers , but
we can redefine it ourselves. Add the following to the module:
importControl.Monad.Trans.Reader
liftReaderT ::m a-&gt;ReaderT r m a
liftReaderT m=ReaderT (const m)
Then our app can be defined as follows:</p>
<p>CHAPTER 26. STACK ‘EM UP 1586
main=scotty3000$ do
get&quot;/:word&quot; $ do
beam&lt;-param&quot;word&quot;
(ActionT
.(ExceptT .fmapRight)
.liftReaderT
.lift
) (putStrLn &quot;hello&quot;)
html$
mconcat [ &quot;<h1>Scotty, &quot; ,
beam,
&quot; me up!</h1>&quot; ]
Or instead of liftReaderT , we could’ve done:
.(\m-&gt;ReaderT (const m))
Or:
(ActionT
.(ExceptT .fmapRight)
.ReaderT .const
.lift
) (putStrLn &quot;hello&quot;)</p>
<p>CHAPTER 26. STACK ‘EM UP 1587
Now for that last liftoverStateT ! Remembering that it was
the lazy StateT that the type of ActionT mentioned, we see the
following MonadTrans instance:
instance MonadTrans (StateTs)where
lift m=StateT$\s-&gt; do
a&lt;-m
return (a, s)
First, let’s get our import in place:
importControl.Monad.Trans.State.Lazy
hiding(get)
We needed to hide getbecause scotty already has a diﬀerent
getfunction defined and we don’t need the one from StateT.
Then inlining that into our app code:</p>
<p>CHAPTER 26. STACK ‘EM UP 1588
main=scotty3000$ do
get&quot;/:word&quot; $ do
beam&lt;-param&quot;word&quot;
(ActionT
.(ExceptT .fmapRight)
.ReaderT .const
.\m-&gt;StateT(\s-&gt; do
a&lt;-m
return (a, s))
) (putStrLn &quot;hello&quot;)
html$
mconcat [ &quot;<h1>Scotty, &quot; ,
beam,
&quot; me up!</h1>&quot; ]
Note that we needed an outer lambda before the StateT
in order to get the monadic action we were lifting. At this
point, we’re in the outermost position we can be, and since
ActionM defines ActionT ’s outermost monadic type as being IO,
that means our putStrLn works fine after all this lifting.
Typically a MonadTrans instance lifts over only one layer at
a time, but scotty abstracts away the underlying structure so
that you don’t have to care. That’s why it goes ahead and does
the next three lifts for you. The critical thing to realize here is
that lifting means you’re embedding an expression in a larger</p>
<p>CHAPTER 26. STACK ‘EM UP 1589
context by adding structure that doesn’t do anything.
MonadTrans instances
Now you see why we have MonadTrans and have a picture of
whatlift, the only method of MonadTrans , does.
Here are some examples of MonadTrans instances:
1.IdentityT
instance MonadTrans IdentityT where
lift=IdentityT
2.MaybeT
instance MonadTrans MaybeTwhere
lift=MaybeT.liftMJust</p>
<p>CHAPTER 26. STACK ‘EM UP 1590
lift
::(Monadm)
=&gt;m a-&gt;t m a
(MaybeT.liftMJust)
::Monadm
=&gt;m a-&gt;MaybeTm a
MaybeT
::m (Maybea)-&gt;MaybeTm a
(liftMJust)
::Monadm
=&gt;m a-&gt;m (Maybea)
Roughly speaking, this has taken an m aand lifted it into
aMaybeT context.
Thegeneralpatternwith MonadTrans instancesdemonstrated
byMaybeT is that you’re usually going to lift the injection
of the known structure (with MaybeT , the known structure
isMaybe) over some Monad. Injection of structure usually
meansreturn , but since with MaybeT we know we want Maybe
structure, we use Just. That transforms an m aintom (T a)
where capital Tis some concrete type you’re lifting the m
ainto. Then to cap it all oﬀ, you use the data constructor
for your monad transformer, and the value is now lifted
into the larger context. Here’s a summary of the stages</p>
<p>CHAPTER 26. STACK ‘EM UP 1591
the type of the value goes through:
v::Monadm=&gt;m a
liftMJust::Monadm=&gt;m a-&gt;m (Maybea)
liftMJustv::m (Maybea)
MaybeT(liftMJustv)::MaybeTm a
See if you can work out the types of this one:
3.ReaderT
instance MonadTrans (ReaderT r)where
lift=ReaderT .const
And now, write some instances!
Exercises: Lift More
Keep in mind what these are doing, follow the types, lift till
you drop.
1.You thought you were done with EitherT .
instance MonadTrans (EitherT e)where
lift=undefined
2.OrStateT . This one’ll be more obnoxious. It’s fine if you’ve
seen this before.</p>
<p>CHAPTER 26. STACK ‘EM UP 1592
instance MonadTrans (StateTs)where
lift=undefined
Prolific lifting is the failure mode
Apologies to the original authors, but sometimes with the use
of concretely and explicitly typed monad transformers you’ll
see stuﬀ like this:
addSubWidget ::(YesodSubRoute sub master)
=&gt;sub
-&gt;WidgetT sub master a
-&gt;WidgetT sub' master a
addSubWidget sub w=
domaster&lt;-liftHandler getYesod
letsr=fromSubRoute sub master
i&lt;-WidgetT $lift$lift$lift
$lift$lift$lift
$lift get</p>
<p>CHAPTER 26. STACK ‘EM UP 1593
w'&lt;-liftHandler
$toMasterHandlerMaybe sr
(const sub) Nothing
$flip runStateT i $runWriterT
$runWriterT $runWriterT
$runWriterT $runWriterT
$runWriterT $runWriterT
$unWidgetT w
let((((((((a,
body),
title),
scripts),
stylesheets),
style),
jscript),
h),
i')=w'</p>
<p>CHAPTER 26. STACK ‘EM UP 1594
WidgetT $ do
tell body
lift$tell title
lift$lift$tell scripts
lift$lift$lift
$tell stylesheets
lift$lift$lift$lift
$tell style
lift$lift$lift$lift$lift
$tell jscript
lift$lift$lift$lift$lift
$lift$tell h
lift$lift$lift$lift
$lift$lift$lift$put i'
return a
Do not write code like this. Especially, do not write code
like this and then proceed to blog about how terrible monad
transformers are.
Wrap it, smack it, pre-lift it
OK, so how do we avoid that horror show? Well, there are a lot
of ways, but one of the most robust and common is newtyp-
ing your Monadstack and abstracting away the representation.</p>
<p>CHAPTER 26. STACK ‘EM UP 1595
From there, you provide the functionality leveraging the rep-
resentation as part of your API. A good example of this comes
to us from… scotty .
Let’s take a gander at the ActionM type we mentioned earlier:
Prelude&gt; import Web.Scotty
-- again, to make the type read more nicely
-- we import some other modules.
Prelude&gt; import Data.Text.Lazy
Prelude&gt; :info ActionM
type ActionM = Web.Scotty.Internal.Types.ActionT Text IO
-- Defined in ‘Web.Scotty’
scotty hides the underlying type by default because you
ordinarily wouldn’t care or think about it in the course of writ-
ing your application. What scotty does here is good practice.
This design keeps the underlying implementation hidden by
default but lets us import an Internal module to get at the
representation in case we need to:
Prelude&gt; import Web.Scotty.Internal.Types
-- more modules to clean up the types
Prelude&gt; import Control.Monad.Trans.Reader
Prelude&gt; import Control.Monad.Trans.State.Lazy
Prelude&gt; import Control.Monad.Trans.Except
Prelude&gt; :info ActionT</p>
<p>CHAPTER 26. STACK ‘EM UP 1596
type role ActionT nominal representational nominal
newtype ActionT e (m :: * -&gt; *) a
= ActionT
{runAM :: ExceptT
(ActionError e)
(ReaderT ActionEnv
(StateT ScottyResponse m))
a}
instance (Monad m, ScottyError e) =&gt; Monad (ActionT e m)
instance Functor m =&gt; Functor (ActionT e m)
instance Monad m =&gt; Applicative (ActionT e m)
What’s nice about this approach is that it subjects the con-
sumers (which could include yourself) of your type to less
noise within an application. It also doesn’t require reading
papers written by people trying very hard to impress a thesis
advisor, although poking through prior art for ideas is rec-
ommended. It can reduce or eliminate manual lifting within
theMonadas well. Note that we only had to use liftonce to
perform an I/O action in ActionM even though the underly-
ing implementation has more than one transformer flying
around.</p>
<p>CHAPTER 26. STACK ‘EM UP 1597
26.10 MonadIO aka zoom-zoom
There’s more than one way to skin a cat and there’s more than
one way to lift an action over additional structure. MonadIO is
a diﬀerent design than MonadTrans because rather than lifting
through one layer at a time, MonadIO is intended to keep lifting
yourIOaction until it is lifted over all structure embedded in
the outermost IOtype. The idea here is that you’d write liftIO
once and it would instantiate to all of the following types:
liftIO::IOa-&gt;ExceptT eIOa
liftIO::IOa-&gt;ReaderT rIOa
liftIO::IOa-&gt;StateTsIOa
-- As Sir Mix-A-Lot once said,
-- stack 'em up deep
liftIO::IOa-&gt;StateTs (ReaderT rIO) a
liftIO::IOa
-&gt;ExceptT
e
(StateTs (ReaderT rIO))
a
You don’t have to lift multiple times if you’re trying to reach
a base (outermost) Monadthat happens to be IO, because you
haveliftIO .</p>
<p>CHAPTER 26. STACK ‘EM UP 1598
In thetransformers library, the MonadIO class resides in the
module Control.Monad.IO.Class :
class(Monadm)=&gt;MonadIO mwhere
-- | Lift a computation
-- from the 'IO' monad.
liftIO::IOa-&gt;m a
The commentary within the module is reasonably helpful,
though it doesn’t highlight what makes MonadIO diﬀerent from
MonadTrans :
Monads in which IO computations may be embed-
ded. Any monad built by applying a sequence of
monad transformers to the IO monad will be an in-
stance of this class.
Instances should satisfy the following laws, which
state that liftIO is a transformer of monads:
1.liftIO.return=return
2.liftIO(m&gt;&gt;=f)=
liftIO m &gt;&gt;=(liftIO .f)
Let us modify the scotty example app to print a string:</p>
<p>CHAPTER 26. STACK ‘EM UP 1599
{-# LANGUAGE OverloadedStrings #-}
moduleMainwhere
importWeb.Scotty
importControl.Monad.IO.Class
importData.Monoid (mconcat)
main=scotty3000$ do
get&quot;/:word&quot; $ do
beam&lt;-param&quot;word&quot;
liftIO (putStrLn &quot;hello&quot;)
html$
mconcat [ &quot;<h1>Scotty, &quot; ,
beam,
&quot; me up!</h1>&quot; ]
If you then run mainin a REPL or build a binary and execute
it, you’ll be able to request a response from the server using
your web browser (as we showed you earlier) or a command
line application like curl. If you used a browser and see “hello”
printed more than once, it’s likely your browser made the
request more than once. You shouldn’t see this behavior if
you test it with curl.</p>
<p>CHAPTER 26. STACK ‘EM UP 1600
Example MonadIO instances
1.IdentityT
instance (MonadIO m)
=&gt;MonadIO (IdentityT m)where
liftIO=IdentityT .liftIO
2.EitherT
instance (MonadIO m)
=&gt;MonadIO (EitherT e m)where
liftIO=lift.liftIO
Exercises: Some Instances
1.MaybeT
instance (MonadIO m)
=&gt;MonadIO (MaybeTm)where
liftIO=undefined
2.ReaderT
instance (MonadIO m)
=&gt;MonadIO (ReaderT r m)where
liftIO=undefined
3.StateT</p>
<p>CHAPTER 26. STACK ‘EM UP 1601
instance (MonadIO m)
=&gt;MonadIO (StateTs m)where
liftIO=undefined
Hint: your instances should be simple.
26.11 Monad transformers in use
MaybeT in use
These are some example of MaybeT in use; we will not comment
upon them and instead let you research them further yourself
if you want. Origins of the code are noted in the samples.
-- github.com/wavewave/hoodle-core
recentFolderHook
::MainCoroutine (MaybeFilePath )
recentFolderHook = do
xstate&lt;-get
(r::MaybeFilePath )&lt;-runMaybeT $ do
hset&lt;-hoist (view hookSet xstate)
rfolder &lt;-
hoist (H.recentFolderHook hset)
liftIO rfolder
return r</p>
<p>CHAPTER 26. STACK ‘EM UP 1602
-- github.com/devalot/hs-exceptions
-- src/maybe.hs
addT::FilePath
-&gt;FilePath
-&gt;IO(MaybeInteger)
addTf1 f2=runMaybeT $ do
s1&lt;-sizeT f1
s2&lt;-sizeT f2
return (s1 +s2)</p>
<p>CHAPTER 26. STACK ‘EM UP 1603
-- wavewave/ghcjs-dom-delegator
-- example/Example.hs
main::IO()
main= do
clickbarref &lt;-
asyncCallback1 AlwaysRetain clickbar
clickbazref &lt;-
asyncCallback1 AlwaysRetain clickbaz
r&lt;-runMaybeT $ do
doc&lt;-MaybeTcurrentDocument
bar&lt;-lift.toJSRef
=&lt;&lt;MaybeT
(documentQuerySelector doc
(&quot;.bar&quot;::JSString ))
baz&lt;-lift.toJSRef
=&lt;&lt;MaybeT
(documentQuerySelector doc
(&quot;.baz&quot;::JSString ))
lift$ do
ref&lt;-newObj
del&lt;-delegator ref
addEvent bar &quot;click&quot; clickbarref
addEvent baz &quot;click&quot; clickbazref
caserof
Nothing -&gt;print&quot;something wrong&quot;
Just_ -&gt;print&quot;welldone&quot;</p>
<p>CHAPTER 26. STACK ‘EM UP 1604
Temporary extension of structure
Although we commonly think of monad transformers as being
used to define one big context for an application, particularly
with things like ReaderT , there are other ways. One pattern that
is often useful is temporarily extending additional structure
to avoid boilerplate. Here’s an example using plain old Maybe
andscotty :</p>
<p>CHAPTER 26. STACK ‘EM UP 1605
{-# LANGUAGE OverloadedStrings #-}
moduleMainwhere
importControl.Monad.IO.Class
importData.Maybe (fromMaybe )
importData.Text.Lazy (Text)
importWeb.Scotty
param'::Parsable a
=&gt;Text-&gt;ActionM (Maybea)
param'k=rescue ( Just&lt;$&gt;param k)
(const (return Nothing))
main=scotty3000$ do
get&quot;/:word&quot; $ do
beam'&lt;-param'&quot;word&quot;
letbeam=fromMaybe &quot;&quot;beam'
i&lt;-param'&quot;num&quot;
liftIO$print (i ::MaybeInteger)
html$
mconcat [ &quot;<h1>Scotty, &quot; ,
beam,
&quot; me up!</h1>&quot; ]</p>
<p>CHAPTER 26. STACK ‘EM UP 1606
This works well enough but could get tedious in a hurry if
we had a bunch of stuﬀ that returned ActionM (Maybe ...) and
we wanted to short-circuit the moment any of them failed.
So, we do something similar but with MaybeT and building up
more data in one go:</p>
<p>CHAPTER 26. STACK ‘EM UP 1607
{-# LANGUAGE OverloadedStrings #-}
moduleMainwhere
importControl.Monad.IO.Class
importControl.Monad.Trans.Class
importControl.Monad.Trans.Maybe
importData.Maybe (fromMaybe )
importData.Text.Lazy (Text)
importWeb.Scotty
param'::Parsable a
=&gt;Text-&gt;MaybeTActionM a
param'k=MaybeT$
rescue ( Just&lt;$&gt;param k)
(const (return Nothing))
typeReco=
(Integer,Integer,Integer,Integer)
main=scotty3000$ do
get&quot;/:word&quot; $ do
beam&lt;-param&quot;word&quot;
reco&lt;-runMaybeT $ do
a&lt;-param'&quot;1&quot;
liftIO$print a
b&lt;-param'&quot;2&quot;
c&lt;-param'&quot;3&quot;
d&lt;-param'&quot;4&quot;
(lift.lift)$print b
return ((a, b, c, d) ::Reco)
liftIO$print reco
html$
mconcat [ &quot;<h1>Scotty, &quot; ,
beam,
&quot; me up!</h1>&quot; ]</p>
<p>CHAPTER 26. STACK ‘EM UP 1608
Some important things to note here:
1.We only had to use liftIO once, even in the presence of
additional structure, whereas with liftwe had to lift twice
to address MaybeT andActionM .
2.The one big bind of the MaybeT means we could take the
existence of 𝑎,𝑏,𝑐, and𝑑for granted in that context, but
therecovalue itself is Maybe Reco because any part of the
computation could fail in the absence of the needed pa-
rameter.
3.It knows what monad we mean for that doblock because
of therunMaybeT in front of the do. This serves the dual
purpose of unpacking the MaybeT into an ActionM (Maybe
Reco)which we can bind out into Maybe Reco .
ExceptT aka EitherT in use
The example with Maybeandscotty may not have totally satis-
fied because the failure mode isn’t helpful to an end-user —
all they know is “Nothing.” Accordingly, Maybeis usually some-
thing that should get handled early and often in a place local
to where it was produced so that you avoid mysterious Nothing
values floating around and short-circuiting your code. They’re
not something you want to return to end-users either. Fortu-
nately, we have Either for more descriptive short-circuiting
computations!</p>
<p>CHAPTER 26. STACK ‘EM UP 1609
Scotty, again
We’ll use scotty again to demonstrate this. Once again, we’ll
show you a plain example:
{-# LANGUAGE OverloadedStrings #-}
moduleMainwhere
importControl.Monad.IO.Class
importData.Text.Lazy (Text)
importWeb.Scotty
param'::Parsable a
=&gt;Text-&gt;ActionM (EitherStringa)
param'k=
rescue ( Right&lt;$&gt;param k)
(const
(return
(Left$&quot;The key: &quot;
++show k
++&quot; was missing!&quot; )))</p>
<p>CHAPTER 26. STACK ‘EM UP 1610
main=scotty3000$ do
get&quot;/:word&quot; $ do
beam&lt;-param&quot;word&quot;
a&lt;-param'&quot;1&quot;
leta'=either (const 0) id a
liftIO$print (a ::EitherStringInt)
liftIO$print (a' ::Int)
html$
mconcat [ &quot;<h1>Scotty, &quot; ,
beam,
&quot; me up!</h1>&quot; ]
Note that we had to manually fold the Either if we wanted
to address the desired Intvalue. Try to avoid having default
fallback values in real code though. This could get nutty in
a hurry if we had many things we were pulling out of the
parameters.
Let’s do that but with ExceptT fromtransformers . Remember,
ExceptT is another name for EitherT :</p>
<p>CHAPTER 26. STACK ‘EM UP 1611
{-# LANGUAGE OverloadedStrings #-}
moduleMainwhere
importControl.Monad.IO.Class
importControl.Monad.Trans.Class
importControl.Monad.Trans.Except
importData.Text.Lazy (Text)
import qualified Data.Text.Lazy asTL
importWeb.Scotty</p>
<p>CHAPTER 26. STACK ‘EM UP 1612
param'::Parsable a
=&gt;Text-&gt;ExceptT StringActionM a
param'k=
ExceptT $
rescue ( Right&lt;$&gt;param k)
(const
(return
(Left$&quot;The key: &quot;
++show k
++&quot; was missing!&quot; )))
typeReco=
(Integer,Integer,Integer,Integer)
tshow=TL.pack.show</p>
<p>CHAPTER 26. STACK ‘EM UP 1613
main=scotty3000$ do
get&quot;/&quot;$ do
reco&lt;-runExceptT $ do
a&lt;-param'&quot;1&quot;
liftIO$print a
b&lt;-param'&quot;2&quot;
c&lt;-param'&quot;3&quot;
d&lt;-param'&quot;4&quot;
(lift.lift)$print b
return ((a, b, c, d) ::Reco)
caserecoof
(Lefte)-&gt;text (TL.pack e)
(Rightr)-&gt;
html$
mconcat [ &quot;<h1>Success! Reco was: &quot; ,
tshow r,
&quot;</h1>&quot;]
If you pass it a request like:
http://localhost:3000/?1=1
It’ll ask for the parameter 2because that was the next param
you asked for after 1.
If you pass it a request like:
http://localhost:3000/?1=1&amp;2=2&amp;3=3&amp;4=4</p>
<p>CHAPTER 26. STACK ‘EM UP 1614
You should see the response in your browser or terminal
of:
Success! Reco was: (1,2,3,4)
As before, we get to benefit from one big bind under the
ExceptT .
Slightly more advanced code
From some code7by Sean Chalmers8.
Some context for the EitherT application you’ll see:
typeEta=EitherT SDLErrIOa
mkWindow ::HasSDLErr m=&gt;
String
-&gt;CInt-&gt;CInt
-&gt;mSDL.Window
mkRenderer ::HasSDLErr m
=&gt;SDL.Window-&gt;mSDL.Renderer
7https://github.com/mankyKitty/Meteor/
8http://mankykitty.github.io/</p>
<p>CHAPTER 26. STACK ‘EM UP 1615
hasSDLErr ::(MonadIO m,MonadError e m)
=&gt;(a-&gt;b)
-&gt;(a-&gt;Bool)
-&gt;e-&gt;IOa-&gt;m b
hasSDLErr g f e a =
liftIO a
&gt;&gt;=\r-&gt;
bool (return $g r)
(throwError e) $f r
class(MonadIO m,MonadError SDLErrm)
=&gt;HasSDLErr mwhere
decide ::(a-&gt;Bool)
-&gt;SDLErr-&gt;IOa-&gt;m a
decide' ::(Eqn,Numn)
=&gt;SDLErr-&gt;IOn-&gt;m()
instance HasSDLErr
(EitherT SDLErrIO)where
decide =hasSDLErr id
decide' =hasSDLErr (const ()) (/=0)
Then in use:</p>
<p>CHAPTER 26. STACK ‘EM UP 1616
initialise ::Et(SDL.Window,SDL.Renderer )
initialise = do
initSDL [ SDL.SDL_INIT_VIDEO ]
win&lt;-
mkWindow &quot;Meteor!&quot;
screenHeight
screenWidth
rdr&lt;-mkRenderer win
return (win,rdr)
createMeteor ::IO(EitherSDLErrMeteorS)
createMeteor = do
eM&lt;-runEitherT initialise
return$mkMeteor &lt;$&gt;eM
where
emptyBullets =V.empty
mkMeteor (w,r) =MeteorS w r
getInitialPlayer
emptyBullets
getInitialMobs
False</p>
<p>CHAPTER 26. STACK ‘EM UP 1617
26.12 Monads do not commute
Remember that monads in general do not commute, and
you aren’t guaranteed something sensible for every possible
combination of types. The kit we have for constructing and
using monad transformers is useful but is not a license to not
think!
Hypothetical Exercise
Consider ReaderT r Maybe andMaybeT (Reader r) — are these
types equivalent? Do they do the same thing? Try writing
otherwise similar bits of code with each and see if you can
prove they’re the same or diﬀerent.
26.13 Transform if you want to
If you find monad transformers difficult or annoying, then
don’t bother! Most of the time you can get by with liftIO and
plainIOactions, functions, Maybevalues, etc. Do the simplest
(for you) thing first when mapping out something new or un-
familiar. It’s better to let more structured formulations of
programs fall out naturally from having kicked around some-
thing uncomplicated than to blow out your working memory
budget in one go. Don’t worry about seeming unsophisticated;</p>
<p>CHAPTER 26. STACK ‘EM UP 1618
in our opinion, being happy and productive is better than
being fancy.
Keep it basic in your first attempt. Never make it more
elaborate initially than is strictly necessary. You’ll figure out
when the transformer variant of a type will save you complex-
ity in the process of writing your programs. We have taken
you through these topics because you’ll need at least a passing
familiarity to use modern Haskell libraries or frameworks, but
it’s not a design dictate you must follow.
26.14 Chapter Exercises
Write the code
1.rDecis a function that should get its argument in the con-
text ofReader and return a value decremented by one.
rDec::Numa=&gt;Readera a
rDec=undefined
Prelude&gt; import Control.Monad.Trans.Reader
Prelude&gt; runReader rDec 1
0
Prelude&gt; fmap (runReader rDec) [1..10]
[0,1,2,3,4,5,6,7,8,9]</p>
<p>CHAPTER 26. STACK ‘EM UP 1619
Note that “Reader” from transformers isReaderT ofIdentity
and that runReader is a convenience function throwing
awaythemeaninglessstructureforyou. Playwith runReaderT
if you like.
2.Once you have an rDecthat works, make it and any inner
lambdas pointfree if that’s not already the case.
3.rShowisshow, but in Reader .
rShow::Showa
=&gt;ReaderT aIdentity String
rShow=undefined
Prelude&gt; runReader rShow 1
&quot;1&quot;
Prelude&gt; fmap (runReader rShow) [1..10]
[&quot;1&quot;,&quot;2&quot;,&quot;3&quot;,&quot;4&quot;,&quot;5&quot;,&quot;6&quot;,&quot;7&quot;,&quot;8&quot;,&quot;9&quot;,&quot;10&quot;]
4.Once you have an rShowthat works, make it pointfree.
5.rPrintAndInc will first print the input with a greeting, then
return the input incremented by one.
rPrintAndInc ::(Numa,Showa)
=&gt;ReaderT aIOa
rPrintAndInc =undefined</p>
<p>CHAPTER 26. STACK ‘EM UP 1620
Prelude&gt; runReaderT rPrintAndInc 1
Hi: 1
2
Prelude&gt; traverse (runReaderT rPrintAndInc) [1..10]
Hi: 1
Hi: 2
Hi: 3
Hi: 4
Hi: 5
Hi: 6
Hi: 7
Hi: 8
Hi: 9
Hi: 10
[2,3,4,5,6,7,8,9,10,11]
6.sPrintIncAccum first prints the input with a greeting, then
puts the incremented input as the new state, and returns
the original input as a String .
sPrintIncAccum ::(Numa,Showa)
=&gt;StateTaIOString
sPrintIncAccum =undefined
Prelude&gt; runStateT sPrintIncAccum 10
Hi: 10</p>
<p>CHAPTER 26. STACK ‘EM UP 1621
(&quot;10&quot;,11)
Prelude&gt; mapM (runStateT sPrintIncAccum) [1..5]
Hi: 1
Hi: 2
Hi: 3
Hi: 4
Hi: 5
[(&quot;1&quot;,2),(&quot;2&quot;,3),(&quot;3&quot;,4),(&quot;4&quot;,5),(&quot;5&quot;,6)]
Fix the code
The code won’t typecheck as written; fix it so that it does. Feel
free to add imports if it provides something useful. Functions
will be used that we haven’t introduced. You’re not allowed
to change the types asserted. You may have to fix the code in
more than one place.
importControl.Monad.Trans.Maybe
importControl.Monad
isValid ::String-&gt;Bool
isValid v='!'<code>elem</code> v</p>
<p>CHAPTER 26. STACK ‘EM UP 1622
maybeExcite ::MaybeTIOString
maybeExcite = do
v&lt;-getLine
guard$isValid v
return v
doExcite ::IO()
doExcite = do
putStrLn &quot;say something excite!&quot;
excite&lt;-maybeExcite
caseexciteof
Nothing -&gt;putStrLn &quot;MOAR EXCITE&quot;
Juste-&gt;
putStrLn
(&quot;Good, was very excite: &quot; ++e)
Hit counter
We’re going to provide an initial scaﬀold of a scotty application
which counts hits to specific URIs. It also prefixes the keys
with a prefix defined on app initialization, retrieved via the
command line arguments.</p>
<p>CHAPTER 26. STACK ‘EM UP 1623
{-# LANGUAGE OverloadedStrings #-}
moduleMainwhere
importControl.Monad.Trans.Class
importControl.Monad.Trans.Reader
importData.IORef
import qualified Data.Map asM
importData.Maybe (fromMaybe )
importData.Text.Lazy (Text)
import qualified Data.Text.Lazy asTL
importSystem.Environment (getArgs)
importWeb.Scotty.Trans
dataConfig=
Config{
-- that's one, one click!
-- two...two clicks!
-- Three BEAUTIFUL clicks! ah ah ahhhh
counts::IORef(M.MapTextInteger)
, prefix ::Text
}
Stuﬀ inside ScottyT is, except for things that escape via IO,
eﬀectively read-only so we can’t use StateT . It would overcom-</p>
<p>CHAPTER 26. STACK ‘EM UP 1624
plicate things to attempt to do so and you should be using a
proper database for production applications.
typeScotty=
ScottyT Text(ReaderT ConfigIO)
typeHandler =
ActionT Text(ReaderT ConfigIO)
bumpBoomp ::Text
-&gt;M.MapTextInteger
-&gt;(M.MapTextInteger,Integer)
bumpBoomp k m=undefined
app::Scotty()
app=
get&quot;/:key&quot; $ do
unprefixed &lt;-param&quot;key&quot;
letkey'=mappend undefined unprefixed
newInteger &lt;-undefined
html$
mconcat [ &quot;<h1>Success! Count was: &quot;
,TL.pack$show newInteger
,&quot;</h1>&quot;
]</p>
<p>CHAPTER 26. STACK ‘EM UP 1625
main::IO()
main= do
[prefixArg] &lt;-getArgs
counter &lt;-newIORef M.empty
letconfig=undefined
runR=undefined
scottyT 3000runR app
Code is missing and broken. Your task is to make it work,
whatever is necessary.
You should be able to run the server from inside of GHCi,
passing arguments like so:
Prelude&gt; :main lol
Setting phasers to stun... (port 3000) (ctrl-c to quit)
You could also build a binary and pass the arguments from
your shell, but do what you like. Once it’s running, you should
be able to bump the counts like so:
$ curl localhost:3000/woot</p>
<h1 id="success-count-was-1"><a class="header" href="#success-count-was-1">Success! Count was: 1</a></h1>
$ curl localhost:3000/woot
<h1 id="success-count-was-2"><a class="header" href="#success-count-was-2">Success! Count was: 2</a></h1>
$ curl localhost:3000/blah
<h1 id="success-count-was-1-1"><a class="header" href="#success-count-was-1-1">Success! Count was: 1</a></h1>
<p>CHAPTER 26. STACK ‘EM UP 1626
Note that the underlying “key” used in the counter when
youGET /woot is&quot;lolwoot&quot; because we passed ”lol” to main. For
a giggle, try the URI for one of the keys in your browser and
mash refresh a bunch.
If you get stuck, consider checking for examples such as the
reader file in scotty ’s examples directory of the git repository.
Morra
1.Write the game Morra9usingStateT andIO. The state
being accumulated is the score of the player and the com-
puter AI the player is playing against. To start, make the
computer choose its play randomly.
On exit, report the scores for the player and the computer,
congratulating the winner.
2.Add a human vs. human mode to the game with inter-
stitial screens between input prompts so the players can
change out of the hotseat without seeing the other player’s
answer.
3.Improve the computer AI slightly by making it remem-
ber 3-grams of the player’s behavior, adjusting its answer
instead of deciding randomly when the player’s behavior
matches a known behavior. For example:
9You can find descriptions of the rules and gameplay of the Morra game online.</p>
<p>CHAPTER 26. STACK ‘EM UP 1627
-- p is Player
-- c is Computer
-- Player is odds, computer is evens.
P: 1
C: 1</p>
<ul>
<li>C wins
P: 2
C: 1</li>
<li>P wins
P: 2
C: 1</li>
<li>P wins
At this point, the computer should register the pattern (1,
2, 2) player picked 2 after 1 and 2. Next time the player
picks 1 followed by 2, the computer should assume the
next play will be 2 and pick 2 in order to win.
4.The 3-gram thing is pretty simple and dumb. Humans are
still bad at being random; they often have sub-patterns
in their moves.
26.15 Defintion
In general, the term leak refers to something that consumes
a resource in a way that renders it unusable or irrecoverable;</li>
</ul>
<p>CHAPTER 26. STACK ‘EM UP 1628
specifically, when we talk about a memory leak, we’re talking
about consuming memory in a way that renders it not usable
or recoverable by other programs or parts of a program. This
can happen if your program is written in such a way that it
accumulates large amounts of unevaluated thunks or holds in
memory a reference to something that it’s not using anymore.
The garbage collector cannot sweep those things away, so the
amount of memory a program is using can increase, some-
times rapidly and alarmingly, while the amount of available
or free memory decreases.
26.16 Follow-up resources
1.Parallel and Concurrent Programming in Haskell; Simon
Marlow; http://chimera.labs.oreilly.com/books/1230000000929</p>
<p>Chapter 27
Nonstrictness
Progress doesn’t come
from early risers —
progress is made by lazy
men looking for easier
ways to do things.
Robert A. Heinlein
1629</p>
<p>CHAPTER 27. NONSTRICTNESS 1630
27.1 Laziness
This chapter concerns the ways in which Haskell programs are
evaluated. We’ve addressed this a bit in previous chapters, for
example, in the Folds chapter where we went into some detail
about how folds evaluate. In this chapter, our goal is to give
you enough information about Haskell’s evaluation strategy
that you’ll be able to reason confidently about the reduction
process of your expressions and introduce stricter evaluation
where that is wanted.
Most programming languages have strict evaluation seman-
tics. Haskell technically has nonstrict — not lazy — evaluation,
but the diﬀerence between lazy and nonstrict is not practi-
cally relevant, so you’ll hear Haskell referred to as either a lazy
language or a nonstrict one.
A very rough outline of Haskell’s evaluation strategy is this:
most expressions are only reduced, or evaluated, when neces-
sary. When the evaluation process begins, a thunk is created
for each expression. We’ll go into more detail about this in
the chapter, but a thunk is like a placeholder in the underly-
ing graph of the program. Whatever expression the thunk is
holding a place for can be evaluated when necessary, but if
it’s never needed, it never gets reduced, and then the garbage
collector comes along and sweeps it away. If it is evaluated,
because it’s in a graph, it can be often shared between expres-
sions — that is, once x = 1 + 1 has been evaluated, anytime 𝑥</p>
<p>CHAPTER 27. NONSTRICTNESS 1631
is forced it does not have to be re-computed.
This is the laziness of Haskell: don’t do more work than
needed. Don’t evaluate until necessary. Don’t re-evaluate if
you don’t have to. We’ll go through the details of how this
works, exceptions to the general principles, and how to control
the evaluation by adding strictness where desired.
Specifically, we will:
•define call-by-name and call-by-need evaluation;
•explain the main eﬀects of nonstrict evaluation;
•live the Thunk Life1;
•consider the runtime behavior of non-strict code in terms
of sharing;
•developmethodsforobservingsharingbehaviorandmea-
suring program efficiency;
•bottom out with the bottoms.
27.2 Observational Bottom Theory
In our discussion about nonstrictness in Haskell, we’re going
to be talking about bottom2a lot. This is partly because non-
strictness is defined by the ability to evaluate expressions that
1We love you, Jesse!
2Observational bottom theory is not a real thing. Do not email us about this.</p>
<p>CHAPTER 27. NONSTRICTNESS 1632
have bindings which are bottom in them, as long as the bot-
tom itself is never forced. Bottoms also give us a convenient
method of observing evaluation in Haskell. By causing the
program to halt immediately with an error, bottom serves as
our first means of understanding nonstrictness in Haskell. You
probably recall we have used this trick before.
Standards and obligations
Technically Haskell is only obligated to be nonstrict, not lazy.
A truly lazy language memoizes, or holds in memory, the
results of all the functions it does evaluate, and, outside of toy
programs, this tends to use unacceptably large amounts of
memory. Implementations of Haskell, such as GHC Haskell,
are only obligated to be nonstrict such that they have the same
behavior with respect to bottom; they are not required to take
a particular approach to how the program executes or how
efficiently it does so.
The essence of nonstrictness is that you can have an expres-
sion which results in a value, even if bottom or infinite data
lurks within. For example, the following would only work in a
nonstrict language:
Prelude&gt; fst (1, undefined)
1
Prelude&gt; snd (undefined, 2)
2</p>
<p>CHAPTER 27. NONSTRICTNESS 1633
The idea is that any given implementation of nonstrictness
is acceptable as long as it respects when it’s supposed to return
a value successfully or bottom out.
27.3 Outside in, inside out
Strict languages evaluate inside out; nonstrict languages like
Haskell evaluate outside in. Outside in means that evaluation
proceeds from the outermost parts of expressions and works
inward based on what values are forced. This means the order
of evaluation and what gets evaluated can vary depending on
inputs.
The following example is written in a slightly arcane way
to make the evaluation order more obvious:
possiblyKaboom =
\f-&gt;f fst snd ( 0, undefined)
-- booleans in lambda form
true::a-&gt;a-&gt;a
true=\a-&gt;(\b-&gt;a)
false::a-&gt;a-&gt;a
false=\a-&gt;(\b-&gt;b)
When we apply possiblyKaboom totrue,trueis the𝑓,fstis
the𝑎, andsndis the𝑏. Semantically, case matches, guards</p>
<p>CHAPTER 27. NONSTRICTNESS 1634
expressions, and if-then-else expressions could all be rewritten
in this manner (they are not in fact decomposed this way by the
compiler), by nesting lambdas and reducing from the outside
in:
(\f-&gt;
f fst snd ( 0, undefined))
(\a-&gt;(\b-&gt;a))
(\a-&gt;(\b-&gt;a)) fst snd ( 0, undefined)
(\b-&gt;fst) snd ( 0, undefined)
fst(0, undefined)
0
The next example is written in more normal Haskell but
will return the same result. When we apply the function to
Truehere, we case on the Trueto return the first value of the
tuple:
possiblyKaboom b=
casebof
True-&gt;fst tup
False-&gt;snd tup
wheretup=(0, undefined)
The bottom is inside a tuple, and the tuple is bound inside
of a lambda that cases on a boolean value and returns either the
first or second element of the tuple. Since we start evaluating</p>
<p>CHAPTER 27. NONSTRICTNESS 1635
from the outside, as long as this function is only ever applied
toTrue, that bottom will never cause a problem. However, at
the risk of stating the obvious, we do not encourage you to
write programs with bottoms lying around willy-nilly.
When we say evaluation works outside in, we’re talking
about evaluating a series of nested expressions, and not only
are we starting from the outside and working in, but we’re also
only evaluating some of the expressions some of the time. In
Haskell, we evaluate expressions when we need them rather
than when they are first referred to or constructed. This is one
of the ways in which nonstrictness makes Haskell expressive
— we can refer to values before we’ve done the work to create
them.
This pattern applies to data structures and lambdas alike.
You’ve already seen the eﬀects of outside-in evaluation in the
chapter on folds. Outside-in evaluation is why we can take the
length of a list without touching any of the contents. Consider
the following:
-- using an old definition of foldr
foldrk z xs=go xs
where
go[]=z
go (y:ys)=y <code>k</code> go ys
c=foldr const 'z'['a'..'e']</p>
<p>CHAPTER 27. NONSTRICTNESS 1636
Expanding the foldrin𝑐:
c=const'z'&quot;abcde&quot; =go&quot;abcde&quot;
where
go[]='z'
go ('a':&quot;bcde&quot;)='a'<code>const</code> go &quot;bcde&quot;
-- So the first step of evaluating
-- of the fold here is:
const'a'(go&quot;bcde&quot;)
constx y =x
const'a'(go&quot;bcde&quot;)='a'
The second argument and step of the fold is never evalu-
ated:
const'a'_ ='a'
It doesn’t even matter if the next value is bottom:
Prelude&gt; foldr const 'z' ['a', undefined]
'a'
This is outside-in showing itself. The constfunction was in
the outermost position so it was evaluated first.</p>
<p>CHAPTER 27. NONSTRICTNESS 1637
27.4 What does the other way look like?
In strict languages, you cannot ordinarily bind a computa-
tion to a name without having already done all the work to
construct it.
We’ll use this example program to compare inside-out and
outside-in (strict and non-strict) evaluation strategies:
moduleOutsideIn where
hypo::IO()
hypo= do
letx::Int
x=undefined
s&lt;-getLine
casesof
&quot;hi&quot;-&gt;print x
_ -&gt;putStrLn &quot;hello&quot;
For a strict language, this is a problem. A strict language
cannot evaluate hyposuccessfully unless the 𝑥isn’t bottom.
This is because strict languages will force the bottom before
binding 𝑥. A strict language is evaluating each binding as it
comes into scope, not when a binding is used.
In nonstrict Haskell, you can probably guess how this’ll go:
Prelude&gt; hypo</p>
<p>CHAPTER 27. NONSTRICTNESS 1638
s
hello
Prelude&gt; hypo
hi
*** Exception: Prelude.undefined
The idea is that evaluation is driven by demand, not by
construction. We don’t get the exception unless we’re forcing
evaluation of 𝑥— outside in.
27.5 Can we make Haskell strict?
Let’s see if we can replicate the results of a strict language,
though, which will give us a good picture of how Haskell is
diﬀerent. We can add strictness here in the following manner:
hypo'::IO()
hypo'= do
letx::Integer
x=undefined
s&lt;-getLine
casex <code>seq</code> s of
&quot;hi&quot;-&gt;print x
_ -&gt;putStrLn &quot;hello&quot;
Running it will give this result:</p>
<p>CHAPTER 27. NONSTRICTNESS 1639
Prelude&gt; hypo'
asd
*** Exception: Prelude.undefined
Why? Because this little seqfunction magically forces eval-
uation of the first argument if and when the second argument
has to be evaluated. Adding seqmeans that anytime 𝑠is evalu-
ated,𝑥must also be evaluated. We’ll get into more detail in a
moment.
One thing to note before we investigate seqis that we man-
aged to run getLine before the bottom got evaluated, so this
still isn’t quite what a strict language would’ve done. Case
expressions are in general going to force evaluation. This
makes sense if you realize it has to evaluate the expression to
discriminate on the cases. A small example to demonstrate:
letb= ???
casebof
True-&gt; ...
False
Here𝑏could be pretty much anything. It must evaluate 𝑏
to find out if the expression results in TrueorFalse.</p>
<p>CHAPTER 27. NONSTRICTNESS 1640
seq and ye shall find
Before we move any further with making Haskell stricter, let’s
talk about seqa little bit. One thing is that the type is, uh, a bit
weird:
seq::a-&gt;b-&gt;b
Clearly there’s more going on here than flip const . It might
help to know that in some old versions of Haskell, it used to
have the type:
seq::Evala=&gt;a-&gt;b-&gt;b
Evalis short for evaluation to weak head normal form, and
it provided a method for forcing evaluation. Instances were
provided for all the types in base. It was elided in part so you
could use seqin your code without churning your polymor-
phic type variables and forcing a bunch of changes. With
respect to bottom, seqis defined as behaving in the following
manner:
seqbottom b =bottom
seqliterallyAnythingNotBottom b =b
Now why does seqlook like const’s gawky cousin? Because
evaluation in Haskell is demand driven, we can’t guarantee that
something will ever be evaluated period . Instead we have to</p>
<p>CHAPTER 27. NONSTRICTNESS 1641
create links between nodes in the graph of expressions where
forcing one expression will force yet another expression. Let’s
look at another example:
Prelude&gt; :{
*Main| let wc x z =
*Main| let y =
*Main| undefined <code>seq</code> 'y' in x
*Main| :}
Prelude&gt; foldr wc 'z' ['a'..'e']
'a'
Prelude&gt; foldr (flip wc) 'z' ['a'..'e']
'z'
We never evaluated 𝑦, so we never forced the bottom. How-
ever, we can lash yet another data dependency from 𝑦to𝑥:
Prelude&gt; let bot = undefined
Prelude&gt; :{
*Main| let wc x z =
*Main| let y =
*Main| bot <code>seq</code> 'y'
*Main| in y <code>seq</code> x
*Main| :}
Prelude&gt; foldr wc 'z' ['a'..'e']
*** Exception: Prelude.undefined
Prelude&gt; foldr (flip wc) 'z' ['a'..'e']</p>
<p>CHAPTER 27. NONSTRICTNESS 1642
*** Exception: Prelude.undefined
Previously the evaluation dependency was between the
bottom value and 𝑦:
undefined <code>seq</code> y
-- forcing y necessarily forces undefined
y -&gt; undefined
Changing the expression as we did caused the following to
happen:
undefined <code>seq</code> y <code>seq</code> x
-- forcing x necessarily forces y
-- forcing y necessarily forces undefined
x -&gt; y -&gt; undefined
We think of this as a chain reaction.
All we can do is chuck a life raft from one value to another
as a means of saying, “if you want to get him, you gotta get
through me!” We can even set our life-raft buddies adrift!
Check it out:</p>
<p>CHAPTER 27. NONSTRICTNESS 1643
notGonnaHappenBru ::Int
notGonnaHappenBru =
letx=undefined
y=2
z=(x <code>seq</code> y <code>seq</code> 10,11)
insnd z
The above will not bottom out! Our life-raft buddies are
bobbing in the ocean blue, with no tugboat evaluator to pull
them in.
seq and weak head normal form
Whatseqdoes is evaluate your expression up to weak head nor-
mal form. We’ve discussed it before, but if you’d like a deeper
investigation and contrast of weak head normal form and nor-
mal form, we strongly recommend Simon Marlow’s Parallel
and Concurrent Programming in Haskell3. WHNF evaluation
means it stops at the first data constructor or lambda. Let’s
test that hypothesis!
Prelude&gt; let dc = (,) undefined undefined
Prelude&gt; let noDc = undefined
Prelude&gt; let lam = _ -&gt; undefined
Prelude&gt; dc <code>seq</code> 1
1
3http://chimera.labs.oreilly.com/books/1230000000929</p>
<p>CHAPTER 27. NONSTRICTNESS 1644
Prelude&gt; noDc <code>seq</code> 1
*** Exception: Prelude.undefined
Prelude&gt; lam <code>seq</code> 1
1
Right-o. No surprises, right? Right? Okay.
Sincedchas a data constructor, seqdoesn’t need to care
about the values inside that constructor; weak head normal
form evaluation only requires it to evaluate the constructor.
On the other hand, noDchas no data constructor or lambda
outside the value, so there’s no head for the evaluation to stop
at. Finally, lamhas a lambda outside the expression which has
the same eﬀect on evaluation as a data constructor does.
Case matching also chains evaluation
This forcing behavior happens already without seq! For ex-
ample, when you case or pattern match on something, you’re
forcing the value you pattern matched on because it doesn’t
know which data constructor is relevant until it is evaluated
to the depth required to yield the depth of data constructors
you pattern matched. Let’s look at an example:</p>
<p>CHAPTER 27. NONSTRICTNESS 1645
dataTest=
ATest2
|BTest2
deriving (Show)
dataTest2=
CInt
|DInt
deriving (Show)
forceNothing ::Test-&gt;Int
forceNothing _ =0
forceTest ::Test-&gt;Int
forceTest (A_)=1
forceTest (B_)=2
forceTest2 ::Test-&gt;Int
forceTest2 (A(Ci))=i
forceTest2 (B(Ci))=i
forceTest2 (A(Di))=i
forceTest2 (B(Di))=i
We’ll test forceNothing first:
Prelude&gt; forceNothing undefined</p>
<p>CHAPTER 27. NONSTRICTNESS 1646
0
Prelude&gt; forceNothing (A undefined)
0
It’ll never bottom out because it never forces anything. It’s
just a constant value that drops its argument on the floor. What
aboutforceTest ?
Prelude&gt; forceTest (A undefined)
1
Prelude&gt; forceTest (B undefined)
2
Prelude&gt; forceTest undefined
*** Exception: Prelude.undefined
We only get a bottom when the outermost Testvalue is
bottom because that’s the only value whose data constructors
we’re casing on. And then with forceTest2 :
Prelude&gt; forceTest2 (A (C 0))
0
Prelude&gt; forceTest2 (A (C undefined))
*** Exception: Prelude.undefined
Prelude&gt; forceTest2 (A undefined)
*** Exception: Prelude.undefined</p>
<p>CHAPTER 27. NONSTRICTNESS 1647
Prelude&gt; forceTest2 undefined
*** Exception: Prelude.undefined
There we go: outside -&gt; in.
Core Dump
Not the usual core dump you might be thinking of. In this
case, we’re talking about the underlying language that GHC
Haskell gets simplified to after the compiler has desugared our
code.
Our first means of determining strictness was by injecting
bottoms into our expressions and observing the evaluation.
Injecting bottoms everywhere allows us to see clearly what’s
being evaluated strictly and what’s not. Our second means of
determining strictness in Haskell is examining GHC Core4.
Here’s the example we’ll be working with:
moduleCoreDump where
discriminatory ::Bool-&gt;Int
discriminatory b=
casebof
False-&gt;0
True-&gt;1
4https://ghc.haskell.org/trac/ghc/wiki/Commentary/Compiler/CoreSynType</p>
<p>CHAPTER 27. NONSTRICTNESS 1648
Load this up in GHCi in the following manner:
Prelude&gt; :set -ddump-simpl
Prelude&gt; :l code/coreDump.hs
[1 of 1] Compiling CoreDump
================ Tidy Core ==============
... some noise...
You should then get the following GHC Core output:
discriminatory ::Bool-&gt;Int
[GblId,Arity=1,
Caf=NoCafRefs ,
Str=DmdType]
discriminatory =
(b_aZJ::Bool)-&gt;
caseb_aZJof _[Occ=Dead] {
False-&gt;GHC.Types.I#0;
True-&gt;GHC.Types.I#1
}
We’re not going to dissemble: GHC Core is a bit ugly. How-
ever, there are some means of cleaning it up. One is to use the
-dsuppress-all flag:
Prelude&gt; :set -dsuppress-all</p>
<p>CHAPTER 27. NONSTRICTNESS 1649
Prelude&gt; :r
Note that you may need to poke the file to force it to reload.
This then outputs:
discriminatory
discriminatory =
\b_aZY-&gt;
caseb_aZYof _{
False-&gt;I#0;
True-&gt;I#1
}
A titch more readable. The idea here is that the simpler
Core language gives us a clearer idea of when precisely some-
thing will be evaluated. For the sake of simplicity, we’ll revisit
a previous example:
forceNothing _ =0
In Core, it looks like this:
forceNothing =_ -&gt;I#0#
We’re looking for case expressions in GHC Core to find out
where the strictness is in our code, because case expressions
must be evaluated. There aren’t any cases here, so it forces</p>
<p>CHAPTER 27. NONSTRICTNESS 1650
strictly5nothing! The I# O#is the underlying representation
of anIntliteral which is exposed in GHC Core. On with the
show!
Let’s see what the Core for forceTest looks like:
forceTest =
\ds_d2oX -&gt;
caseds_d2oX of _{
Ads1_d2pI -&gt;I#1#;
Bds1_d2pJ -&gt;I#2#
}
From the GHC Core for this we can see that we force one
value, the outermost data constructors of the Testtype. The
contents of those data constructors are given a name but never
used and so are never evaluated.
5HAAAAAAAAAAAA</p>
<p>CHAPTER 27. NONSTRICTNESS 1651
forceTest2 =
\ds_d2n2 -&gt;
caseds_d2n2 of _{
Ads1_d2oV -&gt;
caseds1_d2oV of _{
Ci_a1lo-&gt;i_a1lo;
Di_a1lq-&gt;i_a1lq
};
Bds1_d2oW -&gt;
caseds1_d2oW of _{
Ci_a1lp-&gt;i_a1lp;
Di_a1lr-&gt;i_a1lr
}
}
WithforceTest2 the outsideness and insideness shows more
clearly. In the outer part of the function, we do the same as
forceTest , but the diﬀerence is that we end up also forcing the
contents of the outer Testdata constructors. The function has
four possible results that aren’t bottom and if it isn’t passed bot-
tom it’ll always force twice — once for Testand once for Test2.
It returns but does not itself force or evaluate the contents of
theTest2data constructor.
In Core, a case expression always evaluates what it cases
on — even if no pattern matching is performed — whereas in</p>
<p>CHAPTER 27. NONSTRICTNESS 1652
Haskell proper, values are forced when matching on data con-
structors. We recommend reading the GHC documentation
on the Core language in the footnote above if you’d like to
leverage Core to understand your Haskell code’s performance
or behavior more deeply.
Now let us use this to analyze something:
discriminatory ::Bool-&gt;Int
discriminatory b=
letx=undefined
in case bof
False-&gt;0
True-&gt;1
What does the Core for this look like?
discriminatory
discriminatory =
\b_a10c-&gt;
caseb_a10cof _{
False-&gt;I#0;
True-&gt;I#1
}
GHC is too clever for our shenanigans! It knows we’ll never
evaluate 𝑥, so it drops it. What if we force it to evaluate 𝑥before
we evaluate 𝑏?</p>
<p>CHAPTER 27. NONSTRICTNESS 1653
discriminatory ::Bool-&gt;Int
discriminatory b=
letx=undefined
in case x <code>seq</code> b of
False-&gt;0
True-&gt;1
Then the Core:
discriminatory =
\b_a10D-&gt;
let{
x_a10E
x_a10E=undefined } in
case
casex_a10Eof _{
__DEFAULT -&gt;b_a10D
}of _{
False-&gt;I#0;
True-&gt;I#1
}
What’s happened here is that there are now two case ex-
pressions, one nested in another. The nesting is to make the
evaluation of 𝑥obligatory before evaluating 𝑏. This is how seq
changes your code.</p>
<p>CHAPTER 27. NONSTRICTNESS 1654
A Core diﬀerence In Haskell, case matching is strict — or, at
least, the pattern matching of it is — up to WHNF. In Core,
cases are always strict6to WHNF. This doesn’t seem to be a
distinction that matters, but there are times when the distinc-
tion becomes relevant. In Haskell, this will not bottom out:
caseundefined of{_ -&gt;False}
When that gets transliterated into Core, it recognizes that
we didn’t actually use the case match for anything and drops
the case expression entirely, simplifying it to just the data
constructor False.
However, this Core expression is syntactically similar to the
Haskell above, but it will bottom out:
caseundefined of{DEFAULT -&gt;False}
Case in Core is strict even if there’s one case and it doesn’t
match on anything. Core and Haskell are not the same lan-
guage, but anytime you need to know if two expressions in
Haskell are the same, one way to know for sure is by examining
the Core.
6https://ghc.haskell.org/trac/ghc/wiki/Commentary/Compiler/CoreSynType#
Caseexpressions</p>
<p>CHAPTER 27. NONSTRICTNESS 1655
A little bit stricter now
Okay, we had a nice little digression there into wonderland!
Let’s get back to the point which is…we still haven’t quite man-
aged to accomplish what a strict language would have done
with our hypofunction, because we did partially evaluate the
expression. We evaluated the 𝑠which forced the 𝑥which is
what finally gave us the exception. A strict language would not
even have evaluated 𝑠, because evaluating 𝑠would depend on
the𝑥inside already being evaluated.
What if we want our Haskell program to do as a strict lan-
guage would’ve done?
hypo''::IO()
hypo''= do
letx::Integer
x=undefined
s&lt;-x <code>seq</code> getLine
casesof
&quot;hi&quot;-&gt;print x
_ -&gt;putStrLn &quot;hello&quot;
Notice we moved the seqto the earliest possible point in
ourIOaction. This one’ll just pop without so much as a by-
your-leave:
Prelude&gt; hypo''</p>
<p>CHAPTER 27. NONSTRICTNESS 1656
*** Exception: Prelude.undefined
The reason is that we’re forcing evaluation of the bottom
before we evaluate getLine , which would have performed the
eﬀect of awaiting user input. While this reproduces the ob-
servable results of what a strict language might have done,
it isn’t truly the same thing because we’re not firing oﬀ the
error upon the construction of the bottom. It’s not possible for
an expression to be evaluated until the path evaluation takes
through your program has reached that expression. In Haskell,
the tree doesn’t fall in the woods until you walk through the
forest and get to the tree. For that matter, the tree didn’t exist
until you walked up to it.
Exercises: Evaluate
Expand the expression in as much detail as possible. Then,
work outside-in to see what the expression evaluates to.
1.const1undefined
2.constundefined 1
3.flipconst undefined 1
4.flipconst1undefined
5.constundefined undefined</p>
<p>CHAPTER 27. NONSTRICTNESS 1657
6.foldrconst'z'['a'..'e']
7.foldr(flip const) 'z'['a'..'e']
27.6 Call by name, call by need
Another way we can talk about diﬀerent evaluation strategies
is by distinguishing them on the basis of call by name, call by
need, and call by value.
1.Call by value: Argument expressions have been evaluated
before entering a function. The expressions that bindings
reference are evaluated before creating the binding. This
is conventionally called strict. This is inside-out evalua-
tion.
2.Call by name: Expressions can be arguments to a function
without having been evaluated, or in some cases, never
being evaluated. You can create bindings to expressions
without evaluating them first. Nonstrictness includes this
evaluation strategy. This is outside-in.
3.Call by need: This is the same as call by name, but expres-
sions are only evaluated once. This only happens some
of the time in GHC Haskell, usually when an expression
isn’t a lambda that takes arguments and also has a name.
Results are typically shared within that name only in GHC</p>
<p>CHAPTER 27. NONSTRICTNESS 1658
Haskell (that is, other implementations of Haskell may
choose to do things diﬀerently). This is also nonstrict and
outside-in.
27.7 Nonstrict evaluation changes what
we can do
We’ll cover normal order evaluation (the nonstrict strategy
Haskell prescribes for its implementations) in more detail later.
Now, we’ll look at examples of what nonstrictness enables. The
following will work in languages with a strict or a nonstrict
evaluation strategy:
Prelude&gt; let myList = [1, 2, 3]
Prelude&gt; tail myList
[2,3]
That works in either strict or nonstrict languages because
there is nothing there that can’t be evaluated. However, if we
keep in mind that undefined as an instance of bottom will throw
an error when forced:
Prelude&gt; undefined
*** Exception: Prelude.undefined
We’ll see a diﬀerence between strict and nonstrict. This will
only work in languages that are nonstrict:</p>
<p>CHAPTER 27. NONSTRICTNESS 1659
Prelude&gt; let myList = [undefined, 2, 3]
Prelude&gt; tail myList
[2,3]
A strict language would have crashed on construction of
myList due to the presence of bottom. This is because strict
languages eagerly evaluate all expressions as soon as they
are constructed. The moment [undefined, 2, 3] was declared,
undefined would’ve been evaluated as an argument to (:)and
raised the exception. In Haskell, however, nonstrict evaluation
means that bottom value won’t be evaluated unless it is needed
for some reason.
Take a look at the next example and, before going on, see
if you can figure out whether it will throw an exception and
why:
Prelude&gt; head $ sort [1, 2, 3, undefined]
When we call headon a list that has been passed to sort, we
only need the lowest value in the list and that’s all the work we
will do. The problem is that in order for sortto know what the
lowest value is, it must evaluate undefined which then throws
the error.</p>
<p>CHAPTER 27. NONSTRICTNESS 1660
27.8 Thunk Life
A thunk is used to reference suspended computations that
might be performed or computed at a later point in your pro-
gram. You can get into considerably more detail7on this topic,
but essentially thunks are computations not yet evaluated up
to weak head normal form. If you read the GHC notes on
this you’ll see references to head normal form — it’s the same
thing as weak head normal form.
Not all values get thunked
We’re going to be using the GHCi command sprint in this
section as one means of showing when something is thunked.
You may remember this from the Lists chapter, but let’s refresh
our memories a bit.
Thesprint command allows us to show what has been eval-
uated already by printing in the REPL. An underscore is used
to represent values that haven’t been evaluated yet. We noted
before that this command can have some quirky behavior, al-
though this chapter will explain some of the things that cause
those seemingly unpredictable behaviors.
Let’s start with a simple example:
Prelude&gt; let myList = [1, 2] :: [Integer]
7https://ghc.haskell.org/trac/ghc/wiki/Commentary/Rts/Storage/HeapObjects</p>
<p>CHAPTER 27. NONSTRICTNESS 1661
Prelude&gt; :sprint myList
myList = [1,2]
Wait a second — what happened here? Why is the list shown
fully evaluated when it’s not been needed by anything? This
is an opportunistic strictness. GHC will not thunk (and thus
delay) data constructors. Data constructors are known to be
constant, which justifies the safety of the optimization. The
data constructors here are cons (:), theInteger s, and the empty
list — all of them are constants.
But aren’t data constructors functions? Data constructors
are like functions when they’re unapplied, and constants once
they are fully applied. Since all the data constructors in the
above example are fully applied already, evaluating to weak
headnormalformmeansevaluatingeverythingbecausethere’s
nothing left to apply.
Now back to the thunkery.
A graph of the values of myList looks like:
myList
|
:
/ <br />
1 :
/ </p>
<p>CHAPTER 27. NONSTRICTNESS 1662
2 :
/ <br />
3 []
Here there aren’t any unevaluated thunks; it’s just the final
values that have been remembered. However, if we make it
more polymorphic:
Prelude&gt; let myList2 = [1, 2, 3]
Prelude&gt; :t myList2
myList2 :: Num t =&gt; [t]
Prelude&gt; :sprint myList2
myList2 = _
we’ll see an unevaluated thunk represented by the under-
score at the very top level of the expression. Since the type
is not concrete, there’s an implicit function Num a -&gt; a under-
neath, awaiting application to something that will force it to
evaluate to a concrete type. There’s nothing here triggering
that evaluation, so the whole list remains an unevaluated thunk.
We’ll get into more detail about how typeclass constraints eval-
uate soon.
GHC will also stop opportunistically evaluating as soon as
it hits a computation:
Prelude&gt; let xs = [1, 2, id 1] :: [Integer]
Prelude&gt; :sprint xs</p>
<p>CHAPTER 27. NONSTRICTNESS 1663
myList = [1,2,_]
It’s a trivial computation, but GHCi conveniently leaves it
be. Here’s the thunk graph for the above:
myList
|
:
/ <br />
1 :
/ <br />
2 :
/ <br />
_ []
Now let us consider another case that might be slightly
confusing initially for some:
Prelude&gt; let xs = [1, 2, id 1] :: [Integer]
Prelude&gt; let xs' = xs ++ undefined
Prelude&gt; :sprint xs'
myList' = _
Whoa whoa whoa. What’s going on here? The whole thing
is thunked because it’s not in weak head normal form. Why
isn’t it in weak head normal form already? Because the out-
ermost term isn’t a data constructor like (:). The outermost
term is the function (++):</p>
<p>CHAPTER 27. NONSTRICTNESS 1664
myList' = (++) _ _
The function is outermost, despite the fact that it is super-
ficially an infix operator, because the function is the lambda.
The arguments are passed into the function body to be evalu-
ated.
27.9 Sharing is caring
Sharing here roughly means what we’ve implied above: that
when a computation is named, the results of evaluating that
computation can be shared between all references to that name
without re-evaluating it. We care about sharing because mem-
ory is finite, even today in the land of chickens in every pot
and smartphones in every pocket. The idea here is that non-
strictness is a fine thing, but call-by-name semantics aren’t
always enough to make it sufficiently efficient. What is suffi-
ciently efficient? That depends on context and whether it’s
your dissertation or not.
One of the points of confusion for people when trying to
figure out how GHC Haskell really runs code is that it turns
sharing on and oﬀ (that is, it oscillates between call-by-need
and call-by-name) based on necessity and what it thinks will
produce faster code. Part of the reason it can do this at all
without breaking your code is because the compiler knows
when your code does or does not perform I/O.</p>
<p>CHAPTER 27. NONSTRICTNESS 1665
Using trace to observe sharing
The base library has a module named Debug.Trace that has
functions useful for observing sharing. We’ll mostly use trace
here, but feel free to poke around for whatever else might catch
your fancy. Debug.Trace is a means of cheating the type system
and putting a putStrLn without having IOin the type. This is def-
initely something you want to restrict to experimentation and
education; do not use it as a logging mechanism in production
code — it won’t do what you think. However, it does give us a
convenient means of observing when things evaluate.
Let us demonstrate how we can use this to see when things
get evaluated:
Prelude&gt; import Debug.Trace
Prelude&gt; let a = trace &quot;a&quot; 1
Prelude&gt; let b = trace &quot;b&quot; 2
Prelude&gt; a + b
b
a
3
This isn’t an example of sharing, but it demonstrates how
tracecan be used to observe evaluation. We can see that 𝑏
got printed first because that was the first argument that the
addition function evaluated, but you cannot and should not
rely on the evaluation order of the arguments to addition.</p>
<p>CHAPTER 27. NONSTRICTNESS 1666
Here we’re talking about the order in which the arguments to
a single application of addition are forced, not associativity.
You can count on addition being left associative, but within
each pairing, which in the pair of arguments gets forced is not
guaranteed.
Let’s look at a longer example and see how it shows us where
the evaluations occur:
importDebug.Trace (trace)
inc=(+1)
twice=inc.inc
howManyTimes =
inc (trace &quot;I got eval'd&quot; (1+1))
+twice
(trace&quot;I got eval'd&quot; (1+1))
howManyTimes' =
letonePlusOne =
trace&quot;I got eval'd&quot; (1+1)
ininc onePlusOne +twice onePlusOne
Prelude&gt; howManyTimes
I got eval'd</p>
<p>CHAPTER 27. NONSTRICTNESS 1667
I got eval'd
7
Prelude&gt; howManyTimes'
I got eval'd
7
Cool, with that in mind, let’s talk about ways to promote
and prevent sharing.
What promotes sharing
Kindness. Also, names. Names turn out to be a pretty good
way to make GHC share something, if it could’ve otherwise
been shared. First, let’s consider the example of something
that won’t get shared:
Prelude&gt; import Debug.Trace
Prelude&gt; let x = trace &quot;x&quot; (1 :: Int)
Prelude&gt; let y = trace &quot;y&quot; (1 :: Int)
Prelude&gt; x + y
x
y
2
This seems intuitive and reasonable, but the values of 𝑥
and𝑦cannot be shared because they have diﬀerent names.</p>
<p>CHAPTER 27. NONSTRICTNESS 1668
So, even though they have the same value, they have to be
evaluated separately.
GHC does use this intuition that you’ll expect results to be
shared when they have the same name to make performance
more predictable. If we add two values that have the same
name, it will get evaluated once and only once:
Prelude&gt; import Debug.Trace
Prelude&gt; let a = trace &quot;a&quot; (1 :: Int)
Prelude&gt; a + a
a
2
Prelude&gt; a + a
2
Indirection won’t change this either:
Prelude&gt; let x = trace &quot;x&quot; (1 :: Int)
Prelude&gt; (id x) + (id x)
x
2
Prelude&gt; (id x) + (id x)
2
GHC knows what’s up, despite the addition of identity func-
tions. Notice the second time we ran it, it didn’t evaluate 𝑥at</p>
<p>CHAPTER 27. NONSTRICTNESS 1669
all. The value of 𝑥is now held there in memory so whenever
your program calls 𝑥, it already knows the value.
In general, GHC relies on an intuition around names and
sharing to make performance more predictable. However,
this won’t always behave in ways you expect. Consider the
case of a list with a single character…and a String with a sin-
gle character. They’re actually the same thing, but the way
they get constructed is not. This produces diﬀerences in the
opportunistic strictness GHC will engage in.
Prelude&gt; let a = Just ['a']
Prelude&gt; :sprint a
a = Just &quot;a&quot;
Prelude&gt; let a = Just &quot;a&quot;
Prelude&gt; :sprint a
a = Just _
So uh, what gives? Well, the deal is that the strictness analy-
sis driven optimization GHC performs here is limited to data
constructors only, no computation! But where’s the function
you ask? Well if we turn on our night vision goggles…
Prelude&gt; let a = Just ['a']
returnIO
(: ((Just (: (C# 'a') ([])))</p>
<p>CHAPTER 27. NONSTRICTNESS 1670
<code>cast</code> ...) ([]))
Prelude&gt; let a = Just &quot;a&quot;
returnIO
(: ((Just (unpackCString# &quot;a&quot;#))
<code>cast</code> ...) ([]))
The issue is that a call to a primitive function in GHC.Base
interposes between Justand aCString literal. The reason string
literals aren’t actually lists of characters at time of construction
is mostly to present optimization opportunities, such as when
we convert string literals into ByteString orTextvalues. More
on that in the next chapter!
What subverts or prevents sharing
Sometimes we don’t want sharing. Sometimes we want to
know why sharing didn’t happen when we did want it. Un-
derstanding what kinds of things prevent sharing is therefore
useful.
Inlining expressions where they get used prevents sharing
because it creates independent thunks that will get computed
separately. In this example, instead of declaring the value of 𝑓
to equal 1, we make it a function:
Prelude&gt; :{</p>
<p>CHAPTER 27. NONSTRICTNESS 1671
Prelude| let f :: a -&gt; Int
Prelude| f _ = trace &quot;f&quot; 1
Prelude| :}
Prelude&gt; f 'a'
f
1
Prelude&gt; f 'a'
f
1
In the next examples you can directly compare the dif-
ference between assigning a name to the value of (2 + 2) versus
inlining it directly. When it’s named, it gets shared and not
re-evaluated:
Prelude&gt; let a :: Int; a = trace &quot;a&quot; 2 + 2
Prelude&gt; let b = (a + a)
Prelude&gt; b
a
8
Prelude&gt; b
8
Here we saw 𝑎once, which makes sense as we expect the
result to get shared.
Prelude&gt; :{</p>
<p>CHAPTER 27. NONSTRICTNESS 1672
Prelude| let c :: Int;
Prelude| c = (trace &quot;a&quot; 2 + 2)
Prelude| + (trace &quot;a&quot; 2 + 2)
Prelude| :}
Prelude&gt; c
a
a
8
Prelude&gt; c
8
Here an expression equivalent to 𝑎didn’t get shared be-
cause the two occurrences of the expression weren’t bound
to the same name. This is a trivial example of inlining. This
illustrates the diﬀerence in how things evaluate when an ex-
pression is bound to a name versus when it gets repeated via
inlining in an expression.
Being a function with explicit, named arguments also pre-
vents sharing. Haskell is not fully lazy; it is merely nonstrict,
so it is not required to remember the result of every func-
tion application for a given set of arguments, nor would it be
desirable given memory constraints. A demonstration:
Prelude&gt; :{
Prelude| let f :: a -&gt; Int
Prelude| f = trace &quot;f&quot; const 1</p>
<p>CHAPTER 27. NONSTRICTNESS 1673
Prelude| :}
Prelude&gt; f 'a'
f
1
Prelude&gt; f 'a'
1
Prelude&gt; f 'b'
1
The explicit, named arguments part here is critical! Eta re-
duction (i.e., writing pointfree code, thus dropping the named
arguments) will change the sharing properties of your code.
This will be explained in more detail in the next chapter.
Typeclass constraints also prevent sharing. If we forget to
add a concrete type to an earlier example, we evaluate 𝑎twice:
Prelude&gt; let blah = Just 1
Prelude&gt; fmap ((+1) :: Int -&gt; Int) blah
Just 2
Prelude&gt; :sprint blah
blah = _
Prelude&gt; :t blah
blah :: Num a =&gt; Maybe a
Prelude&gt; let bl = Just 1
Prelude&gt; :t bl</p>
<p>CHAPTER 27. NONSTRICTNESS 1674
bl :: Num a =&gt; Maybe a
Prelude&gt; :sprint bl
bl = _
Prelude&gt; fmap (+1) bl
Just 2
Prelude&gt; let fm = fmap (+1) bl
Prelude&gt; :t fm
fm :: Num b =&gt; Maybe b
Prelude&gt; :sprint fm
fm = _
Prelude&gt; fm
Just 2
Prelude&gt; :sprint fm
fm = _
Prelude&gt; :{
Prelude| let fm' =
Prelude| fmap ((+1) :: Int -&gt; Int) bla
Prelude| :}
Prelude&gt; fm'
Just eval'd 1
2
Prelude&gt; :sprint fm'
fm' = Just 2</p>
<p>CHAPTER 27. NONSTRICTNESS 1675
Again, that’s because typeclass constraints are a function
in Core. They are awaiting application to something that will
make them become concrete types. We’re going to go into a
bit more detail on this in the next section.
Implicit parameters are implemented similarly to type-
class constraints and have the same eﬀect on sharing. Sharing
doesn’t work in the presence of constraints (typeclasses or im-
plicit parameters) because typeclass constraints and implicit
parameters decay into function arguments when the compiler
simplifies the code:
Prelude&gt; :set -XImplicitParams
Prelude&gt; import Debug.Trace
Prelude&gt; :{
Prelude| let add :: (?x :: Int) =&gt; Int
Prelude| add = trace &quot;add&quot; 1 + ?x
Prelude| :}
Prelude&gt; let ?x = 1 in add
add
2
Prelude&gt; let ?x = 1 in add
add
2
We won’t talk about implicit parameters too much more as
wedon’tthinkthey’reagoodideaforgeneraluse. Inmostcases</p>
<p>CHAPTER 27. NONSTRICTNESS 1676
where you believe you want implicit parameters, more likely
you want Reader ,ReaderT , or a plain old function argument.
Why polymorphic values never seem to get
forced
As we’ve said, GHC engages in opportunistic strictness when it
can do so safely without making an otherwise valid expression
result in bottom. This is one of the things that confounds the
use ofsprint to observe evaluation in GHCi — GHC will often
be opportunistically strict with data constructors if it knows
the contents definitely can’t be a bottom, such as when they’re
a literal value. It gets more complicated when we consider
that, under the hood, typeclass constraints are simplified into
additional arguments.
Reusing a similar example from earlier we will first observe
this in action, then we’ll talk about why it happens:
Prelude&gt; :{
Prelude| let blah =
Prelude| Just (trace &quot;eval'd 1&quot; 1)
Prelude| :}
Prelude&gt; :sprint blah
blah = _
Prelude&gt; :t blah
blah :: Num a =&gt; Maybe a</p>
<p>CHAPTER 27. NONSTRICTNESS 1677
Prelude&gt; fmap (+1) blah
Just eval'd 1
2
Prelude&gt; fmap (+1) blah
Just eval'd 1
2
Prelude&gt; :sprint blah
blah = _
So we have at least some evidence that we’re re-evaluating.
Does it change when it’s concrete?
Prelude&gt; :{
Prelude| let blah =
Prelude| Just (trace &quot;eval'd 1&quot;
Prelude| (1 :: Int))
Prelude| :}
Prelude&gt; :sprint blah
blah = Just _
TheIntvalue being obscured by traceprevented oppor-
tunistic evaluation there. However, eliding the Num a =&gt; a in
favor of a concrete type does bring sharing back:
Prelude&gt; fmap (+1) blah
Just eval'd 1
2</p>
<p>CHAPTER 27. NONSTRICTNESS 1678
Prelude&gt; fmap (+1) blah
Just 2
Now our trace gets emitted only once. The idea here is that
after the typeclass constraints get simplified to the underlying
GHC Core language, they’re really function arguments.
It doesn’t matter if you use a function that accepts a concrete
type and forces the Num a =&gt; a , it’ll re-do the work on each
evaluation because of the typeclass constraint. For example:
Prelude&gt; fmap ((+1) :: Int -&gt; Int) blah
Just 2
Prelude&gt; :sprint blah
blah = _
Prelude&gt; :t blah
blah :: Num a =&gt; Maybe a
Prelude&gt; let bl = Just 1
Prelude&gt; :t bl
bl :: Num a =&gt; Maybe a
Prelude&gt; :sprint bl
bl = _
Prelude&gt; fmap (+1) bl
Just 2
Prelude&gt; let fm = fmap (+1) bl
Prelude&gt; :t fm
fm :: Num b =&gt; Maybe b</p>
<p>CHAPTER 27. NONSTRICTNESS 1679
Prelude&gt; :sprint fm
fm = _
Prelude&gt; fm
Just 2
Prelude&gt; :sprint fm
fm = _
Prelude&gt; :{
Prelude| let fm' =
Prelude| fmap ((+1) :: Int -&gt; Int)
Prelude| blah
Prelude| :}
Prelude&gt; fm'
Just eval'd 1
2
Prelude&gt; :sprint fm'
fm' = Just 2
So, what’s the deal here with the typeclass constraints? It’s as
ifNum a =&gt; a were really Num a -&gt; a . In Core, they are. The only
way to apply that function argument is to reach an expression
that provides a concrete type satisfying the constraint. Here’s
a demonstration of the diﬀerence in behavior using values:
Prelude&gt; let poly = 1
Prelude&gt; let conc = poly :: Int
Prelude&gt; :sprint poly</p>
<p>CHAPTER 27. NONSTRICTNESS 1680
poly = _
Prelude&gt; :sprint conc
conc = _
Prelude&gt; poly
1
Prelude&gt; conc
1
Prelude&gt; :sprint poly
poly = _
Prelude&gt; :sprint conc
conc = 1
Num a =&gt; a is a function awaiting an argument, while Intis
not. Behold the Core:
moduleBlahwhere
a::Numa=&gt;a
a=1
concrete ::Int
concrete =1
Prelude&gt; :l code/blah.hs
[1 of 1] Compiling Blah</p>
<p>CHAPTER 27. NONSTRICTNESS 1681
================ Tidy Core ==============
Result size of Tidy Core =
{terms: 9, types: 9, coercions: 0}
concrete
concrete = I# 1
a
a =
\ @ a1_aRN $dNum_aRP -&gt;
fromInteger $dNum_aRP (__integer 1)
Do you see how 𝑎has a lambda? In order to know what
instance of the typeclass to deploy at any given time, the type
has to be concrete. As we’ve seen, types can become concrete
through assignment or type defaulting. Whichever way it
becomes concrete, the result is the same: once the concrete
type is known, the typeclass constraint function gets applied
to the typeclass instance for that type. If you don’t declare the
concrete type, it will have to re-evaluate this function every
time, because it can’t know that the type didn’t change some-
where along the way. So, because it remains a function and
unapplied functions are not shareable values, polymorphic
expressions can’t be shared.
Mostly the behavior doesn’t change when it involves values
defined in terms of functions, but if you forget the type con-</p>
<p>CHAPTER 27. NONSTRICTNESS 1682
cretion it’ll stay _and you’ll be confused and upset. Observe:
Prelude&gt; :{
Prelude| let blah :: Int -&gt; Int
Prelude| blah x = x + 1
Prelude| :}
Prelude&gt; let woot = blah 1
Prelude&gt; :sprint blah
blah = _
Prelude&gt; :sprint woot
woot = _
Prelude&gt; woot
2
Prelude&gt; :sprint woot
woot = 2
Values of a concrete, constant type can be shared, once
evaluated. Polymorphic values may be evaluated once but still
not shared because, underneath, they continue to be functions
awaiting application.
Preventing sharing on purpose
When do we want to prevent sharing? When we don’t want
a large datum hanging out in memory that was calculated to
provide a much smaller answer. First an example that demon-
strates sharing:</p>
<p>CHAPTER 27. NONSTRICTNESS 1683
Prelude&gt; import Debug.Trace
Prelude&gt; let f x = x + x
Prelude&gt; f (trace &quot;hi&quot; 2)
hi
4
We see “hi” once because 𝑥got evaluated once. In the next
example, 𝑥gets evaluated twice:
Prelude&gt; let f x = (x ()) + (x ())
Prelude&gt; f (_ -&gt; trace &quot;hi&quot; 2)
hi
hi
4
Using unit ()as arguments to 𝑥turned𝑥into a very trivial,
weird-looking function, which is why the value of 𝑥can no
longer be shared. It doesn’t matter much since that “function”
𝑥doesn’t really do anything.
OK, that was weird; maybe it’ll be easier to see if we use
some more traditional-seeming argument to 𝑥:
Prelude&gt; let f x = (x 2) + (x 10)
Prelude&gt; f (\x -&gt; trace &quot;hi&quot; (x + 1))
hi
hi
14</p>
<p>CHAPTER 27. NONSTRICTNESS 1684
Using a lambda that mentions the argument in some fash-
ion disables sharing:
Prelude&gt; let g = _ -&gt; trace &quot;hi&quot; 2
Prelude&gt; f g
hi
hi
4
However, this worked in part because the function passed
to𝑓had the argument as part of the declaration, even though
it used underscore to ignore it. Notice what happens if we
make it pointfree:
Prelude&gt; let g = const (trace &quot;hi&quot; 2)
Prelude&gt; f g
hi
4
We’re going to get into a little more detail about this dis-
tinction in the next chapter, but the idea here is that functions
aren’t shared when there are named arguments but are when
the arguments are elided, as in pointfree. So, one way to pre-
vent sharing is adding named arguments.</p>
<p>CHAPTER 27. NONSTRICTNESS 1685
Forcing sharing
You can force sharing by giving your expression a name. The
most common way of doing this is with let.
-- calculates 1 + 1 twice
(1+1)<em>(1+1)
-- shares 1 + 1 result under 'x'
letx=1+1
inx</em>x
With that in mind, if you take a look at the forever function
inControl.Monad , you might see something a little mysterious
looking:
forever ::(Monadm)=&gt;m a-&gt;m b
forever a= leta'=a&gt;&gt;a'ina'
Why the letexpression? Well, we want sharing here so that
running a monadic action indefinitely doesn’t leak memory.
The sharing here causes GHC to overwrite the thunk as it runs
each step in the evaluation, which is quite handy. Otherwise,
it would keep constructing new thunks indefinitely and that
would be very unfortunate.</p>
<p>CHAPTER 27. NONSTRICTNESS 1686
27.10 Refutable and irrefutable patterns
When we’re talking about pattern matching, it’s important to
be aware that there are refutable and irrefutable patterns. An
irrefutable pattern is one which will never fail to match. A
refutable pattern is one which has potential failures. Often,
the problem is one of specificity.
refutable ::Bool-&gt;Bool
refutable True=False
refutable False=True
irrefutable ::Bool-&gt;Bool
irrefutable x=not x
oneOfEach ::Bool-&gt;Bool
oneOfEach True=False
oneOfEach _ =True
Remember, the pattern is refutable or not, not the function
itself. The function refutable is refutable because each case is
refutable; each case could be given an input that fails to match.
In contrast, irrefutable has an irrefutable pattern; that is, its
pattern doesn’t rely on matching with a specific value.
In the case of oneOfEach , the first pattern is refutable because
it pattern matches on the Truedata constructor. irrefutable</p>
<p>CHAPTER 27. NONSTRICTNESS 1687
and the second match of oneOfEach are irrefutable because they
don’t need to look inside the data they are applied to.
That said, the second pattern match of oneOfEach being ir-
refutable isn’t terribly semantically meaningful as Haskell will
have to inspect the data to see if it matches the first case any-
way.
Theirrefutable function works for any inhabitant (all two
of them) of Boolbecause it doesn’t specify which Boolvalue in
the pattern to match. You could think of an irrefutable pattern
as one which will never fail to match. If an irrefutable pattern
for a particular value comes before a refutable pattern, the
refutable pattern will never get invoked.
This little function appeared in an earlier chapter, but we’ll
bring it back for a quick demonstration:
isItTwo ::Integer -&gt;Bool
isItTwo 2=True
isItTwo _ =False
In the case of Bool, the order of matching TrueandFalse
specifically doesn’t matter, but in cases like isItTwo where one
case is specific and the other is a catchall otherwise case, the
ordering will certainly matter. You can reorder the expressions
ofisItTwo to see what happens, although it’s probably clear.</p>
<p>CHAPTER 27. NONSTRICTNESS 1688
Lazy patterns
Lazy patterns are also irrefutable.
strictPattern ::(a, b)-&gt;String
strictPattern (a,b)=const&quot;Cousin It&quot; a
lazyPattern ::(a, b)-&gt;String
lazyPattern ~(a,b)=const&quot;Cousin It&quot; a
The tilde is how one makes a pattern match lazy. A caveat
is that since it makes the pattern irrefutable, you can’t use
it to discriminate cases of a sum — it’s useful for unpacking
products that might not get used.
Prelude&gt; strictPattern undefined
*** Exception: Prelude.undefined
Prelude&gt; lazyPattern undefined
&quot;Cousin It&quot;
And as we see here, in the lazy pattern version since const
didn’t actually need 𝑎from the tuple, we never forced the
bottom. The default behavior is to just go ahead and force
it before evaluating the function body, mostly for more pre-
dictable memory usage and performance.</p>
<p>CHAPTER 27. NONSTRICTNESS 1689
27.11 Bang patterns
Sometimes we want to evaluate an argument to a function
whether we use it or not. We can do this with seqas in the
following example:
{-# LANGUAGE BangPatterns #-}
moduleManualBang where
doesntEval ::Bool-&gt;Int
doesntEval b=1
manualSeq ::Bool-&gt;Int
manualSeq b=b <code>seq</code> 1
Or we can also do it with a bang pattern on 𝑏— note the
exclamation point:
banging ::Bool-&gt;Int
banging !b=1
Let’s look at the Core for those three:</p>
<p>CHAPTER 27. NONSTRICTNESS 1690
doesntEval
doesntEval =
_ -&gt;I#1#
manualSeq
manualSeq =
\b_a1ia-&gt;
caseb_a1iaof _
{ __DEFAULT -&gt;I#1#}
banging
banging =
\b_a1ib-&gt;
caseb_a1ibof _
{ __DEFAULT -&gt;I#1#}
If you try passing bottom to each function you’ll find that
manualSeq and banging are forcing their argument despite not
using it for anything. Remember that forcing something is
expressed in Core as a case expression and that case evaluates
up to weak head normal form in Core.
Bang patterns in data
When we evaluate the outer data constructor of a datatype,
at times we’d also like to evaluate the contents to weak head</p>
<p>CHAPTER 27. NONSTRICTNESS 1691
normal form just like with functions.
One way to see the diﬀerence between strict and nonstrict
constructor arguments is how they behave when they are un-
defined. Let’s look at an example (note the exclamation mark):
dataFoo=FooInt!Int
first(Foox_)=x
second(Foo_y)=y
Since the nonstrict argument isn’t evaluated by second , pass-
ing in undefined doesn’t cause a problem:</p>
<blockquote>
<p>second (Foo undefined 1)
1
But the strict argument can’t be undefined, even if we don’t
use the value:
first (Foo 1 undefined)
*** Exception: Prelude.undefined
You could do this manually with seq, but it’s a little tedious.
Here’s another example with two equivalent datatypes, one
of them with strictness annotations on the contents and one
without:</p>
</blockquote>
<p>CHAPTER 27. NONSTRICTNESS 1692
{-# LANGUAGE BangPatterns #-}
moduleManualBang where
dataDoesntForce =
TisLazy IntString
gibString ::DoesntForce -&gt;String
gibString (TisLazy _s)=s
-- note the exclamation marks again
dataBangBang =
SheShotMeDown !Int!String
gimmeString ::BangBang -&gt;String
gimmeString (SheShotMeDown _s)=s
Then testing those in GHCi:
Prelude&gt; let x = TisLazy undefined &quot;blah&quot;
Prelude&gt; gibString x
&quot;blah&quot;
Prelude&gt; let s = SheShotMeDown
Prelude&gt; let x = s undefined &quot;blah&quot;
Prelude&gt; gimmeString x
&quot;*** Exception: Prelude.undefined</p>
<p>CHAPTER 27. NONSTRICTNESS 1693
The idea here is that in some cases, it’s cheaper to just com-
pute something than to construct a thunk and then evaluate
it later. This case is particularly common in numerics code
where you have a lot of IntandDouble values running around
which are individually cheap to conjure. If the values are both
cheap to compute and small, then you may as well make them
strict unless you’re trying to dance around bottoms. Types
with underlying primitive representations IntandDouble most
assuredly qualify as small.
A good rule to follow is lazy in the spine, strict in the leaves!
Sometimes a “leak” isn’t really a leak but temporarily exces-
sive memory that subsides because you made 1,000,000 tiny
values into less-tiny thunks when you could’ve just computed
them as your algorithm progressed.
27.12 Strict and StrictData
If you’re using GHC 8.0 or newer, you can avail yourself of the
Strict andStrictData extensions. The key thing to realize is
Strict /StrictData are just letting you avoid putting in pervasive
uses of seqand bang patterns yourself. They don’t add any-
thing to the semantics of the language. Accordingly, it won’t
suddenly make lazy data structures defined elsewhere behave
diﬀerently, although it does make functions defined in that
module processing lazy data structures behave diﬀerently.</p>
<p>CHAPTER 27. NONSTRICTNESS 1694
Let’s play with that (if you have GHC 8.0 or newer; if not,
this code won’t work):
{-# LANGUAGE Strict #-}
moduleStrictTest where
blahx=1
main=print (blah undefined)
The above will bottom out because blahis defined under
the module with the Strict extension and will get translated
into the following:
blahx=x <code>seq</code> 1
-- or with bang patterns
blah!x=1
So, the Strict andStrictData extensions are a means of
avoiding noise when everything or almost everything in a
module is supposed to be strict. You can use the tilde for ir-
refutable patterns to recover laziness on a case by case basis:</p>
<p>CHAPTER 27. NONSTRICTNESS 1695
{-# LANGUAGE Strict #-}
moduleLazyInHostileTerritory where
willForce x=1
willNotForce ~x=1
Admittedlytheseareglorifiedrenamesof const, butitdoesn’t
matterforthepurposesofdemonstratingwhathappens. Here’s
what we’ll see in GHCi when we pass them bottom:
Prelude&gt; willForce undefined
*** Exception: Prelude.undefined
Prelude&gt; willNotForce undefined
1
So even when you’re using the Strict extension, you can
selectively recover laziness when desired.
27.13 Adding strictness
Now we shall examine how applying strictness to a datatype
and operations we’re already familiar with can change how
they behave in the presence of bottom through the list type.
This is intended to be mostly demonstrative rather than a
practical example.</p>
<p>CHAPTER 27. NONSTRICTNESS 1696
moduleStrictTest1 where
dataLista=
Nil
|Consa (Lista)deriving Show
sTake::Int-&gt;Lista-&gt;Lista
sTaken_
|n&lt;=0=Nil
sTakenNil=Nil
sTaken (Consx xs)=
(Consx (sTake (n -1) xs))
twoEls =Cons1(Consundefined Nil)
oneEl =sTake1twoEls
The name of the module here is a bit of a misnomer. List
here is lazy, just like the built-in [a]in the Haskell prelude.
Ourtakederivative named sTakeis lazy too.
Now let’s load up this code in our REPL and test it out:
Prelude&gt; twoEls
Cons 1 (Cons
*** Exception: Prelude.undefined
Prelude&gt; oneEl</p>
<p>CHAPTER 27. NONSTRICTNESS 1697
Cons 1 Nil
Now let’s experiment with adding strictness to diﬀerent
parts of our program and observe what changes in our code’s
behavior.
First we’re going to add BangPatterns so that we have a syn-
tactically convenient way to denote when and where we want
strictness:
moduleStrictTest2 where
dataLista=
Nil
|Cons!a (Lista)deriving Show
sTake::Int-&gt;Lista-&gt;Lista
sTaken_
|n&lt;=0=Nil
sTakenNil=Nil
sTaken (Consx xs)=
(Consx (sTake (n -1) xs))
twoEls=Cons1(Consundefined Nil)
oneEl=sTake1twoEls</p>
<p>CHAPTER 27. NONSTRICTNESS 1698
Noting the placement of the exclamation marks denoting
strictness, let’s run it in GHCi and see if it does what we want:
Prelude&gt; twoEls
Cons 1 *** Exception: Prelude.undefined
Prelude&gt; oneEl
Cons 1 Nil</p>
<p>CHAPTER 27. NONSTRICTNESS 1699
{-# LANGUAGE BangPatterns #-}
moduleStrictTest3 where
dataLista=
Nil
|Cons!a (Lista)deriving Show
sTake::Int-&gt;Lista-&gt;Lista
sTaken_
|n&lt;=0=Nil
sTakenNil=Nil
sTaken (Consx!xs)=
(Consx (sTake (n -1) xs))
twoEls=Cons1(Consundefined Nil)
oneEl=sTake1twoEls
threeElements =Cons2twoEls
oneElT =sTake1threeElements
We added strictness to the 𝑥𝑠so thatsTakeis going to force
more of the list. Let’s see what happens:
Prelude&gt; twoEls
Cons 1 *** Exception: Prelude.undefined</p>
<p>CHAPTER 27. NONSTRICTNESS 1700
Prelude&gt; oneEl
*** Exception: Prelude.undefined
Prelude&gt; threeElements
Cons 2 (Cons 1
*** Exception: Prelude.undefined
Prelude&gt; oneElT
Cons 2 Nil
Let’s add more strictness:</p>
<p>CHAPTER 27. NONSTRICTNESS 1701
moduleStrictTest4 where
dataLista=
Nil
|Cons!a!(Lista)deriving Show
sTake::Int-&gt;Lista-&gt;Lista
sTaken_
|n&lt;=0=Nil
sTakenNil=Nil
sTaken (Consx xs)=
(Consx (sTake (n -1) xs))
twoEls=Cons1(Consundefined Nil)
oneEl=sTake1twoEls
And run it again:
Prelude&gt; twoEls
*** Exception: Prelude.undefined
Prelude&gt; oneEl
*** Exception: Prelude.undefined
So, what’s the upshot of our experiments with adding strict-
ness here?</p>
<p>CHAPTER 27. NONSTRICTNESS 1702
NCons sTake
1Cons a (List a) Cons x xs
2Cons !a (List a) Cons x xs
3Cons !a (List a) Cons x !xs
4Cons !a !(List a) Cons x xs
Then the results themselves:
NtwoEls oneEl
1Cons 1 (Cons *** Cons 1 Nil
2Cons 1 *** Cons 1 Nil
3Cons 1 *** ***
4*** ***
You can see clearly what adding strictness in diﬀerent places
does to our evaluation in terms of bottom.
27.14 Chapter Exercises
Strict List
Try messing around with the following list type and compare
what it does with the bang-patterned list variants we experi-
mented with earlier:</p>
<p>CHAPTER 27. NONSTRICTNESS 1703
{-# LANGUAGE Strict #-}
moduleStrictList where
dataLista=
Nil|
Consa (Lista)
deriving (Show)
take'n_ | n&lt;=0=Nil
take'_Nil =Nil
take'n (Consx xs) =
(Consx (take' (n -1) xs))
map'_Nil =Nil
map'f (Consx xs)=
(Cons(f x) (map' f xs))
repeat' x=xswherexs=(Consx xs)
main= do
print$take'10$map' (+1) (repeat' 1)</p>
<p>CHAPTER 27. NONSTRICTNESS 1704
What will :sprint output?
We show you a definition or multiple definitions, you deter-
mine what :sprint will output when passed the bindings listed
in your head before testing it.
1.letx=1
2.letx=['1']
3.letx=[1]
4.letx=1::Int
5.letf=\x-&gt;x
letx=f1
6.letf::Int-&gt;Int; f=\x-&gt;x
letx=f1
Will printing this expression result in bottom?
1.snd(undefined, 1)
2.letx=undefined
lety=x <code>seq</code> 1insnd (x, y)
3.length$[1..5]++undefined</p>
<p>CHAPTER 27. NONSTRICTNESS 1705
4.length$[1..5]++[undefined]
5.const1undefined
6.const1(undefined <code>seq</code> 1)
7.constundefined 1
Make the expression bottom
Using only bang patterns or seq, make the code bottom out
when executed.
1.x=undefined
y=&quot;blah&quot;
main= do
print (snd (x, y))
27.15 Follow-up resources
1.The Incomplete Guide to Lazy Evaluation (in Haskell);
Heinrich Apfelmus
https://hackhands.com/guide-lazy-evaluation-haskell/
2.Chapter 2. Basic Parallelism: The Eval Monad; Parallel
and Concurrent Programming in Haskell; Simon Marlow;
http://chimera.labs.oreilly.com/books/1230000000929/ch02.html</p>
<p>CHAPTER 27. NONSTRICTNESS 1706
3.Lazy evaluation illustrated for Haskell divers; Takenobu
Tani
4.A Natural Semantics for Lazy Evaluation; John Launch-
bury
5.AnOperationalSemanticsforParallelCall-by-Need; Clem
Baker-Finch, David King, Jon Hall and Phil Trinder</p>
<p>Chapter 28
Basic libraries
Bad programmers worry
about the code. Good
programmers worry
about data structures and
their relationships.
Linus Torvalds
1707</p>
<p>CHAPTER 28. BASIC LIBRARIES 1708
28.1 Basic libraries and data structures
Data structures are kind of important. Insofar as computers
are fast, they aren’t getting much faster — at least, the CPU
isn’t. This is usually a lead-in for a parallelism/concurrency
sales pitch. But this isn’t that book.
The data structures you choose to represent your problem
aﬀect the speed and memory involved in processing your data,
perhaps to a larger extent than is immediately obvious. At the
level of your program, making the right decision about how
to represent your data is the first important step to writing
efficient programs. In fact, your choice of data structure can
aﬀect whether it’s worthwhile or even makes sense to attempt
to parallelize something.
This chapter is here to help you make the decision of the
optimal data structures for your programs. We can’t prescribe
one or the other of similar data structures because how ef-
fective they are will depend a lot on what you’re trying to
do. So, our first step will be to give you tools to measure for
yourself how diﬀerent structures will perform in your context.
We’ll also cover some of the mistakes that can cause your
memory usage and execution time to explode.
This chapter will
•demonstrate how to measure the usage of time and space
in your programs;</p>
<p>CHAPTER 28. BASIC LIBRARIES 1709
•oﬀer guidelines on when weak head normal form or nor-
mal form are appropriate when benchmarking code;
•define constant applicative forms and explain argument
saturation;
•demonstrate and critically evaluate when to use diﬀerent
data structures in diﬀerent circumstances;
•sacrifice some jargon for the jargon gods.
We’re going to kick this chapter oﬀ with some benchmark-
ing.
28.2 Benchmarking with Criterion
It’s a common enough thing to want to know how fast our
code is. If you can’t benchmark properly, then you can’t know
if you used six microseconds or only five, and can only ask
yourself, “Well, do I feel lucky?”
Well, do ya, punk?
If you’d rather not trust your performance to guesswork,
the best way to measure performance is to sample many times
in order to establish a confidence interval. Fortunately, that
work has already been done for us in the wonderful library
criterion1by Bryan O’Sullivan.
1http://hackage.haskell.org/package/criterion</p>
<p>CHAPTER 28. BASIC LIBRARIES 1710
As it happens, criterion comes with a pretty nice tutorial2,
but we’ll still work through an example so you can follow along
with this chapter. In our toy program here, we’re looking to
write a total version of (!!)which returns Maybeto make the
bottoms unnecessary. When you compile code for bench-
marking, make sure you’re using -Oor-O2in the build flags to
GHC. Those can be specified by running GHC manually:
-- with stack
$ stack ghc -- -O2 bench.hs
-- without stack
$ ghc -O2 bench.hs
Or via the Cabal setting ghc-options3.
Let’s get our module set up:
2http://www.serpentine.com/criterion/tutorial.html
3https://www.haskell.org/cabal/users-guide/</p>
<p>CHAPTER 28. BASIC LIBRARIES 1711
moduleMainwhere
importCriterion.Main
infixl9!?
_ !? n|n&lt;0=Nothing
[]!? _ = Nothing
(x:<em>)!?0 =Justx
(</em>:xs)!?n =xs!?(n-1)
myList::[Int]
myList=[1..9999]
main::IO()
main=defaultMain
[ bench &quot;index list 9999&quot;
$whnf (myList !!)9998
, bench &quot;index list maybe index 9999&quot;
$whnf (myList !?)9998
]
Our version of (!!)shouldn’t have anything too surprising
going on. We have declared that it’s a left-associating infix op-
erator (infixl ) with a precedence of 9. We haven’t talked much
about the associativity or fixity of operators since Chapter 2.</p>
<p>CHAPTER 28. BASIC LIBRARIES 1712
This is the same associativity and precedence as the normal
(!!)operator in base.
Criterion.Main is the convenience module to import from
criterion if you’re running benchmarks in a Mainmodule. Usu-
ally you’ll have a benchmark stanza in your Cabal file that be-
haves like an executable. It’s also possible to do it as a one-oﬀ
using Stack:
$ stack build criterion
$ stack ghc -- -O2 benchIndex.hs
$ ./benchIndex
Heremainuses a function from criterion calledwhnf. The
functions whnfandnf(also in criterion ), as you might guess,
refer to weak head normal form and normal form, respectively.
Weak head normal form, as we said before, evaluates to the
first data constructor. That means that if your outermost data
constructor is a Maybe, it’s only going to evaluate enough to find
out if it’s a Nothing or aJust— if there is a Just a , it won’t count
the cost of evaluating the 𝑎value.
Usingnfwould mean you wanted to include the cost of
fully evaluating the 𝑎as well as the first data constructor. The
key when determining whether you want whnfornfis to think
about what you’re trying to benchmark and if reaching the first
data constructor will do all the work you’re trying to measure
or not. We’ll talk more about what the diﬀerence is here and
how to decide which you need in a bit.</p>
<p>CHAPTER 28. BASIC LIBRARIES 1713
In our case, what we want is to compare two things: the
weak head normal form evaluation of the original indexing
operator and that of our safe version, applied to the same long
list. We only need weak head normal form because (!!)and
(!?)don’t return a data constructor until they’ve done the
work already, as we can see by taking a look at the first three
cases:
_ !? n|n&lt;0=Nothing
[]!? _ = Nothing
(x:<em>)!?0 =Justx
These first three cases aren’t reached until you’ve gone
through the list as far as you’re going to go. The recursive case
below doesn’t return a data constructor. Instead, it invokes
itself repeatedly until one of the above cases is reached. Eval-
uating to WHNF cannot and does not pause in a self-invoked
recursive case like this:
(</em>:xs)!?n =xs!?(n-1)
-- Self function call,
-- not yet in weak head.
When evaluated to weak head normal form the above will
continue until it reaches the index, you reach the element, or
you hit the end of the list. Let us consider an example:</p>
<p>CHAPTER 28. BASIC LIBRARIES 1714
[1,2,3]!?2
-- matches final case
(<em>:[2,3])!?2
=[2,3]!?(2-1)
-- not a data constructor, keep going
[2,3]!?1
-- matches final case
(</em>:[3])!?1
=[3]!?(1-1)
-- not a data constructor, keep going
[3]!?0
-- matches Just case
(x:[])!?0=Justx
-- We stop at Just
In the above, we happen to know 𝑥is 3, but it’ll get thunked
if it wasn’t opportunistically evaluated on construction of the
list.
Next, let’s look at the types of the following functions:</p>
<p>CHAPTER 28. BASIC LIBRARIES 1715
defaultMain ::[Benchmark ]-&gt;IO()
whnf::(a-&gt;b)-&gt;a-&gt;Benchmarkable
nf::Control.DeepSeq.NFDatab=&gt;
(a-&gt;b)-&gt;a-&gt;Benchmarkable
The reason it wants a function it can apply an argument
to is so that the result isn’t shared, which we discussed in the
previous chapter. We want it to re-perform the work for each
sampling in the benchmark results, so this design prevents
that sharing. Keep in mind that if you want to use your own
datatype with nf, which has an NFData constraint you will need
to provide your own instance. You can find examples in the
deepseq library on Hackage.
Our goal with this example is to equalize the performance
diﬀerence between (!?)and(!!). In this case, we’ve derived
the implementation of (!?)from the Report version of (!!).
Here’s how it looks in base:</p>
<p>CHAPTER 28. BASIC LIBRARIES 1716
-- Go to the Data.List docs in <code>base</code>,
-- click the source link for (!!)
#ifdefUSE_REPORT_PRELUDE
xs!!n|n&lt;0=
error&quot;Prelude.!!: negative index&quot;
[]!! _ =
error&quot;Prelude.!!: index too large&quot;
(x:<em>)!!0=x
(</em>:xs)!!n=xs!!(n-1)
#else
However, after you run the benchmarks, you’ll find our
version based on the above isn’t quite as fast.4Fair enough! It
turns out that most of the time when there’s a Report version as
well as a non-Report version of a function in base, it’s because
they found a way to optimize it and make it faster. If we look
down from the #else, we can find the version that replaced it:
4Note that if you get weird benchmark results, you’ll want to resort to the old pro-
grammer’s trick of wiping your build. With Stack you’d run stack clean , with Cabal it’d
becabal clean . Inexplicable things happen sometimes. You shouldn’t need to do this
regularly, though.</p>
<p>CHAPTER 28. BASIC LIBRARIES 1717
-- negIndex and tooLarge are a bottom
-- and a const bottom respectively.
{-# INLINABLE (!!) #-}
xs!!n
|n&lt;0=negIndex
|otherwise =
foldr
(\x r k-&gt; case kof
0-&gt;x
_ -&gt;r (k-1))
tooLarge xs n
The non-Report version is written in terms of foldr, which
often benefits from the various rewrite rules and optimizations
attached to foldr— rules we will not be explaining here at all,
sorry. This version also has a pragma letting GHC know it’s
okay to inline the code of the function where it’s used when
the cost estimator thinks it’s worthwhile to do so. So, let’s
change our version of this operator to match this version to
make use of those same optimizations:</p>
<p>CHAPTER 28. BASIC LIBRARIES 1718
infixl9!?
{-# INLINABLE (!?) #-}
xs!?n
|n&lt;0=Nothing
|otherwise =
foldr
(\x r k-&gt;
casekof
0-&gt;Justx
_ -&gt;r (k-1))
(constNothing) xs n
If you run this, you’ll find that…things have not improved.
So, what can we do to improve the performance of our opera-
tor?
Well, unless you added one already, you’ll notice the type
signature is missing. If you add a declaration that the number
argument is an Int, it should now perform the same as the
original:</p>
<p>CHAPTER 28. BASIC LIBRARIES 1719
infixl9!?
{-# INLINABLE (!?) #-}
(!?)::[a]-&gt;Int-&gt;Maybea
xs!?n
|n&lt;0=Nothing
|otherwise =
foldr
(\x r k-&gt;
casekof
0-&gt;Justx
_ -&gt;r (k-1))
(constNothing) xs n
Change the function in your module to reflect this and run
the benchmark again to check.
The issue was that the version with an inferred type was
defaulting the Num a =&gt; a toInteger which compiles to a less
efficient version of this code than does one that specifies the
typeInt. TheIntversion will turn into a more primitive, faster
loop. You can verify this for yourself by specifying the type
Integer and re-running the code or comparing the GHC Core
output for each version.</p>
<p>CHAPTER 28. BASIC LIBRARIES 1720
More on whnf and nf
Let’s return now to the question of when we should use whnf
ornf. You want to use whnfwhen the first data constructor is a
meaningful indicator of whether the work you’re interested in
has been done. Consider the simplistic example of a program
that is meant to locate some data in a database, say, a person’s
name and whether there are any known addresses for that
person. If it finds any data, it might print that information
into a file.
Thepartyou’reprobablytryingtojudgetheperformanceof
is the lookup function that finds the data and assesses whether
it exists, not how fast your computer can print the list of ad-
dresses into a file. In that case, what you care about is at the
level of weak head normal form, and whnfwill tell you more
precisely how long it is taking to find the data and decide
whether you have a Nothing or aJust a .
On the other hand, if you are interested in measuring the
time it takes to print your results, in addition to looking up the
data, then you may want to evaluate to normal form. There
are times when measuring that makes sense. We’ll see some
examples shortly.
For now, let us consider each indexing operator, the (!!)
that exists in baseand the one we’ve written that uses Maybe
instead of bottoms.
In the former case, the final result has the type 𝑎. The</p>
<p>CHAPTER 28. BASIC LIBRARIES 1721
function doesn’t stop recursing until it either returns bottom
or the value at that index. In either case, it’s done all the work
you’d care to measure — traversing the list. Evaluation to
WHNF means stopping at your 𝑎value.
In the latter case with Maybe, evaluation to WHNF means
stopping at either JustorNothing . It won’t evaluate the contents
of theJustdata constructor under whnf, but it will under nf.
Either is sufficient for the purposes of the benchmark as, again,
we’re measuring how quickly this code reaches the value at an
index in the list.
Let us consider an example with a few changes:</p>
<p>CHAPTER 28. BASIC LIBRARIES 1722
moduleMainwhere
importCriterion.Main
importDebug.Trace
myList::[Int]
myList=trace&quot;myList was evaluated&quot;
([1..9999]++[undefined])
-- your version of (!?) here
main::IO()
main=defaultMain
[ bench &quot;index list 9999&quot;
$whnf (myList !!)9998
, bench &quot;index list maybe index 9999&quot;
$nf (myList !?)9999
]
Notice what we did here. We added an undefined in what
will be the index position 9999. With the (!!)operator, we are
accessing the index just before that bottom value because there
is no outer data constructor (such as Nothing orJust) where we
could stop the evaluation. Both whnfandnfwill necessarily
force that bottom value.</p>
<p>CHAPTER 28. BASIC LIBRARIES 1723
We also modified the whnftonffor the benchmark of (!?).
Now it will evaluate the undefined it found at that index under
the bottom in the first run of the benchmark and fail:
benchmarking index list maybe index 9999
criterion1: Prelude.undefined
A function value that returned bottom instead of a data
constructor would’ve also acted as a stopping point for WHNF.
Consider the following:
Prelude&gt; (Just undefined) <code>seq</code> 1
1
Prelude&gt; (_ -&gt; undefined) <code>seq</code> 1
1
Prelude&gt; ((_ -&gt; Just undefined) 0) <code>seq</code> 1
1
Prelude&gt; ((_ -&gt; undefined) 0) <code>seq</code> 1
*** Exception: Prelude.undefined
Much of the time, whnfis going to cover the thing you’re
trying to benchmark.
Making the case for nf
Let us now look at an example of when whnfisn’t sufficient for
benchmarking, something that uses guarded recursion, unlike
(!!):</p>
<p>CHAPTER 28. BASIC LIBRARIES 1724
moduleMainwhere
importCriterion.Main
myList::[Int]
myList=[1..9999]
main::IO()
main=defaultMain
[ bench &quot;map list 9999&quot; $
whnf (map ( +1)) myList
]
The above is an example of guarded recursion because a
data constructor is interposed between each recursion step.
The data constructor is the cons cell when we’re talking about
map. Guarded recursion lets us consume the recursion steps
up to weak head normal form incrementally on demand.
Importantly, foldrcan be used to implement guarded and
unguarded recursion, depending entirely on what the folding
function does rather than any special provision made by foldr
itself. So what happens when we benchmark this?
Linking bin/bench ...
time
8.844 ns (8.670 ns .. 9.033 ns)</p>
<p>CHAPTER 28. BASIC LIBRARIES 1725
0.998 R² (0.997 R² .. 1.000 R²)
mean
8.647 ns (8.567 ns .. 8.751 ns)
std dev
293.3 ps (214.7 ps .. 412.9 ps)
variance introduced by outliers:
57% (severely inflated)
Well, that’s suspect. Does it really take 8.8 nanoseconds
to traverse a 10,000 element linked list in Haskell? We saw
an example of how long it should take, roughly. This is an
example of our benchmark being too lazy. The issue is that
mapuses guarded recursion and the cons cells of the list are
interposed between each recursion of map. You may recall this
from the lists and folds chapters. So, it ends up evaluating
only this far:
(_ : _)
Ah, that first data constructor. It has neither done the work
of incrementing the value nor has it traversed the rest of the
list. It’s just sitting there at the first cons cell. Using bottoms,
you can progressively prove to yourself what whnfis evaluating
by replacing things and re-running the benchmark:</p>
<p>CHAPTER 28. BASIC LIBRARIES 1726
-- is it applying (+1)?
myList=(undefined :[2..9999])
-- Is it going any further in the list?
myList::[Int]
myList=(undefined :undefined)
-- This should s'plode because
-- it'll be looking for that first
-- data constructor or (-&gt;) to stop at
myList::[Int]
myList=undefined
No matter, we can fix this!
-- change this bit
whnf(map (+1)) myList
-- into:
nf(map (+1)) myList
Then we get:
time
122.5 μs (121.7 μs .. 123.9 μs)
0.999 R² (0.998 R² .. 1.000 R²)</p>
<p>CHAPTER 28. BASIC LIBRARIES 1727
mean
123.0 μs (122.0 μs .. 125.6 μs)
std dev
5.404 μs (2.806 μs .. 9.323 μs)
That is considerably more realistic considering we’ve eval-
uated the construction of a whole new list. This is slower than
the indexing operation because we’re not just kicking a new
value out, we’re also constructing a new list.
In general when deciding between whnfandnf, ask yourself,
“when I have reached the first data constructor, have I done
most or all of the work that matters?” Be careful not to use nf
too much. If you have a function that returns a nontrivial data
structure or collection for which it’s already done all the work
to produce, nfwill make your code look excessively slow and
lead you on a wild goose chase.
28.3 Profiling your programs
We’re going to do our best to convey what you should know
about profiling programs with GHC and what we think is con-
ceptually less well covered, but we aren’t going to presume to
replace the GHC User Guide. We strongly recommend you
read the guide5for more information.
5https://downloads.haskell.org/~ghc/latest/docs/html/users_guide/profiling.html</p>
<p>CHAPTER 28. BASIC LIBRARIES 1728
Profiling time usage
Sometimes rather than seeing how fast our programs are, we
want to know why they’re slow or fast and where they’re spend-
ing their time. To that end, we use profiling. First, let’s put
together a simple example for motivating this:
-- profilingTime.hs
moduleMainwhere
f::IO()
f= do
print ([ 1..]!!999999)
putStrLn &quot;f&quot;
g::IO()
g= do
print ([ 1..]!!9999999)
putStrLn &quot;g&quot;
main::IO()
main= do
f
g
Given that we traverse 10 times as much list structure in the</p>
<p>CHAPTER 28. BASIC LIBRARIES 1729
case of 𝑔, we believe we should see something like 10 times
as much CPU time spent in 𝑔. We can do the following to
determine if that’s the case:
$ stack ghc -- -prof -fprof-auto </p>
<blockquote>
<p>-rtsopts -O2 profile.hs
./profile +RTS -P
cat profile.prof
Breaking down what each flag does:
1.-profenables profiling. Profiling isn’t enabled by default
because it can lead to slower programs but this generally
isn’t an issue when you’re investigating the performance
of your programs. Used alone, -profwill require you to
annotate cost centers manually, places for GHC to mark
for keeping track of how much time is spent evaluating
something.
2.-fprof-auto assigns all bindings not marked inline a cost
center named after the binding. This is fine for little stuﬀ
or otherwise not terribly performance-sensitive stuﬀ, but
if you’re dealing with a large program or one sensitive
to perturbations from profiling, it may be better to not
use this and instead assign your “SCCs” manually. SCC is
what the GHC documentation calls a cost center.</p>
</blockquote>
<p>CHAPTER 28. BASIC LIBRARIES 1730
3.-rtsopts enables you to pass GHC RTS options to the gen-
erated binary. This is optional so you can get a smaller
binary if desired. We need this to tell our program to
dump the profile to the .proffile named after our pro-
gram.
4.-O2enables the highest level of program optimizations.
This is wise if you care about performance but -Oby itself
also enables optimizations, albeit somewhat less aggres-
sive ones. Either option can make sense when bench-
marking; it’s a case by case thing, but most Haskell pro-
grammers feel pretty free to default to -O2.
After examining the .proffile which contains the profiler
output, this is roughly what we’ll see:
Sun Feb 14 21:34 2016
Time and Allocation
Profiling Report (Final)
profile +RTS -P -RTS
total time = 0.22 secs
(217 ticks @ 1000 us, 1 processor)
total alloc = 792,056,016 bytes
(excludes profiling overheads)</p>
<p>CHAPTER 28. BASIC LIBRARIES 1731
COST CENTRE MODULE %time %alloc ticks bytes
g Main 91.2 90.9 198 720004344
f Main 8.8 9.1 19 72012568
...later noise snipped,
we care about the above...
And indeed, 91.2% time spent in 𝑔, 8.8% time spent in 𝑓would
seem to validate our hypothesis here.
Time isn’t the only thing we can profile. We’d also like
to know about the space (or memory) diﬀerent parts of our
program are responsible for using.
Profiling heap usage
We have measured time; now we shall measure space. Well,
memory anyway; we’re not astrophysicists. We’re going to
keep this quick and boring so that we might be able to get to
the good stuﬀ:</p>
<p>CHAPTER 28. BASIC LIBRARIES 1732
moduleMainwhere
importControl.Monad
blah::[Integer]
blah=[1..1000]
main::IO()
main=
replicateM_ 10000(print blah)
ghc -prof -fprof-auto -rtsopts -O2 loci.hs
./loci +RTS -hc -p
hp2ps loci.hp
If you open the loci.ps postscript file with your PDF reader
of choice, you’ll see how much memory the program used
over the time the program ran. Note that you’ll need the
program to run a minimum amount of time for the profiler
to get any samples of the heap size.
28.4 Constant applicative forms
Whenwe’retalkingaboutmemoryusageandsharinginHaskell,
we have to also talk about CAFs: constant applicative forms.
CAFs are expressions that have no free variables and are held</p>
<p>CHAPTER 28. BASIC LIBRARIES 1733
in memory to be shared with all other expressions in a module.
They can be literal values or partially applied functions that
have no named arguments.
We’re going to construct a very large CAF here. Notice we
are mapping over an infinite list and want to know how much
memory this uses. You might consider betting on a lot:
moduleMainwhere
incdInts ::[Integer]
incdInts =map (+1) [1..]
main::IO()
main= do
print (incdInts !!1000)
print (incdInts !!9001)
print (incdInts !!90010)
print (incdInts !!9001000)
print (incdInts !!9501000)
print (incdInts !!9901000)
Now we can profile that:
Thu Jan 21 23:25 2016
Time and Allocation
Profiling Report (Final)</p>
<p>CHAPTER 28. BASIC LIBRARIES 1734
cafSaturation +RTS -p -RTS
total time = 0.28 secs
(283 ticks @ 1000 us, 1 processor)
total alloc = 1,440,216,712 bytes
(excludes profiling overheads)
COST CENTRE MODULE %time %alloc
incdInts Main 90.1 100.0
main Main 9.9 0.0
-- some irrelevant bits elided
COST CENTRE MODULE no. entries %time %alloc
MAIN MAIN 45 0 0.0 0.0
CAF Main 89 0 0.0 0.0
incdInts Main 91 1 90.1 100.0
main Main 90 1 9.9 0.0
Note how incdInts is its own constant applicative form (CAF)
here apart from main. And notice the size of that memory
allocation. It’s because that mapping over an infinite list is a
top-level value that can be shared throughout a module, so it
must be evaluated and the results held in memory in order to
be shared.
CAFs include
•values;</p>
<p>CHAPTER 28. BASIC LIBRARIES 1735
•partially applied functions without named arguments;
•fully applied functions such as incdInts above, although
that would be rare in real code.
CAFs can make some programs faster since you don’t have
to keep re-evaluating shared values; however, CAFs can be-
come memory-intensive quite quickly. The important take-
away is that, if you find your program using much more mem-
ory than you expected, find the golden CAF and kill it.
Fortunately, CAFs mostly occur in toy code. Real world
code is usually pulling the data from somewhere, which avoids
the problem of holding large amounts of data in memory.
Let’s look at a way to avoid creating a CAF by introducing
an argument into our incdInts example:
moduleMainwhere
-- not a CAF
incdInts ::[Integer]-&gt;[Integer]
incdInts x=map (+1) x
main::IO()
main= do
print (incdInts [ 1..]!!1000)
If you examine the profile:</p>
<p>CHAPTER 28. BASIC LIBRARIES 1736
CAF
main
incdInts
Pointfree top-level declarations will be CAFs, but pointful
ones will not. We’d discussed this to some degree in the pre-
vious chapter as well. The reason the diﬀerence matters is
often not because of the total allocation reported by the pro-
file, which is often misleading anyway. Rather it’s important
because lists are as much control structures as data structures
and it’s very cheap in GHC to construct a list which is thrown
away immediately. Doing so might increase how much mem-
ory you allocate in total, but unlike a CAF, it won’t stay in your
heap which may lead to lower peak memory usage and the
runtime spending less time collecting garbage.
Indeed, this is not a standalone CAF. But what if we eta
reduce it (that is, remove its named arguments) so that it is
pointfree?</p>
<p>CHAPTER 28. BASIC LIBRARIES 1737
moduleMainwhere
-- Gonna be a CAF this time.
incdInts ::[Integer]-&gt;[Integer]
incdInts =map (+1)
main::IO()
main= do
print (incdInts [ 1..]!!1000)
This time when you look at the profile, it’ll be its own CAF:
CAF
incdInts
main
incdInts
GREAT SCOTT!
It doesn’t really change the performance for something so
trivial, but you get the idea. The big diﬀerence between the
two is in the heap profiles. Check them and you will likely see
what we mean.
28.5 Map
We’re going to start our exploration of data structures with Map.
It’s worth pointing out here that most of the structures we’ll</p>
<p>CHAPTER 28. BASIC LIBRARIES 1738
look at are, in some sense, replacements for the lists we have
depended on throughout the book. Lists and strings are useful
for a lot of things, but they’re not the most performant or even
the most useful way to structure your data. What is the most
performant or useful for any given program can vary, so we
can’t give a blanket recommendation that you should always
use any one of the structures we’re going to talk about. You
have to judge that based on what problems you’re trying to
solve and use benchmarking and profiling tools to help you
fine tune the performance.
Most of the data structures we’ll be looking at are in the
containers6library. If you build it, Mapwill come. And also
Sequence andSetand some other goodies. You’ll notice a lot of
the data structures have a similar API, but each are designed
for diﬀerent sets of circumstances.
We’ve used the Maptype before to represent associations of
unique pairings of keys to values. You may remember it from
the Testing chapter in particular, where we used it to look up
Morse code translations of alphabetic characters. Those were
fun times, so carefree and innocent.
The structure of the Maptype looks like this:
6http://hackage.haskell.org/package/containers</p>
<p>CHAPTER 28. BASIC LIBRARIES 1739
dataMapk a
=Bin
{-# UNPACK #-}
!Size!k a
!(Mapk a)!(Mapk a)
|Tip
typeSize=Int
You may recognize the exclamation marks denoting strict-
nessfromthe sectionson bangpatterns inthe previouschapter.
Tip is a data constructor for capping oﬀ the branch of a tree.
If you’d like to find out about the unpacking of strict fields,
which is what the UNPACK pragma is for; see the GHC documen-
tation for more information.
What’s something that’s faster with Map?
Well, lookups by key in particular are what it’s used for. Con-
siderthefollowingcomparisonofanassociationlistand Data.Map :
moduleMainwhere
importCriterion.Main
import qualified Data.Map asM</p>
<p>CHAPTER 28. BASIC LIBRARIES 1740
genList ::Int-&gt;[(String,Int)]
genList n=go n[]
wherego0xs=(&quot;0&quot;,0):xs
go n' xs =
go (n'-1) ((show n', n') :xs)
pairList ::[(String,Int)]
pairList =genList 9001
testMap ::M.MapStringInt
testMap =M.fromList pairList
main::IO()
main=defaultMain
[ bench &quot;lookup one thing, list&quot; $
whnf (lookup &quot;doesntExist&quot; ) pairList
, bench &quot;lookup one thing, map&quot; $
whnf (M.lookup&quot;doesntExist&quot; ) testMap
]
Association lists such as pairList are fine if you need some-
thing cheap and cheerful for a very small series of pairs, but
you’re better oﬀ using Mapby default when you have keys and
values. You get a handy set of baked-in functions for looking
things up and an efficient means of doing so. Insert operations</p>
<p>CHAPTER 28. BASIC LIBRARIES 1741
also benefit from being able to find the existing key-value pair
inMapmore quickly than in association lists.
What’s slower with Map?
Using an Intas your key type is usually a sign you’d be better oﬀ
with aHashMap ,IntMap , orVector , depending on the semantics of
your problem. If you need good memory density and locality
— which will make aggregating and reading values of a large
Vector faster, then Mapmight be inappropriate and you’ll want
Vector instead.
28.6 Set
This is also in containers. It’s like a Map, but without the ‘value’
part of the key-value pair. You may be asking yourself, why
do I want only keys?
When we use Map, it has an Ordconstraint on the functions
to ensure the keys are in order. That is one of the things that
makes lookups in Mapparticularly efficient. Knowing that the
keys will be ordered divides the problem space up by halves:
if we’re looking for the key 6 in a set of keys from 1-10, we
don’t have to search in the first half of the set because those
numbers are less than 6. Set, likeMap, is structured associatively,
not linearly.</p>
<p>CHAPTER 28. BASIC LIBRARIES 1742
Functions with Sethave the same Ordconstraint, but now
we don’t have key-value pairs — we only have keys. Another
way to think of it is the keys are now the values. That means
thatSetrepresents a unique, ordered set of values.
Here is the datatype for Set:
dataSeta
=Bin
{-# UNPACK #-}
!Size!a!(Seta)!(Seta)
|Tip
typeSize=Int
It’s eﬀectively equivalent to a Maptype with unit values.
moduleMainwhere
importCriterion.Main
import qualified Data.Map asM
import qualified Data.Set asS
bumpIt(i, v)=(i+1, v+1)</p>
<p>CHAPTER 28. BASIC LIBRARIES 1743
m::M.MapIntInt
m=M.fromList $take10000stream
wherestream=iterate bumpIt ( 0,0)
s::S.SetInt
s=S.fromList $take10000stream
wherestream=iterate ( +1)0
membersMap ::Int-&gt;Bool
membersMap i=M.member i m
membersSet ::Int-&gt;Bool
membersSet i=S.member i s
main::IO()
main=defaultMain
[ bench &quot;member check map&quot; $
whnf membersMap 9999
, bench &quot;member check set&quot; $
whnf membersSet 9999
]
If you benchmark the above, you should get very similar
results for both, with Mapperhaps being a smidgen slower than
Set. They’re similar because they’re nearly identical data struc-
tures in the containers library.</p>
<p>CHAPTER 28. BASIC LIBRARIES 1744
There’s not much more to say. It has the same pros and
cons7asMap, with the same performance of the core operations,
save that you’re more limited.
Exercise: Benchmark Practice
Make a benchmark to prove for yourself whether MapandSet
have similar performance. Try operations other than mem-
bership testing, such as insertion or unions.
28.7 Sequence
Sequence is a nifty datatype built atop finger trees; we aren’t
going to address finger trees in this book, unfortunately, but we
encourage you to check them out.8Sequence appends cheaply
on the front and the back, which avoids a common problem
with lists where you can only cons to the front cheaply.
Here is the datatype for Sequence :
7HA HA GET IT?!
8see, for example, Finger Trees: A Simple General-purpose Data Structure by Ralf
Hinze and Ross Paterson</p>
<p>CHAPTER 28. BASIC LIBRARIES 1745
newtype Seqa=Seq(FingerTree (Elema))
-- Elem is so elements and nodes can be
-- distinguished in the types of the
-- implementation. Don't sweat it.
newtype Elema=Elem{ getElem ::a }
dataFingerTree a
=Empty
|Singlea
|Deep{-# UNPACK #-} !Int!(Digita)
(FingerTree (Nodea))!(Digita)
What’s faster with Sequence?
Updates (cons and append) to both ends of the data structure
and concatenation are what Sequence is particularly known for.
You won’t want to resort to using Sequence by default though,
as the list type is often competitive. Sequence will have more
efficient access to the tail than a list will. Here’s an example
whereSequence does better because the list is a bit big:</p>
<p>CHAPTER 28. BASIC LIBRARIES 1746
moduleMainwhere
importCriterion.Main
import qualified Data.Sequence asS
lists::[[Int]]
lists=replicate 10[1..100000]
seqs::[S.SeqInt]
seqs=
replicate 10(S.fromList [ 1..100000])
main::IO()
main=defaultMain
[ bench &quot;concatenate lists&quot; $
nf mconcat lists
, bench &quot;concatenate sequences&quot; $
nf mconcat seqs
]
Note that in the above, a substantial amount of the time in
the benchmark is spent traversing the data structure because
we’re evaluating to normal form to ensure all the work is done.
Incidentally, this accentuates the diﬀerence between a list and
Sequence because it’s faster to index or traverse a sequence than</p>
<p>CHAPTER 28. BASIC LIBRARIES 1747
a list. Consider the following:
moduleMainwhere
importCriterion.Main
import qualified Data.Sequence asS
lists::[Int]
lists=[1..100000]
seqs::S.SeqInt
seqs=S.fromList [ 1..100000]
main::IO()
main=defaultMain
[ bench &quot;indexing list&quot; $
whnf (\xs-&gt;xs!!9001) lists
, bench &quot;indexing sequence&quot; $
whnf (flip S.index9001) seqs
]
The diﬀerence between list and Sequence in the above when
we ran the benchmark was two orders of magnitude.</p>
<p>CHAPTER 28. BASIC LIBRARIES 1748
What’s slower with Sequence?
Sequence is a persistent data structure like Map, so the memory
density isn’t as good as it is with Vector (we’re getting there).
Indexing by Intwill be faster with Vector as well. List will be
faster with consing and concatenation in some cases, if the
lists are small. When you know you need cheap appending
to the end of a long list, it’s worth giving Sequence a try, but
it’s better to base the final decision on benchmarking data if
performance matters.
28.8 Vector
The next data structure we’re going to look at is not in contain-
ers. It’s in its own library called, unsurprisingly, vector9. You’ll
notice it says vectors are “efficient arrays.” We’re not going to
look at arrays, or Haskell’s Arraytype, specifically here, though
you may already be familiar with the idea.
Onerarelyusesarrays, ormorespecifically, Array10inHaskell.
Vector is almost always what you want instead of an array. The
default Vector type is implemented as a slice wrapper of Array;
we can see this in the definition of the datatype:
9https://hackage.haskell.org/package/vector
10http://hackage.haskell.org/package/array</p>
<p>CHAPTER 28. BASIC LIBRARIES 1749
-- | Boxed vectors, supporting
-- efficient slicing.
dataVectora=
Vector{-# UNPACK #-} !Int
{-# UNPACK #-} !Int
{-# UNPACK #-} !(Arraya)
deriving (Typeable )
There are many variants of Vector, although we won’t dis-
cuss them all here. These include boxed, unboxed, immutable,
mutable, and storable vectors, but the plain version above is
the usual one you’d use. We’ll talk about mutable vectors in
their own section. Boxed means the vector can reference any
datatype you want; unboxed represents raw values without
pointer indirection.11The latter can save a lot of memory but
is limited to types like Bool,Char,Double ,Float,Int,Word, tuples
of unboxable values. Since a newtype is unlifted and doesn’t
introduce any pointer indirection, you can unbox any newtype
that contains an unboxable type.
When does one want a Vector in Haskell?
You want a Vector when
11This book isn’t the right place to talk about what pointers are in detail. Briefly, they
are references to things in memory, rather than the things themselves.</p>
<p>CHAPTER 28. BASIC LIBRARIES 1750
•you need memory efficiency close to the theoretical max-
imum for the data you are working with;
•your data access is almost exclusively in terms of indexing
via anIntvalue;
•you want uniform access times for accessing each element
in the data structure; and/or,
•you will construct a Vector once and read it many times (al-
ternatively, you plan to use a mutable Vector for ongoing,
efficient updates).
What’s this about slicing?
Remember this from the definition of Vector ?
-- | Boxed vectors, supporting
-- efficient slicing.
Slicing refers to slicing oﬀ a portion, or creating a sub-array.
TheVector type is designed for making slicing much cheaper
than it otherwise would be. Consider the following:</p>
<p>CHAPTER 28. BASIC LIBRARIES 1751
moduleMainwhere
importCriterion.Main
import qualified Data.Vector asV
slice::Int-&gt;Int-&gt;[a]-&gt;[a]
slicefrom len xs =
take len (drop from xs)
l::[Int]
l=[1..1000]
v::V.VectorInt
v=V.fromList [ 1..1000]
main::IO()
main=defaultMain
[ bench &quot;slicing list&quot; $
whnf (head .slice100900) l
, bench &quot;slicing vector&quot; $
whnf (V.head.V.slice100900) v
]
If you run this benchmark, Vector should be faster than
list. Why? Because when we constructed that new Vector with</p>
<p>CHAPTER 28. BASIC LIBRARIES 1752
V.slice , all it had to do was the following:
-- from Data.Vector
instance G.VectorVectorawhere
-- other methods elided
{-# INLINE basicUnsafeSlice #-}
basicUnsafeSlice j n ( Vectori_arr)=
Vector(i+j) n arr
What makes Vector nicer than lists and Arrayin this respect
is that when you construct a slice or view of another Vector , it
doesn’t have to construct as much new data. It returns a new
wrapper around the original underlying array with a new index
and oﬀset with a reference to the same original Array. Doing
the same with an ordinary Arrayor a list would’ve required
copying more data. Speed comes from being sneaky and
skipping work.
Updating vectors
Persistent vectors are not great at handling updates on an
ongoing basis, but there are some situations in which they can
surprise you. One such case is fusion12. Fusion, or loop fusion,
12Stream Fusion; Duncan Coutts; http://code.haskell.org/~dons/papers/icfp088-coutts.
pdf</p>
<p>CHAPTER 28. BASIC LIBRARIES 1753
means that as an optimization the compiler can fuse several
loops into one megaloop and do it in one pass:
moduleMainwhere
importCriterion.Main
import qualified Data.Vector asV
testV'::Int-&gt;V.VectorInt
testV'n=
V.map (+n)$V.map (+n)$
V.map (+n)$V.map (+n)
(V.fromList [ 1..10000])
testV::Int-&gt;V.VectorInt
testVn=
V.map ( (+n).(+n)
.(+n).(+n) )
(V.fromList [ 1..10000])</p>
<p>CHAPTER 28. BASIC LIBRARIES 1754
main::IO()
main=defaultMain
[ bench &quot;vector map prefused&quot; $
whnf testV 9998
, bench &quot;vector map will be fused&quot; $
whnf testV' 9998
]
The vector library has loop fusion built in, so in a lot of
cases, such as with mapping, you won’t construct 4 vectors just
because you mapped four times. Through the use of GHC
RULES13the code in testV’ will change into what you see in
testV. It’s worth noting that part of the reason this optimization
is sound is because we know what code performs eﬀects and
what does not because we have types.
However, loop fusion isn’t a panacea and there will be situ-
ations where you want to change one element at a time, selec-
tively. Let’s consider more efficient ways of updating vectors.
We’re going to use the (//)operator from the vector library
here. It’s a batch update operator that allows you to modify
several elements of the vector at one time:
13https://wiki.haskell.org/GHC/Using_rules</p>
<p>CHAPTER 28. BASIC LIBRARIES 1755
moduleMainwhere
importCriterion.Main
importData.Vector ((//))
import qualified Data.Vector asV
vec::V.VectorInt
vec=V.fromList [ 1..10000]
slow::Int-&gt;V.VectorInt
slown=go n vec
wherego0v=v
go n v=go (n-1) (v//[(n,0)])
batchList ::Int-&gt;V.VectorInt
batchList n=vec//updates
whereupdates =
fmap (\n-&gt;(n,0)) [0..n]
main::IO()
main=defaultMain
[ bench &quot;slow&quot;$whnf slow 9998
, bench &quot;batch list&quot; $
whnf batchList 9998
]</p>
<p>CHAPTER 28. BASIC LIBRARIES 1756
The issue with the first example is that we’re using a batch
API… but not in batch. It’s much cheaper (500–1000x in our
tests) to construct the list of updates all at once and then feed
them to the (//)function. We can make it even faster still by
using the update function that uses a vector of updates:
batchVector ::Int-&gt;V.VectorInt
batchVector n=V.unsafeUpdate vec updates
whereupdates =
fmap (\n-&gt;(n,0))
(V.fromList [ 0..n])
Benchmarkingthis versionshould getyoucode thatis about
1.4x faster. But we’re greedy. So we want more.
Mutable Vectors
“Moria! Moria! Wonder of the Northern world. Too
deep we delved there, and woke the nameless fear.”
— Glóin, The Fellowship of the Ring
What if we want something even faster? Although many
people don’t realize this about Haskell, we can obtain the bene-
fits of in-place updates if we so desire. What sets Haskell apart
is that we cannot do so in a way that compromises referential
transparency or the nice equational properties our expressions</p>
<p>CHAPTER 28. BASIC LIBRARIES 1757
have. First we’ll demonstrate how to do this, then we’ll talk
about how this is designed to behave predictably.
Here comes an example:
moduleMainwhere
importControl.Monad.Primitive
importControl.Monad.ST
importCriterion.Main
import qualified Data.Vector asV
import qualified Data.Vector.Mutable asMV
import qualified
Data.Vector.Generic.Mutable asGM
mutableUpdateIO
::Int
-&gt;IO(MV.MVector RealWorld Int)
mutableUpdateIO n= do
mvec&lt;-GM.new (n+1)
go n mvec
wherego0v=return v
go n v=
(MV.write v n 0)&gt;&gt;go (n-1) v</p>
<p>CHAPTER 28. BASIC LIBRARIES 1758
mutableUpdateST ::Int-&gt;V.VectorInt
mutableUpdateST n=runST$ do
mvec&lt;-GM.new (n+1)
go n mvec
wherego0v=V.freeze v
go n v=
(MV.write v n 0)&gt;&gt;go (n-1) v
main::IO()
main=defaultMain
[ bench &quot;mutable IO vector&quot;
$whnfIO (mutableUpdateIO 9998)
, bench &quot;mutable ST vector&quot;
$whnf mutableUpdateST 9998
]
We’re going to talk about runSTshortly. For the moment,
let’s concentrate on the fact that the mutable IOversion is
approximately 7,000x faster than the original unbatched loop
in our tests. The STversion is about 1.5x slower than the IO
version, but still very fast. The added time in the STversion is
from freezing the mutable vector into an ordinary vector. We
won’t explain STfully here, but, as we’ll see, it can be handy
when you want to temporarily make something mutable and
ensure no mutable references are exposed outside of the ST
monad.</p>
<p>CHAPTER 28. BASIC LIBRARIES 1759
Here were the timings we got with the various vector oper-
ations on our computer:
Variant Microseconds
slow 133,600
batchList 244
batchVector 176
mutableUpdateST 32
mutableUpdateIO 19
Notably, most of the performance improvement was from
not doing something silly. Don’t resort to the use of mutation
viaIOorSTexcept where you know it’s necessary. It’s worth
noting that given our test involved updating almost 10,000
indices in the vector, we spent an average of 1.9 nanoseconds
per update when we used mutability and 17.6 ns when we did
it in batch with a persistent vector.
A sidebar on the ST Monad
STcan be thought of as a mutable variant of the strict State
monad. From another angle, it could be thought of as IO
restricted exclusively to mutation which is guaranteed safe.
Safe how? STis relying on the principle behind that old
philosophical saw about the tree that falls in the forest with no
one to see it fall. The idea behind this “morally eﬀect-free” un-
derstanding of mutable state was introduced in the paper Lazy</p>
<p>CHAPTER 28. BASIC LIBRARIES 1760
Functional State Threads.14It unfreezes your data, mutates
it, then refreezes it so that it can’t mutate anymore. Thus it
manages to mutate and still maintain referential transparency.
ForSTto work properly, the code that mutates the data must
not get reordered by the optimizer or otherwise monkeyed
with, much like the code in IO. So there must be something
underlying the type which prevents GHC ruining our day. Let
us examine the STtype:
Prelude&gt; import Control.Monad.ST
Prelude&gt; :info ST
type role ST nominal representational
newtype ST s a =
GHC.ST.ST (GHC.ST.STRep s a)
...
Prelude&gt; import GHC.ST
Prelude&gt; :info STRep
type STRep s a =
GHC.Prim.State# s
-&gt; (# GHC.Prim.State# s, a #)
There’s a bit of ugliness in here that shouldn’t be too sur-
prising after you’ve seen GHC Core in the previous chapter.
What’s important is that 𝑠is getting its type from the thing
14Lazy Functional State Threads; John Launchbury and Simon Peyton Jones</p>
<p>CHAPTER 28. BASIC LIBRARIES 1761
you’re mutating, but it has no value-level witness. The State
monad here is therefore erasable; it can encapsulate this mu-
tation process and then melt away.
It’s important to understand that 𝑠isn’t the state you’re
mutating. The mutation is a side eﬀect of having entered
the closures that perform the eﬀect. This strict, unlifted state
transformer monad happens to structure your code in a way
that preserves the intended order of the computations and
their associated eﬀects.
By closures here, we mean lambda expressions. Of course
we do, because this whole book is one giant lambda expression
under the hood. Entering each lambda performs its eﬀects.
The eﬀects of a series of lambdas are not batched, because the
ordering is important when we’re performing eﬀects, as each
new expression might depend on the eﬀects of the previous
one. The eﬀects are performed and then, if the value that
should result from the computation is also going to be used,
the value is evaluated.
So what is the 𝑠type for? Well, while it’s all well and good to
ask politely that programmers freeze a mutable reference into
a persistent, immutable data structure as the final result…you
can’t count on that. STenforces it at compile time by making
it so that 𝑠will never unify with anything outside of the ST
Monad. The trick for this is called existential quantification15,
15Existentially quantified types; Haskell Wikibook
https://en.wikibooks.org/wiki/Haskell/Existentially_quantified_types</p>
<p>CHAPTER 28. BASIC LIBRARIES 1762
but having said this won’t necessarily mean anything to you
right now. But it does prevent you from accidentally leaking
mutable references to code outside ST, which could then lead
to code that does unpredictable things depending on the state
of the bits in memory.
Avoid dipping in and out of STover and over. The thaws
and freezes will cost you in situations where it might have
been better to just use IO. Batching your mutation is best for
performance and code comprehensibility.
Exercises: Vector
Setup a benchmark harness with criterion to profile how much
memory boxed and unboxed vectors containing the same
data uses. You can combine this with a benchmark to give it
something to do for a few seconds. We’re not giving you a lot
because you’re going to have to learn to read documentation
and source code anyway.
28.9 String types
The title is a slight bit of a misnomer as we’ll talk about two
string (or text) types and one type for representing sequences
ofbytes. Here’sabriefexplanationof String ,Text, andByteString :</p>
<p>CHAPTER 28. BASIC LIBRARIES 1763
String
You know what String is. It’s a type alias for a list of Char, yet
underneath it’s not quite as simple as an actual list of Char.
One of the benefits of using strings is the simplicity, and
they’re easy enough to explain. For most demonstration and
toy program purposes, they’re fine.
However, like lists themselves, they can be infinite. The
memory usage for even very large strings can get out of control
rapidly. Furthermore, they have very inefficient character-by-
character indexing into the String. The time taken to do a
lookup increases proportionately with the length of the list.
Text
This one comes in the text16library on Hackage. This is best
when you have plain text, but need to store the data more
efficiently — particularly as it concerns memory usage.
We’ve used this one a bit in previous chapters, when we
were playing with OverloadedStrings . The benefits here are that
you get:
•compact representation in memory; and
•efficient indexing into the string.
16http://hackage.haskell.org/package/text</p>
<p>CHAPTER 28. BASIC LIBRARIES 1764
However, Textis encoded as UTF-16, and that isn’t what
most people expect given UTF-8’s popularity. In Text’s defense,
UTF-16 is often faster due to cache friendliness via chunkier
and more predictable memory accesses.
Don’t trust your gut, measure
We mentioned Texthas a more compact memory represen-
tation, but what do you think the memory profile for the
following program will look up? High first, then low, or low
first, then high?</p>
<p>CHAPTER 28. BASIC LIBRARIES 1765
moduleMainwhere
importControl.Monad
import qualified Data.Text asT
import qualified Data.Text.IO asTIO
import qualified System.IO asSIO
-- replace &quot;/usr/share/dict/words&quot;
-- as appropriate
dictWords ::IOString
dictWords =
SIO.readFile &quot;/usr/share/dict/words&quot;
dictWordsT ::IOT.Text
dictWordsT =
TIO.readFile &quot;/usr/share/dict/words&quot;
main::IO()
main= do
replicateM_ 1000(dictWords &gt;&gt;=print)
replicateM_ 1000
(dictWordsT &gt;&gt;=TIO.putStrLn)
The issue is that Textwent ahead and loaded the entire file
into memory each of the ten times you forced the IOaction.</p>
<p>CHAPTER 28. BASIC LIBRARIES 1766
ThereadFile operation for String , however, was lazy and only
read as much of the file into memory as needed to print the
data to stdout. The proper way to handle incrementally pro-
cessing data is with streaming17, but this is not something we’ll
cover in detail in this book. However, there is a lazy way we
could change this:
-- Add the following to the module you
-- already made for profiling String
-- and Text.
import qualified Data.Text.Lazy asTL
import qualified Data.Text.Lazy.IO asTLIO
dictWordsTL ::IOTL.Text
dictWordsTL =
TLIO.readFile &quot;/usr/share/dict/words&quot;
main::IO()
main= do
replicateM_ 1000(dictWords &gt;&gt;=print)
replicateM_ 1000
(dictWordsT &gt;&gt;=TIO.putStrLn)
replicateM_ 1000
(dictWordsTL &gt;&gt;=TLIO.putStrLn)
17https://wiki.haskell.org/Pipes</p>
<p>CHAPTER 28. BASIC LIBRARIES 1767
Now you should see memory usage plummet again after
a middle plateau because you’re reading in as much text as
necessary to print, able to deallocate as you go. This is some,
but not all, of the benefits of streaming but we do strongly
recommend using streaming rather than relying on a lazy IO
API.
ByteString
Thisisnotastring. Ortext. Notnecessarilyanyway. ByteString s
are sequences of bytes represented (indirectly) as a vector of
Word8values. Text on a computer is made up of bytes, but it
has to have a particular encoding in order for it to be text.
Encodings of text can be ASCII, UTF-8, UTF-16, or UTF-32,
usually UTF-8 or UTF-16. The Texttype encodes the data as
UTF-16, partly for performance. It’s often faster to read larger
chunks of data at a time from memory, so the 16-bits-per-rune
encoding of UTF-16 performs a bit better in most cases.
The main benefit of ByteString is that it’s easy to use via the
OverloadedStrings extension but is bytes instead of text. This
addresses a larger problem space than mere text.
The flip side of that, of course, is that it encompasses byte
data that isn’t comprehensible text. That’s a drawback if you
didn’t mean to permit non-text byte sequences in your data.</p>
<p>CHAPTER 28. BASIC LIBRARIES 1768
ByteString examples
Here’s an example highlighting that ByteStrings are not always
text:
{-# LANGUAGE OverloadedStrings #-}
moduleBSwhere
import qualified Data.Text.IO asTIO
import qualified Data.Text.Encoding asTE
import qualified Data.ByteString.Lazy asBL
-- https://hackage.haskell.org/package/zlib
import qualified
Codec.Compression.GZip asGZip
We’re going to use the gzipcompression format to compress
some data. This is so we have an example of data that includes
bytes that aren’t a valid text encoding.
input::BL.ByteString
input=&quot;123&quot;
compressed ::BL.ByteString
compressed =GZip.compress input
TheGZipmodule expects a lazy ByteString , probably so that
it’s streaming friendly.</p>
<p>CHAPTER 28. BASIC LIBRARIES 1769
main::IO()
main= do
TIO.putStrLn $TE.decodeUtf8 (s input)
TIO.putStrLn $
TE.decodeUtf8 (s compressed)
wheres=BL.toStrict
Theencodingmoduleinthetextlibraryexpectsstrict ByteString s,
so we have to make them strict before attempting a decoding.
The second text decode will fail because there’ll be a byte that
isn’t recognizably correct as an encoding of text information.
ByteString traps
You might think to yourself at some point, “I’d like to convert
aString to aByteString !” This is a perfectly reasonable thing to
want to do, but many Haskellers will mistakenly use the Char8
module in the bytestring library when that is not really what
they want. The Char8module is really a convenience for data
that mingles bytes and ASCII data18there. It doesn’t work for
Unicode and shouldn’t be used anywhere there’s even a hint
of possibility that there could be Unicode data. For example:
18Since ASCII is 7 bits and Char8is 8 bits you could use the eighth bit to represent
Latin-1 characters. However, since you will usually intend to convert the Char8data to
encodings like UTF-8 and UTF-16 which use the eighth bit diﬀerently that would be
unwise.</p>
<p>CHAPTER 28. BASIC LIBRARIES 1770
moduleChar8ProllyNotWhatYouWant where
import qualified Data.Text asT
import qualified Data.Text.Encoding asTE
import qualified Data.ByteString asB
import qualified
Data.ByteString.Char8 asB8
-- utf8-string
import qualified
Data.ByteString.UTF8 asUTF8
-- Manual unicode encoding of Japanese text
-- GHC Haskell allows UTF8 in source files
s::String
s=&quot;\12371\12435\12395\12385\12399\12289<br />
\20803\27671\12391\12377\12363\65311 &quot;
utf8ThenPrint ::B.ByteString -&gt;IO()
utf8ThenPrint =
putStrLn .T.unpack.TE.decodeUtf8</p>
<p>CHAPTER 28. BASIC LIBRARIES 1771
throwsException ::IO()
throwsException =
utf8ThenPrint ( B8.pack s)
bytesByWayOfText ::B.ByteString
bytesByWayOfText =TE.encodeUtf8 ( T.pack s)
-- letting utf8-string do it for us
libraryDoesTheWork ::B.ByteString
libraryDoesTheWork =UTF8.fromString s
thisWorks ::IO()
thisWorks =utf8ThenPrint bytesByWayOfText
alsoWorks ::IO()
alsoWorks =
utf8ThenPrint libraryDoesTheWork
Then we go to run the code that attempts to get a ByteString
via theChar8module which contains Unicode:
Prelude&gt; throwsException
*** Exception: Cannot decode byte '\x93':
Data.Text.Internal.Encoding.decodeUtf8:
Invalid UTF-8 stream</p>
<p>CHAPTER 28. BASIC LIBRARIES 1772
You can use ordfromData.Char to get the Intvalue of the
byte of a character:
Prelude&gt; import Data.Char (ord)
Prelude&gt; :t ord
ord :: Char -&gt; Int
Prelude&gt; ord 'A'
65
Prelude&gt; ord '\12435'
12435
The second example seems obvious, but when the data is
represented natively on your computer this is more useful.
Use non-English websites to get sample data to test.
We can now use the ordering of characters to find the first
character that breaks Char8:
Prelude&gt; let xs = ['A'..'\12435']
Prelude&gt; let cs = map (:[]) xs
Prelude&gt; mapM_ (utf8ThenPrint . B8.pack) cs
... bunch of output ...
Then to narrow this down, we know we need to find what
came after the tilde and the \DELcharacter:
... some trial and error ...
Prelude&gt; let f = take 3 $ drop 60</p>
<p>CHAPTER 28. BASIC LIBRARIES 1773
Prelude&gt; mapM_ putStrLn (f cs)
}
~
Hrm, okay, but where is this in the ASCII table? We can
use the opposite of the ordfunction from Data.Char ,chrto
determine this:
Prelude&gt; import Data.Char (chr)
Prelude&gt; :t chr
chr :: Int -&gt; Char
Prelude&gt; map chr [0..128]
... prints the first 129 characters ...
What it printed corresponds to the ASCII table, which is
how UTF-8 represents the same characters. Now we can use
this function to narrow down precisely what our code fails at:
-- works fine
Prelude&gt; utf8ThenPrint (B8.pack [chr 127])
-- fails
Prelude&gt; utf8ThenPrint (B8.pack [chr 128])
*** Exception: Cannot decode byte '\x80':
Data.Text.Internal.Encoding.decodeUtf8:
Invalid UTF-8 stream</p>
<p>CHAPTER 28. BASIC LIBRARIES 1774
Don’t use Unicode characters with the Char8module! This
problem isn’t exclusive to Haskell — all programming lan-
guages must acknowledge the existence of diﬀerent encodings
for text.
Char8 is bad mmmmmkay TheChar8module is not for Uni-
code or for text more generally! The packfunction it contains
is for ASCII data only! This fools programmers because the
UTF-8 encoding of the English alphabet with some Latin ex-
tension characters intentionally overlaps exactly with the same
bytes ASCII uses to encode those characters. So the following
will work but is wrong in principle:
Prelude&gt; utf8ThenPrint (B8.pack &quot;blah&quot;)
blah
Getting a UTF-8 bytestring via the textorutf8-string li-
braries works fine, as you’ll see if you take a look at the result
ofthisWorks andalsoWorks .
When would I use ByteString instead of Text for
textual data?
This does happen sometimes, usually because you want to
keep data that arrived in a UTF-8 encoding in UTF-8. Of-
ten this happens because you read UTF-8 data from a file or
network socket and you don’t want the overhead of bouncing</p>
<p>CHAPTER 28. BASIC LIBRARIES 1775
it into and back out of Text. If you do this, you might want
to use newtypes to avoid accidentally mixing this data with
non-UTF-8 bytestrings.
28.10 Chapter Exercises
Diﬀerence List
Lists are really nice, but they don’t append or concatenate
cheaply. We covered Sequence as one potential solution to this,
but there’s a simpler data structure that solves slow appending
specifically, the diﬀerence list!
Rather than justify and explain diﬀerence lists, part of the
exercise is figuring out what it does and why (although feel
free to look up the documentation on Hackage). Attempt
the exercise before resorting to the tutorial in the follow-up
reading. First, the DListtype is built on top of ordinary lists,
but it’s a function:
newtype DLista=DL{ unDL::[a]-&gt;[a] }
The API that follows is based on code by Don Stewart and
Sean Leather. Here’s what you need to write:
1.empty::DLista
empty=undefined
{-# INLINE empty #-}</p>
<p>CHAPTER 28. BASIC LIBRARIES 1776
2.singleton ::a-&gt;DLista
singleton =undefined
{-# INLINE singleton #-}
3.toList::DLista-&gt;[a]
toList=undefined
{-# INLINE toList #-}
4.-- Prepend a single element to a dlist.
infixr<code>cons</code>
cons ::a-&gt;DLista-&gt;DLista
consx xs=DL((x:).unDL xs)
{-# INLINE cons #-}
5.-- Append a single element to a dlist.
infixl<code>snoc</code>
snoc::DLista-&gt;a-&gt;DLista
snoc=undefined
{-# INLINE snoc #-}
6.-- Append dlists.
append::DLista-&gt;DLista-&gt;DLista
append=undefined
{-# INLINE append #-}</p>
<p>CHAPTER 28. BASIC LIBRARIES 1777
What’s so nifty about DListis thatcons,snoc, andappend all
take the same amount of time no matter how long the dlist is.
That is to say, they take a constant amount of time rather than
growing with the size of the data structure.
Your goal is to get the following benchmark harness running
with the performance expected:
schlemiel ::Int-&gt;[Int]
schlemiel i=go i[]
wherego0xs=xs
go n xs =go (n-1) ([n]++xs)
constructDlist ::Int-&gt;[Int]
constructDlist i=toList$go i empty
wherego0xs=xs
go n xs =
go (n-1)
(singleton n <code>append</code> xs)
main::IO()
main=defaultMain
[ bench &quot;concat list&quot; $
whnf schlemiel 123456
, bench &quot;concat dlist&quot; $
whnf constructDlist 123456
]</p>
<p>CHAPTER 28. BASIC LIBRARIES 1778
If you run the above, the DListvariant should be about twice
as fast.
A simple queue
We’re going to write another data structure in terms of list, but
this time it’ll be a queue. The main feature of queues is that
we can add elements to the front cheaply and take items oﬀ
the back of the queue cheaply.
-- From Okasaki's Purely
-- Functional Data Structures
dataQueuea=
Queue{ enqueue ::[a]
, dequeue ::[a]
}deriving (Eq,Show)
-- adds an item
push::a-&gt;Queuea-&gt;Queuea
push=undefined
pop::Queuea-&gt;Maybe(a,Queuea)
pop=undefined
We’re going to give you less code this time, but your task is
to implement the above and write a benchmark comparing it</p>
<p>CHAPTER 28. BASIC LIBRARIES 1779
against performing alternating pushes and pops from a queue
based on a single list. Alternating so that you can’t take advan-
tage of reversing the list after a long series of pushes in order
to perform a long series of pops efficiently.
Don’t forget to handle the case where the dequeue is empty
and you need to shift items from the enqueue to the dequeue.
You need to do so without violating “first come, first served”.
Lastly, benchmark it against Sequence . Come up with a vari-
ety of tests. Add additional operations for your Queuetype if
you want.
28.11 Follow-up resources
1.Criterion tutorial; Bryan O’Sullivan
http://www.serpentine.com/criterion/tutorial.html
2.Demystifying DList; Tom Ellis
http://h2.jaguarpaw.co.uk/posts/demystifying-dlist/
3.Memory Management; GHC; Haskell Wiki
https://wiki.haskell.org/GHC/Memory_Management
4.Performance; Haskell Wiki
https://wiki.haskell.org/Performance
5.Pragmas, specifically UNPACK ; GHC Documentation</p>
<p>CHAPTER 28. BASIC LIBRARIES 1780
6.High Performance Haskell; Johan Tibell
http://johantibell.com/files/slides.pdf
7.Haskell Performance Patterns; Johan Tibell
8.Faster persistent data structures through hashing; Johan
Tibell
9.Lazy Functional State Threads; John Launchbury and
Simon Peyton Jones
10.Write Haskell as fast as C: exploiting strictness, laziness
and recursion; Don Stewart
11.Haskell as fast as C: A case study; Jan Stolarek
12.Haskell FFT 11: Optimisation Part 1; Ian Ross
13.Understanding the RealWorld; Edsko de Vries
14.Stream Fusion; Duncan Coutts
http://code.haskell.org/~dons/papers/icfp088-coutts.pdf
15.Purely functional data structures; Chris Okasaki</p>
<p>Chapter 29
IO
In those days, many
successful projects started
out as graffitis on a beer
mat in a very, very smoky
pub.
Peter J. Landin
1781</p>
<p>CHAPTER 29. IO 1782
29.1 IO
You should be proud of yourself for making it this far into the
book. You’re juggling monads. You’ve defeated an army of
monad transformers. You’re comfortable with using algebraic
structures in typeclass form. You’ve got a basic understand-
ing of how Haskell terms evaluate, nonstrictness, and sharing.
With those things in hand, let’s talk about IO.
We’ve used the IOtype at various times throughout the
book, with only cursory explanation. You no doubt know that
we use this type in Haskell as a means of keeping our chocolate
separate from our peanut butter — that is, our pure functions
from our eﬀectful ones. Perhaps you’re wondering how it all
works, what’s underneath that opaque type. To many people,
IOseems mysterious.
An eﬀectful function is one which has an observable impact
on the environment it is evaluated in, other than computing
and returning a result. Examples of eﬀects includes writing to
standard output (like putStrLn ), reading from standard input
(getChar ), or modifying state destructively ( ST). Implicit to this,
is that this almost always means the code requires it be evalu-
ated in a particular order. Haskell expressions which aren’t in
IOwill always return the same result regardless of what order
they are evaluated in; we lose this guarantee and others besides
onceIOis introduced.
Most explanations of the IOtype in Haskell don’t help much,</p>
<p>CHAPTER 29. IO 1783
either. They seem designed to confuse the reader rather than
convey anything useful. Don’t look now, but somebody is writ-
ing an explanation of IOright now that uses van Laarhoven
Free Monads and costate comonad coalgebras to explain some-
thing that’s much simpler than either of those topics.
We’re not going to do that here. We will instead try to
demystify IOa bit. The important thing about IOis that it’s a
special kind of datatype that disallows sharing in some cases.
In this chapter, we will
•explain how IOworks operationally;
•explore what it should mean to you when you read a type
that has IOin it;
•provide a bit more detail about the IO Functor ,Applicative ,
andMonad.
29.2 Where IO explanations go astray
A lot of explanations of IOare misleading or muddled. We’ve
already alluded to the overwrought and complex explanations
ofIO. Others call it “the IO Monad ” and seem to equate IOwith
Monad. While IOis a type that has a Monadinstance, it is not only
aMonadand monads are not only IO. Other presentations imply
that once you enter IO, you destroy purity and referential
transparency. And some references don’t bother to say much</p>
<p>CHAPTER 29. IO 1784
aboutIObecause the fact that it remains opaque won’t stop
you from doing most of what you want to use it for anyway.
We want to oﬀer some guidance in critically evaluating
explanations of IO. Let us consider one of the most popular
explanations of IO, the one that attempts to explain IOin terms
ofState.
Burn the State to the ground!
The temptation to use Stateto get someone comfortable with
the idea of IOis strong. Give the following passage early in the
documentation to GHC.IO a gander:
The IO Monad is just an instance of the ST monad,
where the state is the real world.
The motivation for these explanations is easy to understand
when you look at the underlying types:</p>
<p>CHAPTER 29. IO 1785
-- from ghc-prim
importGHC.Prim
importGHC.Types
newtype States a
=State{runState ::s-&gt;(a, s)}
-- :info IO
newtype IOa=
IO(State#RealWorld
-&gt;(#State#RealWorld , a#))
Yep, it sure looks like State! However, this is less meaningful
or helpful than you’d think at first.
The issue with this explanation is that you don’t usefully
see or interact with the underlying State#1inIO. It’s not State
in the sense that one uses State,StateT, or even ST, although
the behavior of the 𝑠here is certainly very like that of ST.
TheStatehere is a signalling mechanism for telling GHC
what order your IOactions are in and what a unique IOaction
is. If we look through the GHC.Prim documentation, we see:
State# is the primitive, unlifted type of states. It has
one type parameter, thus State# RealWorld, or State#
1The#indicates a primitive type. These are types that cannot be defined in Haskell
itself and are exported by the GHC.Prim module.</p>
<p>CHAPTER 29. IO 1786
𝑠, where 𝑠is a type variable. The only purpose of
the type parameter is to keep diﬀerent state threads
separate. It is represented by nothing at all.
RealWorld is deeply magical. It is primitive, but it
is not unlifted (hence ptrArg). We never manipulate
values of type RealWorld; it’s only used in the type
system, to parameterise State# .
When they say that RealWorld is “represented by nothing at
all,” they mean it literally uses zero bits of memory. The state
tokens underlying the IOtype are erased during compile time
and add no overhead to your runtime. So the problem with
explaining IOin terms of Stateis not precisely that it’s wrong;
it’s that it’s not a Stateyou can meaningfully interact with or
control in the way you’d expect from the other Statetypes.
29.3 The reason we need this type
So, let’s try to move from there to an understanding of IOthat
is meaningful to us in our day-to-day Haskelling. IOprimarily
exists to give us a way to order operations and to disable some
of the sharing that we talked so much about in the chapter on
nonstrictness.
GHC is ordinarily free to do a lot of reordering of opera-
tions, delaying of evaluation, sharing of named values, dupli-
cating code via inlining, and other optimizations in order to</p>
<p>CHAPTER 29. IO 1787
increase performance. The main thing the IOtype does is turn
oﬀ most of those abilities.
What?
No, really. That’s a lot of it.
Order and chaos
As we’ve seen in the previous chapters, GHC can normally
reorder operations. This is disabled in IO(as inST).IOactions
are instead enclosed within nested lambdas — nesting is the
only way to ensure that actions are sequenced within a pure
lambda calculus.
Nesting lambdas is how we guarantee that this
main= do
putStr &quot;1&quot;
putStr &quot;2&quot;
putStrLn &quot;3&quot;
will output “123” and we want that guarantee. The underly-
ing representation of IOallows the actions to be nested, and
therefore sequenced.
When we enter a lambda expression, any eﬀects that need to
beperformedwillbeperformedfirst, beforeanycomputations
are evaluated. Then if there is a computation to evaluate, that</p>
<p>CHAPTER 29. IO 1788
may be evaluated next, before we enter the next lambda to
perform the next eﬀect and so on. We’ve seen how this plays
out in previous chapters; think of the parsers that perform the
eﬀect of moving a “cursor” through the text without reducing
to any value; also recall what we saw with STand mutable
vectors.
In fact, the reason we have Monadis because it was a means
of abstracting away the nested lambda noise that underlies IO.
29.4 Sharing
In addition to enforcing ordering, IOturns oﬀ a lot of the shar-
ing we talked about in the nonstrictness chapter. As we’ll soon
see, it doesn’t disable all forms of sharing — it couldn’t, because
all Haskell programs have a mainaction with an obligatory IO
type. But we’ll get to that in a moment.
For now, let’s turn our attention to what sharing is disabled
and why. Usually in Haskell, we’re pretty confident that if a
function is going to be evaluated at all, it will result in a value
of a certain type, bearing in mind that this could be a Nothing
value or an empty list. When we declare the type, we say, “if
this is evaluated at all, we will have a value of this type as a
result.”
But with the IOtype, you’re not guaranteed anything. Values
oftypeIO aarenotan 𝑎; they’readescriptionofhowyoumight
get an𝑎. Something of type IO String is not a computation</p>
<p>CHAPTER 29. IO 1789
that, if evaluated, will result in a String; it’s a description of
how you might get that String from the “real world,” possibly
performing eﬀects along the way. Describing IOactions does
not perform them, just as having a recipe for a cake does not
give you a cake.2
In this environment, where you do not have a value but
only a means of getting a value, it wouldn’t make sense to say
that value could be shared.
The time has come
So, one of the key features of IOis that it turns oﬀ sharing.
Let’s use an example to think of why we want this. We have
this library function that gets the current UTC time from the
system clock:
-- from Data.Time.Clock
getCurrentTime ::IOUTCTime
Without IOpreventing sharing, how would this work? When
you fetched the time once, it would share that result, and the
time would be whatever time it was the first time you forced it.
Unfortunately, this is not a means of stopping time; we would
continue to age, but your program wouldn’t work at all the
way you’d intended.
2See Brent Yorgey’s explanation of IOfor the cis194 class at UPenn http://www.cis.
upenn.edu/~cis194/spring13/lectures/08-IO.html</p>
<p>CHAPTER 29. IO 1790
But if that’s so, and it’s clearly a value with a name that
could be shared, why isn’t it?
getCurrentTime ::IOUTCTime
-- ^-- that right there
Remember: what we have here is a description of how we
can get the current time when we need it. We do not have the
current time yet, so it isn’t a value that can be shared, and we
don’t want it to be shared anyway, because we want it to get a
new time each time we run it.
And the way we run it is by defining mainin that module
for the runtime system to find and execute. Everything inside
mainis within IOso that everything is nested and sequenced
and happens in the order you’re expecting.
Another example
Let’s look at another example of IOturning oﬀ sharing. You
remember the whnfandnffunctions from criterion that we
used in the last chapter. You may recall that we want to turn
oﬀ sharing for those so that they get evaluated over and over
again; if the result was shared, our benchmarking would only
tell us how long it takes to evaluate it once instead of giving us
an average of evaluating it many times. The way we disabled
sharing for those functions is by applying them to arguments.</p>
<p>CHAPTER 29. IO 1791
But the IOvariants of those functions do not require this
function application in order to disable sharing, because the
IOparameter itself disables sharing. Contrast the following
types:
whnf::(a-&gt;b)-&gt;a-&gt;Benchmarkable
nf::NFDatab
=&gt;(a-&gt;b)-&gt;a-&gt;Benchmarkable
whnfIO::IOa-&gt;Benchmarkable
nfIO::NFDataa=&gt;IOa-&gt;Benchmarkable
TheIOvariants don’t need a function argument to apply
because sharing is already prevented by being an IOaction —
it can be executed over and over without resorting to adding
an argument.
As we said earlier, IOdoesn’t turn oﬀ all sharing everywhere;
it couldn’t, or else sharing would be meaningless because main
is always in IO. But it’s important to understand when sharing
will be disabled and why, because if you’ve got this notion of
sharing running around in the back of your head you’ll have
the wrong intuitions for how Haskell code works. Which then
leads to…</p>
<p>CHAPTER 29. IO 1792
The code! It doesn’t work!
We’re going to use an example here that takes advantage of
theMVartype. This is based on a real code event that was how
Chris finally learned what IOmeans and the example he first
used to explain it to Julie.
TheMVartype is a means of synchronizing shared data in
Haskell. To give a very cursory overview, the MVarcan hold
one value at a time. You put a value into it; it holds onto it
until you take that value out. Then and only then can you
put another cat in the box.3We cannot hope to best Simon
Marlow’s work4on this front, so if you want more information
about it, we strongly recommend you peruse Marlow’s book.
OK, so we’ll set up some toy code here with the idea that
we want to put a value into an MVarand then take it back out:
3What you need is a cat gif. https://twitter.com/argumatronic/status/
631158432859488258
4Parallel &amp; Concurrent Programming in Haskell http://chimera.labs.oreilly.com/
books/1230000000929</p>
<p>CHAPTER 29. IO 1793
moduleWhatHappens where
importControl.Concurrent
myData::IO(MVarInt)
myData=newEmptyMVar
main::IO()
main= do
mv&lt;-myData
putMVar mv 0
mv'&lt;-myData
zero&lt;-takeMVar mv'
print zero
This will spew an error about being stuck or in a deadlock.
The problem here is that the type IO MVar a ofnewEmptyMVar is
a recipe for producing as many empty MVars as you need or
want; it is not a reference to a single, shared MVar. In other
words, the two references to myData here are not referring to
the same MVar.
Taking from an empty MVarblocks until something is put
into the MVar. Consider the following ordering:
take
put</p>
<p>CHAPTER 29. IO 1794
take
put
That will terminate successfully. An attempt to take a value
from the MVarblocked, then a value was put in it, then another
blocked take occurred, then there was another put to satisfy
the second take. This is fine.
The following is an example of something that will dead-
lock:
put
take
take
Whatever part of your program performed the second take
will now be blocked until a second put occurs. If your program
is designed such that no put ever occurs again, it’s deadlocked.
A deadlock error looks like the following:
Prelude&gt; main
*** Exception:
thread blocked indefinitely
in an MVar operation
When you see a type like:
IOString</p>
<p>CHAPTER 29. IO 1795
You don’t have a String; you have a means of (possibly)
obtaining a String , with some eﬀects possibly performed along
the way. Similarly, what happened earlier is that we had two
MVars with two diﬀerent lifetimes and that looked something
like this:
mv mv'
put take (the final one)
The point here is that this type
IO(MVara)
tells you that you have a recipe for producing as many
empty MVars as you want, not a reference to a single shared
MVar.
You can share the MVar, but it has to be done explicitly rather
than implicitly. Failing to explicitly share the MVarreference
after binding it once will simply spew out new, empty MVars.
Again, we recommend Simon Marlow’s book when you’re
ready to explore MVars in more detail.
29.5 IO doesn’t disable sharing for
everything
As we mentioned earlier, IOdoesn’t disable sharing for every-
thing, and it wouldn’t make sense if it did. It only disables</p>
<p>CHAPTER 29. IO 1796
sharing for the terminal value it reduces to. Values that are
not dependent on IOfor their evaluation can still be shared,
even within a larger IOaction such as main.
In the following example, we’ll use Debug.Trace again to show
us when things are being shared. For blah, thetraceis outside
theIOaction, so we’ll use outer trace :
importDebug.Trace
blah::IOString
blah=return&quot;blah&quot;
blah'=trace&quot;outer trace&quot; blah
And for woot, we’ll use inner trace inside the IOaction:
woot::IOString
woot=return (trace &quot;inner trace&quot; &quot;woot&quot;)
Then we throw both of them into a larger IOaction,main:</p>
<p>CHAPTER 29. IO 1797
main::IO()
main= do
b&lt;-blah'
putStrLn b
putStrLn b
w&lt;-woot
putStrLn w
putStrLn w
Prelude&gt; main
outer trace
blah
blah
inner trace
woot
woot
We only saw inner and outer emitted once because IOis not
intended to disable sharing for values not in IOthat happen to
be used in the course of running of an IOaction.
29.6 Purity is losing meaning
It’s common at this time to use the words “purely functional”
or to talk about purity when one means without eﬀects. This is
inaccurate and not very useful as a definition, but we’re going</p>
<p>CHAPTER 29. IO 1798
to provide some context here and an alternative understand-
ing.
Semantically, pedantically accurate
Purity and “pure functional” have undergone a few changes in
connotation and denotation since the 1950s. What was origi-
nally meant when describing a pure functional programming
language is that the semantics of the language would only be
lambda calculus. For quite a long time, impure functional lan-
guages were more typical. They admitted the augmentation of
lambda calculus, usually so that the means to describe imper-
ative, eﬀectful programs was embedded within the semantics.
The strength of Haskell is that by sticking to lambda calculus,
we not only have a much simpler core language for describing
our language, but we retain referential transparency in the
language. We use nested lambdas (hidden behind a Monadab-
straction) to order and encapsulate eﬀects while maintaining
referential transparency.
Referential transparency
Referential transparency is something you are probably fa-
miliar with, even if you’ve never called it that before. Put
casually, it means that any function, when given the same in-
puts, returns the same result. More precisely, an expression</p>
<p>CHAPTER 29. IO 1799
is referentially transparent when it can be replaced with its
value without changing the behavior of a program.
One source of the confusion between purity as referential
transparency and purity as pure lambda calculus could be that
in a pure lambda calculus, referential transparency is assured.
Thus, a pure lambda calculus is necessarily pure in the other
sense as well.
The mistake people make with IOis that they conflate the
eﬀects with the semantics of the program. A function that
returns IO ais still referentially transparent, because given the
same arguments, it’ll generate the same IOaction every time!
To make this point:
moduleIORefTrans where
importControl.Monad (replicateM )
importSystem.Random (randomRIO )
gimmeShelter ::Bool-&gt;IO[Int]
gimmeShelter True=
replicateM 10(randomRIO ( 0,10))
gimmeShelter False=pure [0]
The trick here is to realize that while executing IO [Int] can
and does produce diﬀerent literal values when the argument
isTrue, it’s still producing the same result (i.e., a list of ran-</p>
<p>CHAPTER 29. IO 1800
dom numbers) for the same input. Referential transparency is
preserved because we’re still returning the same IO action, or
“recipe,” for the same argument, the same means of obtaining
a list of Int. Every Trueinput to this function will return a list
of random Ints:
Prelude&gt; gimmeShelter True
[1,8,7,9,10,4,2,9,3,6]
Prelude&gt; gimmeShelter True
[10,0,7,1,10,2,4,0,9,3]
Prelude&gt; gimmeShelter False
[0]
The sense we’re trying to convey here is that as far as Haskell
is concerned, it’s a language for evaluating expressions and
constructing IOactions that get executed by mainat some point
later.
29.7 IO’s Functor, Applicative, and
Monad
Another mistake people make is in implying that IOis aMonad,
rather than accounting for the fact that, like all Monads,IOis
a datatype that has a Monadinstance — as well as Functor and
Applicative instances:</p>
<p>CHAPTER 29. IO 1801
fmap: construct an action which performs the same eﬀects
but transforms the 𝑎into a𝑏:
fmap::(a-&gt;b)-&gt;IOa-&gt;IOb
(&lt;<em>&gt;): construct an action that performs the eﬀects of both
the function and value arguments, applying the function to
the value:
(&lt;</em>&gt;)::IO(a-&gt;b)-&gt;IOa-&gt;IOb
join: merge the eﬀects of a nested IOaction:
join::IO(IOa)-&gt;IOa
The IO Functor
What does fmapmean with respect to IO? As always, we want
an example:
fmap(+1) (randomIO ::IOInt)
If we’re going to get that Intvalue, we will have to perform
some eﬀects. What fmapdoes here is lift our incrementing
function over the eﬀects that we might perform to obtain the
Intvalue. It doesn’t aﬀect the eﬀects, because the eﬀects here
are part of that IOstructure. Using fmaphere returns a recipe for
obtaining an Intthat also increments the result of the original
action that was lifted over.</p>
<p>CHAPTER 29. IO 1802
The key here is that we didn’t perform any eﬀects. We pro-
duced a new IOaction in terms of the old one by transforming
the final result of the IOaction.
Applicative and IO
IOalso has an Applicative instance, as we mentioned in the
Applicative chapter. You might remember an example like
this:
Prelude&gt; (++) &lt;$&gt; getLine &lt;<em>&gt; getLine
hello
julie
&quot;hellojulie&quot;
There we fmapped the concatenation operator over two
(potential) IO String s to produce the final result. Let’s look at
another, more interesting example:
(+)
&lt;$&gt;(randomIO ::IOInt)
&lt;</em>&gt;(randomIO ::IOInt)
After the initial fmap, we have a means of obtaining a func-
tion which is monoidally lifted over a means of obtaining an
Int. What this means is that you’ll get a single new means
of obtaining the result of having applied the function which
performs the eﬀects of both.</p>
<p>CHAPTER 29. IO 1803
Monad and IO
ForIO,pureorreturn can be read as an eﬀect-free embedding
of a value in a recipe-creating environment. Let’s consider the
following examples.
First, GHCi does basically two things: it can print values
not inIO, such as these:
Prelude&gt; &quot;I'll pile on the candy&quot;
&quot;I'll pile on the candy&quot;
Prelude&gt; 1
1
It can also run IOactions and print their results, if any. When
you have values of type IO (IO a) , what you have is a recipe
for making a recipe that produces an 𝑎. Consider why the
following example using printdoes not print anything:
Prelude&gt; :{
*Main| let embedInIO =
*Main| return :: a -&gt; IO a
*Main| :}
Prelude&gt; embedInIO 1
1
Prelude&gt; :{
*Main| let s =
*Main| &quot;I'll put in some ingredients&quot;</p>
<p>CHAPTER 29. IO 1804
*Main| :}
Prelude&gt; embedInIO (print s)
In order to merge those eﬀects and get a single IO awhich
will print a result in GHCi, we need join:
Prelude&gt; let s = &quot;It's a piece of cake&quot;
Prelude&gt; join $ embedInIO (print s)
&quot;It's a piece of cake&quot;
Prelude&gt; embedInIO (embedInIO 1)
Prelude&gt; join $ embedInIO (embedInIO 1)
1
What sets the IO Monad apart from the Applicative is that the
eﬀects performed by the outer IOaction can influence what
recipe you get in the inner part. The nesting also lets us express
order dependence, a useful trick for lambda calculi noted by
Peter J. Landin5.
An example for eﬀect:
5A correspondence between ALGOL 60 and Church’s Lambda-notations; P.J. Landin</p>
<p>CHAPTER 29. IO 1805
moduleNestedIO where
importData.Time.Calendar
importData.Time.Clock
importSystem.Random
huehue::IO(Either(IOInt) (IO()))
huehue= do
t&lt;-getCurrentTime
let(<em>,</em>, dayOfMonth) =
toGregorian (utctDay t)
caseeven dayOfMonth of
True-&gt;
return$LeftrandomIO
False-&gt;
return$
Right(putStrLn &quot;no soup for you&quot; )
TheIOaction we return here is contingent on having per-
formed eﬀects and observed whether the day of the month
was an even number6or an odd one. Note this is inexpressible
withApplicative . If you’d like a way to run it and see what
happens, try the following:
Prelude&gt; blah &lt;- huehue
6Why? Monad chapter’s long passed, we need something to be spooky.</p>
<p>CHAPTER 29. IO 1806
Prelude&gt; either (&gt;&gt;= print) id blah
-7077132465932290066
It was the 28th of January when we wrote this. Your mileage
may vary.
Monadic associativity
Haskellers will often get confused when they are told Monad’s
bind is associative because they’ll think of IOas a counterex-
ample. The mistake being made here is mistaking the con-
struction of IOactions for the execution of IOactions. As far
as Haskell is concerned, we only construct IOactions to be
executed when we call main. Semantically, IOactions aren’t
something we do, but something we talk about. Binding over
anIOaction doesn’t execute it, it produces a new IOaction in
terms of the old one.
You can reconcile yourself with this framing by remem-
bering how IOactions are like recipes, an analogy created by
Brent Yorgey that we’re fond of.
29.8 Well, then, how do we MVar?
Earlier in the chapter, we showed you an example of when
IOprevents sharing, using the MVartype. Our previous code
would block because the following:</p>
<p>CHAPTER 29. IO 1807
myData::IO(MVarInt)
myData=newEmptyMVar
is anIOaction that produces an empty MVar; it isn’t a stable
reference to a single given MVar. We have a couple ways of
fixing this. One is by passing the single stable reference as an
argument. The following will terminate successfully:
moduleWhatHappens where
importControl.Concurrent
main::IO()
main= do
mv&lt;-newEmptyMVar
putMVar mv ( 0::Int)
zero&lt;-takeMVar mv
print zero
There is a somewhat more evil and unnecessary way of
doing it. We’ll use this opportunity to examine an unsafe
means of enabling sharing for an IOaction: unsafePerformIO !
Consider that the following will also terminate:</p>
<p>CHAPTER 29. IO 1808
moduleWhatHappens where
importControl.Concurrent
importSystem.IO.Unsafe
myData::MVarInt
myData=unsafePerformIO newEmptyMVar
main::IO()
main= do
putMVar myData 0
zero&lt;-takeMVar myData
print zero
The type of unsafePerformIO isIO a -&gt; a , which is seemingly
impossible and not a good idea in general. In real code, you
should pass references to MVars as an argument or via ReaderT ,
but the combination of MVarandunsafePerformIO gives us an
opportunity to see in very stark terms what it means to use
unsafePerformIO in our code. The new empty MVarcan now be
shared implicitly, as often as you want, instead of creating a
new one each time.
Do not use unsafePerformIO when unnecessary or where it
could break referential transparency in your code! If you
aren’t sure — don’t use it! There are other unsafe IOfunctions,</p>
<p>CHAPTER 29. IO 1809
too, but there is rarely a need for any of them, and in general
you should prefer explicit rather than implicit.
29.9 Chapter Exercises
File I/O with Vigenère
ReusingtheVigenèrecipheryouwrotebackinalgebraicdatatypes
and wrote tests for in testing, make an executable that takes
a key and a mode argument. If the mode is -dthe executable
decrypts the input from standard in and writes the decrypted
text to standard out. If the mode is -ethe executable blocks
on input from standard input ( stdin) and writes the encrypted
output to stdout .
Consider this an opportunity to learn more about how file
handles and the following members of the baselibrary work:
System.Environment .getArgs ::IO[String]
System.IO.hPutStr
::Handle-&gt;String-&gt;IO()
System.IO.hGetChar ::Handle-&gt;IOChar
System.IO.stdout::Handle
System.IO.stdin::Handle
Whatever OS you’re on, you’ll need to learn how to feed
files as input to your utility and how to redirect standard out
to a file. Part of the exercise is figuring this out for yourself.</p>
<p>CHAPTER 29. IO 1810
You’ll want to use hGetChar more than once to accept a string
which is encrypted or decrypted.
Add timeouts to your utility
UsehWaitForInput to make your utility timeout if no input is
provided within a span of time of your choosing. You can
make it an optional command-line argument. Exit with a
nonzero error code and an error message printed to standard
error (stderr ) instead of stdout .
System.IO.hWaitForInput
::Handle-&gt;Int-&gt;IOBool
System.IO.stderr::Handle
Config directories
Reusing the INI parser from the Parsing chapter, parse a direc-
tory of INI config files into a Mapwhose key is the filename and
whose value is the result of parsing the INI file. Only parse
files in the directory that have the file extension .ini.
29.10 Follow-up resources
1.Referential Transparency; Haskell Wiki
https://wiki.haskell.org/Referential_transparency</p>
<p>CHAPTER 29. IO 1811
2.IO Inside; Haskell Wiki
https://wiki.haskell.org/IO_inside
3.Unraveling the mystery of the IO Monad; Edward Z. Yang
4.Primitive Haskell; Michael Snoyman
https://github.com/commercialhaskell/haskelldocumentation/
blob/master/content/primitive-haskell.md
5.Evaluation order and state tokens; Michael Snoyman
https://wiki.haskell.org/Evaluation_order_and_state_tokens
6.Haskell GHC Illustrated; Takenobu Tani
7.Tackling the Awkward Squad; Simon PEYTON JONES
http://research.microsoft.com/en-us/um/people/simonpj/papers/
marktoberdorf/mark.pdf
8.Note [IO hack in the demand analyser]; GHC source code
9.Monadic I/O in Haskell 1.3; Andrew D. Gordon and Kevin
Hammond
10.Notions of computation and monads; Eugenio Moggi
http://www.disi.unige.it/person/MoggiE/ftp/ic91.pdf
11.The Next 700 Programming Languages; P. J. Landin
12.Haskell Report 1.2</p>
<p>Chapter 30
When things go wrong
It is easier to write an
incorrect program than
understand a correct one
Alan Perlis
1812</p>
<p>CHAPTER 30. WHEN THINGS GO WRONG 1813
30.1 Exceptions
Let’s face it: in the execution of a program, a lot of things can
happen, not all of them expected or desired. In those unhappy
times when things have not gone as we wanted them to, we
will throw or raise an exception. The term exception refers
to the condition that has interrupted the expected execution
of the program. Encountering an exception causes an error,
or exception, message to appear, informing you that due to
some condition you weren’t prepared for, the execution of the
program has halted in an unfortunate way.
In previous chapters, we’ve covered ways of using Maybe,
Either , andValidation types to handle certain error conditions
explicitly. Raising exceptional conditions via such datatypes
isn’t always ideal, however. In some cases, exceptions can
be faster by eliding repeated checks for an adverse condition.
Exceptions are not explicitly part of the interfaces you’re using,
and that has immediate consequences when trying to reason
about the ways in which your program could fail.
Letting exceptions arise as they will — and the program
halt willy-nilly — is suboptimal. Exception handling is a way
of dealing with errors and giving the program some alternate
means of execution or termination should one arise. This
chapter is going to cover both exceptions and what they look
like as well as various means of handling them.
In this chapter, we will:</p>
<p>CHAPTER 30. WHEN THINGS GO WRONG 1814
•examine the Exception typeclass and methods;
•dip our toes into existential quantification;
•discuss ways of handling exceptions.
30.2 The Exception class and methods
Exceptions are plain old types and values like you’ve seen
throughout the book. The types that encode exceptions must
have an instance of the Exception typeclass. The origins of
exceptions as they exist in Haskell today are in Simon Mar-
low’s work on an extensible hierarchy of exceptions1which
are discriminated at runtime. Using this extensible hierarchy
allows you to both catch exceptions that may have various
types and also add new exception types as the need arises.
TheException typeclass definition looks like this:
class(Typeable e,Showe)=&gt;
Exception ewhere
toException ::e-&gt;SomeException
fromException ::SomeException -&gt;Maybee
displayException ::e-&gt;String
-- Defined in ‘GHC.Exception’
1http://community.haskell.org/~simonmar/papers/ext-exceptions.pdf</p>
<p>CHAPTER 30. WHEN THINGS GO WRONG 1815
We’ll take a look at those methods in a moment. The Show
constraint is there so that we can print the exception to the
screen in a readable form for whatever type 𝑒ends up being.
Typeable is a typeclass that defines methods of identifying types
at runtime. We will talk about this more and explain why these
constraints are necessary to our Exception class soon.
The list of types that have an Exception instance is long:
-- some instances elided
instance Exception IOException
instance Exception Deadlock
instance Exception BlockedIndefinitelyOnSTM
instance
Exception BlockedIndefinitelyOnMVar
instance Exception AsyncException
instance Exception AssertionFailed
instance Exception AllocationLimitExceeded
instance Exception SomeException
instance Exception ErrorCall
instance Exception ArithException
We won’t talk in detail about each of these, but you may be
able to figure out what, for example, BlockedIndefinitelyOnMVar
is used for. We’ll note that it’s simply a datatype with one
inhabitant:</p>
<p>CHAPTER 30. WHEN THINGS GO WRONG 1816
dataBlockedIndefinitelyOnMVar =
BlockedIndefinitelyOnMVar
-- Defined in ‘GHC.IO.Exception’
If we look at ArithException , we’ll find that it’s a sum type
with several values:
dataArithException
=Overflow
|Underflow
|LossOfPrecision
|DivideByZero
|Denormal
|RatioZeroDenominator
instance Exception ArithException
If you import the Control.Exception module, you can poke
atArithException ’s data constructors and see that they’re plain
old values, nothing unusual at all.
But there is something diﬀerent going on here
We’re going to start unpacking all this to see how the parts
work together. First, let’s take a look at the methods of the
Exception typeclass:</p>
<p>CHAPTER 30. WHEN THINGS GO WRONG 1817
toException ::e-&gt;SomeException
fromException ::SomeException -&gt;Maybee
We don’t have much occasion to use the toException and
fromException functions themselves. Instead, we use other func-
tions that call them for us. As it turns out, the toException
methodisquitesimilartothedataconstructorfor SomeException .
You may have noticed that SomeException is also a type that is
listed as having an instance of the Exception typeclass, and now
here it is in the Exception methods. It seems a bit circular, but
it turns out that SomeException is ultimately the key to the way
we handle exceptions.
A brief introduction to existential quantification
SomeException acts as a sort of parent type for all the other ex-
ception types, so that we can handle many exception types at
once, without having to match all of them. Let’s examine how:
dataSomeException where
SomeException
::Exception e=&gt;e-&gt;SomeException
This may not seem odd at first glance. That is due, in part,
to the fact that the weirdness is hiding in a construction called
a GADT, for generalized algebraic datatype. For the most</p>
<p>CHAPTER 30. WHEN THINGS GO WRONG 1818
part, GADTs are out of the scope of this book, being well
into intermediate Haskell territory that is fun to explore but
not strictly necessary to programming in Haskell. What the
GADT syntax is hiding there is something called existential
quantification.
We could rewrite the SomeException type like this without a
change in meaning:
dataSomeException =
forall e .Exception e=&gt;SomeException e
Ordinarily, the forall quantifies variables universally, as
you might guess from the word all. However, the SomeException
type constructor doesn’t take an argument; the type variable
𝑒is a parameter of the data constructor. It takes an 𝑒and
results in a SomeException . Moving the quantifier to the data
constructor limits the scope of its application, and changes
the meaning from for all e to there exists some e. That is exis-
tential quantification. It means that any type that implements
theException class can be that 𝑒and be subsumed under the
SomeException type.
We aren’t going to examine existential quantification deeply
here; this is a mere taste. Usually when type constructors are
parameterized, they are universally quantified. Arguments
have to be supplied to satisfy them. Your Maybe a type is, as</p>
<p>CHAPTER 30. WHEN THINGS GO WRONG 1819
we’ve noted before, a sort of function waiting for an argument
to be supplied to be a fully realized type.
Butwhenweexistentiallyquantifyatype, aswith SomeException ,
we can’t do much with that polymorphic type variable in its
data constructor. We can’t concretize it. Other than adding
constraints, we can’t know anything about it. It must remain
polymorphic, and we can cram any value of any type that im-
plements its constraint into that role. It’s like a polymorphic
parasite just hanging out on your type.
So, any exception type — any type with an instance of
theException typeclass — can be that 𝑒and be handled as a
SomeException . We need Typeable andShowin order to determine
what type of exception we’re dealing with, as we will soon see.
So, wait, what?
For an example of what existential quantification lets us do,
we’re going to show you an example that doesn’t rely on the
magic of the runtime exception machinery. Here we’ll be
returning errors in Either of totally diﬀerent types without
having to unify them under a single sum type:</p>
<p>CHAPTER 30. WHEN THINGS GO WRONG 1820
{-# LANGUAGE ExistentialQuantification #-}
{-# LANGUAGE GADTs #-}
moduleWhySomeException where
importControl.Exception
(ArithException (..)
,AsyncException (..))
importData.Typeable
dataMyException =
forall e .
(Showe,Typeable e)=&gt;MyException e
instance ShowMyException where
showsPrec p ( MyException e)=
showsPrec p e</p>
<p>CHAPTER 30. WHEN THINGS GO WRONG 1821
multiError ::Int
-&gt;EitherMyException Int
multiError n=
casenof
0-&gt;
Left(MyException DivideByZero )
1-&gt;
Left(MyException StackOverflow )
_ -&gt;Rightn
What’s special about the above is that we have a Leftcase
in ourEither that includes error values of two totally diﬀerent
types without enumerating them in a sum type. MyException
doesn’t appear to have a polymorphic argument in the type
constructor, but it does in the data constructor. We are able
to apply the MyException data constructor to values of diﬀerent
types because of the existentially quantified type for 𝑒.
dataSomeError =
ArithArithException
|AsyncAsyncException
|SomethingElse
deriving (Show)</p>
<p>CHAPTER 30. WHEN THINGS GO WRONG 1822
discriminateError ::MyException
-&gt;SomeError
discriminateError (MyException e)=
casecast eof
(Justarith)-&gt;Aritharith
Nothing -&gt;
casecast eof
(Justasync)-&gt;Asyncasync
Nothing -&gt;SomethingElse
runDisc n=
either discriminateError
(constSomethingElse ) (multiError n)
Then trying this out:
Prelude&gt; runDisc 0
Arith divide by zero
Prelude&gt; runDisc 1
Async stack overflow
Prelude&gt; runDisc 2
SomethingElse
This is the essence of why we need existential quantification
for exceptions — so that we can throw various exception types
without being forced to centralize and unify them under a
sum type. Don’t abuse this facility.</p>
<p>CHAPTER 30. WHEN THINGS GO WRONG 1823
Prior to this design, there were a few ways you could do
exception handling. Some of the more apparent methods
would’ve been one big sum type or strings. The problem is
that neither of them are meaningfully extensible to structured,
proper data types. We want, in a sense, a hierarchy of values
where catching a “parent” means catching any of the possible
“children.” The combination of SomeException and the Typeable
typeclass gives you a means of throwing diﬀerent exceptions
of diﬀerent types and then catching some or all of them in a
handler without wrapping them in a sum type.
Typeable
TheTypeable typeclasslivesinthe Data.Typeable module. Typeable
exists to permit types to be known at runtime, allowing for a
sort of dynamic typechecking. It allows you to learn the type
of a value at runtime and also to compare the types of two val-
ues and check that they are the same. Typeable is particularly
useful when you have code that needs to allow various types
to be passed to it but needs to enforce or trigger on specific
types.
This is ordinarily unwise, but it makes sense when you’re
talking about exceptions. When we’re concerned with excep-
tion handling, we want to be able to check whether values of
possibly varying types match the Exception type we’re trying to
handle, and we need to do that at runtime, when the exception</p>
<p>CHAPTER 30. WHEN THINGS GO WRONG 1824
occurs. Thus we need this runtime witness to the types of the
exceptions.
Let’s look at a method called cast, simplified from its im-
plementation in base:
cast::(Typeable a,Typeable b)
=&gt;a-&gt;Maybeb
We don’t usually call this function directly, but it gets called
for us by the fromException function, and fromException is called
by thecatchfunction.
At runtime, when an exception is thrown, it starts rolling
back through the stack, looking for a catch. When it finds a
catch, it checks to see what type of exception this catchcatches.
It callsfromException andcastto check if the type of the excep-
tion that got thrown matches the type of an exception we’re
handling with the catch. Acatchthat handles a SomeException
will match any type of exception, due to the flexibility of that
type.
If they don’t match, we get a Nothing value; the exception
will keep rolling up through the stack, looking for a catchthat
can handle the exception that was thrown. If it doesn’t find
one, your program just dies an unseemly death.
If they do match, the Just a allows us to catch the exception.</p>
<p>CHAPTER 30. WHEN THINGS GO WRONG 1825
30.3 This machine kills programs
Exceptions can result from pure code:
Prelude&gt; 2 <code>div</code> 0
*** Exception: divide by zero
However, running code is an I/O action (and GHCi is implic-
itly invoking IO), so most of the time when you need to worry
about exceptions, you’ll be in IO. Even when they happen in
pure code, exceptions may only be caught, or handled, in IO.
IOcontains the implicit contract, “You cannot expect this
computation to succeed unconditionally.” It turns out the
outside world is a harsh mistress — just about any IOaction
can fail, even putStrLn .
First, let’s demonstrate that any I/O action can fail. We will
assume that you do not currently have a file called aaain your
working directory. So, when you run this code, it will create
the file, write to it, print “wrote to file” in your terminal and
terminate successfully:
-- writePls.hs
moduleMainwhere
main= do
writeFile &quot;aaa&quot;&quot;hi&quot;
putStrLn &quot;wrote to file&quot;</p>
<p>CHAPTER 30. WHEN THINGS GO WRONG 1826
You can fire up your REPL and load that, or you can compile
the binary like this (this is review, so if you already have all
this down, then go ahead and do it):
stack ghc -- <filename> -o <output file name>
And run it like this:
$ ./<output file name>
So, if you called the output file wp, for example, your termi-
nal session might look like this:
$ stack ghc -- writepls.hs -o wp
[stack compilation messages]
$ ./wp
wrote to file
$ cat aaa
hi
Cool, that all worked. That worked in part because writeFile
will go ahead and create a file and give it write permissions
if the file you’re trying to write to does not exist. But what if
you’re trying to write to a file that does already exist and does
not have write permissions?
Make a read-only file named zzzthat we can experiment
with. To make a file that cannot be written to on Linux or OSX,
the following suffices:</p>
<p>CHAPTER 30. WHEN THINGS GO WRONG 1827
$ touch zzz
$ chmod 400 zzz
Suppose that file cohabits a directory where we’re trying to
execute this program:
-- writePls.hs
moduleMainwhere
main= do
writeFile &quot;zzz&quot;&quot;hi&quot;
putStrLn &quot;wrote to file&quot;
It’s the same program we had for the aaafile, just with the
file name changed. You can fire up your REPL and load that,
or you can compile the binary as we did above.
Then, if you run this program with such a file, you’ll get the
following result:
$ ./wp
wp: zzz: openFile: permission denied (Permission denied)
There’s a hole in our bucket, dear Liza: an exception.
Catch me if you can
Let’s fix that, dear Henry. We’ll start with some rudimentary
exception handling:</p>
<p>CHAPTER 30. WHEN THINGS GO WRONG 1828
moduleMainwhere
importControl.Exception
importData.Typeable
handler ::SomeException -&gt;IO()
handler (SomeException e)= do
print (typeOf e)
putStrLn ( &quot;We errored! It was: &quot;
++show e)
main=
writeFile &quot;zzz&quot;&quot;hi&quot;
<code>catch</code> handler
We’re still going to terminate without writing to the file, for
the same reasons as above. The program will run and termi-
nate successfully, but it’ll mention the error and say that it
failed with an IOException . We’ll get a bit more information
about why the program failed and be able to log that informa-
tion with our exception handler if we wish. Sometimes, that’s
exactly what you want: for your program to log the exception
and then die. Soon, we’ll look at some other options for han-
dling exceptions in a way that lets your program proceed with
an alternate execution.</p>
<p>CHAPTER 30. WHEN THINGS GO WRONG 1829
For now, let’s turn our attention to catch:
catch::Exception e
=&gt;IOa
-&gt;(e-&gt;IOa)
-&gt;IOa
You may recall we mentioned catchearlier because it calls
fromException andcastfor us. It runs only if the exception
matching the type you specified gets thrown, and it gives you
an opportunitity to recover from the error and still satisfy
the original type that your IOaction purported to be. If no
exception gets thrown, then nothing happens with that 𝑒and
theIO aat the front is the same as the IO aat the end.
Let’s expand our rudimentary error handling in a way that
allows the program an alternate execution method instead
of allowing it to die. This time, the mainaction still wants to
write to that read-only file, but this time our handler gives
it an alternate file that does not exist to write to (if you do
have a file called bbbin your present working directory, you
can change the name of the writeFile argument to some other
name, anything as long as it doesn’t exist in your directory
yet):</p>
<p>CHAPTER 30. WHEN THINGS GO WRONG 1830
-- writePls.hs
moduleMainwhere
importControl.Exception
importData.Typeable
handler ::SomeException -&gt;IO()
handler (SomeException e)= do
putStrLn ( &quot;Running main caused an error! <br />
\It was: &quot;
++show e)
writeFile &quot;bbb&quot;&quot;hi&quot;
main=
writeFile &quot;zzz&quot;&quot;hi&quot;
<code>catch</code> handler
When writing to zzzfails, it should print the error message
to the terminal. If you check your directory, you should see
your alternate file, named in the handler function, and if you
look inside that, it should say “hi” to you.
Let’s look at another, slightly more complex, use of catch.
This is taken from a program that deletes things from a Twitter
account and relies on the library twitter-conduit .2This portion
2https://www.stackage.org/package/twitter-conduit</p>
<p>CHAPTER 30. WHEN THINGS GO WRONG 1831
of the program can fail when it doesn’t have access to the
appropriate credentials for talking to a Twitter account. So,
we built an exception handler that tells it what to do when that
exception arises:
withCredentials action= do
twinfo&lt;-
loadCredentials <code>catch</code> handleMissing
casetwinfoof
Nothing -&gt;
getTWInfo &gt;&gt;=saveCredentials
Justtwinfo-&gt;action twinfo
wherehandleMissing ::IOException
-&gt;IO(MaybeTWInfo)
handleMissing _ =returnNothing
We turn an IOException into an IO (Maybe a) so we can case
on the Maybeto tell it what to do in the Nothing case. In this
case, if we throw an IOException and return a Nothing value, our
program will execute this:
getTWInfo &gt;&gt;=saveCredentials
By saving the credentials (the code that does the saving is
not shown here), we hopefully won’t encounter this exception
the next time we try to run it. In which case, we perform the</p>
<p>CHAPTER 30. WHEN THINGS GO WRONG 1832
action that is named in the Just twinfo line (said action is also
not shown here, sorry!).
30.4 Want either? Try!
Sometimes we’d like to lift exceptions out into explicit Either
values. This is quite doable, but you can’t erase the fact that
you performed I/O in the process. It’s also no guarantee you’ll
catch all exceptions. Here’s the function we need to turn im-
plicit exceptions into an explicit Either :
-- Control.Exception
try::Exception e
=&gt;IOa
-&gt;IO(Eithere a)
Then to use it, we could write something like the following
code (please note, this will not compile to a binary the way
earlier examples did because it is not a Mainexecutable; use
GHCi):</p>
<p>CHAPTER 30. WHEN THINGS GO WRONG 1833
moduleTryExcept where
importControl.Exception
willIFail ::Integer
-&gt;IO(EitherArithException ())
willIFail denom=
try$print$div5denom
Here we print the result because you can only handle ex-
ceptions in IO, evidenced by the types of tryandcatch. If you
feed this some inputs, you’ll see something like the following:
Prelude&gt; willIFail 1
5
Right ()
Prelude&gt; willIFail 0
Left divide by zero
One thing to keep in mind is that exceptions in Haskell are
like exceptions in most other programming languages — they
are imprecise. An exception not caught by a particular bit of
code will get rolled up by the exception until it’s either caught
or kills your program.
If you wanted to get rid of the Right () that it’s printing in
the successful cases, here’s one way to get rid of it:</p>
<p>CHAPTER 30. WHEN THINGS GO WRONG 1834
onlyReportError ::Showe
=&gt;IO(Eithere a)
-&gt;IO()
onlyReportError action= do
result&lt;-action
caseresultof
Lefte-&gt;print e
Right_ -&gt;return()
willFail ::Integer -&gt;IO()
willFail denom=
onlyReportError $willIFail denom
Or you could use catch:
willIFail' ::Integer -&gt;IO()
willIFail' denom=
print (div 5denom) <code>catch</code> handler
wherehandler ::ArithException
-&gt;IO()
handler e =print e
Let’s expand on this. We want to take the above examples
and turn them into an executable binary, which is a problem,
because in an executable, maincan’t take arguments. So, we’ll
have to do some serious modification in order to be able to</p>
<p>CHAPTER 30. WHEN THINGS GO WRONG 1835
pass arguments to mainwhen we call it. We’re going to import
System.Environment so that we can make use of a function called
getArgs that allows us to pass arguments in at the point where
we callmain:
moduleMainwhere
importControl.Exception
importSystem.Environment (getArgs)
willIFail ::Integer
-&gt;IO(EitherArithException ())
willIFail denom=
try$print$div5denom
onlyReportError ::Showe
=&gt;IO(Eithere a)
-&gt;IO()
onlyReportError action= do
result&lt;-action
caseresultof
Lefte-&gt;print e
Right_ -&gt;return()</p>
<p>CHAPTER 30. WHEN THINGS GO WRONG 1836
testDiv ::String-&gt;IO()
testDiv d=
onlyReportError $willIFail (read d)
main::IO()
main= do
args&lt;-getArgs
mapM_ testDiv args
The use of mapM_here might not be obvious, so let’s unpack
that a bit. It is essentially a less general traverse function that
throws away its end result and only produces the eﬀects. In this
case, those eﬀects are going to be the results of mapping our
testDiv function over a list of arguments — returning either
the result of a successful division or the type of an exception.
We’ll compile this one to an executable binary again, as we
did earlier in the chapter. To pass in the arguments, it will
look like this:
$ stack ghc -- writepls.hs -o wp
[stack noise]
$ ./wp 4 5 0 9
1
1
divide by zero
0</p>
<p>CHAPTER 30. WHEN THINGS GO WRONG 1837
IncaseyouwantedtotrythisintheREPL,reproducingwhat
you did above, use the indexmain@ :mainGHCi command and
pass the same arguments.
Prelude&gt; :main 4 5 0 9
1
1
divide by zero
0
Notice that, now that the exception is handled, we can still
get that last result — we have survived an ArithException !
30.5 The unbearable imprecision of
trying
Let’s do another little experiment:
importControl.Exception
canICatch ::Exception e
=&gt;e
-&gt;IO(EitherArithException ())
canICatch e=
try$throwIO e</p>
<p>CHAPTER 30. WHEN THINGS GO WRONG 1838
The new thing here is throwIO , a function that allows you
to throw an exception. Right now we want to demonstrate
that this handler doesn’t catch all types of exceptions, so we’re
usingthrowIO to cause exceptions of various types to be thrown.
TheLefthere can only handle or catch an ArithException ,
not any other kind. So when we throw a diﬀerent type of
exception, we get the following:
Prelude&gt; canICatch DivideByZero
Left divide by zero
Prelude&gt; canICatch StackOverflow
*** Exception: stack overflow
Prelude&gt; :t DivideByZero
DivideByZero :: ArithException
Prelude&gt; :t StackOverflow
StackOverflow :: AsyncException
The latter case blew past our trybecause we were trying to
catch an ArithException , not an AsyncException .
We’vementionedseveraltimesthat SomeException willmatch
on all types that implement the Exception typeclass, so try
rewriting the above such that the StackOverflow or any other
exception can also be caught.
We’ll continue the experiment by making a program that
runs until an unhandled exception stops the party:</p>
<p>CHAPTER 30. WHEN THINGS GO WRONG 1839
moduleStoppingTheParty where
importControl.Concurrent (threadDelay )
importControl.Exception
importControl.Monad (forever)
importSystem.Random (randomRIO )
randomException ::IO()
randomException = do
i&lt;-randomRIO ( 1,10::Int)
ifi <code>elem</code> [ 1..9]
thenthrowIO DivideByZero
elsethrowIO StackOverflow
main::IO()
main=forever $ do
lettryS::IO()
-&gt;IO(EitherArithException ())
tryS=try
_ &lt;-tryS randomException
putStrLn &quot;Live to loop another day!&quot;
-- microseconds
threadDelay ( 1*1000000)
We’ve talked about forever before; it causes the program
execution to loop indefinitely. We have added the threadDelay</p>
<p>CHAPTER 30. WHEN THINGS GO WRONG 1840
to slow the looping down so that what’s happening is more
noticeable. Note that the thread is delayed by a number of
microseconds.
ThetrySallows it to survive the ArithException s. We throw
away those exceptions and keep looping, but we can only throw
away the exception that we matched on ( ArithException ). At
some point, when our random number is 10, we will throw an
AsyncException instead of an ArithException , and our program
will die a rapid death. Try modifying this one so that both
exceptions are handled and the loop never terminates.
30.6 Why throwIO?
It may have seemed odd to you (or not!) to encounter throwIO
above. Why do we want to stop a program by purposely throw-
ing an exception? In the real world, we often do want to do
that — to stop the program when some condition occurs, but
it may be difficult to see that from what we’ve shown you so
far.
There’s a function called throwthat allows exceptions, such
as the arithmetic exceptions, but you rarely use it. It’s what
allows the divfunction to throw a DivideByZero exception when
that happens, but outside of such library functions, you don’t
need it.
The diﬀerence between throwandthrowIO can be seen in the
type:</p>
<p>CHAPTER 30. WHEN THINGS GO WRONG 1841
throwIO ::Exception e=&gt;e-&gt;IOa
Partiality in the form of throwing an exception can be
thought of as an eﬀect. The conventional way to throw an
exception is to use throwIO , which has IOin its result. This is
the same thing as throw, butthrowIO embeds the exception in
IO. You always handle exceptions in IO3. Handling exceptions
must be done in IOeven if they were thrown without an IOtype.
You almost never want throwas it throws exceptions without
any warning in the type, even IO.
We’ll look at an example of an unconditionally thrown ex-
ception in IOso you can see how it aﬀects the control flow of
your program:
importControl.Exception
main::IO()
main= do
throwIO DivideByZero
putStrLn &quot;lol&quot;
Prelude&gt; main
*** Exception: divide by zero
Likethrow,throwIO is often called for us, behind the scenes,
by library functions. Often, in interacting with the real world,
3Why? Because catching and handling exceptions means you could produce diﬀerent
results from the same inputs. That breaks referential transparency.</p>
<p>CHAPTER 30. WHEN THINGS GO WRONG 1842
we need to tell our program that in certain conditions, we want
it to stop or to give us an error message and let us know things
went wrong. We’ll take a look at a couple of examples from
real code, a library called http-client4by Michael Snoyman,
that uses throwIO to throw some exceptions when httpthings
haven’t gone the way we wanted them to:
connectionReadLine ::Connection
-&gt;IOByteString
connectionReadLine conn= do
bs&lt;-connectionRead conn
when (S.null bs) $
throwIO IncompleteHeaders
connectionReadLineWith conn bs
In the above, throwIO will throw an IncompleteHeaders excep-
tion when the ByteString header is empty. In the next example,
it’s used to throw a ResponseTimeout exception when, well, the
response times out:
4https://www.stackage.org/package/http-client</p>
<p>CHAPTER 30. WHEN THINGS GO WRONG 1843
parseStatusHeaders ::Connection
-&gt;MaybeInt
-&gt;Maybe(IO())
-&gt;IOStatusHeaders
parseStatusHeaders conn timeout' cont
|Justk&lt;-cont=
getStatusExpectContinue k
|otherwise =
getStatus
where
withTimeout = casetimeout' of
Nothing -&gt;id
Justt-&gt;
timeout t &gt;=&gt;
maybe
(throwIO ResponseTimeout )
return
-- ... other code elided ...
You can use http-client without worrying about how he
makes the exceptions happen. But let’s next take a look at
making our own exception types for those times when you
do need to worry about it. Keep in mind that since time of
writing, http-client has changed how it defines and throws
exceptions, but the examples should still be useful.</p>
<p>CHAPTER 30. WHEN THINGS GO WRONG 1844
30.7 Making our own exception types
Often we’ll want our own exception types, like http-client has.
They enable us to be more precise about what’s going on in
our program. Let’s work through a small example to emit one
of a couple diﬀerent possible errors in an otherwise simple
function to see how we could do this:
moduleOurExceptions where
importControl.Exception
dataNotDivThree =
NotDivThree
deriving (Eq,Show)
instance Exception NotDivThree
dataNotEven =
NotEven
deriving (Eq,Show)
instance Exception NotEven
Note here that Exception instances are derivable — you don’t
need to write an instance. Continuing on:</p>
<p>CHAPTER 30. WHEN THINGS GO WRONG 1845
evenAndThreeDiv ::Int-&gt;IOInt
evenAndThreeDiv i
|rem i3/=0=throwIO NotDivThree
|odd i=throwIO NotEven
|otherwise =return i
Then we’ll see the error and success conditions:
*OurExceptions&gt; evenAndThreeDiv 0
0
*OurExceptions&gt; evenAndThreeDiv 1
*** Exception: NotDivThree
*OurExceptions&gt; evenAndThreeDiv 2
*** Exception: NotDivThree
*OurExceptions&gt; evenAndThreeDiv 3
*** Exception: NotEven
*OurExceptions&gt; evenAndThreeDiv 6
6
*OurExceptions&gt; evenAndThreeDiv 9
*** Exception: NotEven
*OurExceptions&gt; evenAndThreeDiv 12
12
There is an issue with this setup, although it’s common.
What if we want to know what input or inputs caused the
error? We need to add context!</p>
<p>CHAPTER 30. WHEN THINGS GO WRONG 1846
Adding context
Convenient subsection titling! Anyhow, let’s modify that:
moduleOurExceptions where
importControl.Exception
dataNotDivThree =
NotDivThree Int
deriving (Eq,Show)
instance Exception NotDivThree
dataNotEven =
NotEven Int
deriving (Eq,Show)
instance Exception NotEven
evenAndThreeDiv ::Int-&gt;IOInt
evenAndThreeDiv i
|rem i3/=0=throwIO ( NotDivThree i)
|odd i=throwIO ( NotEven i)
|otherwise =return i
Now when we get errors, we can know what input caused
the error:</p>
<p>CHAPTER 30. WHEN THINGS GO WRONG 1847
*OurExceptions&gt; evenAndThreeDiv 12
12
*OurExceptions&gt; evenAndThreeDiv 9
*** Exception: NotEven 9
*OurExceptions&gt; evenAndThreeDiv 8
*** Exception: NotDivThree 8
*OurExceptions&gt; evenAndThreeDiv 3
*** Exception: NotEven 3
*OurExceptions&gt; evenAndThreeDiv 2
Catch one, catch all
Now, you can probably figure out how to catch these two dif-
ferent errors:
catchNotDivThree ::IOInt
-&gt;(NotDivThree -&gt;IOInt)
-&gt;IOInt
catchNotDivThree =catch
catchNotEven ::IOInt
-&gt;(NotEven -&gt;IOInt)
-&gt;IOInt
catchNotEven =catch
Or perhaps with try:</p>
<p>CHAPTER 30. WHEN THINGS GO WRONG 1848
Prelude&gt; type EA e = IO (Either e Int)
Prelude&gt; try (evenAndThreeDiv 2) :: EA NotEven
*** Exception: NotDivThree 2
Prelude&gt; try (evenAndThreeDiv 2) :: EA NotDivThree
Left (NotDivThree 2)
Thetypesynonymisn’tsemanticallyimportant, butitshrinks
the noise a bit. Now, you could handle both errors with the
catches function:
catches ::IOa-&gt;[Handler a]-&gt;IOa
catchBoth ::IOInt
-&gt;IOInt
catchBoth ioInt=
catches ioInt
[Handler
((NotEven _)-&gt;return maxBound)
,Handler
((NotDivThree _)-&gt;return minBound)
]
ThemaxBound /minBound thingisnotgoodcodeforrealuse, just
a convenience. Incidentally, the same trick the SomeException
type uses to hide type arguments is used by the Handler type
to wrap the values in the list of exception handlers: existential
quantification.</p>
<p>CHAPTER 30. WHEN THINGS GO WRONG 1849
dataHandler awhere
Handler ::Exception e
=&gt;(e-&gt;IOa)-&gt;Handler a
-- Defined in ‘Control.Exception’
We can make a list of handlers that handle exceptions of
varying types because the exception types are existentially
quantified under Handler ’s datatype.
But what if this isn’t convenient enough? What if we have a
family of semantically related or otherwise similar exceptions
we want to catch as a group? For this we revive our old friend,
the sum type!</p>
<p>CHAPTER 30. WHEN THINGS GO WRONG 1850
moduleOurExceptions where
importControl.Exception
dataEATD=
NotEven Int
|NotDivThree Int
deriving (Eq,Show)
instance Exception EATD
evenAndThreeDiv ::Int-&gt;IOInt
evenAndThreeDiv i
|rem i3/=0=throwIO ( NotDivThree i)
|even i=throwIO ( NotEven i)
|otherwise =return i
Now when we want to catch either error, we only need one
handler and then we can pattern match on the exception type
just like good old fashioned datatypes:
Prelude&gt; type EA e = IO (Either e Int)
Prelude&gt; try (evenAndThreeDiv 0) :: EA EATD
Left (NotEven 0)
Prelude&gt; try (evenAndThreeDiv 1) :: EA EATD
Left (NotDivThree 1)</p>
<p>CHAPTER 30. WHEN THINGS GO WRONG 1851
Nifty, eh? The notion here is to exercise the same taste
and judgment in designing your error types as you would in
your happy-path types. Preserve context and try to make it so
somebody could understand the problem you’re solving from
the types. If necessary. On a desert island. With a lot of rum.
And sea turtles.
30.8 Surprising interaction with
bottom
One thing to watch out for is situations where you catch an
exception for a value that might be bottom. Due to nonstrict-
ness, the bottom could’ve been forced before or after your
exception handler, so you might be surprised if you expected
either:
•that your exception handler was meant to catch the bot-
tom, or
•that no bottoms would cause your program to fail after
having caught, say, a SomeException .
The proper coping mechanism for this is a glass of scotch
and to realize the following things:
•The exception handling mechanism is not for, nor should
be used for, catching bottoms.</p>
<p>CHAPTER 30. WHEN THINGS GO WRONG 1852
•Having caught an exception, even SomeException , without
rethrowing an exception doesn’t mean your program
won’t fail.
To demonstrate the point, we’ll show you a case where we
caught an exception from a bottom and a case where a bottom
leap-frogged our handler:
importControl.Exception
noWhammies ::IO(EitherSomeException ())
noWhammies =
try undefined
megaButtums ::IO(EitherSomeException ())
megaButtums =
try$return undefined
Do you think these should have the same result? We’ve got
bad news:
Prelude&gt; noWhammies
Left Prelude.undefined
Prelude&gt; megaButtums
Right *** Exception: Prelude.undefined
The issue is that nonstrictness means burying the bottom
in areturn causes the bottom to not get forced until you’re</p>
<p>CHAPTER 30. WHEN THINGS GO WRONG 1853
already past the try, resulting in an uncaught error inside the
Rightconstructor. The take-away here shouldn’t be, “laziness
is terrifying,” but rather, “write total programs that don’t use
bottom.” It’s not only unforced bottoms that can cause pro-
grams that shouldn’t have any uncaught exceptions to fail
either, there’s also…
30.9 Asynchronous Exceptions
Asynchronousexceptionsarethepredatorshuntingyourhappy
little programs. You probably don’t have much experience
with anything like this unless you’ve written Erlang before.
Even then, Erlang’s asynchronous exceptions are handled by
a separate process. Most languages don’t have anything like
this if only because they don’t have a hope of making it safe
within their implementation runtimes.
moduleMainwhere
-- we haven't explained this.
-- tough cookies.
importControl.Concurrent
(forkIO,threadDelay )
importControl.Exception
importSystem.IO</p>
<p>CHAPTER 30. WHEN THINGS GO WRONG 1854
openAndWrite ::IO()
openAndWrite = do
h&lt;-openFile &quot;test.dat&quot; WriteMode
threadDelay 1500
hPutStr h
(replicate 100000000 '0'++&quot;abc&quot;)
hClose h
dataPleaseDie =
PleaseDie
deriving Show
instance Exception PleaseDie
main::IO()
main= do
threadId &lt;-forkIO openAndWrite
threadDelay 1000
throwTo threadId PleaseDie
If you run this program, the intended result is that you’ll
have a file named test.dat with only zeroes that didn’t reach
the “abc” at the end. Since we can’t predict the future, if you
have a disk with preternaturally fast I/O, increase the argu-
ments to replicate to reproduce the intended issue. If it ain’t
broken, break it.</p>
<p>CHAPTER 30. WHEN THINGS GO WRONG 1855
What happened was that we threw an asynchronous excep-
tion from the main thread to our child thread, short-circuiting
what we were doing in the middle of doing it. If you did this
in a loop, you’d leak file handles, too. Done continually over a
period of time, leaking file handles can cause your process to
get killed or your computer to become unstable.
We can think of asynchronous exceptions as exceptions
raised from a diﬀerent thread than the one that’ll receive the
error. They’re immensely useful and give us a means of talk-
ing about error conditions that are quite real and possible in
languages that don’t have formal asynchronous exceptions.
Your process can get axe-murdered by the operating system
out of nowhere in any language. We just happen to have the
ability to do the same within the programming language at the
thread level as well. The issue is that we want to temporarily
ignore exceptions until we’ve finished what we’re doing. This
is so the state of the file is correct but also so that we don’t leak
resources like file handles or perhaps database connections or
something similar.5Never fear, we can fix this!
5In this case, leaking means having too many (files, database connections, etc.) open
at one time, thus consuming all the resources your OS can allocate, the way trying to
hold too much in memory for too long causes memory leaks.</p>
<p>CHAPTER 30. WHEN THINGS GO WRONG 1856
moduleMainwhere
-- we haven't explained this.
-- tough cookies.
importControl.Concurrent
(forkIO,threadDelay )
importControl.Exception
importSystem.IO
openAndWrite ::IO()
openAndWrite = do
h&lt;-openFile &quot;test.dat&quot; AppendMode
threadDelay 1500
hPutStr h
(replicate 10000000 '0'++&quot;abc&quot;)
hClose h
dataPleaseDie =
PleaseDie
deriving Show
instance Exception PleaseDie
main::IO()
main= do
threadId &lt;-forkIO (mask_ openAndWrite)
threadDelay 1000
throwTo threadId PleaseDie</p>
<p>CHAPTER 30. WHEN THINGS GO WRONG 1857
Here we used mask_fromControl.Exception in order to mask
or delay exceptions thrown to our child thread until the IO
actionopenAndWrite wascomplete. Incidentally, sincetheendof
the mask is the last thing our child thread does, the exception
our main thread tried to throw to the child blows up in its
face, Wile E. Coyote style, and is now thrown within the main
thread.
Don’t panic!
Async exceptions are helpful and manifest in less obvious ways
in other language runtimes and ecosystems. Don’t try to catch
everything; just let it die, and make sure you have a process
supervisor and good logs. No execution is better than bad
execution.
30.10 Follow-up Reading
1.ABeginner’sGuidetoExceptionsinHaskell; ErinSwenson-
Healey
https://www.youtube.com/watch?v=PWS0Whf6-wc
2.Chapter 8. Overlapping Input/Output; Parallel and Con-
current Programming in Haskell; Simon Marlow;
http://chimera.labs.oreilly.com/books/1230000000929/ch08.html</p>
<p>CHAPTER 30. WHEN THINGS GO WRONG 1858
3.Chapter 9. Cancellation and Timeouts; Parallel and Con-
current Programming in Haskell; Simon Marlow;
http://chimera.labs.oreilly.com/books/1230000000929/ch09.html
4.An Extensible Dynamically-Typed Hierarchy of Excep-
tions; Simon Marlow
http://community.haskell.org/~simonmar/papers/ext-exceptions.
pdf</p>
<p>Chapter 31
Final project
1859</p>
<p>CHAPTER 31. FINAL PROJECT 1860
31.1 Final project
For our final project, we’re doing something a little weird, but
small and modernized a bit from the original design. Surely
no one who knows us from Twitter or IRC will be surprised
that we’ve chosen something eccentric for this, but we felt it
was important to show you an end-to-end project that brings
in so much real world it’ll make your head spin.
In this chapter,
•FINGER DAEMONS.
31.2 fingerd
Dating back to 1971, the finger1service was a means of figuring
out how to contact colleagues or other people on the same
computer network and whether they were on the network at
a given time, often on the same mainframe in a time when
computing was usually time-shared on the same physical ma-
chine.finger was originally intended to be used to share an
office number, email address, basic contact details like that.
By the time the 1990s and public internet access was widely
available finger was also used to deliver .planor.project files
as sort of pre-Twitter/Tumblr microblog.
1http://www.rajivshah.com/Case_Studies/Finger/Finger.htm</p>
<p>CHAPTER 31. FINAL PROJECT 1861
We’re going to be writing a finger daemon in this chapter.
Finger daemon programs are often called fingerd. A daemon
is a process that runs in the background without direct user
interaction; in the case of finger , the daemon acts as the server
side of the protocol, while the finger program itself is on the
client side. When you use finger from your command line,
it sends a request to the finger daemon, and the daemon re-
sponds with the requested information if it can.
We use this as an example in part because it’s not a typical
web app, only requires working with text, and because the
text-based protocol is spare and easy to debug once you know
how. This chapter is going to be somewhat more Unix/Linux-
oriented than previous ones, for a few reasons. Windows users
will find that not all of the examples can be followed along
literally, but the final version of the finger daemon2should
work.
Caveat for the Windows users
You will not be able to follow all of the instructions here ver-
batim. You can still build and hack on the project, but if you
aren’t willing to install a finger client for testing your finger
daemon via Cygwin then you’ll need to write your own client.
2A daemon is a computer program that runs as a background process</p>
<p>CHAPTER 31. FINAL PROJECT 1862
31.3 Exploring finger
If you had fingerd running on your local machine under the
username callen, the result of having done so might look some-
thing like:3
$ finger callen@localhost
Login: callen Name: callen
Directory: /home/callen Shell: /bin/zsh
On OS X, this will work, without having fired up or installed
a finger service, by not specifying a hostname to query:
$ finger callen
Login: callen Name: Chris Allen
Directory: /Users/callen Shell: /bin/bash
Spooky! Don’t ask. The finger protocol operates over Trans-
mission Control Protocol (TCP) sockets, something it has in
common with the protocol used by web browsers. However,
while they both use TCP, a finger daemon is not a web server.
It’s something much simpler. Rather than having an entire
application protocol layered atop TCP like the web (HTTP)
does, it’s a single message text protocol. Rather than go into a
long explanation of the internet, UDP, and TCP, let’s say TCP
3You can still use finger to check on the status of the bathrooms in the Random Hall
dormitory at MIT by typing finger @bathroom.mit.edu in your terminal. Try it.</p>
<p>CHAPTER 31. FINAL PROJECT 1863
is a protocol for sending messages back and forth between a
client and a server. Those messages can be raw bytes or text.
A socket is an address where a message can be delivered.4
Leaving aside the socket business, the way this should work
is roughly like this: the client requests some information, and
that request is trasmitted to the server with TCP magic. The
server (our friendly daemon) dishes up that information (if it
has it), TCP magic sends it to the client, then the client prints
the information in your terminal. We will start our project
with a little TCP echo server that prints the literal text the
client sent so that we can understand the cases we’re dealing
with.
Project overview
To kick this oﬀ, we’ll use Stack with the stack new command
like so:
$ stack new fingerd simple
This gets us a simple project with a single executable stanza
in the Cabal file. The final version after we’ve added Debug.hs
will have the following layout:
$ tree .
4If you’re new to networking and sockets, this guide by Julia Evans is a great intro-
duction. http://jvns.ca/zines/#networking-ack</p>
<p>CHAPTER 31. FINAL PROJECT 1864
.
├── LICENSE
├── Setup.hs
├── fingerd.cabal
├── src
│   ├── Debug.hs
│   └── Main.hs
└── stack.yaml
fingerd.cabal
Our Cabal file will mention an executable we’re not going to
give you yet, so you can leave the placeholder Stack generated
there for now. Note we have gently reformatted the text to fit
this book’s format.
name: fingerd
version: 0.1.0.0
synopsis: Simple project template
description: Please see README.md
homepage: https://github.com/u/fingerd
license: BSD3
license-file: LICENSE
author: Chris Allen
maintainer: cma@bitemyapp.com
copyright: 2016, Chris Allen
category: Web</p>
<p>CHAPTER 31. FINAL PROJECT 1865
build-type: Simple
cabal-version: &gt;=1.10
executable debug
ghc-options: -Wall
hs-source-dirs: src
main-is: Debug.hs
default-language: Haskell2010
build-depends: base &gt;= 4.7 &amp;&amp; &lt; 5
, network
executable fingerd
ghc-options: -Wall
hs-source-dirs: src
main-is: Main.hs
default-language: Haskell2010
build-depends: base &gt;= 4.7 &amp;&amp; &lt; 5
, bytestring
, network
, raw-strings-qq
, sqlite-simple
, text
Now that we have taken care of that, let’s write some code.</p>
<p>CHAPTER 31. FINAL PROJECT 1866
src/Debug.hs
This is our first source file. We’re going to use this program to
show us what the client sends and send it back. In this respect,
it’s almost identical to the echo server demonstrated in the
documentation of the network5library we’re relying on. The
diﬀerence is that it also prints a literal representation of the
text that was sent.
Our debug program is a TCP server, similar to a web server
which provides a web page, but lower level and limited to
sending raw text back and forth. What is diﬀerent is that a web
server communicates with browsers over a TCP socket using a
structured protocol rich with metadata, routes, and a standard
describing that protocol. What we’re doing is older and more
primitive.
moduleMainwhere
importControl.Monad (forever)
importNetwork.Socket hiding(recv)
importNetwork.Socket.ByteString
(recv,sendAll)
5https://www.stackage.org/package/network The example we’re referring to is in the Net-
work.Socket.ByteString module. Click on it and look for the example.</p>
<p>CHAPTER 31. FINAL PROJECT 1867
logAndEcho ::Socket-&gt;IO()
logAndEcho sock=forever $ do
(soc,_)&lt;-accept sock
printAndKickback soc
sClose soc
whereprintAndKickback conn = do
msg&lt;-recv conn 1024
print msg
sendAll conn msg
This sets up our server. Its argument is a socket ( sock) that
listens for new client connections; due to our use of forever ,
that socket remains open indefinitely. The accept action will
block until a client connects to the server. The socket socis
the result of accept -ing a connection for communicating with
the client.
The server can receive up to 1024 bytes of text from the
client. All it does here is print the text literally, then echo
what the client sent right back to the client that made the
connection. Note that recvis permitted to return fewer than
the maximum bytes specified if that’s all the client sent. Then
the connection to the client is closed — we apply sClose to
socbut not to sock, sosock, the server socket, remains open.
Because this action loops forever, the next thing we do is await
another client connection.</p>
<p>CHAPTER 31. FINAL PROJECT 1868
main::IO()
main=withSocketsDo $ do
addrinfos &lt;-getAddrInfo
(Just(defaultHints
{addrFlags =
[AI_PASSIVE ]}))
Nothing (Just&quot;79&quot;)
letserveraddr =head addrinfos
sock&lt;-socket (addrFamily serveraddr)
StreamdefaultProtocol
bindSocket sock (addrAddress serveraddr)
listen sock 1
logAndEcho sock
sClose sock
At the beginning of main,withSocketsDo is not going to do any-
thing at all unless you’re on Windows. If you are on Windows,
it’s obligatory to use the sockets API in the network library. The
address information stuﬀ is mostly noise and can be ignored
as a means for describing what kind of TCP server we’re firing
up and what port it’s listening on.
The important part is the (Just &quot;79&quot;) part — that’s the port
we’re listening for connections on. Also note that you’ll need
administrative privileges on most operating systems to listen
on that port.</p>
<p>CHAPTER 31. FINAL PROJECT 1869
TCP socket libraries like network often call everything a
socket. Server listening for connections? That’s a socket.
Client connection that you were listening for? That’s a socket.
Everything’s a socket, and nothing’s a wrench.
The next bit constructs a sort of socket descriptor with
socket . Thenwebindthesockettotheaddress(port)wewanted.
Lastly, we let the operating system know we’re prepared to
listen for connections from clients with listen . From there, we
fire oﬀ our server logic which runs indefinitely. If and when
logAndEcho finishes, we’ll close the socket server and then our
story is over.
The next step, assuming your project is built, is to fire up the
debug server — note that it’ll want administrative privileges
for using port 79:
$ sudo <code>stack exec which debug</code>
{... build noise and a password prompt ...}
That will get our echo server set up, and we can now test
it using telnet to connect. Telnet is often used to debug TCP
services that use text to communicate. Note that you’ll need
to usesudoor otherwise make use of administrator powers to
start the program because it wants to use a network port that
only administrators or root accounts have access to in most
operating systems. Usually this is the first 1024 ports. Once
you have the debug server running in one terminal, you’ll
connect to it from a new terminal like so:</p>
<p>CHAPTER 31. FINAL PROJECT 1870
$ telnet localhost 79
Trying 127.0.0.1...
Connected to localhost.
Escape character is '^]'.
From there, telnet is waiting for you to type something and
then hit enter:
blah
blah
Connection closed by foreign host.
In the above, we typed “blah,” hit enter, got “blah” echoed
back to us, then the server closed the connection. Remember
thatsClose is applied to socin ourlogAndEcho function, ensuring
that the temporary telnet connection is closed. However, the
server is still open, and you can make further requests by
reopening the telnet connection.
Let us take a look at the server side to see what it printed:
&quot;blah\r\n&quot;
We used printrather than putStrLn inlogAndEcho on purpose,
so we could get a literal representation of the data that was
sent. In this case, the string “blah” and the special characters
\rand\nwere sent. On Unix-based operating systems such</p>
<p>CHAPTER 31. FINAL PROJECT 1871
as Linux, \nis the default line-ending character. Microsoft
Windows uses \rfollowed by \nfor the same.
Having done that, let us now do the same with a finger
client:
$ finger callen@localhost
[localhost]
Trying 127.0.0.1...
callen
$ finger @localhost
[localhost]
Trying 127.0.0.1...
Particularly if you’re on a Mac, you may get some noise
here like this:
Trying ::1...
finger: connect: Connection refused
Trying 127.0.0.1...
It should connect after that. It attempts to use IPv6 first to
reach your finger daemon; when it can’t, it should use IPv4.
You can probably ignore this.
Then the output server-side for this would be:
&quot;callen\r\n&quot;
&quot;\r\n&quot;</p>
<p>CHAPTER 31. FINAL PROJECT 1872
The first command asked the finger daemon running at
localhost for information on the user callen; the second asked
for a listing of users. With the printed output the server gave
us, we now know what queries from a finger client will look
like to our TCP server. With that done, we’ll now write up the
final TCP server itself.
31.4 Slightly modernized fingerd
Historically, the data that finger returns about users was part
of the operating system. That information is still typically
stored in the OS, but for security reasons, it’s no longer rou-
tinely shared through finger requests. We’re going to update
the source of data for finger by using an embedded SQL6
database called SQLite. A database is a convenient yet robust
way of sorting and reading data, and SQLite is a lightweight
database. The data will be stored in a file within the main
project directory, so there won’t be a lot of mystery or magic
involved in interacting with it.
First we’ll show you the TCP server’s framing of the logic,
then we’ll show you how the database interaction works. From
here, all the code goes into your Main.hs file.
6Pronounced “squirrel.”</p>
<p>CHAPTER 31. FINAL PROJECT 1873
{-# LANGUAGE OverloadedStrings #-}
{-# LANGUAGE QuasiQuotes #-}
{-# LANGUAGE RecordWildCards #-}
OverloadedStrings you already know. QuasiQuotes is for the
literals, which you’ve seen before. RecordWildCards is the new
one and isn’t too difficult to figure out. It spares us manually
yanking the contents of a record into scope; instead, the record
accessors become bindings to the contents such that,
{-# LANGUAGE RecordWildCards #-}
moduleRWCDemo where
dataBlah=
Blah{ myThing ::Int}
wewBlah{..}=print myThing
wewwill print the myThing inside of the Blahargument it is
applied to without needing to apply myThing to aBlahvalue or
to destructure the contents of Blahin the pattern match. It’s
purely a convenience.</p>
<p>CHAPTER 31. FINAL PROJECT 1874
moduleMainwhere
importControl.Exception
importControl.Monad (forever)
importData.List (intersperse )
importData.Text (Text)
import qualified Data.Text asT
importData.Text.Encoding
(decodeUtf8 ,encodeUtf8 )
We’ll need the ability to decode a Textvalue from a UTF-8
ByteString andthenre-encodea TextvalueasaUTF-8 ByteString .
importData.Typeable
importDatabase.SQLite.Simple
hiding(close)
import qualified Database.SQLite.Simple
asSQLite
importDatabase.SQLite.Simple.Types
importNetwork.Socket hiding(close,recv)
importData.ByteString (ByteString )
import qualified Data.ByteString asBS
importNetwork.Socket.ByteString
(recv,sendAll)
importText.RawString.QQ</p>
<p>CHAPTER 31. FINAL PROJECT 1875
Creating the database We’re using the sqlite-simple library
to make a self-contained database stored in a file in the same
directory as our project. This will act as the repository of users
our finger daemon can report on.
dataUser=
User{
userId ::Integer
, username ::Text
, shell ::Text
, homeDirectory ::Text
, realName ::Text
, phone ::Text
}deriving (Eq,Show)
Now we dig into where the data comes from. Useris the
datatype describing our user records. It’s not super structured
or interesting, but gets things rolling. The only bit potentially
out of the ordinary here is that we have a userId field of type
Integer in order to provide the database with what’s called a
primary key. This is to provide a means of uniquely identify-
ing data in the database independent of the text fields in our
record type, among other things.
Weneedsomeboilerplatetypeclassinstancesformarshalling
and unmarshalling data to and from the SQLite database:</p>
<p>CHAPTER 31. FINAL PROJECT 1876
instance FromRow Userwhere
fromRow =User&lt;$&gt;field
&lt;<em>&gt;field
&lt;</em>&gt;field
&lt;<em>&gt;field
&lt;</em>&gt;field
&lt;*&gt;field
instance ToRowUserwhere
toRow (Userid_ username shell homeDir
realName phone) =
toRow (id_, username, shell, homeDir,
realName, phone)
This should remind you of FromJSON andToJSON .
createUsers ::Query
createUsers =[r|
CREATETABLEIFNOTEXISTSusers
(idINTEGER PRIMARY KEYAUTOINCREMENT ,
username TEXTUNIQUE,
shellTEXT, homeDirectory TEXT,
realName TEXT, phone TEXT)
|]</p>
<p>CHAPTER 31. FINAL PROJECT 1877
TheQuerytype is a newtype wrapper for a Textvalue. Con-
veniently, Queryhas anIsString instance, so string literals can
beQueryvalues. This isn’t really a query, though; it’s a SQL
statement defining the database table that will contain our
user data. The primary key stuﬀ is noise saying that the row is
namedidand that we want that field to autoincrement without
needing to do it ourselves. That is, if the last row we inserted
into the database had the id 1, then the new one will be auto-
assigned the primary key 2. The rest of it describes field names
and their representation (“TEXT”), but you’ll note we require
usernames to be unique so that there cannot be two Uservalues
with the same username.
insertUser ::Query
insertUser =
&quot;INSERT INTO users <br />
\VALUES (?, ?, ?, ?, ?, ?)&quot;
allUsers ::Query
allUsers =
&quot;SELECT * from users&quot;
getUserQuery ::Query
getUserQuery =
&quot;SELECT * from users where username = ?&quot;</p>
<p>CHAPTER 31. FINAL PROJECT 1878
This is utility stuﬀ for inserting a new user, getting all users
from the user table, and getting all the fields for a single user
with a particular username. The question marks are how the
sqlite-simple library parameterizes database queries.
dataDuplicateData =
DuplicateData
deriving (Eq,Show,Typeable )
instance Exception DuplicateData
The type above is a one-oﬀ exception we throw whenever
we get something other than zero or one users for a particular
username. That should be impossible, but you never know.
typeUserRow =
(Null,Text,Text,Text,Text,Text)
UserRow is a type synonym for the tuples we insert to create
a new user.</p>
<p>CHAPTER 31. FINAL PROJECT 1879
getUser ::Connection
-&gt;Text
-&gt;IO(MaybeUser)
getUser conn username = do
results &lt;-
query conn getUserQuery ( Onlyusername)
caseresults of
[]-&gt;return$Nothing
[user]-&gt;return$Justuser
_ -&gt;throwIO DuplicateData
TheOnlydata constructor is how we pass a single argument
instead of a 2-or-greater tuple to our query parameters when
using the sqlite-simple library. This is needed because basehas
no one-tuple type and getUserQuery takes a single parameter.
We check for none, one, or many results converting it into a
Nothing ,Just, orIOexception.
Finally, we need a utility function for creating the database
with a single example row of data:</p>
<p>CHAPTER 31. FINAL PROJECT 1880
createDatabase ::IO()
createDatabase = do
conn&lt;-open&quot;finger.db&quot;
execute_ conn createUsers
execute conn insertUser meRow
rows&lt;-query_ conn allUsers
mapM_ print (rows ::[User])
SQLite.close conn
wheremeRow::UserRow
meRow=
(Null,&quot;callen&quot; ,&quot;/bin/zsh&quot; ,
&quot;/home/callen&quot; ,&quot;Chris Allen&quot; ,
&quot;555-123-4567&quot; )
Stack may balk because you have a module called Mainthat
has nomaindefined. If that’s the case for you, you can do this:
main::IO()
main=createDatabase
We’ll change that mainlater, but that will get your executable
building for now.
Running this a second time will error without changing the
database. If you need or want to reset the database, you can
delete the finger.db file.</p>
<p>CHAPTER 31. FINAL PROJECT 1881
Before you continue The code that follows will assume and
require a SQLite database by the name of finger.db with the
schema outlined in createUsers exists in the same directory as
where you run your fingerd service.
To run createDatabase , you could do the following:
$ stack ghci --main-is fingerd:exe:fingerd
{... noise noise ...}
Prelude&gt; createDatabase
User {userId = 1, ... noise ... }
With that in place, you can continue implementing your
finger daemon.
Let your fingers do the walking
We’re still in our Mainmodule here. You should have created
the database already, but now we’ll write the functions that
will allow the server to listen and respond to client queries.</p>
<p>CHAPTER 31. FINAL PROJECT 1882
returnUsers ::Connection
-&gt;Socket
-&gt;IO()
returnUsers dbConn soc = do
rows&lt;-query_ dbConn allUsers
letusernames =map username rows
newlineSeparated =
T.concat$
intersperse &quot;\n&quot;usernames
sendAll soc (encodeUtf8 newlineSeparated)
returnUsers uses a database Connection and aSocket for talk-
ing to the user. The database connection is used to get a list
of all the users in the database which is then changed into
a newline separated Textvalue. Then that is encoded into a
UTF-8ByteString which is sent through the socket to the client.
formatUser ::User-&gt;ByteString
formatUser (User_username shell
homeDir realName _)=BS.concat
[&quot;Login: &quot; , e username, &quot;\t\t\t\t &quot;,
&quot;Name: &quot; , e realName, &quot;\n&quot;,
&quot;Directory: &quot; , e homeDir, &quot;\t\t\t&quot;,
&quot;Shell: &quot; , e shell, &quot;\n&quot;]
wheree=encodeUtf8</p>
<p>CHAPTER 31. FINAL PROJECT 1883
This function is used to format Userrecords as a UTF-8
ByteString value. The format is intended to mimic popular
fingerd implementations but we’re not going for precision
here.
returnUser ::Connection
-&gt;Socket
-&gt;Text
-&gt;IO()
returnUser dbConn soc username = do
maybeUser &lt;-
getUser dbConn ( T.strip username)
casemaybeUser of
Nothing -&gt; do
putStrLn
(&quot;Couldn't find matching user <br />
\for username: &quot;
++(show username))
return()
Justuser-&gt;
sendAll soc (formatUser user)
This is the single user query case, where we use formatUser
to provide detailed information to the client on a single user.
We have to handle the case where no user by the username
provided was found. As it stands, the Nothing case here will</p>
<p>CHAPTER 31. FINAL PROJECT 1884
print the report that no user was found by that username in the
server terminal but will not send that information — or any
information — to the client side. You may want to change that,
as it might be useful to tell the end user why no information
was returned.
If a user is found, we send the formatted ByteString of the
Userrecord to the client. The stripping of the username text
prior to querying is because the literal data sent for a user-
name query is &quot;yourname\r\n&quot; and in order for that to match
“yourname,” we need to strip the control characters from the
text, which stripfromData.Text does for us.
handleQuery ::Connection
-&gt;Socket
-&gt;IO()
handleQuery dbConn soc = do
msg&lt;-recv soc 1024
casemsgof
&quot;\r\n&quot;-&gt;returnUsers dbConn soc
name-&gt;
returnUser dbConn soc
(decodeUtf8 name)
handleQuery receives up to 1024 bytes of data. Based on
that data the client sends to the server, the case discriminates
between when it should send a list of all users or only a single</p>
<p>CHAPTER 31. FINAL PROJECT 1885
user. Fortunately, the protocol is relatively uncomplicated,
so we don’t have to do any parsing as would ordinarily be
required for communicating with a more elaborate protocol.
handleQueries ::Connection
-&gt;Socket
-&gt;IO()
handleQueries dbConn sock =forever $ do
(soc,_)&lt;-accept sock
putStrLn &quot;Got connection, handling query&quot;
handleQuery dbConn soc
sClose soc
It’s similar to the echo server, save for the additional ar-
gument of the database connection and the logging of when
connections were accepted.
Now we need to change mainto assemble our whole pro-
gram:</p>
<p>CHAPTER 31. FINAL PROJECT 1886
main::IO()
main=withSocketsDo $ do
addrinfos &lt;-
getAddrInfo
(Just(defaultHints
{addrFlags =[AI_PASSIVE ]}))
Nothing (Just&quot;79&quot;)
letserveraddr =head addrinfos
sock&lt;-socket (addrFamily serveraddr)
StreamdefaultProtocol
bindSocket sock (addrAddress serveraddr)
listen sock 1
-- only one connection open at a time
conn&lt;-open&quot;finger.db&quot;
handleQueries conn sock
SQLite.close conn
sClose sock
The only new bit above is the opening of a connection to a
SQLite database located in the same directory as your project.
The connection to the database is passed to the query-handling
code, which runs indefinitely like the echo-and-log server. If
it somehow stops without throwing an exception, we close the
server socket, just to be good little programmers.
Now we’re done, assuming you’ve created a SQLite database</p>
<p>CHAPTER 31. FINAL PROJECT 1887
usingcreateDatabase which is valid and accessible to your pro-
gram, the following should work. You’ll want to do this in one
terminal:
$ stack build
$ sudo <code>stack exec which fingerd</code>
Theninanother, diﬀerent, shellsessionthefollowingshould
work:
$ finger callen@localhost
Login: callen Name: Chris Allen
Directory: /home/callen Shell: /bin/zsh
And that’s it. In the exercises, we’ve given some ways to
extend this, and we hope you’ve enjoyed this little foray into
TCP sockets and basic networking. Security concerns aside,
thefinger protocol has been used over the years for some
pretty cool things. Perhaps most famously, John Carmack
used.planfiles as a kind of microblog to deliver updates on
the development process of Quake.7
31.5 Chapter Exercises
1.Try using the sqlite3 command line interface to add a
new user or modify an existing user in finger.db .
7http://atrophied.co.uk/read/john-carmacks-plan-archive</p>
<p>CHAPTER 31. FINAL PROJECT 1888
2.Write an executable separate of fingerd anddebugwhich
allows you to add new users to the database.
3.Add the ability to modify an existing user in the database.
4.Bound on a diﬀerent port, try creating a “control socket”
that permits inserting new data into the database while
the server is running. This will probably require, at mini-
mum, learning how to use forkIO and the basics of concur-
rency in Haskell among other things. Design the format
for representing the user rows passed over the TCP socket
yourself. For bonus points, write your own client exe-
cutable that takes the arguments from the command line
as well.
5.Celebrate completing this massive book.</p>
<p>Index
(), see unit
(-&gt;), see function type
constructor
(:), see cons
(&lt;-), see bind
($), 79, 80, 82, 389–391, 1059
<em>, see kind
(++), see concatenation
::, see type signature
&lt;</em>, seeApplicative
&lt;*&gt;, 1054, 1055, 1057, 1059,
1145, see also Applicative
&lt;|&gt;, seeAlternative
&lt;$&gt;, seefmap
=&lt;&lt;, see flip bind
=&gt;, see typeclass constraint
&gt;&gt;, seeMonad
&gt;&gt;=, see bind
[], see list syntax
eta reduction, 1017, 1022~, see tilde
(||), 1231
|, see pipe
abs, 378, 382
abstract datatype, 1031, 1149
abstraction, 7, 32
accessor function, 1508
actual type, 190, 284
actual vs expected type, 1477
ad hoc polymorphism, see
constrained
polymorphism
aeson, 1425, 1476, 1481, 1484,
1488
algebra, 611, 615, 627, 636,
888, 889, 904, 941, 942
algebra, definition, 955
algebraic datatype, 627
All(newtype), 909
1889</p>
<p>INDEX 1890
all, 789
alpha equivalence, 9, 15, 218
Alternative , 1427, 1429, 1433,
1501
ambiguous type, 281, 839,
842, 929
AmbT, 1567
anamorphism, 742
anarchy, 375
anonymous function, 8, 339,
340, 344, 492, 509, 512
anonymous function
definition, 406
syntax, 200
anonymous product, 172,
613, 632
Any(newtype), 909
API definition, 1426
application, 3, 7, 21, 32
Applicative , 1053, 1054, 1127,
1131, 1143, 1150, 1170,
1197, 1226, 1227, 1231,
1283, 1319, 1337, 1341,
1353, 1514, 1525, 1563
Applicative
Compose , 1549IO, 1802
Reader , 1351
composition law, 1108
definition, 1138
homomorphism law, 1109
identity law, 1106
interchange law, 1111
applicative, 1110
Arbitrary , 835, 837, 843, 863,
869, 873, 928, 1012
arbitrary precision, 146
argument, 3, 7, 11, 45, 192,
330, 334, 336, 392, 611
argument
multiple, 15, 192, 194, 195,
198, 330, 332, 333, 459
type, see type argument
argument, definition, 95
arithmetic, 51, 68
arity, 161, 165, 611
arity, definition, 174
Array, 1752
array, 1748
as patterns, 693
ASCII, 1769, 1773
association list, see Map(type)</p>
<p>INDEX 1891
associativity, 53–55, 185, 196,
367, 541, 550–553, 557,
569, 885, 890, 892, 905,
915, 925, 927, 932, 941,
1190, 1712
associativity, Monad, 1806
AST, 1460, 1475
asynchronous exception,
1853, 1855
attoparsec , 1425, 1465, 1469
backtracking, 1469, 1471
bang bang, 121, see indexing
bang pattern, 1691, 1693,
1697, 1739
BangPatterns , 1689
base, 258, 502, 787, 937, 1665,
1712
base case, 422, 423, 425, 437,
443, 445, 462, 463, 535,
539
base monad, 1573
benchmarking, 1709, 1716,
1724, 1727, 1739, 1743,
1747, 1751
benchmarkingstring types, 1766
vectors, 1759
beta reduction, 10, 11, 13, 17
Bifunctor , 1519
binary tree, 681, 682, 746, 851
bind, 775, 777, 780, 781, 784,
797, 1144, 1145, 1148, 1156,
1170, 1191, 1202, 1296,
1516, 1522, 1526, 1528,
1531, 1534, 1539, 1543,
1575
bind, definition, 1210
binding, 46, 87, 88, 330, 334,
335, 338, 344
binding
definition, 406
local, 109, 117, 129
top level, 130
Bloodhound (library), 676
Bool, 134, 135, 148, 152, 155,
156, 250, 251, 286, 378,
382, 592, 594, 595, 616,
908
Bool, fun with, 153
bool, 510
Boole, George, 134</p>
<p>INDEX 1892
Boolean logic, 153
bottom, 236, 347, 431–434,
487, 495, 496, 498, 509,
543, 545, 546, 560, 570,
723, 897, 1180, 1531, 1533,
1632, 1635, 1640, 1658,
1676, 1702, 1722, 1851,
1853
bottom, definition, 412
Bounded , 143, 251
burrito, 1141, 1518
ByteString , 1239, 1253, 1255,
1415, 1469, 1670, 1762,
1767, 1874
ByteString
String conversion, 1769
lazy, 1476, 1479, 1768
lazy vs strict, 1476
strict, 1769
versusText, 1775
bytestring (library), 1241,
1769
Cabal, 752, 753, 1716
.cabal file, 754, 757, 759, 764,
786, 822, 826, 847, 1237,1712
cabal install, 139
Caesar cipher, 519
CAF (constant applicative
form), 1733, 1734, 1736,
1737
call by name, 1657, 1664
call by need, 1657, 1658
call by value, 1657
cardinality, 615, 616, 618, 619,
627, 628, 631–634, 637,
1474
Carnap, Rudolf, 959, 1048
Cartesian product, 1072
case expression, 360–363,
375, 537, 710, 806, 1639,
1644, 1652, 1654
cassava , 1425
cast, seeTypeable
catamorphism, 531, 908,
1260, 1268, 1270
catamorphism, definition,
585
catch, 1824, 1829–1831, 1833
catMaybes , 1287, 1294
Char, 99, 100, 150</p>
<p>INDEX 1893
Char8, 1769
character, 99, 100
checkers (library), 1115, 1117,
1191, 1308
Chomsky hierarchy, 1461
Church, Alonzo, 2
Clinton, George, 1050
closure, 1761
CoArbitrary , 873, 1012
combinator, 21, 22, 1399
command line argument,
1226
comment syntax, 65, 66
commutative monoid, 917
commutativity, 203, 916, 917,
1617
compare , 286
comparison functions, 147,
152, 284
compile a binary, 1826, 1832
compile time, 181, 605, 620,
622
composability, 985
Compose (type), 1308, 1509,
1511, 1513, 1514, 1516,
1548composition, 387–392, 394,
395, 397, 400, 425–428,
511, 1199, 1295, 1307,
1320, 1335, 1505, 1518
composition
Traversable , 1308
definition, 415
law, 980, 984, 1108
concat , 107, 110, 1147
concatenation, 105, 107, 110,
113, 115, 891, 894, 916,
1748
concatenation, definition,
129
concrete type, 187, 196, 209,
211, 212, 226, 279, 313,
314, 595, 598, 599, 603,
721–724, 1526, 1543, 1677,
1679, 1681
concurrency, 1708
conditional, 155
conduit (library), 1567
conjunction, 133, 154, 161,
526
conjunction ( Monoid ), 909,
910</p>
<p>INDEX 1894
cons (:), 119, 459, 460, 464,
487, 503, 504, 533, 554,
555
cons cell, 464, 465, 485–487,
489, 494, 546
cons cell, definition, 528
cons, definition, 526
Const(type), 1024, 1303
const, 538, 547, 556, 559, 569,
1024
Constant (type), 1024, 1025,
1080
Constant (type)Functor , 1025
constant, 595, 596, 600, 604,
618
constant applicative form,
see CAF
constrained polymorphism,
174, 183, 187, 190, 209,
211, 215, 239, 249, 279,
310, 314, 981, 1466, 1468
constructor, 594, 595, 603,
604, 606, 642
constructor
data, see data constructor
nullary, see nullaryconstructor
smart, see smart
constructor
type, see type constructor
constructor class, 979
containers (library), 1738
Control.Exception , 1857
Control.Monad , 1685
ContT, 1567
criterion , 1709, 1712, 1790
CSV parsing, 1228, 1425
curry, 201
Curry, Haskell, 15
currying, 15, 43, 192,
194–196, 199, 330, 367
currying, definition, 406
daemon, 1861
Damas-Hindley-Milner, 180,
217
data constructor, 133–135,
137, 152, 153, 162, 179,
260, 262, 344, 345, 349,
350, 352, 353, 360, 361,
363, 491, 591, 595, 596,
599, 600, 602, 607, 608,</p>
<p>INDEX 1895
611, 612, 619, 632, 633,
638, 707, 710, 729, 730,
732, 1229, 1266, 1644,
1661, 1676, 1720, 1724,
1819
data constructor
currying, 729
definition, 173
infix, 460, 679, 937
data declaration, 134, 135,
179, 258, 592–594, 597,
606, 608, 705
data declaration
definition, 174
how to read, 134
data structure, 1708, 1736,
1738
Data.Bool , 510
Data.Char , 517, 1772, 1773
Data.Foldable , 1262
Data.Map , 1073, 1219
Data.Maybe , 1356
Data.Monoid , 898, 1262
Data.Tuple , 163
database, 1872, 1875, 1877,
1882, 1885database
FromRow , 1876
ToRow, 1876
datatype, 128, 132, 135, 152,
179, 353, 360, 592
datatype
algebraic, see algebraic
datatype
definition, 129, 703
recursive, 460, 485
Debug.Trace , 1665, 1796
declaration, 41, 45, 46, 61, 85,
108, 152
declaration
class, see typeclass
declaration
data, see data declaration
instance , see typeclass
instance
type, see type alias
fixity, 1712, 1776
import, 765
local, 108
module, 758
newtype, 620
top level, 117, 223, 226</p>
<p>INDEX 1896
type signature, 99, 196
deepseq , 1715
dependency, 753, 758, 787,
823, 824, 826, 827, 833
DeriveGeneric , 874
deriving, 136, 257, 259, 289,
290, 609, 624, 710, 1844
deriving Show, 265, 301, 303
deriving, definition, 323
desugar, 199, 465, 505, 1155,
1161
diﬀerence list, see DList
disjunction, 133, 134, 152, 154,
527, 627, 652, 1231
disjunction ( Monoid ), 909, 910
distributive property, 636,
637, 639, 640
division, 73, 442, 443, 445
division
fractional, 145, 146, 187
integral, 68
DList, 1388, 1775
dosyntax, 104, 774, 779,
781–784, 797, 806, 828,
1033, 1145, 1154, 1155,
1161, 1169, 1234, 1350,1554
documentation, 181, 827
Double , 139, 145, 146, 1693
drop, 120, 469, 471
dropWhile , 469, 472, 473
dynamic typechecking, 1823
eﬀects, 103, 299–301, 774,
783, 1031, 1149, 1159,
1161, 1221, 1227, 1761,
1782, 1788, 1798, 1799,
1836, 1841
eﬀects, definition, 323
Either , 640, 709, 712–714, 716,
717, 722, 844, 1006, 1021,
1132, 1255, 1276, 1298,
1306, 1608, 1819, 1821,
1832
Either
Applicative , 1131
Functor , 1021
Monad, 1182
EitherT , 1235, 1555, 1600,
1610, 1614
elem, 203, 482, 801, 1276
Elliott, Conal, 1115, 1567</p>
<p>INDEX 1897
empty list, 503
Enum, 251, 261, 275, 294, 467
Enumfunctions, 294
enumFromTo , 296
Eq, 148, 249, 251, 252, 256,
258–260, 270, 272, 292,
293, 710
Eqfunctions, 254
equality, 147, 148, 249, 252,
253, 259, 264, 293
Erlang, 1853
error, 433, 434, 1376
error
ambiguous type variable,
1272
could not deduce, 277, 284,
312, 341
expected vs actual type,
189, 255, 284, 368, 435,
566, 622, 648, 653, 666,
707, 729, 777, 782, 900,
963, 1030, 1201
expecting one more
argument, 725–727
no instance for, 114, 151,
158, 216, 225, 259, 265,271, 272, 292, 302, 308,
310, 434, 731, 895, 899
no instance for Show, 162,
288
not in scope, 118, 336, 732,
767
too many arguments, 900
error message, how to read,
114, 119, 152, 288
eta reduction, 1673, 1736
Eval(typeclass), 1640
evaluate, 3
evaluation, 20, 41, 47, 49, 196,
485, 487–491, 493–498,
504, 508, 539, 550, 1630,
1633, 1640, 1788
evaluation
foldl, 549, 553–555,
559–561, 569
foldr, 538, 539, 541–543,
545, 551, 569
call by need, 491
folds, 541
inside out, 1635
outside in, 1635
recursive function, 538,</p>
<p>INDEX 1898
541, 543
strategies, 1657
evaluation order, 1782, 1786,
1790
Exception (typeclass), 1815,
1844
Exception ,throw, 1840
exception, 122, 347, 361, 431,
432, 1421, 1813, 1825,
1833, 1878
exception
mask_, 1857
asynchronous, see
asynchronous
exception
empty structure, 1278
handling , see exception
handling
loop, 431
missing field, 660
no match, 665
no parse, 304
non-exhaustive patterns,
266, 347, 433, 462
thread blocked, 1794
throw, 1838, 1840, 1842,1843, 1845
undefined, 508, 546, 547,
559, 569
exception handling, 1823,
1824, 1827, 1829, 1831,
1833, 1840, 1841
exception handling
catch, 1824, 1829, 1833, see
alsocatch
Either , 1832
Maybe, 1831
try, 1832, 1833, 1848
bottom, 1851
ExceptT , 1569, 1570, 1584, 1610
executable, 757, 759, 1835,
1880
executable, with arguments,
1836
ExistentialQuantification ,
1819
existential quantification,
1762, 1817, 1821, 1848
exitSuccess , 792
expected type, 190, 256, 284
exponentiation, 55
export, 849</p>
<p>INDEX 1899
expression, 3, 7, 41, 43, 47, 48,
60, 85, 133
expression problem, 249
expression, definition, 95
factorial, 421, 425
fail, 1422
fibonacci, 436, 441, 576–578
file, 58
filter , 511, 578
finger , 1860
finger tree, 1744
finger, MIT, 1862
First(newtype), 910, 912
FlexibleInstances , 1044
flip, 365, 553
flip bind ( Monad), 1288
Float, 138, 145
floating point numbers, 139
fmap, 500, 502, 508, 714, 732,
962, 1018, 1057, 1059,
1087, 1145, 1157, 1201,
1221, 1224, 1274, 1288,
1295, 1302, 1318, 1330,
1539, 1574
fmap,IO, 1801fmap, infix, 1087
fold, 531, 532, 903, 1038,
1542, 1543, 1635
fold, 1263
fold left, see foldl
fold right, see foldr
fold, definition, 585
Foldable , 112, 482, 532, 790,
903, 1261, 1262, 1302,
1303, 1390
foldl, 548, 549, 559, 570, 572,
588, 1269
foldl' , 561
foldMap , 1261, 1263, 1265,
1270
foldr, 532, 533, 536, 545, 569,
571, 1261, 1267, 1269,
1717, 1724
forall , 1818
foreign function interface
(FFI), 1475
forever , 809, 1685, 1840
Fractional , 138, 146, 215, 276,
277
fractional, 145
FromJSON , 1484, 1486</p>
<p>INDEX 1900
fromMaybe , 1293, 1359
fst, 162, 508
function, 3, 4, 7, 41, 43–45,
192, 298, 330, 332, 333,
984
function
anonymous, 8
application, 10, 41, 46–48,
80, 185, 195, 330, 334,
335, 389, 427, 507, 598,
707, 973, 984, 1059, 1071,
1110, 1199, 1219
body, 46
composition, see function
composition
datatype, 184
first-class, 3, 330
head, 46
higher-order, see
higher-order function
infix, 51
mathematical, 48
parameter, 192
prefix, 51
structure, 7, 8
unsafe, 122function composition, 985,
995, 1203, 1209, 1325,
1328, 1331, 1334, 1507,
1509, 1573
function type, 192, 194, 195,
252, 289, 1319
function type
Applicative , 1337
Functor , 1328, 1330, 1336
Monad, 1348
Monoid , 1218, 1220
asReader , 1334
function type constructor,
185, 195, 197, 367, 723,
729, 748, 875
function, definition, 96
functional dependencies,
596
Functor , 500, 502, 714, 958,
960–962, 965, 972, 973,
984, 1005, 1018, 1039,
1054, 1057, 1143, 1197,
1221, 1276, 1319, 1328,
1330, 1334, 1511, 1524
Functor laws, 979, 983, 1010
Functor , definition, 1047</p>
<p>INDEX 1901
functor, 959, 960, 1142, 1200,
1318, 1325
functor, applicative, 1145
fusion, 1753, 1754
GADTs, 1818, 1819
garbage collection, 1628,
1631, 1736
Gen, 835, 837, 839, 861, 864,
866, 869, 873
GeneralizedNewtypeDeriving ,
624, 626
generalized algebraic
datatype, see GADTs
generator, 477, 478
generator
multiple, 479, 480
Generic , 874
getArgs , 1835
getChar , 1782
getLine , 775, 777, 1157
GHC 8.0, 1693
GHC Core, 1647, 1652, 1654,
1680, 1689
GHC extension, see
language extensionGHC flag, 1729
GHC flag
-ddump , 1435, 1648
-fprof-auto , 1729
-I, 921
-O2, 1710, 1730
-O, 1710
-prof, 1729
-rtsopts , 1730
-Wall, 267, 348, 385
GHC optimization, 1661,
1716, 1730, 1754, 1787
GHC optimization,
strictness, 1669, 1676
GHC Rules, 1754
GHC.Prim , 1785
GHCi, 36, 41, 45, 339, 929,
1031, 1032, 1825
GHCi block syntax, 345, 348,
378
GHCi command
:browse , 358, 765, 827
:info, 53, 78, 135, 144, 155,
250, 252, 843
:kind, 598, 676, 721
:load, 40</p>
<p>INDEX 1902
:main, 1837
:module , 40, 770
:reload , 47
:set, 348, 385, 770, 1436,
1648
:sprint , 488, 493, 1660
:type, 99, 111, 152, 182, 618,
964
GHCi options, 766
Gibbard, Cale, 540
git, 753, 754
gopattern, 445
Gofer, 979
guard, 377, 379, 380,
382–385, 711
guarded recursion, 1724
gzip, 1768
Hackage, 258
HashMap , 1741
Haskell ninjas, 307
Haskell Report, 279, 596,
600, 720, 1716
head, 120
heap profiling, 1731
hGetChar , 1809hgrev(library), 1226
higher-kinded, 720, 722, 726,
729
higher-kinded
polymorphism
definition, 1047
higher-kinded type, 674, 677,
680, 681, 720, 965, 974,
979, 1005, 1007, 1034,
1047, 1262
higher-kinded type, Functor ,
972
higher-kinded type,
definition, 748
higher-order function, 200,
365, 366, 369, 375, 387,
421, 425, 472, 500, 512,
1399
higher-order function
definition, 413
Hindley-Milner, see
Damas-Hindley-Milner
homomorphism, 1109, 1130
Hoogle, 252
hspec(testing), 822, 825, 828,
833, 845, 1456</p>
<p>INDEX 1903
http-client (library), 1842,
1843
Hutton’s Razor, 702
I/O, 103, 301, 775, 1825
id, 210, 1507, 1508
idempotent, 881
idempotent, definition, 886
Identity (type), 270, 865,
1078, 1269, 1270, 1302,
1307, 1507, 1508, 1511,
1524, 1568, 1569
identity, 905
identity
function, 11, 12, 21, see id
law, 979, 983, 1106, 1189
property, 929, 932
identity value, 425, 535, 536,
561, 890–892, 917, 933,
936, 939, 941, 1270
IdentityT , 1508, 1523, 1524,
1526, 1530, 1534, 1538,
1541, 1544, 1589, 1600
idiom, 1139
ifexpression, 155, 156, 360,
361, 363, 377–380, 509,510, 783, 805
immutability, 504, 511, 643,
683
imperative programming,
783, 1149
import, 109, 155, 163, 246,
765, 767, 787, 825, 850,
855
import
hiding, 1469, 1866
qualified, 768, 1244
qualified as, 769, 850, 1244
import syntax, 1583, 1587
indentation, 59, 60
indexing, 121, 577, 796, 1748
infinite list, 1733
infix operator, 51, 53, 68, 79,
185, 194, 202, 678, 1712
infix operator
associativity, 53–55, 194,
195
precedence, 53, 54, 195
prefix, 52, 82, 110, 115
sectioning, see sectioning
infix, definition, 96
infixl , 54</p>
<p>INDEX 1904
infixr , 55, 1776
:info, 135
INI, 1444
INLINABLE , 1717
INLINE , 1776
inlining, 1670, 1672, 1787
input, 3
input/output, see I/O
instance, 140, 146, 257
instance , 260
instance, orphan, see orphan
instance
InstanceSigs , 1343, 1515, 1516,
1531
Int, 138, 141, 616, 1693
IntversusInteger , 1719
Int32, 1368
Int8, 142, 616, 629
Integer , 128, 138, 140, 143,
895, 1488
Integer ,Monoid , 895, 896
integer, 68, 140
Integral , 274, 275
Integral functions, 274
interface, 249
intersperse , 791IntMap , 1741
IO (), 103, 300, 776, 779,
1031, 1158
IO, 103, 299, 301, 774, 781,
782, 837, 1031, 1032,
1149, 1154, 1224, 1233,
1235, 1244, 1505, 1522,
1573, 1597, 1759, 1782,
1784, 1825, 1841
IO
Applicative , 1075, 1801,
1802
Functor , 1031, 1157, 1800
Monad, 1149, 1804
asState, 1785, 1786
associativity, 1806
exceptions, 1833
sharing, 1664
unsafe functions, 1809
IOaction, 301, 1161, 1255,
1787, 1789
IO, definition, 323
IO Monad , the, 1784
IOException , 1828, 1831
IRC, 540, 591
irrefutable pattern,</p>
<p>INDEX 1905
1686–1688, 1694
isomorphism, 885
IsString , 1239, 1241
JavaScript, 1488
join, seeMonad, 1148, 1154,
1160, 1209, 1296, 1529,
1536, 1539, 1543, 1801
join,IO, 1804
JSON, 676, 677, 1397, 1425,
1476, 1481, 1488
JSON parsing, 1227
key-value pair, see Map(type)
keyword
~, 1688, 1694
!, 1691, 1739
*, 598, 966
--, 65
-&gt;, 192, 966
::, 99, 106, 196, 720
&lt;-, 781
=&gt;, 190
=, 45
@, 693
#, 625, 1786
_, 137, 345as, 769, 1244
case, see case expression,
see case expression
class, 305, 960
data, 135, 174, 593
deriving , 257, 609
do, 104, 775, 779
forall , 1818
hiding , 1469, 1866
if-then-else , 156
if, 156
import , 765, 767
infixl , 54, 1712
infixr , 55, 1776
instance , 251, 260, 261
let, 45, 85
let,in, 61
module , 751
newtype , 620
qualified , 768, 769, 1244
type, 620, 633
where, 85, 88, 261, 961
|, 134, 593
kind, 597–599, 603, 674, 681,
720, 722, 723, 725, 726,
966, 971, 978, 989</p>
<p>INDEX 1906
kind inference, 970
Kleisli composition, 1202,
1203, 1209
lambda, 3, 31, 195
lambda calculus, 2, 32, 43,
48, 180, 192, 298, 427,
1161, 1798
lambda expression, 1761
lambda term, 7
language extension
BangPatterns , 1689
DeriveGeneric , 874
ExistentialQuantification ,
1819
GADTs, 1819
GeneralizedNewtypeDeriving ,
624, 626
InstanceSigs , 1343, 1515,
1516, 1531
NegativeLiterals , 631
NoImplicitPrelude , 766
NoMonomorphismRestriction ,
226
OverloadedStrings , 1214,
1239, 1416, 1469, 1485,1598, 1605, 1767
QuasiQuotes , 1433, 1485,
1873
RankNTypes , 1035
RecordWildCards , 1873
StrictData , 1693
Strict , 1693
TypeApplications , 964
Last(newtype), 910, 912
laws, 904
laws
Applicative , 1106
Functor , 979, 982
Monad, 1188
Monoid , 904
Traversable , 1307
mathematical, 955
laziness, 1630, 1632, see also
nonstrictness
leaf, 682
length , 167, 186, 215, 490,
494–496, 535, 544, 1275
let, 45, 59, 85, 86, 108, 224,
335, 338, 1685
letexpression, 86, 338
letversuswhere, 85</p>
<p>INDEX 1907
lexing, 1460, 1461
library, 759, 761, 764
library
aeson, 1476
attoparsec , 1425
bytestring , 1241, 1476, 1769
checkers , 1115, 1191
containers , 849, 1738
criterion , 1709
hspec, 822
http-client , 1842
network , 1866
parsec , 1468
parsers , 1412, 1426
QuickCheck , 871, 1012, 1115
random , 792, 1367
scientific , 139, 145
scotty , 1247, 1256, 1576
snap, 1224
sqlite-simple , 1875
text, 1223, 1241, 1763, 1774
time, 1789
transformers , 1378, 1564,
1585, 1598
trifecta , 1415, 1425, 1468
uuid, 1223vector , 1290, 1748
wreq, 1300
lift, seeMonadTrans , 1589,
1608
liftA2 , 1229, 1231
lifting, 977, 981, 1032, 1054,
1055, 1059, 1150, 1221,
1229, 1331, 1511, 1574,
1575, 1580, 1582, 1588
lifting
definition, 1048
liftIO , 1253, 1597, 1608
liftM, 1151
lines, 793
List, 592, 597, 678–680, 726,
727
list
Applicative , 1061, 1068,
1069
Monad, 1163
Monoid , 1265
datatype, 459, 527, 528
empty, 536
infinite, 542, 543, 561, 576
structure, 486, 487,
494–496, 507, 529</p>
<p>INDEX 1908
type constructor, 165
list comprehension, 477–479,
482, 512
list comprehension, with
condition, 478, 479, 482
list functions, 119
list monoid, 1120
list syntax, 165, 460
lists, 98–100, 105, 112, 113,
119, 165, 464, 500, 503,
891, 893, 1260, 1567,
1695, 1738, 1745, 1747,
1748, 1752
ListT, 1567
logging, 1566
lookup , 1072, 1356
loop fusion, see fusion
LTS Haskell, 754, 755
Main, 89, 1832, 1880
:main, 1837
main, 102–104, 1243, 1256,
1788, 1835, 1880
main, with arguments, 1836
many, seeAlternative
Map(type), 850, 853, 1073,1219, 1738, 1739, 1741,
1748
map, 500, 502, 506–508, 533,
534, 962, 1725
mapM_, 1836
mapM, 1289
mappend , 891, 893, 896, 899,
1216
mappend
infix, 901, 1215, 1219
Marlow, Simon, 1792, 1814
marshalling, 1475, 1476, 1486,
see also serialization
marshalling, definition, 1501
max, 286
maxBound , 143
maximum , 1277
Maybe, 128, 433–435, 462, 463,
705, 706, 708, 722, 724,
725, 727, 802, 805, 844,
871, 910, 1062, 1224,
1255, 1270, 1404, 1604,
1608, 1831
Maybe
Applicative , 1066, 1075,
1083, 1097, 1172</p>
<p>INDEX 1909
Functor , 1015
Monad, 1166, 1172, 1174
Monoid , 1066
MaybeT , 1506, 1548, 1552, 1589,
1591, 1601, 1606
mconcat , 901, 1214
memoization, 1632
memory, 143, 1731, 1733, 1736
memory leak, 1566, 1693
memory leak, definition,
1627
mempty , 891, 893, 902, 1268,
1272, 1419
min, 286
minBound , 143
minimal complete instance,
258, 1285, 1427
minimum , 1277
mod, 69, 71
mod, diﬀerence from rem, 75
module, 58, 107, 109, 117,
246, 751, 753
module
definition, 239
export, 762
import, 765modules, 175
Monad, 775, 779, 1142–1144,
1197, 1224, 1233, 1235,
1252, 1327, 1328, 1341,
1345, 1353, 1366, 1516,
1526, 1543, 1563, 1784,
1788, 1798
Monad
(&gt;&gt;), 1402
fail, 1422
IO, 1804
Reader , 1348
composition, 1200
laws, 1188
monad, 828, 839, 866, 1617
monad transformer, 1216,
1235, 1256, 1355, 1378,
1505, 1506, 1508, 1518,
1520, 1523, 1541–1544,
1547, 1571, 1576, 1591,
1595, 1604
monad transformer,
definition, 1362
monad, definition, 1209
MonadFail , 1422
MonadIO , 1597, 1598, 1600</p>
<p>INDEX 1910
MonadTrans , 1574, 1575, 1582,
1589, 1591
Monoid , 891, 892, 902, 942,
1197, 1215, 1218, 1226,
1262, 1263, 1265
Monoid
Bool, 909, 930, 932
Integer , 895
Maybe, 910–912
Monoid , of functions, 1218
monoid, 888, 890, 892, 893,
897, 902, 903, 909,
1064, 1071, 1110, 1133,
1152, 1213, 1221, 1233,
1260, 1262, 1263
monoid
commutative, 902
definition, 955
monoidal functor, 1053,
1059, 1066, 1110
monomorphism restriction,
226, 1317
Morse code, 1292
mtl(library), 1383
mutable state, 1760
mutable vector, 1757, 1758mutation, 1366, 1757, 1759,
1761, 1762
MVar, 1792, 1806, 1808
named entities, 175
natural transformation,
1034, 1038, 1132
negate , 77
negation, 84
NegativeLiterals , 630, 631
negative number, 76
nesting, 15, 42, 1161, 1787,
1790, 1798, 1804
network-uri (library), 1252
network (library), 1234, 1866,
1868
network interface, 1475
newtype, 306, 349, 350, 591,
620–622, 624, 810, 897,
898, 908, 911, 919, 1334,
1338, 1507, 1508, 1578,
1595, 1749, 1775
nf, 1712
NICTA, 1563
nil, 434
NoImplicitPrelude , 766</p>
<p>INDEX 1911
NoMonomorphismRestriction ,
226
non-exhaustive patterns,
267–269, 385
NonEmpty , 463, 937, 939
nonstrict evaluation, 460,
485–488, 500, 507
nonstrictness, 47, 196, 508,
542, 546, 561, 1150, 1630,
1632, 1633, 1635,
1657–1659, 1672, 1694,
1851, 1853
nonstrictness, sharing, 1664
normal form, 20, 21, 42, 49,
490–492, 499, 637, 639,
640, 1712, 1720, 1727,
1747
normal order, 29, 31, 33
not, 135
null, 1274
nullary, 593, 596, 611, 618
nullary constructor, 729
nullary type, 720
Num, 139, 146, 188, 249, 252,
273, 276, 1240
Numfunctions, 273number, 47
numeric literal, 41, 183, 188,
215, 219, 249, 345, 623,
1240
numeric type, 137
O’Sullivan, Bryan, 1709
Only, 1879
operator, 51, 890
operator
infix, see infix operator
operator, definition, 96
optimization, 181, 1753
Ord, 148, 150, 251, 261, 272,
284, 286, 289, 290, 292,
293, 312, 369, 371, 682,
851, 1741
Ordfunctions, 284
Ordering , 286
orphan instance, 919,
921–923, 928
otherwise, 381, 382, 385
overflow, 141, 143
OverloadedStrings , 1214, 1239,
1241, 1243, 1416, 1469,
1485, 1598, 1605, 1767</p>
<p>INDEX 1912
package, 753
parallelism, 1708
param, 1249
parameter, 7, 45, 46, 195, 209,
330–334, 438, 721
parameter, definition, 95
parametric polymorphism,
174, 209–211, 213, 239,
310
parametricity, 211, 213, 239,
314, 1039
parentheses, 54, 56, 79, 82,
84, 196, 367, 390, 392,
551, 707
parse error, 62, 64, 66, 826
parsec (library), 1425, 1465,
1468, 1470
Parser (type), 1403, 1422
parser, 1226, 1227, 1250, 1399,
1500, 1565
parser
Hutton-Meijer, 1405
parser combinator, 1399
parser combinator
definition, 1501
parsers (library), 1426, 1429Parsing (typeclass), 1427
parsing, 1396, 1398, 1401,
1460, 1461, 1465, 1468,
1474, 1486
parsing, backtracking, see
backtracking
partial application, 83, 196,
197, 202, 204, 1007, 1319
partial function, 122, 265,
266, 268, 292, 304, 347,
361, 432, 433
pattern match
non-exhaustive, 347, 348
pattern matching, 137, 164,
232, 344–347, 349, 350,
352–357, 361, 363, 375,
460, 463, 494, 496, 503,
545, 621, 647, 710, 713,
719, 1031, 1224, 1527,
1644, 1654, 1686, 1850
pattern matching
definition, 406
lazy, 1688
penguins, 355
Peyton-Jones, Simon, 1149
phantom type, 597, 601, 912,</p>
<p>INDEX 1913
1025
pipe, 134, 381, 477, 527, 593,
594
pipes(library), 1567, 1766
pointer, 898, 1749
pointfree, 392–394, 399,
400, 1673, 1684, 1736
pointfree
definition, 416
polymorphic literal, 1239
polymorphism, 113, 142, 152,
183, 208, 211, 216, 217,
283, 284, 310, 334, 488,
1047, 1317, 1521, 1662,
1681
polymorphism
ad hoc, see constrained
polymorphism
constrained, see
constrained
polymorphism
definition, 174, 239
higher-kinded, 749
parametric, see
parametric
polymorphismpragma, 624, 625
pragma
INLINABLE , 1717
LANGUAGE , 624
MINIMAL , 1261, 1285
UNPACK , 1739
precedence, 53, 55, 76, 79,
389, 1712
prefix, 51
Prelude , 155, 495, 766, 767,
1262, 1283, 1295
primary key, 1875, 1877
primitive type, 1785, 1786
principal type, 239
print it , 288
print, 101, 288, 289, 298, 299,
301, 396, 397, 784
Product (newtype), 896, 898,
901
Product (type), 644, 651, 1272
product, 459, 460, 612, 613
product , 1278
product type, 161, 354, 594,
615, 631–635, 649, 650,
798, 867
product type, definition, 526</p>
<p>INDEX 1914
profiling, 1727, 1729, 1731,
1733, 1737
prompt, 41
property test, 927
property testing, definition,
885
pseudorandom, 837, 1367,
1369
puppies, 384
pure, 1054, 1144, 1197
pure,IO, 1803
purity, 3, 298, 1798, 1799,
1825
putStr , 101
putStrLn , 101, 784, 1157
quantification
existential, see existential
quantification
universal, 1819
QuasiQuotes , 1433, 1485, 1873
queue, 1778
QuickCheck , 821, 833, 835, 871,
927, 928, 930, 1010,
1012, 1115, 1199, 1308
random (function), 787, 1370random (library), 792, 1367
random number generation,
1223, 1244, 1367, 1382
random values, 793, 837
randomRIO , 795, 797
range syntax, 203, 465, 467,
470, 493, 494
RankNTypes , 1034, 1035
Rational , 139, 145
Read, 251, 303, 304, 1250
Read, is not good, 303, 304
read, 1031
Reader , 989, 1224, 1325, 1327,
1330, 1334, 1335, 1337,
1340, 1341, 1522, 1564,
1566, 1569, 1619
Reader
Functor , 1336
Monad, 1348
ReaderT , 1354, 1355, 1557, 1561,
1564, 1566, 1569, 1585,
1619, 1676, 1808
readFile , 1766
Real, 275
RealWorld , 1785, 1786
record</p>
<p>INDEX 1915
accessor, 635, 665, 1544
syntax, 591, 635, 901
record type, 591
RecordWildCards , 1873
recursion, 420, 421, 428, 439,
494, 500, 504, 534, 535
recursion
definition, 455
guarded, 1724
tail, 587
recursive function, 436, 438,
441, 443–445, 576
recursive function
evaluation, 425, 440, 446
recursive type, 682
Redis, 1246, 1255
reduce, 3
reducible expression, 42, 48,
49
reduction, 41, 47, 80
referential transparency, 3,
1757, 1760, 1799, 1841
referential transparency, IO,
1799
refutable pattern, 1686
regular expression, 1461:reload , 47
remainder, 68
REPL, 36, 41, 47, 54
replicate , 1854
replicateM , 1245
return , 781, 782, 839, 864,
1032, 1144, 1189, 1591
runtime, 605, 620, 1824
RWST, 1565
scan, 549, 573, 574, 576
Schönfinkel, Moses, 15
Scientific , 139, 145, 1488
scope, 40, 87, 99, 108, 117, 119,
155, 223, 335, 338, 339,
608, 751, 765
scope
definition, 129
lexical, 337, 339
scotty (web framework),
1214, 1215, 1244, 1247,
1256, 1569, 1576, 1584,
1587, 1595, 1598, 1604,
1609, 1622
sectioning, 81, 83, 202, 204
semantics, 77</p>
<p>INDEX 1916
semantics
IO, 1806
Haskell, 25, 1798
program, 1799
Semigroup , 936, 937, 939, 942
semigroup, 888, 936
semigroup, definition, 955
seq, 1639, 1640, 1643, 1655,
1689, 1693
Sequence (type), 1738, 1744,
1745, 1747, 1748
sequence , 1290, 1294, 1296,
1298
sequenceA , 1285, 1289–1291,
1308, see also
Traversable
sequencing, 1144, 1149, 1154,
1161, 1227, 1787, see
Monad, 1790
serialization, 296, 303, 1397,
1475, 1486, 1491, 1501
server, 1861, 1867
Set(type), 1738, 1741
set, 133
set theory, 133, 615
Setup.hs , 846shadowing, 336–339
sharing, 1664, 1668–1670,
1672, 1677, 1681–1685,
1733, 1734, 1783, 1786,
1788, 1790, 1791
sharing, IO, 1789, 1791, 1796
Show, 136, 251, 261, 265, 288,
289, 296, 297, 299,
301–303, 396, 731, 830,
1815
Showfunctions, 297
show, 798
side eﬀect, see eﬀects
Simons, 651
smart constructor, 1234
snap(web framework), 1224
snd, 162
snoc, 1776
Snoyman, Michael, 1842
socket, 1234, 1863, 1867, 1869
some, seeAlternative
SomeException , 1817–1819,
1823, 1824, 1838
source code, 45
spine, 464, 465, 485–487, 494,
531, 545</p>
<p>INDEX 1917
spine
definition, 529
recursion, 541, 542, 544,
547, 557, 559–561
spine strict, 490, 494
splitAt , 469, 471
:sprint , 1660, 1676, 1679
SQLite, 1872, 1877, 1881
sqlite-simple (library), 1875,
1879
ST, 1366, 1758–1760, 1762,
1782, 1785, 1788
Stack, 103, 752, 753, 755, 1239,
1712, 1880
stack.yaml , 755
Stack commands, 754, 784,
846, 1826
Stack commands
build, 755, 760, 824, 827,
833, 1579, 1712, 1887
clean, 1716
exec, 756, 760, 776, 859,
1869
ghci, 36, 755, 824, 862,
1579, 1881
ghciwith options, 766ghc, 1710, 1712, 1729, 1836
init, 824
install , 139
new, 784, 1863
setup, 755
compile a binary, 1836
Stackage, 754
StackOverflow (exception),
1838
State# , 1786
State, 1354, 1366, 1367, 1371,
1378, 1404, 1406, 1564,
1566, 1759, 1761, 1784,
1785
state, 1365, 1371
StateT , 1406, 1561, 1563, 1564,
1566, 1587
static typing, 181
StdGen , 1368
stdin, 1809
stdout , 1809
streaming, 1566, 1567
Strict , 1693
StrictData , 1693
strictness, 488, 494, 497, 508,
538, 545, 561, 1217, 1631,</p>
<p>INDEX 1918
1637–1639, 1654, 1655,
1657, 1661, 1691, 1695,
1701
String , 99, 100, 106, 113, 119,
126, 128, 259, 289, 296,
297, 301, 303, 304, 482,
1239, 1669, 1670, 1762,
1766
String , definition, 128
strings, 98, 100, 105
subclass, 212
Sum(newtype), 896, 898, 1219
Sum(type), 644, 652, 1272
sum, 497, 534, 1278
sum type, 134, 152, 350, 353,
459, 503, 594, 607, 615,
616, 627, 628, 638, 640,
649, 652, 665, 709, 716,
869, 871
sum type, definition, 526
superclass, 146, 212, 293, 323,
1143
syntactic sugar, 77, 100, 180,
464, 774, 1154
syntactic sugar, definition,
96syntax, 59
System.Environment , 1835
System F, 180
tail, 120, 727
tail call, definition, 587
take, 120, 469, 470, 509, 544,
578
takeWhile , 469, 472
TCP, 1863
Template Haskell, 1435
term level, 133, 152, 161, 175
terminate, 41, 431
testing, 181
testing
property, 820, 833, 861,
862, 873
spec, 820, 822, 828
unit, 819
Text, 1223, 1239, 1247, 1253,
1256, 1670, 1762–1764,
1766, 1775
text(library), 1223, 1241,
1763, 1774
thread, 1855, 1857
threadDelay , 1840</p>
<p>INDEX 1919
throw, 1840, 1841
throwIO , 1838, 1840–1842,
1845
thunk, 1628, 1631, 1660, 1662,
1670, 1685, 1693
tie fighter, 1054
tilde, 1688, 1694
time(library), 1789
ToJSON , 1486
token (parsing), 1444, 1460
tokenize, 1460, 1461, 1465
tokenizer, definition, 1501
toList , 1273
top level, 106–108, 117
total function, 433
trace, 1665, 1677
transformer stack, 1363
transformers (library), 1378,
1383, 1564, 1566, 1569,
1585, 1598, 1610, 1619
Traversable , 1283, 1284, 1306
Traversable laws, 1307
Traversable naturality law,
1307
traverse , 857, 1285, 1287,
1289, 1291, 1292, 1295,1296, 1299, 1307, 1836
tree, binary, see binary tree
trifecta (library), 1400, 1415,
1425, 1465, 1468, 1469
Trivial (type), 258, 259, 596,
597
try(exceptions), 1832, 1833
try(parsing), 1444, 1473
tuple, 161, 198, 256, 356–358,
445, 471, 480, 613, 632,
633, 640, 721, 989, 1006,
1064, 1229, 1306
tuple
Applicative , 1064
Functor , 1064
constructor, 164
definition, 172
single element, 1879
syntax, 161, 164
typeclass instances, 1306
tuple functions, 162, 163
Turing completeness, 421
twitter-conduit (library), 1831
two’s complement, 143
type, 98, 100, 101, 132, 133,
249, 591</p>
<p>INDEX 1920
type
concrete, see concrete
type
definition, 129
higher-kinded, see
higher-kinded
lifted, 723
static, 605
unlifted, 723
type alias, 100, 165, 442, 622,
624, 633, 639, 651, 654,
707, 715, 717, 1224, see
also type synonym
type alias, definition, 174
type argument, 459,
594–597, 606, 608, 609,
613, 619, 633, 650, 674,
680, 681, 721, 722, 725,
729, 1509, 1521
type assignment, 142, 196,
279
type constant, 720, 973
type constructor, 112,
133–135, 152, 179, 184,
260, 361, 593, 595, 596,
599, 638, 720–724, 728,748, 974, 979, 1039, 1050,
1507, 1509, 1511, 1521
type constructor
definition, 173
infix, 679
type declaration, 106
type defaulting, 142, 146,
279–281, 929, 1481, 1681,
1719
type error, 114
type families, 596
type inference, 109, 180, 217,
218, 224, 279, 680, 1719
type inference, definition,
239
type level, 175
type parameter, 161
type signature, 39, 99, 100,
106, 112, 133, 181, 196,
222, 436, 442, 445, 598,
603, 720, 1718
type signature
how to read, 111, 148, 185
type synonym, 106, 442, 443,
1578, see also type alias
type theory, 615</p>
<p>INDEX 1921
type variable, 112, 176, 210,
211, 215, 239, 334, 594,
597, 600, 601
Typeable , 1815, 1823, 1824
TypeApplications , 964, 1068
typechecking, 181
typeclass, 114, 136, 140, 146,
187, 209, 249, 258, 306,
532, 624, 890, 892, 922,
979, 1466
typeclass
definition, 172, 239
dispatched by type, 304,
307, 309
unique pairing, 1039
typeclass constraint, 146, 148,
183, 187, 190, 211, 215,
219, 223, 256, 271, 272,
275, 279, 281, 292, 293,
310, 312–314, 731, 912,
1029, 1054, 1143, 1662,
1673, 1675, 1676, 1679,
1681
typeclass declaration, 305,
307, 960
typeclass deriving, 257, seealso deriving
typeclass hierarchy, 252
typeclass inheritance, 212,
275, 276
typeclass inheritance,
definition, 323
typeclass instance, 249–251,
253, 257–261, 263, 264,
270–272, 289–292, 302,
304–306, 308, 609, 622,
624, 625, 836
typeclass instance
Show, 799
how to read, 262
unique, 923
typeclass instance,
definition, 323
types vs terms, 209, 211, 249,
310, 595, 596, 605, 651
unary, 596, 612, 613, 619, 620
unconditional case, 269
uncurry, 198, 199, 201
uncurry , 1358
undefined, 236, 468, 487,
495, 508, 543, 545, 546,</p>
<p>INDEX 1922
1531, 1658
underscore, 137, 269,
345–347, 354, 375, 496,
503, 801
unfold, 742
Unicode, 100, 1253, 1769, 1771
unit, 299, 301, 782, 839
unit testing, definition, 885
unmarshalling, 1484, see also
serialization
unmarshalling, definition,
1501
UNPACK , 1739
unsafePerformIO , 1807, 1808
URL shortener, 1237
UTC time, 1789
UTF-16, 1764, 1767, 1769
UTF-8, 1253, 1256, 1415, 1764,
1769, 1773–1775, 1874
utf8-string (library), 1774
uuid(library), 1223
Validation , 1132, 1133, 1186
value, 3, 41, 45, 47, 99, 133,
135, 330–333, 595, 599,
600, 604, 618, 1676value, definition, 96
variable, 3, 7, 9, 44, 46, 99,
176, 330, 334
variable
bound, 7, 11, 13
free, 13, 15, 21
naming conventions, 176
single letter, 177
type, see type variable
Vector , 1290, 1741, 1748, 1750,
1752, 1756
Vector , mutable, 1757, 1758
vector, 1767
vector (library), 1290, 1748,
1754
vector
batch updates, 1756
boxed, 1749
slicing, 1750
unboxed, 1749
Vigenère cipher, 692, 1809
Wadler, Philip, 209, 249
Wall, 268
-Wall, 267
warning, 268, 269, 348</p>
<p>INDEX 1923
warning
non-exhaustive patterns,
267, 348
out of range, 142
pattern match overlap,
346
shadowing, 348
weak head normal form, 49,
490, 491, 493, 494, 499,
1217, 1640, 1643, 1654,
1661, 1663, 1712, 1713,
1720, 1721, 1725
web application, 1214, 1215,
1225, 1249
web framework, see scotty
web server, 1256
where, 85, 86, 88, 108, 117, 224,
261, 362, 384whitespace, 59
whnf, see weak head normal
form
whnf, 1712
Windows, 1861
Word8, 1767
words, 794
wreq(library), 1300
writeFile , 1826, 1829
Writer , 1564, 1566
WriterT , 1564, 1566
XML, 1397
xmonad , 1216, 1220
Y combinator, 421, 428
zip, 514, 1121
zipList , 1120, 1123
zipWith , 515, 806, 1152</p>
<div style="break-before: page; page-break-before: always;"></div><p>CSAPP</p>
<div style="break-before: page; page-break-before: always;"></div><p>Computer	Systems
A	Programmer's	Perspective</p>
<p>Computer	Systems
A	Programmer's	Perspective
Third	Edition
Randal	E.	Bryant
Carnegie	Mellon	University
David	R.	O'Hallaron
Carnegie	Mellon	University
Pearson
Boston
 
Columbus
 
Hoboken
 
Indianapolis
 
New	York	San
Francisco
 
Amsterdam
 
Cape	Town
 
Dubai
 
London
 
Madrid
 
Milan
 
Munich
 
Paris
 
Montreal
 
Toronto
 
Delhi
 
Mexico
 
City
 
Sao
Paulo
 
Sydney
 
Hong	Kong
 
Seoul
 
Singapore
 
Taipei
 
Tokyo</p>
<p>Vice	President	and	Editorial	Director:	Marcia	J.	Horton
Executive	Editor:	Matt	Goldstein
Editorial	Assistant:	Kelsey	Loanes
VP	of	Marketing:	Christy	Lesko
Director	of	Field	Marketing:	Tim	Galligan
Product	Marketing	Manager:	Bram	van	Kempen
Field	Marketing	Manager:	Demetrius	Hall
Marketing	Assistant:	Jon	Bryant
Director	of	Product	Management:	Erin	Gregg
Team	Lead	Product	Management:	Scott	Disanno
Program	Manager:	Joanne	Manning
Procurement	Manager:	Mary	Fischer
Senior	Specialist,	Program	Planning	and	Support:	Maura	Zaldivar-Garcia
over	Designer:	Joyce	Wells
Manager,	Rights	Management:	Rachel	Youdelman</p>
<p>Associate	Project	Manager,	Rights	Management:	William	J.	Opaluch
Full-Service	Project	Management:	Paul	Anagnostopoulos,	Windfall
Software
Composition:	Windfall	Software
Printer/Binder:	Courier	Westford
Cover	Printer:	Courier	Westford
Typeface:	10/12	Times	10,	ITC	Stone	Sans
The	graph	on	the	front	cover	is	a	&quot;memory	mountain&quot;	that	shows	the
measured	read	throughput	of	an	Intel	Core	i7	processor	as	a	function	of
spatial	and	temporal	locality.
Copyright	©	2016,	2011,	and	2003	by	Randal	E.	Bryant	and	David	R.
O'Hallaron.
All	Rights	Reserved.	Printed	in	the	United	States	of	America.
This	publication	is	protected	by	copyright,	and	permission	should	be
obtained	from	the	publisher	prior	to	any	prohibited	reproduction,	storage
in	a	retrieval	system,	or	transmission	in	any	form	or	by	any	means,
electronic,	mechanical,	photocopying,	recording,	or	otherwise.	For
information	regarding	permissions,	request	forms	and	the	appropriate
contacts	within	the	Pearson	Education	Global	Rights	&amp;	Permissions
department,	please	visit	
www.pearsoned.com/
permissions/
.
Many	of	the	designations	by	manufacturers	and	seller	to	distinguish	their
products	are	claimed	as	trademarks.	Where	those	designations	appear	in</p>
<p>this	book,	and	the	publisher	was	aware	of	a	trademark	claim,	the
designations	have	been	printed	in	initial	caps	or	all	caps.
The	author	and	publisher	of	this	book	have	used	their	best	efforts	in
preparing	this	book.	These	efforts	include	the	development,	research,
and	testing	of	theories	and	programs	to	determine	their	effectiveness.
The	author	and	publisher	make	no	warranty	of	any	kind,	expressed	or
implied,	with	regard	to	these	programs	or	the	documentation	contained	in
this	book.	The	author	and	publisher	shall	not	be	liable	in	any	event	for
incidental	or	consequential	damages	with,	or	arising	out	of,	the
furnishing,	performance,	or	use	of	these	programs.
Pearson	Education	Ltd.,	
London
Pearson	Education	Singapore,	Pte.	Ltd
Pearson	Education	Canada,	Inc.
Pearson	Education—Japan
Pearson	Education	Australia	PTY,	Limited
Pearson	Education	North	Asia,	Ltd.,	
Hong	Kong
Pearson	Educaciń	de	Mexico,	S.A.	de	C.V.
Pearson	Education	Malaysia,	Pte.	Ltd.
Pearson	Education,	Inc.,	
Upper	Saddle	River,	New	Jersey</p>
<p>Library	of	Congress	Cataloging-in-Publication	Data
Bryant,	Randal	E.
 
Computer	systems	:	a	programmer's	perspective	/	Randal	E.	Bryant,
Carnegie	Mellon	University,	David	R.	O'Hallaron,	Carnegie	Mellon.
University.—Third	edition.
   
pages	cm
 
Includes	bibliographical	references	and	index.
 
ISBN	978-0-13-409266-9—ISBN	0-13-409266-X
 </p>
<ol>
<li>Computer	systems.	2.	Computers.	3.	Telecommunication.	4.	User
interfaces	(Computer	systems)	I.	O'Hallaron,	David	R.	(David	Richard)	II.
Title.
 
QA76.5.B795	2016
 
005.3—
dc23
                                                                                   
2015000930
10	9	8	7	6	5	4	3	2	1
www.pearsonhighered.com</li>
</ol>
<p> 
ISBN	10:	0-13-409266-X
ISBN	13:	978-0-13-409266-9</p>
<p>To	the	students	and	instructors	of	the	15−213	course	at
Carnegie	Mellon	University,	for	inspiring	us	to	develop	and
refine	the	material	for	this	book.</p>
<p>MasteringEngineering
For	
Computer	Systems:	A	Programmer's	Perspective
,	Third	Edition
Mastering	is	Pearson's	proven	online	Tutorial	Homework	program,	newly
available	with	the	third	edition	of	
Computer	Systems:	A	Programmer's
Perspective
.	The	Mastering	platform	allows	you	to	integrate	dynamic
homework—with	many	problems	taken	directly	from	the
Bryant/O'Hallaron	textbook—with	automatic	grading.	Mastering	allows
you	to	easily	track	the	performance	of	your	entire	class	on	an
assignment-by-assignment	basis,	or	view	the	detailed	work	of	an
individual	student.
For	more	information	or	a	demonstration	of	the	course,	visit
www.MasteringEngineering.com
or	contact	your	local	Pearson
representative.
®</p>
<p>Contents
Preface	
xix
About	the	Authors	
xxxv
1	
A	Tour	of	Computer	Systems	
1
1.1	
Information	Is	Bits	+	Context	
3
1.2	
Programs	Are	Translated	by	Other	Programs	into	Different
Forms	
4
1.3	
It	Pays	to	Understand	How	Compilation	Systems	Work	
6
1.4	
Processors	Read	and	Interpret	Instructions	Stored	in	Memory
7
1.4.1	
Hardware	Organization	of	a	System	
8
1.4.2	
Running	the	
Program	
10
1.5	
Caches	Matter	
11
1.6	
Storage	Devices	Form	a	Hierarchy	
14
1.7	
The	Operating	System	Manages	the	Hardware	
14
1.7.1	
Processes	
15
1.7.2	
Threads	
17
1.7.3	
Virtual	Memory	
18
1.7.4	
Files	
19</p>
<p>1.8	
Systems	Communicate	with	Other	Systems	Using	Networks
19
1.9	
Important	Themes	
22
1.9.1	
Amdahl's	Law	
22
1.9.2	
Concurrency	and	Parallelism	
24
1.9.3	
The	Importance	of	Abstractions	in	Computer	Systems	
26
1.10	
Summary</p>
<p>27
Bibliographic	Notes	
28
Solutions	to	Practice	Problems	
28
Part	
I	
Program	Structure	and	Execution
2	
Representing	and	Manipulating	Information	
31
2.1	
Information	Storage	
34
2.1.1	
Hexadecimal	Notation	
36
2.1.2	
Data	Sizes	
39
2.1.3	
Addressing	and	Byte	Ordering	
42
2.1.4	
Representing	Strings	
49
2.1.5	
Representing	Code	
49
2.1.6	
Introduction	to	Boolean	Algebra	
50
2.1.7	
Bit-Level	Operations	in	C	
54
2.1.8	
Logical	Operations	in	C	
56
2.1.9	
Shift	Operations	in	C	
57</p>
<p>2.2	
Integer	Representations	
59
2.2.1	
Integral	Data	Types	
60
2.2.2	
Unsigned	Encodings	
62
2.2.3	
Two's-Complement	Encodings	
64
2.2.4	
Conversions	between	Signed	and	Unsigned	
70
2.2.5	
Signed	versus	Unsigned	in	C	
74
2.2.6	
Expanding	the	Bit	Representation	of	a	Number	
76
2.2.7	
Truncating	Numbers	
81
2.2.8	
Advice	on	Signed	versus	Unsigned	
83
2.3	
Integer	Arithmetic	
84
2.3.1	
Unsigned	Addition	
84
2.3.2	
Two's-Complement	Addition	
90
2.3.3	
Two's-Complement	Negation	
95
2.3.4	
Unsigned	Multiplication	
96
2.3.5	
Two's-Complement	Multiplication	
97
2.3.6	
Multiplying	by	Constants	
101
2.3.7	
Dividing	by	Powers	of	2	
103
2.3.8	
Final	Thoughts	on	Integer	Arithmetic	
107
2.4	
Floating	Point	
108
2.4.1	
Fractional	Binary	Numbers	
109</p>
<p>2.4.2	
IEEE	Floating-Point	Representation	
112
2.4.3	
Example	Numbers	
115
2.4.4	
Rounding	
120
2.4.5	
Floating-Point	Operations	
122
2.4.6	
Floating	Point	in	C	
124
2.5	
Summary</p>
<p>126
Bibliographic	Notes	
127
Homework	Problems	
128
Solutions	to	Practice	Problems	
143
3	
Machine-Level	Representation	of	Programs	
163
3.1	
A	Historical	Perspective	
166
3.2	
Program	Encodings	
169
3.2.1	
Machine-Level	Code	
170
3.2.2	
Code	Examples	
172
3.2.3	
Notes	on	Formatting	
175
3.3	
Data	Formats	
177
3.4	
Accessing	Information	
179
3.4.1	
Operand	Specifiers	
180
3.4.2	
Data	Movement	Instructions	
182
3.4.3	
Data	Movement	Example	
186</p>
<p>3.4.4	
Pushing	and	Popping	Stack	Data	
189
3.5	
Arithmetic	and	Logical	Operations	
191
3.5.1	
Load	Effective	Address	
191
3.5.2	
Unary	and	Binary	Operations	
194
3.5.3	
Shift	Operations	
194
3.5.4	
Discussion	
196
3.5.5	
Special	Arithmetic	Operations	
197
3.6	
Control	
200
3.6.1	
Condition	Codes	
201
3.6.2	
Accessing	the	Condition	Codes	
202
3.6.3	
Jump	Instructions	
205
3.6.4	
Jump	Instruction	Encodings	
207
3.6.5	
Implementing	Conditional	Branches	with	Conditional
Control	
209
3.6.6	
Implementing	Conditional	Branches	with	Conditional
Moves	
214
3.6.7	
Loops	
220
3.6.8	
Switch	Statements	
232
3.7	
Procedures	
238
3.7.1	
The	Run-Time	Stack	
239
3.7.2	
Control	Transfer	
241</p>
<p>3.7.3	
Data	Transfer	
245
3.7.4	
Local	Storage	on	the	Stack	
248
3.7.5	
Local	Storage	in	Registers	
251
3.7.6	
Recursive	Procedures	
253
3.8	
Array	Allocation	and	Access	
255
3.8.1	
Basic	Principles	
255
3.8.2	
Pointer	Arithmetic	
257
3.8.3	
Nested	Arrays	
258
3.8.4	
Fixed-Size	Arrays	
260
3.8.5	
Variable-Size	Arrays	
262
3.9	
Heterogeneous	Data	Structures	
265
3.9.1	
Structures	
265
3.9.2	
Unions	
269
3.9.3	
Data	Alignment	
273
3.10	
Combining	Control	and	Data	in	Machine-Level	Programs
276
3.10.1	
Understanding	Pointers	
277
3.10.2	
Life	in	the	Real	World:	Using	the	
GDB
Debugger	
279
3.10.3	
Out-of-Bounds	Memory	References	and	Buffer
Overflow	
279</p>
<p>3.10.4	
Thwarting	Buffer	Overflow	Attacks	
284
3.10.5	
Supporting	Variable-Size	Stack	Frames	
290
3.11	
Floating-Point	Code	
293
3.11.1	
Floating-Point	Movement	and	Conversion
Operations	
296
3.11.2	
Floating-Point	Code	in	Procedures	
301
3.11.3	
Floating-Point	Arithmetic	Operations	
302
3.11.4	
Defining	and	Using	Floating-Point	Constants	
304
3.11.5	
Using	Bitwise	Operations	in	Floating-Point	Code	
305
3.11.6	
Floating-Point	Comparison	Operations	
306
3.11.7	
Observations	about	Floating-Point	Code	
309
3.12	
Summary</p>
<p>309
Bibliographic	Notes	
310
Homework	Problems	
311
Solutions	to	Practice	Problems	
325
4	
Processor	Architecture	
351
4.1	
The	Y86-64	Instruction	Set	Architecture	
355
4.1.1	
Programmer-Visible	State	
355
4.1.2	
Y86-64	Instructions	
356
4.1.3	
Instruction	Encoding	
358</p>
<p>4.1.4	
Y86-64	Exceptions	
363
4.1.5	
Y86-64	Programs	
364
4.1.6	
Some	Y86-64	Instruction	Details	
370
4.2	
Logic	Design	and	the	Hardware	Control	Language	HCL
372
4.2.1	
Logic	Gates	
373
4.2.2	
Combinational	Circuits	and	HCL	Boolean
Expressions	
374
4.2.3	
Word-Level	Combinational	Circuits	and	HCL	Integer
Expressions	
376
4.2.4	
Set	Membership	
380
4.2.5	
Memory	and	Clocking	
381
4.3	
Sequential	Y86-64	Implementations	
384
4.3.1	
Organizing	Processing	into	Stages	
384
4.3.2	
SEQ	Hardware	Structure	
396
4.3.3	
SEQ	Timing	
400
4.3.4	
SEQ	Stage	Implementations	
404
4.4	
General	Principles	of	Pipelining	
412
4.4.1	
Computational	Pipelines	
412
4.4.2	
A	Detailed	Look	at	Pipeline	Operation	
414
4.4.3	
Limitations	of	Pipelining	
416</p>
<p>4.4.4	
Pipelining	a	System	with	Feedback	
419
4.5	
Pipelined	Y86-64	Implementations	
421
4.5.1	
SEQ+:	Rearranging	the	Computation	Stages	
421
4.5.2	
Inserting	Pipeline	Registers	
422
4.5.3	
Rearranging	and	Relabeling	Signals	
426
4.5.4	
Next	PC	Prediction	
427
4.5.5	
Pipeline	Hazards	
429
4.5.6	
Exception	Handling	
444
4.5.7	
PIPE	Stage	Implementations	
447
4.5.8	
Pipeline	Control	Logic	
455
4.5.9	
Performance	Analysis	
464
4.5.10	
Unfinished	Business	
468
4.6	
Summary</p>
<p>470
4.6.1	
Y86-64	Simulators	
472
Bibliographic	Notes	
473
Homework	Problems	
473
Solutions	to	Practice	Problems	
480
5	
Optimizing	Program	Performance	
495
5.1	
Capabilities	and	Limitations	of	Optimizing	Compilers	
498
5.2	
Expressing	Program	Performance	
502</p>
<p>5.3	
Program	Example	
504
5.4	
Eliminating	Loop	Inefficiencies	
508
5.5	
Reducing	Procedure	Calls	
512
5.6	
Eliminating	Unneeded	Memory	References	
514
5.7	
Understanding	Modern	Processors	
517
5.7.1	
Overall	Operation	
518
5.7.2	
Functional	Unit	Performance	
523
5.7.3	
An	Abstract	Model	of	Processor	Operation	
525
5.8	
Loop	Unrolling</p>
<p>531
5.9	
Enhancing	Parallelism</p>
<p>536
5.9.1	
Multiple	Accumulators	
536
5.9.2	
Reassociation	Transformation	
541
5.10	
Summary	of	Results	for	Optimizing	Combining	Code	
547
5.11	
Some	Limiting	Factors	
548
5.11.1	
Register	Spilling	
548
5.11.2	
Branch	Prediction	and	Misprediction	Penalties	
549
5.12	
Understanding	Memory	Performance	
553
5.12.1	
Load	Performance	
554
5.12.2	
Store	Performance	
555
5.13	
Life	in	the	Real	World:	Performance	Improvement</p>
<p>Techniques	
561
5.14	
Identifying	and	Eliminating	Performance	Bottlenecks	
562
5.14.1	
Program	Profiling	
562
5.14.2	
Using	a	Profiler	to	Guide	Optimization	
565
5.15	
Summary</p>
<p>568
Bibliographic	Notes	
569
Homework	Problems	
570
Solutions	to	Practice	Problems	
573
6	
The	Memory	Hierarchy	
579
6.1	
Storage	Technologies	
581
6.1.1	
Random	Access	Memory	
581
6.1.2	
Disk	Storage	
589
6.1.3	
Solid	State	Disks	
600
6.1.4	
Storage	Technology	Trends	
602
6.2	
Locality	
604
6.2.1	
Locality	of	References	to	Program	Data	
606
6.2.2	
Locality	of	Instruction	Fetches	
607
6.2.3	
Summary	of	Locality	
608
6.3	
The	Memory	Hierarchy	
609
6.3.1	
Caching	in	the	Memory	Hierarchy	
610</p>
<p>6.3.2	
Summary	of	Memory	Hierarchy	Concepts	
614
6.4	
Cache	Memories	
614
6.4.1	
Generic	Cache	Memory	Organization	
615
6.4.2	
Direct-Mapped	Caches	
617
6.4.3	
Set	Associative	Caches	
624
6.4.4	
Fully	Associative	Caches	
626
6.4.5	
Issues	with	Writes	
630
6.4.6	
Anatomy	of	a	Real	Cache	Hierarchy	
631</p>
<p>6.4.7	
Performance	Impact	of	Cache	Parameters	
631
6.5	
Writing	Cache-Friendly	Code	
633
6.6	
Putting	It	Together:	The	Impact	of	Caches	on	Program
Performance	
639
6.6.1	
The	Memory	Mountain	
639
6.6.2	
Rearranging	Loops	to	Increase	Spatial	Locality	
643
6.6.3	
Exploiting	Locality	in	Your	Programs	
647
6.7	
Summary</p>
<p>648
Bibliographic	Notes	
648
Homework	Problems	
649
Solutions	to	Practice	Problems	
660
Part	
II	
Running	Programs	on	a	System
7	
Linking	
669
7.1	
Compiler	Drivers	
671
7.2	
Static	Linking	
672
7.3	
Object	Files	
673
7.4	
Relocatable	Object	Files	
674
7.5	
Symbols	and	Symbol	Tables	
675
7.6	
Symbol	Resolution	
679
7.6.1	
How	Linkers	Resolve	Duplicate	Symbol	Names	
680
7.6.2	
Linking	with	Static	Libraries	
684</p>
<p>7.6.3	
How	Linkers	Use	Static	Libraries	to	Resolve
References	
688
7.7	
Relocation	
689
7.7.1	
Relocation	Entries	
690
7.7.2	
Relocating	Symbol	References	
691
7.8	
Executable	Object	Files	
695
7.9	
Loading	Executable	Object	Files	
697
7.10	
Dynamic	Linking	with	Shared	Libraries	
698
7.11	
Loading	and	Linking	Shared	Libraries	from	Applications
701
7.12	
Position-Independent	Code	(PIC)	
704
7.13	
Library	Interpositioning	
707
7.13.1	
Compile-Time	Interpositioning	
708
7.13.2	
Link-Time	Interpositioning	
708
7.13.3	
Run-Time	Interpositioning	
710
7.14	
Tools	for	Manipulating	Object	Files	
713
7.15	
Summary</p>
<p>713
Bibliographic	Notes	
714
Homework	Problems	
714
Solutions	to	Practice	Problems	
717</p>
<p>8	
Exceptional	Control	Flow	
721
8.1	
Exceptions	
723
8.1.1	
Exception	Handling	
724
8.1.2	
Classes	of	Exceptions	
726
8.1.3	
Exceptions	in	Linux/x86-64	Systems	
729
8.2	
Processes	
732
8.2.1	
Logical	Control	Flow	
732
8.2.2	
Concurrent	Flows	
733
8.2.3	
Private	Address	Space	
734
8.2.4	
User	and	Kernel	Modes	
734
8.2.5	
Context	Switches	
736
8.3	
System	Call	Error	Handling	
737
8.4	
Process	Control	
738
8.4.1	
Obtaining	Process	IDs	
739
8.4.2	
Creating	and	Terminating	Processes	
739
8.4.3	
Reaping	Child	Processes	
743
8.4.4	
Putting	Processes	to	Sleep	
749
8.4.5	
Loading	and	Running	Programs	
750
8.4.6	
Using	
to	Run	Programs	
753</p>
<p>8.5	
Signals	
756
8.5.1	
Signal	Terminology	
758
8.5.2	
Sending	Signals	
759
8.5.3	
Receiving	Signals	
762
8.5.4	
Blocking	and	Unblocking	Signals	
764
8.5.5	
Writing	Signal	Handlers	
766
8.5.6	
Synchronizing	Flows	to	Avoid	Nasty	Concurrency
Bugs	
776
8.5.7	
Explicitly	Waiting	for	Signals	
778
8.6	
Nonlocal	Jumps	
781
8.7	
Tools	for	Manipulating	Processes	
786
8.8	
Summary</p>
<p>787
Bibliographic	Notes	
787
Homework	Problems	
788
Solutions	to	Practice	Problems	
795
9	
Virtual	Memory	
801
9.1	
Physical	and	Virtual	Addressing	
803
9.2	
Address	Spaces	
804
9.3	
VM	as	a	Tool	for	Caching	
805
9.3.1	
DRAM	Cache	Organization	
806</p>
<p>9.3.2	
Page	Tables	
806
9.3.3	
Page	Hits	
808
9.3.4	
Page	Faults	
808
9.3.5	
Allocating	Pages	
810
9.3.6	
Locality	to	the	Rescue	Again	
810
9.4	
VM	as	a	Tool	for	Memory	Management	
811
9.5	
VM	as	a	Tool	for	Memory	Protection	
812
9.6	
Address	Translation	
813
9.6.1	
Integrating	Caches	and	VM	
817
9.6.2	
Speeding	Up	Address	Translation	with	a	TLB	
817
9.6.3	
Multi-Level	Page	Tables	
819
9.6.4	
Putting	It	Together:	End-to-End	Address	Translation
821
9.7	
Case	Study:	The	Intel	Core	i7/Linux	Memory	System	
825
9.7.1	
Core	i7	Address	Translation	
826
9.7.2	
Linux	Virtual	Memory	System	
828
9.8	
Memory	Mapping	
833
9.8.1	
Shared	Objects	Revisited	
833
9.8.2	
The	
Function	Revisited	
836
9.8.3	
The	
Function	Revisited	
836</p>
<p>9.8.4	
User-Level	Memory	Mapping	with	the	
Function
837
9.9	
Dynamic	Memory	Allocation	
839
9.9.1	
The	
and	
Functions	
840
9.9.2	
Why	Dynamic	Memory	Allocation?	
843
9.9.3	
Allocator	Requirements	and	Goals	
844
9.9.4	
Fragmentation	
846
9.9.5	
Implementation	Issues	
846
9.9.6	
Implicit	Free	Lists	
847
9.9.7	
Placing	Allocated	Blocks	
849
9.9.8	
Splitting	Free	Blocks	
849
9.9.9	
Getting	Additional	Heap	Memory	
850
9.9.10	
Coalescing	Free	Blocks	
850
9.9.11	
Coalescing	with	Boundary	Tags	
851
9.9.12	
Putting	It	Together:	Implementing	a	Simple	Allocator
854
9.9.13	
Explicit	Free	Lists	
862
9.9.14	
Segregated	Free	Lists	
863
9.10	
Garbage	Collection	
865
9.10.1	
Garbage	Collector	Basics	
866</p>
<p>9.10.2	
Mark&amp;Sweep	Garbage	Collectors	
867
9.10.3	
Conservative	Mark&amp;Sweep	for	C	Programs	
869
9.11	
Common	Memory-Related	Bugs	in	C	Programs	
870
9.11.1	
Dereferencing	Bad	Pointers	
870
9.11.2	
Reading	Uninitialized	Memory	
871
9.11.3	
Allowing	Stack	Buffer	Overflows	
871
9.11.4	
Assuming	That	Pointers	and	the	Objects	They	Point
to	Are	the	Same	Size	
872
9.11.5	
Making	Off-by-One	Errors	
872
9.11.6	
Referencing	a	Pointer	Instead	of	the	Object	It	Points
To	
873
9.11.7	
Misunderstanding	Pointer	Arithmetic	
873
9.11.8	
Referencing	Nonexistent	Variables	
874
9.11.9	
Referencing	Data	in	Free	Heap	Blocks	
874
9.11.10	
Introducing	Memory	Leaks	
875
9.12	
Summary</p>
<p>875
Bibliographic	Notes	
876
Homework	Problems	
876
Solutions	to	Practice	Problems	
880
Part	
III	
Interaction	and	Communication	between	Programs</p>
<p>10	
System-Level	I/O	
889
10.1	
Unix	I/O	
890
10.2	
Files	
891
10.3	
Opening	and	Closing	Files	
893
10.4	
Reading	and	Writing	Files	
895
10.5	
Robust	Reading	and	Writing	with	the	R
IO
Package	
897
10.5.1	
R
IO
Unbuffered	Input	and	Output	Functions	
897
10.5.2	
R
IO
Buffered	Input	Functions	
898
10.6	
Reading	File	Metadata	
903
10.7	
Reading	Directory	Contents	
905
10.8	
Sharing	Files	
906
10.9	
I/O	Redirection	
909
10.10	
Standard	I/O	
911
10.11	
Putting	It	Together:	Which	I/O	Functions	Should	I	Use?
911
10.12	
Summary</p>
<p>913
Bibliographic	Notes	
914
Homework	Problems	
914
Solutions	to	Practice	Problems	
915
11	
Network	Programming	
917</p>
<p>11.1	
The	Client-Server	Programming	Model	
918
11.2	
Networks	
919
11.3	
The	Global	IP	Internet	
924
11.3.1	
IP	Addresses	
925
11.3.2	
Internet	Domain	Names	
927
11.3.3	
Internet	Connections	
929
11.4	
The	Sockets	Interface	
932
11.4.1	
Socket	Address	Structures	
933
11.4.2	
The	
Function	
934
11.4.3	
The	
Function	
934
11.4.4	
The	
Function	
935
11.4.5	
The	
Function	
935
11.4.6	
The	
Function	
936
11.4.7	
Host	and	Service	Conversion	
937
11.4.8	
Helper	Functions	for	the	Sockets	Interface	
942
11.4.9	
Example	Echo	Client	and	Server	
944
11.5	
Web	Servers	
948
11.5.1	
Web	Basics	
948
11.5.2	
Web	Content	
949
11.5.3	
HTTP	Transactions	
950</p>
<p>11.5.4	
Serving	Dynamic	Content	
953
11.6	
Putting	It	Together:	The	
TINY
Web	Server	
956
11.7	
Summary</p>
<p>964
Bibliographic	Notes	
965
Homework	Problems	
965
Solutions	to	Practice	Problems	
966
12	
Concurrent	Programming	
971
12.1	
Concurrent	Programming	with	Processes	
973
12.1.1	
A	Concurrent	Server	Based	on	Processes	
974
12.1.2	
Pros	and	Cons	of	Processes	
975
12.2	
Concurrent	Programming	with	I/O	Multiplexing	
977
12.2.1	
A	Concurrent	Event-Driven	Server	Based	on	I/O
Multiplexing	
980
12.2.2	
Pros	and	Cons	of	I/O	Multiplexing	
985
12.3	
Concurrent	Programming	with	Threads	
985
12.3.1	
Thread	Execution	Model	
986
12.3.2	
Posix	Threads	
987
12.3.3	
Creating	Threads	
988
12.3.4	
Terminating	Threads	
988
12.3.5	
Reaping	Terminated	Threads	
989</p>
<p>12.3.6	
Detaching	Threads	
989
12.3.7	
Initializing	Threads	
990
12.3.8	
A	Concurrent	Server	Based	on	Threads	
991
12.4	
Shared	Variables	in	Threaded	Programs	
992
12.4.1	
Threads	Memory	Model	
993
12.4.2	
Mapping	Variables	to	Memory	
994
12.4.3	
Shared	Variables	
995
12.5	
Synchronizing	Threads	with	Semaphores	
995
12.5.1	
Progress	Graphs	
999
12.5.2	
Semaphores	
1001
12.5.3	
Using	Semaphores	for	Mutual	Exclusion	
1002
12.5.4	
Using	Semaphores	to	Schedule	Shared	Resources
1004
12.5.5	
Putting	It	Together:	A	Concurrent	Server	Based	on
Prethreading	
1008
12.6	
Using	Threads	for	Parallelism	
1013
12.7	
Other	Concurrency	Issues	
1020
12.7.1	
Thread	Safety	
1020
12.7.2	
Reentrancy	
1023
12.7.3	
Using	Existing	Library	Functions	in	Threaded
Programs	
1024</p>
<p>12.7.4	
Races	
1025
12.7.5	
Deadlocks	
1027
12.8	
Summary</p>
<p>1030
Bibliographic	Notes	
1030
Homework	Problems	
1031
Solutions	to	Practice	Problems	
1036
A	
Error	Handling	
1041
A.1	
Error	Handling	in	Unix	Systems	
1042
A.2	
Error-Handling	Wrappers	
1043
References	
1047
Index	
1053</p>
<p>Preface
This	book	(known	as	CS:APP)	is	for	computer	scientists,	computer
engineers,	and	others	who	want	to	be	able	to	write	better	programs	by
learning	what	is	going	on	&quot;under	the	hood&quot;	of	a	computer	system.
Our	aim	is	to	explain	the	enduring	concepts	underlying	all	computer
systems,	and	to	show	you	the	concrete	ways	that	these	ideas	affect	the
correctness,	performance,	and	utility	of	your	application	programs.	Many
systems	books	are	written	from	a	
builder's	perspective
,	describing	how	to
implement	the	hardware	or	the	systems	software,	including	the	operating
system,	compiler,	and	network	interface.	This	book	is	written	from	a
programmer's	perspective
,	describing	how	application	programmers	can
use	their	knowledge	of	a	system	to	write	better	programs.	Of	course,
learning	what	a	system	is	supposed	to	do	provides	a	good	first	step	in
learning	how	to	build	one,	so	this	book	also	serves	as	a	valuable
introduction	to	those	who	go	on	to	implement	systems	hardware	and
software.	Most	systems	books	also	tend	to	focus	on	just	one	aspect	of
the	system,	for	example,	the	hardware	architecture,	the	operating
system,	the	compiler,	or	the	network.	This	book	spans	all	of	these
aspects,	with	the	unifying	theme	of	a	programmer's	perspective.
If	you	study	and	learn	the	concepts	in	this	book,	you	will	be	on	your	way
to	becoming	the	rare	
power	programmer
who	knows	how	things	work	and
how	to	fix	them	when	they	break.	You	will	be	able	to	write	programs	that
make	better	use	of	the	capabilities	provided	by	the	operating	system	and
systems	software,	that	operate	correctly	across	a	wide	range	of	operating</p>
<p>conditions	and	run-time	parameters,	that	run	faster,	and	that	avoid	the
flaws	that	make	programs	vulnerable	to	cyberattack.	You	will	be	prepared
to	delve	deeper	into	advanced	topics	such	as	compilers,	computer
architecture,	operating	systems,	embedded	systems,	networking,	and
cybersecurity.
Assumptions	about	the	Reader's
Background
This	book	focuses	on	systems	that	execute	x86-64	machine	code.	x86-
64	is	the	latest	in	an	evolutionary	path	followed	by	Intel	and	its
competitors	that	started	with	the	8086	microprocessor	in	1978.	Due	to	the
naming	conventions	used	by	Intel	for	its	microprocessor	line,	this	class	of
microprocessors	is	referred	to	colloquially	as	&quot;x86.&quot;	As	semiconductor
technology	has	evolved	to	allow	more	transistors	to	be	integrated	onto	a
single	chip,	these	processors	have	progressed	greatly	in	their	computing
power	and	their	memory	capacity.	As	part	of	this	progression,	they	have
gone	from	operating	on	16-bit	words,	to	32-bit	words	with	the	introduction
of	IA32	processors,	and	most	recently	to	64-bit	words	with	x86-64.
We	consider	how	these	machines	execute	C	programs	on	Linux.	Linux	is
one	of	a	number	of	operating	systems	having	their	heritage	in	the	Unix
operating	system	developed	originally	by	Bell	Laboratories.	Other
members	of	this	class
New	to	C?	
Advice	on	the	C</p>
<p>programming	language
To	help	readers	whose	background	in	C	programming	is	weak	(or
nonexistent),	we	have	also	included	these	special	notes	to
highlight	features	that	are	especially	important	in	C.	We	assume
you	are	familiar	with	C++	or	Java.
of	operating	systems	include	Solaris,	FreeBSD,	and	MacOS	X.	In	recent
years,	these	operating	systems	have	maintained	a	high	level	of
compatibility	through	the	efforts	of	the	Posix	and	Standard	Unix
Specification	standardization	efforts.	Thus,	the	material	in	this	book
applies	almost	directly	to	these	&quot;Unix-like&quot;	operating	systems.
The	text	contains	numerous	programming	examples	that	have	been
compiled	and	run	on	Linux	systems.	We	assume	that	you	have	access	to
such	a	machine,	and	are	able	to	log	in	and	do	simple	things	such	as
listing	files	and	changing	directories.	If	your	computer	runs	Microsoft
Windows,	we	recommend	that	you	install	one	of	the	many	different	virtual
machine	environments	(such	as	VirtualBox	or	VMWare)	that	allow
programs	written	for	one	operating	system	(the	guest	OS)	to	run	under
another	(the	host	OS).
We	also	assume	that	you	have	some	familiarity	with	C	or	C++.	If	your
only	prior	experience	is	with	Java,	the	transition	will	require	more	effort
on	your	part,	but	we	will	help	you.	Java	and	C	share	similar	syntax	and
control	statements.	However,	there	are	aspects	of	C	(particularly
pointers,	explicit	dynamic	memory	allocation,	and	formatted	I/O)	that	do
not	exist	in	Java.	Fortunately,	C	is	a	small	language,	and	it	is	clearly	and
beautifully	described	in	the	classic	&quot;K&amp;R&quot;	text	by	Brian	Kernighan	and
Dennis	Ritchie	[
61
].	Regardless	of	your	programming	background,</p>
<h2>consider	K&amp;R	an	essential	part	of	your	personal	systems	library.	If	your
prior	experience	is	with	an	interpreted	language,	such	as	Python,	Ruby,
or	Perl,	you	will	definitely	want	to	devote	some	time	to	learning	C	before
you	attempt	to	use	this	book.
Several	of	the	early	chapters	in	the	book	explore	the	interactions
between	C	programs	and	their	machine-language	counterparts.	The
machine-language	examples	were	all	generated	by	the	GNU	
GCC
compiler	running	on	x86-64	processors.	We	do	not	assume	any	prior
experience	with	hardware,	machine	language,	or	assembly-language
programming.
How	to	Read	the	Book
Learning	how	computer	systems	work	from	a	programmer's	perspective
is	great	fun,	mainly	because	you	can	do	it	actively.	Whenever	you	learn
something	new,	you	can	try	it	out	right	away	and	see	the	result	firsthand.
In	fact,	we	believe	that	the	only	way	to	learn	systems	is	to	
do
systems,
either	working	concrete	problems	or	writing	and	running	programs	on	real
systems.
This	theme	pervades	the	entire	book.	When	a	new	concept	is	introduced,
it	is	followed	in	the	text	by	one	or	more	
practice	problems
that	you	should
work</h2>
<p>code/intro/hello.c</p>
<hr />
<p>code/intro/hello.c
Figure	
1	
A	typical	code	example.
immediately	to	test	your	understanding.	Solutions	to	the	practice
problems	are	at	the	end	of	each	chapter.	As	you	read,	try	to	solve	each
problem	on	your	own	and	then	check	the	solution	to	make	sure	you	are
on	the	right	track.	Each	chapter	is	followed	by	a	set	of	
homework
problems
of	varying	difficulty.	Your	instructor	has	the	solutions	to	the
homework	problems	in	an	instructor's	manual.	For	each	homework
problem,	we	show	a	rating	of	the	amount	of	effort	we	feel	it	will	require:
♦	Should	require	just	a	few	minutes.	Little	or	no	programming
required.
♦♦	Might	require	up	to	20	minutes.	Often	involves	writing	and	testing
some	code.	(Many	of	these	are	derived	from	problems	we	have	given
on	exams.)</p>
<p>♦♦♦	Requires	a	significant	effort,	perhaps	1−2	hours.	Generally
involves	writing	and	testing	a	significant	amount	of	code.
♦♦♦♦	A	lab	assignment,	requiring	up	to	10	hours	of	effort.
Each	code	example	in	the	text	was	formatted	directly,	without	any	manual
intervention,	from	a	C	program	compiled	with	
GCC
and	tested	on	a	Linux
system.	Of	course,	your	system	may	have	a	different	version	of	
GCC
,	or	a
different	compiler	altogether,	so	your	compiler	might	generate	different
machine	code;	but	the	overall	behavior	should	be	the	same.	All	of	the
source	code	is	available	from	the	CS:APP	Web	page	(&quot;CS:APP&quot;	being
our	shorthand	for	the	book's	title)	at	csapp.cs.cmu.edu.	In	the	text,	the
filenames	of	the	source	programs	are	documented	in	horizontal	bars	that
surround	the	formatted	code.	For	example,	the	program	in	
Figure	
1
can	be	found	in	the	file	
in	directory	
.	We	encourage
you	to	try	running	the	example	programs	on	your	system	as	you
encounter	them.
To	avoid	having	a	book	that	is	overwhelming,	both	in	bulk	and	in	content,
we	have	created	a	number	of	
Web	asides
containing	material	that
supplements	the	main	presentation	of	the	book.	These	asides	are
referenced	within	the	book	with	a	notation	of	the	form	
CHAP
:
TOP
,	where
CHAP
is	a	short	encoding	of	the	chapter	subject,	and	
TOP
is	a	short	code
for	the	topic	that	is	covered.	For	example,	Web	Aside	
DATA
:
BOOL
contains
supplementary	material	on	Boolean	algebra	for	the	presentation	on	data
representations	in	
Chapter	
2
,	while	Web	Aside	
ARCH
:
VLOG
contains
material	describing	processor	designs	using	the	Verilog	hardware
description	language,	supplementing	the	presentation	of	processor
design	in	
Chapter	
4
.	All	of	these	Web	asides	are	available	from	the
CS:APP	Web	page.</p>
<p>Book	Overview
The	CS:APP	book	consists	of	12	chapters	designed	to	capture	the	core
ideas	in	computer	systems.	Here	is	an	overview.
Chapter	
1
:	A	Tour	of	Computer	Systems.	
This	chapter
introduces	the	major	ideas	and	themes	in	computer	systems	by
tracing	the	life	cycle	of	a	simple	&quot;hello,	world&quot;	program.
Chapter	
2
:	Representing	and	Manipulating	Information.	
We
cover	computer	arithmetic,	emphasizing	the	properties	of	unsigned
and	two's-complement	number	representations	that	affect
programmers.	We	consider	how	numbers	are	represented	and
therefore	what	range	of	values	can	be	encoded	for	a	given	word	size.
We	consider	the	effect	of	casting	between	signed	and	unsigned
numbers.	We	cover	the	mathematical	properties	of	arithmetic
operations.	Novice	programmers	are	often	surprised	to	learn	that	the
(two's-complement)	sum	or	product	of	two	positive	numbers	can	be
negative.	On	the	other	hand,	two's-complement	arithmetic	satisfies
many	of	the	algebraic	properties	of	integer	arithmetic,	and	hence	a
compiler	can	safely	transform	multiplication	by	a	constant	into	a
sequence	of	shifts	and	adds.	We	use	the	bit-level	operations	of	C	to
demonstrate	the	principles	and	applications	of	Boolean	algebra.	We
cover	the	IEEE	floating-point	format	in	terms	of	how	it	represents
values	and	the	mathematical	properties	of	floating-point	operations.
Having	a	solid	understanding	of	computer	arithmetic	is	critical	to
writing	reliable	programs.	For	example,	programmers	and	compilers</p>
<p>cannot	replace	the	expression	
with	
,	due	to	the
possibility	of	overflow.	They	cannot	even	replace	it	with	the
expression	
,	due	to	the	asymmetric	range	of	negative	and
positive	numbers	in	the	two's-complement	representation.	Arithmetic
overflow	is	a	common	source	of	programming	errors	and	security
vulnerabilities,	yet	few	other	books	cover	the	properties	of	computer
arithmetic	from	a	programmer's	perspective.
Chapter	
3
:	Machine-Level	Representation	of	Programs.	
We
teach	you	how	to	read	the	x86-64	machine	code	generated	by	a	C
compiler.	We	cover	the	basic	instruction	patterns	generated	for
different	control	constructs,	such	as	conditionals,	loops,	and	switch
statements.	We	cover	the	implementation	of	procedures,	including
stack	allocation,	register	usage	conventions,	and	parameter	passing.
We	cover	the	way	different	data	structures	such	as	structures,	unions,
and	arrays	are	allocated	and	accessed.	We	cover	the	instructions	that
implement	both	integer	and	floating-point	arithmetic.	We	also	use	the
machine-level	view	of	programs	as	a	way	to	understand	common
code	security	vulnerabilities,	such	as	buffer	overflow,	and	steps	that
the	programmer,
Aside	
What	is	an	aside?
You	will	encounter	asides	of	this	form	throughout	the	text.
Asides	are	parenthetical	remarks	that	give	you	some	additional
insight	into	the	current	topic.	Asides	serve	a	number	of
purposes.	Some	are	little	history	lessons.	For	example,	where
did	C,	Linux,	and	the	Internet	come	from?	Other	asides	are
meant	to	clarify	ideas	that	students	often	find	confusing.	For
example,	what	is	the	difference	between	a	cache	line,	set,	and</p>
<p>block?	Other	asides	give	real-world	examples,	such	as	how	a
floating-point	error	crashed	a	French	rocket	or	the	geometric
and	operational	parameters	of	a	commercial	disk	drive.	Finally,
some	asides	are	just	fun	stuff.	For	example,	what	is	a
&quot;hoinky&quot;?
grammer,	the	compiler,	and	the	operating	system	can	take	to	reduce
these	threats.	Learning	the	concepts	in	this	chapter	helps	you
become	a	better	programmer,	because	you	will	understand	how
programs	are	represented	on	a	machine.	One	certain	benefit	is	that
you	will	develop	a	thorough	and	concrete	understanding	of	pointers.
Chapter	
4
:	Processor	Architecture.	
This	chapter	covers	basic
combinational	and	sequential	logic	elements,	and	then	shows	how
these	elements	can	be	combined	in	a	datapath	that	executes	a
simplified	subset	of	the	x86-64	instruction	set	called	&quot;Y86-64.&quot;	We
begin	with	the	design	of	a	single-cycle	datapath.	This	design	is
conceptually	very	simple,	but	it	would	not	be	very	fast.	We	then
introduce	
pipelining
,	where	the	different	steps	required	to	process	an
instruction	are	implemented	as	separate	stages.	At	any	given	time,
each	stage	can	work	on	a	different	instruction.	Our	five-stage
processor	pipeline	is	much	more	realistic.	The	control	logic	for	the
processor	designs	is	described	using	a	simple	hardware	description
language	called	HCL.	Hardware	designs	written	in	HCL	can	be
compiled	and	linked	into	simulators	provided	with	the	textbook,	and
they	can	be	used	to	generate	Verilog	descriptions	suitable	for
synthesis	into	working	hardware.
Chapter	
5
:	Optimizing	Program	Performance.	
This	chapter
introduces	a	number	of	techniques	for	improving	code	performance,
with	the	idea	being	that	programmers	learn	to	write	their	C	code	in</p>
<p>such	a	way	that	a	compiler	can	then	generate	efficient	machine	code.
We	start	with	transformations	that	reduce	the	work	to	be	done	by	a
program	and	hence	should	be	standard	practice	when	writing	any
program	for	any	machine.	We	then	progress	to	transformations	that
enhance	the	degree	of	instruction-level	parallelism	in	the	generated
machine	code,	thereby	improving	their	performance	on	modern
&quot;superscalar&quot;	processors.	To	motivate	these	transformations,	we
introduce	a	simple	operational	model	of	how	modern	out-of-order
processors	work,	and	show	how	to	measure	the	potential
performance	of	a	program	in	terms	of	the	critical	paths	through	a
graphical	representation	of	a	program.	You	will	be	surprised	how
much	you	can	speed	up	a	program	by	simple	transformations	of	the	C
code.
Chapter	
6
:	The	Memory	Hierarchy.	
The	memory	system	is	one	of
the	most	visible	parts	of	a	computer	system	to	application
programmers.	To	this	point,	you	have	relied	on	a	conceptual	model	of
the	memory	system	as	a	linear	array	with	uniform	access	times.	In
practice,	a	memory	system	is	a	hierarchy	of	storage	devices	with
different	capacities,	costs,	and	access	times.	We	cover	the	different
types	of	RAM	and	ROM	memories	and	the	geometry	and	organization
of	magnetic-disk	and	solid	state	drives.	We	describe	how	these
storage	devices	are	arranged	in	a	hierarchy.	We	show	how	this
hierarchy	is	made	possible	by	locality	of	reference.	We	make	these
ideas	concrete	by	introducing	a	unique	view	of	a	memory	system	as	a
&quot;memory	mountain&quot;	with	ridges	of	temporal	locality	and	slopes	of
spatial	locality.	Finally,	we	show	you	how	to	improve	the	performance
of	application	programs	by	improving	their	temporal	and	spatial
locality.
Chapter	
7
:	Linking.	
This	chapter	covers	both	static	and	dynamic</p>
<p>linking,	including	the	ideas	of	relocatable	and	executable	object	files,
symbol	resolution,	relocation,	static	libraries,	shared	object	libraries,
position-independent	code,	and	library	interpositioning.	Linking	is	not
covered	in	most	systems	texts,	but	we	cover	it	for	two	reasons.	First,
some	of	the	most	confusing	errors	that	programmers	can	encounter
are	related	to	glitches	during	linking,	especially	for	large	software
packages.	Second,	the	object	files	produced	by	linkers	are	tied	to
concepts	such	as	loading,	virtual	memory,	and	memory	mapping.
Chapter	
8
:	Exceptional	Control	Flow.	
In	this	part	of	the
presentation,	we	step	beyond	the	single-program	model	by
introducing	the	general	concept	of	exceptional	control	flow	(i.e.,
changes	in	control	flow	that	are	outside	the	normal	branches	and
procedure	calls).	We	cover	examples	of	exceptional	control	flow	that
exist	at	all	levels	of	the	system,	from	low-level	hardware	exceptions
and	interrupts,	to	context	switches	between	concurrent	processes,	to
abrupt	changes	in	control	flow	caused	by	the	receipt	of	Linux	signals,
to	the	nonlocal	jumps	in	C	that	break	the	stack	discipline.
This	is	the	part	of	the	book	where	we	introduce	the	fundamental	idea
of	a	
process
,	an	abstraction	of	an	executing	program.	You	will	learn
how	processes	work	and	how	they	can	be	created	and	manipulated
from	application	programs.	We	show	how	application	programmers
can	make	use	of	multiple	processes	via	Linux	system	calls.	When	you
finish	this	chapter,	you	will	be	able	to	write	a	simple	Linux	shell	with
job	control.	It	is	also	your	first	introduction	to	the	nondeterministic
behavior	that	arises	with	concurrent	program	execution.
Chapter	
9
:	Virtual	Memory.	
Our	presentation	of	the	virtual
memory	system	seeks	to	give	some	understanding	of	how	it	works
and	its	characteristics.	We	want	you	to	know	how	it	is	that	the</p>
<p>different	simultaneous	processes	can	each	use	an	identical	range	of
addresses,	sharing	some	pages	but	having	individual	copies	of
others.	We	also	cover	issues	involved	in	managing	and	manipulating
virtual	memory.	In	particular,	we	cover	the	operation	of	storage
allocators	such	as	the	standard-library	
and	
operations.
Covering	
this	material	serves	several	purposes.	It	reinforces	the
concept	that	the	virtual	memory	space	is	just	an	array	of	bytes	that
the	program	can	subdivide	into	different	storage	units.	It	helps	you
understand	the	effects	of	programs	containing	memory	referencing
errors	such	as	storage	leaks	and	invalid	pointer	references.	Finally,
many	application	programmers	write	their	own	storage	allocators
optimized	toward	the	needs	and	characteristics	of	the	application.
This	chapter,	more	than	any	other,	demonstrates	the	benefit	of
covering	both	the	hardware	and	the	software	aspects	of	computer
systems	in	a	unified	way.	Traditional	computer	architecture	and
operating	systems	texts	present	only	part	of	the	virtual	memory	story.
Chapter	
10
:	System-Level	I/O.	
We	cover	the	basic	concepts	of
Unix	I/O	such	as	files	and	descriptors.	We	describe	how	files	are
shared,	how	I/O	redirection	works,	and	how	to	access	file	metadata.
We	also	develop	a	robust	buffered	I/O	package	that	deals	correctly
with	a	curious	behavior	known	as	
short	counts
,	where	the	library
function	reads	only	part	of	the	input	data.	We	cover	the	C	standard
I/O	library	and	its	relationship	to	Linux	I/O,	focusing	on	limitations	of
standard	I/O	that	make	it	unsuitable	for	network	programming.	In
general,	the	topics	covered	in	this	chapter	are	building	blocks	for	the
next	two	chapters	on	network	and	concurrent	programming.
Chapter	
11
:	Network	Programming.	
Networks	are	interesting	I/O
devices	to	program,	tying	together	many	of	the	ideas	that	we	study</p>
<p>earlier	in	the	text,	such	as	processes,	signals,	byte	ordering,	memory
mapping,	and	dynamic	storage	allocation.	Network	programs	also
provide	a	compelling	context	for	concurrency,	which	is	the	topic	of	the
next	chapter.	This	chapter	is	a	thin	slice	through	network
programming	that	gets	you	to	the	point	where	you	can	write	a	simple
Web	server.	We	cover	the	client-server	model	that	underlies	all
network	applications.	We	present	a	programmer's	view	of	the	Internet
and	show	how	to	write	Internet	clients	and	servers	using	the	sockets
interface.	Finally,	we	introduce	HTTP	and	develop	a	simple	iterative
Web	server.
Chapter	
12
:	Concurrent	Programming.	
This	chapter	introduces
concurrent	programming	using	Internet	server	design	as	the	running
motivational	example.	We	compare	and	contrast	the	three	basic
mechanisms	for	writing	concurrent	programs—processes,	I/O
multiplexing,	and	threads—and	show	how	to	use	them	to	build
concurrent	Internet	servers.	We	cover	basic	principles	of
synchronization	using	
P
and	
V
semaphore	operations,	thread	safety
and	reentrancy,	race	conditions,	and	deadlocks.	Writing	concurrent
code	is	essential	for	most	server	applications.	We	also	describe	the
use	of	thread-level	programming	to	express	parallelism	in	an
application	program,	enabling	faster	execution	on	multi-core
processors.	Getting	all	of	the	cores	working	on	a	single	computational
problem	requires	a	careful	coordination	of	the	concurrent	threads,
both	for	correctness	and	to	achieve	high	performance.
New	to	This	Edition</p>
<p>The	first	edition	of	this	book	was	published	with	a	copyright	of	2003,	while
the	second	had	a	copyright	of	2011.	Considering	the	rapid	evolution	of
computer	technology,	the	book	content	has	held	up	surprisingly	well.	Intel
x86	machines	running	C	programs	under	Linux	(and	related	operating
systems)	has	proved	to	be	a	combination	that	continues	to	encompass
many	systems	today.	However,	changes	in	hardware	technology,
compilers,	program	library	interfaces,	and	the	experience	of	many
instructors	teaching	the	material	have	prompted	a	substantial	revision.
The	biggest	overall	change	from	the	second	edition	is	that	we	have
switched	our	presentation	from	one	based	on	a	mix	of	IA32	and	x86-64	to
one	based	exclusively	on	x86-64.	This	shift	in	focus	affected	the	contents
of	many	of	the	chapters.	Here	is	a	summary	of	the	significant	changes.
Chapter	
1
:	A	Tour	of	Computer	Systems	
We	have	moved	the
discussion	of	Amdahl's	Law	from	
Chapter	
5
into	this	chapter.
Chapter	
2
:	Representing	and	Manipulating	Information.	
A
consistent	bit	of	feedback	from	readers	and	reviewers	is	that	some	of
the	material	in	this	chapter	can	be	a	bit	overwhelming.	So	we	have
tried	to	make	the	material	more	accessible	by	clarifying	the	points	at
which	we	delve	into	a	more	mathematical	style	of	presentation.	This
enables	readers	to	first	skim	over	mathematical	details	to	get	a	high-
level	overview	and	then	return	for	a	more	thorough	reading.
Chapter	
3
:	Machine-Level	Representation	of	Programs.	
We
have	converted	from	the	earlier	presentation	based	on	a	mix	of	IA32
and	x86-64	to	one	based	entirely	on	x86-64.	We	have	also	updated
for	the	style	of	code	generated	by	more	recent	versions	of	
GCC
.	The
result	is	a	substantial	rewriting,	including	changing	the	order	in	which
some	of	the	concepts	are	presented.	We	also	have	included,	for	the</p>
<p>first	time,	a	presentation	of	the	machine-level	support	for	programs
operating	on	floating-point	data.	We	have	created	a	Web	aside
describing	IA32	machine	code	for	legacy	reasons.
Chapter	
4
:	Processor	Architecture.	
We	have	revised	the	earlier
processor	design,	based	on	a	32-bit	architecture,	to	one	that	supports
64-bit	words	and	operations.
Chapter	
5
:	Optimizing	Program	Performance.	
We	have	updated
the	material	to	reflect	the	performance	capabilities	of	recent
generations	of	x86-64	processors.	With	the	introduction	of	more
functional	units	and	more	sophisticated	control	logic,	the	model	of
program	performance	we	developed	based	on	a	data-flow
representation	of	programs	has	become	a	more	reliable	predictor	of
performance	than	it	was	before.
Chapter	
6
:	The	Memory	Hierarchy.	
We	have	updated	the	material
to	reflect	more	recent	technology.
Chapter	
7
:	Linking.	
We	have	rewritten	this	chapter	for	x86-64,
expanded	the	discussion	of	using	the	GOT	and	PLT	to	create
position-independent	code,	and	added	a	new	section	on	a	powerful
linking	technique	known	as	
library	interpositioning.
Chapter	
8
:	Exceptional	Control	Flow.	
We	have	added	a	more
rigorous	treatment	of	signal	handlers,	including	async-signal-safe
functions,	specific	guidelines	for	writing	signal	handlers,	and	using
sigsuspend	to	wait	for	handlers.
Chapter	
9
:	Virtual	Memory.	
This	chapter	has	changed	only
slightly.</p>
<p>Chapter	
10
:	System-Level	I/O.	
We	have	added	a	new	section	on
files	and	the	file	hierarchy,	but	otherwise,	this	chapter	has	changed
only	slightly.
Chapter	
11
:	Network	Programming.	
We	have	introduced
techniques	for	protocol-independent	and	thread-safe	network
programming	using	the	modern	getaddrinfo	and	getnameinfo
functions,	which	replace	the	obsolete	and	non-reentrant
gethostbyname	and	gethostbyaddr	functions.
Chapter	
12
:	Concurrent	Programming.	
We	have	increased	our
coverage	of	using	thread-level	parallelism	to	make	programs	run
faster	on	multi-core	machines.
In	addition,	we	have	added	and	revised	a	number	of	practice	and
homework	problems	throughout	the	text.
Origins	of	the	Book
This	book	stems	from	an	introductory	course	that	we	developed	at
Carnegie	Mellon	University	in	the	fall	of	1998,	called	15−213:	Introduction
to	Computer	Systems	(ICS)	[
14
].	The	ICS	course	has	been	taught	every
semester	since	then.	Over	400	students	take	the	course	each	semester.
The	students	range	from	sophomores	to	graduate	students	in	a	wide
variety	of	majors.	It	is	a	required	core	course	for	all	undergraduates	in	the
CS	and	ECE	departments	at	Carnegie	Mellon,	and	it	has	become	a
prerequisite	for	most	upper-level	systems	courses	in	CS	and	ECE.</p>
<p>The	idea	with	ICS	was	to	introduce	students	to	computers	in	a	different
way.	Few	of	our	students	would	have	the	opportunity	to	build	a	computer
system.	On	the	other	hand,	most	students,	including	all	computer
scientists	and	computer	engineers,	would	be	required	to	use	and
program	computers	on	a	daily	basis.	So	we	decided	to	teach	about
systems	from	the	point	of	view	of	the	programmer,	using	the	following
filter:	we	would	cover	a	topic	only	if	it	affected	the	performance,
correctness,	or	utility	of	user-level	C	programs.
For	example,	topics	such	as	hardware	adder	and	bus	designs	were	out.
Topics	such	as	machine	language	were	in;	but	instead	of	focusing	on
how	to	write	assembly	language	by	hand,	we	would	look	at	how	a	C
compiler	translates	C	constructs	into	machine	code,	including	pointers,
loops,	procedure	calls,	and	switch	statements.	Further,	we	would	take	a
broader	and	more	holistic	view	of	the	system	as	both	hardware	and
systems	software,	covering	such	topics	as	linking,	loading,	
processes,
signals,	performance	optimization,	virtual	memory,	I/O,	and	network	and
concurrent	programming.
This	approach	allowed	us	to	teach	the	ICS	course	in	a	way	that	is
practical,	concrete,	hands-on,	and	exciting	for	the	students.	The
response	from	our	students	and	faculty	colleagues	was	immediate	and
overwhelmingly	positive,	and	we	realized	that	others	outside	of	CMU
might	benefit	from	using	our	approach.	Hence	this	book,	which	we
developed	from	the	ICS	lecture	notes,	and	which	we	have	now	revised	to
reflect	changes	in	technology	and	in	how	computer	systems	are
implemented.
Via	the	multiple	editions	and	multiple	translations	of	this	book,	ICS	and
many	variants	have	become	part	of	the	computer	science	and	computer</p>
<p>engineering	curricula	at	hundreds	of	colleges	and	universities	worldwide.
For	Instructors:	Courses	Based	on
the	Book
Instructors	can	use	the	CS:APP	book	to	teach	a	number	of	different	types
of	systems	courses.	Five	categories	of	these	courses	are	illustrated	in
Figure	
2
.	The	particular	course	depends	on	curriculum	requirements,
personal	taste,	and	the	backgrounds	and	abilities	of	the	students.	From
left	to	right	in	the	figure,	the	courses	are	characterized	by	an	increasing
emphasis	on	the	programmer's	perspective	of	a	system.	Here	is	a	brief
description.
ORG.	
A	computer	organization	course	with	traditional	topics	covered
in	an	un-traditional	style.	Traditional	topics	such	as	logic	design,
processor	architecture,	assembly	language,	and	memory	systems	are
covered.	However,	there	is	more	emphasis	on	the	impact	for	the
programmer.	For	example,	data	representations	are	related	back	to
the	data	types	and	operations	of	C	programs,	and	the	presentation	on
assembly	code	is	based	on	machine	code	generated	by	a	C	compiler
rather	than	handwritten	assembly	code.
ORG+.	
The	ORG	course	with	additional	emphasis	on	the	impact	of
hardware	on	the	performance	of	application	programs.	Compared	to
ORG,	students	learn	more	about	code	optimization	and	about
improving	the	memory	performance	of	their	C	programs.</p>
<p>ICS.	
The	baseline	ICS	course,	designed	to	produce	enlightened
programmers	who	understand	the	impact	of	the	hardware,	operating
system,	and	compilation	system	on	the	performance	and	correctness
of	their	application	programs.	A	significant	difference	from	ORG+	is
that	low-level	processor	architecture	is	not	covered.	Instead,
programmers	work	with	a	higher-level	model	of	a	modern	out-of-order
processor.	The	ICS	course	fits	nicely	into	a	10-week	quarter,	and	can
also	be	stretched	to	a	15-week	semester	if	covered	at	a	more
leisurely	pace.
ICS+.	
The	baseline	ICS	course	with	additional	coverage	of	systems
programming	topics	such	as	system-level	I/O,	network	programming,
and	concurrent	programming.	This	is	the	semester-long	Carnegie
Mellon	course,	which	covers	every	chapter	in	CS:APP	except	low-
level	processor	architecture.
Course
Chapter
Topic
ORG
ORG+
ICS
ICS+
SP
1
Tour	of	systems
•
•
•
•
•
2
Data	representation
•
•
•
•
⊙
3
Machine	language
•
•
•
•
•
4
Processor	architecture
•
•
5
Code	optimization
•
•
•
6
Memory	hierarchy
⊙
•
•
•
⊙
7
Linking
⊙
⊙
•
(d)
(a)
(a)
(c)
(d)</p>
<p>8
Exceptional	control	flow
•
•
•
9
Virtual	memory
⊙
•
•
•
•
10
System-level	I/O
•
•
11
Network	programming
•
•
12
Concurrent	programming
•
•
Figure	
2	
Five	systems	courses	based	on	the	CS:APP	book.
ICS+	is	the	15−213	course	from	Carnegie	Mellon.	Notes:	The	
(c)
symbol	denotes	partial	coverage	of	a	chapter,	as	follows:	(a)
hardware	only;	(b)	no	dynamic	storage	allocation;	(c)	no	dynamic
linking;	(d)	no	floating	point.
SP.	
A	systems	programming	course.	This	course	is	similar	to	ICS+,
but	it	drops	floating	point	and	performance	optimization,	and	it	places
more	emphasis	on	systems	programming,	including	process	control,
dynamic	linking,	system-level	I/O,	network	programming,	and
concurrent	programming.	Instructors	might	want	to	supplement	from
other	sources	for	advanced	topics	such	as	daemons,	terminal	control,
and	Unix	IPC.
The	main	message	of	
Figure	
2
is	that	the	CS:APP	book	gives	a	lot	of
options	to	students	and	instructors.	If	you	want	your	students	to	be
exposed	to	lower-level	processor	architecture,	then	that	option	is
available	via	the	ORG	and	ORG+	courses.	On	the	other	hand,	if	you
want	to	switch	from	your	current	computer	organization	course	to	an	ICS
or	ICS+	course,	but	are	wary	of	making	such	a	drastic	change	all	at	once,
then	you	can	move	toward	ICS	incrementally.	You	can	start	with	ORG,
(b)</p>
<p>which	teaches	the	traditional	topics	in	a	nontraditional	way.	Once	you	are
comfortable	with	that	material,	then	you	can	move	to	ORG+,	and
eventually	to	ICS.	If	students	have	no	experience	in	C	(e.g.,	they	have
only	programmed	in	Java),	you	could	spend	several	weeks	on	C	and
then	cover	the	material	of	ORG	or	ICS.
Finally,	we	note	that	the	ORG+	and	SP	courses	would	make	a	nice	two-
term	sequence	(either	quarters	or	semesters).	Or	you	might	consider
offering	ICS+	as	one	term	of	ICS	and	one	term	of	SP.
For	Instructors:	Classroom-Tested
Laboratory	Exercises
The	ICS+	course	at	Carnegie	Mellon	receives	very	high	evaluations	from
students.	Median	scores	of	5.0/5.0	and	means	of	4.6/5.0	are	typical	for
the	student	course	evaluations.	Students	cite	the	fun,	exciting,	and
relevant	laboratory	exercises	as	the	primary	reason.	The	labs	are
available	from	the	CS:APP	Web	page.	Here	are	examples	of	the	labs	that
are	provided	with	the	book.
Data	Lab.	
This	lab	requires	students	to	implement	simple	logical	and
arithmetic	functions,	but	using	a	highly	restricted	subset	of	C.	For
example,	they	must	compute	the	absolute	value	of	a	number	using
only	bit-level	operations.	This	lab	helps	students	understand	the	bit-
level	representations	of	C	data	types	and	the	bit-level	behavior	of	the
operations	on	data.</p>
<p>Binary	Bomb	Lab.	
A	
binary	bomb
is	a	program	provided	to	students
as	an	object-code	file.	When	run,	it	prompts	the	user	to	type	in	six
different	strings.	If	any	of	these	are	incorrect,	the	bomb	&quot;explodes,&quot;
printing	an	error	message	and	logging	the	event	on	a	grading	server.
Students	must	&quot;defuse&quot;	their	own	unique	bombs	by	disassembling
and	reverse	engineering	the	programs	to	determine	what	the	six
strings	should	be.	The	lab	teaches	students	to	understand	assembly
language	and	also	forces	them	to	learn	how	to	use	a	debugger.
Buffer	Overflow	Lab.	
Students	are	required	to	modify	the	run-time
behavior	of	a	binary	executable	by	exploiting	a	buffer	overflow
vulnerability.	This	lab	teaches	the	students	about	the	stack	discipline
and	about	the	danger	of	writing	code	that	is	vulnerable	to	buffer
overflow	attacks.
Architecture	Lab.	
Several	of	the	homework	problems	of	
Chapter
4
can	be	combined	into	a	lab	assignment,	where	students	modify
the	HCL	description	of	a	processor	to	add	new	instructions,	change
the	branch	prediction	policy,	or	add	or	remove	bypassing	paths	and
register	ports.	The	resulting	processors	can	be	simulated	and	run
through	automated	tests	that	will	detect	most	of	the	possible	bugs.
This	lab	lets	students	experience	the	exciting	parts	of	processor
design	without	requiring	a	complete	background	in	logic	design	and
hardware	description	languages.
Performance	Lab.	
Students	must	optimize	the	performance	of	an
application	kernel	function	such	as	convolution	or	matrix	transposition.
This	lab	provides	a	very	clear	demonstration	of	the	properties	of
cache	memories	and	gives	students	experience	with	low-level
program	optimization.
Cache	Lab.	
In	this	alternative	to	the	performance	lab,	students	write	a</p>
<p>general-purpose	cache	simulator,	and	then	optimize	a	small	matrix
transpose	kernel	to	minimize	the	number	of	misses	on	a	simulated
cache.	We	use	the	Valgrind	tool	to	generate	real	address	traces	for
the	matrix	transpose	kernel.
Shell	Lab.	
Students	implement	their	own	Unix	shell	program	with	job
control,	including	the	Ctrl+C	and	Ctrl+Z	keystrokes	and	the	
and	
commands.	
This	is	the	student's	first	introduction	to
concurrency,	and	it	gives	them	a	clear	idea	of	Unix	process	control,
signals,	and	signal	handling.
Malloc	Lab.	
Students	implement	their	own	versions	of	
and	(optionally)	
This	lab	gives	students	a	clear
understanding	of	data	layout	and	organization,	and	requires	them	to
evaluate	different	trade-offs	between	space	and	time	efficiency.
Proxy	Lab.	
Students	implement	a	concurrent	Web	proxy	that	sits
between	their	browsers	and	the	rest	of	the	World	Wide	Web.	This	lab
exposes	the	students	to	such	topics	as	Web	clients	and	servers,	and
ties	together	many	of	the	concepts	from	the	course,	such	as	byte
ordering,	file	I/O,	process	control,	signals,	signal	handling,	memory
mapping,	sockets,	and	concurrency.	Students	like	being	able	to	see
their	programs	in	action	with	real	Web	browsers	and	Web	servers.
The	CS:APP	instructor's	manual	has	a	detailed	discussion	of	the	labs,	as
well	as	directions	for	downloading	the	support	software.
Acknowledgments	for	the	Third</p>
<p>Edition
It	is	a	pleasure	to	acknowledge	and	thank	those	who	have	helped	us
produce	this	third	edition	of	the	CS:APP	text.
We	would	like	to	thank	our	Carnegie	Mellon	colleagues	who	have	taught
the	ICS	course	over	the	years	and	who	have	provided	so	much	insightful
feedback	and	encouragement:	Guy	Blelloch,	Roger	Dannenberg,	David
Eckhardt,	Franz	Franchetti,	Greg	Ganger,	Seth	Goldstein,	Khaled	Harras,
Greg	Kesden,	Bruce	Maggs,	Todd	Mowry,	Andreas	Nowatzyk,	Frank
Pfenning,	Markus	Pueschel,	and	Anthony	Rowe.	David	Winters	was	very
helpful	in	installing	and	configuring	the	reference	Linux	box.
Jason	Fritts	(St.	Louis	University)	and	Cindy	Norris	(Appalachian	State)
provided	us	with	detailed	and	thoughtful	reviews	of	the	second	edition.
Yili	Gong	(Wuhan	University)	wrote	the	Chinese	translation,	maintained
the	errata	page	for	the	Chinese	version,	and	contributed	many	bug
reports.	Godmar	Back	(Virginia	Tech)	helped	us	improve	the	text
significantly	by	introducing	us	to	the	notions	of	async-signal	safety	and
protocol-independent	network	programming.
Many	thanks	to	our	eagle-eyed	readers	who	reported	bugs	in	the	second
edition:	Rami	Ammari,	Paul	Anagnostopoulos,	Lucas	Bärenfänger,
Godmar	Back,	Ji	Bin,	Sharbel	Bousemaan,	Richard	Callahan,	Seth
Chaiken,	Cheng	Chen,	Libo	Chen,	Tao	Du,	Pascal	Garcia,	Yili	Gong,
Ronald	Greenberg,	Dorukhan	Gülöz,	Dong	Han,	Dominik	Helm,	Ronald
Jones,	Mustafa	Kazdagli,	Gordon	Kindlmann,	Sankar	Krishnan,	Kanak
Kshetri,	Junlin	Lu,	Qiangqiang	Luo,	Sebastian	Luy,	Lei	Ma,	Ashwin</p>
<p>Nanjappa,	Gregoire	Paradis,	Jonas	Pfenninger,	Karl	Pichotta,	David
Ramsey,	Kaustabh	Roy,	David	Selvaraj,	Sankar	Shanmugam,	Dominique
Smulkowska,	Dag	Sørbø,	Michael	Spear,	Yu	Tanaka,	Steven
Tricanowicz,	Scott	Wright,	Waiki	Wright,	Han	Xu,	Zhengshan	Yan,	Firo
Yang,	Shuang	Yang,	John	Ye,	Taketo	Yoshida,	Yan	Zhu,	and	Michael
Zink.
Thanks	also	to	our	readers	who	have	contributed	to	the	labs,	including
God-mar	Back	(Virginia	Tech),	Taymon	Beal	(Worcester	Polytechnic
Institute),	Aran	Clauson	(Western	Washington	University),	Cary	Gray
(Wheaton	College),	Paul	Haiduk	(West	Texas	A&amp;M	University),	Len
Hamey	(Macquarie	University),	Eddie	Kohler	(Harvard),	Hugh	Lauer
(Worcester	Polytechnic	Institute),	Robert	Marmorstein	(Longwood
University),	and	James	Riely	(DePaul	University).
Once	again,	Paul	Anagnostopoulos	of	Windfall	Software	did	a	masterful
job	of	typesetting	the	book	and	leading	the	production	process.	Many
thanks	to	Paul	and	his	stellar	team:	Richard	Camp	(copyediting),	Jennifer
McClain	(proofreading),	Laurel	Muller	(art	production),	and	Ted	Laux
(indexing).	Paul	even	spotted	a	bug	in	our	description	of	the	origins	of	the
acronym	BSS	that	had	persisted	undetected	since	the	first	edition!
Finally,	we	would	like	to	thank	our	friends	at	Prentice	Hall.	Marcia	Horton
and	our	editor,	Matt	Goldstein,	have	been	unflagging	in	their	support	and
encouragement,	and	we	are	deeply	grateful	to	them.
Acknowledgments	from	the	Second</p>
<p>Edition
We	are	deeply	grateful	to	the	many	people	who	have	helped	us	produce
this	second	edition	of	the	CS:APP	text.
First	and	foremost,	we	would	like	to	recognize	our	colleagues	who	have
taught	the	ICS	course	at	Carnegie	Mellon	for	their	insightful	feedback	and
encouragement:	Guy	Blelloch,	Roger	Dannenberg,	David	Eckhardt,	Greg
Ganger,	Seth	Goldstein,	Greg	Kesden,	Bruce	Maggs,	Todd	Mowry,
Andreas	Nowatzyk,	Frank	Pfenning,	and	Markus	Pueschel.
Thanks	also	to	our	sharp-eyed	readers	who	contributed	reports	to	the
errata	page	for	the	first	edition:	Daniel	Amelang,	Rui	Baptista,	Quarup
Barreirinhas,	Michael	Bombyk,	Jörg	Brauer,	Jordan	Brough,	Yixin	Cao,
James	Caroll,	Rui	Carvalho,	Hyoung-Kee	Choi,	Al	Davis,	Grant	Davis,
Christian	Dufour,	Mao	Fan,	Tim	Freeman,	Inge	Frick,	Max	Gebhardt,	Jeff
Goldblat,	Thomas	Gross,	Anita	Gupta,	John	Hampton,	Hiep	Hong,	Greg
Israelsen,	Ronald	Jones,	Haudy	Kazemi,	Brian	Kell,	Constantine
Kousoulis,	Sacha	Krakowiak,	Arun	Krishnaswamy,	Martin	Kulas,	Michael
Li,	Zeyang	Li,	Ricky	Liu,	Mario	Lo	Conte,	Dirk	Maas,	Devon	Macey,	Carl
Marcinik,	Will	Marrero,	Simone	Martins,	Tao	Men,	Mark	Morrissey,
Venkata	Naidu,	Bhas	Nalabothula,	Thomas	Niemann,	Eric	Peskin,	David
Po,	Anne	Rogers,	John	Ross,	Michael	Scott,	Seiki,	Ray	Shih,	Darren
Shultz,	Erik	Silkensen,	Suryanto,	Emil	Tarazi,	Nawanan	Theera-
Ampornpunt,	Joe	Trdinich,	Michael	Trigoboff,	James	Troup,	Martin
Vopatek,	Alan	West,	Betsy	Wolff,	Tim	Wong,	James	Woodruff,	Scott
Wright,	Jackie	Xiao,	Guanpeng	Xu,	Qing	Xu,	Caren	Yang,	Yin
Yongsheng,	Wang	Yuanxuan,	Steven	Zhang,	and	Day	Zhong.	Special</p>
<p>thanks	to	Inge	Frick,	who	identified	a	subtle	deep	copy	bug	in	our	lock-
and-copy	example,	and	to	Ricky	Liu	for	his	amazing	proofreading	skills.
Our	Intel	Labs	colleagues	Andrew	Chien	and	Limor	Fix	were
exceptionally	supportive	throughout	the	writing	of	the	text.	Steve
Schlosser	graciously	provided	some	disk	drive	characterizations.	Casey
Helfrich	and	Michael	Ryan	installed	
and	maintained	our	new	Core	i7	box.
Michael	Kozuch,	Babu	Pillai,	and	Jason	Campbell	provided	valuable
insight	on	memory	system	performance,	multi-core	systems,	and	the
power	wall.	Phil	Gibbons	and	Shimin	Chen	shared	their	considerable
expertise	on	solid	state	disk	designs.
We	have	been	able	to	call	on	the	talents	of	many,	including	Wen-Mei
Hwu,	Markus	Pueschel,	and	Jiri	Simsa,	to	provide	both	detailed
comments	and	high-level	advice.	James	Hoe	helped	us	create	a	Verilog
version	of	the	Y86	processor	and	did	all	of	the	work	needed	to	synthesize
working	hardware.
Many	thanks	to	our	colleagues	who	provided	reviews	of	the	draft
manuscript:	James	Archibald	(Brigham	Young	University),	Richard
Carver	(George	Mason	University),	Mirela	Damian	(Villanova	University),
Peter	Dinda	(Northwestern	University),	John	Fiore	(Temple	University),
Jason	Fritts	(St.	Louis	University),	John	Greiner	(Rice	University),	Brian
Harvey	(University	of	California,	Berkeley),	Don	Heller	(Penn	State
University),	Wei	Chung	Hsu	(University	of	Minnesota),	Michelle	Hugue
(University	of	Maryland),	Jeremy	Johnson	(Drexel	University),	Geoff
Kuenning	(Harvey	Mudd	College),	Ricky	Liu,	Sam	Madden	(MIT),	Fred
Martin	(University	of	Massachusetts,	Lowell),	Abraham	Matta	(Boston
University),	Markus	Pueschel	(Carnegie	Mellon	University),	Norman</p>
<p>Ramsey	(Tufts	University),	Glenn	Reinmann	(UCLA),	Michela	Taufer
(University	of	Delaware),	and	Craig	Zilles	(UIUC).
Paul	Anagnostopoulos	of	Windfall	Software	did	an	outstanding	job	of
typesetting	the	book	and	leading	the	production	team.	Many	thanks	to
Paul	and	his	superb	team:	Rick	Camp	(copyeditor),	Joe	Snowden
(compositor),	MaryEllen	N.	Oliver	(proofreader),	Laurel	Muller	(artist),	and
Ted	Laux	(indexer).
Finally,	we	would	like	to	thank	our	friends	at	Prentice	Hall.	Marcia	Horton
has	always	been	there	for	us.	Our	editor,	Matt	Goldstein,	provided	stellar
leadership	from	beginning	to	end.	We	are	profoundly	grateful	for	their
help,	encouragement,	and	insights.
Acknowledgments	from	the	First
Edition
We	are	deeply	indebted	to	many	friends	and	colleagues	for	their
thoughtful	criticisms	and	encouragement.	A	special	thanks	to	our	15−213
students,	whose	infectious	energy	and	enthusiasm	spurred	us	on.	Nick
Carter	and	Vinny	Furia	generously	provided	their	malloc	package.
Guy	Blelloch,	Greg	Kesden,	Bruce	Maggs,	and	Todd	Mowry	taught	the
course	over	multiple	semesters,	gave	us	encouragement,	and	helped
improve	the	course	material.	Herb	Derby	provided	early	spiritual
guidance	and	encouragement.	Allan	Fisher,	Garth	Gibson,	Thomas
Gross,	Satya,	Peter	Steenkiste,	and	Hui	Zhang	encouraged	us	to</p>
<p>develop	the	course	from	the	start.	A	suggestion	from	Garth	early	on	got
the	whole	ball	rolling,	and	this	was	picked	up	and	refined	with	the	help	of
a	group	led	by	Allan	Fisher.	Mark	Stehlik	and	Peter	Lee	have	been	very
supportive	about	building	this	material	into	the	undergraduate	curriculum.
Greg	Kesden	provided	helpful	feedback	on	the	impact	of	ICS	on	the	OS
course.	Greg	Ganger	and	Jiri	Schindler	graciously	provided	some	disk
drive	characterizations	
and	answered	our	questions	on	modern	disks.
Tom	Stricker	showed	us	the	memory	mountain.	James	Hoe	provided
useful	ideas	and	feedback	on	how	to	present	processor	architecture.
A	special	group	of	students—Khalil	Amiri,	Angela	Demke	Brown,	Chris
Colohan,	Jason	Crawford,	Peter	Dinda,	Julio	Lopez,	Bruce	Lowekamp,
Jeff	Pierce,	Sanjay	Rao,	Balaji	Sarpeshkar,	Blake	Scholl,	Sanjit	Seshia,
Greg	Steffan,	Tiankai	Tu,	Kip	Walker,	and	Yinglian	Xie—were
instrumental	in	helping	us	develop	the	content	of	the	course.	In	particular,
Chris	Colohan	established	a	fun	(and	funny)	tone	that	persists	to	this	day,
and	invented	the	legendary	&quot;binary	bomb&quot;	that	has	proven	to	be	a	great
tool	for	teaching	machine	code	and	debugging	concepts.
Chris	Bauer,	Alan	Cox,	Peter	Dinda,	Sandhya	Dwarkadas,	John	Greiner,
Don	Heller,	Bruce	Jacob,	Barry	Johnson,	Bruce	Lowekamp,	Greg
Morrisett,	Brian	Noble,	Bobbie	Othmer,	Bill	Pugh,	Michael	Scott,	Mark
Smotherman,	Greg	Steffan,	and	Bob	Wier	took	time	that	they	did	not
have	to	read	and	advise	us	on	early	drafts	of	the	book.	A	very	special
thanks	to	Al	Davis	(University	of	Utah),	Peter	Dinda	(Northwestern
University),	John	Greiner	(Rice	University),	Wei	Hsu	(University	of
Minnesota),	Bruce	Lowekamp	(College	of	William	&amp;	Mary),	Bobbie
Othmer	(University	of	Minnesota),	Michael	Scott	(University	of
Rochester),	and	Bob	Wier	(Rocky	Mountain	College)	for	class	testing	the
beta	version.	A	special	thanks	to	their	students	as	well!</p>
<p>We	would	also	like	to	thank	our	colleagues	at	Prentice	Hall.	Marcia
Horton,	Eric	Frank,	and	Harold	Stone	have	been	unflagging	in	their
support	and	vision.	Harold	also	helped	us	present	an	accurate	historical
perspective	on	RISC	and	CISC	processor	architectures.	Jerry	Ralya
provided	sharp	insights	and	taught	us	a	lot	about	good	writing.
Finally,	we	would	like	to	acknowledge	the	great	technical	writers	Brian
Kernighan	and	the	late	W.	Richard	Stevens,	for	showing	us	that	technical
books	can	be	beautiful.
Thank	you	all.
Randy	Bryant
Dave	O'Hallaron
Pittsburgh,	Pennsylvania</p>
<p>About	the	Authors
Randal	E.	Bryant
received	his	bachelor's	degree	from	the	University	of
Michigan	in	1973	and	then	attended	graduate	school	at	the
Massachusetts	Institute	of	Technology,	receiving	his	PhD	degree	in
computer	science	in	1981.	He	spent	three	years	as	an	assistant
professor	at	the	California	Institute	of	Technology,	and	has	been	on	the
faculty	at	Carnegie	Mellon	since	1984.	For	five	of	those	years	he	served
as	head	of	the	Computer	Science	Department,	and	for	ten	of	them	he
served	as	Dean	of	the	School	of	Computer	Science.	He	is	currently	a
university	professor	of	computer	science.	He	also	holds	a	courtesy
appointment	with	the	Department	of	Electrical	and	Computer
Engineering.
Professor	Bryant	has	taught	courses	in	computer	systems	at	both	the
undergraduate	and	graduate	level	for	around	40	years.	Over	many	years
of	teaching	computer	architecture	courses,	he	began	shifting	the	focus
from	how	computers	are	designed	to	how	programmers	can	write	more
efficient	and	reliable	programs	if	they	understand	the	system	better.
Together	with	Professor	O'Hallaron,	he	developed	the	course	15−213,</p>
<p>Introduction	to	Computer	Systems,	at	Carnegie	Mellon	that	is	the	basis
for	this	book.	He	has	also	taught	courses	in	algorithms,	programming,
computer	networking,	distributed	systems,	and	VLSI	design.
Most	of	Professor	Bryant's	research	concerns	the	design	of	software
tools	to	help	software	and	hardware	designers	verify	the	correctness	of
their	systems.	These	include	several	types	of	simulators,	as	well	as
formal	verification	tools	that	prove	the	correctness	of	a	design	using
mathematical	methods.	He	has	published	over	150	technical	papers.	His
research	results	are	used	by	major	computer	manufacturers,	including
Intel,	IBM,	Fujitsu,	and	Microsoft.	He	has	won	several	major	awards	for
his	research.	These	include	two	inventor	recognition	awards	and	a
technical	achievement	award	from	the	Semiconductor	Research
Corporation,	the	Kanellakis	Theory	and	Practice	Award	from	the
Association	for	Computer	Machinery	(ACM),	and	the	W.	R.	G.	Baker
Award,	the	Emmanuel	Piore	Award,	the	Phil	Kaufman	Award,	and	the	A.
Richard	Newton	Award	from	the	Institute	of	Electrical	and	Electronics
Engineers	(IEEE).	He	is	a	fellow	of	both	the	ACM	and	the	IEEE	and	a
member	of	both	the	US	National	Academy	of	Engineering	and	the
American	Academy	of	Arts	and	Sciences.</p>
<p>David	R.	O'Hallaron
is	a	professor	of	computer	science	and	electrical
and	computer	engineering	at	Carnegie	Mellon	University.	He	received	his
PhD	from	the	University	of	Virginia.	He	served	as	the	director	of	Intel
Labs,	Pittsburgh,	from	2007	to	2010.
He	has	taught	computer	systems	courses	at	the	undergraduate	and
graduate	levels	for	20	years	on	such	topics	as	computer	architecture,
introductory	computer	systems,	parallel	processor	design,	and	Internet
services.	Together	with	Professor	Bryant,	he	developed	the	course	at
Carnegie	Mellon	that	led	to	this	book.	In	2004,	he	was	awarded	the
Herbert	Simon	Award	for	Teaching	Excellence	by	the	CMU	School	of
Computer	Science,	an	award	for	which	the	winner	is	chosen	based	on	a
poll	of	the	students.
Professor	O'Hallaron	works	in	the	area	of	computer	systems,	with
specific	interests	in	software	systems	for	scientific	computing,	data-
intensive	computing,	and	virtualization.	The	best-known	example	of	his
work	is	the	Quake	project,	an	endeavor	involving	a	group	of	computer
scientists,	civil	engineers,	and	seismologists	who	have	developed	the
ability	to	predict	the	motion	of	the	ground	during	strong	earthquakes.	In
2003,	Professor	O'Hallaron	and	the	other	members	of	the	Quake	team
won	the	Gordon	Bell	Prize,	the	top	international	prize	in	high-
performance	computing.	His	current	work	focuses	on	the	notion	of
autograding,	that	is,	programs	that	evaluate	the	quality	of	other
programs.</p>
<p>Chapter	
1	
A	Tour	of	Computer
Systems
1.1	
Information	Is	Bits	+	Context	
3
1.2	
Programs	Are	Translated	by	Other	Programs	into	Different
Forms	
4
1.3	
It	Pays	to	Understand	How	Compilation	Systems	Work	
6
1.4	
Processors	Read	and	Interpret	Instructions	Stored	in	Memory	
7
1.5	
Caches	Matter	
11
1.6	
Storage	Devices	Form	a	Hierarchy	
14
1.7	
The	Operating	System	Manages	the	Hardware	
14
1.8	
Systems	Communicate	with	Other	Systems	Using	Networks	
19
1.9	
Important	Themes	
22
1.10	
Summary</p>
<p>27
Bibliographic	Notes	
28
Solutions	to	Practice	Problems	
28</p>
<p>A	
computer	system
consists	of	hardware	and
systems	software	that	work	together	to	run
application	programs.	Specific	implementations	of
systems	change	over	time,	but	the	underlying
concepts	do	not.	All	computer	systems	have	similar
hardware	and	software	components	that	perform
similar	functions.	This	book	is	written	for
programmers	who	want	to	get	better	at	their	craft	by
understanding	how	these	components	work	and
how	they	affect	the	correctness	and	performance	of
their	programs.
You	are	poised	for	an	exciting	journey.	If	you
dedicate	yourself	to	learning	the	concepts	in	this
book,	then	you	will	be	on	your	way	to	be	coming	a
rare	&quot;power	programmer,&quot;	enlightened	by	an
understanding	of	the	underlying	computer	system
and	its	impact	on	your	application	programs.
You	are	going	to	learn	practical	skills	such	as	how	to
avoid	strange	numerical	errors	caused	by	the	way
that	computers	represent	numbers.	You	will	learn
how	to	optimize	your	C	code	by	using	clever	tricks
that	exploit	the	designs	of	modern	processors	and
memory	systems.	You	will	learn	how	the	compiler
implements	procedure	calls	and	how	to	use	this
knowledge	to	avoid	the	security	holes	from	buffer
overflow	vulnerabilities	that	plague	network	and
Internet	software.	You	will	learn	how	to	recognize</p>
<h2>and	avoid	the	nasty	errors	during	linking	that
confound	the	average	programmer.	You	will	learn
how	to	write	your	own	Unix	shell,	your	own	dynamic
storage	allocation	package,	and	even	your	own	Web
server.	You	will	learn	the	promises	and	pitfalls	of
concurrency,	a	topic	of	increasing	importance	as
multiple	processor	cores	are	integrated	onto	single
chips.
In	their	classic	text	on	the	C	programming	language
[
61
],	Kernighan	and	Ritchie	introduce	readers	to	C
using	the	
program	shown	in	
Figure	
1.1
.
Although	
is	a	very	simple	program,	every
major	part	of	the	system	must	work	in	concert	in
order	for	it	to	run	to	completion.	In	a	sense,	the	goal
of	this	book	is	to	help	you	understand	what	happens
and	why	when	you	run	
on	your	system.
We	begin	our	study	of	systems	by	tracing	the
lifetime	of	the	
program,	from	the	time	it	is
created	by	a	programmer,	until	it	runs	on	a	system,
prints	its	simple	message,	and	terminates.	As	we
follow	the	lifetime	of	the	program,	we	will	briefly
introduce	the	key	concepts,	terminology,	and
components	that	come	into	play.	Later	chapters	will
expand	on	these	ideas.</h2>
<p>code/intro/hello.c</p>
<hr />
<p>code/intro/hello.c
Figure	
1.1	
The	
program.
(
Source:
[
60
])</p>
<p>Figure	
1.2	
The	ASCII	text	representation	of</p>
<p>1.1	
Information	Is	Bits	+	Context
Our	
program	begins	life	as	a	
source	program
(or	
source	file
)	that
the	programmer	creates	with	an	editor	and	saves	in	a	text	file	called
The	source	program	is	a	sequence	of	bits,	each	with	a	value	of
0	or	1,	organized	in	8-bit	chunks	called	
bytes
.	Each	byte	represents	some
text	character	in	the	program.
Most	computer	systems	represent	text	characters	using	the	ASCII
standard	that	represents	each	character	with	a	unique	byte-size	integer
value.
For	example,	
Figure	
1.2
shows	the	ASCII	representation	of	the
program.</p>
<ol>
<li></li>
</ol>
<p>Other	encoding	methods	are	used	to	represent	text	in	non-English	languages.	See	the	aside	on
page	50	for	a	discussion	on	this.
The	
program	is	stored	in	a	file	as	a	sequence	of	bytes.	Each
byte	has	an	integer	value	that	corresponds	to	some	character.	For
example,	the	first	byte	has	the	integer	value	35,	which	corresponds	to	the
character	`
'.	The	second	byte	has	the	integer	value	105,	which
corresponds	to	the	character	
,	and	so	on.	Notice	that	each	text	line	is
terminated	by	the	invisible	
newline
character	
,	which	is	represented
by	the	integer	value	10.	Files	such	as	
that	consist	exclusively	of
ASCII	characters	are	known	as	
text	files
.	All	other	files	are	known	as
binary	files
.
1</p>
<p>The	representation	of	
illustrates	a	fundamental	idea:	All
information	in	a	system—including	disk	files,	programs	stored	in	memory,
user	data	stored	in	memory,	and	data	transferred	across	a	network—is
represented	as	a	bunch	of	bits.	The	only	thing	that	distinguishes	different
data	objects	is	the	context	in	which	we	view	them.	For	example,	in
different	contexts,	the	same	sequence	of	bytes	might	represent	an
integer,	floating-point	number,	character	string,	or	machine	instruction.
As	programmers,	we	need	to	understand	machine	representations	of
numbers	because	they	are	not	the	same	as	integers	and	real	numbers.
They	are	finite
Aside	
Origins	of	the	C	programming
language
C	was	developed	from	1969	to	1973	by	Dennis	Ritchie	of	Bell
Laboratories.	The	American	National	Standards	Institute	(ANSI)
ratified	the	ANSI	C	standard	in	1989,	and	this	standardization	later
became	the	responsibility	of	the	International	Standards
Organization	(ISO).	The	standards	define	the	C	language	and	a
set	of	library	functions	known	as	the	
C	standard	library
.	Kernighan
and	Ritchie	describe	ANSI	C	in	their	classic	book,	which	is	known
affectionately	as	&quot;K&amp;R&quot;	[
61
].	In	Ritchie's	words	[
92
],	C	is	&quot;quirky,
flawed,	and	an	enormous	success.&quot;	So	why	the	success?
C	was	closely	tied	with	the	Unix	operating	system.	
C	was
developed	from	the	beginning	as	the	system	programming
language	for	Unix.	Most	of	the	Unix	kernel	(the	core	part	of	the
operating	system),	and	all	of	its	supporting	tools	and	libraries,</p>
<p>were	written	in	C.	As	Unix	became	popular	in	universities	in
the	late	1970s	and	early	1980s,	many	people	were	exposed	to
C	and	found	that	they	liked	it.	Since	Unix	was	written	almost
entirely	in	C,	it	could	be	easily	ported	to	new	machines,	which
created	an	even	wider	audience	for	both	C	and	Unix.
C	is	a	small,	simple	language.	
The	design	was	controlled	by
a	single	person,	rather	than	a	committee,	and	the	result	was	a
clean,	consistent	design	with	little	baggage.	The	K&amp;R	book
describes	the	complete	language	and	standard	library,	with
numerous	examples	and	exercises,	in	only	261	pages.	The
simplicity	of	C	made	it	relatively	easy	to	learn	and	to	port	to
different	computers.
C	was	designed	for	a	practical	purpose.	
C	was	designed	to
implement	the	Unix	operating	system.	Later,	other	people
found	that	they	could	write	the	programs	they	wanted,	without
the	language	getting	in	the	way.
C	is	the	language	of	choice	for	system-level	programming,	and
there	is	a	huge	installed	base	of	application-level	programs	as
well.	However,	it	is	not	perfect	for	all	programmers	and	all
situations.	C	pointers	are	a	common	source	of	confusion	and
programming	errors.	C	also	lacks	explicit	support	for	useful
abstractions	such	as	classes,	objects,	and	exceptions.	Newer
languages	such	as	C++	and	Java	address	these	issues	for
application-level	programs.
approximations	that	can	behave	in	unexpected	ways.	This	fundamental
idea	is	explored	in	detail	in	
Chapter	
2
.</p>
<p>1.2	
Programs	Are	Translated	by
Other	Programs	into	Different	Forms
The	
program	begins	life	as	a	high-level	C	program	because	it	can
be	read	and	understood	by	human	beings	in	that	form.	However,	in	order
to	run	
on	the	system,	the	individual	C	statements	must	be
translated	by	other	programs	into	a	sequence	of	low-level	
machine-
language
instructions.	These	instructions	are	then	packaged	in	a	form
called	an	
executable	object	program
and	stored	as	a	binary	disk	file.
Object	programs	are	also	referred	to	as	
executable	object	files
.
On	a	Unix	system,	the	translation	from	source	file	to	object	file	is
performed	by	a	
compiler	driver:
Figure	
1.3	
The	compilation	system.</p>
<p>Here,	the	
GCC</p>
<p>compiler	driver	reads	the	source	file	
and	translates
it	into	an	executable	object	file	
.	The	translation	is	performed	in	the
sequence	of	four	phases	shown	in	
Figure	
1.3
.	The	programs	that
perform	the	four	phases	(
preprocessor
,	
compiler
,	
assembler
,	and	
linker
)
are	known	collectively	as	the	
compilation	system
.
Preprocessing	phase.	
The	preprocessor	(cpp)	modifies	the	original
C	program	according	to	directives	that	begin	with	the	`
'	character.
For	example,	the	
command	in	line	1	of	
tells	the	preprocessor	to	read	the	contents	of	the	system	header	file
and	insert	it	directly	into	the	program	text.	The	result	is
another	C	program,	typically	with	the	
suffix.
Compilation	phase.	
The	compiler	(
)	translates	the	text	file
into	the	text	file	
,	which	contains	an	
assembly-
language	program
.	This	program	includes	the	following	definition	of
:
Each	of	lines	2-7	in	this	definition	describes	one	low-level	machine-
language	instruction	in	a	textual	form.	Assembly	language	is	useful</p>
<p>because	it	provides	a	common	output	language	for	different	compilers
for	different	high-level	languages.	For	example,	C	compilers	and
Fortran	compilers	both	generate	output	files	in	the	same	assembly
language.
Assembly	phase.	
Next,	the	assembler	(
)	translates	
into
machine-language	instructions,	packages	them	in	a	form	known	as	a
relocatable	object	program
,	and	stores	the	result	in	the	object	file
This	file	is	a	binary	file	containing	17	bytes	to	encode	the
instructions	for	function	main.	If	we	were	to	view	
with	a	text
editor,	it	would	appear	to	be	gibberish.
Aside	
The	GNU	project
G
CC</p>
<p>is	one	of	many	useful	tools	developed	by	the	GNU	(short
for	GNU's	Not	Unix)	project.	The	GNU	project	is	a	tax-exempt
charity	started	by	Richard	Stallman	in	1984,	with	the	ambitious
goal	of	developing	a	complete	Unix-like	system	whose	source
code	is	unencumbered	by	restrictions	on	how	it	can	be
modified	or	distributed.	The	GNU	project	has	developed	an
environment	with	all	the	major	components	of	a	Unix	operating
system,	except	for	the	kernel,	which	was	developed	separately
by	the	Linux	project.	The	GNU	environment	includes	the	
EMACS
editor,	
GCC</p>
<p>compiler,	
GDB</p>
<p>debugger,	assembler,	linker,	utilities
for	manipulating	binaries,	and	other	components.	The	
GCC
compiler	has	grown	to	support	many	different	languages,	with
the	ability	to	generate	code	for	many	different	machines.
Supported	languages	include	C,	C++,	Fortran,	Java,	Pascal,
Objective-C,	and	Ada.
The	GNU	project	is	a	remarkable	achievement,	and	yet	it	is
often	overlooked.	The	modern	open-source	movement</p>
<p>(commonly	associated	with	Linux)	owes	its	intellectual	origins
to	the	GNU	project's	notion	of	
free	software
(&quot;free&quot;	as	in	&quot;free
speech,&quot;	not	&quot;free	beer&quot;).	Further,	Linux	owes	much	of	its
popularity	to	the	GNU	tools,	which	provide	the	environment	for
the	Linux	kernel.
Linking	phase.	
Notice	Notice	that	our	
program	calls	the	
function,	which	is	part	of	the	
standard	C	library
provided	by	every	C
compiler.	The	
function	resides	in	a	separate	precompiled
object	file	called	
,	which	must	somehow	be	merged	with	our
program.	The	linker	(
)	handles	this	merging.	The	result	is
the	
file,	which	is	an	executable	object	file	(or	simply	
executable
)
that	is	ready	to	be	loaded	into	memory	and	executed	by	the	system.</p>
<p>1.3	
It	Pays	to	Understand	How
Compilation	Systems	Work
For	simple	programs	such	as	
,	we	can	rely	on	the	compilation
system	to	produce	correct	and	efficient	machine	code.	However,	there
are	some	important	reasons	why	programmers	need	to	understand	how
compilation	systems	work:
Optimizing	program	performance.	
Modern	compilers	are
sophisticated	tools	that	usually	produce	good	code.	As	programmers,
we	do	not	need	to	know	the	inner	workings	of	the	compiler	in	order	to
write	efficient	code.	However,	in	order	to	make	good	coding	decisions
in	our	C	programs,	we	do	need	a	basic	understanding	of	machine-
level	code	and	how	the	compiler	translates	different	C	statements	into
machine	code.	For	example,	is	a	
statement	always	more
efficient	than	a	sequence	of	
statements?	How	much
overhead	is	incurred	by	a	function	call?	Is	a	
loop	more	efficient
than	a	
loop?	Are	pointer	references	more	efficient	than	array
indexes?	Why	does	our	loop	run	so	much	faster	if	we	sum	into	a	local
variable	instead	of	an	argument	that	is	passed	by	reference?	How	can
a	function	run	faster	when	we	simply	rearrange	the	parentheses	in	an
arithmetic	expression?
In	
Chapter	
3
,	we	introduce	x86-64,	the	machine	language	of	recent
generations	of	Linux,	Macintosh,	and	Windows	computers.	We
describe	how	compilers	translate	different	C	constructs	into	this</p>
<p>language.	In	
Chapter	
5
,	you	will	learn	how	to	tune	the	performance
of	your	C	programs	by	making	simple	transformations	to	the	C	code
that	help	the	compiler	do	its	job	better.	In	
Chapter	
6
,	you	will	learn
about	the	hierarchical	nature	of	the	memory	system,	how	C	compilers
store	data	arrays	in	memory,	and	how	your	C	programs	can	exploit
this	knowledge	to	run	more	efficiently.
Understanding	link-time	errors.	
In	our	experience,	some	of	the
most	perplexing	programming	errors	are	related	to	the	operation	of
the	linker,	especially	when	you	are	trying	to	build	large	software
systems.	For	example,	what	does	it	mean	when	the	linker	reports	that
it	cannot	resolve	a	reference?	What	is	the	difference	between	a	static
variable	and	a	global	variable?	What	happens	if	you	define	two	global
variables	in	different	C	files	with	the	same	name?	What	is	the
difference	between	a	static	library	and	a	dynamic	library?	Why	does	it
matter	what	order	we	list	libraries	on	the	command	line?	And	scariest
of	all,	why	do	some	linker-related	errors	not	appear	until	run	time?
You	will	learn	the	answers	to	these	kinds	of	questions	in	
Chapter	
7
.
Avoiding	security	holes.	
For	many	years,	
buffer	overflow
vulnerabilities
have	accounted	for	many	of	the	security	holes	in
network	and	Internet	servers.	These	vulnerabilities	exist	because	too
few	programmers	understand	the	need	to	carefully	restrict	the
quantity	and	forms	of	data	they	accept	from	untrusted	sources.	A	first
step	in	learning	secure	programming	is	to	understand	the
consequences	of	the	way	data	and	control	information	are	stored	on
the	program	stack.	We	cover	the	stack	discipline	and	buffer	overflow
vulnerabilities	in	
Chapter	
3
as	part	of	our	study	of	assembly
language.	We	will	also	learn	about	methods	that	can	be	used	by	the
programmer,	compiler,	and	operating	system	to	reduce	the	threat	of
attack.</p>
<p>1.4	
Processors	Read	and	Interpret
Instructions	Stored	in	Memory
At	this	point,	our	
source	program	has	been	translated	by	the
compilation	system	into	an	executable	object	file	called	
that	is
stored	on	disk.	To	run	the	executable	file	on	a	Unix	system,	we	type	its
name	to	an	application	program	known	as	a	
shell:
The	shell	is	a	command-line	interpreter	that	prints	a	prompt,	waits	for	you
to	type	a	command	line,	and	then	performs	the	command.	If	the	first	word
of	the	command	line	does	not	correspond	to	a	built-in	shell	command,
then	the	shell</p>
<p>Figure	
1.4	
Hardware	organization	of	a	typical	system.
CPU:	central	processing	unit,	ALU:	arithmetic/logic	unit,	PC:	program
counter,	USB:	Universal	Serial	Bus.
assumes	that	it	is	the	name	of	an	executable	file	that	it	should	load	and
run.	So	in	this	case,	the	shell	loads	and	runs	the	
program	and	then
waits	for	it	to	terminate.	The	
program	prints	its	message	to	the
screen	and	then	terminates.	The	shell	then	prints	a	prompt	and	waits	for
the	next	input	command	line.
1.4.1	
Hardware	Organization	of	a
System
To	understand	what	happens	to	our	
program	when	we	run	it,	we
need	to	understand	the	hardware	organization	of	a	typical	system,	which
is	shown	in	
Figure	
1.4
.	This	particular	picture	is	modeled	after	the</p>
<p>family	of	recent	Intel	systems,	but	all	systems	have	a	similar	look	and
feel.	Don't	worry	about	the	complexity	of	this	figure	just	now.	We	will	get
to	its	various	details	in	stages	throughout	the	course	of	the	book.
Buses
Running	throughout	the	system	is	a	collection	of	electrical	conduits	called
buses
that	carry	bytes	of	information	back	and	forth	between	the
components.	Buses	are	typically	designed	to	transfer	fixed-size	chunks	of
bytes	known	as	
words
.	The	number	of	bytes	in	a	word	(the	
word	size
)	is
a	fundamental	system	parameter	that	varies	across	systems.	Most
machines	today	have	word	sizes	of	either	4	bytes	(32	bits)	or	8	bytes	(64
bits).	In	this	book,	we	do	not	assume	any	fixed	definition	of	word	size.
Instead,	we	will	specify	what	we	mean	by	a	&quot;word&quot;	in	any	context	that
requires	this	to	be	defined.
I/O	Devices
Input/output	(I/O)	devices	are	the	system's	connection	to	the	external
world.	Our	example	system	has	four	I/O	devices:	a	keyboard	and	mouse
for	user	input,	a	display	for	user	output,	and	a	disk	drive	(or	simply	disk)
for	long-term	storage	of	data	and	programs.	Initially,	the	executable	
program	resides	on	the	disk.
Each	I/O	device	is	connected	to	the	I/O	bus	by	either	a	
controller
or	an
adapter
.	The	distinction	between	the	two	is	mainly	one	of	packaging.
Controllers	are	chip	sets	in	the	device	itself	or	on	the	system's	main
printed	circuit	board	(often	called	the	
motherboard
).	An	adapter	is	a	card
that	plugs	into	a	slot	on	the	motherboard.	Regardless,	the	purpose	of</p>
<p>each	is	to	transfer	information	back	and	forth	between	the	I/O	bus	and	an
I/O	device.
Chapter	
6
has	more	to	say	about	how	I/O	devices	such	as	disks	work.
In	
Chapter	
10
,	you	will	learn	how	to	use	the	Unix	I/O	interface	to
access	devices	from	your	application	programs.	We	focus	on	the
especially	interesting	class	of	devices	known	as	networks,	but	the
techniques	generalize	to	other	kinds	of	devices	as	well.
Main	Memory
The	
main	memory
is	a	temporary	storage	device	that	holds	both	a
program	and	the	data	it	manipulates	while	the	processor	is	executing	the
program.	Physically,	main	memory	consists	of	a	collection	of	
dynamic
random	access	memory
(DRAM)	chips.	Logically,	memory	is	organized	as
a	linear	array	of	bytes,	each	with	its	own	unique	address	(array	index)
starting	at	zero.	In	general,	each	of	the	machine	instructions	that
constitute	a	program	can	consist	of	a	variable	number	of	bytes.	The	sizes
of	data	items	that	correspond	to	C	program	variables	vary	according	to
type.	For	example,	on	an	x86-64	machine	running	Linux,	data	of	type
require	2	bytes,	types	
and	
4	bytes,	and	types	
and
8	bytes.
Chapter	
6
has	more	to	say	about	how	memory	technologies	such	as
DRAM	chips	work,	and	how	they	are	combined	to	form	main	memory.
Processor</p>
<p>The	
central	processing	unit
(CPU),	or	simply	
processor
,	is	the	engine	that
interprets	(or	
executes
)	instructions	stored	in	main	memory.	At	its	core	is
a	word-size	storage	device	(or	
register
)	called	the	
program	counter
(PC).
At	any	point	in	time,	the	PC	points	at	(contains	the	address	of)	some
machine-language	instruction	in	main	memory.
2.	
PC	is	also	a	commonly	used	acronym	for	&quot;personal	computer.&quot;	However,	the	distinction
between	the	two	should	be	clear	from	the	context.
From	the	time	that	power	is	applied	to	the	system	until	the	time	that	the
power	is	shut	off,	a	processor	repeatedly	executes	the	instruction	pointed
at	by	the	program	counter	and	updates	the	program	counter	to	point	to
the	next	instruction.	A	processor	
appears
to	operate	according	to	a	very
simple	instruction	execution	model,	defined	by	its	
instruction	set
architecture
.	In	this	model,	instructions	execute	
in	strict	sequence,	and
executing	a	single	instruction	involves	performing	a	series	of	steps.	The
processor	reads	the	instruction	from	memory	pointed	at	by	the	program
counter	(PC),	interprets	the	bits	in	the	instruction,	performs	some	simple
operation	dictated	by	the	instruction,	and	then	updates	the	PC	to	point	to
the	next	instruction,	which	may	or	may	not	be	contiguous	in	memory	to
the	instruction	that	was	just	executed.
There	are	only	a	few	of	these	simple	operations,	and	they	revolve	around
main	memory,	the	
register	file
,	and	the	
arithmetic/logic	unit
(ALU).	The
register	file	is	a	small	storage	device	that	consists	of	a	collection	of	word-
size	registers,	each	with	its	own	unique	name.	The	ALU	computes	new
data	and	address	values.	Here	are	some	examples	of	the	simple
operations	that	the	CPU	might	carry	out	at	the	request	of	an	instruction:
2</p>
<p>Load:	
Copy	a	byte	or	a	word	from	main	memory	into	a	register,
overwriting	the	previous	contents	of	the	register.
Store:	
Copy	a	byte	or	a	word	from	a	register	to	a	location	in	main
memory,	overwriting	the	previous	contents	of	that	location.
Operate:	
Copy	the	contents	of	two	registers	to	the	ALU,	perform	an
arithmetic	operation	on	the	two	words,	and	store	the	result	in	a
register,	overwriting	the	previous	contents	of	that	register.
Jump:	
Extract	a	word	from	the	instruction	itself	and	copy	that	word
into	the	program	counter	(PC),	overwriting	the	previous	value	of	the
PC.
We	say	that	a	processor	appears	to	be	a	simple	implementation	of	its
instruction	set	architecture,	but	in	fact	modern	processors	use	far	more
complex	mechanisms	to	speed	up	program	execution.	Thus,	we	can
distinguish	the	processor's	instruction	set	architecture,	describing	the
effect	of	each	machine-code	instruction,	from	its	
microarchitecture
,
describing	how	the	processor	is	actually	implemented.	When	we	study
machine	code	in	
Chapter	
3
,	we	will	consider	the	abstraction	provided
by	the	machine's	instruction	set	architecture.	
Chapter	
4
has	more	to
say	about	how	processors	are	actually	implemented.	
Chapter	
5
describes	a	model	of	how	modern	processors	work	that	enables
predicting	and	optimizing	the	performance	of	machine-language
programs.
1.4.2	
Running	the	
Program
Given	this	simple	view	of	a	system's	hardware	organization	and
operation,	we	can	begin	to	understand	what	happens	when	we	run	our</p>
<p>example	program.	We	must	omit	a	lot	of	details	here	that	will	be	filled	in
later,	but	for	now	we	will	be	content	with	the	big	picture.
Initially,	the	shell	program	is	executing	its	instructions,	waiting	for	us	to
type	a	command.	As	we	type	the	characters	
at	the	keyboard,	the
shell	program	reads	each	one	into	a	register	and	then	stores	it	in
memory,	as	shown	in	
Figure	
1.5
.
When	we	hit	the	enter	key	on	the	keyboard,	the	shell	knows	that	we	have
finished	typing	the	command.	The	shell	then	loads	the	executable	
file	by	executing	a	sequence	of	instructions	that	copies	the	code	and	data
in	the	
Figure	
1.5	
Reading	the	
command	from	the	keyboard.
object	file	from	disk	to	main	memory.	The	data	includes	the	string	of
characters	
that	will	eventually	be	printed	out.</p>
<p>Using	a	technique	known	as	
direct	memory	access
(DMA,	discussed	in
Chapter	
6
),	the	data	travel	directly	from	disk	to	main	memory,	without
passing	through	the	processor.	This	step	is	shown	in	
Figure	
1.6
.
Once	the	code	and	data	in	the	
object	file	are	loaded	into	memory,
the	processor	begins	executing	the	machine-language	instructions	in	the
program's	
routine.	These	instructions	copy	the	bytes	in	the
string	from	memory	to	the	register	file,	and	from	there	to
the	display	device,	where	they	are	displayed	on	the	screen.	This	step	is
shown	in	
Figure	
1.7
.</p>
<p>1.5	
Caches	Matter
An	important	lesson	from	this	simple	example	is	that	a	system	spends	a
lot	of	time	moving	information	from	one	place	to	another.	The	machine
instructions	in	the	
program	are	originally	stored	on	disk.	When	the
program	is	loaded,	they	are	copied	to	main	memory.	As	the	processor
runs	the	program,	instructions	are	copied	from	main	memory	into	the
processor.	Similarly,	the	data	string	
,	originally	on	disk,	is
copied	to	main	memory	and	then	copied	from	main	memory	to	the
display	device.	From	a	programmer's	perspective,	much	of	this	copying	is
overhead	that	slows	down	the	&quot;real	work&quot;	of	the	program.	Thus,	a	major
goal	for	system	designers	is	to	make	these	copy	operations	run	as	fast
as	possible.
Because	of	physical	laws,	larger	storage	devices	are	slower	than	smaller
storage	devices.	And	faster	devices	are	more	expensive	to	build	than
their	slower</p>
<p>Figure	
1.6	
Loading	the	executable	from	disk	into	main	memory.
Figure	
1.7	
Writing	the	output	string	from	memory	to	the	display.</p>
<p>Figure	
1.8	
Cache	memories.
counterparts.	For	example,	the	disk	drive	on	a	typical	system	might	be
1,000	times	larger	than	the	main	memory,	but	it	might	take	the	processor
10,000,000	times	longer	to	read	a	word	from	disk	than	from	memory.
Similarly,	a	typical	register	file	stores	only	a	few	hundred	bytes	of
information,	as	opposed	to	billions	of	bytes	in	the	main	memory.
However,	the	processor	can	read	data	from	the	register	file	almost	100
times	faster	than	from	memory.	Even	more	troublesome,	as
semiconductor	technology	progresses	over	the	years,	this	
processor-
memory	gap
continues	to	increase.	It	is	easier	and	cheaper	to	make
processors	run	faster	than	it	is	to	make	main	memory	run	faster.
To	deal	with	the	processor-memory	gap,	system	designers	include
smaller,	faster	storage	devices	called	
cache	memories
(or	simply	caches)
that	serve	as	temporary	staging	areas	for	information	that	the	processor
is	likely	to	need	in	the	near	future.	
Figure	
1.8
shows	the	cache
memories	in	a	typical	system.	An	
L1	cache
on	the	processor	chip	holds
tens	of	thousands	of	bytes	and	can	be	accessed	nearly	as	fast	as	the
register	file.	A	larger	
L2	cache
with	hundreds	of	thousands	to	millions	of
bytes	is	connected	to	the	processor	by	a	special	bus.	It	might	take	5
times	longer	for	the	processor	to	access	the	L2	cache	than	the	L1	cache,</p>
<p>but	this	is	still	5	to	10	times	faster	than	accessing	the	main	memory.	The
L1	and	L2	caches	are	implemented	with	a	hardware	technology	known
as	
static	random	access	memory
(SRAM).	Newer	and	more	powerful
systems	even	have	three	levels	of	cache:	L1,	L2,	and	L3.	The	idea
behind	caching	is	that	a	system	can	get	the	effect	of	both	a	very	large
memory	and	a	very	fast	one	by	exploiting	
locality
,	the	tendency	for
programs	to	access	data	and	code	in	localized	regions.	By	setting	up
caches	to	hold	data	that	are	likely	to	be	accessed	often,	we	can	perform
most	memory	operations	using	the	fast	caches.
One	of	the	most	important	lessons	in	this	book	is	that	application
programmers	who	are	aware	of	cache	memories	can	exploit	them	to
improve	the	performance	of	their	programs	by	an	order	of	magnitude.
You	will	learn	more	about	these	important	devices	and	how	to	exploit
them	in	
Chapter	
6
.
Figure	
1.9	
An	example	of	a	memory	hierarchy.</p>
<p>1.6	
Storage	Devices	Form	a
Hierarchy
This	notion	of	inserting	a	smaller,	faster	storage	device	(e.g.,	cache
memory)	between	the	processor	and	a	larger,	slower	device	(e.g.,	main
memory)	turns	out	to	be	a	general	idea.	In	fact,	the	storage	devices	in
every	computer	system	are	organized	as	a	
memory	hierarchy
similar	to
Figure	
1.9
.	As	we	move	from	the	top	of	the	hierarchy	to	the	bottom,
the	devices	become	slower,	larger,	and	less	costly	per	byte.	The	register
file	occupies	the	top	level	in	the	hierarchy,	which	is	known	as	level	0	or
L0.	We	show	three	levels	of	caching	L1	to	L3,	occupying	memory
hierarchy	levels	1	to	3.	Main	memory	occupies	level	4,	and	so	on.
The	main	idea	of	a	memory	hierarchy	is	that	storage	at	one	level	serves
as	a	cache	for	storage	at	the	next	lower	level.	Thus,	the	register	file	is	a
cache	for	the	L1	cache.	Caches	L1	and	L2	are	caches	for	L2	and	L3,
respectively.	The	L3	cache	is	a	cache	for	the	main	memory,	which	is	a
cache	for	the	disk.	On	some	networked	systems	with	distributed	file
systems,	the	local	disk	serves	as	a	cache	for	data	stored	on	the	disks	of
other	systems.
Just	as	programmers	can	exploit	knowledge	of	the	different	caches	to
improve	performance,	programmers	can	exploit	their	understanding	of
the	entire	memory	hierarchy.	
Chapter	
6
will	have	much	more	to	say
about	this.</p>
<p>1.7	
The	Operating	System	Manages
the	Hardware
Back	to	our	
example.	When	the	shell	loaded	and	ran	the	
program,	and	when	the	
program	printed	its	message,	neither
program	accessed	the
Figure	
1.10	
Layered	view	of	a	computer	system.
Figure	
1.11	
Abstractions	provided	by	an	operating	system.
keyboard,	display,	disk,	or	main	memory	directly.	Rather,	they	relied	on
the	services	provided	by	the	
operating	system
.	We	can	think	of	the
operating	system	as	a	layer	of	software	interposed	between	the
application	program	and	the	hardware,	as	shown	in	
Figure	
1.10
.	All
attempts	by	an	application	program	to	manipulate	the	hardware	must	go
through	the	operating	system.</p>
<p>The	operating	system	has	two	primary	purposes:	(1)	to	protect	the
hardware	from	misuse	by	runaway	applications	and	(2)	to	provide
applications	with	simple	and	uniform	mechanisms	for	manipulating
complicated	and	often	wildly	different	low-level	hardware	devices.	The
operating	system	achieves	both	goals	via	the	fundamental	abstractions
shown	in	
Figure	
1.11
:	
processes
,	
virtual	memory
,	and	
files
.	As	this
figure	suggests,	files	are	abstractions	for	I/O	devices,	virtual	memory	is
an	abstraction	for	both	the	main	memory	and	disk	I/O	devices,	and
processes	are	abstractions	for	the	processor,	main	memory,	and	I/O
devices.	We	will	discuss	each	in	turn.
1.7.1	
Processes
When	a	program	such	as	
runs	on	a	modern	system,	the	operating
system	provides	the	illusion	that	the	program	is	the	only	one	running	on
the	system.	The	program	appears	to	have	exclusive	use	of	both	the
processor,	main	memory,	and	I/O	devices.	The	processor	appears	to
execute	the	instructions	in	the	program,	one	after	the	other,	without
interruption.	And	the	code	and	data	of	the	program	appear	to	be	the	only
objects	in	the	system's	memory.	These	illusions	are	provided	by	the
notion	of	a	process,	one	of	the	most	important	and	successful	ideas	in
computer	science.
A	
process
is	the	operating	system's	abstraction	for	a	running	program.
Multiple	processes	can	run	concurrently	on	the	same	system,	and	each
process	appears	to	have	exclusive	use	of	the	hardware.	By	
concurrently
,
we	mean	that	the	instructions	of	one	process	are	interleaved	with	the</p>
<p>instructions	of	another	process.	In	most	systems,	there	are	more
processes	to	run	than	there	are	CPUs	to	run	them.
Aside	
Unix,	Posix,	and	the	Standard
Unix	Specification
The	1960s	was	an	era	of	huge,	complex	operating	systems,	such
as	IBM's	OS/360	and	Honeywell's	Multics	systems.	While	OS/360
was	one	of	the	most	successful	software	projects	in	history,
Multics	dragged	on	for	years	and	never	achieved	wide-scale	use.
Bell	Laboratories	was	an	original	partner	in	the	Multics	project	but
dropped	out	in	1969	because	of	concern	over	the	complexity	of
the	project	and	the	lack	of	progress.	In	reaction	to	their	unpleasant
Multics	experience,	a	group	of	Bell	Labs	researchers—Ken
Thompson,	Dennis	Ritchie,	Doug	McIlroy,	and	Joe	Ossanna—
began	work	in	1969	on	a	simpler	operating	system	for	a	Digital
Equipment	Corporation	PDP-7	computer,	written	entirely	in
machine	language.	Many	of	the	ideas	in	the	new	system,	such	as
the	hierarchical	file	system	and	the	notion	of	a	shell	as	a	user-
level	process,	were	borrowed	from	Multics	but	implemented	in	a
smaller,	simpler	package.	In	1970,	Brian	Kernighan	dubbed	the
new	system	&quot;Unix&quot;	as	a	pun	on	the	complexity	of	&quot;Multics.&quot;	The
kernel	was	rewritten	in	C	in	1973,	and	Unix	was	announced	to	the
outside	world	in	1974	[
93
].
Because	Bell	Labs	made	the	source	code	available	to	schools
with	generous	terms,	Unix	developed	a	large	following	at
universities.	The	most	influential	work	was	done	at	the	University
of	California	at	Berkeley	in	the	late	1970s	and	early	1980s,	with</p>
<p>Berkeley	researchers	adding	virtual	memory	and	the	Internet
protocols	in	a	series	of	releases	called	Unix	4.xBSD	(Berkeley
Software	Distributimn).	Concurrently,	Bell	Labs	was	releasing	their
own	versions,	which	became	known	as	System	V	Unix.	Versions
from	other	vendors,	such	as	the	Sun	Microsystems	Solaris
system,	were	derived	from	these	original	BSD	and	System	V
versions.
Trouble	arose	in	the	mid	1980s	as	Unix	vendors	tried	to
differentiate	themselves	by	adding	new	and	often	incompatible
features.	To	combat	this	trend,	IEEE	(Institute	for	Electrical	and
Electronics	Engineers)	sponsored	an	effort	to	standardize	Unix,
later	dubbed	&quot;Posix&quot;	by	Richard	Stallman.	The	result	was	a	family
of	standards,	known	as	the	Posix	standards,	that	cover	such
issues	as	the	C	language	interface	for	Unix	system	calls,	shell
programs	and	utilities,	threads,	and	network	programming.	More
recently,	a	separate	standardization	effort,	known	as	the
&quot;Standard	Unix	Specification,&quot;	has	joined	forces	with	Posix	to
create	a	single,	unified	standard	for	Unix	systems.	As	a	result	of
these	standardization	efforts,	the	differences	between	Unix
versions	have	largely	disappeared.
Traditional	systems	could	only	execute	one	program	at	a	time,	while
newer	
multi-core
processors	can	execute	several	programs
simultaneously.	In	either	case,	a	single	CPU	can	appear	to	execute
multiple	processes	concurrently	by	having	the	processor	switch	among
them.	The	operating	system	performs	this	interleaving	with	a	mechanism
known	as	
context	switching
.	To	simplify	the	rest	of	this	discussion,	we
consider	only	a	
uniprocessor	system
containing	a	single	CPU.	We	will
return	to	the	discussion	of	
multiprocessor
systems	in	
Section	
1.9.2
.</p>
<p>The	operating	system	keeps	track	of	all	the	state	information	that	the
process	needs	in	order	to	run.	This	state,	which	is	known	as	the	
context
,
includes	information	such	as	the	current	values	of	the	PC,	the	register
file,	and	the	contents	of	main	memory.	At	any	point	in	time,	a
uniprocessor	system	can	only	execute	the	code	for	a	single	process.
When	the	operating	system	decides	to	transfer	control	from	the	current
process	to	some	new	process,	it	performs	a	
context	switch
by	saving	the
context	of	the	current	process,	restoring	the	context	of	the	new	process,
and
Figure	
1.12	
Process	context	switching.
then	passing	control	to	the	new	process.	The	new	process	picks	up
exactly	where	it	left	off.	
Figure	
1.12
shows	the	basic	idea	for	our
example	
scenario.
There	are	two	concurrent	processes	in	our	example	scenario:	the	shell
process	and	the	
process.	Initially,	the	shell	process	is	running
alone,	waiting	for	input	on	the	command	line.	When	we	ask	it	to	run	the
program,	the	shell	carries	out	our	request	by	invoking	a	special
function	known	as	a	
system	call
that	passes	control	to	the	operating
system.	The	operating	system	saves	the	shell's	context,	creates	a	new
process	and	its	context,	and	then	passes	control	to	the	new	</p>
<p>process.	After	
terminates,	the	operating	system	restores	the
context	of	the	shell	process	and	passes	control	back	to	it,	where	it	waits
for	the	next	command-line	input.
As	
Figure	
1.12
indicates,	the	transition	from	one	process	to	another	is
managed	by	the	operating	system	
kernel
.	The	kernel	is	the	portion	of	the
operating	system	code	that	is	always	resident	in	memory.	When	an
application	program	requires	some	action	by	the	operating	system,	such
as	to	read	or	write	a	file,	it	executes	a	special	
system	call
instruction,
transferring	control	to	the	kernel.	The	kernel	then	performs	the	requested
operation	and	returns	back	to	the	application	program.	Note	that	the
kernel	is	not	a	separate	process.	Instead,	it	is	a	collection	of	code	and
data	structures	that	the	system	uses	to	manage	all	the	processes.
Implementing	the	process	abstraction	requires	close	cooperation
between	both	the	low-level	hardware	and	the	operating	system	software.
We	will	explore	how	this	works,	and	how	applications	can	create	and
control	their	own	processes,	in	
Chapter	
8
.
1.7.2	
Threads
Although	we	normally	think	of	a	process	as	having	a	single	control	flow,
in	modern	systems	a	process	can	actually	consist	of	multiple	execution
units,	called	
threads
,	each	running	in	the	context	of	the	process	and
sharing	the	same	code	and	global	data.	Threads	are	an	increasingly
important	programming	model	because	of	the	requirement	for
concurrency	in	network	servers,	because	it	is	easier	to	share	data
between	multiple	threads	than	between	multiple	processes,	and	because</p>
<p>threads	are	typically	more	efficient	than	processes.	Multi-threading	is	also
one	way	to	make	programs	run	faster	when	multiple	processors	are
available,	as	we	will	discuss	in
Figure	
1.13	
Process	virtual	address	space.
(The	regions	are	not	drawn	to	scale.)
Section	
1.9.2
.	You	will	learn	the	basic	concepts	of	concurrency,
including	how	to	write	threaded	programs,	in	
Chapter	
12
.
1.7.3	
Virtual	Memory
Virtual	memory
is	an	abstraction	that	provides	each	process	with	the
illusion	that	it	has	exclusive	use	of	the	main	memory.	Each	process	has
the	same	uniform	view	of	memory,	which	is	known	as	its	
virtual	address
space
.	The	virtual	address	space	for	Linux	processes	is	shown	in	
Figure</p>
<p>1.13
.	(Other	Unix	systems	use	a	similar	layout.)	In	Linux,	the	topmost
region	of	the	address	space	is	reserved	for	code	and	data	in	the
operating	system	that	is	common	to	all	processes.	The	lower	region	of
the	address	space	holds	the	code	and	data	defined	by	the	user's
process.	Note	that	addresses	in	the	figure	increase	from	the	bottom	to
the	top.
The	virtual	address	space	seen	by	each	process	consists	of	a	number	of
well-defined	areas,	each	with	a	specific	purpose.	You	will	learn	more
about	these	areas	later	in	the	book,	but	it	will	be	helpful	to	look	briefly	at
each,	starting	with	the	lowest	addresses	and	working	our	way	up:
Program	code	and	data.	
Code	begins	at	the	same	fixed	address	for
all	processes,	followed	by	data	locations	that	correspond	to	global	C
variables.	The	code	and	data	areas	are	initialized	directly	from	the
contents	of	an	executable	object	file—in	our	case,	the	
executable.	You	will	learn	more	about	this	part	of	the	address	space
when	we	study	linking	and	loading	in	
Chapter	
7
.
Heap.	
The	code	and	data	areas	are	followed	immediately	by	the	run-
time	
heap
.	Unlike	the	code	and	data	areas,	which	are	fixed	in	size
once	the	process	begins	
running,	the	heap	expands	and	contracts
dynamically	at	run	time	as	a	result	of	calls	to	C	standard	library
routines	such	as	
and	
.	We	will	study	heaps	in	detail	when
we	learn	about	managing	virtual	memory	in	
Chapter	
9
.
Shared	libraries.	
Near	the	middle	of	the	address	space	is	an	area
that	holds	the	code	and	data	for	
shared	libraries
such	as	the	C
standard	library	and	the	math	library.	The	notion	of	a	shared	library	is
a	powerful	but	somewhat	difficult	concept.	You	will	learn	how	they
work	when	we	study	dynamic	linking	in	
Chapter	
7
.</p>
<p>Stack.	
At	the	top	of	the	user's	virtual	address	space	is	the	
user	stack
that	the	compiler	uses	to	implement	function	calls.	Like	the	heap,	the
user	stack	expands	and	contracts	dynamically	during	the	execution	of
the	program.	In	particular,	each	time	we	call	a	function,	the	stack
grows.	Each	time	we	return	from	a	function,	it	contracts.	You	will	learn
how	the	compiler	uses	the	stack	in	
Chapter	
3
.
Kernel	virtual	memory.	
The	top	region	of	the	address	space	is
reserved	for	the	kernel.	Application	programs	are	not	allowed	to	read
or	write	the	contents	of	this	area	or	to	directly	call	functions	defined	in
the	kernel	code.	Instead,	they	must	invoke	the	kernel	to	perform	these
operations.
For	virtual	memory	to	work,	a	sophisticated	interaction	is	required
between	the	hardware	and	the	operating	system	software,	including	a
hardware	translation	of	every	address	generated	by	the	processor.	The
basic	idea	is	to	store	the	contents	of	a	process's	virtual	memory	on	disk
and	then	use	the	main	memory	as	a	cache	for	the	disk.	
Chapter	
9
explains	how	this	works	and	why	it	is	so	important	to	the	operation	of
modern	systems.
1.7.4	
Files
A	
file
is	a	sequence	of	bytes,	nothing	more	and	nothing	less.	Every	I/O
device,	including	disks,	keyboards,	displays,	and	even	networks,	is
modeled	as	a	file.	All	input	and	output	in	the	system	is	performed	by
reading	and	writing	files,	using	a	small	set	of	system	calls	known	as	
Unix
I/O
.</p>
<p>This	simple	and	elegant	notion	of	a	file	is	nonetheless	very	powerful
because	it	provides	applications	with	a	uniform	view	of	all	the	varied	I/O
devices	that	might	be	contained	in	the	system.	For	example,	application
programmers	who	manipulate	the	contents	of	a	disk	file	are	blissfully
unaware	of	the	specific	disk	technology.	Further,	the	same	program	will
run	on	different	systems	that	use	different	disk	technologies.	You	will
learn	about	Unix	I/O	in	
Chapter	
10
.</p>
<p>1.8	
Systems	Communicate	with
Other	Systems	Using	Networks
Up	to	this	point	in	our	tour	of	systems,	we	have	treated	a	system	as	an
isolated	collection	of	hardware	and	software.	In	practice,	modern	systems
are	often	linked	to	other	systems	by	networks.	From	the	point	of	view	of
an	individual	system,	the
Aside	
The	Linux	project
In	August	1991,	a	Finnish	graduate	student	named	Linus	Torvalds
modestly	announced	a	new	Unix-like	operating	system	kernel:</p>
<p>As	Torvalds	indicates,	his	starting	point	for	creating	Linux	was
Minix,	an	operating	system	developed	by	Andrew	S.	Tanenbaum
for	educational	purposes	[
113
].
The	rest,	as	they	say,	is	history.	Linux	has	evolved	into	a	technical
and	cultural	phenomenon.	By	combining	forces	with	the	GNU
project,	the	Linux	project	has	developed	a	complete,	Posix-
compliant	version	of	the	Unix	operating	system,	including	the
kernel	and	all	of	the	supporting	infrastructure.	Linux	is	available	on
a	wide	array	of	computers,	from	handheld	devices	to	mainframe
computers.	A	group	at	IBM	has	even	ported	Linux	to	a	wristwatch!
network	can	be	viewed	as	just	another	I/O	device,	as	shown	in	
Figure
1.14
.	When	the	system	copies	a	sequence	of	bytes	from	main	memory
to	the	network	adapter,	the	data	flow	across	the	network	to	another
machine,	instead	of,	say,	to	a	local	disk	drive.	Similarly,	the	system	can
read	data	sent	from	other	machines	and	copy	these	data	to	its	main
memory.
With	the	advent	of	global	networks	such	as	the	Internet,	copying
information	from	one	machine	to	another	has	become	one	of	the	most</p>
<p>important	uses	of	computer	systems.	For	example,	applications	such	as
email,	instant	messaging,	the	World	Wide	Web,	FTP,	and	telnet	are	all
based	on	the	ability	to	copy	information	over	a	network.
Figure	
1.14	
A	network	is	another	I/O	device.
Figure	
1.15	
Using	telnet	to	run	
remotely	over	a	network.
Returning	to	our	
example,	we	could	use	the	familiar	telnet
application	to	run	
on	a	remote	machine.	Suppose	we	use	a	telnet
client
running	on	our	local	machine	to	connect	to	a	telnet	
server
on	a
remote	machine.	After	we	log	in	to	the	remote	machine	and	run	a	shell,
the	remote	shell	is	waiting	to	receive	an	input	command.	From	this	point,</p>
<p>running	the	
program	remotely	involves	the	five	basic	steps	shown
in	
Figure	
1.15
.
After	we	type	in	the	
string	to	the	telnet	client	and	hit	the	enter	key,
the	client	sends	the	string	to	the	telnet	server.	After	the	telnet	server
receives	the	string	from	the	network,	it	passes	it	along	to	the	remote	shell
program.	Next,	the	remote	shell	runs	the	
program	and	passes	the
output	line	back	to	the	telnet	server.	Finally,	the	telnet	server	forwards	the
output	string	across	the	network	to	the	telnet	client,	which	prints	the
output	string	on	our	local	terminal.
This	type	of	exchange	between	clients	and	servers	is	typical	of	all
network	applications.	In	
Chapter	
11
you	will	learn	how	to	build	network
applications	and	apply	this	knowledge	to	build	a	simple	Web	server.</p>
<p>1.9	
Important	Themes
This	concludes	our	initial	whirlwind	tour	of	systems.	An	important	idea	to
take	away	from	this	discussion	is	that	a	system	is	more	than	just
hardware.	It	is	a	collection	of	intertwined	hardware	and	systems	software
that	must	cooperate	in	order	to	achieve	the	ultimate	goal	of	running
application	programs.	The	rest	of	this	book	will	fill	in	some	details	about
the	hardware	and	the	software,	and	it	will	show	how,	by	knowing	these
details,	you	can	write	programs	that	are	faster,	more	reliable,	and	more
secure.
To	close	out	this	chapter,	we	highlight	several	important	concepts	that	cut
across	all	aspects	of	computer	systems.	We	will	discuss	the	importance
of	these	concepts	at	multiple	places	within	the	book.
1.9.1	
Amdahl's	Law
Gene	Amdahl,	one	of	the	early	pioneers	in	computing,	made	a	simple	but
insightful	observation	about	the	effectiveness	of	improving	the
performance	of	one	part	of	a	system.	This	observation	has	come	to	be
known	as	
Amdahl's	law.
The	main	idea	is	that	when	we	speed	up	one
part	of	a	system,	the	effect	on	the	overall	system	performance	depends
on	both	how	significant	this	part	was	and	how	much	it	sped	up.	Consider
a	system	in	which	executing	some	application	requires	time	
T
.	Suppose
some	part	of	the	system	requires	a	fraction	α	of	this	time,	and	that	we
old</p>
<h1>improve	its	performance	by	a	factor	of	
k.
That	is,	the	component	originally
required	time	α
T
,	and	it	now	requires	time	(α
T
)/
k
.	The	overall
execution	time	would	thus	be
From	this,	we	can	compute	the	speedup	
S
=	
T
/
T
as
As	an	example,	consider	the	case	where	a	part	of	the	system	that	initially
consumed	60%	of	the	time	(α	=	0.6)	is	sped	up	by	a	factor	of	3	(
k
=	3).
Then	we	get	a	speedup	of	1/[0.4	+	0.6/3]	=	1.67×.	Even	though	we	made
a	substantial	improvement	to	a	major	part	of	the	system,	our	net	speedup
was	significantly	less	than	the	speedup	for	the	one	part.	This	is	the	major
insight	of	Amdahl's	law—to	significantly	speed	up	the	entire	system,	we
must	improve	the	speed	of	a	very	large	fraction	of	the	overall	system.
Practice	Problem	
1.1	
(solution	page	
28
)
Suppose	you	work	as	a	truck	driver,	and	you	have	been	hired	to
carry	a	load	of	potatoes	from	Boise,	Idaho,	to	Minneapolis,
Minnesota,	a	total	distance	of	2,500	kilometers.	You	estimate	you
can	average	100	km/hr	driving	within	the	speed	limits,	requiring	a
total	of	25	hours	for	the	trip.
Aside	
Expressing	relative	performance
old
old
T
new</h1>
<p>(
1
−
α
)
T
old</p>
<ul>
<li></li>
</ul>
<h1>(
α
T
old
)
/
k</h1>
<p>T
o
l
d
[
(
1
−
α
)</p>
<ul>
<li></li>
</ul>
<h1>α
/
k
]
old
new
S</h1>
<p>1
(
1
−
α
)</p>
<ul>
<li></li>
</ul>
<p>α
/
k
(1.1)</p>
<p>The	best	way	to	express	a	performance	improvement	is	as	a	ratio
of	the	form	
T
/
T
,	where	
T
is	the	time	required	for	the	original
version	and	
T
is	the	time	required	by	the	modified	version.	This
will	be	a	number	greater	than	1.0	if	any	real	improvement
occurred.	We	use	the	suffix	`×'	to	indicate	such	a	ratio,	where	the
factor	&quot;2.2×&quot;	is	expressed	verbally	as	&quot;2.2	times.&quot;
The	more	traditional	way	of	expressing	relative	change	as	a
percentage	works	well	when	the	change	is	small,	but	its	definition
is	ambiguous.	Should	it	be	100	·	(
T
−	
T
)/
T
,	or	possibly	100	·
(
T
−	
T
)/
T
,	or	something	else?	In	addition,	it	is	less	instructive
for	large	changes.	Saying	that	&quot;performance	improved	by	120%&quot;
is	more	difficult	to	comprehend	than	simply	saying	that	the
performance	improved	by	2.2×.
A
.	
You	hear	on	the	news	that	Montana	has	just	abolished	its	speed
limit,	which	constitutes	1,500	km	of	the	trip.	Your	truck	can	travel
at	150	km/hr.	What	will	be	your	speedup	for	the	trip?
B
.	
You	can	buy	a	new	turbocharger	for	your	truck	at
www.fasttrucks.com
.	They	stock	a	variety	of	models,	but	the
faster	you	want	to	go,	the	more	it	will	cost.	How	fast	must	you
travel	through	Montana	to	get	an	overall	speedup	for	your	trip	of
1.67×?
Practice	Problem	
1.2	
(solution	page	
28
)
The	marketing	department	at	your	company	has	promised	your
customers	that	the	next	software	release	will	show	a	2×
performance	improvement.	You	have	been	assigned	the	task	of
delivering	on	that	promise.	You	have	determined	that	only	80%	of
old
new
old
new
old
new
new
old
new
old</p>
<h1>the	system	can	be	improved.	How	much	(i.e.,	what	value	of	
k
)
would	you	need	to	improve	this	part	to	meet	the	overall
performance	target?
One	interesting	special	case	of	Amdahl's	law	is	to	consider	the	effect	of
setting	
k
to	∞.	That	is,	we	are	able	to	take	some	part	of	the	system	and
speed	it	up	to	the	point	at	which	it	takes	a	negligible	amount	of	time.	We
then	get
So,	for	example,	if	we	can	speed	up	60%	of	the	system	to	the	point
where	it	requires	close	to	no	time,	our	net	speedup	will	still	only	be	1/0.4
=	2.5×.
Amdahl's	law	describes	a	general	principle	for	improving	any	process.	In
addition	to	its	application	to	speeding	up	computer	systems,	it	can	guide
a	company	trying	to	reduce	the	cost	of	manufacturing	razor	blades,	or	a
student	trying	to	improve	his	or	her	grade	point	average.	Perhaps	it	is
most	meaningful	in	the	world	
of	computers,	where	we	routinely	improve
performance	by	factors	of	2	or	more.	Such	high	factors	can	only	be
achieved	by	optimizing	large	parts	of	a	system.
1.9.2	
Concurrency	and	Parallelism
Throughout	the	history	of	digital	computers,	two	demands	have	been
constant	forces	in	driving	improvements:	we	want	them	to	do	more,	and
we	want	them	to	run	faster.	Both	of	these	factors	improve	when	the
S
∞</h1>
<h2>1
(
1</h2>
<p>α
)
(1.2)</p>
<p>processor	does	more	things	at	once.	We	use	the	term	
concurrency
to
refer	to	the	general	concept	of	a	system	with	multiple,	simultaneous
activities,	and	the	term	
parallelism
to	refer	to	the	use	of	concurrency	to
make	a	system	run	faster.	Parallelism	can	be	exploited	at	multiple	levels
of	abstraction	in	a	computer	system.	We	highlight	three	levels	here,
working	from	the	highest	to	the	lowest	level	in	the	system	hierarchy.
Thread-Level	Concurrency
Building	on	the	process	abstraction,	we	are	able	to	devise	systems	where
multiple	programs	execute	at	the	same	time,	leading	to	
concurrency
.
With	threads,	we	can	even	have	multiple	control	flows	executing	within	a
single	process.	Support	for	concurrent	execution	has	been	found	in
computer	systems	since	the	advent	of	time-sharing	in	the	early	1960s.
Traditionally,	this	concurrent	execution	was	only	
simulated
,	by	having	a
single	computer	rapidly	switch	among	its	executing	processes,	much	as	a
juggler	keeps	multiple	balls	flying	through	the	air.	This	form	of
concurrency	allows	multiple	users	to	interact	with	a	system	at	the	same
time,	such	as	when	many	people	want	to	get	pages	from	a	single	Web
server.	It	also	allows	a	single	user	to	engage	in	multiple	tasks
concurrently,	such	as	having	a	Web	browser	in	one	window,	a	word
processor	in	another,	and	streaming	music	playing	at	the	same	time.	Until
recently,	most	actual	computing	was	done	by	a	single	processor,	even	if
that	processor	had	to	switch	among	multiple	tasks.	This	configuration	is
known	as	a	
uniprocessor	system.
When	we	construct	a	system	consisting	of	multiple	processors	all	under
the	control	of	a	single	operating	system	kernel,	we	have	a	
multiprocessor
system
.	Such	systems	have	been	available	for	large-scale	computing</p>
<p>since	the	1980s,	but	they	have	more	recently	become	commonplace	with
the	advent	of	
multi-core
processors	and	
hyperthreading
.	
Figure	
1.16
shows	a	taxonomy	of	these	different	processor	types.
Multi-core	processors	have	several	CPUs	(referred	to	as	&quot;cores&quot;)
integrated	onto	a	single	integrated-circuit	chip.	
Figure	
1.17
illustrates
the	organization	of	a
Figure	
1.16	
Categorizing	different	processor	configurations.
Multiprocessors	are	becoming	prevalent	with	the	advent	of	multi-core
processors	and	hyperthreading.
Figure	
1.17	
Multi-core	processor	organization.</p>
<p>Four	processor	cores	are	integrated	onto	a	single	chip.
typical	multi-core	processor,	where	the	chip	has	four	CPU	cores,	each
with	its	own	L1	and	L2	caches,	and	with	each	L1	cache	split	into	two
parts—one	to	hold	recently	fetched	instructions	and	one	to	hold	data.	The
cores	share	higher	levels	of	cache	as	well	as	the	interface	to	main
memory.	Industry	experts	predict	that	they	will	be	able	to	have	dozens,
and	ultimately	hundreds,	of	cores	on	a	single	chip.
Hyperthreading,	sometimes	called	
simultaneous	multi-threading
,	is	a
technique	that	allows	a	single	CPU	to	execute	multiple	flows	of	control.	It
involves	having	multiple	copies	of	some	of	the	CPU	hardware,	such	as
program	counters	and	register	files,	while	having	only	single	copies	of
other	parts	of	the	hardware,	such	as	the	units	that	perform	floating-point
arithmetic.	Whereas	a	conventional	processor	requires	around	20,000
clock	cycles	to	shift	between	different	threads,	a	hyper	threaded
processor	decides	which	of	its	threads	to	execute	on	a	cycle-by-cycle
basis.	It	enables	the	CPU	to	take	better	advantage	of	its	processing
resources.	For	example,	if	one	thread	must	wait	for	some	data	to	be
loaded	into	a	cache,	the	CPU	can	proceed	with	the	execution	of	a
different	thread.	As	an	example,	the	Intel	Core	i7	processor	can	have
each	core	executing	two	threads,	and	so	a	four-core	system	can	actually
execute	eight	threads	in	parallel.
The	use	of	multiprocessing	can	improve	system	performance	in	two
ways.	First,	it	reduces	the	need	to	simulate	concurrency	when	performing
multiple	tasks.	As	mentioned,	even	a	personal	computer	being	used	by	a
single	person	is	expected	to	perform	many	activities	concurrently.
Second,	it	can	run	a	single	application	program	faster,	but	only	if	that
program	is	expressed	in	terms	of	multiple	threads	that	can	effectively</p>
<p>execute	in	parallel.	Thus,	although	the	principles	of	concurrency	have
been	formulated	and	studied	for	over	50	years,	the	advent	of	multi-core
and	hyperthreaded	systems	has	greatly	increased	the	desire	to	find	ways
to	write	application	programs	that	can	exploit	the	thread-level	parallelism
available	with	
the	hardware.	
Chapter	
12
will	look	much	more	deeply
into	concurrency	and	its	use	to	provide	a	sharing	of	processing	resources
and	to	enable	more	parallelism	in	program	execution.
Instruction-Level	Parallelism
At	a	much	lower	level	of	abstraction,	modern	processors	can	execute
multiple	instructions	at	one	time,	a	property	known	as	
instruction-level
parallelism
.	For	example,	early	microprocessors,	such	as	the	1978-
vintage	Intel	8086,	required	multiple	(typically	3-10)	clock	cycles	to
execute	a	single	instruction.	More	recent	processors	can	sustain
execution	rates	of	2-4	instructions	per	clock	cycle.	Any	given	instruction
requires	much	longer	from	start	to	finish,	perhaps	20	cycles	or	more,	but
the	processor	uses	a	number	of	clever	tricks	to	process	as	many	as	100
instructions	at	a	time.	In	
Chapter	
4
,	we	will	explore	the	use	of
pipelining
,	where	the	actions	required	to	execute	an	instruction	are
partitioned	into	different	steps	and	the	processor	hardware	is	organized
as	a	series	of	stages,	each	performing	one	of	these	steps.	The	stages
can	operate	in	parallel,	working	on	different	parts	of	different	instructions.
We	will	see	that	a	fairly	simple	hardware	design	can	sustain	an	execution
rate	close	to	1	instruction	per	clock	cycle.
Processors	that	can	sustain	execution	rates	faster	than	1	instruction	per
cycle	are	known	as	
superscalar
processors.	Most	modern	processors
support	superscalar	operation.	In	
Chapter	
5
,	we	will	describe	a	high-</p>
<p>level	model	of	such	processors.	We	will	see	that	application
programmers	can	use	this	model	to	understand	the	performance	of	their
programs.	They	can	then	write	programs	such	that	the	generated	code
achieves	higher	degrees	of	instruction-level	parallelism	and	therefore
runs	faster.
Single-Instruction,	Multiple-Data	(SIMD)
Parallelism
At	the	lowest	level,	many	modern	processors	have	special	hardware	that
allows	a	single	instruction	to	cause	multiple	operations	to	be	performed	in
parallel,	a	mode	known	as	
single-instruction,	multiple-data
(SIMD)
parallelism.	For	example,	recent	generations	of	Intel	and	AMD
processors	have	instructions	that	can	add	8	pairs	of	single-precision
floating-point	numbers	(C	data	type	
)	in	parallel.
These	SIMD	instructions	are	provided	mostly	to	speed	up	applications
that	process	image,	sound,	and	video	data.	Although	some	compilers
attempt	to	automatically	extract	SIMD	parallelism	from	C	programs,	a
more	reliable	method	is	to	write	programs	using	special	
vector
data	types
supported	in	compilers	such	as	
GCC
.	We	describe	this	style	of
programming	in	Web	Aside	
OPT
:
SIMD
,	as	a	supplement	to	the	more
general	presentation	on	program	optimization	found	in	
Chapter	
5
.
1.9.3	
The	Importance	of
Abstractions	in	Computer	Systems</p>
<p>The	use	of	
abstractions
is	one	of	the	most	important	concepts	in
computer	science.	For	example,	one	aspect	of	good	programming
practice	is	to	formulate	a	simple	application	program	interface	(API)	for	a
set	of	functions	that	allow	programmers	to	use	the	code	without	having	to
delve	into	its	inner	workings.	Different	programming
Figure	
1.18	
Some	abstractions	provided	by	a	computer	system.
A	major	theme	in	computer	systems	is	to	provide	abstract
representations	at	different	levels	to	hide	the	complexity	of	the	actual
implementations.
languages	provide	different	forms	and	levels	of	support	for	abstraction,
such	as	Java	class	declarations	and	C	function	prototypes.
We	have	already	been	introduced	to	several	of	the	abstractions	seen	in
computer	systems,	as	indicated	in	
Figure	
1.18
.	On	the	processor	side,
the	
instruction	set	architecture
provides	an	abstraction	of	the	actual
processor	hardware.	With	this	abstraction,	a	machine-code	program
behaves	as	if	it	were	executed	on	a	processor	that	performs	just	one
instruction	at	a	time.	The	underlying	hardware	is	far	more	elaborate,
executing	multiple	instructions	in	parallel,	but	always	in	a	way	that	is
consistent	with	the	simple,	sequential	model.	By	keeping	the	same
execution	model,	different	processor	implementations	can	execute	the
same	machine	code	while	offering	a	range	of	cost	and	performance.</p>
<p>On	the	operating	system	side,	we	have	introduced	three	abstractions:
files
as	an	abstraction	of	I/O	devices,	
virtual	memory
as	an	abstraction	of
program	memory,	and	
processes
as	an	abstraction	of	a	running	program.
To	these	abstractions	we	add	a	new	one:	the	
virtual	machine
,	providing
an	abstraction	of	the	entire	computer,	including	the	operating	system,	the
processor,	and	the	programs.	The	idea	of	a	virtual	machine	was
introduced	by	IBM	in	the	1960s,	but	it	has	become	more	prominent
recently	as	a	way	to	manage	computers	that	must	be	able	to	run
programs	designed	for	multiple	operating	systems	(such	as	Microsoft
Windows,	Mac	OS	X,	and	Linux)	or	different	versions	of	the	same
operating	system.
We	will	return	to	these	abstractions	in	subsequent	sections	of	the	book.</p>
<p>1.10	
Summary
A	computer	system	consists	of	hardware	and	systems	software	that
cooperate	to	run	application	programs.	Information	inside	the	computer	is
represented	as	groups	of	bits	that	are	interpreted	in	different	ways,
depending	on	the	context.	Programs	are	translated	by	other	programs
into	different	forms,	beginning	as	ASCII	text	and	then	translated	by
compilers	and	linkers	into	binary	executable	files.
Processors	read	and	interpret	binary	instructions	that	are	stored	in	main
memory.	Since	computers	spend	most	of	their	time	copying	data	between
memory,	I/O	devices,	and	the	CPU	registers,	the	storage	devices	in	a
system	are	arranged	in	a	hierarchy,	with	the	CPU	registers	at	the	top,
followed	by	multiple	levels	of	hardware	cache	memories,	DRAM	main
memory,	and	disk	storage.	Storage	devices	that	are	higher	in	the
hierarchy	are	faster	and	more	costly	per	bit	than	those	lower	in	the
hierarchy.	Storage	devices	that	are	higher	in	the	hierarchy	serve	as
caches	for	devices	that	are	lower	in	the	hierarchy.	Programmers	can
optimize	the	performance	of	their	C	programs	by	understanding	and
exploiting	the	memory	hierarchy.
The	operating	system	kernel	serves	as	an	intermediary	between	the
application	and	the	hardware.	It	provides	three	fundamental	abstractions:
(1)	Files	are	abstractions	for	I/O	devices.	(2)	Virtual	memory	is	an
abstraction	for	both	main	memory	and	disks.	(3)	Processes	are
abstractions	for	the	processor,	main	memory,	and	I/O	devices.</p>
<p>Finally,	networks	provide	ways	for	computer	systems	to	communicate
with	one	another.	From	the	viewpoint	of	a	particular	system,	the	network
is	just	another	I/O	device.</p>
<p>Bibliographic	Notes
Ritchie	has	written	interesting	firsthand	accounts	of	the	early	days	of	C
and	
[
91
,	
92
].	Ritchie	and	Thompson	presented	the	first	published
account	of	
[
93
].	Silberschatz,	Galvin,	and	Gagne	[
102
]	provide	a
comprehensive	history	of	the	different	flavors	of	Unix.	The	GNU
(
www.gnu.org
)	and	Linux	(
www.linux.org
)	Web	pages	have	loads	of
current	and	historical	information.	The	Posix	standards	are	available
online	at	(
www.unix.org
).</p>
<p>Solutions	to	Practice	Problems
Solution	to	Problem	
1.1	
(page
22
)
This	problem	illustrates	that	Amdahl's	law	applies	to	more	than	just
computer	systems.
A
.	
In	terms	of	
Equation</p>
<p>1.1
,	we	have	α	=	0.6	and	
k
=	1.5.	More
directly,	traveling	the	1,500	kilometers	through	Montana	will
require	10	hours,	and	the	rest	of	the	trip	also	requires	10	hours.
This	will	give	a	speedup	of	25/(10	+	10)	=	1.25×.
B
.	
In	terms	of	
Equation</p>
<p>1.1
,	we	have	α	=	0.6,	and	we	require	
S
=
1.67,	from	which	we	can	solve	for	
k
.	More	directly,	to	speed	up	the
trip	by	1.67×,	we	must	decrease	the	overall	time	to	15	hours.	The
parts	outside	of	Montana	will	still	require	10	hours,	so	we	must
drive	through	Montana	in	5	hours.	This	requires	traveling	at	300
km/hr,	which	is	pretty	fast	for	a	truck!
Solution	to	Problem	
1.2	
(page
23
)</p>
<p>Amdahl's	law	is	best	understood	by	working	through	some	examples.
This	one	requires	you	to	look	at	
Equation</p>
<h1>1.1
from	an	unusual
perspective.
This	problem	is	a	simple	application	of	the	equation.	You	are	given	
S
=	2
and	α	=	0.8,	and	you	must	then	solve	for	
k
:
2</h1>
<h2>1
(
1</h2>
<p>0.8
)</p>
<ul>
<li></li>
</ul>
<p>0.8
/
k
0.4</p>
<ul>
<li></li>
</ul>
<h1>1.6
/
k</h1>
<h1>1.0
k</h1>
<p>2.67</p>
<p>Part	
I	
Program	Structure	and
Execution
Our	exploration	of	computer	systems	starts	by	studying	the	computer
itself,	comprising	a	processor	and	a	memory	subsystem.	At	the	core,	we
require	ways	to	represent	basic	data	types,	such	as	approximations	to
integer	and	real	arithmetic.	From	there,	we	can	consider	how	machine-
level	instructions	manipulate	data	and	how	a	compiler	translates	C
programs	into	these	instructions.	Next,	we	study	several	methods	of
implementing	a	processor	to	gain	a	better	understanding	of	how
hardware	resources	are	used	to	execute	instructions.	Once	we
understand	compilers	and	machine-level	code,	we	can	examine	how	to
maximize	program	performance	by	writing	C	programs	that,	when
compiled,	achieve	the	maximum	possible	performance.	We	conclude	with
the	design	of	the	memory	subsystem,	one	of	the	most	complex
components	of	a	modern	computer	system.
This	part	of	the	book	will	give	you	a	deep	understanding	of	how
application	programs	are	represented	and	executed.	You	will	gain	skills
that	help	you	write	programs	that	are	secure,	reliable,	and	make	the	best
use	of	the	computing	resources.</p>
<p>Chapter	
2	
Representing	and
Manipulating	Information
2.1	
Information	Storage	
34
2.2	
Integer	Representations	
59
2.3	
Integer	Arithmetic	
84
2.4	
Floating	Point	
108
2.5	
Summary</p>
<p>126
Bibliographic	Notes	
127
Homework	Problems	
128
Solutions	to	Practice	Problems	
143
Modern	computers	store	and	process	information
represented	as	two-valued	signals.	These	lowly
binary	digits,	or	
bits
,	form	the	basis	of	the	digital
revolution.	The	familiar	decimal,	or	base-10,
representation	has	been	in	use	for	over	1,000	years,
having	been	developed	in	India,	improved	by	Arab
mathematicians	in	the	12th	century,	and	brought	to</p>
<p>the	West	in	the	13th	century	by	the	Italian
mathematician	Leonardo	Pisano	(ca.	1170	to	ca.
1250),	better	known	as	Fibonacci.	Using	decimal
notation	is	natural	for	10-fingered	humans,	but
binary	values	work	better	when	building	machines
that	store	and	process	information.	Two-valued
signals	can	readily	be	represented,	stored,	and
transmitted—for	example,	as	the	presence	or
absence	of	a	hole	in	a	punched	card,	as	a	high	or
low	voltage	on	a	wire,	or	as	a	magnetic	domain
oriented	clockwise	or	counterclockwise.	The
electronic	circuitry	for	storing	and	performing
computations	on	two-valued	signals	is	very	simple
and	reliable,	enabling	manufacturers	to	integrate
millions,	or	even	billions,	of	such	circuits	on	a	single
silicon	chip.
In	isolation,	a	single	bit	is	not	very	useful.	When	we
group	bits	together	and	apply	some	
interpretation
that	gives	meaning	to	the	different	possible	bit
patterns,	however,	we	can	represent	the	elements
of	any	finite	set.	For	example,	using	a	binary
number	system,	we	can	use	groups	of	bits	to
encode	nonnegative	numbers.	By	using	a	standard
character	code,	we	can	encode	the	letters	and
symbols	in	a	document.	We	cover	both	of	these
encodings	in	this	chapter,	as	well	as	encodings	to
represent	negative	numbers	and	to	approximate
real	numbers.</p>
<p>We	consider	the	three	most	important
representations	of	numbers.	
Unsigned
encodings
are	based	on	traditional	binary	notation,
representing	numbers	greater	than	or	equal	to	0.
Two's-complement
encodings	are	the	most	common
way	to	represent	
signed
integers,	that	is,	numbers
that	may	be	either	positive	or	negative.	
Floating-
point
encodings	are	a	base-2	version	of	scientific
notation	for	representing	real	numbers.	Computers
implement	arithmetic	operations,	such	as	addition
and	multiplication,	with	these	different
representations,	similar	to	the	corresponding
operations	on	integers	and	real	numbers.
Computer	representations	use	a	limited	number	of
bits	to	encode	a	number,	and	hence	some
operations	can	
overflow
when	the	results	are	too
large	to	be	represented.	This	can	lead	to	some
surprising	results.	For	example,	on	most	of	today's
computers	(those	using	a	32-bit	representation	for
data	type	int),	computing	the	expression
yields	–884,901,888.	This	runs	counter	to	the
properties	of	integer	arithmetic—computing	the</p>
<p>product	of	a	set	of	positive	numbers	has	yielded	a
negative	result.
On	the	other	hand,	integer	computer	arithmetic
satisfies	many	of	the	familiar	properties	of	true
integer	arithmetic.	For	example,	multiplication	is
associative	and	commutative,	so	that	computing	any
of	the	following	C	expressions	yields	–884,901,888:
The	computer	might	not	generate	the	expected
result,	but	at	least	it	is	consistent!
Floating-point	arithmetic	has	altogether	different
mathematical	properties.	The	product	of	a	set	of
positive	numbers	will	always	be	positive,	although
overflow	will	yield	the	special	value	+∞.	Floating-
point	arithmetic	is	not	associative	due	to	the	finite
precision	of	the	representation.	For	example,	the	C
expression	
will	evaluate	to	0.0	on
most	machines,	while	
will	evaluate</p>
<p>to	3.14.	The	different	mathematical	properties	of
integer	versus.	floating-point	arithmetic	stem	from
the	difference	in	how	they	handle	the	finiteness	of
their	representations—integer	representations	can
encode	a	comparatively	small	range	of	values,	but
do	so	precisely,	while	floating-point	representations
can	encode	a	wide	range	of	values,	but	only
approximately.
By	studying	the	actual	number	representations,	we
can	understand	the	ranges	of	values	that	can	be
represented	and	the	properties	of	the	different
arithmetic	operations.	This	understanding	is	critical
to	writing	programs	that	work	correctly	over	the	full
range	of	numeric	values	and	that	are	portable
across	different	combinations	of	machine,	operating
system,	and	compiler.	As	we	will	describe,	a	number
of	computer	security	vulnerabilities	have	arisen	due
to	some	of	the	subtleties	of	computer	arithmetic.
Whereas	in	an	earlier	era	program	bugs	would	only
inconvenience	people	when	they	happened	to	be
triggered,	there	are	now	legions	of	hackers	who	try
to	exploit	any	bug	they	can	find	to	obtain
unauthorized	access	to	other	people's	systems.	This
puts	a	higher	level	of	obligation	on	programmers	to
understand	how	their	programs	work	and	how	they
can	be	made	to	behave	in	undesirable	ways.
Computers	use	several	different	binary</p>
<p>Computers	use	several	different	binary
representations	to	encode	numeric	values.	You	will
need	to	be	familiar	with	these	representations	as
you	progress	into	machine-level	programming	in
Chapter	
3
.	We	describe	these	encodings	in	this
chapter	and	show	you	how	to	reason	about	number
representations.
We	derive	several	ways	to	perform	arithmetic
operations	by	directly	manipulating	the	bit-level
representations	of	numbers.	Understanding	these
techniques	will	be	important	for	understanding	the
machine-level	code	generated	by	compilers	in	their
attempt	to	optimize	the	performance	of	arithmetic
expression	evaluation.
Our	treatment	of	this	material	is	based	on	a	core	set
of	mathematical	principles.	We	start	with	the	basic
definitions	of	the	encodings	and	then	derive	such
properties	as	the	range	of	representable	numbers,
their	bit-level	representations,	and	the	properties	of
the	arithmetic	operations.	We	believe	it	is	important
for	you	to	examine	the	material	from	this	abstract
viewpoint,	because	programmers	need	to	have	a
clear	understanding	of	how	computer	arithmetic
relates	to	the	more	familiar	integer	and	real
arithmetic.</p>
<p>The	C++	programming	language	is	built	upon	C,
using	the	exact	same	numeric	representations	and
operations.	Everything	said	in	this	chapter	about	C
also	holds	for	C++.	The	Java	language	definition,	on
the	other	hand,	created	a	new	set	of	standards	for
numeric	representations	and	operations.	Whereas
the	C	standards	are	designed	to	allow	a	wide	range
of	implementations,	the	Java	standard	is	quite
specific	on	the	formats	and	encodings	of	data.	We
highlight	the	representations	and	operations
supported	by	Java	at	several	places	in	the	chapter.
Aside	
How	to	read	this	chapter
In	this	chapter,	we	examine	the	fundamental
properties	of	how	numbers	and	other	forms
of	data	are	represented	on	a	computer	and
the	properties	of	the	operations	that
computers	perform	on	these	data.	This
requires	us	to	delve	into	the	language	of
mathematics,	writing	formulas	and	equations
and	showing	derivations	of	important
properties.
To	help	you	navigate	this	exposition,	we	have
structured	the	presentation	to	first	state	a
property	as	a	
principle
in	mathematical
notation.	We	then	illustrate	this	principle	with
examples	and	an	informal	discussion.	We</p>
<p>recommend	that	you	go	back	and	forth
between	the	statement	of	the	principle	and
the	examples	and	discussion	until	you	have
a	solid	intuition	for	what	is	being	said	and
what	is	important	about	the	property.	For
more	complex	properties,	we	also	provide	a
derivation
,	structured	much	like	a
mathematical	proof.	You	should	try	to
understand	these	derivations	eventually,	but
you	could	skip	over	them	on	first	reading.
We	also	encourage	you	to	work	on	the
practice	problems	as	you	proceed	through
the	presentation.	The	practice	problems
engage	you	in	
active	learning
,	helping	you
put	thoughts	into	action.	With	these	as
background,	you	will	find	it	much	easier	to	go
back	and	follow	the	derivations.	Be	assured,
as	well,	that	the	mathematical	skills	required
to	understand	this	material	are	within	reach
of	someone	with	a	good	grasp	of	high	school
algebra.</p>
<p>2.1	
Information	Storage
Rather	than	accessing	individual	bits	in	memory,	most	computers	use
blocks	of	8	bits,	or	
bytes
,	as	the	smallest	addressable	unit	of	memory.	A
machine-level	program	views	memory	as	a	very	large	array	of	bytes,
referred	to	as	
virtual	memory
.	Every	byte	of	memory	is	identified	by	a
unique	number,	known	as	its	
address
,	and	the	set	of	all	possible
addresses	is	known	as	the	
virtual	address	space
.	As	indicated	by	its
name,	this	virtual	address	space	is	just	a	conceptual	image	presented	to
the	machine-level	program.	The	actual	implementation	(presented	in
Chapter	
9
)	uses	a	combination	of	dynamic	random	access	memory
(DRAM),	flash	memory,	disk	storage,	special	hardware,	and	operating
system	software	to	provide	the	program	with	what	appears	to	be	a
monolithic	byte	array.
In	subsequent	chapters,	we	will	cover	how	the	compiler	and	run-time
system	partitions	this	memory	space	into	more	manageable	units	to	store
the	different	
program	objects
,	that	is,	program	data,	instructions,	and
control	information.	Various	mechanisms	are	used	to	allocate	and
manage	the	storage	for	different	parts	of	the	program.	This	management
is	all	performed	within	the	virtual	address	space.	For	example,	the	value
of	a	pointer	in	C—whether	it	points	to	an	integer,	a	structure,	or	some
other	program	object—is	the	virtual	address	of	the	first	byte	of	some
block	of	storage.	The	C	compiler	also	associates	
type
information	with
each	pointer,	so	that	it	can	generate	different	machine-level	code	to
access	the	value	stored	at	the	location	designated	by	the	pointer
depending	on	the	type	of	that	value.	Although	the	C	compiler	maintains</p>
<p>this	type	information,	the	actual	machine-level	program	it	generates	has
no	information	about	data	types.	It	simply	treats	each	program	object	as
a	block	of	bytes	and	the	program	itself	as	a	sequence	of	bytes.
Aside	
The	evolution	of	the	C
programming	language
As	was	described	in	an	aside	on	page	4,	the	C	programming
language	was	first	developed	by	Dennis	Ritchie	of	Bell
Laboratories	for	use	with	the	Unix	operating	system	(also
developed	at	Bell	Labs).	At	the	time,	most	system	programs,	such
as	operating	systems,	had	to	be	written	largely	in	assembly	code
in	order	to	have	access	to	the	low-level	representations	of
different	data	types.	For	example,	it	was	not	feasible	to	write	a
memory	allocator,	such	as	is	provided	by	the	
library
function,	in	other	high-level	languages	of	that	era.
The	original	Bell	Labs	version	of	C	was	documented	in	the	first
edition	of	the	book	by	Brian	Kernighan	and	Dennis	Ritchie	[
60
].
Over	time,	C	has	evolved	through	the	efforts	of	several
standardization	groups.	The	first	major	revision	of	the	original	Bell
Labs	C	led	to	the	ANSI	C	standard	in	1989,	by	a	group	working
under	the	auspices	of	the	American	National	Standards	Institute.
ANSI	C	was	a	major	departure	from	Bell	Labs	C,	especially	in	the
way	functions	are	declared.	ANSI	C	is	described	in	the	second
edition	of	Kernighan	and	Ritchie's	book	[
61
],	which	is	still
considered	one	of	the	best	references	on	C.</p>
<p>The	International	Standards	Organization	took	over	responsibility
for	standardizing	the	C	language,	adopting	a	version	that	was
substantially	the	same	as	ANSI	C	in	1990	and	hence	is	referred	to
as	“ISO	C90.”
This	same	organization	sponsored	an	updating	of	the	language	in
1999,	yielding	“ISO	C99.”	Among	other	things,	this	version
introduced	some	new	data	types	and	provided	support	for	text
strings	requiring	characters	not	found	in	the	English	language.	A
more	recent	standard	was	approved	in	2011,	and	hence	is	named
“ISO	C11,”	again	adding	more	data	types	and	features.	Most	of
these	recent	additions	have	been	
backward	compatible
,	meaning
that	programs	written	according	to	the	earlier	standard	(at	least	as
far	back	as	ISO	C90)	will	have	the	same	behavior	when	compiled
according	to	the	newer	standards.
The	GNU	Compiler	Collection	(
)	can	compile	programs
according	to	the	conventions	of	several	different	versions	of	the	C
language,	based	on	different	command-line	options,	as	shown	in
Figure	
2.1
.	For	example,	to	compile	program	
according
to	ISO	C11,	we	could	give	the	command	line
The	options	
and	
have	identical	effect—the	code	is
compiled	according	to	the	ANSI	or	ISO	C90	standard.	(C90	is
sometimes	referred	to	as	“C89,”	since	its	standardization	effort</p>
<p>began	in	1989.)	The	option	
causes	the	compiler	to	follow
the	ISO	C99	convention.
As	of	the	writing	of	this	book,	when	no	option	is	specified,	the
program	will	be	compiled	according	to	a	version	of	C	based	on
ISO	C90,	but	including	some	features	of	C99,	some	of	C11,	some
of	C++,	and	others	specific	to	
GCC
.	The	GNU	project	is	developing
a	version	that	combines	ISO	C11,	plus	other	features,	that	can	be
specified	with	command-line	option	
.	(Currently,	this
implementation	is	incomplete.)	This	will	become	the	default
version.
C	version
command-line	option
GNU	89
none
,	
ANSI,	ISO	C90
ISO	C99
ISO	C11
Figure	
2.1	
Specifying	different	versions	of	C	to	
.
New	to	C?	
The	role	of	pointers	in	C
Pointers	are	a	central	feature	of	C.	They	provide	the	mechanism
for	referencing	elements	of	data	structures,	including	arrays.	Just
like	a	variable,	a	pointer	has	two	aspects:	its	
value
and	its	
type
.
The	value	indicates	the	location	of	some	object,	while	its	type
indicates	what	kind	of	object	(e.g.,	integer	or	floating-point
number)	is	stored	at	that	location.</p>
<p>Truly	understanding	pointers	requires	examining	their
representation	and	implementation	at	the	machine	level.	This	will
be	a	major	focus	in	
Chapter	
3
,	culminating	in	an	in-depth
presentation	in	
Section	
3.10.1
.
2.1.1	
Hexadecimal	Notation
A	single	byte	consists	of	8	bits.	In	binary	notation,	its	value	ranges	from
00000000
to	11111111
.	When	viewed	as	a	decimal	integer,	its	value
ranges	from	0
to	255
.	Neither	notation	is	very	convenient	for
describing	bit	patterns.	Binary	notation	is	too	verbose,	while	with	decimal
notation	it	is	tedious	to	convert	to	and	from	bit	patterns.	Instead,	we	write
bit	patterns	as	base-16,	or	
hexadecimal
numbers.	Hexadecimal	(or
simply	“hex”)	uses	digits	‘0’	through	‘9’	along	with	characters	‘A’	through
‘F’	to	represent	16	possible	values.	
Figure	
2.2
shows	the	decimal	and
binary	values	associated	with	the	16	hexadecimal	digits.	Written	in
hexadecimal,	the	value	of	a	single	byte	can	range	from	00
to	FF
.
In	C,	numeric	constants	starting	with	
or	
are	interpreted	as	being	in
hexadecimal.	The	characters	‘A’	through	‘F’	may	be	written	in	either
upper-	or	lowercase.	For	example,	we	could	write	the	number	FA1D37B
as	
,	as	
,	or	even	mixing	upper-	and	lower	case	(e.g.,
).	We	will	use	the	C	notation	for	representing	hexadecimal
values	in	this	book.
A	common	task	in	working	with	machine-level	programs	is	to	manually
convert	between	decimal,	binary,	and	hexadecimal	representations	of	bit
2
2
10
10
16
16
16</p>
<p>patterns.	Converting	between	binary	and	hexadecimal	is	straightforward,
since	it	can	be	performed	one	hexadecimal	digit	at	a	time.	Digits	can	be
converted	by	referring	to	a	chart	such	as	that	shown	in	
Figure	
2.2
.
One	simple	trick	for	doing	the	conversion	in	your	head	is	to	memorize	the
decimal	equivalents	of	hex	digits	
,	and	
.
Hex	digit
0
1
2
3
4
5
6
7
Decimal	value
0
1
2
3
4
5
6
7
Binary	value
0000
0001
0010
0011
0100
0101
0110
0111
Hex	digit
8
9
A
B
C
D
E
F
Decimal	value
8
9
10
11
12
13
14
15
Binary	value
1000
1001
1010
1011
1100
1101
1110
1111
Figure	
2.2	
Hexadecimal	notation.
Each	hex	digit	encodes	one	of	16	values.
The	hex	values	
,	and	
can	be	translated	to	decimal	by	computing
their	values	relative	to	the	first	three.
For	example,	suppose	you	are	given	the	number	
.	You	can
convert	this	to	binary	format	by	expanding	each	hexadecimal	digit,	as
follows:
Hexadecimal
Binary</p>
<p>This	gives	the	binary	representation	000101110011101001001100.
Conversely,	given	a	binary	number	1111001010110110110011,	you
convert	it	to	hexadecimal	by	first	splitting	it	into	groups	of	4	bits	each.
Note,	however,	that	if	the	total	number	of	bits	is	not	a	multiple	of	4,	you
should	make	the	
leftmost
group	be	the	one	with	fewer	than	4	bits,
effectively	padding	the	number	with	leading	zeros.	Then	you	translate
each	group	of	bits	into	the	corresponding	hexadecimal	digit:
Binary
Hexadecimal
Practice	Problem	
2.1
(solution	page	
143
)
Perform	the	following	number	conversions:
A
.	
to	binary
B
.	
binary	1100100101111011	to	hexadecimal
C
.	
to	binary
D
.	
binary	1001101110011110110101	to	hexadecimal
When	a	value	
x
is	a	power	of	2,	that	is,	
x
=	2
for	some	nonnegative
integer	
n
,	we	can	readily	write	
x
in	hexadecimal	form	by	remembering
that	the	binary	representation	of	
x
is	simply	1	followed	by	
n
zeros.	The
hexadecimal	digit	0	represents	4	binary	zeros.	So,	for	
n
written	in	the
form	
i
+	4
j
,	where	0	≤	
i
≤	3,	we	can	write	
x
with	a	leading	hex	digit	of	1	(
i
=
0),	2	(
i
=	1),	4	(
i
=	2),	or	8	(
i
=	3),	followed	by	
j
hexadecimal	
.	As	an
n</p>
<p>example,	for	
x
=	2,048	=	211,	we	have	
n
=	11	=	3	+	4·2,	giving
hexadecimal	representation	
.
Practice	Problem	
2.2
(solution	page	
143
)
Fill	in	the	blank	entries	in	the	following	table,	giving	the	decimal
and	hexadecimal	representations	of	different	powers	of	2:
n
2
(decimal)
2
(hexadecimal)
9
512
19</p>
<hr />
<hr />
<p>16,384</p>
<hr />
<hr />
<p>17</p>
<hr />
<hr />
<hr />
<p>32</p>
<hr />
<hr />
<hr />
<p>Converting	between	decimal	and	hexadecimal	representations	requires
using	multiplication	or	division	to	handle	the	general	case.	To	convert	a
decimal	number	
x
to	hexadecimal,	we	can	repeatedly	divide	
x
by	16,
giving	a	quotient	
q
and	a	remainder
r
,	such	that	
x
=	
q
·	16	+	
r
.We	then	use
the	hexadecimal	digit	representing	
r
as	the	least	significant	digit	and
generate	the	remaining	digits	by	repeating	the	process	on	
q
.	As	an
example,	consider	the	conversion	of	decimal	314,156:314,156
n
n</p>
<p>From	this	we	can	read	off	the	hexadecimal	representation	as	
.
Conversely,	to	convert	a	hexadecimal	number	to	decimal,	we	can	multiply
each	of	the	hexadecimal	digits	by	the	appropriate	power	of	16.	For
example,	given	the	number	
,	we	compute	its	decimal	equivalent	as
7	·	16
+	10	·	16	+	15	=	7	·	256	+	10	·	16	+	15	=	1,792	+	160	+	15	=
1,967.
Practice	Problem	
2.3
(solution	page	
144
)
A	single	byte	can	be	represented	by	2	hexadecimal	digits.	Fill	in
the	missing	entries	in	the	following	table,	giving	the	decimal,
binary,	and	hexadecimal	values	of	different	byte	patterns:
Decimal
Binary
Hexadecimal
0
0000	0000
167</p>
<hr />
<hr />
<p>62</p>
<hr />
<hr />
<p>188</p>
<hr />
<hr />
<hr />
<p>0011	0111</p>
<hr />
<hr />
<p>1000	1000</p>
<hr />
<hr />
<p>1111	0011</p>
<hr />
<h1 id="314156314156"><a class="header" href="#314156314156">314,156:314,156</a></h1>
<p>19,634
⋅
16</p>
<ul>
<li></li>
</ul>
<h1>12
(
C
)
19,634</h1>
<p>1,227
⋅
16</p>
<ul>
<li></li>
</ul>
<h1>2
(
2
)
1,227</h1>
<p>76
⋅
16</p>
<ul>
<li></li>
</ul>
<p>11
2</p>
<p>Aside	
Converting	between	decimal
and	hexadecimal
For	converting	larger	values	between	decimal	and
hexadecimal,	it	is	best	to	let	a	computer	or	calculator	do	the
work.	There	are	numerous	tools	that	can	do	this.	One
simple	way	is	to	use	any	of	the	standard	search	engines,
with	queries	such	as
Convert	
to	decimal
or
123	in	hex
Decimal
Binary
Hexadecimal</p>
<hr />
<hr />
<hr />
<hr />
<hr />
<hr />
<p>Practice	Problem	
2.4
(solution	page	
144
)
Without	converting	the	numbers	to	decimal	or	binary,	try	to	solve
the	following	arithmetic	problems,	giving	the	answers	in
hexadecimal.	
Hint:
Just	modify	the	methods	you	use	for
performing	decimal	addition	and	subtraction	to	use	base	16.
A
.	
__________
B
.	
__________</p>
<p>C
.	
__________
D
.	
__________
2.1.2	
Data	Sizes
Every	computer	has	a	
word	size
,	indicating	the	nominal	size	of	pointer
data.	Since	a	virtual	address	is	encoded	by	such	a	word,	the	most
important	system	parameter	determined	by	the	word	size	is	the	maximum
size	of	the	virtual	address	space.	That	is,	for	a	machine	with	a	
w
-bit	word
size,	the	virtual	addresses	can	range	from	0	to	2
—	1,	giving	the
program	access	to	at	most	2
bytes.
In	recent	years,	there	has	been	a	widespread	shift	from	machines	with
32-bit	word	sizes	to	those	with	word	sizes	of	64	bits.	This	occurred	first
for	high-end	machines	designed	for	large-scale	scientific	and	database
applications,	followed	by	desktop	and	laptop	machines,	and	most
recently	for	the	processors	found	in	smartphones.	A	32-bit	word	size
limits	the	virtual	address	space	to	4	gigabytes	(written	4	GB),	that	is,	just
over	4	×	10
bytes.	Scaling	up	to	a	64-bit	word	size	leads	to	a	virtual
address	space	of	16	
exabytes
,	or	around	1.84	×	10
bytes.
Most	64-bit	machines	can	also	run	programs	compiled	for	use	on	32-bit
machines,	a	form	of	backward	compatibility.	So,	for	example,	when	a
program	
is	compiled	with	the	directive
w
w
9
19</p>
<p>then	this	program	will	run	correctly	on	either	a	32-bit	or	a	64-bit	machine.
On	the	other	hand,	a	program	compiled	with	the	directive
will	only	run	on	a	64-bit	machine.	We	will	therefore	refer	to	programs	as
being	either	“32-bit	programs”	or	“64-bit	programs,”	since	the	distinction
lies	in	how	a	program	is	compiled,	rather	than	the	type	of	machine	on
which	it	runs.
Computers	and	compilers	support	multiple	data	formats	using	different
ways	to	encode	data,	such	as	integers	and	floating	point,	as	well	as
different	lengths.	For	example,	many	machines	have	instructions	for
manipulating	single	bytes,	as	well	as	integers	represented	as	2-,	4-,	and
8-byte	quantities.	They	also	support	floating-point	numbers	represented
as	4-	and	8-byte	quantities.
The	C	language	supports	multiple	data	formats	for	both	integer	and
floating-point	data.	
Figure	
2.3
shows	the	number	of	bytes	typically
allocated	for	different	C	data	types.	(We	discuss	the	relation	between
what	is	guaranteed	by	the	C	standard	versus.	what	is	typical	in	
Section
2.2
.)	The	exact	numbers	of	bytes	for	some	data	types	depends	on	how
the	program	is	compiled.	We	show	sizes	for	typical	32-bit	and	64-bit
programs.	Integer	data	can	be	either	
signed
,	able	to	represent	negative,
zero,	and	positive	values,	or	
unsigned
,	only	allowing	nonnegative	values.</p>
<p>Data	type	char	represents	a	single	byte.	Although	the	name	char	derives
from	the	fact	that	it	is	used	to	store	a	single	character	in	a	text	string,	it
can	also	be	used	to	store	integer	values.	Data	types	
,	and
are	intended	to	provide	a	range	of
C	declaration
Bytes
Signed
Unsigned
32-bit
64-bit
1
1
2
2
4
4
4
8
4
4
8
8
4
8
4
4
8
8
Figure	
2.3	
Typical	sizes	(in	bytes)	of	basic	C	data	types.
The	number	of	bytes	allocated	varies	with	how	the	program	is	compiled.
This	chart	shows	the	values	typical	of	32-bit	and	64-bit	programs.
New	to	C?	
Declaring	pointers</p>
<p>For	any	data	type	
T
,	the	declaration
indicates	that	p	is	a	pointer	variable,	pointing	to	an	object	of	type
T
.	For	example,
is	the	declaration	of	a	pointer	to	an	object	of	type	
.
sizes.	Even	when	compiled	for	64-bit	systems,	data	type	
is	usually
just	4	bytes.	Data	type	
commonly	has	4	bytes	in	32-bit	programs
and	8	bytes	in	64-bit	programs.
To	avoid	the	vagaries	of	relying	on	“typical”	sizes	and	different	compiler
settings,	ISO	C99	introduced	a	class	of	data	types	where	the	data	sizes
are	fixed	regardless	of	compiler	and	machine	settings.	Among	these	are
data	types	
and	
,	having	exactly	4	and	8	bytes,
respectively.	Using	fixed-size	integer	types	is	the	best	way	for
programmers	to	have	close	control	over	data	representations.
Most	of	the	data	types	encode	signed	values,	unless	prefixed	by	the
keyword	unsigned	or	using	the	specific	unsigned	declaration	for	fixed-
size	data	types.	The	exception	to	this	is	data	type	
.	Although	most
compilers	and	machines	treat	these	as	signed	data,	the	C	standard	does
not	guarantee	this.	Instead,	as	indicated	by	the	square	brackets,	the</p>
<p>programmer	should	use	the	declaration	
to	guarantee	a	1-
byte	signed	value.	In	many	contexts,	however,	the	program's	behavior	is
insensitive	to	whether	data	type	
is	signed	or	unsigned.
The	C	language	allows	a	variety	of	ways	to	order	the	keywords	and	to
include	or	omit	optional	keywords.	As	examples,	all	of	the	following
declarations	have	identical	meaning:
We	will	consistently	use	the	forms	found	in	
Figure	
2.3
.
Figure	
2.3
also	shows	that	a	pointer	(e.g.,	a	variable	declared	as
being	of	type	
)	uses	the	full	word	size	of	the	program.	Most
machines	also	support	two	different	floating-point	formats:	single
precision,	declared	in	C	as	
,	and	double	precision,	declared	in	C	as
.	These	formats	use	4	and	8	bytes,	respectively.
Programmers	should	strive	to	make	their	programs	portable	across
different	machines	and	compilers.	One	aspect	of	portability	is	to	make	the
program	insensitive	to	the	exact	sizes	of	the	different	data	types.	The	C
standards	set	lower	bounds	
on	the	numeric	ranges	of	the	different	data
types,	as	will	be	covered	later,	but	there	are	no	upper	bounds	(except
with	the	fixed-size	types).	With	32-bit	machines	and	32-bit	programs</p>
<p>being	the	dominant	combination	from	around	1980	until	around	2010,
many	programs	have	been	written	assuming	the	allocations	listed	for	32-
bit	programs	in	
Figure	
2.3
.	With	the	transition	to	64-bit	machines,
many	hidden	word	size	dependencies	have	arisen	as	bugs	in	migrating
these	programs	to	new	machines.	For	example,	many	programmers
historically	assumed	that	an	object	declared	as	type	
could	be	used	to
store	a	pointer.	This	works	fine	for	most	32-bit	programs,	but	it	leads	to
problems	for	64-bit	programs.
2.1.3	
Addressing	and	Byte	Ordering
For	program	objects	that	span	multiple	bytes,	we	must	establish	two
conventions:	what	the	address	of	the	object	will	be,	and	how	we	will	order
the	bytes	in	memory.	In	virtually	all	machines,	a	multi-byte	object	is
stored	as	a	contiguous	sequence	of	bytes,	with	the	address	of	the	object
given	by	the	smallest	address	of	the	bytes	used.	For	example,	suppose	a
variable	
of	type	
has	address	
;	that	is,	the	value	of	the
address	expression	
is	
.	Then	(assuming	data	type	
has	a	32-
bit	representation)	the	4	bytes	of	
would	be	stored	in	memory	locations
,	and	
For	ordering	the	bytes	representing	an	object,	there	are	two	common
conventions.	Consider	a	
w
-bit	integer	having	a	bit	representation	
,	where	
x
is	the	most	significant	bit	and	
x
is	the
least.	Assuming	
w
is	a	multiple	of	8,	these	bits	can	be	grouped	as	bytes,
with	the	most	significant	byte	having	bits	
,	the	least
significant	byte	having	bits	
,	and	the	other	bytes	having	bits
[
x
w
−
1
,
x
w
−
2
,
⋯
,
x
1
,
x
0
]
w
–1
0
[
x
w
−
1
,
x
w
−
2
,
⋯
,
x
w
−
8
]
[
x
7
,
x
6
,
…
,
x
0
]</p>
<p>from	the	middle.	Some	machines	choose	to	store	the	object	in	memory
ordered	from	least	significant	byte	to	most,	while	other	machines	store
them	from	most	to	least.	The	former	convention—where	the	least
significant	byte	comes	first—is	referred	to	as	
little	endian
.	The	latter
convention—where	the	most	significant	byte	comes	first—is	referred	to
as	
big	endian
.
Suppose	the	variable	
of	type	
and	at	address	
has	a
hexadecimal	value	of	
.	The	ordering	of	the	bytes	within	the
address	range	
through	
depends	on	the	type	of	machine:
Note	that	in	the	word	
the	high-order	byte	has	hexadecimal
value	
,	while	the	low-order	byte	has	value	
Most	Intel-compatible	machines	operate	exclusively	in	little-endian	mode.
On	the	other	hand,	most	machines	from	IBM	and	Oracle	(arising	from
their	acquisition
Aside	
Origin	of	“endian”</p>
<p>Here	is	how	Jonathan	Swift,	writing	in	1726,	described	the	history
of	the	controversy	between	big	and	little	endians:
.	.	.	Lilliput	and	Blefuscu	.	.	.	have,	as	I	was	going	to	tell	you,	been	engaged	in	a	most
obstinate	war	for	six-and-thirty	moons	past.	It	began	upon	the	following	occasion.	It	is
allowed	on	all	hands,	that	the	primitive	way	of	breaking	eggs,	before	we	eat	them,
was	upon	the	larger	end;	but	his	present	majesty's	grandfather,	while	he	was	a	boy,
going	to	eat	an	egg,	and	breaking	it	according	to	the	ancient	practice,	happened	to
cut	one	of	his	fingers.	Whereupon	the	emperor	his	father	published	an	edict,
commanding	all	his	subjects,	upon	great	penalties,	to	break	the	smaller	end	of	their
eggs.	The	people	so	highly	resented	this	law,	that	our	histories	tell	us,	there	have
been	six	rebellions	raised	on	that	account;	wherein	one	emperor	lost	his	life,	and
another	his	crown.	These	civil	commotions	were	constantly	fomented	by	the
monarchs	of	Blefuscu;	and	when	they	were	quelled,	the	exiles	always	fled	for	refuge
to	that	empire.	It	is	computed	that	eleven	thousand	persons	have	at	several	times
suffered	death,	rather	than	submit	to	break	their	eggs	at	the	smaller	end.	Many
hundred	large	volumes	have	been	published	upon	this	controversy:	but	the	books	of
the	Big-endians	have	been	long	forbidden,	and	the	whole	party	rendered	incapable
by	law	of	holding	employments.
(Jonathan	Swift.	Gulliver's	Travels,	Benjamin	Motte,	1726.)
In	his	day,	Swift	was	satirizing	the	continued	conflicts	between
England	(Lilliput)	and	France	(Blefuscu).	Danny	Cohen,	an	early
pioneer	in	networking	protocols,	first	applied	these	terms	to	refer
to	byte	ordering	[
24
],	and	the	terminology	has	been	widely
adopted.
of	Sun	Microsystems	in	2010)	operate	in	big-endian	mode.	Note	that	we
said	“most.”	The	conventions	do	not	split	precisely	along	corporate
boundaries.	For	example,	both	IBM	and	Oracle	manufacture	machines
that	use	Intel-compatible	processors	and	hence	are	little	endian.	Many
recent	microprocessor	chips	are	
bi-endian
,	meaning	that	they	can	be</p>
<p>configured	to	operate	as	either	little-	or	big-endian	machines.	In	practice,
however,	byte	ordering	becomes	fixed	once	a	particular	operating	system
is	chosen.	For	example,	ARM	microprocessors,	used	in	many	cell
phones,	have	hardware	that	can	operate	in	either	little-	or	big-endian
mode,	but	the	two	most	common	operating	systems	for	these	chips—
Android	(from	Google)	and	IOS	(from	Apple)	—operate	only	in	little-
endian	mode.
People	get	surprisingly	emotional	about	which	byte	ordering	is	the	proper
one.	In	fact,	the	terms	“little	endian”	and	“big	endian”	come	from	the	book
Gulliver's	Travels
by	Jonathan	Swift,	where	two	warring	factions	could	not
agree	as	to	how	a	soft-boiled	egg	should	be	opened—by	the	little	end	or
by	the	big.	Just	like	the	egg	issue,	there	is	no	technological	reason	to
choose	one	byte	ordering	convention	over	the	other,	and	hence	the
arguments	degenerate	into	bickering	about	sociopolitical	issues.	As	long
as	one	of	the	conventions	is	selected	and	adhered	to	consistently,	the
choice	is	arbitrary.
For	most	application	programmers,	the	byte	orderings	used	by	their
machines	are	totally	invisible;	programs	compiled	for	either	class	of
machine	give	identical	results.	At	times,	however,	byte	ordering	becomes
an	issue.	The	first	is	when	
binary	data	are	communicated	over	a	network
between	different	machines.	A	common	problem	is	for	data	produced	by
a	little-endian	machine	to	be	sent	to	a	big-endian	machine,	or	vice	versa,
leading	to	the	bytes	within	the	words	being	in	reverse	order	for	the
receiving	program.	To	avoid	such	problems,	code	written	for	networking
applications	must	follow	established	conventions	for	byte	ordering	to
make	sure	the	sending	machine	converts	its	internal	representation	to	the
network	standard,	while	the	receiving	machine	converts	the	network</p>
<p>standard	to	its	internal	representation.	We	will	see	examples	of	these
conversions	in	
Chapter	
11
.
A	second	case	where	byte	ordering	becomes	important	is	when	looking
at	the	byte	sequences	representing	integer	data.	This	occurs	often	when
inspecting	machine-level	programs.	As	an	example,	the	following	line
occurs	in	a	file	that	gives	a	text	representation	of	the	machine-level	code
for	an	Intel	x86–64	processor:
This	line	was	generated	by	a	
disassembler
,	a	tool	that	determines	the
instruction	sequence	represented	by	an	executable	program	file.	We	will
learn	more	about	disassemblers	and	how	to	interpret	lines	such	as	this	in
Chapter	
3
.	For	now,	we	simply	note	that	this	line	states	that	the
hexadecimal	byte	sequence	
is	the	byte-level
representation	of	an	instruction	that	adds	a	word	of	data	to	the	value
stored	at	an	address	computed	by	adding	
to	the	current	value
of	the	
program	counter
,	the	address	of	the	next	instruction	to	be
executed.	If	we	take	the	final	4	bytes	of	the	sequence	
and
write	them	in	reverse	order,	we	have	
.	Dropping	the	leading
0,	we	have	the	value	
,	the	numeric	value	written	on	the	right.
Having	bytes	appear	in	reverse	order	is	a	common	occurrence	when
reading	machine-level	program	representations	generated	for	little-
endian	machines	such	as	this	one.	The	natural	way	to	write	a	byte
sequence	is	to	have	the	lowest-numbered	byte	on	the	left	and	the	highest</p>
<p>on	the	right,	but	this	is	contrary	to	the	normal	way	of	writing	numbers	with
the	most	significant	digit	on	the	left	and	the	least	on	the	right.
A	third	case	where	byte	ordering	becomes	visible	is	when	programs	are
written	that	circumvent	the	normal	type	system.	In	the	C	language,	this
can	be	done	using	a	
cast
or	a	
union
to	allow	an	object	to	be	referenced
according	to	a	different	data	type	from	which	it	was	created.	Such	coding
tricks	are	strongly	discouraged	for	most	application	programming,	but
they	can	be	quite	useful	and	even	necessary	for	system-level
programming.
Figure	
2.4
shows	C	code	that	uses	casting	to	access	and	print	the
byte	representations	of	different	program	objects.	We	use	
to
define	data	type	
as	a	pointer	to	an	object	of	type	
Such	a	byte	pointer	references	a	sequence	of	bytes	where	each
byte	is	considered	to	be	a	nonnegative	integer.	The	first	routine
is	given	the	address	of	a	sequence	of	bytes,	indicated	by	a
byte	pointer,	and	a	byte	count.	The	byte	count	is	specified	as	having	data
type	
,	the	preferred	data	type	for	expressing	the	sizes	of	data
structures.	It	prints	the	individual	bytes	in	hexadecimal.	The	C	formatting
directive	
indicates	that	an	integer	should	be	printed	in	hexadecimal
with	at	least	2	digits.</p>
<p>Figure	
2.4	
Code	to	print	the	byte	representation	of	program	objects.
This	code	uses	casting	to	circumvent	the	type	system.	Similar	functions
are	easily	defined	for	other	data	types.
Procedures	
,	and	
demonstrate	how	to
use	procedure	
to	print	the	byte	representations	of	C	program
objects	of	type	
,	and	
,	respectively.	Observe	that	they
simply	pass	
a	pointer	
to	their	argument	
,	casting	the
pointer	to	be	of	type	
.	This	cast	indicates	to	the	compiler
that	the	program	should	consider	the	pointer	to	be	to	a	sequence	of	bytes</p>
<p>rather	than	to	an	object	of	the	original	data	type.	This	pointer	will	then	be
to	the	lowest	byte	address	occupied	by	the	object.
These	procedures	use	the	C	
to	determine	the	number
of	bytes	used	by	the	object.	In	general,	the	expression	
returns
the	number	of	bytes	required	to	store	an	object	of	type	
T
.	Using	
rather	than	a	fixed	value	is	one	step	toward	writing	code	that	is	portable
across	different	machine	types.
We	ran	the	code	shown	in	
Figure	
2.5
on	several	different	machines,
giving	the	results	shown	in	
Figure	
2.6
.	The	following	machines	were
used:
Linux	32
Intel	IA32	processor	running	Linux.
Windows
Intel	IA32	processor	running	Windows.
Sun
Sun	Microsystems	SPARC	processor	running	Solaris.	(These	machines	are	now
produced	by	Oracle.)
Linux	64
Intel	x86–64	processor	running	Linux.</p>
<p>Figure	
2.5	
Byte	representation	examples.
This	code	prints	the	byte	representations	of	sample	data	objects.
Machine
Value
Type
Bytes	(hex)
Linux	32
12,345
Windows
12,345
Sun
12,345
Linux	64
12,345
Linux	32
12,345.0
Windows
12,345.0
Sun
12,345.0
Linux	64
12,345.0
Linux	32
Windows
Sun
Linux	64
Figure	
2.6	
Byte	representations	of	different	data	values.
Results	for	
and	
are	identical,	except	for	byte	ordering.	Pointer
values	are	machine	dependent.</p>
<p>Our	argument	12,345	has	hexadecimal	representation	
.	For
the	
data,	we	get	identical	results	for	all	machines,	except	for	the	byte
ordering.	In	particular,	we	can	see	that	the	least	significant	byte	value	of
is	printed	first	for	Linux	32,	Windows,	and	Linux	64,	indicating	little-
endian	machines,	and	last	for	Sun,	indicating	a	big-endian	machine.
Similarly,	the	bytes	of	the	
data	are	identical,	except	for	the	byte
ordering.	On	the	other	hand,	the	pointer	values	are	completely	different.
The	different	machine/operating	system	configurations	use	different
conventions	for	storage	allocation.	One	feature	to	note	is	that	the	Linux
32,	Windows,	and	Sun	machines	use	4-byte	addresses,	while	the	Linux
64	machine	uses	8-byte	addresses.
New	to	C?	
Naming	data	types	with
The	
declaration	in	C	provides	a	way	of	giving	a	name	to	a
data	type.	This	can	be	a	great	help	in	improving	code	readability,
since	deeply	nested	type	declarations	can	be	difficult	to	decipher.
The	syntax	for	
is	exactly	like	that	of	declaring	a	variable,
except	that	it	uses	a	type	name	rather	than	a	variable	name.	Thus,
the	declaration	of	
in	
Figure	
2.4
has	the	same
form	as	the	declaration	of	a	variable	of	type	
For	example,	the	declaration</p>
<p>defines	type	
be	a	pointer	to	an	int,	and	declares	a
variable	
of	this	type.	Alternatively,	we	could	declare	this
variable	directly	as
New	to	C?	
Formatted	printing	with
The	
function	(along	with	its	cousins	
and	
)
provides	a	way	to	print	information	with	considerable	control	over
the	formatting	details.	The	first	argument	is	a	
format	string
,	while
any	remaining	arguments	are	values	to	be	printed.	Within	the
format	string,	each	character	sequence	starting	with	
indicates
how	to	format	the	next	argument.	Typical	examples	include	
to	print	a	decimal	integer,	
to	print	a	floating-point	number,
and	
to	print	a	character	having	the	character	code	given	by
the	argument.
Specifying	the	formatting	of	fixed-size	data	types,	such	as
,	is	a	bit	more	involved,	as	is	described	in	the	aside	on
page	
67
.</p>
<p>Observe	that	although	the	floating-point	and	the	integer	data	both	encode
the	numeric	value	12,345,	they	have	very	different	byte	patterns:
for	the	integer	and	
for	floating	point.	In	general,
these	two	formats	use	different	encoding	schemes.	If	we	expand	these
hexadecimal	patterns	into	binary	form	and	shift	them	appropriately,	we
find	a	sequence	of	13	matching	bits,	indicated	by	a	sequence	of
asterisks,	as	follows:
This	is	not	coincidental.	We	will	return	to	this	example	when	we	study
floating-point	formats.
New	to	C?	
Pointers	and	arrays
In	function	
(
Figure	
2.4
),	we	see	the	close
connection	between	pointers	and	arrays,	as	will	be	discussed	in
detail	in	
Section	
3.8
.	We	see	that	this	function	has	an	argument
of	type	
(which	has	been	defined	to	be	a
pointer	to	
),	but	we	see	the	array	reference	
on	line	8.	In	C,	we	can	dereference	a	pointer	with	array	notation,
and	we	can	reference	array	elements	with	pointer	notation.	In	this
example,	the	reference	
indicates	that	we	want	to	read
the	byte	that	is	
positions	beyond	the	location	pointed	to	by
.</p>
<p>New	to	C?	
Pointer	creation	and
dereferencing
In	lines	13,	17,	and	21	of	
Figure	
2.4
we	see	uses	of	two
operations	that	give	C	(and	therefore	C++)	its	distinctive	character.
The	C	“address	of”	operator	
creates	a	pointer.	On	all	three	lines,
the	expression	
creates	a	pointer	to	the	location	holding	the
object	indicated	by	variable	
.	The	type	of	this	pointer	depends	on
the	type	of	
,	and	hence	these	three	pointers	are	of	type	
,	and	
,	respectively.	(Data	type	</p>
<pre><code>is	a	special
</code></pre>
<p>kind	of	pointer	with	no	associated	type	information.)
The	cast	operator	converts	from	one	data	type	to	another.	Thus,
the	cast	(
)	
indicates	that	whatever	type	the	pointer
had	before,	the	program	will	now	reference	a	pointer	to	data	of
type	
.	The	casts	shown	here	do	not	change	the
actual	pointer;	they	simply	direct	the	compiler	to	refer	to	the	data
being	pointed	to	according	to	the	new	data	type.</p>
<p>Aside	
Generating	an	ASCII	table
You	can	display	a	table	showing	the	ASCII	character	code	by
executing	the	command	
Practice	Problem	
2.5
(solution	page	
144
)
Consider	the	following	three	calls	to	
:
Indicate	the	values	that	will	be	printed	by	each	call	on	a	little-
endian	machine	and	on	a	big-endian	machine:
A
.	
Little	endian:</p>
<p>Big	endian:</p>
<p>B
.	
Little	endian:</p>
<p>Big	endian:</p>
<p>C
.	
Little	endian:</p>
<p>Big	endian:</p>
<p>Practice	Problem	
2.6
(solution	page	
145
)
Using	
and	
,	we	determine	that	the	integer
3510593	has	hexadecimal	representation	
,	while	the
floating-point	number	3510593.0	has	hexadecimal	representation</p>
<p>A
.	
Write	the	binary	representations	of	these	two	hexadecimal
values.
B
.	
Shift	these	two	strings	relative	to	one	another	to	maximize
the	number	of	matching	bits.	How	many	bits	match?
C
.	
What	parts	of	the	strings	do	not	match?
2.1.4	
Representing	Strings
A	string	in	C	is	encoded	by	an	array	of	characters	terminated	by	the	null
(having	value	0)	character.	Each	character	is	represented	by	some
standard	encoding,	with	the	most	common	being	the	ASCII	character
code.	Thus,	if	we	run	our	routine	
with	arguments	
and
(to	include	the	terminating	character),	we	get	the	result	
.	Observe	that	the	ASCII	code	for	decimal	digit	
x
happens	to	be	
,
and	that	the	terminating	byte	has	the	hex	representation	
.	This	same
result	would	be	obtained	on	any	system	using	ASCII	as	its	character
code,	independent	of	the	byte	ordering	and	word	size	conventions.	As	a
consequence,	text	data	are	more	platform	independent	than	binary	data.
Practice	Problem	
2.7
(solution	page	
145
)
What	would	be	printed	as	a	result	of	the	following	call	to</p>
<p>Note	that	letters	
through	
have	ASCII	codes	
through
.
2.1.5	
Representing	Code
Consider	the	following	C	function:
When	compiled	on	our	sample	machines,	we	generate	machine	code
having	the	following	byte	representations:
Linux	32
Windows
Sun
Linux	64
Aside	
The	Unicode	standard	for	text
encoding</p>
<p>The	ASCII	character	set	is	suitable	for	encoding	English-language
documents,	but	it	does	not	have	much	in	the	way	of	special
characters,	such	as	the	French	‘ç'.	It	is	wholly	unsuited	for
encoding	documents	in	languages	such	as	Greek,	Russian,	and
Chinese.	Over	the	years,	a	variety	of	methods	have	been
developed	to	encode	text	for	different	languages.	The	Unicode
Consortium	has	devised	the	most	comprehensive	and	widely
accepted	standard	for	encoding	text.	The	current	Unicode
standard	(version	7.0)	has	a	repertoire	of	over	100,000	characters
supporting	a	wide	range	of	languages,	including	the	ancient
languages	of	Egypt	and	Babylon.	To	their	credit,	the	Unicode
Technical	Committee	rejected	a	proposal	to	include	a	standard
writing	for	Klingon,	a	fictional	civilization	from	the	television	series
Star	Trek.
The	base	encoding,	known	as	the	“Universal	Character	Set”	of
Unicode,	uses	a	32-bit	representation	of	characters.	This	would
seem	to	require	every	string	of	text	to	consist	of	4	bytes	per
character.	However,	alternative	codings	are	possible	where
common	characters	require	just	1	or	2	bytes,	while	less	common
ones	require	more.	In	particular,	the	UTF-8	representation
encodes	each	character	as	a	sequence	of	bytes,	such	that	the
standard	ASCII	characters	use	the	same	single-byte	encodings	as
they	have	in	ASCII,	implying	that	all	ASCII	byte	sequences	have
the	same	meaning	in	UTF-8	as	they	do	in	ASCII.
The	Java	programming	language	uses	Unicode	in	its
representations	of	strings.	Program	libraries	are	also	available	for
C	to	support	Unicode.
Here	we	find	that	the	instruction	codings	are	different.	Different	machine
types	use	different	and	incompatible	instructions	and	encodings.	Even</p>
<p>identical	processors	running	different	operating	systems	have	differences
in	their	coding	conventions	and	hence	are	not	binary	compatible.	Binary
code	is	seldom	portable	across	different	combinations	of	machine	and
operating	system.
A	fundamental	concept	of	computer	systems	is	that	a	program,	from	the
perspective	of	the	machine,	is	simply	a	sequence	of	bytes.	The	machine
has	no	information	about	the	original	source	program,	except	perhaps
some	auxiliary	tables	maintained	to	aid	in	debugging.	We	will	see	this
more	clearly	when	we	study	machine-level	programming	in	
Chapter	
3
.
2.1.6	
Introduction	to	Boolean
Algebra
Since	binary	values	are	at	the	core	of	how	computers	encode,	store,	and
manipulate	information,	a	rich	body	of	mathematical	knowledge	has
evolved	around	the	study	of	the	values	0	and	1.	This	started	with	the
work	of	George	Boole	(1815–1864)	around	1850	and	thus	is	known	as
Boolean	algebra
.	Boole	observed	that	by	encoding	logic	values	
TRUE</p>
<p>and
FALSE</p>
<p>as	binary	values	1	and	0,	he	could	formulate	an	algebra	that
captures	the	basic	principles	of	logical	reasoning.
The	simplest	Boolean	algebra	is	defined	over	the	two-element	set	{0,	1}.
Figure	
2.7
defines	several	operations	in	this	algebra.	Our	symbols	for
representing	these	operations	are	chosen	to	match	those	used	by	the	C
bit-level	operations,</p>
<p>Figure	
2.7	
Operations	of	Boolean	algebra.
Binary	values	1	and	0	encode	logic	values	
TRUE</p>
<h2>and	
FALSE
,	while
operations	~,	&amp;,	|,	and	^	encode	logical	operations	
NOT
,	
AND
,	
OR
,	and
EXCLUSIVE</h2>
<p>OR
,	respectively.
as	will	be	discussed	later.	The	Boolean	operation	~	corresponds	to	the
logical	operation	
NOT
,	denoted	by	the	symbol	¬.	That	is,	we	say	that	¬
P
is
true	when	
P
is	not	true,	and	vice	versa.	Correspondingly,	~
p
equals	1
when	
p
equals	0,	and	vice	versa.	Boolean	operation	
corresponds	to
the	logical	operation	
AND
,	denoted	by	the	symbol	
∧
.	We	say	that	
P</p>
<p>∧</p>
<p>Q
holds	when	both	
P
is	true	and	
Q
is	true.	Correspondingly,	
p</p>
<p>q
equals	1
only	when	
p
=	1	and	
q
=	1.	Boolean	operation	|	corresponds	to	the	logical
operation	
OR
,	denoted	by	the	symbol	
∨
.	We	say	that	
P</p>
<p>∨</p>
<h2>Q
holds	when
either	
P
is	true	or	
Q
is	true.	Correspondingly,	
p
|	
q
equals	1	when	either	
p
=	1	or	
q
=	1.	Boolean	operation	^	corresponds	to	the	logical	operation
EXCLUSIVE</h2>
<p>OR
,	denoted	by	the	symbol	
⊕
.	We	say	that	
P</p>
<p>⊕</p>
<p>Q
holds	when
either	
P
is	true	or	
Q
is	true,	but	not	both.	Correspondingly,	
p
^	
q
equals	1
when	either	
p
=	1	and	
q
=	0,	or	
p
=	0	and	
q
=	1.
Claude	Shannon	(1916–2001),	who	later	founded	the	field	of	information
theory,	first	made	the	connection	between	Boolean	algebra	and	digital
logic.	In	his	1937	master's	thesis,	he	showed	that	Boolean	algebra	could
be	applied	to	the	design	and	analysis	of	networks	of	electromechanical
relays.	Although	computer	technology	has	advanced	considerably	since,
Boolean	algebra	still	plays	a	central	role	in	the	design	and	analysis	of
digital	systems.</p>
<p>We	can	extend	the	four	Boolean	operations	to	also	operate	on	
bit
vectors
,	strings	of	zeros	and	ones	of	some	fixed	length	
w
.	We	define	the
operations	over	bit	vectors	according	to	their	applications	to	the	matching
elements	of	the	arguments.	Let	
a
and	
b
denote	the	bit	vectors	
and	
,	respectively.	We	define	
a
&amp;	
b
to	also
be	a	bit	vector	of	length	
w
,	where	the	
i
th	element	equals	
a
&amp;	
b
,	for	0	≤	
i
&lt;
w.
The	operations	|,	^,	and	~	are	extended	to	bit	vectors	in	a	similar
fashion.
As	examples,	consider	the	case	where	
w
=	4,	and	with	arguments	
a
=
[0110]	and	
b
=	[1100].	Then	the	four	operations	
a	&amp;	b,	a
|	
b,	a
^	
b
,	and	~
b
yield
Practice	Problem	
2.8
(solution	page	
145
)
Fill	in	the	following	table	showing	the	results	of	evaluating	Boolean
operations	on	bit	vectors.
Web	Aside	DATA:BOOL	
More	on
Boolean	algebra	and	Boolean	rings
The	Boolean	operations	
operating	on	bit
vectors	of	length	
w
form	a	
Boolean	algebra
,	for	any	integer
w
&gt;	0.	The	simplest	is	the	case	where	
w
=	1	and	there	are
just	two	elements,	but	for	the	more	general	case	there	are
2
bit	vectors	of	length	
w.
Boolean	algebra	has	many	of	the
same	properties	as	arithmetic	over	integers.	For	example,
[
a
w
−
1
,
 
a
w
−
2
,
…
,
a
0
]</p>
<p>[
b
w
−
1
,
 
b
w
−
2
,
…
,
b
0
]
i
i
0110
&amp;
1100
0100
¯
0110
|
1100
1110
¯
0110
^
1100
1010
¯
~
1100
0011
¯
w</p>
<p>just	as	multiplication	distributes	over	addition,	written	
a
·	(
b</p>
<ul>
<li></li>
</ul>
<p>c
)	=	(
a
·	
b
)	+	(
a
·	
c
),	Boolean	operation	&amp;	distributes	over
|,	written	
a
&amp;	(
b
|	
c
)	=	(
a
&amp;	
b
)	|	(
a
&amp;	
c
).	In	addition,	however.
Boolean	operation	|	distributes	over	&amp;,	and	so	we	can	write
a
|	(
b
&amp;	
c
)	=	(
a
|	
b
)	&amp;	(
a
|	
c
),	whereas	we	cannot	say	that	
a</p>
<ul>
<li>(
b
·	
c
)	=	(
a
<ul>
<li></li>
</ul>
</li>
</ul>
<p>b
)	·	(
a
+	
c
)	holds	for	all	integers.
When	we	consider	operations	^,	&amp;,	and	~	operating	on	bit
vectors	of	length	
w
,	we	get	a	different	mathematical	form,
known	as	a	
Boolean	ring.
Boolean	rings	have	many
properties	in	common	with	integer	arithmetic.	For	example,
one	property	of	integer	arithmetic	is	that	every	value	
x
has
an	
additive	inverse	–x
,	such	that	
x
+	–
x
=	0.	A	similar
property	holds	for	Boolean	rings,	where	^	is	the	“addition”
operation,	but	in	this	case	each	element	is	its	own	additive
inverse.	That	is,	
a
^	
a
=	0	for	any	value	
a
,	where	we	use	0
here	to	represent	a	bit	vector	of	all	zeros.	We	can	see	this
holds	for	single	bits,	since	0	^	0	=	1	^	1	=	0,	and	it	extends
to	bit	vectors	as	well.	This	property	holds	even	when	we
rearrange	terms	and	combine	them	in	a	different	order,	and
so	(
a
^	
b
)	^	
a
=	
b.
This	property	leads	to	some	interesting
results	and	clever	tricks,	as	we	will	explore	in	
Problem
2.10
.
Operation
Result
a
[01101001]
b
[01010101]
~
a</p>
<hr />
<p>~
b</p>
<hr />
<p>a
&amp;	
b</p>
<hr />
<p>a
|	
b</p>
<hr />
<p>a
^	
b</p>
<hr />
<p>One	useful	application	of	bit	vectors	is	to	represent	finite	sets.	We	can
encode	any	subset	
with	a	bit	vector	
,
where	
a
=	1	if	and	only	if	
i</p>
<p>∊</p>
<p>A.
For	example,	recalling	that	we	write	
a
on	the	left	and	
a
on	the	right,	bit	vector	
a
=	[01101001]	encodes	the	set
A
=	{0,	3,	5,	6},	while	bit	vector	
b
=	[01010101]	encodes	the	set	
B
=	{0,	2,
4,	6}.	With	this	way	of	encoding	sets,	Boolean	operations	|	and	&amp;
correspond	to	set	union	and	intersection,	respectively,	and	~	corresponds
to	set	complement.	Continuing	our	earlier	example,	the	operation	
a
&amp;	
b
yields	bit	vector	[01000001],	while	
A
∩	
B
=	{0,	6}.
We	will	see	the	encoding	of	sets	by	bit	vectors	in	a	number	of	practical
applications.	For	example,	in	
Chapter	
8
,	we	will	see	that	there	are	a
number	of	different	
signals
that	can	interrupt	the	execution	of	a	program.
We	can	selectively	enable	or	disable	different	signals	by	specifying	a	bit-
vector	mask,	where	a	1	in	bit	position	
i
indicates	that	signal	
i
is	enabled
and	a	0	indicates	that	it	is	disabled.	Thus,	the	mask	represents	the	set	of
enabled	signals.
Practice	Problem	
2.9
(solution	page	
146
)
Computers	generate	color	pictures	on	a	video	screen	or	liquid
crystal	display	by	mixing	three	different	colors	of	light:	red,	green,
and	blue.	Imagine	a	simple	scheme,	with	three	different	lights,
A
⊆
{
0
,
1
,
…
,
w
−
1
}</p>
<p>[
a
w
−
1
,
…
,
a
1
,
a
0
]
i
w
–1
0</p>
<p>each	of	which	can	be	turned	on	or	off,	projecting	onto	a	glass
screen:
We	can	then	create	eight	different	colors	based	on	the	absence
(0)	or	presence	(1)	of	light	sources	
R
,	
G
,	and	
B
:
R
G
B
Color
0
0
0
Black
0
0
1
Blue
0
1
0
Green
0
1
1
Cyan
1
0
0
Red
1
0
1
Magenta
1
1
0
Yellow
1
1
1
White</p>
<h2>Each	of	these	colors	can	be	represented	as	a	bit	vector	of	length
3,	and	we	can	apply	Boolean	operations	to	them.
A
.	
The	complement	of	a	color	is	formed	by	turning	off	the
lights	that	are	on	and	turning	on	the	lights	that	are	off.	What
would	be	the	complement	of	each	of	the	eight	colors	listed
above?
B
.	
Describe	the	effect	of	applying	Boolean	operations	on	the
following	colors:
Blue	|	Green	=__________
Yellow	&amp;	Cyan	=__________
Red	^	Magenta	=__________
2.1.7	
Bit-Level	Operations	in	C
One	useful	feature	of	C	is	that	it	supports	bitwise	Boolean	operations.	In
fact,	the	symbols	we	have	used	for	the	Boolean	operations	are	exactly
those	used	by	C:	|	for	
OR
,	&amp;	for	
AND
,	~	for	
NOT
,	and	^	for	
EXCLUSIVE</h2>
<p>OR
.
These	can	be	applied	to	any	“integral”	data	type,	including	all	of	those
listed	in	
Figure	
2.3
.	Here	are	some	examples	of	expression	evaluation
for	data	type	char:
C	expression
Binary	expression
Binary	result
Hexadecimal	result
~[0100	0001]
[1011	1110]
~[0000	0000]
[1111	1111]
[0110	1001]	&amp;	[0101	0101]
[0100	0001]</p>
<p>[0110	1001]	|	[01010101]
[0111	1101]
As	our	examples	show,	the	best	way	to	determine	the	effect	of	a	bit-level
expression	is	to	expand	the	hexadecimal	arguments	to	their	binary
representations,	perform	the	operations	in	binary,	and	then	convert	back
to	hexadecimal.
Practice	Problem	
2.10
(solution	page	
146
)
As	an	application	of	the	property	that	
a
^	
a
=	0	for	any	bit	vector	
a
,
consider	the	following	program:
As	the	name	implies,	we	claim	that	the	effect	of	this	procedure	is
to	swap	the	values	stored	at	the	locations	denoted	by	pointer
variables	
and	
.	Note	that	unlike	the	usual	technique	for
swapping	two	values,	we	do	not	need	a	third	location	to
temporarily	store	one	value	while	we	are	moving	the	other.	There
is	no	performance	advantage	to	this	way	of	swapping;	it	is	merely
an	intellectual	amusement.
Starting	with	values	
a
and	
b
in	the	locations	pointed	to	by	
and
,	respectively,	fill	in	the	table	that	follows,	giving	the	values</p>
<p>stored	at	the	two	locations	after	each	step	of	the	procedure.	Use
the	properties	of	^	to	show	that	the	desired	effect	is	achieved.
Recall	that	every	element	is	its	own	additive	inverse	(that	is,	
a
^	
a
=	0).
Step
*x
*y
Initially
a
b
Step	1</p>
<hr />
<hr />
<p>Step	2</p>
<hr />
<hr />
<p>Step	3</p>
<hr />
<hr />
<p>Practice	Problem	
2.11
(solution	page	
146
)
Armed	with	the	function	
from	
Problem	
2.10
,	you
decide	to	write	code	that	will	reverse	the	elements	of	an	array	by
swapping	elements	from	opposite	ends	of	the	array,	working
toward	the	middle.
You	arrive	at	the	following	function:</p>
<p>When	you	apply	your	function	to	an	array	containing	elements	1,
2,	3,	and	4,	you	find	the	array	now	has,	as	expected,	elements	4,
3,	2,	and	1.	When	you	try	it	on	an	array	with	elements	1,	2,	3,	4,
and	5,	however,	you	are	surprised	to	see	that	the	array	now	has
elements	5,	4,	0,	2,	and	1.	In	fact,	you	discover	that	the	code
always	works	correctly	on	arrays	of	even	length,	but	it	sets	the
middle	element	to	0	whenever	the	array	has	odd	length.
A
.	
For	an	array	of	odd	length	
,	what	are	the
values	of	variables	first	and	last	in	the	final	iteration	of
function	
B
.	
Why	does	this	call	to	function	
set	the	array
element	to	0?
C
.	
What	simple	modification	to	the	code	for	
would	eliminate	this	problem?
One	common	use	of	bit-level	operations	is	to	implement	
masking
operations,	where	a	mask	is	a	bit	pattern	that	indicates	a	selected	set	of
bits	within	a	word.	As	an	example,	the	mask	
(having	ones	for	the
least	significant	8	bits)	indicates	the	low-order	byte	of	a	word.	The	bit-
level	operation	
yields	a	value	consisting	of	the	least	significant
byte	of	
,	but	with	all	other	bytes	set	to	0.	For	example,	with	
,	the	expression	would	yield	
.	The	expression	
will	yield	a	mask	of	all	ones,	regardless	of	the	size	of	the	data
representation.	The	same	mask	can	be	written	
when	data
type	
is	32	bits,	but	it	would	not	be	as	portable.
Practice	Problem	
2.12
(solution	page	
146
)</p>
<p>Write	C	expressions,	in	terms	of	variable	
,	for	the	following
values.	Your	code	should	work	for	any	word	size	
w
≥	8.	For
reference,	we	show	the	result	of	evaluating	the	expressions	for	
,	with	
w
=	32.
A
.	
The	least	significant	byte	of	
,	with	all	other	bits	set	to	0.
[
]
B
.	
All	but	the	least	significant	byte	of	
complemented,	with
the	least	significant	byte	left	unchanged.	
C
.	
The	least	significant	byte	set	to	all	ones,	and	all	other	bytes
of	
left	unchanged.	
Practice	Problem	
2.13
(solution	page	
147
)
The	Digital	Equipment	VAX	computer	was	a	very	popular	machine
from	the	late	1970s	until	the	late	1980s.	Rather	than	instructions
for	Boolean	operations	
AND</p>
<p>and	
OR
,	it	had	instructions	
(bit	set)
and	
(bit	clear).	Both	instructions	take	a	data	word	
and	a
mask	word	
.	They	generate	a	result	
consisting	of	the	bits	of	
modified	according	to	the	bits	of	
With	
,	the	modification
involves	setting	
to	1	at	each	bit	position	where	
is	1.	With	
,
the	modification	involves	setting	
to	0	at	each	bit	position	where
is	1.
To	see	how	these	operations	relate	to	the	C	bit-level	operations,
assume	we	have	functions	
and	
implementing	the	bit	set
and	bit	clear	operations,	and	that	we	want	to	use	these	to
implement	functions	computing	bitwise	operations	|	and	^,	without
using	any	other	C	operations.	Fill	in	the	missing	code	below.	
Hint:
Write	C	expressions	for	the	operations	
and	
.</p>
<p>2.1.8	
Logical	Operations	in	C
C	also	provides	a	set	of	
logical
operators	|	|,	&amp;&amp;,	and	!,	which	correspond
to	the	
OR
,	
AND
,	and	
NOT</p>
<p>operations	of	logic.	These	can	easily	be	confused
with	the	bit-level	operations,	but	their	behavior	is	quite	different.	The
logical	operations	treat	any	nonzero	argument	as	representing	
TRUE</p>
<p>and
argument	0	as	representing	
FALSE
.	They	return	either	1	or	0,	indicating	a</p>
<p>result	of	either	
TRUE</p>
<p>OR</p>
<p>FALSE
,	respectively.	Here	are	some	examples	of
expression	evaluation:
Expression
Result
Observe	that	a	bitwise	operation	will	have	behavior	matching	that	of	its
logical	counterpart	only	in	the	special	case	in	which	the	arguments	are
restricted	to	0	or	1.
A	second	important	distinction	between	the	logical	operators	‘
’	and	‘
’	versus	their	bit-level	counterparts	‘
’	and	‘
’	is	that	the	logical
operators	do	not	evaluate	their	second	argument	if	the	result	of	the
expression	can	be	determined	by	evaluating	the	first	argument.	Thus,	for
example,	the	expression	a	
will	never	cause	a	division	by	zero,
and	the	expression	
will	never	cause	the	dereferencing	of	a	null
pointer.
Practice	Problem	
2.14
(solution	page	
147
)</p>
<p>Suppose	that	
and	
have	byte	values	
and	
,
respectively.	Fill	in	the	following	table	indicating	the	byte	values	of
the	different	C	expressions:
Expression
Value
Expression
Value</p>
<hr />
<hr />
<hr />
<hr />
<hr />
<hr />
<hr />
<hr />
<p>Practice	Problem	
2.15
(solution	page	
148
)
Using	only	bit-level	and	logical	operations,	write	a	C	expression
that	is	equivalent	to	
.	In	other	words,	it	will	return	1	when	
and	
are	equal	and	0	otherwise.
2.1.9	
Shift	Operations	in	C
C	also	provides	a	set	of	
shift
operations	for	shifting	bit	patterns	to	the	left
and	to	the	right.	For	an	operand	
having	bit	representation	
,	the	C	expression	
yields	a	value	with	bit	representation	
.	That	is,	
is	shifted	
k
bits	to	the	left,	dropping
off	the	
k
most	significant	bits	and	filling	the	right	end	with	
k
zeros.	The
shift	amount	should	be	a	value	between	0	and	
w
–	1.	Shift	operations
associate	from	left	to	right,	so	
is	equivalent	to	
[
x
w
−
1
,
x
w
−
2
,
…
,
x
0
]
[
x
w
−
k
−
1
,
x
w
−
k
−
2
,
…
,
x
0
,
0
,
…
,
0
]</p>
<p>There	is	a	corresponding	right	shift	operation,	written	in	C	as	
,	but
it	has	a	slightly	subtle	behavior.	Generally,	machines	support	two	forms	of
right	shift:
Logical	
.	A	logical	right	shift	fills	the	left	end	with	
k
zeros,	giving	a
result	
.
Arithmetic
.	An	arithmetic	right	shift	fills	the	left	end	with	
k
repetitions
of	the	most	significant	bit,	giving	a	result	
.	This	convention	might	seem	peculiar,	but	as	we	will	see,	it	is
useful	for	operating	on	signed	integer	data.
As	examples,	the	following	table	shows	the	effect	of	applying	the	different
shift	operations	to	two	different	values	of	an	8-bit	argument	
x
:
Operation
Value	1
Value	2
Argument	
[01100011]
[10010101]
[0011
0000
]
[0101
0000
]
[
0000
0110]
[
0000
1001]
(arithmetic)
[
0000
0110]
[
1111
1001]
The	italicized	digits	indicate	the	values	that	fill	the	right	(left	shift)	or	left
(right	shift)	ends.	Observe	that	all	but	one	entry	involves	filling	with	zeros.
The	exception	is	the	case	of	shifting	[10010101]	right	arithmetically.	Since
its	most	significant	bit	is	1,	this	will	be	used	as	the	fill	value.
[
0
,
…
,
0
,
x
w
−
1
,
x
w
−
2
,
…
x
k
]
[
x
w
−
1
,
…
,
x
w
−
1
,
x
w
−
1
,
x
w
−
2
,
…
x
k
]</p>
<p>The	C	standards	do	not	precisely	define	which	type	of	right	shift	should
be	used	with	signed	numbers—either	arithmetic	or	logical	shifts	may	be
used.	This	unfortunately	means	that	any	code	assuming	one	form	or	the
other	will	potentially	encounter	portability	problems.	In	practice,	however,
almost	all	compiler/machine	combinations	use	arithmetic	right	shifts	for
signed	data,	and	many	programmers	assume	this	to	be	the	case.	For
unsigned	data,	on	the	other	hand,	right	shifts	must	be	logical.
In	contrast	to	C,	Java	has	a	precise	definition	of	how	right	shifts	should
be	performed.	The	expression	
shifts	
arithmetically	by	
positions,	while	
shifts	it	logically.
Practice	Problem	
2.16
(solution	page	
148
)
Fill	in	the	table	below	showing	the	effects	of	the	different	shift
operations	on	single-byte	quantities.	The	best	way	to	think	about
shift	operations	is	to	work	with	binary	representations.	Convert	the
initial	values	to	binary,	perform	the	shifts,	and	then	convert	back	to
hexadecimal.	Each	of	the	answers	should	be	8	binary	digits	or	2
hexadecimal	digits.
Logical	
Hex
Binary
Binary
Hex
Binary
Hex</p>
<hr />
<hr />
<hr />
<hr />
<hr />
<hr />
<hr />
<hr />
<hr />
<hr />
<hr />
<hr />
<hr />
<hr />
<hr />
<hr />
<hr />
<hr />
<hr />
<hr />
<p>Aside	
Shifting	by	
k
,	for	large	values	of	
k
For	a	data	type	consisting	of	
w
bits,	what	should	be	the	effect	of
shifting	by	some	value	
k
≥	
w
?	For	example,	what	should	be	the
effect	of	computing	the	following	expressions,	assuming	data	type
has	
w
=	32:
The	C	standards	carefully	avoid	stating	what	should	be	done	in
such	a	case.	On	many	machines,	the	shift	instructions	consider
only	the	lower	log</p>
<p>w
bits	of	the	shift	amount	when	shifting	a	
w
-bit
value,	and	so	the	shift	amount	is	computed	as	
k
mod	
w
.	For
example,	with	
w
=	32,	the	above	three	shifts	would	be	computed
as	if	they	were	by	amounts	0,	4,	and	8,	respectively,	giving	results
This	behavior	is	not	guaranteed	for	C	programs,	however,	and	so
shift	amounts	should	be	kept	less	than	the	word	size.
2</p>
<p>Java,	on	the	other	hand,	specifically	requires	that	shift	amounts
should	be	computed	in	the	modular	fashion	we	have	shown.
Aside	
Operator	precedence	issues	with
shift	operations
It	might	be	tempting	to	write	the	expression	
,	intending
it	to	mean	
.	However,	in	C	the	former	expression
is	equivalent	to	
,	since	addition	(and	subtraction)
have	higher	precedence	than	shifts.	The	left-to-right	associativity
rule	then	causes	this	to	be	parenthesized	as	
,
giving	value	512,	rather	than	the	intended	52.
Getting	the	precedence	wrong	in	C	expressions	is	a	common
source	of	program	errors,	and	often	these	are	difficult	to	spot	by
inspection.	When	in	doubt,	put	in	parentheses!</p>
<p>2.2	
Integer	Representations
In	this	section,	we	describe	two	different	ways	bits	can	be	used	to	encode
integers—one	that	can	only	represent	nonnegative	numbers,	and	one
that	can	represent	negative,	zero,	and	positive	numbers.	We	will	see	later
that	they	are	strongly	related	both	in	their	mathematical	properties	and
their	machine-level	implementations.	We	also	investigate	the	effect	of
expanding	or	shrinking	an	encoded	integer	to	fit	a	representation	with	a
different	length.
Figure	
2.8
lists	the	mathematical	terminology	we	introduce	to	precisely
define	and	characterize	how	computers	encode	and	operate	on	integer
data.	This
Symbol
Type
Meaning
Page
B2T
Function
Binary	to	two's	complement
64
B2U
Function
Binary	to	unsigned
62
U2B
Function
Unsigned	to	binary
64
U2T
Function
Unsigned	to	two's	complement
71
T2B
Function
Two's	complement	to	binary
65
T2U
Function
Two's	complement	to	unsigned
71
TMin
Constant
Minimum	two's-complement	value
65
w
w
w
w
w
w
w</p>
<p>TMax
Constant
Maximum	two's-complement	value
65
UMax
Constant
Maximum	unsigned	value
63
Operation
Two's-complement	addition
90
Operation
Unsigned	addition
85
Operation
Two's-complement	multiplication
97
Operation
Unsigned	multiplication
96
Operation
Two's-complement	negation
95
Operation
Unsigned	negation
89
Figure	
2.8	
Terminology	for	integer	data	and	arithmetic	operations.
The	subscript	
w
denotes	the	number	of	bits	in	the	data	representation.
The	“Page”	column	indicates	the	page	on	which	the	term	is	defined.
terminology	will	be	introduced	over	the	course	of	the	presentation.	The
figure	is	included	here	as	a	reference.
2.2.1	
Integral	Data	Types
C	supports	a	variety	of	
integral
data	types—ones	that	represent	finite
ranges	of	integers.	These	are	shown	in	
Figures	
2.9
and	
2.10
,	along
with	the	ranges	of	values	they	can	have	for	“typical”	32-	and	64-bit
programs.	Each	type	can	specify	a	size	with	keyword	char,	short,	long,	as
well	as	an	indication	of	whether	the	represented	numbers	are	all
nonnegative	(declared	as	
),	or	possibly	negative	(the	default.)	As
w
w</p>
<ul>
<li></li>
</ul>
<p>w
t</p>
<ul>
<li></li>
</ul>
<p>w
u</p>
<ul>
<li></li>
</ul>
<p>w
t</p>
<ul>
<li></li>
</ul>
<p>w
u
−
w
t
−
w
u</p>
<p>we	saw	in	
Figure	
2.3
,	the	number	of	bytes	allocated	for	the	different
sizes	varies	according	to	whether	the	program	is	compiled	for	32	or	64
bits.	Based	on	the	byte	allocations,	the	different	sizes	allow	different
ranges	of	values	to	be	represented.	The	only	machine-dependent	range
indicated	is	for	size	designator	
Most	64-bit	programs	use	an	8-byte
representation,	giving	a	much	wider	range	of	values	than	the	4-byte
representation	used	with	32-bit	programs.
One	important	feature	to	note	in	
Figures	
2.9
and	
2.10
is	that	the
ranges	are	not	symmetric—the	range	of	negative	numbers	extends	one
further	than	the	range	of	positive	numbers.	We	will	see	why	this	happens
when	we	consider	how	negative	numbers	are	represented.
C	data	type
Minimum
Maximum
–128
127
0
255
–32,768
32,767
0
65,535
–2,147,483,648
2,147,483,647
0
4,294,967,295
–2,147,483,648
2,147,483,647
0
4,294,967,295
–2,147,483,648
2,147,483,647</p>
<p>0
4,294,967,295
–9,223,372,036,854,775,808
9,223,372,036,854,775,807
0
18,446,744,073,709,551,615
Figure	
2.9	
Typical	ranges	for	C	integral	data	types	for	32-bit
programs.
C	data	type
Minimum
Maximum
−128
127
0
255
–32,768
32,767
0
65,535
–2,147,483,648
2,147,483,647
0
4,294,967,295
–9,223,372,036,854,775,808
9,223,372,036,854,775,807
0
18,446,744,073,709,551,615
–2,147,483,648
2,147,483,647
0
4,294,967,295
–9,223,372,036,854,775,808
9,223,372,036,854,775,807
0
18,446,744,073,709,551,615</p>
<p>Figure	
2.10	
Typical	ranges	for	C	integral	data	types	for	64-bit
programs.
The	C	standards	define	minimum	ranges	of	values	that	each	data	type
must	be	able	to	represent.	As	shown	in	
Figure	
2.11
,	their	ranges	are
the	same	or	smaller	than	the	typical	implementations	shown	in	
Figures
2.9
and	
2.10
.	In	particular,	with	the	exception	of	the	fixed-size	data
types,	we	see	that	they	require	only	a
New	to	C?	
Signed	and	unsigned
numbers	in	C,	C++,	and	Java
Both	C	and	C++	support	signed	(the	default)	and	unsigned
numbers.	Java	supports	only	signed	numbers.
C	data	type
Minimum
Maximum
–127
127
0
255
–32,767
32,767
0
65,535
–32,767
32,767
0
65,535
–2,147,483,647
2,147,483,647
0
4,294,967,295</p>
<p>–2,147,483,648
2,147,483,647
0
4,294,967,295
–9,223,372,036,854,775,808
9,223,372,036,854,775,807
0
18,446,744,073,709,551,615
Figure	
2.11	
Guaranteed	ranges	for	C	integral	data	types.
The	C	standards	require	that	the	data	types	have	at	least	these	ranges	of
values.
symmetric	range	of	positive	and	negative	numbers.	We	also	see	that
data	type	
could	be	implemented	with	2-byte	numbers,	although	this
is	mostly	a	throwback	to	the	days	of	16-bit	machines.	We	also	see	that
size	long	can	be	implemented	with	4-byte	numbers,	and	it	typically	is	for
32-bit	programs.	The	fixed-size	data	types	guarantee	that	the	ranges	of
values	will	be	exactly	those	given	by	the	typical	numbers	of	
Figure
2.9
,	including	the	asymmetry	between	negative	and	positive.
2.2.2	
Unsigned	Encodings
Let	us	consider	an	integer	data	type	of	
w
bits.	We	write	a	bit	vector	as
either	
,	to	denote	the	entire	vector,	or	as	
to	denote
the	individual	bits	within	the	vector.	Treating	
as	a	number	written	in
binary	notation,	we	obtain	the	
unsigned
interpretation	of	
.	In	this
encoding,	each	bit	
x
has	value	0	or	1,	with	the	latter	case	indicating	that
value	2
should	be	included	as	part	of	the	numeric	value.	We	can	express
this	interpretation	as	a	function	
B2U
(for	“binary	to	unsigned,”	length	
w
):
x
→
[
x
w
−
1
,
x
w
−
2
,
…
x
0
]</p>
<p>x
→</p>
<p>x
→
i
i
w</p>
<h1>Figure	
2.12	
Unsigned	number	examples	for
w
=	4.	When	bit	
i
in	the	binary	representation	has	value	1,	it	contributes	2
to	the	value.
Principle:
Definition	of	unsigned	encoding
For	vector	
In	this	equation,	the	notation	
≐
means	that	the	left-hand	side	is	defined	to
be	equal	to	the	right-hand	side.	The	function	
B2U
maps	strings	of	zeros
and	ones	of	length	
w
to	nonnegative	integers.	As	examples,	
Figure
i
x
→</h1>
<h1>[
x
w
−
1
,
x
w
−
2
,
…
,
x
0
]
 
:
B
2
U
w
(
x
→
)</h1>
<h1>˙
∑
i</h1>
<p>0
w
−
1
x
i
2
i
(2.1)
w</p>
<h1>2.12
shows	the	mapping,	given	by	
B2U
,	from	bit	vectors	to	integers	for
the	following	cases:
In	the	figure,	we	represent	each	bit	position	
i
by	a	rightward-pointing	blue
bar	of	length	2
.
The	numeric	value	associated	with	a	bit	vector	then
equals	the	sum	of	the	lengths	of	the	bars	for	which	the	corresponding	bit
values	are	1.
Let	us	consider	the	range	of	values	that	can	be	represented	using	
w
bits.
The	least	value	is	given	by	bit	vector	[00	...	0]	having	integer	value	0,	and
the	greatest	value	is	given	by	bit	vector	[11	...	1]	having	integer	value
.	Using	the	4-bit	case	as	an	example,	we	have
.	Thus,	the	function	
B2U
can	be	defined
as	a	mapping	
.
The	unsigned	binary	representation	has	the	important	property	that	every
number	between	0	and	2
—	1	has	a	unique	encoding	as	a	
w
-bit	value.
For	example,	
there	is	only	one	representation	of	decimal	value	11	as	an
unsigned	4–bit	number—namely,	[1011].	We	highlight	this	as	a
mathematical	principle,	which	we	first	state	and	then	explain.
Principle:
Uniqueness	of	unsigned	encoding
B
2
U
4
(
[
0001
]
)</h1>
<p>0
⋅
2
3</p>
<ul>
<li></li>
</ul>
<p>0
⋅
2
2</p>
<ul>
<li></li>
</ul>
<p>0
⋅
2
1</p>
<ul>
<li></li>
</ul>
<h1>1
⋅
2
0</h1>
<p>0</p>
<ul>
<li></li>
</ul>
<p>0</p>
<ul>
<li></li>
</ul>
<p>0</p>
<ul>
<li></li>
</ul>
<h1 id="1"><a class="header" href="#1">1</a></h1>
<h1>1
B
2
U
4
(
[
0101
]
)</h1>
<p>0
⋅
2
3</p>
<ul>
<li></li>
</ul>
<h1>1
⋅
2
2
(2.2)
i
U
M
a
x
w
≐
Σ
i</h1>
<h1>0
w
−
1
2
i</h1>
<h1>2
w
−
1
U
M
a
x
4</h1>
<h1>B
2
U
4
(
[
1111
]
)</h1>
<h1>2
4
−
1</h1>
<p>15
w
B
2
U
w
:
{
0
,
1
}
w
→
{
0
,
…
,
U
M
a
x
w
}
w</p>
<p>Function	
B2U
is	a	bijection.
The	mathematical	term	
bijection
refers	to	a	function	
f
that	goes	two	ways:
it	maps	a	value	
x
to	a	value	
y
where	
y
=	
f(x)
,	but	it	can	also	operate	in
reverse,	since	for	every	
y
,	there	is	a	unique	value	
x
such	that	
f(x)
=	
y.
This	is	given	by	the	
inverse
function	
f
,	where,	for	our	example,	
x
=	
f
(
y
).
The	function	
B2U
maps	each	bit	vector	of	length	
w
to	a	unique	number
between	0	and	2
–	1,	and	it	has	an	inverse,	which	we	call	
U2B
(for
“unsigned	to	binary”),	that	maps	each	number	in	the	range	0	to	2
–	1	to
a	unique	pattern	of	
w
bits.
2.2.3	
Two's-Complement	Encodings
For	many	applications,	we	wish	to	represent	negative	values	as	well.	The
most	common	computer	representation	of	signed	numbers	is	known	as
two's-complement
form.	This	is	defined	by	interpreting	the	most
significant	bit	of	the	word	to	have	negative	weight.	We	express	this
interpretation	as	a	function	
B2T
(for	“binary	to	two's	complement”	length
w
):
Principle:
Definition	of	two's-complement	encoding
w
−1
−1
w
w
w
w
w</p>
<h1>For	vector	
:
The	most	significant	bit	
x
is	also	called	the	
sign	bit.
Its	“weight”	is	–2
,
the	negation	of	its	weight	in	an	unsigned	representation.	When	the	sign
bit	is	set	to	1,	the	represented	value	is	negative,	and	when	set	to	0,	the
value	is	nonnegative.	As	examples,	
Figure	
2.13
shows	the	mapping,
given	by	
B2T
,	from	bit	vectors	to	integers	for	the	following	cases:
In	the	figure,	we	indicate	that	the	sign	bit	has	negative	weight	by	showing
it	as	a	leftward-pointing	gray	bar.	The	numeric	value	associated	with	a	bit
vector	is	then	given	by	the	combination	of	the	possible	leftward-pointing
gray	bar	and	the	rightward-pointing	blue	bars.
x
→</h1>
<h1>[
x
w
−
1
,
x
w
−
2
,
…
,
x
0
]
B
2
T
w
(
x
→
)</h1>
<p>˙
−
x
w
−
1
2
w
−
1</p>
<ul>
<li></li>
</ul>
<h1>∑
i</h1>
<h1>0
w
−
2
x
i
2
i
(2.3)
w
–1
w
–1
B
2
T
4
(
[
0001
]
)</h1>
<p>−
0
⋅
2
3</p>
<ul>
<li></li>
</ul>
<p>0
⋅
2
2</p>
<ul>
<li></li>
</ul>
<p>0
⋅
2
1</p>
<ul>
<li></li>
</ul>
<h1>1
⋅
2
0</h1>
<p>0</p>
<ul>
<li></li>
</ul>
<p>0</p>
<ul>
<li></li>
</ul>
<p>0</p>
<ul>
<li></li>
</ul>
<h1 id="1-1"><a class="header" href="#1-1">1</a></h1>
<h1>1
B
2
T
4
(
[
0101
]
)</h1>
<p>−
0
⋅
2
3</p>
<ul>
<li></li>
</ul>
<p>1
⋅
2
2</p>
<ul>
<li></li>
</ul>
<p>0
⋅
2
1</p>
<ul>
<li></li>
</ul>
<h1>1
⋅
2
0</h1>
<p>0</p>
<ul>
<li></li>
</ul>
<p>4</p>
<ul>
<li></li>
</ul>
<p>0</p>
<ul>
<li></li>
</ul>
<h1 id="1-2"><a class="header" href="#1-2">1</a></h1>
<h1>5
B
2
T
4
(
[
1011
]
)</h1>
<p>−
1
⋅
2
3</p>
<ul>
<li></li>
</ul>
<p>0
⋅
2
2</p>
<ul>
<li></li>
</ul>
<p>1
⋅
2
1</p>
<ul>
<li></li>
</ul>
<h1>1
⋅
2
0</h1>
<p>−
8</p>
<ul>
<li></li>
</ul>
<p>0</p>
<ul>
<li></li>
</ul>
<p>2</p>
<ul>
<li></li>
</ul>
<h1 id="1-3"><a class="header" href="#1-3">1</a></h1>
<h1>−
5
B
2
T
4
(
[
1111
]
)</h1>
<p>−
1
⋅
2
3</p>
<ul>
<li></li>
</ul>
<p>1
⋅
2
2</p>
<ul>
<li></li>
</ul>
<p>1
⋅
2
1</p>
<ul>
<li></li>
</ul>
<h1>1
⋅
2
0</h1>
<p>−
8</p>
<ul>
<li></li>
</ul>
<p>4</p>
<ul>
<li></li>
</ul>
<p>2</p>
<ul>
<li></li>
</ul>
<h1 id="1-4"><a class="header" href="#1-4">1</a></h1>
<p>−
1
(2.4)</p>
<h1>Figure	
2.13	
Two's-complement	number	examples	for
w
=	4.	Bit	3	serves	as	a	sign	bit;	when	set	to	1,	it	contributes	–2
=	–8	to
the	value.	This	weighting	is	shown	as	a	leftward-pointing	gray	bar.
We	see	that	the	bit	patterns	are	identical	for	
Figures	
2.12
and	
2.13
(as	well	as	for	
Equations	
2.2
and	
2.4
),	but	the	values	differ	when
the	most	significant	bit	is	1,	since	in	one	case	it	has	weight	+8,	and	in	the
other	case	it	has	weight	–8.
Let	us	consider	the	range	of	values	that	can	be	represented	as	a	
w
-bit
two's-complement	number.	The	least	representable	value	is	given	by	bit
vector	[10	...	0]	(set	the	bit	with	negative	weight	but	clear	all	others),
having	integer	value	
.	The	greatest	value	is	given	by	bit
vector	[01	...	1]	(clear	the	bit	with	negative	weight	but	set	all	others),
having	integer	value	
.	Using	the	4-bit	case
as	an	example,	we	have	
and
.
We	can	see	that	
B2T
is	a	mapping	of	bit	patterns	of	length	
w
to	numbers
3
T
M
i
n
w</h1>
<h1>˙
−
2
w
−
1
T
M
a
x
w</h1>
<h1>˙
∑
i</h1>
<h1>0
w
−
2
2
i</h1>
<h1>2
w
−
1
−
1
T
M
i
n
4</h1>
<h1>B
2
T
4
(
[
1000
]
)</h1>
<h1>−
2
3</h1>
<p>−
8</p>
<h1>T
M
a
x
4</h1>
<h1>B
2
T
4
(
[
0111
]
)</h1>
<p>2
2</p>
<ul>
<li></li>
</ul>
<p>2
1</p>
<ul>
<li></li>
</ul>
<h1>2
0</h1>
<p>4</p>
<ul>
<li></li>
</ul>
<p>2</p>
<ul>
<li></li>
</ul>
<h1 id="1-5"><a class="header" href="#1-5">1</a></h1>
<p>7
w</p>
<p>between	
TMin
and	
TMax
,	written	as	
.
As	we	saw	with	the	unsigned	representation,	every	number	within	the
representable	range	has	a	unique	encoding	as	a	
w
-bit	two's-complement
number.	This	leads	to	a	principle	for	two's-complement	numbers	similar
to	that	for	unsigned	numbers:
Principle:
Uniqueness	of	two's-complement	encoding
Function	
B2T
is	a	bijection.
We	define	function	
T2B
(for	“two's	complement	to	binary”)	to	be	the
inverse	of	
B2T
.
That	is,	for	a	number	
x
,	such	that
is	the	(unique)	
w
-bit	pattern	that	encodes	
x
.
Practice	Problem	
2.17
(solution	page	
148
)
Assuming	
w
=	4,	we	can	assign	a	numeric	value	to	each	possible
hexadecimal	digit,	assuming	either	an	unsigned	or	a	two's-
complement	interpretation.	Fill	in	the	following	table	according	to
these	interpretations	by	writing	out	the	nonzero	powers	of	2	in	the
summations	shown	in	
Equations	
2.1
and	
2.3
:
w
w
B
2
T
w
:
{
0
,
1
}
w
→
{
T
M
i
n
w
,
…
,
T
M
a
x
w
}
w
w
w
T
M
i
n
w
≤
x
≤
T
M
a
x
w
,
 
T
2
B
w
(
x
)</p>
<p>x
→</p>
<p>Hexadecimal
Binary
B2U
B2T
[1110]
2
+	2
+	2
=	14
–2
+	2
+	2
=	–2</p>
<hr />
<hr />
<hr />
<hr />
<hr />
<hr />
<hr />
<hr />
<hr />
<hr />
<hr />
<hr />
<hr />
<hr />
<hr />
<p>Figure	
2.14
shows	the	bit	patterns	and	numeric	values	for	several
important	numbers	for	different	word	sizes.	The	first	three	give	the	ranges
of	representable	integers	in	terms	of	the	values	of	
UMax
,	
TMin
,	and
TMax
.	We	will	refer	to	these	three	special	values	often	in	the	ensuing
discussion.	We	will	drop	the	subscript	
w
and	refer	to	the	values
UMax
,
TMin
,	and	
TMax
when	
w
can	be	inferred	from	context	or	is	not	central	to
the	discussion.
A	few	points	are	worth	highlighting	about	these	numbers.	First,	as
observed	in	
Figures	
2.9
and	
2.10
,	the	two's-complement	range	is
asymmetric:	|
TMin
|	=	|
TMax
|	+	1;	that	is,	there	is	no	positive	counterpart
to	
TMin
.	As	we	shall	see,	this	leads	to	some	peculiar	properties	of	two's-
complement	arithmetic	and	can	be	the	source	of	subtle	program	bugs.
This	asymmetry	arises	because	half	the	bit	patterns	(those	with	the	sign
bit	set	to	1)	represent	negative	numbers,	while	half	(those	with	the	sign
bit	set	to	0)	represent	nonnegative	numbers.	Since	0	is	nonnegative,	this
means	that	it	can	represent	one	less	positive	number	than	negative.
4
(
x
→
)
4
(
x
→
)
3
2
1
3
2
1
w
w
w</p>
<p>Second,	the	maximum	unsigned	value	is	just	over	twice	the	maximum
two's-complement	value:	
UMax
=	2
TMax
+	1.	All	of	the	bit	patterns	that
denote	negative	numbers	in	two's-complement	notation	become	positive
values	in	an	unsigned	representation.
Word	size	
w
Value
8
16
32
64
UMax
255
65,535
4,294,967,295
18,446,744,073,709,551,615
TMin
–128
–32,768
–2,147,483,648
–9,223,372,036,854,775,808
TMax
127
32,767
2,147,483,647
9,223,372,036,854,775,807
–1
0
Figure	
2.14	
Important	numbers.
Both	numeric	values	and	hexadecimal	representations	are	shown.
Aside	
More	on	fixed-size	integer	types
For	some	programs,	it	is	essential	that	data	types	be	encoded
using	representations	with	specific	sizes.	For	example,	when
writing	programs	to	enable	a	machine	to	communicate	over	the
w
w
w</p>
<p>Internet	according	to	a	standard	protocol,	it	is	important	to	have
data	types	compatible	with	those	specified	by	the	protocol.	We
have	seen	that	some	C	data	types,	especially	
,	have	different
ranges	on	different	machines,	and	in	fact	the	C	standards	only
specify	the	minimum	ranges	for	any	data	type,	not	the	exact
ranges.	Although	we	can	choose	data	types	that	will	be
compatible	with	standard	representations	on	most	machines,
there	is	no	guarantee	of	portability.
We	have	already	encountered	the	32-	and	64-bit	versions	of	fixed-
size	integer	types	(
Figure	
2.3
);	they	are	part	of	a	larger	class	of
data	types.	The	ISO	C99	standard	introduces	this	class	of	integer
types	in	the	file	
.	This	file	defines	a	set	of	data	types	with
declarations	of	the	form	
and	
,	specifying	
N
-bit
signed	and	unsigned	integers,	for	different	values	of	
N
.	The	exact
values	of	
N
are	implementation	dependent,	but	most	compilers
allow	values	of	8,	16,	32,	and	64.	Thus,	we	can	unambiguously
declare	an	unsigned	16–bit	variable	by	giving	it	type	
,	and
a	signed	variable	of	32	bits	as	
.
Along	with	these	data	types	are	a	set	of	macros	defining	the
minimum	and	maximum	values	for	each	value	of	
N
.	These	have
names	of	the	form	
,	and	
.
Formatted	printing	with	fixed-width	types	requires	use	of	macros
that	expand	into	format	strings	in	a	system-dependent	manner.
So,	for	example,	the	values	of	variables	
x
and	
y
of	type	
and	
can	be	printed	by	the	following	call	to	
:</p>
<p>When	compiled	as	a	64–bit	program,	macro	
expands	to
the	string	
,	while	
expands	to	the	pair	of	strings	
.
When	the	C	preprocessor	encounters	a	sequence	of	string
constants	separated	only	by	spaces	(or	other	whitespace
characters),	it	concatenates	them	together.	Thus,	the	above	call	to
becomes
Using	the	macros	ensures	that	a	correct	format	string	will	be
generated	regardless	of	how	the	code	is	compiled.
Figure	
2.14
also	shows	the	representations	of	constants	–1	and	0.
Note	that	–1	has	the	same	bit	representation	as	
UMax
—a	string	of	all
ones.	Numeric	value	0	is	represented	as	a	string	of	all	zeros	in	both
representations.
The	C	standards	do	not	require	signed	integers	to	be	represented	in
two's-complement	form,	but	nearly	all	machines	do	so.	Programmers
who	are	concerned	with	maximizing	portability	across	all	possible
machines	should	not	assume	any	particular	range	of	representable
values,	beyond	the	ranges	indicated	in	
Figure	
2.11
,	nor	should	they
assume	any	particular	representation	of	signed	numbers.	On	the	other
hand,	many	programs	are	written	assuming	a	two's-complement
representation	of	signed	numbers,	and	the	“typical”	ranges	shown	in
Figures	
2.9
and	
2.10
,	and	these	programs	are	portable	across	a</p>
<h1>broad	range	of	machines	and	compilers.	The	file	
in	the	C
library	defines	a	set	of	constants
Aside	
Alternative	representations	of
signed	numbers
There	are	two	other	standard	representations	for	signed	numbers:
Ones’	complement.
This	is	the	same	as	two's	complement,
except	that	the	most	significant	bit	has	weight	–(2
–	1)	rather
than	–2
:
Sign-magnitude.
The	most	significant	bit	is	a	sign	bit	that
determines	whether	the	remaining	bits	should	be	given	negative
or	positive	weight:
Both	of	these	representations	have	the	curious	property	that	there
are	two	different	encodings	of	the	number	0.	For	both
representations,	[00	...	0]	is	interpreted	as	+0.	The	value	–0	can
be	represented	in	sign-magnitude	form	as	[10	...	0]	and	in	ones’
complement	as	[11	...	1].	Although	machines	based	on	ones'-
complement	representations	were	built	in	the	past,	almost	all
modern	machines	use	two's	complement.	We	will	see	that	sign-
magnitude	encoding	is	used	with	floating-point	numbers.
w–1
w
–1
B
2
O
w
(
x
→
)</h1>
<p>˙
−
x
w
−
1
(
2
w
−
1
−
1
)</p>
<ul>
<li></li>
</ul>
<h1>∑
i</h1>
<h1>0
w
−
2
x
i
2
i
B
2
S
w
(
x
→
)</h1>
<p>˙
(
−
1
)
x
w
−</p>
<ol>
<li></li>
</ol>
<h1>(
∑
i</h1>
<p>0
w
−
2
x
i
2
1
)</p>
<p>Note	the	different	position	of	apostrophes:	
two's
complement
versus	
ones'
complement.	The	term	“two's	complement”	arises
from	the	fact	that	for	nonnegative	
x
we	compute	a	
w
-bit
representation	of	–
x
as	2
–	
x
(a	single	two.)	The	term	“ones’
complement”	comes	from	the	property	that	we	can	compute	–
x
in
this	notation	as	[111	...	1]	–	
x
(multiple	ones).
delimiting	the	ranges	of	the	different	integer	data	types	for	the	particular
machine	on	which	the	compiler	is	running.	For	example,	it	defines
constants	
,	and	
describing	the	ranges	of
signed	and	unsigned	integers.	For	a	two's-complement	machine	in	which
data	type	
has	
w
bits,	these	constants	correspond	to	the	values	of
TMax
,	TMin
,	and	
UMax
.
The	Java	standard	is	quite	specific	about	integer	data	type	ranges	and
representations.	It	requires	a	two's-complement	representation	with	the
exact	ranges	shown	for	the	64-bit	case	(
Figure	
2.10
).	In	Java,	the
single-byte	data	type	is	called	
instead	of	
.	These	detailed
requirements	are	intended	to	enable	Java	programs	to	behave	identically
regardless	of	the	machines	or	operating	systems	running	them.
To	get	a	better	understanding	of	the	two's-complement	representation,
consider	the	following	code	example:
w
w
w
w</p>
<p>12,345
–12,345
53,191
Weight
Bit
Value
Bit
Value
Bit
Value
1
1
1
1
2
0
2
2
4
0
4
4
8
8
0
0
16
16
0
0
32
32
0
0
64
0
64
64
128
0
128
128
256
0
256
256
512
0
512
512
1,024
0
1,024
1,024
2,048
0
2,048
2,048
4,096
4,096
0
0
8,192
8,192
0
0
16,384
0
16,384
16,384</p>
<p>±32,768
0
–32,768
32,768
Total
12,345
–12,345
53,191
Figure	
2.15	
Two's-complement	representations	of	12,345	and	–
12,345,	and	unsigned	representation	of	53,191.
Note	that	the	latter	two	have	identical	bit	representations.
When	run	on	a	big-endian	machine,	this	code	prints	
and	
,
indicating	that	
has	hexadecimal	representation	
,	while	
has
hexadecimal	representation	
.	Expanding	these	into	binary,	we	get
bit	patterns	[0011000000111001]	for	
and	[1100111111000111]	for	
.
As	
Figure	
2.15
shows,	
Equation	
2.3
yields	values	12,345	and	–
12,345	for	these	two	bit	patterns.
Practice	Problem	
2.18
(solution	page	
149
)
In	
Chapter	
3
,	we	will	look	at	listings	generated	by	a
disassembler
,	a	program	that	converts	an	executable	program	file
back	to	a	more	readable	ASCII	form.	These	files	contain	many
hexadecimal	numbers,	typically	representing	values	in	two's-
complement	form.	Being	able	to	recognize	these	numbers	and
understand	their	significance	(for	example,	whether	they	are
negative	or	positive)	is	an	important	skill.
For	the	lines	labeled	A–I	(on	the	right)	in	the	following	listing,
convert	the	hexadecimal	values	(in	32-bit	two's-complement	form)
shown	to	the	right	of	the	instruction	names	(
and	
)
into	their	decimal	equivalents:</p>
<p>2.2.4	
Conversions	between	Signed
and	Unsigned</p>
<p>C	allows	casting	between	different	numeric	data	types.	For	example,
suppose	variable	
is	declared	as	
and	
as	unsigned.	The
expression	
converts	the	value	of	
to	an	unsigned	value,
and	
converts	the	value	of	
to	a	signed	integer.	What	should	be
the	effect	of	casting	signed	value	to	unsigned,	or	vice	versa?	From	a
mathematical	perspective,	one	can	imagine	several	different	conventions.
Clearly,	we	want	to	preserve	any	value	that	can	be	represented	in	both
forms.	On	the	other	hand,	converting	a	negative	value	to	unsigned	might
yield	zero.	Converting	an	unsigned	value	that	is	too	large	to	be
represented	in	two's-complement	form	might	yield	
TMax.
For	most
implementations	of	C,	however,	the	answer	to	this	question	is	based	on	a
bit-level	perspective,	rather	than	on	a	numeric	one.
For	example,	consider	the	following	code:
When	run	on	a	two's-complement	machine,	it	generates	the	following
output:</p>
<p>What	we	see	here	is	that	the	effect	of	casting	is	to	keep	the	bit	values
identical	but	change	how	these	bits	are	interpreted.	We	saw	in	
Figure
2.15
that	the	16-bit	two's-complement	representation	of	–12,345	is
identical	to	the	16-bit	unsigned	representation	of	53,191.	Casting	from
to	
changed	the	numeric	value,	but	not	the	bit
representation.
Similarly,	consider	the	following	code:
When	run	on	a	two's-complement	machine,	it	generates	the	following
output:
We	can	see	from	
Figure	
2.14
that,	for	a	32-bit	word	size,	the	bit
patterns	representing	4,294,967,295	(
UMax
)	in	unsigned	form	and	–1	in
two's-complement	form	are	identical.	In	casting	from	
to	
,	the
underlying	bit	representation	stays	the	same.
This	is	a	general	rule	for	how	most	C	implementations	handle
conversions	between	signed	and	unsigned	numbers	with	the	same	word
32</p>
<h1>size—the	numeric	values	might	change,	but	the	bit	patterns	do	not.	Let
us	capture	this	idea	in	a	more	mathematical	form.	We	defined	functions
U2B
and	
T2B
that	map	numbers	to	their	bit	representations	in	either
unsigned	or	two's-complement	form.	That	is,	given	an	integer	
x
in	the
range	
,	the	function	
U2B
(x)
gives	the	unique	
w
-bit	unsigned
representation	of	
x
.	Similarly,	when	
x
is	in	the	range	
,
the	function	
T2B
(x)
gives	the	unique	
w
-bit	two's-complement
representation	of	
x
.
Now	define	the	function	
.	This	function
takes	a	number	between	
TMin
and	
TMax
and	yields	a	number	between
0	and	
UMax
,	where	the	two	numbers	have	identical	bit	representations,
except	that	the	argument	has	a	two's-complement	representation	while
the	result	is	unsigned.	Similarly,	for	
x
between	0	and	
UMax
,	the	function
U2T
,	defined	as	
,	yields	the	number	having	the
same	two's-complement	representation	as	the	unsigned	representation
of	
x
.
Pursuing	our	earlier	examples,	we	see	from	
Figure	
2.15
that	
T2U
(–
12,345)	=	53,191,	and	that	
U2T
(53,191)	=	–12,345.	That	is,	the	16-bit
pattern	written	in	hexadecimal	as	
is	both	the	two's-complement
representation	of	–12,345	and	the	unsigned	representation	of	53,191.
Note	also	that	12,345	+	53,191	=	65,536	=	2
.	This	property	generalizes
to	a	relationship	between	the	two	numeric	values	(two's	complement	and
unsigned)	represented	by	a	given	bit	pattern.	Similarly,	from	
Figure
2.14
,	we	see	that	
T2U32
(–1)	=	4,294,967,295,	and
U2T
(4,294,967,295)	=	–1.	That	is,	
UMax
has	the	same	bit
representation	in	unsigned	form	as	does	–1	in	two's-complement	form.
w
w
0
≤
x
&lt;
U
M
a
x
w
w
T
M
i
n
w
≤
x
≤
T
M
a
x
w
w
T
2
U
w
 
as
 
T
2
U
w
(
x
)
≐
B
2
U
w
(
T
2
B
w
(
x
)
)
w
w
w
w
w
U
2
T
w</h1>
<p>˙
B
2
T
w
(
U
2
B
w
(
x
)
)
16
16
16
32</p>
<p>We	can	also	see	the	relationship	between	these	two	numbers:	1	+	
UMax
=	2
.
We	see,	then,	that	function	
T2U
describes	the	conversion	of	a
two'scomplement	number	to	its	unsigned	counterpart,	while	
U2T
converts
in	the	opposite	direction.	These	describe	the	effect	of	casting	between
these	data	types	in	most	C	implementations.
Practice	Problem	
2.19
(solution	page	
149
)
Using	the	table	you	filled	in	when	solving	
Problem	
2.17
,	fill	in
the	following	table	describing	the	function	
T2U
:
x
T2U
(x)
–8</p>
<hr />
<p>–3</p>
<hr />
<p>–2</p>
<hr />
<p>–1</p>
<hr />
<p>0</p>
<hr />
<p>5</p>
<hr />
<p>The	relationship	we	have	seen,	via	several	examples,	between	the	two's-
complement	and	unsigned	values	for	a	given	bit	pattern	can	be
expressed	as	a	property	of	the	function	
T2U
:
w
w
4
4</p>
<h1>Principle:
Conversion	from	two's	complement	to	unsigned
For	
x
such	that	
:
For	example,	we	saw	that	
,	and
also	that	
.
This	property	can	be	derived	by	comparing	
Equations	
2.1
and	
2.3
.
Derivation:
Conversion	from	two's	complement	to	unsigned
Comparing	
Equations	
2.1
and	
2.3
,	we	can	see	that
for	bit	pattern	
,	if	we	compute	the	difference	
,	the	weighted	sums	for	bits	from	0	to	
w
–2	will
cancel	each	other,	leaving	a	value	
.	This	gives	a
relationship	
.	We
therefore	have
T
M
i
n
w
≤
x
≤
T
M
a
x
w
T
2
U
w
(
x
)</h1>
<p>{
x</p>
<ul>
<li></li>
</ul>
<h1>2
w
,
x
&lt;
0
x
,
x
≥
0
(2.5)
T
2
U
16
(
−
12
,
345
)</h1>
<p>−
12
,
345</p>
<ul>
<li></li>
</ul>
<h1>2
16</h1>
<h1>53
,
191
T
2
U
w
(
−
1
)</h1>
<p>−
1</p>
<ul>
<li></li>
</ul>
<h1>2
w</h1>
<p>U
M
a
x
w
x
→
B
2
U
w
(
x
→
)
−
B
2
T
w
(
x
→
)
B
2
U
w</p>
<p>(</p>
<p>x
→</p>
<p>)
−
B
2
T
w</p>
<p>(
x
→</p>
<h1 id=""><a class="header" href="#">)</a></h1>
<p>x</p>
<p>w
−
1</p>
<p>(</p>
<p>2</p>
<p>w
−
1</p>
<p>−
−
2</p>
<p>w
−
1</p>
<h1 id="-1"><a class="header" href="#-1">)</a></h1>
<p>x</p>
<p>w
−
1</p>
<p>2
w</p>
<p>B
2
U
w</p>
<p>(</p>
<p>x
→</p>
<h1 id="-2"><a class="header" href="#-2">)</a></h1>
<p>B
2
T
w</p>
<p>(</p>
<p>x
→</p>
<p>)</p>
<ul>
<li></li>
</ul>
<p>x</p>
<p>w
−
1</p>
<p>2
w</p>
<h1>In	a	two's-complement	representation	of	
x
,	bit	
x
determines	whether	or	not	
x
is	negative,	giving	the	two
cases	of	
Equation	
2.5
.
As	examples,	
Figure	
2.16
compares	how	functions	
B2U
and	
B2T
assign	values	to	bit	patterns	for	
w
=	4.	For	the	two's-complement	case,
the	most	significant	bit	serves	as	the	sign	bit,	which	we	diagram	as	a
leftward-pointing	gray	bar.	For	the	unsigned	case,	this	bit	has	positive
weight,	which	we	show	as	a	rightward-pointing	black	bar.	In	going	from
two's	complement	to	unsigned,	the	most	significant	bit	changes	its	weight
from	–8	to	+8.	As	a	consequence,	the	values	that	are	negative	in	a	two's-
complement	representation	increase	by	2
=	16	with	an	unsigned
representation.	Thus,	–5	becomes	+11,	and	–1	becomes	+15.
B
2
U
w
(
T
2
B
w
(
x
)
)</h1>
<h1>T
2
U
w
(
x
)</h1>
<p>x</p>
<ul>
<li></li>
</ul>
<p>x
w
−
1
2
w
(2.6)
w
–1
4</p>
<p>Figure	
2.16	
Comparing	unsigned	and	two's-complement
representations	for
w
=	4.	The	weight	of	the	most	significant	bit	is	–8	for	two's	complement
and	+8	for	unsigned,	yielding	a	net	difference	of	16.
Figure	
2.17	
Conversion	from	two's	complement	to	unsigned.
Function	
T2U
converts	negative	numbers	to	large	positive	numbers.
Figure	
2.17
illustrates	the	general	behavior	of	function	
T2U
.	As	it
shows,	when	mapping	a	signed	number	to	its	unsigned	counterpart,
negative	numbers	are	converted	to	large	positive	numbers,	while
nonnegative	numbers	remain	unchanged.
Practice	Problem	
2.20
(solution	page	
149
)
Explain	how	
Equation	
2.5
applies	to	the	entries	in	the	table	you
generated	when	solving	
Problem	
2.19
.
Going	in	the	other	direction,	we	can	state	the	relationship	between	an
unsigned	number	
u
and	its	signed	counterpart	
U2T
(u)
:
w</p>
<h1>Principle:
Unsigned	to	two's-complement	conversion
For	
u
such	that	0	≤	
u
≤	
UMax
:
Figure	
2.18	
Conversion	from	unsigned	to	two's	complement.
Function	
U2T
converts	numbers	greater	than	
to
negative	values.
This	principle	can	be	justified	as	follows:
Derivation:
w
U
2
T
w
(
u
)</h1>
<h1>{
u
,
u
≥
T
M
a
x
w
u
−
2
w
,
u
&gt;
T
M
a
x
w
(2.7)
T
M
a
x
w</h1>
<p>2
w
−
1
−
1</p>
<h1>Unsigned	to	two's-complement	conversion
Let	
.	This	bit	vector	will	also	be	the	two's-
complement	representation	of	
U2T
(u)
.	
Equations	
2.1
and	
2.3
can	be	combined	to	give
In	the	unsigned	representation	of	
u
,	bit	
u
determines
whether	or	not	
u
is	greater	than	
TMax
=	2
–	1,	giving	the
two	cases	of	
Equation	
2.7
.
The	behavior	of	function	
U2T
is	illustrated	in	
Figure	
2.18
.	For	small	(≤
TMax
)	numbers,	the	conversion	from	unsigned	to	signed	preserves	the
nu-meric	value.	Large	(&gt;	
TMax
)	numbers	are	converted	to	negative
values.
To	summarize,	we	considered	the	effects	of	converting	in	both	directions
between	unsigned	and	two's-complement	representations.	For	values	
x
in	the	range	
,	we	have	
and	
.	That	is,
numbers	in	this	range	have	identical	unsigned	and	two's-complement
representations.	For	values	outside	of	this	range,	the	conversions	either
add	or	subtract	2
.	For	example,	we	have	
—
the	negative	number	closest	to	zero	maps	to	the	largest	unsigned
number.	At	the	other	extreme,	one	can	see	that	
—the	most	negative	number	maps	to	an
unsigned	number	just	outside	the	range	of	positive	two's-complement
u
→</h1>
<h1>U
2
B
w
(
u
)
w
U
2
T
w
(
u
)</h1>
<p>−
u
w
−
1
2
w</p>
<ul>
<li></li>
</ul>
<h1>u
(2.8)
w
–1
w
w
–1
w
w
0
≤
x
≤
T
M
a
x
w
T
2
U
w
(
x
)</h1>
<p>x</p>
<h1>U
2
T
w
(
x
)</h1>
<h1>x
w
T
2
U
w
(
−
1
)</h1>
<p>−
1</p>
<ul>
<li></li>
</ul>
<h1>2
w</h1>
<h1>U
M
a
x
w
T
2
U
w
(
T
M
i
n
w
)</h1>
<p>−
2
w
−
1</p>
<ul>
<li></li>
</ul>
<h1>2
w</h1>
<h1>2
w
−
1</h1>
<p>T
M
a
x
w</p>
<ul>
<li></li>
</ul>
<p>1</p>
<h1>numbers.	Using	the	example	of	
Figure	
2.15
,	we	can	see	that
.
2.2.5	
Signed	versus	Unsigned	in	C
As	indicated	in	
Figures	
2.9
and	
2.10
,	C	supports	both	signed	and
unsigned	arithmetic	for	all	of	its	integer	data	types.	Although	the	C
standard	does	not	specify	a	particular	representation	of	signed	numbers,
almost	all	machines	use	two's	complement.	Generally,	most	numbers	are
signed	by	default.	For	example,	when	declaring	a	constant	such	as	
or	
,	the	value	is	considered	signed.	Adding	character	
or	
as	a	suffix	creates	an	unsigned	constant;	for	example,	
or	
.
C	allows	conversion	between	unsigned	and	signed.	Although	the	C
standard	does	not	specify	precisely	how	this	conversion	should	be	made,
most	systems	follow	the	rule	that	the	underlying	bit	representation	does
not	change.	This	rule	has	the	effect	of	applying	the	function	
U2T
when
converting	from	unsigned	to	signed,	and	
T2U
when	converting	from
signed	to	unsigned,	where	
w
is	the	number	of	bits	for	the	data	type.
Conversions	can	happen	due	to	explicit	casting,	such	as	in	the	following
code:
T
2
U
16
(
−
12
,
345
)</h1>
<p>65
,
536</p>
<ul>
<li></li>
</ul>
<h1>−
12
,
345</h1>
<p>53
,
191
w
w</p>
<p>Alternatively,	they	can	happen	implicitly	when	an	expression	of	one	type
is	assigned	to	a	variable	of	another,	as	in	the	following	code:
When	printing	numeric	values	with	printf,	the	directives	
and	
are	used	to	print	a	number	as	a	signed	decimal,	an	unsigned	decimal,
and	in	hexadecimal	format,	respectively.	Note	that	
does	not	make
use	of	any	type	information,	and	so	it	is	possible	to	print	a	value	of	type
with	directive	
and	a	value	of	type	
with	directive	
.	For
example,	consider	the	following	code:</p>
<h1>When	compiled	as	a	32-bit	program,	it	prints	the	following:
In	both	cases,	
prints	the	word	first	as	if	it	represented	an	unsigned
number	and	second	as	if	it	represented	a	signed	number.	We	can	see	the
conversion	routines	in	action:	
and
.
Some	possibly	nonintuitive	behavior	arises	due	to	C's	handling	of
expressions	containing	combinations	of	signed	and	unsigned	quantities.
When	an	operation	is	performed	where	one	operand	is	signed	and	the
other	is	unsigned,	C	implicitly	casts	the	signed	argument	to	unsigned	and
performs	the	operations
Expression
Type
Evaluation
T
2
U
32
(
−
1
)</h1>
<h1>U
M
a
x
32</h1>
<p>2
32
−
1</p>
<h1>U
2
T
32
(
2
31
)</h1>
<h1>2
31
−
2
32</h1>
<h1>−
2
31</h1>
<p>T
M
i
n
32</p>
<p>Figure	
2.19	
Effects	of	C	promotion	rules.
Nonintuitive	cases	are	marked	by	‘
’.	When	either	operand	of	a
comparison	is	unsigned,	the	other	operand	is	implicitly	cast	to	unsigned.
See	Web	Aside	
DATA
:
TMIN</p>
<p>for	why	we	write	
TMin
as	
.
assuming	the	numbers	are	nonnegative.	As	we	will	see,	this	convention
makes	little	difference	for	standard	arithmetic	operations,	but	it	leads	to
nonintuitive	results	for	relational	operators	such	as	&lt;	and	&gt;.	
Figure
2.19
shows	some	sample	relational	expressions	and	their	resulting
evaluations,	when	data	type	
has	a	32-bit,	two's-complement
representation.	Consider	the	comparison	
.	Since	the	second
operand	is	unsigned,	the	first	one	is	implicitly	cast	to	unsigned,	and
hence	the	expression	is	equivalent	to	the	comparison	
(recall	that	
),	which	of	course	is	false.	The	other	cases
can	be	understood	by	similar	analyses.
Practice	Problem	
2.21
(solution	page	
149
)
Assuming	the	expressions	are	evaluated	when	executing	a	32-bit
program	on	a	machine	that	uses	two's-complement	arithmetic,	fill
in	the	following	table	describing	the	effect	of	casting	and	relational
operations,	in	the	style	of	
Figure	
2.19
:
Expression
Type
Evaluation
–2147483647–1	==	2147483648U</p>
<hr />
<hr />
<h1>32
T
2
U
w
(
−
1
)</h1>
<p>U
M
a
x
w</p>
<p>–2147483647–1	&lt;	2147483647</p>
<hr />
<hr />
<p>–2147483647–1U	&lt;	2147483647</p>
<hr />
<hr />
<p>–2147483647–1	&lt;	–2147483647</p>
<hr />
<hr />
<p>–2147483647–1U	&lt;	–2147483647</p>
<hr />
<hr />
<p>2.2.6	
Expanding	the	Bit
Representation	of	a	Number
One	common	operation	is	to	convert	between	integers	having	different
word	sizes	while	retaining	the	same	numeric	value.	Of	course,	this	may
not	be	possible	when	the	destination	data	type	is	too	small	to	represent
the	desired	value.	Converting	from	a	smaller	to	a	larger	data	type,
however,	should	always	be	possible.
Web	Aside	DATA:TMIN	
Writing	
TMin
in
C
In	
Figure	
2.19
and	in	
Problem	
2.21
,	we	carefully	wrote	the
value	of	
TMin
as	
.	Why	not	simply	write	it	as
either	
or	
Looking	at	the	C	header	file
,	we	see	that	they	use	a	similar	method	as	we	have	to
write	
TMin
and	
TMax
:
32
32
32</p>
<p>Unfortunately,	a	curious	interaction	between	the	asymmetry	of	the
two's-complement	representation	and	the	conversion	rules	of	C
forces	us	to	write	
TMin
in	this	unusual	way.	Although
understanding	this	issue	requires	us	to	delve	into	one	of	the
murkier	corners	of	the	C	language	standards,	it	will	help	us
appreciate	some	of	the	subtleties	of	integer	data	types	and</p>
<div style="break-before: page; page-break-before: always;"></div><h1>representations.
To	convert	an	unsigned	number	to	a	larger	data	type,	we	can	simply	add
leading	zeros	to	the	representation;	this	operation	is	known	as	
zero
extension
,	expressed	by	the	following	principle:
Principle:
Expansion	of	an	unsigned	number	by	zero	extension
Define	bit	vectors	
of	width	
w
and
of	width	
w
′,	where	
w
′	&gt;	
w
.
Then	
.
32
u
→</h1>
<p>[
u
w
−
1
,
u
w
−
1
,
…
,
u
0
]</p>
<h1>u
→
′</h1>
<p>[
0
,
…
,
0
,
u
w
−
1
,
u
w
−
2
,
…
u
0
]</p>
<h1>B
2
U
w
(
u
→
)</h1>
<p>B
2
U
w
′
(
u
→
′
)</p>
<h1>This	principle	can	be	seen	to	follow	directly	from	the	definition	of	the
unsigned	encoding,	given	by	
Equation	
2.1
.
For	converting	a	two's-complement	number	to	a	larger	data	type,	the	rule
is	to	perform	a	
sign	extension
,	adding	copies	of	the	most	significant	bit	to
the	representation,	expressed	by	the	following	principle.	We	show	the
sign	bit	
x
in	blue	to	highlight	its	role	in	sign	extension.
Principle:
Expansion	of	a	two's-complement	number	by	sign
extension
Define	bit	vectors	
of	width	
w
and
of	width	
w
′,	where	
w
′
&gt;	
w
.	Then	
.
As	an	example,	consider	the	following	code:
w
–1
x
→</h1>
<p>[
x
w
−
1
,
x
w
−
2
,
…
,
x
0
]</p>
<h1>x
→
′</h1>
<p>[
x
w
−
1
,
…
,
x
w
−
1
,
x
w
−
1
,
x
w
−
2
,
…
,
x
0
]</p>
<h1>B
2
T
w
(
x
→
)</h1>
<p>B
2
T
w
′
(
x
→
′
)</p>
<p>When	run	as	a	32–bit	program	on	a	big-endian	machine	that	uses	a
two's-complement	representation,	this	code	prints	the	output
We	see	that,	although	the	two's-complement	representation	of	–12,345
and	the	unsigned	representation	of	53,191	are	identical	for	a	16–bit	word
size,	they	differ	for	a	32–bit	word	size.	In	particular,	-12,345	has
hexadecimal	representation	
,	while	53,191	has	hexadecimal
representation	
.	The	former	has	been	sign	extended—16
copies	of	the	most	significant	bit	1,	having	hexadecimal	representation
0xFFFF,	have	been	added	as	leading	bits.	The	latter	has	been	extended
with	16	leading	zeros,	having	hexadecimal	representation	
.</p>
<p>As	an	illustration,	
Figure	
2.20
shows	the	result	of	expanding	from
word	size	
w
=	3	to	
w
=	4	by	sign	extension.	Bit	vector	[
101
]represents	the
value	–4	+	1	=	–3.	Applying	sign	extension	gives	bit	vector	[1101]
representing	the	value	–8	+	4	+	1	=	–3.	We	can	see	that,	for	
w
=	4,	the
combined	value	of	the	two	most	significant	bits,	–8	+	4	=	–4,	matches	the
value	of	the	sign	bit	for	
w
=	3.	Similarly,	bit	vectors	[
111
]	and	[1111]	both
represent	the	value	–1.
With	this	as	intuition,	we	can	now	show	that	sign	extension	preserves	the
value	of	a	two's-complement	number.
Figure	
2.20	
Examples	of	sign	extension	from	
w
=	3	to	
w
=	4.
For	
w
=	4,	the	combined	weight	of	the	upper	2	bits	is	–8	+	4	=	–4,
matching	that	of	the	sign	bit	for	
w
=	3.</p>
<p>Derivation:
Expansion	of	a	two's-complement	number	by	sign
extension	Let	
w′	=	w	+	k.
What	we	want	to	prove	is	that
The	proof	follows	by	induction	on	
k.
That	is,	if	we	can	prove
that	sign	extending	by	1	bit	preserves	the	numeric	value,
then	this	property	will	hold	when	sign	extending	by	an
arbitrary	number	of	bits.	Thus,	the	task	reduces	to	proving
that
Expanding	the	left-hand	expression	with	
Equation	
2.3
gives	the	following:
The	key	property	we	exploit	is	that	
.	Thus,
the	combined	effect	of	adding	a	bit	of	weight	–2
and	of
converting	the	bit	having	weight	–2
to	be	one	with	weight
2
is	to	preserve	the	original	numeric	value.
B
2
T
w</p>
<ul>
<li></li>
</ul>
<h1>k
(
[
x
w
−
1
,
…
,
x
w
−
1
︸
k
 
times
,
 
x
w
−
2
,
…
,
 
x
0
]
)</h1>
<p>B
2
T
(
[
x
w
−
1
,
 
x
w
−
2
,
…
,
 
x
0
]
)
B
2
T
w</p>
<ul>
<li></li>
</ul>
<h1>1
(
[
x
w
−
1
,
 
x
w
−
1
,
 
x
w
−
2
,
…
,
 
x
0
]
)</h1>
<p>B
2
T
w
(
[
x
w
−
1
,
 
x
w
−
2
,
…
,
 
x
0
]
)
B
2
T
w</p>
<ul>
<li></li>
</ul>
<h1>1
(
[
x
w
−
1
,
 
x
w
−
1
,
 
x
w
−
2
,
…
,
 
x
0
]
)</h1>
<p>−
x
w
−
1
2
w</p>
<ul>
<li></li>
</ul>
<h1>∑
i</h1>
<h1>0
w
−
1
x
i
2
i
 </h1>
<p>−
x
w
−
1
2
w</p>
<ul>
<li></li>
</ul>
<p>x
w
−
1
2
w
−
1</p>
<ul>
<li></li>
</ul>
<h1>∑
i</h1>
<h1>0
w
−
2
x
i
2
i
 </h1>
<p>−
x
w
−
1
(
2
w
−
2
w
−
1
)</p>
<ul>
<li></li>
</ul>
<h1>∑
i</h1>
<h1>0
w
−
2
x
i
2
i
 </h1>
<p>−
x
w
−
1
2
w
−
1</p>
<ul>
<li></li>
</ul>
<h1>∑
i</h1>
<h1>0
w
−
2
x
i
2
i
 </h1>
<h1>B
2
T
w
(
[
x
w
−
1
,
 
x
w
−
2
,
…
,
 
x
0
]
)
2
w
−
2
w
−
1</h1>
<p>2
w
−
1
w
w
–1
w
–1</p>
<p>Practice	Problem	
2.22
(solution	page	
150
)
Show	that	each	of	the	following	bit	vectors	is	a	two's-complement
representation	of	–5	by	applying	
Equation	
2.3
:
A
.	
[1011]
B
.	
[11011]
C
.	
[111011]
Observe	that	the	second	and	third	bit	vectors	can	be	derived	from
the	first	by	sign	extension.
One	point	worth	making	is	that	the	relative	order	of	conversion	from	one
data	size	to	another	and	between	unsigned	and	signed	can	affect	the
behavior	of	a	program.	Consider	the	following	code:
When	run	on	a	big-endian	machine,	this	code	causes	the	following	output
to	be	printed:</p>
<p>This	shows	that,	when	converting	from	
to	
,	the	program
first	changes	the	size	and	then	the	type.	That	is,	
is
equivalent	to	
,	evaluating	to	4,294,954,951,	not
,	which	evaluates	to	53,191.	Indeed,	this
convention	is	required	by	the	C	standards.
Practice	Problem	
2.23
(solution	page	
150
)
Consider	the	following	C	functions:
Assume	these	are	executed	as	a	32–bit	program	on	a	machine
that	uses	two's-complement	arithmetic.	Assume	also	that	right
shifts	of	signed	values	are	performed	arithmetically,	while	right
shifts	of	unsigned	values	are	performed	logically.
A
.	
Fill	in	the	following	table	showing	the	effect	of	these
functions	for	several	example	arguments.	You	will	find	it</p>
<p>more	convenient	to	work	with	a	hexadecimal
representation.	Just	remember	that	hex	digits	8	through	F
have	their	most	significant	bits	equal	to	1.</p>
<hr />
<hr />
<hr />
<hr />
<hr />
<hr />
<hr />
<hr />
<p>B
.	
Describe	in	words	the	useful	computation	each	of	these
functions	performs.
2.2.7	
Truncating	Numbers
Suppose	that,	rather	than	extending	a	value	with	extra	bits,	we	reduce
the	number	of	bits	representing	a	number.	This	occurs,	for	example,	in
the	following	code:</p>
<h1>Casting	
to	be	
will	truncate	a	32-bit	
to	a	16-bit	
.	As	we
saw	before,	this	16–bit	pattern	is	the	two's-complement	representation	of
–12,345.	When	casting	this	back	to	
,	sign	extension	will	set	the	high-
order	16	bits	to	ones,	yielding	the	32–bit	two's-complement
representation	of	–12,345.
When	truncating	a	
w
-bit	number	
to	a	
k
-bit
number,	we	drop	the	high-order	
w	–	k
bits,	giving	a	bit	vector	
.	Truncating	a	number	can	alter	its	value—a	form	of
overflow.	For	an	unsigned	number,	we	can	readily	characterize	the
numeric	value	that	will	result.
Principle:
Truncation	of	an	unsigned	number
Let	
be	the	bit	vector	
,	and	let	
be
the	result	of	truncating	it	to	
k
bits:	
and	
.	Then	
x
′	=	
x
mod	2
.
The	intuition	behind	this	principle	is	simply	that	all	of	the	bits	that	were
truncated	have	weights	of	the	form	2
i
,	where	
i
≥	
k
,	and	therefore	each	of
these	weights	reduces	to	zero	under	the	modulus	operation.	This	is
formalized	by	the	following	derivation:
x
→</h1>
<p>[
x
w
−
1
,
x
w
−
2
,
…
,
x
0
]</p>
<h1>x
→
′</h1>
<p>[
x
k
−
1
,
x
k
−
2
,
…
,
x
0
]
x
→</p>
<p>[
x
w
−
1
.
x
w
−
2
,
…
,
x
0
]
x
→
′</p>
<h1>x
→
′</h1>
<h1>[
x
k
−
1
,
x
k
−
2
,
…
,
x
0
]
.
 
L
e
t
 
x</h1>
<p>B
2
U
w
(
x
→
)</p>
<h1>x
′</h1>
<p>B
2
U
k
(
x
→
′
)
k</p>
<h1>Derivation:
Truncation	of	an	unsigned	number
Applying	the	modulus	operation	to	
Equation	
2.1
yields
In	this	derivation,	we	make	use	of	the	property	that	2
mod
2
=	0	for	any	
i
≥	
k
.
A	similar	property	holds	for	truncating	a	two's-complement	number,
except	that	it	then	converts	the	most	significant	bit	into	a	sign	bit:
Principle:
Truncation	of	a	two's-complement	number
Let	
be	the	bit	vector	
,	and	let	
be
the	result	of	truncating	it	to	
k
bits:	
.
B
2
U
w
(
[
x
w
−
1
,
 
x
w
−
2
,
…
,
 
x
0
]
)
 
mod
 
2
k</h1>
<h1>[
∑
i</h1>
<h1>0
w
−
1
x
i
2
i
]
 
mod
 
2
k
 </h1>
<h1>[
∑
i</h1>
<h1>0
k
−
1
x
i
2
i
]
 
mod
 
2
k
 </h1>
<h1>∑
i</h1>
<h1>0
k
−
1
x
i
2
i
 </h1>
<p>B
2
U
k
(
[
x
k
−
1
,
 
x
k
−
2
,
…
 
x
0
]
)
i
k
x
→</p>
<p>[
x
w
−
1
.
x
w
−
2
,
…
,
x
0
]
′
x
→</p>
<h1>x
→
′</h1>
<p>[
x
k
−
1
,
x
k
−
2
,
…
,
x
0
]</p>
<h1>Let	
and	
.	Then	
x
′	=	
U2T
(
x
mod
2
).
In	this	formulation,	
x
mod	2
will	be	a	number	between	0	and	2
–	1.
Applying	function	
U2T
to	it	will	have	the	effect	of	converting	the	most
significant	bit	
x
from	having	weight	2
to	having	weight	–2
.	We	can
see	this	with	the	example	of	converting	value	
x
=	53,191	from	
int
to	
short
.
Since	2
=	65,536	≥	
x
,	we	have	
x
mod	2
=	
x
.	But	when	we	convert	this
number	to	a	16–bit	two's-complement	number,	we	get	
.
Derivation:
Truncation	of	a	two's-complement	number
Using	a	similar	argument	to	the	one	we	used	for	truncation
of	an	unsigned	number	shows	that
That	is,	
x
mod	2
can	be	represented	by	an	unsigned
number	having	bit-level	representation	
.
Converting	this	to	a	two's-complement	number	gives	
).
x</h1>
<p>B
2
T
w
(
x
→
)</p>
<h1>x
′</h1>
<h1>B
2
T
k
(
x
→
′
)
k
k
k
k
k
k
–1
k
–1
k
–1
16
16
x
′</h1>
<h1>53
,
191
−
65
,
536</h1>
<h1>−
12
,
345
B
2
T
w
(
[
x
w
−
1
,
 
x
w
−
2
,
…
,
 
x
0
]
)
 
mod
 
2
k</h1>
<h1>B
2
U
k
(
[
x
k
−
1
,
 
x
k
−
2
,
…
,
 
x
0
]
)
k
[
x
k
−
1
,
x
k
−
2
,
…
,
x
0
]
x
′</h1>
<p>U
2
T
k
(
x
 
mod
 
2
k
)</p>
<p>Summarizing,	the	effect	of	truncation	for	unsigned	numbers	is
while	the	effect	for	two's-complement	numbers	is
Practice	Problem	
2.24
(solution	page	
150
)
Suppose	we	truncate	a	4–bit	value	(represented	by	hex	digits	
through	
)	to	a	3–bit	value	(represented	as	hex	digits	
through
.)	Fill	in	the	table	below	showing	the	effect	of	this	truncation	for
some	cases,	in	terms	of	the	unsigned	and	two's-complement
interpretations	of	those	bit	patterns.
Hex
Unsigned
Two's	complement
Original
Truncated
Original
Truncated
Original
Truncated
0</p>
<hr />
<p>0</p>
<hr />
<p>2</p>
<hr />
<p>2</p>
<hr />
<p>9</p>
<hr />
<p>–7</p>
<hr />
<p>11</p>
<hr />
<p>–5</p>
<hr />
<p>15</p>
<hr />
<p>–1</p>
<hr />
<h1>B
2
U
k
(
[
x
k
−
1
,
 
x
k
−
2
,
…
,
 
x
0
]
)</h1>
<h1>B
2
U
w
(
[
x
w
−
1
,
 
x
w
−
2
,
…
,
 
x
0
]
)
 
mod
 
2
k
(2.9)
B
2
T
k
(
[
x
k
−
1
,
 
x
k
−
2
,
…
,
 
x
0
]
)</h1>
<p>B
2
U
w
(
B
2
U
w
(
[
x
w
−
1
,
 
x
w
−
2
,
…
,
 
x
0
]
)
 
mod
 
2
k
)
(2.10)</p>
<p>Explain	how	
Equations	
2.9
and	
2.10
apply	to	these	cases.
2.2.8	
Advice	on	Signed	versus
Unsigned
As	we	have	seen,	the	implicit	casting	of	signed	to	unsigned	leads	to
some	nonintuitive	behavior.	Nonintuitive	features	often	lead	to	program
bugs,	and	ones	involving	the	nuances	of	implicit	casting	can	be
especially	difficult	to	see.	Since	the	casting	takes	place	without	any	clear
indication	in	the	code,	programmers	often	overlook	its	effects.
The	following	two	practice	problems	illustrate	some	of	the	subtle	errors
that	can	arise	due	to	implicit	casting	and	the	unsigned	data	type.
Practice	Problem	
2.25
(solution	page	
151
)
Consider	the	following	code	that	attempts	to	sum	the	elements	of
an	array	a,	where	the	number	of	elements	is	given	by	parameter
:</p>
<p>When	run	with	argument	
equal	to	0,	this	code	should
return	0.0.	Instead,	it	encounters	a	memory	error.	Explain	why	this
happens.	Show	how	this	code	can	be	corrected.
Practice	Problem	
2.26
(solution	page	
151
)
You	are	given	the	assignment	of	writing	a	function	that	determines
whether	one	string	is	longer	than	another.	You	decide	to	make	use
of	the	string	library	function	
having	the	following
declaration:
Here	is	your	first	attempt	at	the	function:</p>
<p>When	you	test	this	on	some	sample	data,	things	do	not	seem	to
work	quite	right.	You	investigate	further	and	determine	that,	when
compiled	as	a	32-bit	
program,	data	type	
is	defined	(via
)	in	header	file	
to	be	
.
A
.	
For	what	cases	will	this	function	produce	an	incorrect
result?
B
.	
Explain	how	this	incorrect	result	comes	about.
C
.	
Show	how	to	fix	the	code	so	that	it	will	work	reliably.
We	have	seen	multiple	ways	in	which	the	subtle	features	of	unsigned
arithmetic,	and	especially	the	implicit	conversion	of	signed	to	unsigned,
can	lead	to	errors	or	vulnerabilities.	One	way	to	avoid	such	bugs	is	to
never	use	unsigned	numbers.	In	fact,	few	languages	other	than	C
support	unsigned	integers.	Apparently,	these	other	language	designers
viewed	them	as	more	trouble	than	they	are	worth.	For	example,	Java
supports	only	signed	integers,	and	it	requires	that	they	be	implemented
with	two's-complement	arithmetic.	The	normal	right	shift	operator	&gt;&gt;	is
guaranteed	to	perform	an	arithmetic	shift.	The	special	operator	&gt;&gt;&gt;	is
defined	to	perform	a	logical	right	shift.
Unsigned	values	are	very	useful	when	we	want	to	think	of	words	as	just
collections	of	bits	with	no	numeric	interpretation.	This	occurs,	for
example,	when	packing	a	word	with	
flags
describing	various	Boolean
conditions.	Addresses	are	naturally	unsigned,	so	systems	programmers
find	unsigned	types	to	be	helpful.	Unsigned	values	are	also	useful	when
implementing	mathematical	packages	for	modular	arithmetic	and	for
multiprecision	arithmetic,	in	which	numbers	are	represented	by	arrays	of
words.</p>
<p>2.3	
Integer	Arithmetic
Many	beginning	programmers	are	surprised	to	find	that	adding	two
positive	numbers	can	yield	a	negative	result,	and	that	the	comparison	
can	yield	a	different	result	than	the	comparison	
.	These
properties	are	artifacts	of	the	finite	nature	of	computer	arithmetic.
Understanding	the	nuances	of	computer	arithmetic	can	help
programmers	write	more	reliable	code.
2.3.1	
Unsigned	Addition
Consider	two	nonnegative	integers	
x
and	
y
,	such	that	0	≤	
x,	y
&lt;	2
.	Each
of	these	values	can	be	represented	by	a	
w
-bit	unsigned	number.	If	we
compute	their	sum,	however,	we	have	a	possible	range	
.
Representing	this	sum	could	require	
w
+	1	bits.	For	example,	
Figure
2.21
shows	a	plot	of	the	function	
x
+	
y
when	
x
and	
y
have	4-bit
representations.	The	arguments	(shown	on	the	horizontal	axes)	range
from	0	to	15,	but	the	sum	ranges	from	0	to	30.	The	shape	of	the	function
is	a	sloping	plane	(the	function	is	linear	in	both	dimensions).	If	we	were	to
maintain	the	sum	as	a	(
w
+	1)-bit	number	and	add	it	to	another	value,	we
may	require	
w
+	2	bits,	and	so	on.	This	continued	“word	size
w
0
≤
x</p>
<ul>
<li></li>
</ul>
<p>y
≤
2
w</p>
<ul>
<li></li>
</ul>
<p>1
−
2</p>
<p>Figure	
2.21	
Integer	addition.
With	a	4–bit	word	size,	the	sum	could	require	5	bits.
inflation”	means	we	cannot	place	any	bound	on	the	word	size	required	to
fully	represent	the	results	of	arithmetic	operations.	Some	programming
languages,	such	as	Lisp,	actually	support	
arbitrary	size
arithmetic	to
allow	integers	of	any	size	(within	the	memory	limits	of	the	computer,	of
course.)	More	commonly,	programming	languages	support	fixed-size</p>
<p>arithmetic,	and	hence	operations	such	as	“addition”	and	“multiplication”
differ	from	their	counterpart	operations	over	integers.
Let	us	define	the	operation	
for	arguments	
x
and	
y
,	where	0	≤	
x,	y
&lt;
2
,	as	the	result	of	truncating	the	integer	sum	
x
+	
y
to	be	
w
bits	long	and
then	viewing	the	result	as	an	unsigned	number.	This	can	be
characterized	as	a	form	of	modular	arithmetic,	computing	the	sum
modulo	2
by	simply	discarding	any	bits	with	weight	greater	than	2
in
the	bit-level	representation	of	
x
+	
y
.	For	example,	consider	a	4–bit
number	representation	with	
x
=	9	and	
y
=	12,	having	bit	representations
[1001]	and	[1100],	respectively.	Their	sum	is	21,	having	a	5–bit
representation	[10101].	But	if	we	discard	the	high-order	bit,	we	get
[0101],	that	is,	decimal	value	5.	This	matches	the	value	21	mod	16	=	5.
Aside	
Security	vulnerability	in
In	2002,	programmers	involved	in	the	FreeBSD	open-source
operating	systems	project	realized	that	their	implementation	of	the
library	function	had	a	security	vulnerability.	A
simplified	version	of	their	code	went	something	like	this:</p>
<ul>
<li></li>
</ul>
<p>w
u</p>
<p>w
w
w
–1</p>
<p>In	this	code,	we	show	the	prototype	for	library	function	memcpy	on
line	7,	which	is	designed	to	copy	a	specified	number	of	bytes	
from	one	region	of	memory	to	another.
The	function	
,	starting	at	line	14,	is	designed	to
copy	some	of	the	data	maintained	by	the	operating	system	kernel
to	a	designated	region	of	memory	accessible	to	the	user.	Most	of
the	data	structures	maintained	by	the	kernel	should	not	be
readable	by	a	user,	since	they	may	contain	sensitive	information
about	other	users	and	about	other	jobs	running	on	the	system,	but</p>
<p>the	region	shown	as	
was	intended	to	be	one	that	the	user
could	read.	The	parameter	
is	intended	to	be	the	length	of
the	buffer	allocated	by	the	user	and	indicated	by	argument
.	The	computation	at	line	16	then	makes	sure	that	no
more	bytes	are	copied	than	are	available	in	either	the	source	or
the	destination	buffer.
Suppose,	however,	that	some	malicious	programmer	writes	code
that	calls	
with	a	negative	value	of	
.	Then
the	minimum	computation	on	line	16	will	compute	this	value	for
,	which	will	then	be	passed	as	the	parameter	
to	memcpy.
Note,	however,	that	parameter	n	is	declared	as	having	data	type
.	This	data	type	is	declared	(via	
)	in	the	library	file
.	Typically,	it	is	defined	to	be	
for	32–bit	programs
and	
for	64–bit	programs.	Since	argument	
is
unsigned,	
will	treat	it	as	a	very	large	positive	number	and
attempt	to	copy	that	many	bytes	from	the	kernel	region	to	the
user's	buffer.	Copying	that	many	bytes	(at	least	2
)	will	not
actually	work,	because	the	program	will	encounter	invalid
addresses	in	the	process,	but	the	program	could	read	regions	of
the	kernel	memory	for	which	it	is	not	authorized.
We	can	see	that	this	problem	arises	due	to	the	mismatch	between
data	types:	in	one	place	the	length	parameter	is	signed;	in	another
place	it	is	unsigned.	Such	mismatches	can	be	a	source	of	bugs
and,	as	this	example	shows,	can	even	lead	to	security
vulnerabilities.	Fortunately,	there	were	no	reported	cases	where	a
programmer	had	exploited	the	vulnerability	in	FreeBSD.	They
issued	a	security	advisory	“FreeBSD-SA-02:38.signed-error”
advising	system	administrators	on	how	to	apply	a	patch	that	would
31</p>
<p>remove	the	vulnerability.	The	bug	can	be	fixed	by	declaring
parameter	
to	
to	be	of	type	
,	to	be
consistent	with	parameter	
of	
.	We	should	also	declare
local	variable	
and	the	return	value	to	be	of	type	
.
We	can	characterize	operation	
as	follows:
Principle:
Unsigned	addition
For	
x
and	
y
such	that	0	≤	
x,	y
&lt;	2
:
The	two	cases	of	
Equation	
2.11
are	illustrated	in	
Figure	
2.22
,
showing	the	sum	
x
+	
y
on	the	left	mapping	to	the	unsigned	
w
-bit	sum
on	the	right.	The	normal	case	preserves	the	value	of	
x
+	
y
,	while
the	overflow	case	has	the	effect	of	decrementing	this	sum	by	2
.
Derivation:
Unsigned	addition</p>
<ul>
<li></li>
</ul>
<p>w
u</p>
<p>w
x</p>
<ul>
<li></li>
</ul>
<h1>w
u
y</h1>
<p>{
x</p>
<ul>
<li></li>
</ul>
<p>y
,
x</p>
<ul>
<li></li>
</ul>
<p>y
&lt;
2
w
 
Normal
x</p>
<ul>
<li></li>
</ul>
<p>y
−
2
w
,
2
w
≤
x</p>
<ul>
<li></li>
</ul>
<p>y
&lt;
2
w</p>
<ul>
<li></li>
</ul>
<p>1
 
Overflow
(2.11)
x</p>
<ul>
<li></li>
</ul>
<p>w
u
y
w</p>
<p>In	general,	we	can	see	that	if	
,	the	leading	bit	in	the
(
w
+	1)-bit	representation	of	the	sum	will	equal	0,	and
hence	discarding	it	will	not	change	the	numeric	value.	On
the	other	hand,	if	
,	the	leading	bit	in	the	(
w
+
1)-bit	representation	of	the	sum	will	equal	1,	and	hence
discarding	it	is	equivalent	to	subtracting	2
from	the	sum.
An	arithmetic	operation	is	said	to	
overflow
when	the	full	integer	result
cannot	fit	within	the	word	size	limits	of	the	data	type.	As	
Equation	
2.11
indicates,	overflow
Figure	
2.22	
Relation	between	integer	addition	and	unsigned
addition.
When	
x
+
y
is	greater	than	2
–	1,	the	sum	overflows.
x</p>
<ul>
<li></li>
</ul>
<p>y
&lt;
2
w
2
w
≤
x</p>
<ul>
<li></li>
</ul>
<p>y
&lt;
2
w</p>
<ul>
<li></li>
</ul>
<p>1
w
w</p>
<p>Figure	
2.23	
Unsigned	addition.
With	a	4-bit	word	size,	addition	is	performed	modulo	16.
occurs	when	the	two	operands	sum	to	2
or	more.	
Figure	
2.23
shows
a	plot	of	the	unsigned	addition	function	for	word	size	
w
=	4.	The	sum	is
computed	modulo	2
=	16.	When	
x
+	
y
&lt;	16,	there	is	no	overflow,	and
is	simply	
x
+	
y
.	This	is	shown	as	the	region	forming	a	sloping	plane
w
4
x</p>
<ul>
<li></li>
</ul>
<p>4
u
y</p>
<p>labeled	“Normal.”	When	
x
+	
y
≥	16,	the	addition	overflows,	having	the
effect	of	decrementing	the	sum	by	16.	This	is	shown	as	the	region
forming	a	sloping	plane	labeled	“Overflow.”
When	executing	C	programs,	overflows	are	not	signaled	as	errors.	At
times,	however,	we	might	wish	to	determine	whether	or	not	overflow	has
occurred.
Principle:
Detecting	overflow	of	unsigned	addition
For	
x
and	
y
in	the	range	
,	let	
.	Then
the	computation	of	
s
overflowed	if	and	only	if	
s
&lt;	
x
(or
equivalently,	
s
&lt;	
y
).
As	an	illustration,	in	our	earlier	example,	we	saw	that	
.	We	can
see	that	overflow	occurred,	since	5	&lt;	9.
Derivation:
Detecting	overflow	of	unsigned	addition
0
≤
x
,
y
≤
U
M
a
x
w
s
 
≐
x</p>
<ul>
<li></li>
</ul>
<p>w
u
y
9</p>
<ul>
<li></li>
</ul>
<h1>4
u
12</h1>
<p>5</p>
<p>Observe	that	
,	and	hence	if	
s
did	not	overflow,	we	will
surely	have	
s
≥	
x.
On	the	other	hand,	if	
s
did	overflow,	we
have	
.	Given	that	
y
&lt;	2
,	we	have	
,	and
hence	
.
Practice	Problem	
2.27
(solution	page	
152
)
Write	a	function	with	the	following	prototype:
This	function	should	return	1	if	arguments	
and	
can	be	added
without	causing	overflow.
Modular	addition	forms	a	mathematical	structure	known	as	an	
abelian
group
,	named	after	the	Norwegian	mathematician	Niels	Henrik	Abel
(1802–1829).	That	is,	it	is	commutative	(that's	where	the	“abelian”	part
comes	in)	and	associative;	it	has	an	identity	element	0,	and	every
element	has	an	additive	inverse.	Let	us	consider	the	set	of	
w
-bit	unsigned
numbers	with	addition	operation	
.	For	every	value	
x
,	there	must	be
some	value	
such	that	
.	This	additive	inverse
operation	can	be	characterized	as	follows:
x</p>
<ul>
<li></li>
</ul>
<h1>y
≥
x
s</h1>
<p>x</p>
<ul>
<li></li>
</ul>
<h1>y
−
2
w
w
y
−
2
w
&lt;
0
s</h1>
<p>x</p>
<ul>
<li></li>
</ul>
<p>(
y
−
2
w
)
&lt;
x</p>
<ul>
<li></li>
</ul>
<p>w
u
−
w
u
x</p>
<p>−
w
u
x
 </p>
<ul>
<li></li>
</ul>
<h1>w
u
 
x</h1>
<p>0</p>
<p>Principle:
Unsigned	negation
For	any	number	
x
such	that	0	≤	
x
&lt;	2
,	its	
w
-bit	unsigned
negation	
is	given	by	the	following:
This	result	can	readily	be	derived	by	case	analysis:
Derivation:
Unsigned	negation
When	
x
=	0,	the	additive	inverse	is	clearly	0.	For	
x
&gt;	0,
consider	the	value	2
–	
x
.	Observe	that	this	number	is	in
the	range	
.	We	can	also	see	that	
.	Hence	it	is	the	inverse	of	
x
under	
.
w
−
w
u
 
x</p>
<h1>−
w
u
x</h1>
<h1>{
x
,
x</h1>
<p>0
2
w
−
x
,
x
&gt;
0
(2.12)
w
0
&lt;
2
w
−
x
&lt;
2
w
(
x</p>
<ul>
<li></li>
</ul>
<h1>2
w
−
x
)
 
mod
 
2
w</h1>
<h1>2
w
 
mod
 
2
w</h1>
<p>0</p>
<ul>
<li></li>
</ul>
<p>w
u</p>
<p>Practice	Problem	
2.28
(solution	page	
152
)
We	can	represent	a	bit	pattern	of	length	
w
=	4	with	a	single	hex
digit.	For	an	unsigned	interpretation	of	these	digits,	use	
Equation
2.12
to	fill	in	the	following	table	giving	the	values	and	the	bit
representations	(in	hex)	of	the	unsigned	additive	inverses	of	the
digits	shown.
x
Hex
Decimal
Decimal
Hex
0</p>
<hr />
<hr />
<hr />
<p>5</p>
<hr />
<hr />
<hr />
<p>8</p>
<hr />
<hr />
<hr />
<p>D</p>
<hr />
<hr />
<hr />
<p>F</p>
<hr />
<hr />
<hr />
<p>2.3.2	
Two's-Complement	Addition
With	two's-complement	addition,	we	must	decide	what	to	do	when	the
result	is	either	too	large	(positive)	or	too	small	(negative)	to	represent.
Given	integer	values	
x
and	
y
in	the	range	
,	their	sum
is	in	the	range	
,	potentially	requiring	
w
+	1	bits	to
represent	exactly.	As	before,	we	avoid	ever-expanding	data	sizes	by
truncating	the	representation	to	
w
bits.	The	result	is	not	as	familiar
–
4
u
 
x
−
2
w
−
1
≤
x
,
y
≤
2
w
−
1
−
1
−
2
w
≤
x</p>
<ul>
<li></li>
</ul>
<p>y
≤
2
w
−
1
−
2</p>
<p>mathematically	as	modular	addition,	however.	Let	us	define	
to	be
the	result	of	truncating	the	integer	sum	
x
+	
y
to	be	
w
bits	long	and	then
viewing	the	result	as	a	two's-complement	number.
Principle:
Two's-complement	addition
For	integer	values	
x
and	
y
in	the	range	
This	principle	is	illustrated	in	
Figure	
2.24
,	where	the	sum	
x
+	
y
is
shown	on	the	left,	having	a	value	in	the	range	
,	and	the
result	of	truncating	the	sum	to	a	
w
-bit,	two's-complement	number	is
shown	on	the	right.	(The	labels	“Case	1”	to	“Case	4”	in	this	figure	are	for
the	case	analysis	of	the	formal	derivation	of	the	principle.)	When	the	sum
x
+	
y
exceeds	
TMax
(
Case	
4
),	we	say	that	
positive	overflow
has
occurred.	In	this	case,	the	effect	of	truncation	is	to	subtract	2
from	the
sum.	When	the	sum	
x
+	
y
is	less	than	
TMin
(Case	1),	we	say	that
negative	overflow
has	occurred.	In	this	case,	the	effect	of	truncation	is	to
add	2
to	the	sum.
x</p>
<ul>
<li></li>
</ul>
<p>w
t
 
y</p>
<p>−
2
w
−
1
≤
x
,
 
y
 
≤
 
2
w
−
1
−
1
:
x</p>
<ul>
<li></li>
</ul>
<h1>w
t
y</h1>
<p>{
x</p>
<ul>
<li></li>
</ul>
<p>y
−
2
w
,
2
w
−
1
≤
x</p>
<ul>
<li></li>
</ul>
<p>y
 </p>
<p>Positive
 
overflow
x</p>
<ul>
<li></li>
</ul>
<p>y
,
−
2
w
−
1
≤
x</p>
<ul>
<li></li>
</ul>
<p>y
&lt;
2
w
−
1
 </p>
<p>Normal
x</p>
<ul>
<li></li>
</ul>
<p>y</p>
<ul>
<li></li>
</ul>
<p>2
w
,
x</p>
<ul>
<li></li>
</ul>
<p>y
&lt;
−
2
w
−
1
 </p>
<p>Negative
 
overflow
(2.13)
−
2
w
≤
x</p>
<ul>
<li></li>
</ul>
<p>y
≤
2
w
−
2
w
w
w
w</p>
<p>The	
w
-bit	two's-complement	sum	of	two	numbers	has	the	exact	same	bit-
level	representation	as	the	unsigned	sum.	In	fact,	most	computers	use
the	same	machine	instruction	to	perform	either	unsigned	or	signed
addition.
Derivation:
Two's-complement	addition
Since	two's-complement	addition	has	the	exact	same	bit-
level	representation	as	unsigned	addition,	we	can
characterize	the	operation	
as	one	of	converting	its
arguments	to	unsigned,	performing	unsigned	addition,	and
then	converting	back	to	two's	complement:
Figure	
2.24	
Relation	between	integer</p>
<ul>
<li></li>
</ul>
<p>w
t</p>
<p>and	two's-complement	addition.
When	
x
+	
y
is	less	than	–2
,	there	is	a	negative	overflow.
When	it	is	greater	than	or	equal	to	2
,	there	is	a	positive
overflow.
By	
Equation	
2.6
,	we	can	write	
and	
as	
.	Using	the	property	that	
is
simply	addition	modulo	2
,	along	with	the	properties	of
modular	addition,	we	then	have
The	terms	
and	
drop	out	since	they	equal
0	modulo	2
.
To	better	understand	this	quantity,	let	us	define	
z
as	the
integer	sum	
as	
mod	2
,	and	
z
″	as	
.	The	value	
z
″	is	equal	to	
.	We	can	divide	the
analysis	into	four	cases	as	illustrated	in	
Figure	
2.24
:
1
.	
.	Then	we	will	have	
.	This
gives	
.	Examining	
Equation
2.7
,	we	see	that	
z
′	is	in	the	range	such	that	
z
″	=	
z
′.
This	is	the	case	of	negative	overflow.	We	have
added	two	negative	numbers	
x
and	
y
(that's	the	only
w
–1
w
–1
x</p>
<ul>
<li></li>
</ul>
<h1>w
t
y</h1>
<p>U
2
T
w
(
T
2
U
w
(
x
)</p>
<ul>
<li></li>
</ul>
<p>w
u
T
2
U
w
(
y
)
)
(2.14)
T
2
U
w
(
x
)
 
as
 
x
w
−
1
2
w</p>
<ul>
<li></li>
</ul>
<p>x
T
2
U
w
(
y
)</p>
<p>y
w
−
1
2
w</p>
<ul>
<li></li>
</ul>
<p>y</p>
<ul>
<li></li>
</ul>
<p>w
u</p>
<p>w
x</p>
<ul>
<li></li>
</ul>
<h1>w
t
y</h1>
<p>U
2
T
w
(
T
2
U
w
(
x
)</p>
<ul>
<li></li>
</ul>
<h1>w
u
T
2
U
w
(
y
)
)
 </h1>
<p>U
2
T
w
[
(
x
w
−
1
2
w</p>
<ul>
<li></li>
</ul>
<p>x</p>
<ul>
<li></li>
</ul>
<p>y
w
−
1
2
w</p>
<ul>
<li></li>
</ul>
<h1>y
)
 
mod
 
2
w
]
 </h1>
<p>U
2
T
w
[
(
x</p>
<ul>
<li></li>
</ul>
<p>y
)
 
mod
 
2
w
]
x
w
−
1
2
w</p>
<p>y
w
−
1
2
w</p>
<p>w
z
≐
x</p>
<ul>
<li></li>
</ul>
<p>y
,
 
z
'</p>
<p>z
'
≐
z</p>
<p>w
z
″
≐
U
2
T
w
(
z
′
)
x
 </p>
<ul>
<li></li>
</ul>
<h1>w
t
 
y
−
2
w
≤
z
&lt;
−
2
w
−
1
z
′</h1>
<p>z</p>
<ul>
<li></li>
</ul>
<p>2
w
0
≤
z
′
&lt;
−
2
w
−
1</p>
<ul>
<li></li>
</ul>
<h1>2
w</h1>
<p>2
w
−
1
w
–1</p>
<h1>way	we	can	have	
z
&lt;	–2
)	and	obtained	a
nonnegative	result	
.
2
.	
.	Then	we	will	again	have	
,
giving	
.	Examining
Equation	
2.7
,	we	see	that	
z
′	is	in	such	a	range
that	
,	and	therefore	
.
That	is,	our	two's-complement	sum	
z
″	equals	the
integer	sum	
x
+	
y
.
3
.	
.	Then	we	will	have	
z
′	=	
z
,	giving	
,	and	hence	
z
″	=	
z
′	=	
z.
Again,	the	two's-
complement	sum	
z
″	equals	the	integer	sum	
x
+	
y.
4
.	
.	We	will	again	have	
z
′	=	
z
,	giving	
.	But	in	this	range	we	have	
,
giving	
.	This	is	the	case	of	positive
overflow.	We	have	added	two	positive	numbers	
x
and	
y
(that's	the	only	way	we	can	have	
)	and
obtained	a	negative	result	
.
x
y
x
+	
y
Case
–8
–5
–13
3
1
[1000]
[1011]
[10011]
[0011]
–8
–8
–16
0
1
[1000]
[1000]
[10000]
[0000]
w
–1
z
′
′</h1>
<p>x</p>
<ul>
<li></li>
</ul>
<p>y</p>
<ul>
<li></li>
</ul>
<h1>2
w
−
2
w
−
1
≤
z
&lt;
0
z
′</h1>
<p>z</p>
<ul>
<li></li>
</ul>
<p>2
w
−
2
w
−
1</p>
<ul>
<li></li>
</ul>
<h1>2
w</h1>
<h1>2
w
−
1
≤
z
′
&lt;
2
w
z
′
′</h1>
<h1>z
′
−
2
w
z
′
′</h1>
<h1>z
′
−
2
w</h1>
<p>z</p>
<ul>
<li></li>
</ul>
<h1>2
w
−
2
w</h1>
<h1>z
0
≤
z
&lt;
2
w
−
1
0
≤
z
′
&lt;
2
w
−
1
2
w
−
1
≤
z
&lt;
2
w
2
w
−
1
≤
z
′
&lt;
2
w
z
′
′</h1>
<h1>z
′
−
2
w
z
′
′</h1>
<p>x</p>
<ul>
<li></li>
</ul>
<h1>y
−
2
w
z
≥
2
w
−
1
z
′
′</h1>
<p>x</p>
<ul>
<li></li>
</ul>
<p>y
−
2
w
x</p>
<ul>
<li></li>
</ul>
<p>4
t
y</p>
<p>–8
5
–3
–3
2
[1000]
[0101]
[11101]
[1101]
2
5
7
7
3
[0010]
[0101]
[00111]
[0111]
5
5
10
–6
4
[0101]
[0101]
[01010]
[1010]
Figure	
2.25	
Two's-complement	addition	examples.
The	bit-level	representation	of	the	4-bit	two's-complement	sum	can	be
obtained	by	performing	binary	addition	of	the	operands	and	truncating	the
result	to	4	bits.
As	illustrations	of	two's-complement	addition,	
Figure	
2.25
shows	some
examples	when	
w
=	4.	Each	example	is	labeled	by	the	case	to	which	it
corresponds	in	the	derivation	of	
Equation	
2.13
.	Note	that	2
=	16,	and
hence	negative	overflow	yields	a	result	16	more	than	the	integer	sum,
and	positive	overflow	yields	a	result	16	less.	We	include	bit-level
representations	of	the	operands	and	the	result.	Observe	that	the	result
can	be	obtained	by	performing	binary	addition	of	the	operands	and
truncating	the	result	to	4	bits.
Figure	
2.26
illustrates	two's-complement	addition	for	word	size	
w
=	4.
The	operands	range	between	–8	and	7.	When	
x
+	
y
&lt;	–8,	two's-
complement	addition	has	a	negative	overflow,	causing	the	sum	to	be
incremented	by	16.	When	–8	≤	
x
+	
y
&lt;	8,	the	addition	yields	
x
+	
y
.	When
x
+	
y
≥	8,	the	addition	has	a	positive	overflow,	causing	the	sum	to	be
4</p>
<p>decremented	by	16.	Each	of	these	three	ranges	forms	a	sloping	plane	in
the	figure.
Equation	
2.13
also	lets	us	identify	the	cases	where	overflow	has
occurred:
Principle:
Detecting	overflow	in	two's-complement	addition
For	
x
and	
y
in	the	range	
,	let	
.
Then	the	computation	of	
s
has	had	positive	overflow	if	and
only	if	
x
&gt;	0	and	
y
&gt;	0	but	
s
≤	0.	The	computation	has	had
negative	overflow	if	and	only	if	
x
&lt;	0	and	
y
&lt;	0	but	
s
≥	0.
Figure	
2.25
shows	several	illustrations	of	this	principle	for	
w
=	4.	The
first	entry	shows	a	case	of	negative	overflow,	where	two	negative
numbers	sum	to	a	positive	one.	The	final	entry	shows	a	case	of	positive
overflow,	where	two	positive	numbers	sum	to	a	negative	one.
T
M
i
n
w
&lt;
x
,
 
y
≤
T
M
a
x
w
s
≐
x</p>
<ul>
<li></li>
</ul>
<p>w
t
 
y</p>
<p>Figure	
2.26	
Two's-complement	addition.
With	a	4-bit	word	size,	addition	can	have	a	negative	overflow	when	
x
+	
y
&lt;	–8	and	a	positive	overflow	when	
x
+	
y
≥	8.
Derivation:
Detecting	overflow	of	two's-complement	addition</p>
<p>Let	us	first	do	the	analysis	for	positive	overflow.	If	both	
x
&gt;	0
and	
y
&gt;	0	but	
s
≤	0,	then	clearly	positive	overflow	has
occurred.	Conversely,	positive	overflow	requires	(1)	that	
x
&gt;
0	and	
y
&gt;	0	(otherwise,	
),	and	(2)	
s
≤	0	(from
Equation	
2.13
.)	A	similar	set	of	arguments	holds	for
negative	overflow.
Practice	Problem	
2.29
(solution	page	
152
)
Fill	in	the	following	table	in	the	style	of	
Figure	
2.25
.	Give	the
integer	values	of	the	5-bit	arguments,	the	values	of	both	their
integer	and	two's-complement	sums,	the	bit-level	representation	of
the	two's-complement	sum,	and	the	case	from	the	derivation	of
Equation	
2.13
.
x
y
x
+	
y
Case</p>
<hr />
<hr />
<hr />
<hr />
<hr />
<p>[10100]
[10001]</p>
<hr />
<hr />
<hr />
<hr />
<hr />
<hr />
<hr />
<hr />
<p>[11000]
[11000]</p>
<hr />
<hr />
<hr />
<hr />
<hr />
<hr />
<hr />
<hr />
<p>[10111]
[01000]</p>
<hr />
<hr />
<hr />
<hr />
<hr />
<hr />
<hr />
<hr />
<p>x</p>
<ul>
<li></li>
</ul>
<p>y
&lt;
T
M
a
x
w
x
 </p>
<ul>
<li></li>
</ul>
<p>5
t
y</p>
<p>[00010]
[00101]</p>
<hr />
<hr />
<hr />
<hr />
<hr />
<hr />
<hr />
<hr />
<p>[01100]
[00100]</p>
<hr />
<hr />
<hr />
<hr />
<hr />
<hr />
<hr />
<hr />
<p>Practice	Problem	
2.30
(solution	page	
153
)
Write	a	function	with	the	following	prototype:
This	function	should	return	1	if	arguments	
and	
can	be	added
without	causing	overflow.
Practice	Problem	
2.31
(solution	page	
153
)
Your	coworker	gets	impatient	with	your	analysis	of	the	overflow
conditions	for	two's-complement	addition	and	presents	you	with
the	following	implementation	of	</p>
<p>You	look	at	the	code	and	laugh.	Explain	why.
Practice	Problem	
2.32
(solution	page	
153
)
You	are	assigned	the	task	of	writing	code	for	a	function	
,
with	arguments	
and	
,	that	will	return	1	if	computing	
does
not	cause	overflow.	Having	just	written	the	code	for	
Problem
2.30
,	you	write	the	following:
For	what	values	of	
and	
will	this	function	give	incorrect	results?
Writing	a	correct	version	of	this	function	is	left	as	an	exercise
(
Problem	
2.74
).
2.3.3	
Two's-Complement	Negation</p>
<p>We	can	see	that	every	number	
x
in	the	range	
has	an
additive	inverse	under	
,	which	we	denote	
as	follows:
Principle:
Two's-complement	negation
For	
x
in	the	range	
,	its	two's-complement
negation	
is	given	by	the	formula
That	is,	for	
w
-bit,	two's-complement	addition,	
TMin
is	its
own	additive	in-verse,	while	any	other	value	
x
has	–
x
as	its
additive	inverse.
Derivation:
Two's-complement	negation
Observe	that	
.	This
would	cause	negative	overflow,	and	hence
.	For	values	of	
x
such	that	
x
T
M
i
n
w
≤
x
≤
T
M
a
x
w</p>
<p>−
w
t
 
−
w
t
 
x</p>
<p>T
M
i
n
w
≤
x
≤
T
M
a
x
w
−
w
t
 
x</p>
<h1>−
w
t
x</h1>
<h1>{
T
M
i
n
w
,
x</h1>
<p>T
M
i
n
w
−
x
x
&gt;
T
M
i
n
w
(2.15)
w
T
M
i
n
w</p>
<ul>
<li></li>
</ul>
<h1>T
M
i
n
w</h1>
<p>−
2
ω
−
1</p>
<ul>
<li></li>
</ul>
<h1>−
2
w
−
1</h1>
<p>−
2
w
T
M
i
n
w
 </p>
<ul>
<li></li>
</ul>
<h1>w
t
 
T
M
i
n
w
 </h1>
<p> 
−
2
w</p>
<ul>
<li></li>
</ul>
<h1>2
w</h1>
<p>0</p>
<blockquote>
</blockquote>
<p>TMin
,	the	value	–
x
can	also	be	represented	as	a	
w
-bit,
two's-complement	number,	and	their	sum	will	be	–
x
+	
x
=	0.
Practice	Problem	
2.33
(solution	page	
153
)
We	can	represent	a	bit	pattern	of	length	
w
=	4	with	a	single	hex
digit.	For	a	two's-complement	interpretation	of	these	digits,	fill	in
the	following	table	to	determine	the	additive	inverses	of	the	digits
shown:
x
Hex
Decimal
Decimal
Hex
0</p>
<hr />
<hr />
<hr />
<p>5</p>
<hr />
<hr />
<hr />
<p>8</p>
<hr />
<hr />
<hr />
<p>D</p>
<hr />
<hr />
<hr />
<p>F</p>
<hr />
<hr />
<hr />
<p>What	do	you	observe	about	the	bit	patterns	generated	by	two's-
complement	and	unsigned	(
Problem	
2.28
)	negation?
Web	Aside	DATA:TNEG	
it-level
w
−
4
t
 
x</p>
<p>representation	of	two's-complement
negation
There	are	several	clever	ways	to	determine	the	two's-complement
negation	of	a	value	represented	at	the	bit	level.	The	following	two
techniques	are	both	useful,	such	as	when	one	encounters	the
value	
when	debugging	a	program,	and	they	lend
insight	into	the	nature	of	the	two's-complement	representation.
One	technique	for	performing	two's-complement	negation	at	the
bit	level	is	to	complement	the	bits	and	then	increment	the	result.	In
C,	we	can	state	that	for	any	integer	value	x,	computing	the
expressions	
and	
will	give	identical	results.
Here	are	some	examples	with	a	4-bit	word	size:
[0101]
5
[1010]
–6
[1011]
–5
[0111]
7
[1000]
–8
[1001]
–7
[1100]
–4
[0011]
3
[0100]
4
[0000]
0
[1111]
–1
[0000]
0
[1000]
–8
[0111]
7
[1000]
–8
For	our	earlier	example,	we	know	that	the	complement	of	
is
and	the	complement	of	
is	
,	and	so	
is	the
two's-complement	representation	of	–6.
x
→
~
x
→
i
n
c
r
(
~
x
→
)</p>
<p>A	second	way	to	perform	two's-complement	negation	of	a	number
x
is	based	on	splitting	the	bit	vector	into	two	parts.	Let	
k
be	the
position	of	the	rightmost	1,	so	the	bit-level	representation	of	
x
has
the	form	
.	(This	is	possible	as	long	as
x
≠	0.)	The	negation	is	then	written	in	binary	form	as	
.	That	is,	we	complement	each	bit	to	the	left
of	bit	position	
k
.
We	illustrate	this	idea	with	some	4-bit	numbers,	where	we
highlight	the	rightmost	pattern	1,	0,	...,	0	in	italics:
x
–
x
[1
100
]
–4
[0
100
]
4
[
1000
]
–8
[
1000
]
–8
[010
1
]
5
[101
1
]
–5
[011
1
]
7
[100
1
]
–7
2.3.4	
Unsigned	Multiplication
Integers	
x
and	
y
in	the	range	
can	be	represented	as	
w
-bit
unsigned	numbers,	but	their	product	
x
·	
y
can	range	between	0	and	
.	This	could	require	as	many	as	2
w
bits	to	represent.
Instead,	unsigned	multiplication	in	C	is	defined	to	yield	the	
w
-bit	value
given	by	the	low-order	
w
bits	of	the	2
w
-bit	integer	product.	Let	us	denote
this	value	as	
.
[
x
w
−
1
,
x
w
−
2
,
…
,
x
k</p>
<ul>
<li></li>
</ul>
<p>1
,
1
,
0
,
…
0
]
[
∼
x
w
−
1
,
∼
x
w
−
2
,
…
∼
x
k</p>
<ul>
<li></li>
</ul>
<p>1
,
1
,
0
,
…
,
0
]
0
≤
x
,
y
≤
2
w
−
1</p>
<h1>(
2
w
−
1
)
2</h1>
<p>2
2
w
−
2
w</p>
<ul>
<li></li>
</ul>
<p>1</p>
<ul>
<li></li>
</ul>
<p>1
x
 
 </p>
<ul>
<li></li>
</ul>
<p>w
u
 
y</p>
<h1>Truncating	an	unsigned	number	to	
w
bits	is	equivalent	to	computing	its
value	modulo	2
,	giving	the	following:
Principle:
Unsigned	multiplication
For	
x
and	
y
such	that	
:
w
0
≤
x
,
y
≤
U
M
a
x
w
x
∗
w
u
y</h1>
<p>(
x
⋅
y
)
 
mod
 
2
w
(2.16)</p>
<p>2.3.5	
Two's-Complement
Multiplication
Integers	
x
and	
y
in	the	range	
can	be	represented	as
w
-bit	two's-complement	numbers,	but	their	product	
x
·	
y
can	range
between	
and	
.
This	could	require	as	many	as	2
w
bits	to	represent	in	two's-complement
form.	Instead,	signed	multiplication	in	C	generally	is	performed	by
truncating	the	2
w
-bit	product	to	
w
bits.	We	denote	this	value	as	
.
Truncating	a	two's-complement	number	to	
w
bits	is	equivalent	to	first
computing	its	value	modulo	2
and	then	converting	from	unsigned	to
two's	complement,	giving	the	following:
Principle:
Two's-complement	multiplication
For	
x
and	
y
such	that	
TMin
≤	
x,	y
≤	
TMax
:
−
2
w
−
1
≤
x
,
y
≤
2
w
−
1
−
1</p>
<h1>−
2
w
−
1
⋅
(
2
w
−
1
−
1
)</h1>
<p>−
2
2
w
−
2</p>
<ul>
<li></li>
</ul>
<p>2
w
−
1</p>
<h1>−
2
w
−
1
⋅
−
2
w
−
1</h1>
<p>2
2
w
−
2
x
 
 </p>
<ul>
<li></li>
</ul>
<h1>w
t
 
y
w
w
w
x
∗
w
t
y</h1>
<p>U
2
T
w
(
(
x
⋅
y
)
 
mod
 
2
w
)
(2.17)</p>
<p>We	claim	that	the	bit-level	representation	of	the	product	operation	is
identical	for	both	unsigned	and	two's-complement	multiplication,	as
stated	by	the	following	principle:
Principle:
Bit-level	equivalence	of	unsigned	and	two's-complement
multiplication
Let	
and	
be	bit	vectors	of	length	
w.
Define	integers	
x
and	
y
as	the	values	represented	by	these	bits	in	two's-
complement	form:	
and	
.	Define
nonnegative	integers	
x
′	and	
y
′	as	the	values	represented	by
these	bits	in	unsigned	form:	
and	
.	Then
As	illustrations,	
Figure	
2.27
shows	the	results	of	multiplying	different
3-bit	numbers.	For	each	pair	of	bit-level	operands,	we	perform	both
unsigned	and	two's-complement	multiplication,	yielding	6-bit	products,
and	then	truncate	these	to	3	bits.	The	unsigned	truncated	product	always
equals	
x
·	
y
mod	8.	The	bit-level	representations	of	both	truncated
products	are	identical	for	both	unsigned	and	two's-complement
multiplication,	even	though	the	full	6-bit	representations	differ.
x
→</p>
<p>y
→</p>
<h1 id="x"><a class="header" href="#x">x</a></h1>
<p>B
2
T
w
(
x
→
)</p>
<h1 id="y"><a class="header" href="#y">y</a></h1>
<h1>B
2
T
w
(
y
→
)
x
′</h1>
<p>B
2
U
w
(
x
→
)</p>
<h1>y
′</h1>
<h1>B
2
U
w
(
y
→
)
T
2
B
w
(
x
∗
w
t
y
)</h1>
<p>U
2
B
w
(
x
′
∗
w
u
y
′
)</p>
<h1>Mode
x
y
x
·	
y
Truncated	
x
·	
y
Unsigned
5
[101]
3
[011]
15
[001111]
7
[111]
Two's	complement
–3
[101]
3
[011]
–9
[110111]
–1
[111]
Unsigned	complement
4
[100]
7
[111]
28
[011100]
4
[100]
Two's	complement
–4
[100]
–1
[111]
4
[000100]
–4
[100]
Unsigned
3
[011]
3
[011]
9
[001001]
1
[001]
Two's	comp.
3
[011]
3
[011]
9
[001001]
1
[001]
Figure	
2.27	
Three-bit	unsigned	and	two's-complement	multiplication
examples.
Although	the	bit-level	representations	of	the	full	products	may	differ,
those	of	the	truncated	products	are	identical.
Derivation:
Bit-level	equivalence	of	unsigned	and	two's-complement
multiplication
From	
Equation	
2.6
,	we	have	
and	
.	Computing	the	product	of	these	values	modulo	2
gives	the	following:
x
′</h1>
<p>x</p>
<ul>
<li></li>
</ul>
<p>x
w
−
1
2
w</p>
<h1>y
′</h1>
<p>y</p>
<ul>
<li></li>
</ul>
<h1>y
w
−
1
2
w
w
(
x
′
⋅
y
′
)
 
mod
 
2
w</h1>
<p>[
(
x</p>
<ul>
<li></li>
</ul>
<p>x
w
−
1
2
w
)
⋅
(
y</p>
<ul>
<li></li>
</ul>
<h1>y
w
−
1
2
w
)
]
 
mod
 
2
w
 </h1>
<p>[
x
⋅
y</p>
<ul>
<li></li>
</ul>
<p>(
x
w
−
1
y</p>
<ul>
<li></li>
</ul>
<p>y
w
−
1
x
)
2
w</p>
<ul>
<li></li>
</ul>
<h1>x
w
−
1
2
2
w
]
 
mod
 
2
w
 </h1>
<p>(
x
⋅
y
)
 
mod
 
2
w
(2.18)
w
2w</p>
<p>The	terms	with	weight	2
and	2
drop	out	due	to	the
modulus	operator.	By	
Equation	
2.17
,	we	have
.	We	can	apply	the	operation
T2U
to	both	sides	to	get
Combining	this	result	with	
Equations	
2.16
and	
2.18
shows	that	
.	We	can
then	apply	
U2B
to	both	sides	to	get
Practice	Problem	
2.34
(solution	page	
153
)
Fill	in	the	following	table	showing	the	results	of	multiplying	different
3-bit	numbers,	in	the	style	of	
Figure	
2.27
:
Mode
x
y
x
·	
y
Unsigned</p>
<hr />
<p>[100]</p>
<hr />
<p>[101]</p>
<hr />
<hr />
<p>Two's
complement</p>
<hr />
<p>[100]</p>
<hr />
<p>[101]</p>
<hr />
<hr />
<p>Unsigned</p>
<hr />
<p>[010]</p>
<hr />
<p>[111]</p>
<hr />
<hr />
<p>Two's
complement</p>
<hr />
<p>[010]</p>
<hr />
<p>[111]</p>
<hr />
<hr />
<p>w
2w
x
 </p>
<ul>
<li></li>
</ul>
<h1>w
t
 
y</h1>
<h1>U
2
T
w
(
(
x
⋅
y
)
 
mod
 
2
w
)
w
T
2
U
w
(
x
∗
w
t
y
)</h1>
<h1>T
2
U
w
(
U
2
T
w
(
(
x
⋅
y
)
 
mod
 
2
w
)
)</h1>
<p>(
x
⋅
y
)
 
mod
 
2
w
T
2
U
w
(
x
 </p>
<ul>
<li></li>
</ul>
<h1>w
t
y
)</h1>
<h1>(
x
′
⋅
y
′
)
 
mod
 
2
w</h1>
<p>x
′
 </p>
<ul>
<li></li>
</ul>
<h1>w
u
 
y
′
w
U
2
B
w
(
T
2
U
w
(
x
∗
w
t
y
)
)</h1>
<h1>T
2
B
w
(
x
∗
w
t
y
)</h1>
<p>U
2
B
w
(
x
′
∗
w
t
y
′
)</p>
<p>Unsigned</p>
<hr />
<p>[110]</p>
<hr />
<p>[110]</p>
<hr />
<hr />
<p>Two's
complement</p>
<hr />
<p>[110]</p>
<hr />
<p>[110]</p>
<hr />
<hr />
<p>Practice	Problem	
2.35
(solution	page	
154
)
You	are	given	the	assignment	to	develop	code	for	a	function
that	will	determine	whether	two	arguments	can	be
multiplied	without	causing	overflow.	Here	is	your	solution:
You	test	this	code	for	a	number	of	values	of	
and	
,	and	it	seems
to	work	properly.	Your	coworker	challenges	you,	saying,	“If	I	can't
use	subtraction	to	test	whether	addition	has	overflowed	(see
Problem	
2.31
),	then	how	can	you	use	division	to	test	whether
multiplication	has	overflowed?”
Devise	a	mathematical	justification	of	your	approach,	along	the
following	lines.	First,	argue	that	the	case	
x
=	0	is	handled	correctly.
Otherwise,	consider	
w
-bit	numbers	
x
(
x
≠	0),	
y,	p
,	and	
q
,	where	
p
is	the	result	of	performing	two's-complement	multiplication	on	
x
and	
y
,	and	
q
is	the	result	of	dividing	
p
by	
x.</p>
<h1>1
.	
Show	that	
x
·	
y
,	the	integer	product	of	
x
and	
y
,	can	be
written	in	the	form	
,	where	
t
≠	0	if	and	only	if	the
computation	of	
p
overflows.
2
.	
Show	that	
p
can	be	written	in	the	form	
,	where	|
r
|	&lt;
|
x
|.
3
.	
Show	that	
q
=	
y
if	and	only	if	
r
=	
t
=	0.
Practice	Problem	
2.36
(solution	page	
154
)
For	the	case	where	data	type	
has	32	bits,	devise	a	version	of
(
Problem	
2.35
)	that	uses	the	64-bit	precision	of	data
type	
,	without	using	division.
Practice	Problem	
2.37
(solution	page	
155
)
You	are	given	the	task	of	patching	the	vulnerability	in	the	XDR
code	shown	in	the	aside	on	page	100	for	the	case	where	both	data
types	
and	
are	32	bits.	You	decide	to	eliminate	the
possibility	of	the	multiplication	overflowing	by	computing	the
number	of	bytes	to	allocate	using	data	type	
.	You	replace
the	original	call	to	
(line	9)	as	follows:
Aside	
Security	vulnerability	in	the
XDR	library
In	2002,	it	was	discovered	that	code	supplied	by	Sun
Microsystems	to	implement	the	XDR	library,	a	widely	used
facility	for	sharing	data	structures	between	programs,	had	a
x
⋅
y</h1>
<p>p</p>
<ul>
<li></li>
</ul>
<h1>t
2
w
p</h1>
<p>x
⋅
q</p>
<ul>
<li></li>
</ul>
<p>r</p>
<p>security	vulnerability	arising	from	the	fact	that	multiplication
can	overflow	without	any	notice	being	given	to	the	program.
Code	similar	to	that	containing	the	vulnerability	is	shown
below:</p>
<p>The	function	
is	designed	to	copy	
data	structures,	each	consisting	of	
bytes	into	a
buffer	allocated	by	the	function	on	line	9.	The	number	of
bytes	required	is	computed	as	
Imagine,	however,	that	a	malicious	programmer	calls	this
function	with	
being	1,048,577	(2
+	1)	and
being	4,096	(2
)	with	the	program	compiled	for
32	bits.	Then	the	multiplication	on	line	9	will	overflow,
causing	only	4096	bytes	to	be	allocated,	rather	than	the
4,294,971,392	bytes	required	to	hold	that	much	data.	The
loop	starting	at	line	15	will	attempt	to	copy	all	of	those
bytes,	overrunning	the	end	of	the	allocated	buffer,	and
therefore	corrupting	other	data	structures.	This	could	cause
the	program	to	crash	or	otherwise	misbehave.
The	Sun	code	was	used	by	almost	every	operating	system,
and	in	such	widely	used	programs	as	Internet	Explorer	and
the	Kerberos	authentication	system.	The	Computer
Emergency	Response	Team	(CERT),	an	organization	run
by	the	Carnegie	Mellon	Software	Engineering	Institute	to
track	security	vulnerabilities	and	breaches,	issued	advisory
“CA-2002-25,”	and	many	companies	rushed	to	patch	their
code.	Fortunately,	there	were	no	reported	security
breaches	caused	by	this	vulnerability.
A	similar	vulnerability	existed	in	many	implementations	of
the	library	function	
These	have	since	been
patched.	Unfortunately,	many	programmers	call	allocation
20
12</p>
<p>functions,	such	as	
,	using	arithmetic	expressions	as
arguments,	without	checking	these	expressions	for
overflow.	Writing	a	reliable	version	of	
is	left	as	an
exercise	(
Problem	
2.76
.)
Recall	that	the	argument	to	
has	type	
.
A
.	
Does	your	code	provide	any	improvement	over	the	original?
B
.	
How	would	you	change	the	code	to	eliminate	the
vulnerability?
2.3.6	
Multiplying	by	Constants
Historically,	the	integer	multiply	instruction	on	many	machines	was	fairly
slow,	requiring	10	or	more	clock	cycles,	whereas	other	integer	operations
—such	as	addition,	subtraction,	bit-level	operations,	and	shifting—
required	only	1	clock	cycle.	Even	on	the	Intel	Core	i7	Haswell	we	use	as
our	reference	machine,	integer	multiply	requires	3	clock	cycles.	As	a
consequence,	one	important	optimization	used	by	compilers	is	to	attempt
to	replace	multiplications	by	constant	factors	with	combinations	of	shift
and	addition	operations.	We	will	first	consider	the	case	of	multiplying	by	a
power	of	2,	and	then	we	will	generalize	this	to	arbitrary	constants.</p>
<p>Principle:
Multiplication	by	a	power	of	2
Let	
x
be	the	unsigned	integer	represented	by	bit	pattern	
.	Then	for	any	
k
≥	0,	the	
w
+	
k
-bit	unsigned
representation	of	
x
2
is	given	by	
,
where	
k
zeros	have	been	added	to	the	right.
So,	for	example,	11	can	be	represented	for	
w
=	4	as	[1011].	Shifting	this
left	by	
k
=	2	yields	the	6-bit	vector	[101100],	which	encodes	the	unsigned
number	11	·	4	=	44.
Derivation:
Multiplication	by	a	power	of	2
This	property	can	be	derived	using	
Equation	
2.1
:
[
x
w
−
1
,
x
w
−
2
,
…
,
x
0
]
k
[
x
w
−
1
,
x
w
−
2
,
…
,
x
0
,
0
,
…
,
0
]
B
2
U
w</p>
<ul>
<li></li>
</ul>
<h1>k
(
[
x
w
−
1
,
x
w
−
2
,
…
,
x
0
,
0
,
…
,
0
]
)</h1>
<h1>∑
i</h1>
<p>0
w
−
1
x
i
2
i</p>
<ul>
<li></li>
</ul>
<h1>k
 </h1>
<h1>[
∑
i</h1>
<h1>0
w
−
1
x
i
2
i
]
⋅
2
k
 </h1>
<p>x
2
k</p>
<p>When	shifting	left	by	
k
for	a	fixed	word	size,	the	high-order	
k
bits	are
discarded,	yielding
but	this	is	also	the	case	when	performing	multiplication	on	fixed-size
words.	We	can	therefore	see	that	shifting	a	value	left	is	equivalent	to
performing	unsigned	multiplication	by	a	power	of	2:
Principle:
Unsigned	multiplication	by	a	power	of	2
For	C	variables	
and	
with	unsigned	values	
x
and	
k
,
such	that	0	≤	
k
&lt;	
w
,	the	C	expression	
&lt;&lt;	
yields	the
value	
.
Since	the	bit-level	operation	of	fixed-size	two's-complement	arithmetic	is
equivalent	to	that	for	unsigned	arithmetic,	we	can	make	a	similar
statement	about	the	relationship	between	left	shifts	and	multiplication	by
a	power	of	2	for	two's-complement	arithmetic:
Principle:
[
x
w
−
k
−
1
,
x
w
−
k
−
2
,
…
,
x
0
,
0
,
…
,
0
]
x
 </p>
<ul>
<li></li>
</ul>
<p>w
u
 
2
k</p>
<p>Two's-complement	multiplication	by	a	power	of	2
For	C	variables	
and	
with	two's-complement	value	
x
and	unsigned	value	
k
,	such	that	0	≤	
k
&lt;	
w
,	the	C	expression
&lt;&lt;	
yields	the	value	
.
Note	that	multiplying	by	a	power	of	2	can	cause	overflow	with	either
unsigned	or	two's-complement	arithmetic.	Our	result	shows	that	even
then	we	will	get	the	same	effect	by	shifting.	Returning	to	our	earlier
example,	we	shifted	the	4-bit	pattern	[1011]	(numeric	value	11)	left	by	two
positions	to	get	[101100]	(numeric	value	44).	Truncating	this	to	4	bits
gives	[1100]	(numeric	value	12	=	44	mod	16).
Given	that	integer	multiplication	is	more	costly	than	shifting	and	adding,
many	C	compilers	try	to	remove	many	cases	where	an	integer	is	being
multiplied	by	a	constant	with	combinations	of	shifting,	adding,	and
subtracting.	For	example,	suppose	a	program	contains	the	expression
.	Recognizing	that	14	=	2
+	2
+	2
,	the	compiler	can	rewrite	the
multiplication	as	(
)	+	(
)	+	(
),	replacing	one	multiplication
with	three	shifts	and	two	additions.	The	two	computations	will	yield	the
same	result,	regardless	of	whether	
is	unsigned	or	two's	complement,
and	even	if	the	multiplication	would	cause	an	overflow.	Even	better,	the
compiler	can	also	use	the	property	14	=	2
–	2
to	rewrite	the
multiplication	as	(
)	–	(
),	requiring	only	two	shifts	and	a
subtraction.
x
 </p>
<ul>
<li></li>
</ul>
<p>w
t
 
2
k
3
2
1
4
1</p>
<p>Practice	Problem	
2.38
(solution	page	
155
)
As	we	will	see	in	
Chapter	
3
,	the	
LEA</p>
<p>instruction	can	perform
computations	of	the	form	
,	where	
is	either	0,	1,	2,	or
3,	and	
is	either	0	or	some	program	value.	The	compiler	often
uses	this	instruction	to	perform	multiplications	by	constant	factors.
For	example,	we	can	compute	
.
Considering	cases	where	
is	either	0	or	equal	to	
,	and	all
possible	values	of	
,	what	multiples	of	
can	be	computed	with	a
single	
LEA</p>
<p>instruction?
Generalizing	from	our	example,	consider	the	task	of	generating	code	for
the	expression	
*	
K
,	for	some	constant	
K.
The	compiler	can	express	the
binary	representation	of	
K
as	an	alternating	sequence	of	zeros	and	ones:
For	example,	14	can	be	written	as	[(0	...	0)(111)(0)].	Consider	a	run	of
ones	from	bit	position	
n
down	to	bit	position	
m
(
n
≥	
m
).	(For	the	case	of
14,	we	have	
n
=	3	and	
m
=	1.)	We	can	compute	the	effect	of	these	bits	on
the	product	using	either	of	two	different	forms:
Form	A:	
Form	B:	
By	adding	together	the	results	for	each	run,	we	are	able	to	compute	
*	
K
without	any	multiplications.	Of	course,	the	trade-off	between	using
combinations	of	shifting,	adding,	and	subtracting	versus	a	single
[
(
0
…
0
)
(
1
…
1
)
(
0
…
0
)
⋯
(
1
…
1
)
]</p>
<p>multiplication	instruction	depends	on	the	relative	speeds	of	these
instructions,	and	these	can	be	highly	machine	dependent.	Most	compilers
only	perform	this	optimization	when	a	small	number	of	shifts,	adds,	and
subtractions	suffice.
Practice	Problem	
2.39
(solution	page	
156
)
How	could	we	modify	the	expression	for	form	B	for	the	case	where
bit	position	
n
is	the	most	significant	bit?
Practice	Problem	
2.40
(solution	page	
156
)
For	each	of	the	following	values	of	
K
,	find	ways	to	express	
*	
K
using	only	the	specified	number	of	operations,	where	we	consider
both	additions	and	subtractions	to	have	comparable	cost.	You	may
need	to	use	some	tricks	beyond	the	simple	form	A	and	B	rules	we
have	considered	so	far.
K
Shifts
Add/Subs
Expression
6
2
1</p>
<hr />
<p>31
1
1</p>
<hr />
<p>–6
2
1</p>
<hr />
<p>55
2
2</p>
<hr />
<p>Practice	Problem	
2.41
(solution	page	
156
)</p>
<p>For	a	run	of	ones	starting	at	bit	position	
n
down	to	bit	position	
m
(
n
≥	
m
),	we	saw	that	we	can	generate	two	forms	of	code,	A	and	B.
How	should	the	compiler	decide	which	form	to	use?
2.3.7	
Dividing	by	Powers	of	2
Integer	division	on	most	machines	is	even	slower	than	integer
multiplication—requiring	30	or	more	clock	cycles.	Dividing	by	a	power	of
2	can	also	be	performed
(binary)
decimal
12,340/2
0
0011000000110100
12,340
12,340.0
1
0001100000011010
6,170
6,170.0
4
0000001100000011
771
771.25
8
0000000000110000
48
48.203125
Figure	
2.28	
Dividing	unsigned	numbers	by	powers	of	2.
The	examples	illustrate	how	performing	a	logical	right	shift	by	
has	the
same	effect	as	dividing	by	
and	then	rounding	toward	zero.
using	shift	operations,	but	we	use	a	right	shift	rather	than	a	left	shift.	The
two	different	right	shifts—logical	and	arithmetic—serve	this	purpose	for
unsigned	and	two's-complement	numbers,	respectively.</p>
<p>Integer	division	always	rounds	toward	zero.	To	define	this	precisely,	let	us
introduce	some	notation.	For	any	real	number	
a
,	define	
⌊
a
⌋
to	be	the
unique	integer	
a
′	such	that	
.	As	examples,	
.	Similarly,	define	
⌈
a
⌉
to	be	the	unique	integer	
a
′	such	that	
.	As	examples,	
,	and	
⌈
3
⌉
=	3.	For	
x
≥	0	and	
y
&gt;	0,	integer	division	should	yield	
⌊
x
/
y
⌋
,	while	for	
x
&lt;	0	and	
y
&gt;	0,	it	should
yield	
⌈
x
/
y
⌉
.	That	is,	it	should	round	down	a	positive	result	but	round	up	a
negative	one.
The	case	for	using	shifts	with	unsigned	arithmetic	is	straightforward,	in
part	because	right	shifting	is	guaranteed	to	be	performed	logically	for
unsigned	values.
Principle:
Unsigned	division	by	a	power	of	2
For	C	variables	
and	
with	unsigned	values	
x
and	
k
,
such	that	0	≤	
k
&lt;	
w
,	the	C	expression	
yields	the
value	
⌊
x
/2
⌋
.
As	examples,	
Figure	
2.28
shows	the	effects	of	performing	logical	right
shifts	on	a	16-bit	representation	of	12,340	to	perform	division	by	1,	2,	16,
and	256.	The	zeros	shifted	in	from	the	left	are	shown	in	italics.	We	also
show	the	result	we	would	obtain	if	we	did	these	divisions	with	real
a
′
≤
a
&lt;
a
′</p>
<ul>
<li></li>
</ul>
<h1>1
⌊
3.14
⌋</h1>
<h1>3
,
 
⌊
−
3.14
⌋</h1>
<h1>−
4
,
 
and
 
⌊
3
⌋</h1>
<h1>3
a
′
−
1
&lt;
a
≤
a
′
⌈
3.14
⌉</h1>
<h1>4
,
⌈
−
3.14
⌉</h1>
<p>−
3
k</p>
<h2>arithmetic.	These	examples	show	that	the	result	of	shifting	consistently
rounds	toward	zero,	as	is	the	convention	for	integer	division.
Derivation:
Unsigned	division	by	a	power	of	2
Let	
x
be	the	unsigned	integer	represented	by	bit	pattern	
,	and	let	
k
be	in	the	range	0	≤	
k
&lt;	
w.
Let	
x
′
be	the	unsigned	number	with	
w
–	
k
-bit	representation	
,	and	let	
x
″	be	the	unsigned	number	with	
k</h2>
<h1>bit	representation	
.	We	can	therefore	see	that
,	and	that	
.	It	therefore	follows	that	
⌊
x
/2
⌋</h1>
<h1>x
′.
Performing	a	logical	right	shift	of	bit	vector	
by	
k
yields	the	bit	vector
(binary)
decimal
–12340/2
0
–12,340
–12,340.0
1
–6,170
–6,170.0
4
–772
–771.25
8
–49
–48.203125
[
x
w
−
1
,
x
w
−
2
,
…
,
x
0
]
[
x
w
−
1
,
x
w
−
2
,
…
,
x
k
]
[
x
k
−
1
,
…
,
x
0
]
x</h1>
<p>2
k
x
′</p>
<ul>
<li></li>
</ul>
<p>x
′
′
0
≤
x
′
′
&lt;
2
k
k
[
x
w
−
1
,
x
w
−
2
,
…
,
x
0
]</p>
<p>[
0
,
…
,
0
,
x
w
−
1
,
x
w
−
2
,
…
,
x
k
]
k</p>
<p>Figure	
2.29	
Applying	arithmetic	right
shift.
The	examples	illustrate	that	arithmetic	right	shift	is	similar	to
division	by	a	power	of	2,	except	that	it	rounds	down	rather
than	toward	zero.
This	bit	vector	has	numeric	value	
x
′,	which	we	have	seen	is
the	value	that	would	result	by	computing	the	expression	
.
The	case	for	dividing	by	a	power	of	2	with	two's-complement	arithmetic	is
slightly	more	complex.	First,	the	shifting	should	be	performed	using	an
arithmetic
right	shift,	to	ensure	that	negative	values	remain	negative.	Let
us	investigate	what	value	such	a	right	shift	would	produce.
Principle:
Two's-complement	division	by	a	power	of	2,	rounding	down
Let	C	variables	
and	
have	two's-complement	value	
x
and	unsigned	value	
k
,	respectively,	such	that	0	≤	
k
&lt;	
w.
The
C	expression	
,	when	the	shift	is	performed
arithmetically,	yields	the	value	
⌊
x
/2
⌋
.
k</p>
<h1>For	
x
≥	0,	variable	
has	0	as	the	most	significant	bit,	and	so	the	effect	of
an	arithmetic	shift	is	the	same	as	for	a	logical	right	shift.	Thus,	an
arithmetic	right	shift	by	
k
is	the	same	as	division	by	2
for	a	nonnegative
number.	As	an	example	of	a	negative	number,	
Figure	
2.29
shows	the
effect	of	applying	arithmetic	right	shift	to	a	16-bit	representation	of	–
12,340	for	different	shift	amounts.	For	the	case	when	no	rounding	is
required	(
k
=	1),	the	result	will	be	
x
/2
.	When	rounding	is	required,	shifting
causes	the	result	to	be	rounded	downward.	For	example,	the	shifting
right	by	four	has	the	effect	of	rounding	–771.25	down	to	–772.	We	will
need	to	adjust	our	strategy	to	handle	division	for	negative	values	of	
x.
Derivation:
Two's-complement	division	by	a	power	of	2,	rounding	down
Let	
x
be	the	two's-complement	integer	represented	by	bit
pattern	
,	and	let	
k
be	in	the	range	0	≤	
k
&lt;
w.
Let	
x
′	be	the	two's-complement	number	represented	by
the	
w
–	
k
bits	
,	and	let	
x
″	be	the	
unsigned
number	represented	by	the	low-order	
k
bits	
.	By
a	similar	analysis	as	the	unsigned	case,	we	have	
and	
,	giving	
x
′	=	
⌊
x
/2
⌋
.	Furthermore,	observe	that
shifting	bit	vector	
right	
arithmetically
by	
k
yields	the	bit	vector
k
k
[
x
w
−
1
,
x
w
−
2
,
…
,
x
0
]
[
x
w
−
1
,
x
w
−
2
,
…
,
x
k
]
[
x
k
−
1
,
…
,
x
0
]
x</h1>
<p>2
k
x
′</p>
<ul>
<li></li>
</ul>
<p>x
′
′
0
≤
x
′
′
&lt;
2
k
k
[
x
w
−
1
,
x
w
−
2
,
…
,
x
0
]</p>
<p>which	is	the	sign	extension	from	
w
–	
k
bits	to	
w
bits	of	
.	Thus,	this	shifted	bit	vector	is	the	two's-
complement	representation	of	
⌊
x
/2
⌋
.
Bias
–12,340	+	bias	(binary)
(binary)
Decimal
–12,340/2
0
0
–12,340
–12,340.0
1
1
–6,170
–6,170.0
4
15
–771
–771.25
8
255
–48
–48.203125
Figure	
2.30	
Dividing	two's-complement	numbers	by	powers	of	2.
By	adding	a	bias	before	the	right	shift,	the	result	is	rounded	toward	zero.
We	can	correct	for	the	improper	rounding	that	occurs	when	a	negative
number	is	shifted	right	by	“biasing”	the	value	before	shifting.
Principle:
Two's-complement	division	by	a	power	of	2,	rounding	up
[
x
w
−
1
,
…
,
x
w
−
1
,
x
w
−
1
,
x
w
−
2
,
…
,
x
k
]
[
x
w
−
1
,
x
w
−
2
,
…
,
x
k
]
k
k</p>
<p>Let	C	variables	
and	
have	two's-complement	value	
x
and	unsigned	value	
k
,	respectively,	such	that	0	≤	
k
&lt;	
w.
The
C	expression	
,	when	the	shift	is
performed	arithmetically,	yields	the	value	
⌈
x
/2
⌉
.
Figure	
2.30
demonstrates	how	adding	the	appropriate	bias	before
performing	the	arithmetic	right	shift	causes	the	result	to	be	correctly
rounded.	In	the	third	column,	we	show	the	result	of	adding	the	bias	value
to	–12,340,	with	the	lower	
k
bits	(those	that	will	be	shifted	off	to	the	right)
shown	in	italics.	We	can	see	that	the	bits	to	the	left	of	these	may	or	may
not	be	incremented.	For	the	case	where	no	rounding	is	required	(
k
=	1),
adding	the	bias	only	affects	bits	that	are	shifted	off.	For	the	cases	where
rounding	is	required,	adding	the	bias	causes	the	upper	bits	to	be
incremented,	so	that	the	result	will	be	rounded	toward	zero.
The	biasing	technique	exploits	the	property	that	
⌈
x/y
⌉
=	
⌊
(
x
+	
y
–1)/
y
⌋
for
integers	
x
and	
y
such	that	
y
&gt;	0.	As	examples,	when	
x
=	–30	and	
y
=	4,
we	have	
x
+	
y
–	1	=	–27	and	
⌈
–30/4
⌉
=	–7	=	
⌊
–27/4
⌋
.	When	
x
=	–32	and	
y
=	4,	we	have	
x
+	
y
–	1	=	–29	and	
⌈
–32/4
⌉
=	–8	=	
⌊
–29/4
⌋
.
Derivation:
Two's-complement	division	by	a	power	of	2,	rounding	up
k</p>
<p>To	see	that	
⌈
x/y
⌉
=	
⌊
(
x
+	
y
–	1)/
y
⌋
,	suppose	that	
x
=	
qy
+	
r
,
where	0	≤	
r
&lt;	
y
,	giving	(
x
+	
y
–	1)/
y
=	
q
+	(
r
+	
y
–	1)/
y
,	and
so	
⌊
(
x
+	
y
–	1)/
y
⌋
=	
q
+	[(
r
+	
y
–	1)/
y
⌋
.	The	latter	term	will
equal	0	when	
r
=	0	and	1	when	
r
&gt;	0.	That	is,	by	adding	a
bias	of	
y
–	1	to	
x
and	then	rounding	the	division	downward,
we	will	get	
q
when	
y
divides	
x
and	
q
+	1	otherwise.
Returning	to	the	case	where	
y
=	2
,	the	C	expression	
—	1	yields	the	value	
x
+	2
–	1.	Shifting	this	right
arithmetically	by	
k
therefore	yields	
⌈
x
/2
⌉
.
These	analyses	show	that	for	a	two's-complement	machine	using
arithmetic	right	shifts,	the	C	expression
will	compute	the	value	
x
/2
.
Practice	Problem	
2.42
(solution	page	
156
)
Write	a	function	
that	returns	the	value	
for	integer
argument	
.	Your	function	should	not	use	division,	modulus,
multiplication,	any	conditionals	(
),	any	comparison
operators	(e.g.,	&lt;,	&gt;,	or	==),	or	any	loops.	You	may	assume	that
k
k
k
k</p>
<p>data	type	
is	32	bits	long	and	uses	a	two's-complement
representation,	and	that	right	shifts	are	performed	arithmetically.
We	now	see	that	division	by	a	power	of	2	can	be	implemented	using
logical	or	arithmetic	right	shifts.	This	is	precisely	the	reason	the	two	types
of	right	shifts	are	available	on	most	machines.	Unfortunately,	this
approach	does	not	generalize	to	division	by	arbitrary	constants.	Unlike
multiplication,	we	cannot	express	division	by	arbitrary	constants	
K
in
terms	of	division	by	powers	of	2.
Practice	Problem	
2.43
(solution	page	
157
)
In	the	following	code,	we	have	omitted	the	definitions	of	constants
and	
We	compiled	this	code	for	particular	values	of	
and	
.	The
compiler	optimized	the	multiplication	and	division	using	the
methods	we	have	discussed.	The	following	is	a	translation	of	the
generated	machine	code	back	into	C:</p>
<p>What	are	the	values	of	
and	
?
2.3.8	
Final	Thoughts	on	Integer
Arithmetic
As	we	have	seen,	the	“integer”	arithmetic	performed	by	computers	is
really	a	form	of	modular	arithmetic.	The	finite	word	size	used	to	represent
numbers	
limits	the	range	of	possible	values,	and	the	resulting	operations
can	overflow.	We	have	also	seen	that	the	two's-complement
representation	provides	a	clever	way	to	represent	both	negative	and
positive	values,	while	using	the	same	bit-level	implementations	as	are
used	to	perform	unsigned	arithmetic—operations	such	as	addition,
subtraction,	multiplication,	and	even	division	have	either	identical	or	very
similar	bit-level	behaviors,	whether	the	operands	are	in	unsigned	or
two's-complement	form.</p>
<p>We	have	seen	that	some	of	the	conventions	in	the	C	language	can	yield
some	surprising	results,	and	these	can	be	sources	of	bugs	that	are	hard
to	recognize	or	understand.	We	have	especially	seen	that	the	unsigned
data	type,	while	conceptually	straightforward,	can	lead	to	behaviors	that
even	experienced	programmers	do	not	expect.	We	have	also	seen	that
this	data	type	can	arise	in	unexpected	ways—for	example,	when	writing
integer	constants	and	when	invoking	library	routines.
Practice	Problem	
2.44
(solution	page	
157
)
Assume	data	type	
is	32	bits	long	and	uses	a	two's-
complement	representation	for	signed	values.	Right	shifts	are
performed	arithmetically	for	signed	values	and	logically	for
unsigned	values.	The	variables	are	declared	and	initialized	as
follows:
For	each	of	the	following	C	expressions,	either	(1)	argue	that	it	is
true	(evaluates	to	1)	for	all	values	of	
and	
,	or	(2)	give	values	of
and	
for	which	it	is	false	(evaluates	to	0):
A
.	
B
.	</p>
<p>C
.	
D
.	
E
.	
F
.	
G
.	</p>
<p>2.4	
Floating	Point
A	floating-point	representation	encodes	rational	numbers	of	the	form	
V
=
x
×	2
.
It	is	useful	for	performing	computations	involving	very	large
numbers	(|
V
|	
≫
0),
Aside	
The	IEEE
The	Institute	of	Electrical	and	Electronics	Engineers	(IEEE—
pronounced	“eye-triple-ee”)	is	a	professional	society	that
encompasses	all	of	electronic	and	computer	technology.	It
publishes	journals,	sponsors	conferences,	and	sets	up
committees	to	define	standards	on	topics	ranging	from	power
transmission	to	software	engineering.	Another	example	of	an
IEEE	standard	is	the	802.11	standard	for	wireless	networking.
numbers	very	close	to	0	(|
V
|	
≪
1),	and	more	generally	as	an
approximation	to	real	arithmetic.
Up	until	the	1980s,	every	computer	manufacturer	devised	its	own
conventions	for	how	floating-point	numbers	were	represented	and	the
details	of	the	operations	performed	on	them.	In	addition,	they	often	did
not	worry	too	much	about	the	accuracy	of	the	operations,	viewing	speed
and	ease	of	implementation	as	being	more	critical	than	numerical
precision.
All	of	this	changed	around	1985	with	the	advent	of	IEEE	Standard	754,	a
carefully	crafted	standard	for	representing	floating-point	numbers	and	the
y</p>
<p>operations	performed	on	them.	This	effort	started	in	1976	under	Intel's
sponsorship	with	the	design	of	the	8087,	a	chip	that	provided	floating-
point	support	for	the	8086	processor.	Intel	hired	William	Kahan,	a
professor	at	the	University	of	California,	Berkeley,	as	a	consultant	to	help
design	a	floating-point	standard	for	its	future	processors.	They	allowed
Kahan	to	join	forces	with	a	committee	generating	an	industry-wide
standard	under	the	auspices	of	the	Institute	of	Electrical	and	Electronics
Engineers	(IEEE).	The	committee	ultimately	adopted	a	standard	close	to
the	one	Kahan	had	devised	for	Intel.	Nowadays,	virtually	all	computers
support	what	has	become	known	as	
IEEE	floating	point.
This	has	greatly
improved	the	portability	of	scientific	application	programs	across	different
machines.
In	this	section,	we	will	see	how	numbers	are	represented	in	the	IEEE
floating-point	format.	We	will	also	explore	issues	of	
rounding
,	when	a
number	cannot	be	represented	exactly	in	the	format	and	hence	must	be
adjusted	upward	or	downward.	We	will	then	explore	the	mathematical
properties	of	addition,	multiplication,	and	relational	operators.	Many
programmers	consider	floating	point	to	be	at	best	uninteresting	and	at
worst	arcane	and	incomprehensible.	We	will	see	that	since	the	IEEE
format	is	based	on	a	small	and	consistent	set	of	principles,	it	is	really
quite	elegant	and	understandable.
2.4.1	
Fractional	Binary	Numbers
A	first	step	in	understanding	floating-point	numbers	is	to	consider	binary
numbers	having	fractional	values.	Let	us	first	examine	the	more	familiar
decimal	notation.	Decimal	notation	uses	a	representation	of	the	form</p>
<h1>Figure	
2.31	
Fractional	binary	representation.
Digits	to	the	left	of	the	binary	point	have	weights	of	the	form	2
,	while
those	to	the	right	have	weights	of	the	form	1/2
.
where	each	decimal	digit	
d
ranges	between	0	and	9.	This	notation
represents	a	value	
d
defined	as
The	weighting	of	the	digits	is	defined	relative	to	the	decimal	point	symbol
(‘.'),	meaning	that	digits	to	the	left	are	weighted	by	nonnegative	powers	of
10,	giving	integral	values,	while	digits	to	the	right	are	weighted	by
negative	powers	of	10,	giving	fractional	values.	For	example,	12.34
represents	the	number	
.
d
m
d
m
−
1
…
d
1
d
0
.
 
d
−
1
 
d
−
2
…
d
−
n
i
i
i
d</h1>
<h1>∑
i</h1>
<p>−
n
m
10
i
×
d
i
10
1
×
10
1</p>
<ul>
<li></li>
</ul>
<p>2
×
10
0</p>
<ul>
<li></li>
</ul>
<p>3
×
10
−
1</p>
<ul>
<li></li>
</ul>
<h1>4
×
10
−
2</h1>
<p>12
34
100</p>
<p>By	analogy,	consider	a	notation	of	the	form
where	each	binary	digit,	or	bit,	
b
ranges	between	0	and	1,	as	is	illustrated
in	
Figure	
2.31
.	This	notation	represents	a	number	
b
defined	as
The	symbol	‘.’	now	becomes	a	
binary	point
,	with	bits	on	the	left	being
weighted	by	nonnegative	powers	of	2,	and	those	on	the	right	being
weighted	by	negative	powers	of	2.	For	example,	101.11
represents	the
number	
.
One	can	readily	see	from	
Equation	
2.19
that	shifting	the	binary	point
one	position	to	the	left	has	the	effect	of	dividing	the	number	by	2.	For
example,	while	101.11
represents	the	number	
,	10.111
represents
the	number	
.	
Similarly,	shifting	the	binary	point	one
position	to	the	right	has	the	effect	of	multiplying	the	number	by	2.	For
example,	1011.1
represents	the	number	
.
Note	that	numbers	of	the	form	0.11	·	·	·	1
represent	numbers	just	below</p>
<ol>
<li>For	example,	0.111111
represents	
.	We	will	use	the	shorthand
notation	1.0	—	
∊</li>
</ol>
<p>to	represent	such	values.
Assuming	we	consider	only	finite-length	encodings,	decimal	notation
cannot	represent	numbers	such	as	
and	
exactly.	Similarly,	fractional
binary	notation	can	only	represent	numbers	that	can	be	written	
x
×	2
.
Other	values	can	only	be	approximated.	For	example,	the	number	
can
b
m
 
b
m
−
1
…
 
b
1
 
b
0
 
.
 
b
−
1
 
b
−
2
…
 
b
−
n</p>
<ul>
<li></li>
</ul>
<h1>1
 
b
−
n
i
b</h1>
<h1>∑
i</h1>
<p>−
n
m
2
i
×
b
i
(2.19)
2
1
×
2
2</p>
<ul>
<li></li>
</ul>
<p>0
×
2
1</p>
<ul>
<li></li>
</ul>
<p>1
×
2
0</p>
<ul>
<li></li>
</ul>
<p>1
×
2
−
1</p>
<ul>
<li></li>
</ul>
<h1>1
×
2
−
2</h1>
<p>4</p>
<ul>
<li></li>
</ul>
<p>0</p>
<ul>
<li></li>
</ul>
<p>1</p>
<ul>
<li></li>
</ul>
<p>1
2</p>
<ul>
<li></li>
</ul>
<h1>1
4</h1>
<p>5
3
4
2
5
3
4
2
2</p>
<ul>
<li></li>
</ul>
<p>0</p>
<ul>
<li></li>
</ul>
<p>1
2</p>
<ul>
<li></li>
</ul>
<p>1
4</p>
<ul>
<li></li>
</ul>
<h1>1
8</h1>
<p>2
7
8
2
8</p>
<ul>
<li></li>
</ul>
<p>0</p>
<ul>
<li></li>
</ul>
<p>2</p>
<ul>
<li></li>
</ul>
<p>1</p>
<ul>
<li></li>
</ul>
<h1>1
2</h1>
<p>11
1
2
2
2
63
64
1
3
5
7
y
1
5</p>
<p>be	represented	exactly	as	the	fractional	decimal	number	0.20.	As	a
fractional	binary	number,	however,	we	cannot	represent	it	exactly	and
instead	must	approximate	it	with	increasing	accuracy	by	lengthening	the
binary	representation:
Representation
Value
Decimal
0.0
0.0
0.01
0.25
0.010
0.25
0.0011
0.1875
0.00110
0.1875
0.001101
0.203125
0.0011010
0.203125
0.00110011
0.19921875
Practice	Problem	
2.45
(solution	page	
157
)
Fill	in	the	missing	information	in	the	following	table:
Fractional	value
Binary	representation
Decimal	representation
0.001
0.125</p>
<hr />
<hr />
<p>2
0
2
10
2
1
4
10
2
2
8
10
2
3
16
10
2
6
32
10
2
13
64
10
2
26
128
10
2
51
256
10
1
8
3
4</p>
<hr />
<hr />
<hr />
<p>10.1011</p>
<hr />
<hr />
<p>1.001</p>
<hr />
<hr />
<hr />
<p>5.875</p>
<hr />
<hr />
<p>3.1875
Practice	Problem	
2.46
(solution	page	
158
)
The	imprecision	of	floating-point	arithmetic	can	have	disastrous
effects.	On	February	25,	1991,	during	the	first	Gulf	War,	an
American	Patriot	Missile	battery	in	Dharan,	Saudi	Arabia,	failed	to
intercept	an	incoming	Iraqi	Scud	missile.	The	Scud	struck	an
American	Army	barracks	and	killed	28	soldiers.	The	US	General
Accounting	Office	(GAO)	conducted	a	detailed	analysis	of	the
failure	[
76
]	and	determined	that	the	underlying	cause	was	an
imprecision	in	a	numeric	calculation.	In	this	exercise,	you	will
reproduce	part	of	the	GAO's	analysis.
The	Patriot	system	contains	an	internal	clock,	implemented	as	a
counter	that	is	incremented	every	0.1	seconds.	To	determine	the
time	in	seconds,	the	program	would	multiply	the	value	of	this
counter	by	a	24-bit	quantity	that	was	a	fractional	binary
approximation	to	
.	In	particular,	the	binary	representation	of
is	the	nonterminating	sequence	0.000110011[0011]...
,	where
the	portion	in	brackets	is	repeated	indefinitely.	The	program
approximated	0.1,	as	a	value	
x
,	by	considering	just	the	first	23	bits
of	the	sequence	to	the	right	of	the	binary	point:	
x
=
0.00011001100110011001100.	(See	
Problem	
2.51
for	a
5
16
1
10
1
10</p>
<p>2</p>
<p>discussion	of	how	they	could	have	approximated	0.1	more
precisely.)
A
.	
What	is	the	binary	representation	of	0.1	–	
x
?
B
.	
What	is	the	approximate	decimal	value	of	0.1	–	
x
?
C
.	
The	clock	starts	at	0	when	the	system	is	first	powered	up
and	keeps	counting	up	from	there.	In	this	case,	the	system
had	been	running	for	around	100	hours.	What	was	the
difference	between	the	actual	time	and	the	time	computed
by	the	software?
D
.	
The	system	predicts	where	an	incoming	missile	will	appear
based	on	its	velocity	and	the	time	of	the	last	radar
detection.	Given	that	a	Scud	travels	at	around	2,000	meters
per	second,	how	far	off	was	its	prediction?
Normally,	a	slight	error	in	the	absolute	time	reported	by	a	clock
reading	would	not	affect	a	tracking	computation.	Instead,	it	should
depend	on	the	relative	time	between	two	successive	readings.	The
problem	was	that	the	Patriot	software	had	been	upgraded	to	use	a
more	accurate	function	for	reading	time,	but	not	all	of	the	function
calls	had	been	replaced	by	the	new	code.	As	a	result,	the	tracking
software	used	the	accurate	time	for	one	reading	and	the
inaccurate	time	for	the	other	[
103
].
2.4.2	
IEEE	Floating-Point
Representation</p>
<p>Positional	notation	such	as	considered	in	the	previous	section	would	not
be	efficient	for	representing	very	large	numbers.	For	example,	the
representation	of	5	×	2
would	consist	of	the	bit	pattern	101	followed	by
100	zeros.	Instead,	we	would	like	to	represent	numbers	in	a	form	
x
×	2
by	giving	the	values	of	
x
and	
y.
The	IEEE	floating-point	standard	represents	a	number	in	a	form	
V
=	(–1)
×	
M
×	2
:
The	
sign	s
determines	whether	the	number	is	negative	(
s
=	1)	or
positive	(
s
=	0),	where	the	interpretation	of	the	sign	bit	for	numeric
value	0	is	handled	as	a	special	case.
The	
significand	M
is	a	fractional	binary	number	that	ranges	either
between	1	and	2	–	
∊
or	between	0	and	1	–	
∊
.
The	
exponent	E
weights	the	value	by	a	(possibly	negative)	power	of
2.
Figure	
2.32	
Standard	floating-point	formats.
Floating-point	numbers	are	represented	by	three	fields.	For	the	two	most
common	formats,	these	are	packed	in	32-bit	(single-precision)	or	64-bit
100
y
s
E</p>
<p>(double-precision)	words.
The	bit	representation	of	a	floating-point	number	is	divided	into	three
fields	to	encode	these	values:
The	single	sign	bit	
directly	encodes	the	sign	
s.
The	
k
-bit	exponent	field	
=	
e
·	·	·	
e
e
encodes	the	exponent	
E.
The	
n
-bit	fraction	field	
=	
f
·	·	·	
f
f
encodes	the	significand	
M
,
but	the	value	encoded	also	depends	on	whether	or	not	the	exponent
field	equals	0.
Figure	
2.32
shows	the	packing	of	these	three	fields	into	words	for	the
two	most	common	formats.	In	the	single-precision	floating-point	format	(a
in	C),	fields	
,	and	
are	1,	
k
=	8,	and	
n
=	23	bits	each,
yielding	a	32-bit	representation.	In	the	double-precision	floating-point
format	(a	
in	C),	fields	
,	and	
are	1,	
k
=	11,	and	
n
=	52
bits	each,	yielding	a	64-bit	representation.
The	value	encoded	by	a	given	bit	representation	can	be	divided	into	three
different	cases	(the	latter	having	two	variants),	depending	on	the	value	of
exp.	These	are	illustrated	in	
Figure	
2.33
for	the	single-precision
format.
Case	
1
:	Normalized	Values
This	is	the	most	common	case.	It	occurs	when	the	bit	pattern	of	
is
neither	all	zeros	(numeric	value	0)	nor	all	ones	(numeric	value	255	for
single	precision,	2047	for	double).	In	this	case,	the	exponent	field	is
interpreted	as	representing	a	signed	integer	in	
biased
form.	That	is,	the
k
–1
1
0
n
–1
1
0</p>
<p>exponent	value	is	
E
=	
e
–	
Bias
,	where	
e
is	the	unsigned	number	having
bit	representation	
e
·	·	·	
e
e
and	
Bias
is	a	bias	value	equal	to	2
–	1
(127	for	single	precision	and	1023	for	double).	This	yields	exponent
ranges	from	–126	to	+127	for	single	precision	and	–1022	to	+1023	for
double	precision.
The	fraction	field	
is	interpreted	as	representing	the	fractional	value	
f
,
where	0	≤	
f
&lt;	1,	having	binary	representation	0.	
f
·	·	·	
f
f
,	that	is,	with
the
Aside	
Why	set	the	bias	this	way	for
denormalized	values?
Having	the	exponent	value	be	1	–	
Bias
rather	than	simply	–
Bias
might	seem	counterintuitive.	We	will	see	shortly	that	it	provides	for
smooth	transition	from	denormalized	to	normalized	values.
Figure	
2.33	
Categories	of	single-precision	floating-point	values.
k
–1
1
0
k
-1
n
–1
1
0</p>
<p>The	value	of	the	exponent	determines	whether	the	number	is	(1)
normalized,	(2)	denormalized,	or	(3)	a	special	value.
binary	point	to	the	left	of	the	most	significant	bit.	The	significand	is
defined	to	be	
M
=	1	+	
f.
This	is	sometimes	called	an	
implied	leading	1
representation,	because	we	can	view	
M
to	be	the	number	with	binary
representation	1.	
.	This	representation	is	a	trick	for	getting
an	additional	bit	of	precision	for	free,	since	we	can	always	adjust	the
exponent	
E
so	that	significand	
M
is	in	the	range	1	≤	
M
&lt;	2	(assuming
there	is	no	overflow).	We	therefore	do	not	need	to	explicitly	represent	the
leading	bit,	since	it	always	equals	1.
Case	
2
:	Denormalized	Values
When	the	exponent	field	is	all	zeros,	the	represented	number	is	in
denormalized
form.	In	this	case,	the	exponent	value	is	
E
=	1	–	
Bias
,	and
the	significand	value	is	
M
=	
f
,	that	is,	the	value	of	the	fraction	field	without
an	implied	leading	1.
Denormalized	numbers	serve	two	purposes.	First,	they	provide	a	way	to
represent	numeric	value	0,	since	with	a	normalized	number	we	must
always	have	
M
≥	1,	and	hence	we	cannot	represent	0.	In	fact,	the
floating-point	representation	of	+0.0	has	a	bit	pattern	of	all	zeros:	the	sign
bit	is	0,	the	exponent	field	is	all	zeros	(indicating	a	denormalized	value),
and	the	fraction	field	is	all	zeros,	giving	
M
=	
f
=	0.	Curiously,	when	the
sign	bit	is	1,	but	the	other	fields	are	all	zeros,	we	get	the	value	–0.0.	With
IEEE	floating-point	format,	the	values	–0.0	and	+0.0	are	considered
different	in	some	ways	and	the	same	in	others.
f
n
−
1
f
n
−
2
⋯
f
0</p>
<p>A	second	function	of	denormalized	numbers	is	to	represent	numbers	that
are	very	close	to	0.0.	They	provide	a	property	known	as	
gradual
underflow
in	which	possible	numeric	values	are	spaced	evenly	near	0.0.
Case	
3
:	Special	Values
A	final	category	of	values	occurs	when	the	exponent	field	is	all	ones.
When	the	fraction	field	is	all	zeros,	the	resulting	values	represent	infinity,
either	+∞	when	
s
=	0	or	-∞	when	
s
=	1.	Infinity	can	represent	results	that
overflow
,	as	when	we	multiply	two	very	large	numbers,	or	when	we	divide
by	zero.	When	the	fraction	field	is	nonzero,	the	resulting	value	is	called	a
“NaN
,”	short	for	“not	a	number.”	Such	values	are	returned	as	the	result	of
an	operation	where	the	result	cannot	be	given	as	a	real	number	or	as
infinity,	as	when	computing	
or	∞	–	∞.	They	can	also	be	useful	in	some
applications	for	representing	uninitialized	data.
2.4.3	
Example	Numbers
Figure	
2.34
shows	the	set	of	values	that	can	be	represented	in	a
hypothetical	6-bit	format	having	
k
=	3	exponent	bits	and	
n
=	2	fraction
bits.	The	bias	is	2
–	1	=	3.	Part	(a)	of	the	figure	shows	all	representable
values	(other	than	
NaN
).	The	two	infinities	are	at	the	extreme	ends.	The
normalized	numbers	with	maximum	magnitude	are	±14.	The
denormalized	numbers	are	clustered	around	0.	These	can	be	seen	more
clearly	in	part	(b)	of	the	figure,	where	we	show	just	the	numbers	between
–1.0	and	+1.0.	The	two	zeros	are	special	cases	of	denormalized
−
1</p>
<p>3–1</p>
<p>numbers.	Observe	that	the	representable	numbers	are	not	uniformly
distributed—they	are	denser	nearer	the	origin.
Figure	
2.35
shows	some	examples	for	a	hypothetical	8-bit	floating-
point	format	having	
k
=	4	exponent	bits	and	
n
=	3	fraction	bits.	The	bias	is
2
–	1	=	7.	The	figure	is	divided	into	three	regions	representing	the	three
classes	of	numbers.	The	different	columns	show	how	the	exponent	field
encodes	the	exponent	
E
,	while	the	fraction	field	encodes	the	significand
M
,	and	together	they	form	the
Figure	
2.34	
Representable	values	for	6-bit	floating-point	format.
There	are	
k
=	3	exponent	bits	and	
n
=	2	fraction	bits.	The	bias	is	3.
Exponent
Fraction
Value
Description
Bit
representation
e
E
2
f
M
2
×
M
V
Decimal
Zero
0
–
6
0
0.0
Smallest
0
–
0.001953
4–1
E
E
1
64
0
8
0
8
0
512
1
64
1
8
1
8
1
512
1
512</p>
<p>positive
6
0
–
6
0.003906
0
–
6
0.005859
⋮
Largest
denormalized
0
–
6
0.013672
Smallest
normalized
1
–
6
0.015625
1
–
6
0.017578
⋮
6
–
1
0.875
6
–
1
0.9375
One
7
0
1
1
1.0
7
0
1
1.125
7
0
1
1.25
⋮
14
7
128
224
224.0
1
64
2
8
2
8
2
512
2
256
1
64
3
8
3
8
3
512
3
512
1
64
7
8
7
8
7
512
7
512
1
64
0
8
8
8
8
512
1
64
1
64
1
8
9
8
9
512
9
512
1
2
6
8
14
8
14
16
7
8
1
2
7
8
15
8
15
16
15
16
0
8
8
8
8
8
1
8
9
8
9
8
9
8
2
8
10
8
10
8
5
4
6
8
14
8
1792
8</p>
<h1>Largest
normalized
14
7
128
240
240.0
Infinity
—
—
—
—
—
—
∞
—
Figure	
2.35	
Example	nonnegative	values	for	8-bit	floating-point
format.
There	are	
k
=	4	exponent	bits	and	
n
=	3	fraction	bits.	The	bias	is	7.
represented	value	
V
=	2
×	
M
.	Closest	to	0	are	the	denormalized
numbers,	starting	with	0	itself.	Denormalized	numbers	in	this	format	have
E
=	1	–	7	=	–6,	giving	a	weight	
.	The	fractions	
f
and	significands
M
range	over	the	values	0,	
,	giving	numbers	
V
in	the	range	0	to
.
The	smallest	normalized	numbers	in	this	format	also	have	
E
=	1	–	7	=	–6,
and	the	fractions	also	range	over	the	values	0,	
.	However,	the
significands	then	range	from	1	+	0	=	1	to	
,	giving	numbers	
V
in
the	range	
to	
.
Observe	the	smooth	transition	between	the	largest	denormalized	number
and	the	smallest	normalized	number	
.	This	smoothness	is	due
to	our	definition	of	
E
for	denormalized	values.	By	making	it	1	–	
Bias
rather	than	–
Bias
,	we	compensate	for	the	fact	that	the	significand	of	a
denormalized	number	does	not	have	an	implied	leading	1.
As	we	increase	the	exponent,	we	get	successively	larger	normalized
values,	passing	through	1.0	and	then	to	the	largest	normalized	number.
This	number	has	exponent	
E
=7,	giving	a	weight	2
=	128.	The	fraction
6
8
15
8
1920
8
E
2
E</h1>
<h1>1
64
1
8
,
…
,
 
7
8
1
64
 
×
 
7
8</h1>
<p>7
512
1
8
,
 
…
7
8
1</p>
<ul>
<li></li>
</ul>
<h1>7
8</h1>
<h1>15
8
8
512</h1>
<p>1
64
15
512
7
512
8
512
E</p>
<p>equals	
giving	a	significand	
.	Thus,	the	numeric	value	is	
V
=
240.	Going	beyond	this	overflows	to	+∞.
One	interesting	property	of	this	representation	is	that	if	we	interpret	the
bit	representations	of	the	values	in	
Figure	
2.35
as	unsigned	integers,
they	occur	in	ascending	order,	as	do	the	values	they	represent	as
floating-point	numbers.	This	is	no	accident—the	IEEE	format	was
designed	so	that	floating-point	numbers	could	be	sorted	using	an	integer
sorting	routine.	A	minor	difficulty	occurs	when	dealing	with	negative
numbers,	since	they	have	a	leading	1	and	occur	in	descending	order,	but
this	can	be	overcome	without	requiring	floating-point	operations	to
perform	comparisons	(see	
Problem	
2.84
).
Practice	Problem	
2.47
(solution	page	
158
)
Consider	a	5-bit	floating-point	representation	based	on	the	IEEE
floating-point	format,	with	one	sign	bit,	two	exponent	bits	(
k
=	2),
and	two	fraction	bits	(
n
=	2).	The	exponent	bias	is	2
–	1	=	1.
The	table	that	follows	enumerates	the	entire	nonnegative	range	for
this	5-bit	floating-point	representation.	Fill	in	the	blank	table	entries
using	the	following	directions:
e:
The	value	represented	by	considering	the	exponent	field	to
be	an	unsigned	integer
E:
The	value	of	the	exponent	after	biasing
2
:
The	numeric	weight	of	the	exponent
f
:	The	value	of	the	fraction
M:
The	value	of	the	significand
7
8</p>
<h1 id="m"><a class="header" href="#m">M</a></h1>
<p>15
8
2–1
E
E</p>
<p>2
×	
M:
The	(unreduced)	fractional	value	of	the	number
V:
The	reduced	fractional	value	of	the	number
Decimal:	The	decimal	representation	of	the	number
Express	the	values	of	2
,	f,	M
,	2
×	
M
,	and	
V
either	as	integers
(when	possible)	or	as	fractions	of	the	form	
,	where	
y
is	a	power
of	2.	You	need	not	fill	in	entries	marked	—.
Bits
e
E
2
f
M</p>
<hr />
<hr />
<hr />
<hr />
<hr />
<hr />
<hr />
<hr />
<hr />
<hr />
<hr />
<hr />
<hr />
<hr />
<hr />
<hr />
<hr />
<hr />
<hr />
<hr />
<hr />
<hr />
<hr />
<hr />
<hr />
<p>1
0
1
E
E
E
x
y
E
1
4
5
4</p>
<hr />
<hr />
<hr />
<hr />
<hr />
<hr />
<hr />
<hr />
<hr />
<hr />
<hr />
<hr />
<hr />
<hr />
<hr />
<hr />
<hr />
<hr />
<hr />
<hr />
<hr />
<hr />
<hr />
<hr />
<hr />
<hr />
<hr />
<hr />
<hr />
<hr />
<p>—
—
—
—
—
—
—
—
—
—
—
—
—
—
—</p>
<h1>—
—
—
—
—
Figure	
2.36
shows	the	representations	and	numeric	values	of	some
important	single-	and	double-precision	floating-point	numbers.	As	with
the	8-bit	format	shown	in	
Figure	
2.35
,	we	can	see	some	general
properties	for	a	floating-point	representation	with	a	
k
-bit	exponent	and	an
n
-bit	fraction:
The	value	+0.0	always	has	a	bit	representation	of	all	zeros.
The	smallest	positive	denormalized	value	has	a	bit	representation
consisting	of	a	1	in	the	least	significant	bit	position	and	otherwise	all
zeros.	It	has	a	fraction	(and	significand)	value	
M
=	
f
=	2
and	an
exponent	value	
.	The	numeric	value	is	therefore	
.
The	largest	denormalized	value	has	a	bit	representation	consisting	of
an	exponent	field	of	all	zeros	and	a	fraction	field	of	all	ones.	It	has	a
fraction	(and	significand)	value	
M
=	
f
=	1	–	2
(which	we	have	written
1	—	
∊
)	and	an	exponent	value	
E
=	–2
+	2.	The	numeric	value	is
therefore	
,	which	is	just	slightly	smaller	than	the
smallest	normalized	value.
Single	precision
Double	precision
Description
Value
Decimal
Value
Decimal
Zero
00	·	·	·
00
0	·	·	·
00
0
0.0
0
0.0
Smallest
denormalized
00	·	·	·
00
0	·	·	·
01
2
×
1.4	×
10
2
×
4.9	×
10
–
n
E</h1>
<p>−
2
k
−
1</p>
<ul>
<li></li>
</ul>
<h1>2
V</h1>
<p>2
−
n
−
2
k
−
1</p>
<ul>
<li></li>
</ul>
<h1>2 
−
n
k
–1
V</h1>
<p>(
1
−
2
−
n
)
×
2
−
2
k
−
1</p>
<ul>
<li></li>
</ul>
<p>2
−23
−126
−45
−52
−1022
−324</p>
<h1>2
2
Largest
denormalized
00	···
00
1	···
11
(1	–	
∊
)	×
2
1.2	×
10
(1	–	
∊
)	×
2
2.2	×
10
Smallest
normalized
00	···
01
0	···
00
1	×	2
1.2	×
10
1	×	2
2.2	×
10
One
01	···
11
0	···
00
1	×	2
1.0
1	×	2
1.0
Largest
normalized
11	···
10
1	···
11
(2	–	
∊
)	×
2
3.4	×
10
(2	–	
∊
)	×
2
1.8	×
10
Figure	
2.36	
Examples	of	nonnegative	floating-point	numbers.
The	smallest	positive	normalized	value	has	a	bit	representation	with	a
1	in	the	least	significant	bit	of	the	exponent	field	and	otherwise	all
zeros.	It	has	a	significand	value	
M
=	1	and	an	exponent	value	
E
=	–2
+	2.	The	numeric	value	is	therefore	
.
The	value	1.0	has	a	bit	representation	with	all	but	the	most	significant
bit	of	the	exponent	field	equal	to	1	and	all	other	bits	equal	to	0.	Its
significand	value	is	
M
=	1	and	its	exponent	value	is	
E
=	0.
The	largest	normalized	value	has	a	bit	representation	with	a	sign	bit	of
0,	the	least	significant	bit	of	the	exponent	equal	to	0,	and	all	other	bits
equal	to	1.	It	has	a	fraction	value	of	
f
=	1	–	2
,	giving	a	significand	
M
=	2	–	2
(which	we	have	written	2	–	
∊
.
)	It	has	an	exponent	value	
E
=
2
–	1,	giving	a	numeric	value	
.
−126
−1022
−126
−38
−1022
−308
−126
−38
−1022
−308
0
0
127
38
1023
308
k
–
1
V</h1>
<p>2
−
2
k
−
1</p>
<ul>
<li></li>
</ul>
<h1>2
–
n
–
n
k
–1
V</h1>
<h1>(
2
−
2
−
n
)
×
2
2
k
−
1
−
1</h1>
<p>(
1
−
2
−
n
−
1
)
×
2
2
k
−
1</p>
<p>One	useful	exercise	for	understanding	floating-point	representations	is	to
convert	sample	integer	values	into	floating-point	form.	For	example,	we
saw	in	
Figure	
2.15
that	12,345	has	binary	representation
[11000000111001].	We	create	a	normalized	representation	of	this	by
shifting	13	positions	to	the	right	of	a	binary	point,	giving	12345	=
1.1000000111001
×	2
.	To	encode	this	in	IEEE	single-precision	format,
we	construct	the	fraction	field	by	dropping	the	leading	1	and	adding	10
zeros	to	the	end,	giving	binary	representation
[10000001110010000000000].	To	construct	the	exponent	field,	we	add
bias	127	to	13,	giving	140,	which	has	binary	representation	[10001100].
We	combine	this	with	a	sign	bit	of	0	to	get	the	floating-point
representation	in	binary	of	[01000110010000001110010000000000].
Recall	from	
Section	
2.1.3
that	we	observed	the	following	correlation	in
the	bit-level	representations	of	the	integer	value	
and	the
single-precision	floating-point	value	
We	can	now	see	that	the	region	of	correlation	corresponds	to	the	low-
order	bits	of	the	integer,	stopping	just	before	the	most	significant	bit	equal
to	1	(this	bit	forms	the	implied	leading	1),	matching	the	high-order	bits	in
the	fraction	part	of	the	floating-point	representation.
Practice	Problem	
2.48
(solution	page	
159
)
2
13</p>
<p>As	mentioned	in	
Problem	
2.6
,	the	integer	3,510,593	has
hexadecimal	representation	
,	while	the	single-precision
floating-point	number	3,510,593.0	has	hexadecimal	representation
.	Derive	this	floating-point	representation	and	explain
the	correlation	between	the	bits	of	the	integer	and	floating-point
representations.
Practice	Problem	
2.49
(solution	page	
159
)
A
.	
For	a	floating-point	format	with	an	
n
-bit	fraction,	give	a
formula	for	the	smallest	positive	integer	that	cannot	be
represented	exactly	(because	it	would	require	an	(
n
+	1)-bit
fraction	to	be	exact).	Assume	the	exponent	field	size	
k
is
large	enough	that	the	range	of	representable	exponents
does	not	provide	a	limitation	for	this	problem.
B
.	
What	is	the	numeric	value	of	this	integer	for	single-precision
format	(
n
=	23)?
2.4.4	
Rounding
Floating-point	arithmetic	can	only	approximate	real	arithmetic,	since	the
representation	has	limited	range	and	precision.	Thus,	for	a	value	
x
,	we
generally	want	a	systematic	method	of	finding	the	“closest”	matching
value	
x
′	that	can	be	represented	in	the	desired	floating-point	format.	This
is	the	task	of	the	
rounding
operation.	One	key	problem	is	to	define	the
direction	to	round	a	value	that	is	halfway	between	two	possibilities.	For
example,	if	I	have	$1.50	and	want	to	round	it	to	the	nearest	dollar,	should</p>
<p>the	result	be	$1	or	$2?	An	alternative	approach	is	to	maintain	a	lower	and
an	upper	bound	on	the	actual	number.	For	example,	we	could	determine
representable	values	
x
and	
x
such	that	the	value	
x
is	guaranteed	to	lie
between	them:	
x
≤	
x
≤	
x
.	The	IEEE	floating-point	format	defines	four
different	
rounding	modes.
The	default	method	finds	a	closest	match,
while	the	other	three	can	be	used	for	computing	upper	and	lower	bounds.
Figure	
2.37
illustrates	the	four	rounding	modes	applied	to	the	problem
of	rounding	a	monetary	amount	to	the	nearest	whole	dollar.	Round-to-
even	(also	called	round-to-nearest)	is	the	default	mode.	It	attempts	to	find
a	closest	match.	Thus,	it	rounds	$1.40	to	$1	and	$1.60	to	$2,	since	these
are	the	closest	whole	dollar	values.	The	only	design	decision	is	to
determine	the	effect	of	rounding	values	that	are	halfway	between	two
possible	results.	Round-to-even	mode	adopts	the	convention	that	it
rounds	the	number	either	upward	or	downward	such	that	the	least
significant	digit	of	the	result	is	even.	Thus,	it	rounds	both	$1.50	and	$2.50
to	$2.
The	other	three	modes	produce	guaranteed	bounds	on	the	actual	value.
These	can	be	useful	in	some	numerical	applications.	Round-toward-zero
mode	rounds	positive	numbers	downward	and	negative	numbers	upward,
giving	a	value	
such
Mode
$1.40
$1.60
$1.50
$2.50
$–1.50
Round-to-even
$1
$2
$2
$2
$–2
Round-toward-zero
$1
$1
$1
$2
$–1
Round-down
$1
$1
$1
$2
$–2
−</p>
<ul>
<li></li>
</ul>
<p>−</p>
<ul>
<li></li>
</ul>
<p>x
^</p>
<p>Round-up
$2
$2
$2
$3
$–1
Figure	
2.37	
Illustration	of	rounding	modes	for	dollar	rounding.
The	first	rounds	to	a	nearest	value,	while	the	other	three	bound	the	result
above	or	below.
that	
.	Round-down	mode	rounds	both	positive	and	negative
numbers	downward,	giving	a	value	
x
such	that	
x
≤	
x.
Round-up	mode
rounds	both	positive	and	negative	numbers	upward,	giving	a	value	
x
such	that	
x
≤	
x
.
Round-to-even	at	first	seems	like	it	has	a	rather	arbitrary	goal—why	is
there	any	reason	to	prefer	even	numbers?	Why	not	consistently	round
values	halfway	between	two	representable	values	upward?	The	problem
with	such	a	convention	is	that	one	can	easily	imagine	scenarios	in	which
rounding	a	set	of	data	values	would	then	introduce	a	statistical	bias	into
the	computation	of	an	average	of	the	values.	The	average	of	a	set	of
numbers	that	we	rounded	by	this	means	would	be	slightly	higher	than	the
average	of	the	numbers	themselves.	Conversely,	if	we	always	rounded
numbers	halfway	between	downward,	the	average	of	a	set	of	rounded
numbers	would	be	slightly	lower	than	the	average	of	the	numbers
themselves.	Rounding	toward	even	numbers	avoids	this	statistical	bias	in
most	real-life	situations.	It	will	round	upward	about	50%	of	the	time	and
round	downward	about	50%	of	the	time.
Round-to-even	rounding	can	be	applied	even	when	we	are	not	rounding
to	a	whole	number.	We	simply	consider	whether	the	least	significant	digit
is	even	or	odd.	For	example,	suppose	we	want	to	round	decimal	numbers
to	the	nearest	hundredth.	We	would	round	1.2349999	to	1.23	and
|
x
^
|
 
≤
 
|
x
|
−
−</p>
<ul>
<li></li>
<li></li>
</ul>
<p>1.2350001	to	1.24,	regardless	of	rounding	mode,	since	they	are	not
halfway	between	1.23	and	1.24.	On	the	other	hand,	we	would	round	both
1.2350000	and	1.2450000	to	1.24,	since	4	is	even.
Similarly,	round-to-even	rounding	can	be	applied	to	binary	fractional
numbers.	We	consider	least	significant	bit	value	0	to	be	even	and	1	to	be
odd.	In	general,	the	rounding	mode	is	only	significant	when	we	have	a	bit
pattern	of	the	form	
XX
·	·	·	
X.YY
·	·	·	
Y
100	·	·	·,	where	
X
and	
Y
denote
arbitrary	bit	values	with	the	rightmost	
Y
being	the	position	to	which	we
wish	to	round.	Only	bit	patterns	of	this	form	denote	values	that	are
halfway	between	two	possible	results.	As	examples,	consider	the
problem	of	rounding	values	to	the	nearest	quarter	(i.e.,	2	bits	to	the	right
of	the	binary	point.)	We	would	round	
down	to	10.00
(2),	and	
up	to	
,	because	these	values	are
not	halfway	between	two	possible	values.	We	would	round
up	to	11.00
(3)	and	
down	to
,	since	these	values	are	halfway	between	two	possible
results,	and	we	prefer	to	have	the	least	significant	bit	equal	to	zero.
Practice	Problem	
2.50
(solution	page	
159
)
Show	how	the	following	binary	fractional	values	would	be	rounded
to	the	nearest	half	(1	bit	to	the	right	of	the	binary	point),	according
to	the	round-to-even	rule.	In	each	case,	show	the	numeric	values,
both	before	and	after	rounding.
A
.	
10.010
B
.	
10.011
C
.	
10.110
D
.	
11.001
10.00011
2
(
2
3
32
)
2
10.00110
2
(
2
3
16
)
10.01
2
(
2
1
4
)
10.11100
2
(
2
7
8
)
2
10.10100
2
(
2
5
8
)
10.10
2
(
2
1
2
)
2
2
2
2</p>
<p>Practice	Problem	
2.51
(solution	page	
159
)
We	saw	in	
Problem	
2.46
that	the	Patriot	missile	software
approximated	0.1	as	
x
=	0.	00011001100110011001100
.	Suppose
instead	that	they	had	used	IEEE	round-to-even	mode	to	determine
an	approximation	
x
′	to	0.1	with	23	bits	to	the	right	of	the	binary
point.
A
.	
What	is	the	binary	representation	of	
x
′?
B
.	
What	is	the	approximate	decimal	value	of	
x
′	–	0.1?
C
.	
How	far	off	would	the	computed	clock	have	been	after	100
hours	of	operation?
D
.	
How	far	off	would	the	program's	prediction	of	the	position	of
the	Scud	missile	have	been?
Practice	Problem	
2.52
(solution	page	
160
)
Consider	the	following	two	7-bit	floating-point	representations
based	on	the	IEEE	floating-point	format.	Neither	has	a	sign	bit—
they	can	only	represent	nonnegative	numbers.
1
.	
Format	A
There	are	
k	=	3
exponent	bits.	The	exponent	bias	is	3.
There	are	
n
=	4	fraction	bits.
2
.	
Format	B
There	are	
k
=	4	exponent	bits.	The	exponent	bias	is	7.
There	are	
n
=	3	fraction	bits.
Below,	you	are	given	some	bit	patterns	in	format	A,	and	your	task
is	to	convert	them	to	the	closest	value	in	format	B.	If	necessary,
2</p>
<p>you	should	apply	the	round-to-even	rounding	rule.	In	addition,	give
the	values	of	numbers	given	by	the	format	A	and	format	B	bit
patterns.	Give	these	as	whole	numbers	(e.g.,	17)	or	as	fractions
(e.g.,	17/64).
Format	A
Format	B
Bits
Value
Bits
Value
1
1</p>
<hr />
<hr />
<hr />
<hr />
<hr />
<hr />
<hr />
<hr />
<hr />
<p>2.4.5	
Floating-Point	Operations
The	IEEE	standard	specifies	a	simple	rule	for	determining	the	result	of	an
arithmetic	operation	such	as	addition	or	multiplication.	Viewing	floating-
point	values	
x</p>
<p>and	
y
as	real	numbers,	and	some	operation	
⊙
defined	over
real	numbers,	the	computation	should	yield	
Round
(
x</p>
<p>⊙</p>
<p>y
),	the	result	of
applying	rounding	to	the	exact	result	of	the	real	operation.	In	practice,
there	are	clever	tricks	floating-point	unit	designers	use	to	avoid
performing	this	exact	computation,	since	the	computation	need	only	be
sufficiently	precise	to	guarantee	a	correctly	rounded	result.	When	one	of
the	arguments	is	a	special	value,	such	as	–0,	∞,	or	
NaN
,	the	standard
specifies	conventions	that	attempt	to	be	reasonable.	For	example,	1/–0	is
defined	to	yield	-∞,	while	1/+0	is	defined	to	yield	+∞.</p>
<p>One	strength	of	the	IEEE	standard's	method	of	specifying	the	behavior	of
floating-point	operations	is	that	it	is	independent	of	any	particular
hardware	or	software	realization.	Thus,	we	can	examine	its	abstract
mathematical	properties	without	considering	how	it	is	actually
implemented.
We	saw	earlier	that	integer	addition,	both	unsigned	and	two's
complement,	forms	an	abelian	group.	Addition	over	real	numbers	also
forms	an	abelian	group,	but	we	must	consider	what	effect	rounding	has
on	these	properties.	Let	us	define	
x
+</p>
<p>y
to	be	
Round
(
x
+	
y
).	This
operation	is	defined	for	all	values	of	
x
and	
y
,	although	it	may	yield	infinity
even	when	both	
x
and	
y
are	real	numbers	due	to	overflow.	The	operation
is	commutative,	with	
x
+</p>
<p>y
=	
y
+</p>
<p>x
for	all	values	of	
x
and	
y.
On	the	other
hand,	the	operation	is	not	associative.	For	example,	with	single-precision
floating	point	the	expression	
evaluates	to	
—the
value	3.14	is	lost	due	to	rounding.	On	the	other	hand,	the	expression
evaluates	to	3.14.	As	with	an	abelian	group,	most
values	have	inverses	under	floating-point	addition,	that	is,	
x
+
–	
x
=	0.
The	exceptions	are	infinities	(since	+∞	–∞	=	
NaN
),	and	
NaN
s,	since	
NaN
+</p>
<p>x
=	
NaN
for	any	
x.
The	lack	of	associativity	in	floating-point	addition	is	the	most	important
group	property	that	is	lacking.	It	has	important	implications	for	scientific
programmers	and	compiler	writers.	For	example,	suppose	a	compiler	is
given	the	following	code	fragment:
f
f
f
f
f</p>
<p>The	compiler	might	be	tempted	to	save	one	floating-point	addition	by
generating	the	following	code:
However,	this	computation	might	yield	a	different	value	for	
than	would
the	original,	since	it	uses	a	different	association	of	the	addition
operations.	In	most	applications,	the	difference	would	be	so	small	as	to
be	inconsequential.	Unfortunately,	compilers	have	no	way	of	knowing
what	trade-offs	the	user	is	willing	to	make	between	efficiency	and
faithfulness	to	the	exact	behavior	of	the	original	program.	As	a	result,
they	tend	to	be	very	conservative,	avoiding	any	optimizations	that	could
have	even	the	slightest	effect	on	functionality.
On	the	other	hand,	floating-point	addition	satisfies	the	following
monotonicity	property:	if	
a
≥	
b
,	then	
for	any	values	of	
a,	b,
and
x
other	than	
NaN
.	This	property	of	real	(and	integer)	addition	is	not
obeyed	by	unsigned	or	two's-complement	addition.
Floating-point	multiplication	also	obeys	many	of	the	properties	one
normally	associates	with	multiplication.	Let	us	define	
x
*</p>
<p>y
to	be	
Round(x
×	y).
This	operation	is	closed	under	multiplication	(although	possibly
yielding	infinity	or	
NaN
),	it	is	commutative,	and	it	has	1.0	as	a
x</p>
<ul>
<li></li>
</ul>
<p>f
a
≥
x</p>
<ul>
<li></li>
</ul>
<p>f
b</p>
<p>f</p>
<p>multiplicative	identity.	On	the	other	hand,	it	is	not	associative,	due	to	the
possibility	of	overflow	or	the	loss	of	precision	due	to	rounding.	For
example,	with	single-precision	floating	point,	the	expression
,	while	
evaluates	to
.	In	addition,	floating-point	multiplication	does	not	distribute	over
addition.	For	example,	with	single-precision	floating	point,	the	expression
evaluates	to	
,	while	
evaluates
to	
.
On	the	other	hand,	floating-point	multiplication	satisfies	the	following
monotonicity	properties	for	any	values	
a,	b
,	and	
c
other	than	
NaN
:
In	addition,	we	are	also	guaranteed	that	
a
*</p>
<p>a
≥	0,	as	long	as	
a
≠	
NaN.
As
we	saw	earlier,	none	of	these	monotonicity	properties	hold	for	unsigned
or	two's-complement	multiplication.
This	lack	of	associativity	and	distributivity	is	of	serious	concern	to
scientific	programmers	and	to	compiler	writers.	Even	such	a	seemingly
simple	task	as	writing	code	to	determine	whether	two	lines	intersect	in
three-dimensional	space	can	be	a	major	challenge.
2.4.6	
Floating	Point	in	C
All	versions	of	C	provide	two	different	floating-point	data	types:	
and
.	On	machines	that	support	IEEE	floating	point,	these	data	types
a
≥
b
and
c
≥
0
⇒
a
∗
f
c
≥
b
∗
f
c
a
≥
b
and
c
≤
0
⇒
a
∗
f
c
≤
b
∗
f
c
f</p>
<p>correspond	to	single-	and	double-precision	floating	point.	In	addition,	the
machines	use	the	round-to-even	rounding	mode.	Unfortunately,	since	the
C	standards	do	not	require	the	machine	to	use	IEEE	floating	point,	there
are	no	standard	methods	to	change	the	rounding	mode	or	to	get	special
values	such	as	–0,	+∞,	–∞,	or	
NaN.
Most	systems	provide	a	combination
of	include	
files	and	procedure	libraries	to	provide	access	to	these
features,	but	the	details	vary	from	one	system	to	another.	For	example,
the	GNU	compiler	
GCC</p>
<p>defines	program	constants	
(for	+∞)	and
(for	
NaN
)	when	the	following	sequence	occurs	in	the	program	file:
Practice	Problem	
2.53
(solution	page	
160
)
Fill	in	the	following	macro	definitions	to	generate	the	double-
precision	values	+∞,	–∞,	and	–0:
You	cannot	use	any	include	files	(such	as	
),	but	you	can
make	use	of	the	fact	that	the	largest	finite	number	that	can	be
represented	with	double	precision	is	around	1.8	×	10
.
308</p>
<p>When	casting	values	between	
,	and	
formats,	the
program	changes	the	numeric	values	and	the	bit	representations	as
follows	(assuming	data	type	
is	32	bits):
From	
to	
,	the	number	cannot	overflow,	but	it	may	be
rounded.
From	
or	
to	
,	the	exact	numeric	value	can	be
preserved	because	
has	both	greater	range	(i.e.,	the	range	of
representable	values),	as	well	as	greater	precision	(i.e.,	the	number	of
significant	bits).
From	
to	
,	the	value	can	overflow	to	+∞	or	–∞,	since	the
range	is	smaller.	Otherwise,	it	may	be	rounded,	because	the	precision
is	smaller.
From	
or	
to	
,	the	value	will	be	rounded	toward	zero.
For	example,	1.999	will	be	converted	to	1,	while	–1.999	will	be
converted	to	–1.	Furthermore,	the	value	may	overflow.	The	C
standards	do	not	specify	a	fixed	result	for	this	case.	Intel-compatible
microprocessors	designate	the	bit	pattern	[10	...	00]	(
TMin
for	word
size	
w
)	as	an	
integer	indefinite
value.	Any	conversion	from	floating
point	to	integer	that	cannot	assign	a	reasonable	integer	approximation
yields	this	value.	Thus,	the	expression	
,
generating	a	negative	value	from	a	positive	one.
Practice	Problem	
2.54
(solution	page	
160
)
Assume	variables	
and	
are	of	type	
and
,	respectively.	Their	values	are	arbitrary,	except	that	neither
nor	
equals	+∞,	–∞,	or	
NaN.
For	each	of	the	following	C
w</p>
<p>expressions,	either	argue	that	it	will	always	be	true	(i.e.,	evaluate
to	1)	or	give	a	value	for	the	variables	such	that	it	is	not	true	(i.e.,
evaluates	to	0).
A
.	
B
.	
C
.	
D
.	
E
.	
F
.	
G
.	
H
.	</p>
<p>2.5	
Summary
Computers	encode	information	as	bits,	generally	organized	as
sequences	of	bytes.	Different	encodings	are	used	for	representing
integers,	real	numbers,	and	character	strings.	Different	models	of
computers	use	different	conventions	for	encoding	numbers	and	for
ordering	the	bytes	within	multi-byte	data.
The	C	language	is	designed	to	accommodate	a	wide	range	of	different
implementations	in	terms	of	word	sizes	and	numeric	encodings.
Machines	with	64-bit	word	sizes	have	become	increasingly	common,
replacing	the	32-bit	machines	that	dominated	the	market	for	around	30
years.	Because	64-bit	machines	can	also	run	programs	compiled	for	32-
bit	machines,	we	have	focused	on	the	distinction	between	32-and	64-bit
programs,	rather	than	machines.	The	advantage	of	64-bit	programs	is
that	they	can	go	beyond	the	4	GB	address	limitation	of	32-bit	programs.
Most	machines	encode	signed	numbers	using	a	two's-complement
representation	and	encode	floating-point	numbers	using	IEEE	Standard
754.	Understanding	these	encodings	at	the	bit	level,	as	well	as
understanding	the	mathematical	characteristics	of	the	arithmetic
operations,	is	important	for	writing	programs	that	operate	correctly	over
the	full	range	of	numeric	values.
When	casting	between	signed	and	unsigned	integers	of	the	same	size,
most	C	implementations	follow	the	convention	that	the	underlying	bit
pattern	does	not	change.	On	a	two's-complement	machine,	this	behavior</p>
<p>is	characterized	by	functions	
T2U
and	
U2T
,	for	a	
w
-bit	value.	The
implicit	casting	of	C	gives	results	that	many	programmers	do	not
anticipate,	often	leading	to	program	bugs.
Due	to	the	finite	lengths	of	the	encodings,	computer	arithmetic	has
properties	quite	different	from	conventional	integer	and	real	arithmetic.
The	finite	length	can	cause	numbers	to	overflow,	when	they	exceed	the
range	of	the	representation.	Floating-point	values	can	also	underflow,
when	they	are	so	close	to	0.0	that	they	are	changed	to	zero.
The	finite	integer	arithmetic	implemented	by	C,	as	well	as	most	other
programming	languages,	has	some	peculiar	properties	compared	to	true
integer	arithmetic.	For	example,	the	expression	
can	evaluate	to	a
negative	number	due	to	overflow.	Nonetheless,	both	unsigned	and	two's-
complement	arithmetic	satisfy	many	of	the	other	properties	of	integer
arithmetic,	including	associativity,	commutativity,	and	distributivity.	This
allows	compilers	to	do	many	optimizations.	For	example,	in	replacing	the
expression	
by	
,	we	make	use	of	the	associative,
commutative,	and	distributive	properties,	along	with	the	relationship
between	shifting	and	multiplying	by	powers	of	2.
We	have	seen	several	clever	ways	to	exploit	combinations	of	bit-level
operations	and	arithmetic	operations.	For	example,	we	saw	that	with
two's-complement	arithmetic,	
is	equivalent	to	
.	As	another
example,	suppose	we	want	a	bit
Aside	
Ariane	5:	The	high	cost	of
w
w</p>
<p>floating-point	overflow
Converting	large	floating-point	numbers	to	integers	is	a	common
source	of	programming	errors.	Such	an	error	had	disastrous
consequences	for	the	maiden	voyage	of	the	Ariane	5	rocket,	on
June	4,	1996.	Just	37	seconds	after	liftoff,	the	rocket	veered	off	its
flight	path,	broke	up,	and	exploded.	Communication	satellites
valued	at	$500	million	were	on	board	the	rocket.
A	later	investigation	[
73
,	
33
]	showed	that	the	computer	controlling
the	inertial	navigation	system	had	sent	invalid	data	to	the
computer	controlling	the	engine	nozzles.	Instead	of	sending	flight
control	information,	it	had	sent	a	diagnostic	bit	pattern	indicating
that	an	overflow	had	occurred	during	the	conversion	of	a	64-bit
floating-point	number	to	a	16-bit	signed	integer.
The	value	that	overflowed	measured	the	horizontal	velocity	of	the
rocket,	which	could	be	more	than	five	times	higher	than	that
achieved	by	the	earlier	Ariane	4	rocket.	In	the	design	of	the	Ariane
4	software,	they	had	carefully	analyzed	the	numeric	values	and
determined	that	the	horizontal	velocity	would	never	overflow	a	16-
bit	number.	Unfortunately,	they	simply	reused	this	part	of	the
software	in	the	Ariane	5	without	checking	the	assumptions	on
which	it	had	been	based.
pattern	of	the	form	[0,	...	,	0,	1,	...,	1],	consisting	of	
w
–	
k
zeros	followed
by	
k
ones.	Such	bit	patterns	are	useful	for	masking	operations.	This
pattern	can	be	generated	by	the	C	expression	
,	exploiting	the
property	that	the	desired	bit	pattern	has	numeric	value	2
–	1.	For
example,	the	expression	
will	generate	the	bit	pattern	
.
k</p>
<p>Floating-point	representations	approximate	real	numbers	by	encoding
numbers	of	the	form	
x
×	2
.	IEEE	Standard	754	provides	for	several
different	precisions,	with	the	most	common	being	single	(32	bits)	and
double	(64	bits).	IEEE	floating	point	also	has	representations	for	special
values	representing	plus	and	minus	infinity,	as	well	as	not-a-number.
Floating-point	arithmetic	must	be	used	very	carefully,	because	it	has	only
limited	range	and	precision,	and	because	it	does	not	obey	common
mathematical	properties	such	as	associativity.
y</p>
<p>Bibliographic	Notes
Reference	books	on	C	[
45
,	
61
]	discuss	properties	of	the	different	data
types	and	operations.	Of	these	two,	only	Steele	and	Harbison	[
45
]	cover
the	newer	features	found	in	ISO	C99.	There	do	not	yet	seem	to	be	any
books	that	cover	the	features	found	in	ISO	C11.	The	C	standards	do	not
specify	details	such	as	precise	word	sizes	or	numeric	encodings.	Such
details	are	intentionally	omitted	to	make	it	possible	to	implement	C	on	a
wide	range	of	different	machines.	Several	books	have	been	written	giving
advice	to	C	programmers	[
59
,	
74
]	that	warn	about	problems	with
overflow,	implicit	casting	to	unsigned,	and	some	of	the	other	pitfalls	we
have	covered	in	this	chapter.	These	books	also	provide	helpful	advice	on
variable	naming,	coding	styles,	and	code	testing.	Seacord's	book	on
security	issues	in	C	and	C++	programs	[
97
]	combines	information	about
C	programs,	how	they	are	compiled	and	executed,	and	how
vulnerabilities	may	arise.	Books	on	Java	(we	
recommend	the	one
coauthored	by	James	Gosling,	the	creator	of	the	language	[
5
])	describe
the	data	formats	and	arithmetic	operations	supported	by	Java.
Most	books	on	logic	design	[
58
,	
116
]	have	a	section	on	encodings	and
arithmetic	operations.	Such	books	describe	different	ways	of
implementing	arithmetic	circuits.	Overton's	book	on	IEEE	floating	point
[
82
]	provides	a	detailed	description	of	the	format	as	well	as	the	properties
from	the	perspective	of	a	numerical	applications	programmer.</p>
<p>Homework	Problems
2.55	
♦
Compile	and	run	the	sample	code	that	uses	
(file	
)	on	different	machines	to	which	you	have	access.
Determine	the	byte	orderings	used	by	these	machines.
2.56	
♦
Try	running	the	code	for	
for	different	sample	values.
2.57	
♦
Write	procedures	
,	and	
that
print	the	byte	representations	of	C	objects	of	types	
and	
respectively.	Try	these	out	on	several	machines.
2.58	
♦♦</p>
<p>Write	a	procedure	
that	will	return	1	when
compiled	and	run	on	a	little-endian	machine,	and	will	return	0
when	compiled	and	run	on	a	big-endian	machine.	This	program
should	run	on	any	machine,	regardless	of	its	word	size.
2.59	
♦♦
Write	a	C	expression	that	will	yield	a	word	consisting	of	the	least
significant	byte	of	
and	the	remaining	bytes	of	
.	For	operands	
and	
this	would	give	
.
2.60	♦♦
Suppose	we	number	the	bytes	in	a	
w
-bit	word	from	0	(least
significant)	to	
w
/8	–	1	(most	significant).	Write	code	for	the
following	C	function,	which	will	return	an	unsigned	value	in	which
byte	
of	argument	
has	been	replaced	by	byte	
:
Here	are	some	examples	showing	how	the	function	should	work:</p>
<p>Bit-Level	Integer	Coding	Rules
In	several	of	the	following	problems,	we	will	artificially	restrict	what
programming	constructs	you	can	use	to	help	you	gain	a	better
understanding	of	the	bit-level,	
logic,	and	arithmetic	operations	of	C.	In
answering	these	problems,	your	code	must	follow	these	rules:
Assumptions
Integers	are	represented	in	two's-complement	form.
Right	shifts	of	signed	data	are	performed	arithmetically.
Data	type	
is	
w
bits	long.	For	some	of	the	problems,	you	will	be
given	a	specific	value	for	
w
,	but	otherwise	your	code	should	work
as	long	as	
w
is	a	multiple	of	8.	You	can	use	the	expression
to	compute	
w
.
Forbidden
Conditionals	
loops,	switch	statements,	function	calls,
and	macro	invocations.
Division,	modulus,	and	multiplication.
Relative	comparison	operators	
.
Allowed	operations
All	bit-level	and	logic	operations.
Left	and	right	shifts,	but	only	with	shift	amounts	between	0	and	
w
–</p>
<ol>
<li></li>
</ol>
<p>Addition	and	subtraction.
Equality	
and	inequality	
tests.	(Some	of	the	problems	do
not	allow	these.)</p>
<p>Integer	constants	
and	
Casting	between	data	types	
and	
,	either	explicitly	or
implicitly.
Even	with	these	rules,	you	should	try	to	make	your	code	readable	by
choosing	descriptive	variable	names	and	using	comments	to	describe	the
logic	behind	your	solutions.	As	an	example,	the	following	code	extracts
the	most	significant	byte	from	integer	argument	
:
2.61	♦♦
Write	C	expressions	that	evaluate	to	1	when	the	following
conditions	are	true	and	to	0	when	they	are	false.	Assume	
is	of
type	
.
A
.	
Any	bit	of	
equals	1.</p>
<p>B
.	
Any	bit	of	
equals	0.
C
.	
Any	bit	in	the	least	significant	byte	of	
equals	1.
D
.	
Any	bit	in	the	most	significant	byte	of	
equals	0.
Your	code	should	follow	the	bit-level	integer	coding	rules	(page
128
),	with	the	additional	restriction	that	you	may	not	use	equality
or	inequality	
tests.
2.62	♦♦♦
Write	a	function	
that	yields	1	when
run	on	a	machine	that	uses	arithmetic	right	shifts	for	data	type	
and	yields	
otherwise.	Your	code	should	work	on	a	machine	with
any	word	size.	Test	your	code	on	several	machines.
2.63	♦♦♦
Fill	in	code	for	the	following	C	functions.	Function	
performs	a
logical	right	shift	using	an	arithmetic	right	shift	(given	by	value
),	followed	by	other	operations	not	including	right	shifts	or
division.	Function	
performs	an	arithmetic	right	shift	using	a
logical	right	shift	(given	by	value	
),	followed	by	other
operations	not	including	right	shifts	or	division.	You	may	use	the
computation	
to	determine	
w
,	the	number	of	bits	in
data	type	
.	The	shift	amount	
can	range	from	
to	
w
–	1.</p>
<p>2.64	♦
Write	code	to	implement	the	following	function:</p>
<p>Your	function	should	follow	the	bit-level	integer	coding	rules	(page
128
),	except	that	you	may	assume	that	data	type	
has	
w
=	32
bits.
2.65	♦♦♦♦
Write	code	to	implement	the	following	function:
Your	function	should	follow	the	bit-level	integer	coding	rules	(page
128
),	except	that	you	may	assume	that	data	type	
has	
w
=	32
bits.
Your	code	should	contain	a	total	of	at	most	12	arithmetic,	bitwise,
and	logical	operations.
2.66	♦♦♦♦</p>
<p>Write	code	to	implement	the	following	function:
Your	function	should	follow	the	bit-level	integer	coding	rules	(page
128
),	except	that	you	may	assume	that	data	type	
has	
w
=	32
bits.
Your	code	should	contain	a	total	of	at	most	15	arithmetic,	bitwise,
and	logical	operations.
Hint:
First	transform	
into	a	bit	vector	of	the	form	[0	...	011	...	1].
2.67	♦♦
You	are	given	the	task	of	writing	a	procedure	
that
yields	1	when	run	on	a	machine	for	which	an	
is	32	bits,	and
yields	0	otherwise.	You	are	not	allowed	to	use	the	
operator.
Here	is	a	first	attempt:</p>
<p>When	compiled	and	run	on	a	32-bit	SUN	SPARC,	however,	this
procedure	returns	0.	The	following	compiler	message	gives	us	an
indication	of	the	problem:
A
.	
In	what	way	does	our	code	fail	to	comply	with	the	C
standard?
B
.	
Modify	the	code	to	run	properly	on	any	machine	for	which
data	type	
is	at	least	32	bits.
C
.	
Modify	the	code	to	run	properly	on	any	machine	for	which
data	type	
is	at	least	16	bits.
2.68	♦♦</p>
<p>Write	code	for	a	function	with	the	following	prototype:
Your	function	should	follow	the	bit-level	integer	coding	rules	(page
128
).	Be	careful	of	the	case	
=	
w
.
2.69	♦♦♦
Write	code	for	a	function	with	the	following	prototype:
Your	function	should	follow	the	bit-level	integer	coding	rules	(page
128
).	Be	careful	of	the	case	
=	0.</p>
<p>2.70	♦♦
Write	code	for	the	function	with	the	following	prototype:
Your	function	should	follow	the	bit-level	integer	coding	rules	(page
128
).
2.71
You	just	started	working	for	a	company	that	is	implementing	a	set
of	procedures	to	operate	on	a	data	structure	where	4	signed	bytes
are	packed	into	a	32-bit	
.	Bytes	within	the	word	are
numbered	from	0	(least	significant)	to	3	
(most	significant).	You
have	been	assigned	the	task	of	implementing	a	function	for	a
machine	using	two's-complement	arithmetic	and	arithmetic	right
shifts	with	the	following	prototype:</p>
<p>That	is,	the	function	will	extract	the	designated	byte	and	sign
extend	it	to	be	a	32-bit	
.
Your	predecessor	(who	was	fired	for	incompetence)	wrote	the
following	code:
A
.	
What	is	wrong	with	this	code?
B
.	
Give	a	correct	implementation	of	the	function	that	uses	only
left	and	right	shifts,	along	with	one	subtraction.
2.72
You	are	given	the	task	of	writing	a	function	that	will	copy	an	integer
into	a	buffer	
,	but	it	should	do	so	only	if	enough	space	is</p>
<p>available	in	the	buffer.
Here	is	the	code	you	write:
This	code	makes	use	of	the	library	function	
.	Although	its
use	is	a	bit	artificial	here,	where	we	simply	want	to	copy	an	
,	it
illustrates	an	approach	commonly	used	to	copy	larger	data
structures.
You	carefully	test	the	code	and	discover	that	it	
always
copies	the
value	to	the	buffer,	even	when	
is	too	small.
A
.	
Explain	why	the	conditional	test	in	the	code	always
succeeds.	
Hint:
The	
operator	returns	a	value	of	type
.
B
.	
Show	how	you	can	rewrite	the	conditional	test	to	make	it
work	properly.
2.73
Write	code	for	a	function	with	the	following	prototype:</p>
<p>Instead	of	overflowing	the	way	normal	two's-complement	addition
does,	saturating	addition	returns	
TMax
when	there	would	be
positive	overflow,	and	
TMin
when	there	would	be	negative
overflow.	Saturating	arithmetic	is	commonly	used	in	programs	that
perform	digital	signal	processing.
Your	function	should	follow	the	bit-level	integer	coding	rules	(page
128
).
2.74
Write	a	function	with	the	following	prototype:
This	function	should	return	1	if	the	computation	
does	not
overflow.
2.75</p>
<p>Suppose	we	want	to	compute	the	complete	2
w
-bit	representation
of	
x	·	y
,	where	both	
x
and	
y
are	unsigned,	on	a	machine	for	which
data	type	
is	
w
bits.	The	low-order	
w
bits	of	the	product
can	be	computed	with	the	expression	
,	so	we	only	require	a
procedure	with	prototype
that	computes	the	high-order	
w
bits	of	
x	·	y
for	unsigned	variables.
We	have	access	to	a	library	function	with	prototype
that	computes	the	high-order	
w
bits	of	
x	·	y
for	the	case	where	
x
and	
y
are	in	two's-complement	form.	Write	code	calling	this
procedure	to	implement	the	function	for	unsigned	arguments.
Justify	the	correctness	of	your	solution.
Hint:
Look	at	the	relationship	between	the	signed	product	
x	·	y
and
the	unsigned	product	
x′	·	y′
in	the	derivation	of	
Equation	
2.18
.
2.76
The	library	function	
has	the	following	declaration:</p>
<p>According	to	the	library	documentation,	“The	
function
allocates	memory	for	an	array	of	
elements	of	
bytes
each.	The	memory	is	set	to	zero.	If	
or	
is	zero,	then
returns	
.”
Write	an	implementation	of	
that	performs	the	allocation	by
a	call	to	
and	sets	the	memory	to	zero	via	
.	Your
code	should	not	have	any	vulnerabilities	due	to	arithmetic
overflow,	and	it	should	work	correctly	regardless	of	the	number	of
bits	used	to	represent	data	of	type	
.
As	a	reference,	functions	
and	
have	the	following
declarations:
2.77
Suppose	we	are	given	the	task	of	generating	code	to	multiply
integer	variable	
by	various	different	constant	factors	
K
.	To	be
efficient,	we	want	to	use	only	the	operations	+,	–,	and	
≪
.	For	the
following	values	of	
K
,	write	C	expressions	to	perform	the
multiplication	using	at	most	three	operations	per	expression.
A
.	
K	=	17</p>
<p>B
.	
K	=	–7
C
.	
K
=	60
D
.	
K
=	–112
2.78
Write	code	for	a	function	with	the	following	prototype:
The	function	should	compute	
x
/2
with	correct	rounding,	and	it
should	follow	the	bit-level	integer	coding	rules	(page	
128
).
2.79
Write	code	for	a	function	
that,	for	integer	argument	
,
computes	
but	follows	the	bit-level	integer	coding	rules	(page
128
).	Your	code	should	replicate	the	fact	that	the	computation	
can	cause	overflow.
2.80
k</p>
<p>Write	code	for	a	function	
that,	for	integer	argument
,	computes	the	value	of	
,	rounded	toward	zero.	It	should	not
overflow.	Your	function	should	follow	the	bit-level	integer	coding
rules	(page	
128
).
2.81
Write	C	expressions	to	generate	the	bit	patterns	that	follow,	where
a
represents	
k
repetitions	of	symbol	
a
.	Assume	a	
w
-bit	data	type.
Your	code	may	contain	references	to	parameters	
and	
,
representing	the	values	of	
j
and	
k
,	but	not	a	parameter
representing	
w
.
A
.	
1
0
B
.	
0
1
0
2.82
We	are	running	programs	where	values	of	type	
are	32	bits.
They	are	represented	in	two's	complement,	and	they	are	right
shifted	arithmetically.	Values	of	type	
are	also	32	bits.
We	generate	arbitrary	values	
and	
,	and	convert	them	to
unsigned	values	as	follows:
3
4
x
k
w-k
k
w-k-j
k
j</p>
<p>For	each	of	the	following	C	expressions,	you	are	to	indicate
whether	or	not	the	expression	
always
yields	1.	If	it	always	yields	1,
describe	the	underlying	mathematical	principles.	Otherwise,	give
an	example	of	arguments	that	make	it	yield	0.
A
.	
B
.	
C
.	
D
.	
E
.	
2.83
Consider	numbers	having	a	binary	representation	consisting	of	an
infinite	string	of	the	form	0.
y	y	y	y	y	y
...,	where	
y
is	a	
k
-bit
sequence.	For	example,	the	binary	representation	of	
is
0.01010101	...	(
y
=	01),	while	the	representation	of	
is
0.001100110011	...	(
y
=	0011).
A
.	
Let	
Y
=	
B2U
(y)
,	that	is,	the	number	having	binary
representation	
y
.	Give	a	formula	in	terms	of	
Y
and	
k
for	the
value	represented	by	the	infinite	string.	
Hint:
Consider	the
effect	of	shifting	the	binary	point	
k
positions	to	the	right.
1
3
1
5
k</p>
<p>B
.	
What	is	the	numeric	value	of	the	string	for	the	following
values	of	
y
?
a
.	
101
b
.	
0110
c
.	
010011
2.84
Fill	in	the	return	value	for	the	following	procedure,	which	tests
whether	its	first	argument	is	less	than	or	equal	to	its	second.
Assume	the	function	
returns	an	unsigned	32-bit	number
having	the	same	bit	representation	as	its	floating-point	argument.
You	can	assume	that	neither	argument	is	
NaN
.	The	two	flavors	of
zero,	+0	and	–0,	are	considered	equal.</p>
<p>2.85
Given	a	floating-point	format	with	a	
k
-bit	exponent	and	an	
n
-bit
fraction,	write	formulas	for	the	exponent	
E
,	the	significand	
M
,	the
fraction	
f
,	and	the	value	
V
for	the	quantities	that	follow.	In	addition,
describe	the	bit	representation.
A
.	
The	number	7.0
B
.	
The	largest	odd	integer	that	can	be	represented	exactly
C
.	
The	reciprocal	of	the	smallest	positive	normalized	value
2.86
Intel-compatible	processors	also	support	an	“extended-precision”
floating-point	format	with	an	80-bit	word	divided	into	a	sign	bit,	
k
=
15	exponent	bits,	a	single	
integer
bit,	and	
n
=	63	fraction	bits.	The
integer	bit	is	an	explicit	copy	of	the	implied	bit	in	the	IEEE	floating-
point	representation.	That	is,	it	equals	1	for	normalized	values	and
0	for	denormalized	values.	Fill	in	the	following	table	giving	the
approximate	values	of	some	“interesting”	numbers	in	this	format:
Extended	precision
Description
Value
Decimal
Smallest	positive	denormalized</p>
<hr />
<hr />
<p>Smallest	positive	normalized</p>
<hr />
<hr />
<p>Largest	normalized</p>
<hr />
<hr />
<p>This	format	can	be	used	in	C	programs	compiled	for	Intel-
compatible	machines	by	declaring	the	data	to	be	of	type	
.	However,	it	forces	the	compiler	to	generate	code	based	on
the	legacy	8087	floating-point	instructions.	The	resulting	program
will	most	likely	run	much	slower	than	would	be	the	case	for	data
type	
or	
.
2.87
The	2008	version	of	the	IEEE	floating-point	standard,	named	IEEE
754-2008,	includes	a	16-bit	“half-precision”	floating-point	format.	It
was	originally	devised	by	computer	graphics	companies	for	storing
data	in	which	a	higher	dynamic	range	is	required	than	can	be
achieved	with	16-bit	integers.	This	format	has	1	sign	bit,	5
exponent	bits	(
k
=	5),	and	10	fraction	bits	(
n
=	10).	The	exponent
bias	is	2
–	1	=	15.
Fill	in	the	table	that	follows	for	each	of	the	numbers	given,	with	the
following	instructions	for	each	column:
Hex:	The	four	hexadecimal	digits	describing	the	encoded	form.
M
:	The	value	of	the	significand.	This	should	be	a	number	of	the
form	
x
or	
,	where	
x
is	an	integer	and	
y
is	an	integral	power	of
2.	Examples	include	0,	
,	and	
.
E
:	The	integer	value	of	the	exponent.
V
:	The	numeric	value	represented.	Use	the	notation	
x
or	
x
×	2
,
where	
x
and	
z
are	integers.
5–1
x
y
67
64
1
256
z</p>
<p>D
:	The	(possibly	approximate)	numerical	value,	as	is	printed
using	the	
formatting	specification	of	
.
As	an	example,	to	represent	the	number	
,	we	would	have	
s
=	0,
and	
E
=	–1.	Our	number	would	therefore	have	an	exponent
field	of	01110
(decimal	value	15	–	1	=	14)	and	a	significand	field
of	1100000000
,	giving	a	hex	representation	
.	The	numerical
value	is	0.875.
You	need	not	fill	in	entries	marked	—.
Description
Hex
M
E
V
D
–0</p>
<hr />
<hr />
<hr />
<p>–0
–0.0
Smallest
value	&gt;	2</p>
<hr />
<hr />
<hr />
<hr />
<hr />
<p>512</p>
<hr />
<hr />
<hr />
<p>512
512.0
Largest
denormalized</p>
<hr />
<hr />
<hr />
<hr />
<hr />
<p>–∞</p>
<hr />
<p>—
—
-∞
–∞
Number	with
hex
representation</p>
<hr />
<hr />
<hr />
<hr />
<h1>2.88
7
8
M</h1>
<p>7
4
2
2</p>
<p>Consider	the	following	two	9-bit	floating-point	representations
based	on	the	IEEE	floating-point	format.
1
.	
Format	A
There	is	1	sign	bit.
There	are	
k
=	5	exponent	bits.	The	exponent	bias	is	15.
There	are	
n
=	3	fraction	bits.
2
.	
Format	B
There	is	1	sign	bit.
There	are	
k
=	4	exponent	bits.	The	exponent	bias	is	7.
There	are	
n
=	4	fraction	bits.
In	the	following	table,	you	are	given	some	bit	patterns	in	format	A,
and	your	task	is	to	convert	them	to	the	closest	value	in	format	B.	If
rounding	is	necessary	you	should	
round	toward
+∞.	In	addition,
give	the	values	of	numbers	given	by	the	format	A	and	format	B	bit
patterns.	Give	these	as	whole	numbers	(e.g.,	17)	or	as	fractions
(e.g.,	17/64	or	17/2
).
Format	A
Format	B
Bits
Value
Bits
Value</p>
<hr />
<hr />
<hr />
<hr />
<hr />
<hr />
<hr />
<hr />
<hr />
<hr />
<hr />
<hr />
<p>6
−
9
8
−
9
8</p>
<hr />
<hr />
<hr />
<p>2.89
We	are	running	programs	on	a	machine	where	values	of	type	
have	a	32-bit	two's-complement	representation.	Values	of	type
use	the	32-bit	IEEE	format,	and	values	of	type	
use
the	64-bit	IEEE	format.
We	generate	arbitrary	integer	values	
,	
,	and	
,	and	convert
them	to	values	of	type	
follows:
For	each	of	the	following	C	expressions,	you	are	to	indicate
whether	or	not	the	expression	
always
yields	1.	If	it	always	yields	1,
describe	the	underlying	mathematical	principles.	Otherwise,	give
an	example	of	arguments	that	make	it	yield	0.	Note	that	you
cannot	use	an	IA32	machine	running	
GCC</p>
<p>to	test	your	answers,</p>
<p>since	it	would	use	the	80-bit	extended-precision	representation	for
both	
and	
.
A
.	
B
.	
C
.	
D
.	
E
.	
2.90
You	have	been	assigned	the	task	of	writing	a	C	function	to
compute	a	floating-point	representation	of	2
.	You	decide	that	the
best	way	to	do	this	is	to	directly	construct	the	IEEE	single-
precision	representation	of	the	result.	When	
x
is	too	small,	your
routine	will	return	0.0.	When	
x
is	too	large,	it	will	return	+∞.	Fill	in
the	blank	portions	of	the	code	that	follows	to	compute	the	correct
result.	Assume	the	
function	
returns	a	floating-point	value
having	an	identical	bit	representation	as	its	unsigned	argument.
x</p>
<p>2.91
Around	250	B.C.,	the	Greek	mathematician	Archimedes	proved
that	
.	Had	he	had	access	to	a	computer	and	the
standard	library	
,	he	would	have	been	able	to	determine
that	the	single-precision	floating-point	approximation	of	
π
has	the
223
71
&lt;
π
&lt;
22
7</p>
<p>hexadecimal	representation	
.	Of	course,	all	of	these	are
just	approximations,	since	
π
is	not	rational.
A
.	
What	is	the	fractional	binary	number	denoted	by	this
floating-point	value?
B
.	
What	is	the	fractional	binary	representation	of	
?	
Hint:
See	
Problem	
2.83
.
C
.	
At	what	bit	position	(relative	to	the	binary	point)	do	these
two	approximations	to	
π
diverge?
Bit-Level	Floating-Point	Coding
Rules
In	the	following	problems,	you	will	write	code	to	implement	floating-point
functions,	operating	directly	on	bit-level	representations	of	floating-point
numbers.	Your	code	should	exactly	replicate	the	conventions	for	IEEE
floating-point	operations,	including	using	round-to-even	mode	when
rounding	is	required.
To	this	end,	we	define	data	type	
to	be	equivalent	to	
22
7</p>
<p>Rather	than	using	data	type	
in	your	code,	you	will	use	
.
You	may	use	both	
and	
data	types,	including	unsigned	and
integer	constants	and	operations.	You	may	not	use	any	unions,	structs,
or	arrays.	Most	significantly,	you	may	not	use	any	floating-point	data
types,	operations,	or	constants.	Instead,	your	code	should	perform	the	bit
manipulations	that	implement	the	specified	floating-point	operations.
The	following	function	illustrates	the	use	of	these	coding	rules.	For
argument	
f
,	it	returns	±0	if	
f
is	denormalized	(preserving	the	sign	of	
f
),	and
returns	
f
otherwise.</p>
<p>2.92	♦♦
Following	the	bit-level	floating-point	coding	rules,	implement	the
function	with	the	following	prototype:
For	floating-point	number	
f
,	this	function	computes	–
f
.	If	
f
is	
NaN
,
your	function	should	simply	return	
f
.
Test	your	function	by	evaluating	it	for	all	2
values	of	argument	
and	comparing	the	result	to	what	would	be	obtained	using	your
machine's	floating-point	operations.
2.93	
Following	the	bit-level	floating-point	coding	rules,	implement
the	function	with	the	following	prototype:
For	floating-point	number	
f
,	this	function	computes	|
f
|.	If	
f
is	
NaN
,
your	function	should	simply	return	
f
.
Test	your	function	by	evaluating	it	for	all	2
values	of	argument	
and	comparing	the	result	to	what	would	be	obtained	using	your
machine's	floating-point	operations.
32
32</p>
<p>2.94
Following	the	bit-level	floating-point	coding	rules,	implement	the
function	with	the	following	prototype:
For	floating-point	number	
f
,	this	function	computes	2.0	·	
f
.	If	
f
is
NaN
,	your	function	should	simply	return	
f
.
Test	your	function	by	evaluating	it	for	all	2
values	of	argument	
and	comparing	the	result	to	what	would	be	obtained	using	your
machine's	floating-point	operations.
2.95
Following	the	bit-level	floating-point	coding	rules,	implement	the
function	with	the	following	prototype:
For	floating-point	number	
f
,	this	function	computes	0.5	·	
f
.	If	
f
is
NaN
,	your	function	should	simply	return	
f
.
32
32</p>
<p>Test	your	function	by	evaluating	it	for	all	2
values	of	argument	
and	comparing	the	result	to	what	would	be	obtained	using	your
machine's	floating-point	operations.
2.96
Following	the	bit-level	floating-point	coding	rules,	implement	the
function	with	the	following	prototype:
For	floating-point	number	
f
,	this	function	computes	(
)	
f
.	Your
function	should	round	toward	zero.	If	
f
cannot	be	represented	as
an	integer	(e.g.,	it	is	out	of	range,	or	it	is	
NaN
),	then	the	function
should	return	
.
Test	your	function	by	evaluating	it	for	all	2
values	of	argument	
and	comparing	the	result	to	what	would	be	obtained	using	your
machine's	floating-point	operations.
2.97
32
32</p>
<p>Following	the	bit-level	floating-point	coding	rules,	implement	the
function	with	the	following	prototype:
For	argument	
,	this	function	computes	the	bit-level
representation	of	
.
Test	your	function	by	evaluating	it	for	all	2
values	of	argument	
and	comparing	the	result	to	what	would	be	obtained	using	your
machine's	floating-point	operations.
32</p>
<p>Solutions	to	Practice	Problems
Solution	to	Problem	
2.1	
(page
37
)
Understanding	the	relation	between	hexadecimal	and	binary	formats	will
be	important	once	we	start	looking	at	machine-level	programs.	The
method	for	doing	these	conversions	is	in	the	text,	but	it	takes	a	little
practice	to	become	familiar.
A
.	
to	binary:
Hexadecimal
Binary
B
.	
Binary	
to	hexadecimal:
Binary
Hexadecimal
C
.	
to	binary:
Hexadecimal
Binary</p>
<p>D
.	
Binary	
to	hexadecimal:
Binary
Hexadecimal
Solution	to	Problem	
2.2	
(page
37
)
This	problem	gives	you	a	chance	to	think	about	powers	of	2	and	their
hexadecimal	representations.
n
2
(decimal)
2
(hexadecimal)
9
512
19
524,288
14
16,384
16
65,536
17
131,072
5
32
7
128
n
n</p>
<p>Solution	to	Problem	
2.3	
(page
38
)
This	problem	gives	you	a	chance	to	try	out	conversions	between
hexadecimal	and	decimal	representations	for	some	smaller	numbers.	For
larger	ones,	it	becomes	much	more	convenient	and	reliable	to	use	a
calculator	or	conversion	program.
Decimal
Binary
Hexadecimal
0
0000	0000
167	=	10	·	16	+	7
1010	0111
62	=	3	·	16	+	14
0011	1110
188	=	11	·	16	+	12
1011	1100
3	·	16	+	7	=	55
0011	0111
8	·	16	+	8	=	136
1000	1000
15	·	16	+	3	=	243
1111	0011
5	·	16	+	2	=	82
0101	0010
10	·	16	+	12	=	172
1010	1100
14	·	16	+	7	=	231
1110	0111</p>
<p>Solution	to	Problem	
2.4	
(page
39
)
When	you	begin	debugging	machine-level	programs,	you	will	find	many
cases	where	some	simple	hexadecimal	arithmetic	would	be	useful.	You
can	always	convert	numbers	to	decimal,	perform	the	arithmetic,	and
convert	them	back,	but	being	able	to	work	directly	in	hexadecimal	is	more
efficient	and	informative.
A
.	
Adding	
to	hex	
gives	
with	a	carry	of
.
B
.	
Subtracting	
from	
in	the	second	digit
position	requires	a	borrow	from	the	third.	Since	this	digit	is	
,	we
must	also	borrow	from	the	fourth	position.
C
.	
Decimal	64	(2
)	equals	hexadecimal	
D
.	
To	subtract	hex	
(decimal	12)	from	hex
(decimal	10),	we	borrow	16	from	the	second	digit,	giving	hex	
(decimal	14).	In	the	second	digit,	we	now	subtract	3	from	hex	
(decimal	13),	giving	hex	
(decimal	10).
Solution	to	Problem	
2.5	
(page
48
)
6</p>
<p>This	problem	tests	your	understanding	of	the	byte	representation	of	data
and	the	two	different	byte	orderings.
Recall	that	
enumerates	a	series	of	bytes	starting	from	the	one
with	lowest	address	and	working	toward	the	one	with	highest	address.
On	a	little-endian	machine,	it	will	list	the	bytes	from	least	significant	to
most.	On	a	big-endian	machine,	it	will	list	bytes	from	the	most	significant
byte	to	the	least.
Solution	to	Problem	
2.6	
(page
49
)
This	problem	is	another	chance	to	practice	hexadecimal	to	binary
conversion.	It	also	gets	you	thinking	about	integer	and	floating-point
representations.	We	will	explore	these	representations	in	more	detail
later	in	this	chapter.
A
.	
Using	the	notation	of	the	example	in	the	text,	we	write	the	two
strings	as	follows:</p>
<p>B
.	
With	the	second	word	shifted	two	positions	to	the	right	relative	to
the	first,	we	find	a	sequence	with	21	matching	bits.
C
.	
We	find	all	bits	of	the	integer	embedded	in	the	floating-point
number,	except	for	the	most	significant	bit	having	value	1.	Such	is
the	case	for	the	example	in	the	text	as	well.	In	addition,	the
floating-point	number	has	some	nonzero	high-order	bits	that	do
not	match	those	of	the	integer.
Solution	to	Problem	
2.7	
(page
49
)
It	prints	
.	Recall	also	that	the	library	routine	
does	not	count	the	terminating	null	character,	and	so	
printed
only	through	the	character	
.
Solution	to	Problem	
2.8	
(page
51
)</p>
<p>This	problem	is	a	drill	to	help	you	become	more	familiar	with	Boolean
operations.
Operation
Result
a
[01101001]
b
[01010101]
~
a
[10010110]
~
b
[10101010]
a
&amp;	
b
[01000001]
a
|	
b
[01111101]
a
^	
b
[00111100]
Solution	to	Problem	
2.9	
(page
53
)
This	problem	illustrates	how	Boolean	algebra	can	be	used	to	describe
and	reason	about	real-world	systems.	We	can	see	that	this	color	algebra
is	identical	to	the	Boolean	algebra	over	bit	vectors	of	length	3.
A
.	
Colors	are	complemented	by	complementing	the	values	of	
R
,	
G
,
and	
B
.	From	this,	we	can	see	that	white	is	the	complement	of</p>
<p>black,	yellow	is	the	complement	of	blue,	magenta	is	the
complement	of	green,	and	cyan	is	the	complement	of	red.
B
.	
We	perform	Boolean	operations	based	on	a	bit-vector
representation	of	the	colors.	From	this	we	get	the	following:
Solution	to	Problem	
2.10	
(page
54
)
This	procedure	relies	on	the	fact	that	</p>
<div style="break-before: page; page-break-before: always;"></div><h2 id="exclusive"><a class="header" href="#exclusive">EXCLUSIVE</a></h2>
<p>OR</p>
<p>is	commutative	and
associative,	and	that	
a
^	
a
=	0	for	any	
a
.
Step
*x
*y
Initially
a
b
Step	1
a
a
^	
b
Step	2
a
^	(
a
^	
b
)	=	(
a
^	
a
)	^	
b
=	
b
a
^	
b
Step	3
b
b
^	(
a
^	
b
)	=	(
b
^	
b
)	^	
a
=	
a
See	
Problem	
2.11
for	a	case	where	this	function	will	fail.</p>
<p>Solution	to	Problem	
2.11	
(page
55
)
This	problem	illustrates	a	subtle	and	interesting	feature	of	our	inplace
swap	routine.
A
.	
Both	
and	
have	value	
k
,	so	we	are	attempting	to	swap
the	middle	element	with	itself.
B
.	
In	this	case,	arguments	
and	
to	
both	point	to	the
same	location.	When	we	compute	
,	we	get	0.	We	then
store	0	as	the	middle	element	of	the	array,	and	the	subsequent
steps	keep	setting	this	element	to	0.	We	can	see	that	our
reasoning	in	
Problem	
2.10
implicitly	assumed	that	
and	
denote	different	locations.
C
.	
Simply	replace	the	test	in	line	4	of	
to	be	
,	since	there	is	no	need	to	swap	the	middle	element	with	itself.
Solution	to	Problem	
2.12	
(page
55
)
Here	are	the	expressions:
A
.	
B
.	</p>
<p>C
.	
These	expressions	are	typical	of	the	kind	commonly	found	in	performing
low-level	bit	operations.	The	expression	
creates	a	mask	where	the
8	least-significant	bits	equal	0	and	the	rest	equal	1.	Observe	that	such	a
mask	will	be	generated	regardless	of	the	word	size.	By	contrast,	the
expression	
would	only	work	when	data	type	
is	32	bits.
Solution	to	Problem	
2.13	
(page
56
)
These	problems	help	you	think	about	the	relation	between	Boolean
operations	and	typical	ways	that	programmers	apply	masking	operations.
Here	is	the	code:</p>
<h1>The	
operation	is	equivalent	to	Boolean	
OR
—a	bit	is	set	in	
if	either
this	bit	is	set	in	
or	it	is	set	in	
.	On	the	other	hand,	
is
equivalent	to	
;	we	want	the	result	to	equal	1	only	when	the
corresponding	bit	of	
is	1	and	of	
is	0.
Given	that,	we	can	implement	|	with	a	single	call	to	
.	To	implement	
,
we	take	advantage	of	the	property
Solution	to	Problem	
2.14	
(page
57
)
This	problem	highlights	the	relation	between	bit-level	Boolean	operations
and	logical	operations	in	C.	A	common	programming	error	is	to	use	a	bit-
level	operation	when	a	logical	one	is	intended,	or	vice	versa.
Expression
Value
Expression
Value
x
^
y</h1>
<p>(
x
 
&amp;
∼
y
)
 
 
(
∼
x
&amp;
y
)</p>
<p>Solution	to	Problem	
2.15	
(page
57
)
The	expression	is	!	
.
That	is,	
will	be	zero	if	and	only	if	every	bit	of	
matches	the
corresponding	bit	of	
.	We	then	exploit	the	ability	of	
to	determine
whether	a	word	contains	any	nonzero	bit.
There	is	no	real	reason	to	use	this	expression	rather	than	simply	writing	
,	but	it	demonstrates	some	of	the	nuances	of	bit-level	and	logical
operations.
Solution	to	Problem	
2.16	
(page
58
)
This	problem	is	a	drill	to	help	you	understand	the	different	shift
operations.</p>
<p>Logical	
Arithmet	
Hex
Binary
Binary
Hex
Binary
Hex
Binary
Hex
[11000011]
[00011000]
[00110000]
[11110000]
[01110101]
[10101000]
[00011101]
[00011101]
[10000111]
[00111000]
[00100001]
[11100001]
[01100110]
[00110000]
[00011001]
[00011001]
Solution	to	Problem	
2.17	
(page
65
)
In	general,	working	through	examples	for	very	small	word	sizes	is	a	very
good	way	to	understand	computer	arithmetic.
The	unsigned	values	correspond	to	those	in	
Figure	
2.2
.	For	the	two's-
complement	values,	hex	digits	
through	
have	a	most	significant	bit	of
0,	yielding	nonnegative	values,	while	hex	digits	
through	
have	a	most
significant	bit	of	1,	yielding	a	negative	value.
Hexadecimal
Binary
[1110]
2
+2
+2
=	14
–2
+	2
+2
=	–2
x
→
B
2
U
 
4
(
x
→
)
B
2
T
 
4
(
x
→
)
3
2
1
3
2
1</p>
<p>[0000]
0
0
[0101]
2
+	2
=	5
2
+	2
=	5
[1000]
2
=	8
–2
=	–8
[1101]
2
+	2
+	2
=	13
–2
+	2
+	2
=	–3
[1111]
2
+	2
+	2
+	2
=	15
–2
+	2
+	2
+	2
=	–1
Solution	to	Problem	
2.18	
(page
69
)
For	a	32–bit	word,	any	value	consisting	of	8	hexadecimal	digits	beginning
with	one	of	the	digits	
through	
represents	a	negative	number.	It	is
quite	common	to	see	numbers	beginning	with	a	string	of	
,	since	the
leading	bits	of	a	negative	number	are	all	ones.	You	must	look	carefully,
though.	For	example,	the	number	
has	only	7	digits.	Filling	this
out	with	a	leading	zero	gives	
,	a	positive	number.
2
0
2
0
3
3
3
2
0
3
2
0
3
2
1
0
3
2
1
0</p>
<p>Solution	to	Problem	
2.19	
(page
71
)
The	functions	
T2U
and	
U2T
are	very	peculiar	from	a	mathematical
perspective.	It	is	important	to	understand	how	they	behave.
We	solve	this	problem	by	reordering	the	rows	in	the	solution	of	
Problem
2.17
according	to	the	two's-complement	value	and	then	listing	the</p>
<p>unsigned	value	as	the	result	of	the	function	application.	We	show	the
hexadecimal	values	to	make	this	process	more	concrete.
(hex)
x
T2U
(x)
–8
8
–3
13
–2
14
–1
15
0
0
5
5
Solution	to	Problem	
2.20	
(page
73
)
This	exercise	tests	your	understanding	of	
Equation	
2.5
.
For	the	first	four	entries,	the	values	of	
x
are	negative	and	
T2U
(x)
=	
x
+
2
.
For	the	remaining	two	entries,	the	values	of	
x
are	nonnegative	and
T2U
(x)
=	
x
.
x
→</p>
<p>4
4
4
4</p>
<p>Solution	to	Problem	
2.21	
(page
76
)
This	problem	reinforces	your	understanding	of	the	relation	between
two's-complement	and	unsigned	representations,	as	well	as	the	effects	of
the	C	promotion	rules.	Recall	that	
TMin
is	–2,147,483,648,	and	that
when	cast	to	unsigned	it	
becomes	2,147,483,648.	In	addition,	if	either
operand	is	unsigned,	then	the	other	operand	will	be	cast	to	unsigned
before	comparing.
Expression
Type
Evaluation
Unsigned
Signed
Unsigned
Signed
Unsigned
Solution	to	Problem	
2.22	
(page
79
)
32</p>
<p>This	exercise	provides	a	concrete	demonstration	of	how	sign	extension
preserves	the	numeric	value	of	a	two's-complement	representation.
A.
B.
C.
Solution	to	Problem	
2.23	
(page
80
)
The	expressions	in	these	functions	are	common	program	“idioms”	for
extracting	values	from	a	word	in	which	multiple	bit	fields	have	been
packed.	They	exploit	the	zero-filling	and	sign-extending	properties	of	the
different	shift	operations.	Note	carefully	the	ordering	of	the	cast	and	shift
operations.	In	
,	the	shifts	are	performed	on	unsigned	variable	word
and	hence	are	logical.	In	
,	shifts	are	performed	after	casting	
to
and	hence	are	arithmetic.
A
.	</p>
<p>B
.	
Function	
extracts	a	value	from	the	low-order	8	bits	of	the
argument,	giving	an	integer	ranging	between	0	and	255.	Function
extracts	a	value	from	the	low-order	8	bits	of	the	argument,
but	it	also	performs	sign	extension.	The	result	will	be	a	number
between	–128	and	127.
Solution	to	Problem	
2.24	
(page
82
)
The	effect	of	truncation	is	fairly	intuitive	for	unsigned	numbers,	but	not	for
two's-complement	numbers.	This	exercise	lets	you	explore	its	properties
using	very	small	word	sizes.
Hex
Unsigned
Two's	complement
Original
Truncated
Original
Truncated
Original
Truncated
0
0
0
0
2
2
2
2
9
1
–7
1</p>
<p>11
3
–5
3
15
7
–1
-1
As	
Equation	
2.9
states,	the	effect	of	this	truncation	on	unsigned
values	is	to	simply	find	their	residue,	modulo	8.	The	effect	of	the
truncation	on	signed	values	is	a	bit	more	complex.	According	to
Equation	
2.10
,	we	first	compute	the	modulo	8	residue	of	the
argument.	This	will	give	values	0	through	7	for	arguments	0	through	7,
and	also	for	arguments	–8	through	–1.	Then	we	apply	function	
U2T
to
these	residues,	giving	two	repetitions	of	the	sequences	0	through	3	and	–
4	through	–1.
Solution	to	Problem	
2.25	
(page
83
)
This	problem	is	designed	to	demonstrate	how	easily	bugs	can	arise	due
to	the	implicit	casting	from	signed	to	unsigned.	It	seems	quite	natural	to
pass	parameter	
as	an	unsigned,	since	one	would	never	want	to
use	a	negative	length.	The	stopping	criterion	
also	seems
quite	natural.	But	combining	these	two	yields	an	unexpected	outcome!
Since	parameter	
is	unsigned,	the	computation	0	–	1	is	performed
using	unsigned	arithmetic,	which	is	equivalent	to	modular	addition.	The
result	is	then	
UMax
.	The	≤	comparison	is	also	performed	using	an
unsigned	comparison,	and	since	any	number	is	less	than	or	equal	to
3</p>
<p>UMax
,	the	comparison	always	holds!	Thus,	the	code	attempts	to	access
invalid	elements	of	array	
.
The	code	can	be	fixed	either	by	declaring	
to	be	an	
or	by
changing	the	test	of	the	
loop	to	be	
.
Solution	to	Problem	
2.26	
(page
83
)
This	example	demonstrates	a	subtle	feature	of	unsigned	arithmetic,	and
also	the	property	that	we	sometimes	perform	unsigned	arithmetic	without
realizing	it.	This	can	lead	to	very	tricky	bugs.
A
.	
For	what	cases	will	this	function	produce	an	incorrect	result?
The
function	will	incorrectly	return	1	when	s	is	shorter	than	
.
B
.	
Explain	how	this	incorrect	result	comes	about.
Since	
is
defined	to	yield	an	unsigned	result,	the	difference	and	the
comparison	are	both	computed	using	unsigned	arithmetic.	When
is	shorter	than	
,	the	difference	
should
be	negative,	but	instead	becomes	a	large,	unsigned	number,
which	is	greater	than	0.
C
.	
Show	how	to	fix	the	code	so	that	it	will	work	reliably.
Replace	the
test	with	the	following:</p>
<p>Solution	to	Problem	
2.27	
(page
89
)
This	function	is	a	direct	implementation	of	the	rules	given	to	determine
whether	or	not	an	unsigned	addition	overflows.
Solution	to	Problem	
2.28	
(page
89
)
This	problem	is	a	simple	demonstration	of	arithmetic	modulo	16.	The
easiest	way	to	solve	it	is	to	convert	the	hex	pattern	into	its	unsigned
decimal	value.	For	nonzero	values	of	
x
,	we	must	have	
.
Then	we	convert	the	complemented	value	back	to	hex.
(
−
u
4
x
)</p>
<ul>
<li></li>
</ul>
<h1 id="x-1"><a class="header" href="#x-1">x</a></h1>
<p>16</p>
<p>x
Hex
Decimal
Decimal
Hex
0
0
5
11
8
8
13
3
15
1
Solution	to	Problem	
2.29	
(page
93
)
This	problem	is	an	exercise	to	make	sure	you	understand	two's-
complement	addition.
x
y
x
+	
y
Case
–12
–15
–27
5
1
[10100]
[10001]
[100101]
[00101]
–8
–8
–16
–16
2
[11000]
[11000]
[110000]
[10000]
–9
8
–1
–1
2
4
−
u
x
x</p>
<ul>
<li></li>
</ul>
<p>t
5
y</p>
<p>[10111]
[01000]
[111111]
[11111]
2
5
7
7
3
[00010]
[00101]
[000111]
[00111]
12
4
16
–16
4
[01100]
[00100]
[010000]
[10000]
Solution	to	Problem	
2.30	
(page
94
)
This	function	is	a	direct	implementation	of	the	rules	given	to	determine
whether	or	not	a	two's-complement	addition	overflows.</p>
<p>Solution	to	Problem	
2.31	
(page
94
)
Your	coworker	could	have	learned,	by	studying	
Section	
2.3.2
,	that
two's-complement	addition	forms	an	abelian	group,	and	so	the
expression	
will	evaluate	to	
regardless	of	whether	or	not	the
addition	overflows,	and	that	
will	always	evaluate	to	
.
Solution	to	Problem	
2.32	
(page
94
)
This	function	will	give	correct	values,	except	when	
is	
TMin
.	In	this
case,	we	will	have	
also	equal	to	
TMin
,	and	so	the	call	to	function
will	indicate	overflow	when	
is	negative	and	no	overflow	when
is	nonnegative.	In	fact,	the	opposite	is	true:	
(
,	
TMin
)	should
yield	0	when	
is	negative	and	1	when	it	is	nonnegative.
One	lesson	to	be	learned	from	this	exercise	is	that	
TMin
should	be
included	as	one	of	the	cases	in	any	test	procedure	for	a	function.
Solution	to	Problem	
2.33	
(page
95
)</p>
<p>This	problem	helps	you	understand	two's-complement	negation	using	a
very	small	word	size.
For	
w
=	4,	we	have	
TMin
=	–8.	So	–8	is	its	own	additive	inverse,	while
other	values	are	negated	by	integer	negation.
x
Hex
Decimal
Decimal
Hex
The	bit	patterns	are	the	same	as	for	unsigned	negation.
Solution	to	Problem	
2.34	
(page
98
)
This	problem	is	an	exercise	to	make	sure	you	understand	two's-
complement	multiplication.
4
4
−
t
x</p>
<p>Mode
x
y
x	·	y
Truncated	
x	·	y
Unsigned
4
[100]
5
[101]
20
[010100]
4
[100]
Two's	complement
–4
[100]
–3
[101]
12
[001100]
–4
[100]
Unsigned
2
[010]
7
[111]
14
[001110]
6
[110]
Two's	complement
2
[010]
–1
[111]
–2
[111110]
–2
[110]
Unsigned
6
[110]
6
[110]
36
[100100]
4
[100]
Two's	complement
–2
[110]
–2
[110]
4
[000100]
–4
[100]
Solution	to	Problem	
2.35	
(page
99
)
It	is	not	realistic	to	test	this	function	for	all	possible	values	of	
and	
.
Even	if	you	could	run	10	billion	tests	per	second,	it	would	require	over	58
years	to	test	all	combinations	when	data	type	
is	32	bits.	On	the	other
hand,	it	is	feasible	to	test	your	code	by	writing	the	function	with	data	type
or	
and	then	testing	it	exhaustively.
Here's	a	more	principled	approach,	following	the	proposed	set	of
arguments:
1
.	
We	know	that	
x	·	y
can	be	written	as	a	2
w
-bit	two's-complement
number.	Let	
u
denote	the	unsigned	number	represented	by	the
lower	
w
bits,	and	
v
denote	the	two's-complement	number</p>
<p>represented	by	the	upper	
w
bits.	Then,	based	on	
Equation	
2.3
,
we	can	see	that	
x	·	y
=	
v
2
+	
u
.
We	also	know	that	
u
=	
T2U
(p)
,	since	they	are	unsigned	and
two's-complement	numbers	arising	from	the	same	bit	pattern,	and
so	by	
Equation	
2.6
,	we	can	write	
u
=	
p
+	
p
2
,	where	
p
is
the	most	significant	bit	of	
p
.	Letting	
t
=	
v
+	
p
,	we	have	
x	·	y
=	
p
+
t
2
.
When	
t
=	0,	we	have	
x	.	y	=	p
;	the	multiplication	does	not	overflow.
When	
t
=	0,	we	have	
x	·	y
=	
p
;	the	multiplication	does	overflow.
2
.	
By	definition	of	integer	division,	dividing	
p
by	nonzero	
x
gives	a
quotient	
q
and	a	remainder	
r
such	that	
p
=	
x	·	q	+	r
,	and	|
r
|	&lt;	|
x
|.
(We	use	absolute	values	here,	because	the	signs	of	
x
and	
r
may
differ.	For	example,	dividing	–7	by	2	gives	quotient	–3	and
remainder	–1.)
3
.	
Suppose	
q	=	y
.	Then	we	have	
x	·	y	=	x	·	y	+	r	+	t
2
.	From	this,	we
can	see	that	
r	+	t
2
=	0.	But	|
r
|	&lt;	|
x
|	≤	2
,	and	so	this	identity	can
hold	only	if	
t
=	0,	in	which	case	
r
=	0.
Suppose	
r
=	
t
=	0.	Then	we	will	have	
x	·	y	=	x	·	q
,	implying	that	
y	=
q
.
When	
x
equals	0,	multiplication	does	not	overflow,	and	so	we	see	that	our
code	provides	a	reliable	way	to	test	whether	or	not	two's-complement
multiplication	causes	overflow.
Solution	to	Problem	
2.36	
(page
99
)
w
w
w
–1
w
w
–1
w
–1
w
w
w
w</p>
<p>With	64	bits,	we	can	perform	the	multiplication	without	overflowing.	We
then	test	whether	casting	the	product	to	32	bits	changes	the	value:
Note	that	the	casting	on	the	right-hand	side	of	line	5	is	critical.	If	we
instead	wrote	the	line	as
the	product	would	be	computed	as	a	32-bit	value	(possibly	overflowing)
and	then	sign	extended	to	64	bits.
Solution	to	Problem	
2.37	
(page
99
)</p>
<p>A
.	
This	change	does	not	help	at	all.	Even	though	the	computation	of
will	be	accurate,	the	call	to	
will	cause	this	value	to	be
converted	to	a	32-bit	unsigned	number,	and	so	the	same	overflow
conditions	will	occur.
B
.	
With	
having	a	32-bit	unsigned	number	as	its	argument,	it
cannot	possibly	allocate	a	block	of	more	than	2
bytes,	and	so
there	is	no	point	attempting	to	allocate	or	copy	this	much	memory.
Instead,	the	function	should	abort	and	return	
,	as	illustrated	by
the	following	replacement	to	the	original	call	to	
(line	9):
Solution	to	Problem	
2.38	
(page
102
)
In	
Chapter	
3
,	we	will	see	many	examples	of	the	
LEA</p>
<p>instruction	in
action.	The	instruction	is	provided	to	support	pointer	arithmetic,	but	the	C
32</p>
<p>compiler	often	uses	it	as	a	way	to	perform	multiplication	by	small
constants.
For	each	value	of	
k
,	we	can	compute	two	multiples:	2
(when	
is	0)	and
2
+	1	(when	
is	
).	Thus,	we	can	compute	multiples	1,	2,	3,	4,	5,	8,	and
9.
Solution	to	Problem	
2.39	
(page
103
)
The	expression	simply	becomes	-(x&lt;&lt;
m
).	To	see	this,	let	the	word	size
be	
w
so	that	
n
=	
w
—	1.	Form	B	states	that	we	should	compute	(x&lt;&lt;
w
)	—
(x&lt;&lt;
m
),	but	shifting	
to	the	left	by	
w
will	yield	the	value	
.
Solution	to	Problem	
2.40	
(page
103
)
This	problem	requires	you	to	try	out	the	optimizations	already	described
and	also	to	supply	a	bit	of	your	own	ingenuity.
K
Shifts
Add/Subs
Expression
6
2
1
k
k</p>
<p>31
1
1
-6
2
1
55
2
2
Observe	that	the	fourth	case	uses	a	modified	version	of	form	B.	We	can
view	the	bit	pattern	[110111]	as	having	a	run	of	6	ones	with	a	zero	in	the
middle,	and	so	we	apply	the	rule	for	form	B,	but	then	we	subtract	the	term
corresponding	to	the	middle	zero	bit.
Solution	to	Problem	
2.41	
(page
103
)
Assuming	that	addition	and	subtraction	have	the	same	performance,	the
rule	is	to	choose	form	A	when	
n
=	
m
,	either	form	when	
n
=	
m
+	1,	and
form	B	when	
n
&gt;	
m
+	1.
The	justification	for	this	rule	is	as	follows.	Assume	first	that	
m
&gt;	0.	When
n
=	
m
,	form	A	requires	only	a	single	shift,	while	form	B	requires	two	shifts
and	a	subtraction.	When	
n
=	
m
+	1,	both	forms	require	two	shifts	and
either	an	addition	or	a	subtraction.	When
n
&gt;	
m
+	1,	form	B	requires	only
two	shifts	and	one	subtraction,	while	form	A	requires	
n
—	
m
+	1	&gt;	2	shifts
and	
n
—	
m
&gt;	1	additions.	For	the	case	of	
m
=	0,	we	get	one	fewer	shift
for	both	forms	A	and	B,	and	so	the	same	rules	apply	for	choosing
between	the	two.</p>
<p>Solution	to	Problem	
2.42	
(page
107
)
The	only	challenge	here	is	to	compute	the	bias	without	any	testing	or
conditional	operations.	We	use	the	trick	that	the	expression	
generates	a	word	with	all	ones	if	
is	negative,	and	all	zeros	otherwise.
By	masking	off	the	appropriate	bits,	we	get	the	desired	bias	value.
Solution	to	Problem	
2.43	
(page
107
)
We	have	found	that	people	have	difficulty	with	this	exercise	when
working	directly	with	assembly	code.	It	becomes	more	clear	when	put	in
the	form	shown	in	
.
We	can	see	that	
is	
is	computed	as	
.</p>
<p>We	can	see	that	
is	8;	a	bias	value	of	7	is	added	when	y	is	negative,
and	the	right	shift	is	by	3.
Solution	to	Problem	
2.44	
(page
108
)
These&quot;C	puzzle”	problems	provide	a	clear	demonstration	that
programmers	must	understand	the	properties	of	computer	arithmetic:
A
.	
False
.	Let	
be	–2,147,483,648	(
TMin
).	We	will	then	have	
equal	to	2,147,483,647	(
TMax
).
B
.	
True
.	If	
evaluates	to	0,	then	we	must	have	bit	
x
equal	to	1.	When	shifted	left	by	29,	this	will	become	the	sign	bit.
C
.	
False
.	When	
is	65,535	
,	x*x	is	-131,071	
.
D
.	
True
.	If	
is	nonnegative,	then	
is	nonpositive.
E
.	
False
.	Let	
be	–2,147,483,648	(
TMin
).	Then	both	
and	
are
negative.
F
.	
True
.	Two's-complement	and	unsigned	addition	have	the	same	bit-
level	behavior,	and	they	are	commutative.
32
32
2
32</p>
<p>G
.	
True
.	
.	Thus,	the	left-hand	side	is
equivalent	to	
.
Solution	to	Problem	
2.45	
(page
111
)
Understanding	fractional	binary	representations	is	an	important	step	to
understanding	floating-point	encodings.	This	exercise	lets	you	try	out
some	simple	examples.
0.001
0.125
0.11
0.75
1.1001
1.5625
10.1011
2.6875
1.001
1.125
101.111
5.875
11.0011
3.1875
One	simple	way	to	think	about	fractional	binary	representations	is	to
represent	a	number	as	a	fraction	of	the	form	
.	We	can	write	this	in
binary	using	the	binary	representation	of	
x
,	with	the	binary	point	inserted
1
8
3
4
25
16
43
16
9
8
47
8
51
16
x
2
k</p>
<p>k
positions	from	the	right.	As	an	example,	for	
,	we	have	25
=
11001
.	We	then	put	the	binary	point	four	positions	from	the	right	to	get
1.1001
.
Solution	to	Problem	
2.46	
(page
111
)
In	most	cases,	the	limited	precision	of	floating-point	numbers	is	not	a
major	problem,	because	the	
relative
error	of	the	computation	is	still	fairly
low.	In	this	example,	however,	the	system	was	sensitive	to	the	
absolute
error.
A
.	
We	can	see	that	0.1	—	
has	the	binary	representation
B
.	
Comparing	this	to	the	binary	representation	of	
,	we	can	see
that	it	is	simply	
,	which	is	around	9.54	×	10
.
C
.	
9.54	×	10
×	100	×	60	×	60	×	10	≈	0.343	seconds.
D
.	
0.343	×	2,000	≈	687	meters.
Solution	to	Problem	
2.47	
(page
117
)
25
16
10
2
2
0.000000000000000000000001100
[
1100
]
…
 
2
1
10
2
−
20
 
×
 
1
10
−8
−8</p>
<p>Working	through	floating-point	representations	for	very	small	word	sizes
helps	clarify	how	IEEE	floating	point	works.	Note	especially	the	transition
between	denormalized	and	normalized	values.
Bits
e
E
2
f
M
2
×	
M
V
Decimal
0
0
1
0
0.0
0
0
1
0.25
0
0
1
0.5
0
0
1
0.75
1
0
1
1
1.0
1
0
1
1.25
1
0
1
1.5
1
0
1
1.75
2
1
2
2
2.0
2
1
2
2.5
2
1
2
3
3.0
2
1
2
3.5
—
—
—
—
—
—
∞
—
—
—
—
—
—
—
NaN
—
—
—
—
—
—
—
NaN
—
E
E
0
4
0
4
0
4
1
4
1
4
1
4
1
4
2
4
2
4
2
4
1
2
3
4
3
4
3
4
3
4
0
4
4
4
4
4
1
4
5
4
5
4
5
4
2
4
6
4
6
4
3
2
3
4
7
4
7
4
7
4
0
4
4
4
8
4
1
4
5
4
10
4
5
2
2
4
6
4
12
4
3
4
7
4
14
4
7
2</p>
<p>—
—
—
—
—
—
NaN
—
Solution	to	Problem	
2.48	
(page
119
)
Hexadecimal	
is	equivalent	to	binary
[1101011001000101000001].	Shifting	this	right	21	places	gives
1.101011001000101000001
×	2
.	We	form	the	fraction	field	by	dropping
the	leading	1	and	adding	two	zeros,	giving
The	exponent	is	formed	by	adding	bias	127	to	21,	giving	148	(binary
[10010100]).	We	combine	this	with	a	sign	field	of	0	to	give	a	binary
representation
We	see	that	the	matching	bits	in	the	two	representations	correspond	to
the	low-order	bits	of	the	integer,	up	to	the	most	significant	bit	equal	to	1
matching	the	high-order	21	bits	of	the	fraction:
2
21
[
10101100100010100000100
]
[
01001010010101100100010100000100
]</p>
<p>Solution	to	Problem	
2.49	
(page
120
)
This	exercise	helps	you	think	about	what	numbers	cannot	be	represented
exactly	in	floating	point.
A
.	
The	number	has	binary	representation	1,	followed	by	
n
zeros,
followed	by	1,	giving	value	2
+	1.
B
.	
When	
n
=	23,	the	value	is	2
+	1	=	16,777,217.
Solution	to	Problem	
2.50	
(page
121
)
Performing	rounding	by	hand	helps	reinforce	the	idea	of	round-to-even
with	binary	numbers.
Origin;
Rounded
10.010
10.0
2
10.011
10.1
10.110
11.0
3
11.001
11.0
3
n
+1
24
2
2
1
4
2
2
3
8
2
1
2
2
2
3
4
2
3
1
8</p>
<p>Solution	to	Problem	
2.51	
(page
122
)
A
.	
Looking	at	the	nonterminating	sequence	for	
,	we	see	that	the
2	bits	to	the	right	of	the	rounding	position	are	1,	so	a	better
approximation	to	
would	be	obtained	by	incrementing	
x
to	get
x
′	=	0.00011001100110011001101
,	which	is	larger	than	0.1.
B
.	
We	can	see	that	
x
′	–	0.1	has	binary	representation
Comparing	this	to	the	binary	representation	of	
,	we	can	see
that	it	is	2
×	
,	which	is	around	2.38	×	10
.
C
.	
2.38	×	10
×	100	×	60	×	60	×	10	≈	0.086	seconds,	a	factor	of	4
less	than	the	error	in	the	Patriot	system.
D
.	
0.086	×	2,000	≈	171	meters.
Solution	to	Problem	
2.52	
(page
122
)
This	problem	tests	a	lot	of	concepts	about	floating-point	representations,
including	the	encoding	of	normalized	and	denormalized	values,	as	well
as	rounding.
Format	A
Format	B
1</p>
<p>10</p>
<p>1</p>
<p>10</p>
<p>2
0.0000000000000000000000000
[
1100
]
1</p>
<p>10</p>
<p>−22
2
−
22
×
1
10
−8
−8</p>
<p>Bits
Value
Bits
Value
Comments
1
1
Round	down
16
Round	up
Denorm	→	norm
Solution	to	Problem	
2.53	
(page
125
)
In	general,	it	is	better	to	use	a	library	macro	rather	than	inventing	your
own	code.	This	code	seems	to	work	on	a	variety	of	machines,	however.
We	assume	that	the	value	
overflows	to	infinity.
15
2
15
2
25
32
3
4
31
2
1
64
1
64</p>
<p>Solution	to	Problem	
2.54	
(page
125
)
Exercises	such	as	this	one	help	you	develop	your	ability	to	reason	about
floating-point	operations	from	a	programmer's	perspective.	Make	sure
you	understand	each	of	the	answers.
A
.	
Yes,	since	
has	greater	precision	and	range	than	
.
B
.	
No.	For	example,	when	
is	
TMax.
C
.	
No.	For	example,	when	
is	
,	we	will	get	+	∞	on	the	right.
D
.	
Yes,	since	
has	greater	precision	and	range	than	
.
E
.	
Yes,	since	a	floating-point	number	is	negated	by	simply	inverting
its	sign	bit.
F
.	
Yes,	the	numerators	and	denominators	will	both	be	converted	to
floating-point	representations	before	the	division	is	performed.
G
.	
Yes,	although	it	may	overflow	to	+	∞.
H
.	
No.	For	example,	when	f	is	
and	
is	1.0,	the	expression
will	be	rounded	to	
,	and	so	the	expression	on	the	left-</p>
<p>hand	side	will	evaluate	to	0.0,	while	the	right-hand	side	will	be	1.0.</p>
<p>Chapter	
3	
Machine-Level
Representation	of	Programs
3.1	
A	Historical	Perspective	
166
3.2	
Program	Encodings</p>
<p>169
3.3	
Data	Formats</p>
<p>177
3.4	
Accessing	Information</p>
<p>179
3.5	
Arithmetic	and	Logical	Operations</p>
<p>191
3.6	
Control</p>
<p>200
3.7	
Procedures</p>
<p>238
3.8	
Array	Allocation	and	Access</p>
<p>255
3.9	
Heterogeneous	Data	Structures</p>
<p>265
3.10	
Combining	Control	and	Data	in	Machine-Level	Programs</p>
<p>276
3.11	
Floating-Point	Code</p>
<p>293
3.12	
Summary</p>
<p>309
Bibliographic	Notes	
310</p>
<p>Homework	Problems	
311
Solutions	to	Practice	Problems</p>
<p>325
Computers	execute	
machine	code
,	sequences	of
bytes	encoding	the	low-level	operations	that
manipulate	data,	manage	memory,	read	and	write
data	on	storage	devices,	and	communicate	over
networks.	A	compiler	generates	machine	code
through	a	series	of	stages,	based	on	the	rules	of	the
programming	language,	the	instruction	set	of	the
target	machine,	and	the	conventions	followed	by	the
operating	system.	The	
GCC</p>
<p>C	compiler	generates	its
output	in	the	form	of	
assembly	code
,	a	textual
representation	of	the	machine	code	giving	the
individual	instructions	in	the	program.	
G
CC</p>
<p>then
invokes	both	an	
assembler
and	a	
linker
to	generate
the	executable	machine	code	from	the	assembly
code.	In	this	chapter,	we	will	take	a	close	look	at
machine	code	and	its	human-readable
representation	as	assembly	code.
When	programming	in	a	high-level	language	such
as	C,	and	even	more	so	in	Java,	we	are	shielded
from	the	detailed	machine-level	implementation	of
our	program.	In	contrast,	when	writing	programs	in
assembly	code	(as	was	done	in	the	early	days	of
computing)	a	programmer	must	specify	the	low-level
instructions	the	program	uses	to	carry	out	a
computation.	Most	of	the	time,	it	is	much	more</p>
<p>productive	and	reliable	to	work	at	the	higher	level	of
abstraction	provided	by	a	high-level	language.	The
type	checking	provided	by	a	compiler	helps	detect
many	program	errors	and	makes	sure	we	reference
and	manipulate	data	in	consistent	ways.	With
modern	optimizing	compilers,	the	generated	code	is
usually	at	least	as	efficient	as	what	a	skilled
assembly-language	programmer	would	write	by
hand.	Best	of	all,	a	program	written	in	a	high-level
language	can	be	compiled	and	executed	on	a
number	of	different	machines,	whereas	assembly
code	is	highly	machine	specific.
So	why	should	we	spend	our	time	learning	machine
code?	Even	though	compilers	do	most	of	the	work
in	generating	assembly	code,	being	able	to	read
and	understand	it	is	an	important	skill	for	serious
programmers.	By	invoking	the	compiler	with
appropriate	command-line	parameters,	the	compiler
will	generate	a	file	showing	its	output	in	assembly-
code	form.	By	reading	this	code,	we	can	understand
the	optimization	capabilities	of	the	compiler	and
analyze	the	underlying	inefficiencies	in	the	code.	As
we	will	experience	in	
Chapter	
5
,	programmers
seeking	to	maximize	the	performance	of	a	critical
section	of	code	often	try	different	variations	of	the
source	code,	each	time	compiling	and	examining
the	generated	assembly	code	to	get	a	sense	of	how
efficiently	the	program	will	run.	Furthermore,	there</p>
<p>are	times	when	the	layer	of	abstraction	provided	by
a	high-level	language	hidesinformationabouttherun-
timebehaviorofaprogramthatweneedtounder-stand.
For	example,	when	writing	concurrent	programs
using	a	thread	package,	as	covered	in	
Chapter
12
,	it	is	important	to	understand	how	program	data
are	shared	or	kept	private	by	the	different	threads
and	precisely	how	and	where	shared	data	are
accessed.	Such	information	is	visible	at	the
machine-code	level.	As	another	example,	many	of
the	ways	programs	can	be	attacked,	allowing
malware	to	infest	a	system,	involve	nuances	of	the
way	programs	store	their	run-time	control
information.	Many	attacks	involve	exploiting
weaknesses	in	system	programs	to	overwrite
information	and	thereby	take	control	of	the	system.
Understanding	how	these	vulnerabilities	arise	and
how	to	guard	against	them	requires	a	knowledge	of
the	machine-level	representation	of	programs.	The
need	for	programmers	to	learn	
machine	code	has
shifted	over	the	years	from	one	of	being	able	to
write	programs	directly	in	assembly	code	to	one	of
being	able	to	read	and	understand	the	code
generated	by	compilers.
In	this	chapter,	we	will	learn	the	details	of	one
particular	assembly	language	and	see	how	C
programs	get	compiled	into	this	form	of	machine
code.	Reading	the	assembly	code	generated	by	a</p>
<p>compiler	involves	a	different	set	of	skills	than	writing
assembly	code	by	hand.	We	must	understand	the
transformations	typical	compilers	make	in
converting	the	constructs	of	C	into	machine	code.
Relative	to	the	computations	expressed	in	the	C
code,	optimizing	compilers	can	rearrange	execution
order,	eliminate	unneeded	computations,	replace
slow	operations	with	faster	ones,	and	even	change
recursive	computations	into	iterative	ones.
Understanding	the	relation	between	source	code
and	the	generated	assembly	can	often	be	a
challenge—it's	much	like	putting	together	a	puzzle
having	a	slightly	different	design	than	the	picture	on
the	box.	It	is	a	form	of	
reverse	engineering
—trying
to	understand	the	process	by	which	a	system	was
created	by	studying	the	system	and	working
backward.	In	this	case,	the	system	is	a	machine-
generated	assembly-language	program,	rather	than
something	designed	by	a	human.	This	simplifies	the
task	of	reverse	engineering	because	the	generated
code	follows	fairly	regular	patterns	and	we	can	run
experiments,	having	the	compiler	generate	code	for
many	different	programs.	In	our	presentation,	we
give	many	examples	and	provide	a	number	of
exercises	illustrating	different	aspects	of	assembly
language	and	compilers.	This	is	a	subject	where
mastering	the	details	is	a	prerequisite	to	under-
standing	the	deeper	and	more	fundamental
concepts.	Those	who	say	&quot;I	understand	the	general</p>
<p>principles,	I	don't	want	to	bother	learning	the	details&quot;
are	deluding	themselves.	It	is	critical	for	you	to
spend	time	studying	the	examples,	working	through
the	exercises,	and	checking	your	solutions	with
those	provided.
Our	presentation	is	based	on	x86-64,	the	machine
language	for	most	of	the	processors	found	in	today's
laptop	and	desktop	machines,	as	well	as	those	that
power	very	large	data	centers	and	supercomputers.
This	language	has	evolved	over	a	long	history,
starting	with	Intel	Corporation's	first	16-bit	processor
in	1978,	through	to	the	expansion	to	32	bits,	and
most	recently	to	64	bits.	Along	the	way,	features
have	been	added	to	make	better	use	of	the
available	semiconductor	technology,	and	to	satisfy
the	demands	of	the	marketplace.	Much	of	the
development	has	been	driven	by	Intel,	but	its	rival
Advanced	Micro	Devices	(AMD)	has	also	made
important	contributions.	The	result	is	a	rather
peculiar	design	with	features	that	make	sense	only
when	viewed	from	a	historical	perspective.	It	is	also
laden	with	features	providing	backward	compatibility
that	are	not	used	by	modern	compilers	and
operating	systems.	We	will	focus	on	the	subset	of
the	features	used	by	
GCC</p>
<p>and	Linux.	This	allows	us
to	avoid	much	of	the	complexity	and	many	of	the
arcane	features	of	x86-64.</p>
<p>Our	technical	presentation	starts	with	a	quick	tour	to
show	the	relation	between	C,	assembly	code,	and
machine	code.	We	then	proceed	to	the	details	of
x86-64,	starting	with	the	representation	and
manipulation	of	data	and	the	implementation	of
control.	We	see	how	control	constructs	in	C,	such	as
,	
,	and	
statements,	are	implemented.
We	then	cover	the	implementation	of	procedures,
including	how	the	program	maintains	a	run-time
stack	to	support	the
Web	Aside	ASM:IA32	
IA32
programming
IA32,	the	32-bit	predecessor	to	x86-64,	was
introduced	by	Intel	in	1985.	It	served	as	the
machine	language	of	choice	for	several
decades.	Most	x86	microprocessors	sold
today,	and	most	operating	systems	installed
on	these	machines,	are	designed	to	run	x86-
64.	However,	they	can	also	execute	IA32
programs	in	a	backward	compatibility	mode.
As	a	result,	many	application	programs	are
still	based	on	IA32.	In	addition,	many	existing
systems	cannot	execute	x86-64,	due	to
limitations	of	their	hardware	or	system
software.	IA32	continues	to	be	an	important</p>
<p>machine	language.	You	will	find	that	having	a
background	in	x86-64	will	enable	you	to	learn
the	IA32	machine	language	quite	readily.
passing	of	data	and	control	between	procedures,	as
well	as	storage	for	local	variables.	Next,	we
consider	how	data	structures	such	as	arrays,
structures,	and	unions	are	implemented	at	the
machine	level.	With	this	background	in	machine-
level	programming,	we	can	examine	the	problems	of
out-of-bounds	memory	references	and	the
vulnerability	of	systems	to	buffer	overflow	attacks.
We	finish	this	part	of	the	presentation	with	some	tips
on	using	the	
GDB</p>
<p>debugger	for	examining	the	run-
time	behavior	of	a	machine-level	program.	The
chapter	concludes	with	a	presentation	on	machine-
program	representations	of	code	involving	floating-
point	data	and	operations.
The	computer	industry	has	recently	made	the
transition	from	32-bit	to	64-bit	machines.	A	32-bit
machine	can	only	make	use	of	around	4	gigabytes
(2
bytes)	of	random	access	memory,	With	memory
prices	dropping	at	dramatic	rates,	and	our
computational	demands	and	data	sizes	increasing,
it	has	become	both	economically	feasible	and
technically	desirable	to	go	beyond	this	limitation.
Current	64-bit	machines	can	use	up	to	256
terabytes	(2
bytes)	of	memory,	and	could	readily
be	extended	to	use	up	to	16	exabytes	(2
bytes).
32
48
64</p>
<p>Although	it	is	hard	to	imagine	having	a	machine	with
that	much	memory,	keep	in	mind	that	4	gigabytes
seemed	like	an	extreme	amount	of	memory	when
32-bit	machines	became	commonplace	in	the	1970s
and	1980s.
Our	presentation	focuses	on	the	types	of	machine-
level	programs	generated	when	compiling	C	and
similar	programming	languages	targeting	modern
operating	systems.	As	a	consequence,	we	make	no
attempt	to	describe	many	of	the	features	of	x86-64
that	arise	out	of	its	legacy	support	for	the	styles	of
programs	written	in	the	early	days	of
microprocessors,	when	much	of	the	code	was
written	manually	and	where	programmers	had	to
struggle	with	the	limited	range	of	addresses	allowed
by	16-bit	machines.</p>
<p>3.1	
A	Historical	Perspective
The	Intel	processor	line,	colloquially	referred	to	as	
x86
,	has	followed	a
long	evolutionary	development.	It	started	with	one	of	the	first	single-chip
16-bit	microprocessors,	where	many	compromises	had	to	be	made	due
to	the	limited	capabilities	of	integrated	circuit	technology	at	the	time.
Since	then,	it	has	grown	to	take	advantage	
of	technology	improvements
as	well	as	to	satisfy	the	demands	for	higher	performance	and	for
supporting	more	advanced	operating	systems.
The	list	that	follows	shows	some	models	of	Intel	processors	and	some	of
their	key	features,	especially	those	affecting	machine-level	programming.
We	use	the	number	of	transistors	required	to	implement	the	processors
as	an	indication	of	how	they	have	evolved	in	complexity.	In	this	table,	&quot;K&quot;
denotes	1,000	(10
),	&quot;M&quot;	denotes	1,000,000	(10
),	and	&quot;G&quot;	denotes
1,000,000,000	(10
).
8086	(1978,	29	K	transistors).	
One	of	the	first	single-chip,	16-bit
microprocessors.	The	8088,	a	variant	of	the	8086	with	an	8-bit
external	bus,	formed	the	heart	of	the	original	IBM	personal	computers.
IBM	contracted	with	then-tiny	Microsoft	to	develop	the	MS-DOS
operating	system.	The	original	models	came	with	32,768	bytes	of
memory	and	two	floppy	drives	(no	hard	drive).	Architecturally,	the
machines	were	limited	to	a	655,360-byte	address	space—addresses
were	only	20	bits	long	(1,048,576	bytes	addressable),	and	the
operating	system	reserved	393,216	bytes	for	its	own	use.	In	1980,
Intel	introduced	the	8087	floating-point	coprocessor	(45	K	transistors)
3
6
9</p>
<p>to	operate	alongside	an	8086	or	8088	processor,	executing	the
floating-point	instructions.	The	8087	established	the	floating-point
model	for	the	x86	line,	often	referred	to	as	&quot;x87.&quot;
80286	(1982,	134	K	transistors).	
Added	more	(and	now	obsolete)
addressing	modes.	Formed	the	basis	of	the	IBM	PC-AT	personal
computer,	the	original	platform	for	MS	Windows.
i386	(1985,	275	K	transistors).	
Expanded	the	architecture	to	32	bits.
Added	the	flat	addressing	model	used	by	Linux	and	recent	versions	of
the	Windows	operating	system.	This	was	the	first	machine	in	the
series	that	could	fully	support	a	Unix	operating	system.
i486	(1989,	1.2	M	transistors).	
Improved	performance	and	integrated
the	floating-point	unit	onto	the	processor	chip	but	did	not	significantly
change	the	instruction	set.
Pentium	(1993,	3.1	M	transistors).	
Improved	performance	but	only
added	minor	extensions	to	the	instruction	set.
PentiumPro	(1995,	5.5	M	transistors).	
Introduced	a	radically	new
processor	design,	internally	known	as	the	
P6
microarchitecture.
Added	a	class	of	&quot;conditional	move&quot;	instructions	to	the	instruction	set.
Pentium/MMX	(1997,	4.5	M	transistors).	
Added	new	class	of
instructions	to	the	Pentium	processor	for	manipulating	vectors	of
integers.	Each	datum	can	be	1,	2,	or	4	bytes	long.	Each	vector	totals
64	bits.
Pentium	II	(1997,	7	M	transistors).	
Continuation	of	the	P6
microarchitecture.
Pentium	III	(1999,	8.2	M	transistors).	
Introduced	SSE,	a	class	of
instructions	for	manipulating	vectors	of	integer	or	floating-point	data.</p>
<p>Each	datum	can	be	1,	2,	or	4	bytes,	packed	into	vectors	of	128	bits.
Later	versions	of	this	chip	
went	up	to	24	M	transistors,	due	to	the
incorporation	of	the	level-2	cache	on	chip.
Pentium	4	(2000,	42	M	transistors).	
Extended	SSE	to	SSE2,	adding
new	data	types	(including	double-precision	floating	point),	along	with
144	new	instructions	for	these	formats.	With	these	extensions,
compilers	can	use	SSE	instructions,	rather	than	x87	instructions,	to
compile	floating-point	code.
Pentium	4E	(2004,	125	M	transistors).	
Added	
hyperthreading
,	a
method	to	run	two	programs	simultaneously	on	a	single	processor,	as
well	as	EM64T,	Intel's	implementation	of	a	64-bit	extension	to	IA32
developed	by	Advanced	Micro	Devices	(AMD),	which	we	refer	to	as
x86-64.
Core	2	(2006,	291	M	transistors).	
Returned	to	a	microarchitecture
similar	to	P6.	First	
multi-core
Intel	microprocessor,	where	multiple
processors	are	implemented	on	a	single	chip.	Did	not	support
hyperthreading.
Core	i7,	Nehalem	(2008,	781	M	transistors).	
Incorporated	both
hyperthreading	and	multi-core,	with	the	initial	version	supporting	two
executing	programs	on	each	core	and	up	to	four	cores	on	each	chip.
Core	i7,	Sandy	Bridge	(2011,	1.17	G	transistors).	
Introduced	AVX,
an	extension	of	the	SSE	to	support	data	packed	into	256-bit	vectors.
Core	i7,	Haswell	(2013,	1.4	G	transistors).	
Extended	AVX	to	AVX2,
adding	more	instructions	and	instruction	formats.
Each	successive	processor	has	been	designed	to	be	backward
compatible—able	to	run	code	compiled	for	any	earlier	version.	As	we	will</p>
<p>see,	there	are	many	strange	artifacts	in	the	instruction	set	due	to	this
evolutionary	heritage.	Intel	has	had	several	names	for	their	processor
line,	including	
IA32
,	for	&quot;Intel	Architecture	32-bit&quot;	and	most	recently
Intel64
,	the	64-bit	extension	to	IA32,	which	we	will	refer	to	as	
x86-64
.	We
will	refer	to	the	overall	line	by	the	commonly	used	colloquial	name	&quot;x86,&quot;
reflecting	the	processor	naming	conventions	up	through	the	i486.
Over	the	years,	several	companies	have	produced	processors	that	are
compatible	with	Intel	processors,	capable	of	running	the	exact	same
machine-level	programs.	Chief	among	these	is	Advanced	Micro	Devices
(AMD).	For	years,	AMD	lagged	just	behind	Intel	in	technology,	forcing	a
marketing	strategy	where	they	produced	processors	that	were	less
expensive	although	somewhat	lower	in	performance.	They	became	more
competitive	around	2002,	being	the	first	to	break	the	1-gigahertz	clock-
speed	barrier	for	a	commercially	available	microprocessor,	and
introducing	x86-64,	the	widely	adopted	64-bit	extension	to	Intel's	IA32.
Although	we	will	talk	about	Intel	processors,	our	presentation	holds	just
as	well	for	the	compatible	processors	produced	by	Intel's	rivals.
Much	of	the	complexity	of	x86	is	not	of	concern	to	those	interested	in
programs	for	the	Linux	operating	system	as	generated	by	the	
GCC
compiler.	The	memory	model	provided	in	the	original	8086	and	its
extensions	in	the	80286	became	obsolete	with	the	i386.	The	original	x87
floating-point	instructions	became	obsolete
Aside	
Moore's	Law</p>
<p>If	we	plot	the	number	of	transistors	in	the	different	Intel	processors
versus	the	year	of	introduction,	and	use	a	logarithmic	scale	for	the
y
-axis,	we	can	see	that	the	growth	has	been	phenomenal.	Fitting	a
line	through	the	data,	we	see	that	the	number	of	transistors
increases	at	an	annual	rate	of	approximately	37%,	meaning	that
the	number	of	transistors	doubles	about	every	26	months.	This
growth	has	been	sustained	over	the	multiple-decade	history	of	x86
microprocessors.
In	1965,	Gordon	Moore,	a	founder	of	Intel	Corporation,
extrapolated	from	the	chip	technology	of	the	day	(by	which	they
could	fabricate	circuits	with	around	64	transistors	on	a	single	chip)
to	predict	that	the	number	of	transistors	per	chip	would	double
every	year	for	the	next	10	years.	This	prediction	became	known
as	
Moore's	Law
.	As	it	turns	out,	his	prediction	was	just	a	little	bit
optimistic,	but	also	too	short-sighted.	Over	more	than	50	years,
the	semiconductor	industry	has	been	able	to	double	transistor
counts	on	average	every	18	months.</p>
<p>Similar	exponential	growth	rates	have	occurred	for	other	aspects
of	computer	technology,	including	the	storage	capacities	of
magnetic	disks	and	semiconductor	memories.	These	remarkable
growth	rates	have	been	the	major	driving	forces	of	the	computer
revolution.
with	the	introduction	of	SSE2.	Although	we	see	vestiges	of	the	historical
evolution	of	x86	in	x86-64	programs,	many	of	the	most	arcane	features	of
x86	do	not	appear.</p>
<p>3.2	
Program	Encodings
Suppose	we	write	a	C	program	as	two	files	
and	
We	can	then
compile	this	code	using	a	Unix	command	line:
The	command	
indicates	the	
GCC</p>
<p>C	compiler.	Since	this	is	the	default
compiler	on	Linux,	we	could	also	invoke	it	as	simply	
.	The	command-
line	option	–
instructs	the	compiler	to	apply	a	level	of	optimization	that
yields	machine	code	that	follows	the	overall	structure	of	the	original	C
code.	Invoking	higher	levels	of	optimization	can	generate	code	that	is	so
heavily	transformed	that	the	relationship	between	the	generated	machine
code	and	the	original	source	code	is	difficult	to	understand.	We	will
therefore	use	–
optimization	as	a	learning	tool	and	then	see	what
happens	as	we	increase	the	level	of	optimization.	In	practice,	higher
levels	of	optimization	(e.g.,	specified	with	the	option	–
or	–
)	are
considered	a	better	choice	in	terms	of	the	resulting	program
performance.</p>
<ol>
<li></li>
</ol>
<p>This	optimization	level	was	introduced	in	
GCC</p>
<p>version	4.8.	Earlier	versions	of	
GCC
,	as	well	as
non-GNU	compilers,	will	not	recognize	this	option.	For	these,	using	optimization	level	one
(specified	with	the	command-line	flag	
)	is	probably	the	best	choice	for	generating	code	that
follows	the	original	program	structure.
1</p>
<p>The	
command	invokes	an	entire	sequence	of	programs	to	turn	the
source	code	into	executable	code.	First,	the	C	
preprocessor
expands	the
source	code	to	include	any	files	specified	with	
commands	and
to	expand	any	macros,	specified	with	
declarations.	Second,	the
compiler
generates	assembly-code	versions	of	the	two	source	files
having	names	
and	
Next,	the	
assembler
converts	the
assembly	code	into	binary	
object-code
files	
and	
Object	code
is	one	form	of	machine	code—it	contains	binary	representations	of	all	of
the	instructions,	but	the	addresses	of	global	values	are	not	yet	filled	in.
Finally,	the	
linker
merges	these	two	object-code	files	along	with	code
implementing	library	functions	(e.g.,	
)	and	generates	the	final
executable	code	file	
(as	specified	by	the	command-line	directive	
).
Executable	code	is	the	second	form	of	machine	code	we	will	consider—it
is	the	exact	form	of	code	that	is	executed	by	the	processor.	The	relation
between	these	different	forms	of	machine	code	and	the	linking	process	is
described	in	more	detail	in	
Chapter	
7
.
3.2.1	
Machine-Level	Code
As	described	in	
Section	
1.9.3
,	computer	systems	employ	several
different	forms	of	abstraction,	hiding	details	of	an	implementation	through
the	use	of	a	simpler	abstract	model.	Two	of	these	are	especially
important	for	machine-level	programming.	First,	the	format	and	behavior
of	a	machine-level	program	is	defined	by	the	
instruction	set	architecture
,
or	ISA,	defining	the	processor	state,	the	format	of	the	instructions,	and
the	effect	each	of	these	instructions	will	have	on	the	state.	Most	ISAs,
including	x86-64,	describe	the	behavior	of	a	program	as	if	each</p>
<p>instruction	is	executed	in	sequence,	with	one	instruction	completing
before	the	next	one	begins.	The	processor	hardware	is	far	more
elaborate,	executing	many	instructions	concurrently,	but	it	employs
safeguards	to	ensure	that	the	overall	behavior	matches	the	sequential
operation	dictated	by	the	ISA.	Second,	the	memory	addresses	used	by	a
machine-level	program	are	
virtual	addresses
,	providing	a	memory	model
that	
appears	to	be	a	very	large	byte	array.	The	actual	implementation	of
the	memory	system	involves	a	combination	of	multiple	hardware
memories	and	operating	system	software,	as	described	in	
Chapter	
9
.
The	compiler	does	most	of	the	work	in	the	overall	compilation	sequence,
transforming	programs	expressed	in	the	relatively	abstract	execution
model	provided	by	C	into	the	very	elementary	instructions	that	the
processor	executes.	The	assembly-code	representation	is	very	close	to
machine	code.	Its	main	feature	is	that	it	is	in	a	more	readable	textual
format,	as	compared	to	the	binary	format	of	machine	code.	Being	able	to
understand	assembly	code	and	how	it	relates	to	the	original	C	code	is	a
key	step	in	understanding	how	computers	execute	programs.
The	machine	code	for	x86-64	differs	greatly	from	the	original	C	code.
Parts	of	the	processor	state	are	visible	that	normally	are	hidden	from	the
C	programmer:
The	
program	counter
(commonly	referred	to	as	the	PC,	and	called
in	x86-64)	indicates	the	address	in	memory	of	the	next
instruction	to	be	executed.
The	integer	
register	file
contains	16	named	locations	storing	64-bit
values.	These	registers	can	hold	addresses	(corresponding	to	C
pointers)	or	integer	data.	Some	registers	are	used	to	keep	track	of</p>
<p>critical	parts	of	the	program	state,	while	others	are	used	to	hold
temporary	data,	such	as	the	arguments	and	local	variables	of	a
procedure,	as	well	as	the	value	to	be	returned	by	a	function.
The	condition	code	registers	hold	status	information	about	the	most
recently	executed	arithmetic	or	logical	instruction.	These	are	used	to
implement	conditional	changes	in	the	control	or	data	flow,	such	as	is
required	to	implement	if	and	while	statements.
A	set	of	vector	registers	can	each	hold	one	or	more	integer	or	floating-
point	values.
Whereas	C	provides	a	model	in	which	objects	of	different	data	types	can
be	declared	and	allocated	in	memory,	machine	code	views	the	memory
as	simply	a	large	byte-addressable	array.	Aggregate	data	types	in	C	such
as	arrays	and	structures	are	represented	in	machine	code	as	contiguous
collections	of	bytes.	Even	for	scalar	data	types,	assembly	code	makes	no
distinctions	between	signed	or	unsigned	integers,	between	different	types
of	pointers,	or	even	between	pointers	and	integers.
The	program	memory	contains	the	executable	machine	code	for	the
program,	some	information	required	by	the	operating	system,	a	run-time
stack	for	managing	procedure	calls	and	returns,	and	blocks	of	memory
allocated	by	the	user	(e.g.,	by	using	the	
library	function).	As
mentioned	earlier,	the	program	memory	is	addressed	using	virtual
addresses.	At	any	given	time,	only	limited	subranges	of	virtual	addresses
are	considered	valid.	For	example,	x86-64	virtual	addresses	are
represented	by	64-bit	words.	In	current	implementations	of	these
machines,	the	upper	16	bits	must	be	set	to	zero,	and	so	an	address	can
potentially	specify	a	byte	over	a	range	of	2
,	or	64	terabytes.	More
48</p>
<p>typical	programs	will	only	have	access	to	a	few	megabytes,	or	perhaps
several	gigabytes.	The	operating	system	manages
Aside	
The	ever-changing	forms	of
generated	code
In	our	presentation,	we	will	show	the	code	generated	by	a
particular	version	of	
GCC</p>
<p>with	particular	settings	of	the	command-
line	options.	If	you	compile	code	on	your	own	machine,	chances
are	you	will	be	using	a	different	compiler	or	a	different	version	of
GCC</p>
<p>and	hence	will	generate	different	code.	The	open-source
community	supporting	
GCC</p>
<p>keeps	changing	the	code	generator,
attempting	to	generate	more	efficient	code	according	to	changing
code	guidelines	provided	by	the	microprocessor	manufacturers.
Our	goal	in	studying	the	examples	shown	in	our	presentation	is	to
demonstrate	how	to	examine	assembly	code	and	map	it	back	to
the	constructs	found	in	high-level	programming	languages.	You
will	need	to	adapt	these	techniques	to	the	style	of	code	generated
by	your	particular	compiler.
this	virtual	address	space,	translating	virtual	addresses	into	the	physical
addresses	of	values	in	the	actual	processor	memory.
A	single	machine	instruction	performs	only	a	very	elementary	operation.
For	example,	it	might	add	two	numbers	stored	in	registers,	transfer	data
between	memory	and	a	register,	or	conditionally	branch	to	a	new
instruction	address.	The	compiler	must	generate	sequences	of	such
instructions	to	implement	program	constructs	such	as	arithmetic
expression	evaluation,	loops,	or	procedure	calls	and	returns.</p>
<p>3.2.2	
Code	Examples
Suppose	we	write	a	C	code	file	
containing	the	following	function
definition:
To	see	the	assembly	code	generated	by	the	C	compiler,	we	can	use	the	-
S	option	on	the	command	line:
This	will	cause	
GCC</p>
<p>to	run	the	compiler,	generating	an	assembly	file
,	and	go	no	further.	(Normally	it	would	then	invoke	the	assembler
to	generate	an	object-code	file.)
The	assembly-code	file	contains	various	declarations,	including	the
following	set	of	lines:</p>
<p>Aside	
How	do	I	display	the	byte
representation	of	a	program?
To	display	the	binary	object	code	for	a	program	(say,	
),	we
use	a	
disassembler
(described	below)	to	determine	that	the	code
for	the	procedure	is	14	bytes	long.	Then	we	run	the	GNU
debugging	tool	
GDB</p>
<p>on	file	
and	give	it	the	command
telling	it	to	display	(abbreviated	<code>x')	14	hex-formatted	(also	</code>x')
bytes	(`b')	starting	at	the	address	where	function	multstore	is
located.	You	will	find	that	
GDB
has	many	useful	features	for
analyzing	machine-level	programs,	as	will	be	discussed	in
Section	
3.10.2
.</p>
<p>Each	indented	line	in	the	code	corresponds	to	a	single	machine
instruction.	For	example,	the	
instruction	indicates	that	the	contents
of	register	
should	be	pushed	onto	the	program	stack.	All	information
about	local	variable	names	or	data	types	has	been	stripped	away.
If	we	use	the	-c	command-line	option,	
GCC</p>
<p>will	both	compile	and
assemble	the	code
This	will	generate	an	object-code	file	
that	is	in	binary	format	and
hence	cannot	be	viewed	directly.	Embedded	within	the	1,368	bytes	of	the
file	
is	a	14-byte	sequence	with	the	hexadecimal	representation
This	is	the	object	code	corresponding	to	the	assembly	instructions	listed
previously.	A	key	lesson	to	learn	from	this	is	that	the	program	executed
by	the	machine	is	simply	a	sequence	of	bytes	encoding	a	series	of
instructions.	The	machine	has	very	little	information	about	the	source
code	from	which	these	instructions	were	generated.</p>
<p>To	inspect	the	contents	of	machine-code	files,	a	class	of	programs	known
as	
disassemblers
can	be	invaluable.	These	programs	generate	a	format
similar	to	assembly	code	from	the	machine	code.	With	Linux	systems,	the
program	
OBJDUMP</p>
<p>(for	&quot;object	dump&quot;)	can	serve	this	role	given	the	
command-line	flag:
The	result	(where	we	have	added	line	numbers	on	the	left	and
annotations	in	italicized	text)	is	as	follows:</p>
<p>On	the	left	we	see	the	14	hexadecimal	byte	values,	listed	in	the	byte
sequence	shown	earlier,	partitioned	into	groups	of	1	to	5	bytes	each.
Each	of	these	groups	is	a	single	instruction,	with	the	assembly-language
equivalent	shown	on	the	right.
Several	features	about	machine	code	and	its	disassembled
representation	are	worth	noting:
x86-64	instructions	can	range	in	length	from	1	to	15	bytes.	The
instruction	encoding	is	designed	so	that	commonly	used	instructions
and	those	with	fewer	operands	require	a	smaller	number	of	bytes	than
do	less	common	ones	or	ones	with	more	operands.
The	instruction	format	is	designed	in	such	a	way	that	from	a	given
starting	position,	there	is	a	unique	decoding	of	the	bytes	into	machine
instructions.	For	example,	only	the	instruction	
can	start
with	byte	value	
.
The	disassembler	determines	the	assembly	code	based	purely	on	the
byte	sequences	in	the	machine-code	file.	It	does	not	require	access	to
the	source	or	assembly-code	versions	of	the	program.
The	disassembler	uses	a	slightly	different	naming	convention	for	the
instructions	than	does	the	assembly	code	generated	by	
GCC
.	In	our
example,	it	has	omitted	the	suffix	<code> '	from	many	of	the	instructions. These	suffixes	are	size	designators	and	can	be	omitted	in	most cases.	Conversely,	the	disassembler	adds	the	suffix	</code>
'	to	the	
and	
instructions.	Again,	these	suffixes	can	safely	be	omitted.
Generating	the	actual	executable	code	requires	running	a	linker	on	the
set	of	object-code	files,	one	of	which	must	contain	a	function	
.
Suppose	in	file	
we	had	the	following	function:</p>
<p>Then	we	could	generate	an	executable	program	prog	as	follows:
The	file	
has	grown	to	8,655	bytes,	since	it	contains	not	just	the
machine	code	for	the	procedures	we	provided	but	also	code	used	to	start
and	terminate	the	program	as	well	as	to	interact	with	the	operating
system.</p>
<p>We	can	disassemble	the	file	
The	disassembler	will	extract	various	code	sequences,	including	the
following:
This	code	is	almost	identical	to	that	generated	by	the	disassembly	of
One	important	difference	is	that	the	addresses	listed	along	the
left	are	different—the	linker	has	shifted	the	location	of	this	code	to	a
different	range	of	addresses.	A	second	difference	is	that	the	linker	has
filled	in	the	address	that	the	
instruction	should	use	in	calling	the
function	
(line	4	of	the	disassembly).	One	task	for	the	linker	is	to
match	function	calls	with	the	locations	of	the	executable	code	for	those
functions.	A	final	difference	is	that	we	see	two	additional	lines	of	code</p>
<p>(lines	8-9).	These	instructions	will	have	no	effect	on	the	program,	since
they	occur	after	the	return	instruction	(line	7).	They	have	been	inserted	to
grow	the	code	for	the	function	to	16	bytes,	enabling	a	better	placement	of
the	next	block	of	code	in	terms	of	memory	system	performance.
3.2.3	
Notes	on	Formatting
The	assembly	code	generated	by	
GCC
is	difficult	for	a	human	to	read.	On
one	hand,	it	contains	information	with	which	we	need	not	be	concerned,
while	on	the	other	hand,	it	does	not	provide	any	description	of	the
program	or	how	it	works.	For	example,	suppose	we	give	the	command
to	generate	the	file	
The	full	content	of	the	file	is	as	follows:</p>
<p>All	of	the	lines	beginning	with	`.'	are	directives	to	guide	the	assembler
and	linker.	We	can	generally	ignore	these.	On	the	other	hand,	there	are
no	explanatory	remarks	about	what	the	instructions	do	or	how	they	relate
to	the	source	code.
To	provide	a	clearer	presentation	of	assembly	code,	we	will	show	it	in	a
form	that	omits	most	of	the	directives,	while	including	line	numbers	and
explanatory	annotations.	For	our	example,	an	annotated	version	would
appear	as	follows:</p>
<p>We	typically	show	only	the	lines	of	code	relevant	to	the	point	being
discussed.	Each	line	is	numbered	on	the	left	for	reference	and	annotated
on	the	right	by	a	brief	description	of	the	effect	of	the	instruction	and	how	it
relates	to	the	computations	of	the	original	C	code.	This	is	a	stylized
version	of	the	way	assembly-language	programmers	format	their	code.
We	also	provide	Web	asides	to	cover	material	intended	for	dedicated
machine-language	enthusiasts.	One	Web	aside	describes	IA32	machine
code.	Having	a	background	in	x86-64	makes	learning	IA32	fairly	simple.
Another	Web	aside	gives	a	brief	presentation	of	ways	to	incorporate
assembly	code	into	C	programs.	For	some	applications,	the	programmer
must	drop	down	to	assembly	code	to	access	low-level	features	of	the
machine.	One	approach	is	to	write	entire	functions	in	assembly	code	and
combine	them	with	C	functions	during	the	linking	stage.	A
Aside	
ATT	versus	Intel	assembly-code
formats
In	our	presentation,	we	show	assembly	code	in	ATT	format
(named	after	AT&amp;T,	the	company	that	operated	Bell	Laboratories
for	many	years),	the	default	format	for	
GCC
,	
OBJDUMP
,	and	the	other
tools	we	will	consider.	Other	programming	tools,	including	those
from	Microsoft	as	well	as	the	documentation	from	Intel,	show
assembly	code	in	
Intel
format.	The	two	formats	differ	in	a	number
of	ways.	As	an	example,	
GCC</p>
<p>can	generate	code	in	Intel	format	for
the	sum	function	using	the	following	command	line:</p>
<p>This	gives	the	following	assembly	code:
We	see	that	the	Intel	and	ATT	formats	differ	in	the	following	ways:
The	Intel	code	omits	the	size	designation	suffixes.	We	see
instruction	push	and	
instead	of	
and	
.
The	Intel	code	omits	the	`%'	character	in	front	of	register
names,	using	
instead	of	
.
The	Intel	code	has	a	different	way	of	describing	locations	in
memory—for	example,	
rather	than	
.
Instructions	with	multiple	operands	list	them	in	the	reverse
order.	This	can	be	very	confusing	when	switching	between	the
two	formats.
Although	we	will	not	be	using	Intel	format	in	our	presentation,	you
will	encounter	it	in	documentation	from	Intel	and	Microsoft.</p>
<p>second	is	to	use	
GCC
's	support	for	embedding	assembly	code	directly
within	C	programs.</p>
<p>3.3	
Data	Formats
Due	to	its	origins	as	a	16-bit	architecture	that	expanded	into	a	32-bit	one,
Intel	uses	the	term	&quot;word&quot;	to	refer	to	a	16-bit	data	type.	Based	on	this,
they	refer	to	32-bit	quantities	as	&quot;double	words,&quot;	and	64-bit	quantities	as
&quot;quad	words.&quot;	
Figure	
3.1
shows	the	x86-64	representations	used	for
the	primitive	data	types	of	C.	Standard	
values	are	stored	as	double
words	(32	bits).	Pointers	(shown	here	as	char	*)	are	stored	as	8-byte
quad	words,	as	would	be	expected	in	a	64-bit	machine.	With	x86-64,	data
type	long	is	implemented	with	64	bits,	allowing	a	very	wide	range	of
values.	Most	of	our	code	examples	in	this	chapter	use	pointers	and	long
data
Web	Aside	ASM:EASM	
Combining
assembly	code	with	C	programs
Although	a	C	compiler	does	a	good	job	of	converting	the
computations	expressed	in	a	program	into	machine	code,	there
are	some	features	of	a	machine	that	cannot	be	accessed	by	a	C
program.	For	example,	every	time	an	x86-64	processor	executes
an	arithmetic	or	logical	operation,	it	sets	a	1-bit	
condition	code
flag,	named	
PF</p>
<h2>(for	&quot;parity	flag&quot;),	to	1	when	the	lower	8	bits	in	the
resulting	computation	have	an	even	number	of	ones	and	to	0
otherwise.	Computing	this	information	in	C	requires	at	least	seven
shifting,	masking,	and	
EXCLUSIVE</h2>
<p>OR</p>
<p>operations	(see	
Problem
2.65
).	Even	though	the	hardware	performs	this	computation	as</p>
<p>part	of	every	arithmetic	or	logical	operation,	there	is	no	way	for	a
C	program	to	determine	the	value	of	the	
PF</p>
<p>condition	code	flag.
This	task	can	readily	be	performed	by	incorporating	a	small
number	of	assembly-code	instructions	into	the	program.
There	are	two	ways	to	incorporate	assembly	code	into	C
programs.	First,	we	can	write	an	entire	function	as	a	separate
assembly-code	file	and	let	the	assembler	and	linker	combine	this
with	code	we	have	written	in	C.	Second,	we	can	use	the	
inline
assembly
feature	of	
GCC
,	where	brief	sections	of	assembly	code
can	be	incorporated	into	a	C	program	using	the	
directive.	This
approach	has	the	advantage	that	it	minimizes	the	amount	of
machine-specific	code.
Of	course,	including	assembly	code	in	a	C	program	makes	the
code	specific	to	a	particular	class	of	machines	(such	as	x86-64),
and	so	it	should	only	be	used	when	the	desired	feature	can	only
be	accessed	in	this	way.
C	declaration
Intel	data	type
Assembly-code	suffix
Size	(bytes)
Byte
1
Word
2
Double	word
4
Quad	word
8
*
Quad	word
8
Single	precision
4
Double	precision
8</p>
<p>Figure	
3.1	
Sizes	of	C	data	types	in	x86-64.
With	a	64-bit	machine,	pointers	are	8	bytes	long.
types,	and	so	they	will	operate	on	quad	words.	The	x86-64	instruction	set
includes	a	full	complement	of	instructions	for	bytes,	words,	and	double
words	as	well.
Floating-point	numbers	come	in	two	principal	formats:	single-precision	(4-
byte)	values,	corresponding	to	C	data	type	
,	and	double-precision
(8-byte)	values,	corresponding	to	C	data	type	double.	Microprocessors	in
the	x86	family	historically	implemented	all	floating-point	operations	with	a
special	80-bit	(10-byte)	floating-point	format	(see	
Problem	
2.86
).	This
format	can	be	specified	in	C	programs	using	the	declaration	long	double.
We	recommend	against	using	this	format,	however.	It	is	not	portable	to
other	classes	of	machines,	and	it	is	typically	
not	implemented	with	the
same	high-performance	hardware	as	is	the	case	for	single-	and	double-
precision	arithmetic.
As	the	table	of	
Figure	
3.1
indicates,	most	assembly-code	instructions
generated	by	
GCC</p>
<p>have	a	single-character	suffix	denoting	the	size	of	the
operand.	For	example,	the	data	movement	instruction	has	four	variants:
(move	byte),	
(move	word),	
(move	double	word),	and
(move	quad	word).	The	suffix	<code> '	is	used	for	double	words,	since 32-bit	quantities	are	considered	to	be	&quot;long	words.&quot;	The	assembly	code uses	the	suffix	</code>
'	to	denote	a	4-byte	integer	as	well	as	an	8-byte	double-
precision	floating-point	number.	This	causes	no	ambiguity,	since	floating-
point	code	involves	an	entirely	different	set	of	instructions	and	registers.</p>
<p>3.4	
Accessing	Information
An	x86-64	central	processing	unit	(CPU)	contains	a	set	of	16	
general-
purpose	registers
storing	64-bit	values.	These	registers	are	used	to	store
integer	data	as	well	as	pointers.	
Figure	
3.2
diagrams	the	16	registers.
Their	names	all	begin	with	
,	but	otherwise	follow	multiple	different
naming	conventions,	owing	to	the	historical	evolution	of	the	instruction
set.	The	original	8086	had	eight	16-bit	registers,	shown	in	
Figure	
3.2
as	registers	
through	
.	Each	had	a	specific	purpose,	and	hence
they	were	given	names	that	reflected	how	they	were	to	be	used.	With	the
extension	to	IA32,	these	registers	were	expanded	to	32-bit	registers,
labeled	
through	
.	In	the	extension	to	x86-64,	the	original	eight
registers	were	expanded	to	64	bits,	labeled	
through	
.	In
addition,	eight	new	registers	were	added,	and	these	were	given	labels
according	to	a	new	naming	convention:	
through	
.
As	the	nested	boxes	in	
Figure	
3.2
indicate,	instructions	can	operate
on	data	of	different	sizes	stored	in	the	low-order	bytes	of	the	16	registers.
Byte-level	operations	can	access	the	least	significant	byte,	16-bit
operations	can	access	the	least	significant	2	bytes,	32-bit	operations	can
access	the	least	significant	4	bytes,	and	64-bit	operations	can	access
entire	registers.
In	later	sections,	we	will	present	a	number	of	instructions	for	copying	and
generating	1-,	2-,	4-,	and	8-byte	values.	When	these	instructions	have
registers	as	destinations,	two	conventions	arise	for	what	happens	to	the
remaining	bytes	in	the	register	for	instructions	that	generate	less	than	8</p>
<p>bytes:	Those	that	generate	1-or	2-byte	quantities	leave	the	remaining
bytes	unchanged.	Those	that	generate	4-byte	quantities	set	the	upper	4
bytes	of	the	register	to	zero.	The	latter	convention	was	adopted	as	part	of
the	expansion	from	IA32	to	x86-64.
As	the	annotations	along	the	right-hand	side	of	
Figure	
3.2
indicate,
different	registers	serve	different	roles	in	typical	programs.	Most	unique
among	them	is	the	stack	pointer,	
,	used	to	indicate	the	end	position
in	the	run-time	stack.	Some	instructions	specifically	read	and	write	this
register.	The	other	15	registers	have	more	flexibility	in	their	uses.	A	small
number	of	instructions	make	specific	use	of	certain	registers.	More
importantly,	a	set	of	standard	programming	conventions	governs	how	the
registers	are	to	be	used	for	managing	the	stack,	passing	function</p>
<p>Figure	
3.2	
Integer	registers.
The	low-order	portions	of	all	16	registers	can	be	accessed	as	byte,	word
(16-bit),	double	word	(32-bit),	and	quad	word	(64-bit)	quantities.
arguments,	returning	values	from	functions,	and	storing	local	and
temporary	data.	We	will	cover	these	conventions	in	our	presentation,</p>
<p>especially	in	
Section	
3.7
,	where	we	describe	the	implementation	of
procedures.
3.4.1	
Operand	Specifiers
Most	instructions	have	one	or	more	
operands
specifying	the	source
values	to	use	in	performing	an	operation	and	the	destination	location	into
which	to	place	the
Type
Form
Operand	value
Name
Immediate
Imm
Imm
Immediate
Register
R[
]
Register
Memory
Imm
M[
Imm
]
Absolute
Memory
M[R[
]]
Indirect
Memory
Imm</p>
<p>M[
Imm
+	R[
]]
Base	+	displacement
Memory
M[R[
]	+	R[
]
Indexed
Memory
Imm
M[
Imm
+	R[
]	+	R[
]]
Indexed
Memory
M[R[
]	
]
Scaled	indexed
Memory
Imm</p>
<p>M[
Imm
+	R[
]	
]
Scaled	indexed
Memory
M[R[
]	+	R[
]	
]
Scaled	indexed
Memory
Imm</p>
<p>M[
Imm
+	R[
]	+	R[
]	
]
Scaled	indexed</p>
<p>Figure	
3.3	
Operand	forms.
Operands	can	denote	immediate	(constant)	values,	register	values,	or
values	from	memory.	The	scaling	factor	
s
must	be	either	1,	2,	4,	or	8.
result.	x86-64	supports	a	number	of	operand	forms	(see	
Figure	
3.3
).
Source	values	can	be	given	as	constants	or	read	from	registers	or
memory.	Results	can	be	stored	in	either	registers	or	memory.	Thus,	the
different	operand	possibilities	can	be	classified	into	three	types.	The	first
type,	
immediate
,	is	for	constant	values.	In	ATT-format	assembly	code,
these	are	written	with	a	`$'	followed	by	an	integer	using	standard	C
notation—for	example,	$-577	or	
.	Different	instructions	allow
different	ranges	of	immediate	values;	the	assembler	will	automatically
select	the	most	compact	way	of	encoding	a	value.	The	second	type,
register
,	denotes	the	contents	of	a	register,	one	of	the	sixteen	8-,	4-,	2-,
or	1-byte	low-order	portions	of	the	registers	for	operands	having	64,	32,
16,	or	8	bits,	respectively.	In	
Figure	
3.3
,	we	use	the	notation	
to
denote	an	arbitrary	register	
a
and	indicate	its	value	with	the	reference
,	viewing	the	set	of	registers	as	an	array	R	indexed	by	register
identifiers.
The	third	type	of	operand	is	a	
memory
reference,	in	which	we	access
some	memory	location	according	to	a	computed	address,	often	called	the
effective	address
.	Since	we	view	the	memory	as	a	large	array	of	bytes,
we	use	the	notation	M
[
Addr
]	to	denote	a	reference	to	the	
b
-byte	value
stored	in	memory	starting	at	address	
Addr
.	To	simplify	things,	we	will
generally	drop	the	subscript	
b
.
b</p>
<p>As	
Figure	
3.3
shows,	there	are	many	different	
addressing	modes
allowing	different	forms	of	memory	references.	The	most	general	form	is
shown	at	the	bottom	of	the	table	with	syntax	
Imm
(
).	Such	a
reference	has	four	components:	an	immediate	offset	
Imm
,	a	base	register
,	an	index	register	
,	and	a	scale	factor	
s
,	where	
s
must	be	1,	2,	4,	or
8.	Both	the	base	and	index	must	be	64-bit	registers.	The	effective
address	is	computed	as	
Imm
+	
This	general	form	is
often	seen	when	referencing	elements	of	arrays.	The	other	forms	are
simply	special	cases	of	this	general	form	where	some	of	the	components
are	omitted.	As	we	
will	see,	the	more	complex	addressing	modes	are
useful	when	referencing	array	and	structure	elements.
Practice	Problem	
3.1	
(solution	page	
325
)
Assume	the	following	values	are	stored	at	the	indicated	memory
addresses	and	registers:
Address
Value
Register
Value
Fill	in	the	following	table	showing	the	values	for	the	indicated
operands:
Operand
Value</p>
<hr />
<hr />
<hr />
<hr />
<hr />
<hr />
<hr />
<hr />
<hr />
<p>3.4.2	
Data	Movement	Instructions
Among	the	most	heavily	used	instructions	are	those	that	copy	data	from
one	location	to	another.	The	generality	of	the	operand	notation	allows	a
simple	data	movement	instruction	to	express	a	range	of	possibilities	that
in	many	machines	would	require	a	number	of	different	instructions.	We
present	a	number	of	different	data	movement	instructions,	differing	in
their	source	and	destination	types,	what	conversions	they	perform,	and
other	side	effects	they	may	have.	In	our	presentation,	we	group	the	many
different	instructions	into	
instruction	classes
,	where	the	instructions	in	a
class	perform	the	same	operation	but	with	different	operand	sizes.</p>
<p>Figure	
3.4
lists	the	simplest	form	of	data	movement	instructions—
MOV
class.	These	instructions	copy	data	from	a	source	location	to	a
destination	location,	without	any	transformation.	The	class	consists	of
four	instructions:	
,	and	
.	All	four	of	these
instructions	have	similar	effects;	they	differ	primarily	in	that	they	operate
on	data	of	different	sizes:	1,	2,	4,	and	8	bytes,	respectively.
Instruction
Effect
Description
MOV
S
,	
D
D
←	
S
Move
 
Move	byte
 
Move	word
 
Move	double	word
 
Move	quad	word
I
,	
R
R
←	
I
Move	absolute	quad	word
Figure	
3.4	
Simple	data	movement	instructions.
The	source	operand	designates	a	value	that	is	immediate,	stored	in	a
register,	or	stored	in	memory.	The	destination	operand	designates	a
location	that	is	either	a	register	or	a	memory	address.	x86-64	imposes
the	restriction	that	a	move	instruction	cannot	have	both	operands	refer	to
memory	locations.	Copying	a	value	from	one	memory	location	to	another
requires	two	instructions—the	first	to	load	the	source	value	into	a	register,
and	the	second	to	write	this	register	value	to	the	destination.	Referring	to
Figure	
3.2
,	register	operands	for	these	instructions	can	be	the	labeled</p>
<p>portions	of	any	of	the	16	registers,	where	the	size	of	the	register	must
match	the	size	designated	by	the	last	character	of	the	instruction	(
,	or	
).	For	most	cases,	the	
MOV</p>
<p>instructions	will	only	update
the	specific	register	bytes	or	memory	locations	indicated	by	the
destination	operand.	The	only	exception	is	that	when	
has	a	register
as	the	destination,	it	will	also	set	the	high-order	4	bytes	of	the	register	to
0.	This	exception	arises	from	the	convention,	adopted	in	x86-64,	that	any
instruction	that	generates	a	32-bit	value	for	a	register	also	sets	the	high-
order	portion	of	the	register	to	0.
The	following	
MOV</p>
<p>instruction	examples	show	the	five	possible
combinations	of	source	and	destination	types.	Recall	that	the	source
operand	comes	first	and	the	destination	second.
A	final	instruction	documented	in	
Figure	
3.4
is	for	dealing	with	64-bit
immediate	data.	The	regular	
instruction	can	only	have	immediate
source	operands	that	can	be	represented	as	32-bit	two's-complement
numbers.	This	value	is	then	sign	extended	to	produce	the	64-bit	value	for
the	destination.	The	
instruction	can	have	an	arbitrary	64-bit
immediate	value	as	its	source	operand	and	can	only	have	a	register	as	a
destination.</p>
<p>Figures	
3.5
and	
3.6
document	two	classes	of	data	movement
instructions	for	use	when	copying	a	smaller	source	value	to	a	larger
destination.	All	of	these	instructions	copy	data	from	a	source,	which	can
be	either	a	register	or	stored
Aside	
Understanding	how	data
movement	changes	a	destination
register
As	described,	there	are	two	different	conventions	regarding
whether	and	how	data	movement	instructions	modify	the	upper
bytes	of	a	destination	register.	This	distinction	is	illustrated	by	the
following	code	sequence:</p>
<p>In	the	following	discussion,	we	use	hexadecimal	notation.	In	the
example,	the	instruction	on	line	1	initializes	register	
to	the
pattern	
.	The	remaining	instructions	have
immediate	value	–1	as	their	source	values.	Recall	that	the
hexadecimal	representation	of	–1	is	of	the	form	
,	where	the
number	of	
F
'
S</p>
<p>is	twice	the	number	of	bytes	in	the	representation.
The	
instruction	(line	2)	therefore	sets	the	low-order	byte	of
to	
FF
,	while	the	
instruction	(line	3)	sets	the	low-order	2
bytes	to	
FFFF
,	with	the	remaining	bytes	unchanged.	The	
instruction	(line	4)	sets	the	low-order	4	bytes	to	
,	but	it
also	sets	the	high-order	4	bytes	to	
.	Finally,	the	
instruction	(line	5)	sets	the	complete	register	to	
.
Instruction
Effect
Description
R
←	ZeroExtend
(S)
Move	with	zero	extension
Move	zero-extended	byte	to	word
Move	zero-extended	byte	to	double	word
Move	zero-extended	word	to	double	word
Move	zero-extended	byte	to	quad	word
Move	zero-extended	word	to	quad	word
Figure	
3.5	
Zero-extending	data	movement	instructions.
These	instructions	have	a	register	or	memory	location	as	the	source	and
a	register	as	the	destination.</p>
<p>in	memory,	to	a	register	destination.	Instructions	in	the	
MOVZ</p>
<p>class	fill	out
the	remaining	bytes	of	the	destination	with	zeros,	while	those	in	the	
MOVS
class	fill	them	out	by	sign	extension,	replicating	copies	of	the	most
significant	bit	of	the	source	operand.	Observe	that	each	instruction	name
has	size	designators	as	its	final	two	characters—the	first	specifying	the
source	size,	and	the	second	specifying	the	destination	size.	As	can	be
seen,	there	are	three	instructions	in	each	of	these	classes,	covering	all
cases	of	1-and	2-byte	source	sizes	and	2-	and	4-byte	destination	sizes,
considering	only	cases	where	the	destination	is	larger	than	the	source,	of
course.
Instruction
Effect
Description
MOVS</p>
<p>S
,
R
R
←	SignExtend
(S)
Move	with	sign	extension
Move	sign-extended	byte	to	word
Move	sign-extended	byte	to	double	word
Move	sign-extended	word	to	double	word
Move	sign-extended	byte	to	quad	word
Move	sign-extended	word	to	quad	word
Move	sign-extended	double	word	to	quad	word
%rax	←	SignExtend(%eax)
Sign-extend	%eax	to	%rax
Figure	
3.6	
Sign-extending	data	movement	instructions.
The	
MOVS</p>
<p>instructions	have	a	register	or	memory	location	as	the	source
and	a	register	as	the	destination.	The	
instruction	is	specific	to</p>
<p>registers	
and	
.
Note	the	absence	of	an	explicit	instruction	to	zero-extend	a	4-byte	source
value	to	an	8-byte	destination	in	
Figure	
3.5
.	Such	an	instruction	would
logically	be	named	
,	but	this	instruction	does	not	exist.	Instead,
this	type	of	data	movement	can	be	implemented	using	a	
instruction
having	a	register	as	the	destination.	This	technique	takes	advantage	of
the	property	that	an	instruction	generating	a	4-byte	value	with	a	register
as	the	destination	will	fill	the	upper	4	bytes	with	zeros.	Otherwise,	for	64-
bit	destinations,	moving	with	sign	extension	is	supported	for	all	three
source	types,	and	moving	with	zero	extension	is	supported	for	the	two
smaller	source	types.
Figure	
3.6
also	documents	the	
instruction.	This	instruction	has
no	operands—it	always	uses	register	
as	its	source	and	
as	the
destination	for	the	sign-extended	result.	It	therefore	has	the	exact	same
effect	as	the	instruction	
,	but	it	has	a	more	compact
encoding.
Practice	Problem	
3.2	
(solution	page	
325
)
For	each	of	the	following	lines	of	assembly	language,	determine
the	appropriate	instruction	suffix	based	on	the	operands.	(For
example,	
can	be	rewritten	as	
)</p>
<p>Aside	
Comparing	byte	movement
instructions
The	following	example	illustrates	how	different	data	movement
instructions	either	do	or	do	not	change	the	high-order	bytes	of	the
destination.	Observe	that	the	three	byte-movement	instructions
,	and	
differ	from	each	other	in	subtle	ways.
Here	is	an	example:
In	the	following	discussion,	we	use	hexadecimal	notation	for	all	of
the	values.	The	first	two	lines	of	the	code	initialize	registers	
and	
to	
and	AA,	respectively.	The	remaining</p>
<p>instructions	all	copy	the	low-order	byte	of	
to	the	low-order
byte	of	
.	The	
instruction	(line	3)	does	not	change	the
other	bytes.	The	
instruction	(line	4)	sets	the	other	7	bytes
to	either	all	ones	or	all	zeros	depending	on	the	high-order	bit	of
the	source	byte.	Since	hexadecimal	A	represents	binary	value
,	sign	extension	causes	the	higher-order	bytes	to	each	be	set
to	FF.	The	
instruction	(line	5)	always	sets	the	other	7	bytes
to	zero.
Practice	Problem	
3.3	
(solution	page	
326
)
Each	of	the	following	lines	of	code	generates	an	error	message
when	we	invoke	the	assembler.	Explain	what	is	wrong	with	each
line.
3.4.3	
Data	Movement	Example</p>
<p>As	an	example	of	code	that	uses	data	movement	instructions,	consider
the	data	exchange	routine	shown	in	
Figure	
3.7
,	both	as	C	code	and	as
assembly	code	generated	by	
GCC
.
As	
Figure	
3.7(b)
shows,	function	exchange	is	implemented	with	just
three	instructions:	two	data	movements	(
)	plus	an	instruction	to
return	back	to	the	point	from	which	the	function	was	called	(
).	We	will
cover	the	details	of	function	call	and	return	in	
Section	
3.7
.	Until	then,	it
suffices	to	say	that	arguments	are	passed	to	functions	in	registers.	Our
annotated	assembly	code	documents	these.	A	function	returns	a	value	by
storing	it	in	register	
,	or	in	one	of	the	low-order	portions	of	this
register.
a
.	
C	code
b
.	
Assembly	code</p>
<p>Figure	
3.7	
C	and	assembly	code	for	exchange	routine.
Registers	
and	
hold	parameters	
and	
,	respectively.
When	the	procedure	begins	execution,	procedure	parameters	
and	
are	stored	in	registers	
and	
,	respectively.	Instruction	2	then
reads	
from	memory	and	stores	the	value	in	register	
,	a	direct
implementation	of	the	operation	
in	the	C	program.	Later,	register
will	be	used	to	return	a	value	from	the	function,	and	so	the	return
value	will	be	
.	Instruction	3	writes	
to	the	memory	location	designated
by	
in	register	
,	a	direct	implementation	of	the	operation	
.
This	example	illustrates	how	the	
MOV</p>
<p>instructions	can	be	used	to	read
from	memory	to	a	register	(line	2),	and	to	write	from	a	register	to	memory
(line	3).
Two	features	about	this	assembly	code	are	worth	noting.	First,	we	see
that	what	we	call	&quot;pointers&quot;	in	C	are	simply	addresses.	Dereferencing	a
pointer	involves	copying	that	pointer	into	a	register,	and	then	using	this
register	in	a	memory	reference.	Second,	local	variables	such	as	
are
often	kept	in	registers	rather	than	stored	in	memory	locations.	Register
access	is	much	faster	than	memory	access.
Practice	Problem	
3.4	
(solution	page	
326
)
Assume	variables	sp	and	dp	are	declared	with	types</p>
<p>where	
and	
are	data	types	declared	with	
.	We
wish	to	use	the	appropriate	pair	of	data	movement	instructions	to
implement	the	operation
New	to	C?	
Some	examples	of	pointers
Function	exchange	(
Figure	
3.7(a)
)	provides	a	good	illustration
of	the	use	of	pointers	in	C.	Argument	
is	a	pointer	to	a	long
integer,	while	
is	a	long	integer	itself.	The	statement
indicates	that	we	should	read	the	value	stored	in	the	location
designated	by	
and	store	it	as	a	local	variable	named	
.	This
read	operation	is	known	as	pointer	
dereferencing
.	The	C	operator
`*'	performs	pointer	dereferencing.	The	statement</p>
<p>does	the	reverse—it	writes	the	value	of	parameter	
at	the
location	designated	by	
.	This	is	also	a	form	of	pointer
dereferencing	(and	hence	the	operator	*),	but	it	indicates	a	write
operation	since	it	is	on	the	left-hand	side	of	the	assignment.
The	following	is	an	example	of	exchange	in	action:
This	code	will	print
The	C	operator	`&amp;'	(called	the	&quot;address	of&quot;	operator)	
creates
a
pointer,	in	this	case	to	the	location	holding	local	variable	a.
Function	exchange	overwrites	the	value	stored	in	a	with	3	but
returns	the	previous	value,	4,	as	the	function	value.	Observe	how
by	passing	a	pointer	to	exchange,	it	could	modify	data	held	at
some	remote	location.
Assume	that	the	values	of	
and	
are	stored	in	registers	
and
,	respectively.	For	each	entry	in	the	table,	show	the	two	instructions
that	implement	the	specified	data	movement.	The	first	instruction	in	the</p>
<p>sequence	should	read	from	memory,	do	the	appropriate	conversion,	and
set	the	appropriate	portion	of	register	
.	The	second	instruction
should	then	write	the	appropriate	portion	of	
to	memory.	In	both
cases,	the	portions	may	be	
,	
,	
,	or	
,	and	they	may	differ
from	one	another.
Recall	that	when	performing	a	cast	that	involves	both	a	size	change	and
a	change	of	&quot;signedness&quot;	in	C,	the	operation	should	change	the	size	first
(
Section	
2.2.6
).
Instruction</p>
<hr />
<hr />
<hr />
<hr />
<hr />
<hr />
<hr />
<hr />
<hr />
<hr />
<hr />
<hr />
<p>Practice	Problem	
3.5	
(solution	page	
327
)
You	are	given	the	following	information.	A	function	with	prototype
is	compiled	into	assembly	code,	yielding	the	following:
Parameters	
,	and	
are	stored	in	registers	
,	and
,	respectively.
Write	C	code	for	
that	will	have	an	effect	equivalent	to	the
assembly	code	shown.
3.4.4	
Pushing	and	Popping	Stack</p>
<p>Data
The	final	two	data	movement	operations	are	used	to	push	data	onto	and
pop	data	from	the	program	stack,	as	documented	in	
Figure	
3.8
.	As	we
will	see,	the	stack	plays	a	vital	role	in	the	handling	of	procedure	calls.	By
way	of	background,	a	stack	is	a	data	structure	where	values	can	be
added	or	deleted,	but	only	according	to	a	&quot;last-in,	first-out&quot;	discipline.	We
add	data	to	a	stack	via	a	
push
operation	and	remove	it	via	a	
pop
operation,	with	the	property	that	the	value	popped	will	always	be	the
value	that	was	most	recently	pushed	and	is	still	on	the	stack.	A	stack	can
be	implemented	as	an	array,	where	we	always	insert	and	remove
elements	from	one
Instruction
Effect
Description</p>
<p>S
Push	quad	word</p>
<p>D
D
←	
Pop	quad	word
Figure	
3.8	
Push	and	pop	instructions.</p>
<p>Figure	
3.9	
Illustration	of	stack	operation.
By	convention,	we	draw	stacks	upside	down,	so	that	the	&quot;top&quot;	of	the
stack	is	shown	at	the	bottom.	With	x86-64,	stacks	grow	toward	lower
addresses,	so	pushing	involves	decrementing	the	stack	pointer	(register
)	and	storing	to	memory,	while	popping	involves	reading	from
memory	and	incrementing	the	stack	pointer.
end	of	the	array.	This	end	is	called	the	
top
of	the	stack.	With	x86-64,	the
program	stack	is	stored	in	some	region	of	memory.	As	illustrated	in
Figure	
3.9
,	the	stack	grows	downward	such	that	the	top	element	of	the
stack	has	the	lowest	address	of	all	stack	elements.	(By	convention,	we
draw	stacks	upside	down,	with	the	stack	&quot;top&quot;	shown	at	the	bottom	of	the
figure.)	The	stack	pointer	
holds	the	address	of	the	top	stack
element.
The	
instruction	provides	the	ability	to	push	data	onto	the	stack,
while	the	
instruction	pops	it.	Each	of	these	instructions	takes	a</p>
<p>single	operand—the	data	source	for	pushing	and	the	data	destination	for
popping.
Pushing	a	quad	word	value	onto	the	stack	involves	first	decrementing	the
stack	pointer	by	8	and	then	writing	the	value	at	the	new	top-of-stack
address.	
Therefore,	the	behavior	of	the	instruction	
is
equivalent	to	that	of	the	pair	of	instructions
except	that	the	
instruction	is	encoded	in	the	machine	code	as	a
single	byte,	whereas	the	pair	of	instructions	shown	above	requires	a	total
of	8	bytes.	The	first	two	columns	in	
Figure	
3.9
illustrate	the	effect	of
executing	the	instruction	
when	
is	
and	
is
.	First	
is	decremented	by	8,	giving	
,	and	then	
is
stored	at	memory	address	
.
Popping	a	quad	word	involves	reading	from	the	top-of-stack	location	and
then	incrementing	the	stack	pointer	by	8.	Therefore,	the	instruction	
is	equivalent	to	the	following	pair	of	instructions:</p>
<p>The	third	column	of	
Figure	
3.9
illustrates	the	effect	of	executing	the
instruction	
immediately	after	executing	the	
.	Value	
is	read	from	memory	and	written	to	register	
.	Register	
incremented	back	to	
.	As	shown	in	the	figure,	the	value	
remains	at	memory	location	
until	it	is	overwritten	(e.g.,	by	another
push	operation).	However,	the	stack	top	is	always	considered	to	be	the
address	indicated	by	
.
Since	the	stack	is	contained	in	the	same	memory	as	the	program	code
and	other	forms	of	program	data,	programs	can	access	arbitrary
positions	within	the	stack	using	the	standard	memory	addressing
methods.	For	example,	assuming	the	topmost	element	of	the	stack	is	a
quad	word,	the	instruction	
will	copy	the	second	quad
word	from	the	stack	to	register	
.</p>
<p>3.5	
Arithmetic	and	Logical
Operations
Figure	
3.10
lists	some	of	the	x86-64	integer	and	logic	operations.
Most	of	the	operations	are	given	as	instruction	classes,	as	they	can	have
different	variants	with	different	operand	sizes.	(Only	
has	no	other
size	variants.)	For	example,	the	instruction	class	
ADD</p>
<p>consists	of	four
addition	instructions:	
,	and	
,	adding	bytes,	words,
double	words,	and	quad	words,	respectively.	Indeed,	each	of	the
instruction	classes	shown	has	instructions	for	operating	on	these	four
different	sizes	of	data.	The	operations	are	divided	into	four	groups:	load
effective	address,	unary,	binary,	and	shifts.	
Binary
operations	have	two
operands,	while	
unary
operations	have	one	operand.	These	operands
are	specified	using	the	same	notation	as	described	in	
Section	
3.4
.
3.5.1	
Load	Effective	Address
The	
load	effective	address
instruction	
is	actually	a	variant	of	the
instruction.	It	has	the	form	of	an	instruction	that	reads	from	memory
to	a	register,
Instruction
Effect
Description
S
,	
D
D
←	&amp;
S
Load	effective	address</p>
<p>INC
D
D
←	
D
+1
Increment
DEC
D
D
←	
D
-1
Decrement
NEG
D
D
←	-
D
Negate
NOT
D
D
←	~
D
Complement
ADD
S
,	
D
D
←	
D</p>
<ul>
<li></li>
</ul>
<h2>S
Add
SUB
S
,	
D
D
←	
D</h2>
<p>S
Subtract
IMUL
S
,	
D
D
←	
D</p>
<ul>
<li></li>
</ul>
<p>S
Multiply
XOR
S
,	
D
D
←
D
^	
S
Exclusive-or
OR
S
,	
D
D
←	
D
|	
S
Or
AND
S
,	
D
D
←	
D
&amp;
S
And
SAL
k
,	
D
D
←	
D	&lt;&lt;k
Left	shift
SHL
k
,	
D
D
←	
D
&lt;&lt;	
k
Left	shift	(same	as	
SAL
)
SAR
k
,	
D
D
←	
D
&gt;&gt;</p>
<p>k
Arithmetic	right	shift
k
,	
D
D
←	
D
&gt;&gt;</p>
<p>k
Logical	right	shift
Figure	
3.10	
Integer	arithmetic	operations.
The	load	effective	address	(leaq)	instruction	is	commonly	used	to	perform
simple	arithmetic.	The	remaining	ones	are	more	standard	unary	or	binary
operations.	We	use	the	notation	
and	
to	denote	arithmetic	and
logical	right	shift,	respectively.	Note	the	nonintuitive	ordering	of	the
operands	with	ATT-format	assembly	code.
A
L</p>
<p>but	it	does	not	reference	memory	at	all.	Its	first	operand	appears	to	be	a
memory	reference,	but	instead	of	reading	from	the	designated	location,
the	instruction	copies	the	effective	address	to	the	destination.	We
indicate	this	computation	in	
Figure	
3.10
using	the	C	address	operator
&amp;
S
.	This	instruction	can	be	used	to	generate	pointers	for	later	memory
references.	In	addition,	it	can	be	used	to	compactly	describe	common
arithmetic	operations.	For	example,	if	register	
contains	value	
x
,	then
the	instruction	
will	set	register	
to	5
x
+	7.
Compilers	often	find	clever	uses	of	
that	have	nothing	to	do	with
effective	address	computations.	The	destination	operand	must	be	a
register.
Practice	Problem	
3.6	
(solution	page	
327
)
Suppose	register	
holds	value	
x
and	
holds	value	
y
.	Fill	in
the	table	below	with	formulas	indicating	the	value	that	will	be
stored	in	register	
for	each	of	the	given	assembly-code
instructions:
Instruction
Result</p>
<hr />
<hr />
<hr />
<hr />
<hr />
<hr />
<p>As	an	illustration	of	the	use	of	
in	compiled	code,	consider	the
following	C	program:
When	compiled,	the	arithmetic	operations	of	the	function	are
implemented	by	a	sequence	of	three	
functions,	as	is	documented
by	the	comments	on	the	right-hand	side:
The	ability	of	the	
instruction	to	perform	addition	and	limited	forms	of
multiplication	proves	useful	when	compiling	simple	arithmetic
expressions	such	as	this	example.</p>
<p>Practice	Problem	
3.7	
(solution	page	
328
)
Consider	the	following	code,	in	which	we	have	omitted	the
expression	being	computed:
Compiling	the	actual	function	with	
GCC</p>
<p>yields	the	following
assembly	code:
Fill	in	the	missing	expression	in	the	C	code.
3.5.2	
Unary	and	Binary	Operations</p>
<p>Operations	in	the	second	group	are	unary	operations,	with	the	single
operand	serving	as	both	source	and	destination.	This	operand	can	be
either	a	register	or	a	memory	location.	For	example,	the	instruction	
causes	the	8-byte	element	on	the	top	of	the	stack	to	be
incremented.	This	syntax	is	reminiscent	of	the	C	increment	(++)	and
decrement	(−−)	operators.
The	third	group	consists	of	binary	operations,	where	the	second	operand
is	used	as	both	a	source	and	a	destination.	This	syntax	is	reminiscent	of
the	C	assignment	operators,	such	as	
.	Observe,	however,	that	the
source	operand	is	given	first	and	the	destination	second.	This	looks
peculiar	for	noncommutative	operations.	For	example,	the	instruction
decrements	register	
by	the	value	in	
.	(It	helps	to
read	the	instruction	as	&quot;Subtract	
from	
.&quot;)	The	first	operand	can
be	either	an	immediate	value,	a	register,	or	a	memory	location.	The
second	can	be	either	a	register	or	a	memory	location.	As	with	the	
MOV
instructions,	the	two	operands	cannot	both	be	memory	locations.	Note
that	when	the	second	operand	is	a	memory	location,	the	processor	must
read	the	value	from	memory,	perform	the	operation,	and	then	write	the
result	back	to	memory.
Practice	Problem	
3.8	
(solution	page	
328
)
Assume	the	following	values	are	stored	at	the	indicated	memory
addresses	and	registers:
Address
Value
Register
Value</p>
<p>Fill	in	the	following	table	showing	the	effects	of	the	following
instructions,	in	terms	of	both	the	register	or	memory	location	that
will	be	updated	and	the	resulting	value:
Instruction
Destination
Value</p>
<hr />
<hr />
<hr />
<hr />
<hr />
<hr />
<hr />
<hr />
<hr />
<hr />
<hr />
<hr />
<p>3.5.3	
Shift	Operations
The	final	group	consists	of	shift	operations,	where	the	shift	amount	is
given	first	and	the	value	to	shift	is	given	second.	Both	arithmetic	and
logical	right	shifts	are	
possible.	The	different	shift	instructions	can	specify
the	shift	amount	either	as	an	immediate	value	or	with	the	single-byte
register	
.	(These	instructions	are	unusual	in	only	allowing	this	specific
register	as	the	operand.)	In	principle,	having	a	1-byte	shift	amount	would
8</p>
<p>make	it	possible	to	encode	shift	amounts	ranging	up	to	2
−	1	=	255.	With
x86-64,	a	shift	instruction	operating	on	data	values	that	are	
w
bits	long
determines	the	shift	amount	from	the	low-order	
m
bits	of	register	
,
where	2
=	
w
.	The	higher-order	bits	are	ignored.	So,	for	example,	when
register	
has	hexadecimal	value	
,	then	instruction	
would
shift	by	7,	while	
would	shift	by	15,	
would	shift	by	31,	and	
would	shift	by	63.
As	
Figure	
3.10
indicates,	there	are	two	names	for	the	left	shift
instruction:	
SAL</p>
<p>and	
SHL
.	Both	have	the	same	effect,	filling	from	the	right
with	zeros.	The	right	shift	instructions	differ	in	that	
SAR</p>
<p>performs	an
arithmetic	shift	(fill	with	copies	of	the	sign	bit),	whereas	
SHR</p>
<p>performs	a
logical	shift	(fill	with	zeros).	The	destination	operand	of	a	shift	operation
can	be	either	a	register	or	a	memory	location.	We	denote	the	two
different	right	shift	operations	in	
Figure	
3.10
as	
(arithmetic)	and
(logical).
Practice	Problem	
3.9	
(solution	page	
328
)
Suppose	we	want	to	generate	assembly	code	for	the	following	C
function:
≪
≫
8
m</p>
<p>The	code	that	follows	is	a	portion	of	the	assembly	code	that
performs	the	actual	shifts	and	leaves	the	final	value	in	register
Two	key	instructions	have	been	omitted.	Parameters	x	and
n	are	stored	in	registers	
and	
,	respectively.
≪
≫
Fill	in	the	missing	instructions,	following	the	annotations	on	the
right.	The	right	shift	should	be	performed	arithmetically.
a
.	
C	code</p>
<p>b
.	
Assembly	code
Figure	
3.11	
C	and	assembly	code	for	arithmetic	function.
3.5.4	
Discussion
We	see	that	most	of	the	instructions	shown	in	
Figure	
3.10
can	be
used	for	either	unsigned	or	two's-complement	arithmetic.	Only	right
shifting	requires	instructions	that	differentiate	between	signed	versus
unsigned	data.	This	is	one	of	the	features	that	makes	two's-complement
arithmetic	the	preferred	way	to	implement	signed	integer	arithmetic.
Figure	
3.11
shows	an	example	of	a	function	that	performs	arithmetic
operations	and	its	translation	into	assembly	code.	Arguments	
,	and
are	initially	stored	in	registers	
,	and	
,	respectively.	The
assembly-code	instructions	correspond	closely	with	the	lines	of	C	source</p>
<p>code.	Line	2	computes	the	value	of	
.	Lines	3	and	4	compute	the
expression	
by	a	combination	of	
and	shift	instructions.	Line	5
computes	the	
AND</p>
<p>of	
and	
.	The	final	subtraction	is
computed	by	line	6.	Since	the	destination	of	the	subtraction	is	register
,	this	will	be	the	value	returned	by	the	function.
In	the	assembly	code	of	
Figure	
3.11
,	the	sequence	of	values	in
register	
corresponds	to	program	values	
,	and	
(as	the
return	value).	In	general,	compilers	generate	code	that	uses	individual
registers	for	multiple	program	values	and	moves	program	values	among
the	registers.
Practice	Problem	
3.10	
(solution	page	
329
)
In	the	following	variant	of	the	function	of	
Figure	
3.11(a)
,	the
expressions	have	been	replaced	by	blanks:</p>
<h2>The	portion	of	the	generated	assembly	code	implementing	these
expressions	is	as	follows:
Based	on	this	assembly	code,	fill	in	the	missing	portions	of	the	C
code.
Practice	Problem	
3.11	
(solution	page	
329
)
It	is	common	to	find	assembly-code	lines	of	the	form
in	code	that	was	generated	from	C	where	no	
EXCLUSIVE</h2>
<h2>OR
operations	were	present.
A
.	
Explain	the	effect	of	this	particular	
EXCLUSIVE</h2>
<p>OR</p>
<p>instruction
and	what	useful	operation	it	implements.</p>
<p>B
.	
What	would	be	the	more	straightforward	way	to	express	this
operation	in	assembly	code?
C
.	
Compare	the	number	of	bytes	to	encode	these	two	different
implementations	of	the	same	operation.
3.5.5	
Special	Arithmetic	Operations
As	we	saw	in	
Section	
2.3
,	multiplying	two	64-bit	signed	or	unsigned
integers	can	yield	a	product	that	requires	128	bits	to	represent.	The	x86-
64	instruction	set	provides	limited	support	for	operations	involving	128-bit
(16-byte)	numbers.	Continuing	with	the	naming	convention	of	word	(2
bytes),	double	word	(4	bytes),	and	quad	word	(8	bytes),	Intel	refers	to	a
16-byte	quantity	as	an	
oct	word
.	
Figure	
3.12
Instruction
Effect
Description</p>
<p>S
R[
]:R[
]	←	
S
×	R[
]
Signed	full	multiply</p>
<p>S
R[
]:R[
]	←	
S
×	R[
]
Unsigned	full	multiply
R[
]:R[
]	←	SignExtend(R[
])
Convert	to	oct	word</p>
<p>S
R[
]	←	R[
]:R[
]	mod	
S
;
R[
]	←	R[
]:R[
]	÷	
S
Signed	divide</p>
<p>S
R[
]	←	R[
]:R[
]	mod	
S
;
R[
]	←	R[
]:R[
]	÷	
S
Unsigned	divide
Figure	
3.12	
Special	arithmetic	operations.</p>
<p>These	operations	provide	full	128-bit	multiplication	and	division,	for	both
signed	and	unsigned	numbers.	The	pair	of	registers	
and	
are
viewed	as	forming	a	single	128-bit	oct	word.
describes	instructions	that	support	generating	the	full	128-bit	product	of
two	64-bit	numbers,	as	well	as	integer	division.
The	
instruction	has	two	different	forms	One	form,	shown	in	
Figure
3.10
,	is	as	a	member	of	the	
IMUL</p>
<p>instruction	class.	In	this	form,	it	serves
as	a	&quot;two-operand&quot;	multiply	instruction,	generating	a	64-bit	product	from
two	64-bit	operands.	It	implements	the	operations	
and	
described	in	
Sections	
2.3.4
and	
2.3.5
.	(Recall	that	when	truncating
the	product	to	64	bits,	both	unsigned	multiply	and	two's-complement
multiply	have	the	same	bit-level	behavior.)
Additionally,	the	x86-64	instruction	set	includes	two	different	&quot;one-
operand&quot;	multiply	instructions	to	compute	the	full	128-bit	product	of	two
64-bit	values—one	for	unsigned	(
)	and	one	for	two's-complement
(
)	multiplication.	For	both	of	these	instructions,	one	argument	must
be	in	register	
,	and	the	other	is	given	as	the	instruction	source
operand.	The	product	is	then	stored	in	registers	
(high-order	64	bits)
and	
(low-order	64	bits).	Although	the	name	
is	used	for	two
distinct	multiplication	operations,	the	assembler	can	tell	which	one	is
intended	by	counting	the	number	of	operands.
As	an	example,	the	following	C	code	demonstrates	the	generation	of	a
128-bit	product	of	two	unsigned	64-bit	numbers	x	and	y:</p>
<ul>
<li></li>
</ul>
<p>64
u</p>
<ul>
<li></li>
</ul>
<p>64
t</p>
<p>In	this	program,	we	explicitly	declare	
and	
to	be	64-bit	numbers,
using	definitions	declared	in	the	file	
,	as	part	of	an	extension	of
the	C	standard.	Unfortunately,	this	standard	does	not	make	provisions	for
128-bit	values.	Instead,	
we	rely	on	support	provided	by	
GCC</p>
<p>for	128-bit
integers,	declared	using	the	name	
.	Our	code	uses	a	
declaration	to	define	data	type	
,	following	the	naming	pattern
for	other	data	types	found	in	
The	code	specifies	that	the
resulting	product	should	be	stored	at	the	16	bytes	designated	by	pointer
.
The	assembly	code	generated	by	
GCC</p>
<p>for	this	function	is	as	follows:</p>
<p>Observe	that	storing	the	product	requires	two	
instructions:	one	for
the	low-order	8	bytes	(line	4),	and	one	for	the	high-order	8	bytes	(line	5).
Since	the	code	is	generated	for	a	little-endian	machine,	the	high-order
bytes	are	stored	at	higher	addresses,	as	indicated	by	the	address
specification	
.
Our	earlier	table	of	arithmetic	operations	(
Figure	
3.10
)	does	not	list
any	division	or	modulus	operations.	These	operations	are	provided	by	the
single-operand	divide	instructions	similar	to	the	single-operand	multiply
instructions.	The	signed	division	instruction	
takes	as	its	dividend
the	128-bit	quantity	in	registers	
(high-order	64	bits)	and	
(low-
order	64	bits).	The	divisor	is	given	as	the	instruction	operand.	The
instruction	stores	the	quotient	in	register	
and	the	remainder	in
register	
.
For	most	applications	of	64-bit	addition,	the	dividend	is	given	as	a	64-bit
value.	This	value	should	be	stored	in	register	
.	The	bits	of	
should	then	be	set	to	either	all	zeros	(unsigned	arithmetic)	or	the	sign	bit
of	
(signed	arithmetic).	The	latter	operation	can	be	performed	using
the	instruction	
.
This	instruction	takes	no	operands—it	implicitly
reads	the	sign	bit	from	
and	copies	it	across	all	of	
.
2.	
This	instruction	is	called	
in	the	Intel	documentation,	one	of	the	few	cases	where	the	ATT-
format	name	for	an	instruction	does	not	match	the	Intel	name.
2</p>
<p>As	an	illustration	of	the	implementation	of	division	with	x86-64,	the
following	C	function	computes	the	quotient	and	remainder	of	two	64-bit,
signed	numbers:
This	compiles	to	the	following	assembly	code:</p>
<p>In	this	code,	argument	rp	must	first	be	saved	in	a	different	register	(line
2),	since	argument	register	
is	required	for	the	division	operation.
Lines	3-4	then	prepare	the	dividend	by	copying	and	sign-extending	
.
Following	the	division,	the	quotient	in	register	
gets	stored	at	
(line
6),	while	the	remainder	in	register	
gets	stored	at	
(line	7).
Unsigned	division	makes	use	of	the	divq	instruction.	Typically,	register
is	set	to	zero	beforehand.
Practice	Problem	
3.12	
(solution	page	
329
)
Consider	the	following	function	for	computing	the	quotient	and
remainder	of	two	unsigned	64-bit	numbers:
Modify	the	assembly	code	shown	for	signed	division	to	implement
this	function.</p>
<p>3.6	
Control
So	far,	we	have	only	considered	the	behavior	of	
straight-line
code,	where
instructions	follow	one	another	in	sequence.	Some	constructs	in	C,	such
as	conditionals,	loops,	and	switches,	require	conditional	execution,	where
the	sequence	of	operations	that	get	performed	depends	on	the	outcomes
of	tests	applied	to	the	data.	Machine	code	provides	two	basic	low-level
mechanisms	for	implementing	conditional	behavior:	it	tests	data	values
and	then	alters	either	the	control	flow	or	the	data	flow	based	on	the
results	of	these	tests.
Data-dependent	control	flow	is	the	more	general	and	more	common
approach	for	implementing	conditional	behavior,	and	so	we	will	examine
this	first.	Normally,	
both	statements	in	C	and	instructions	in	machine	code
are	executed	
sequentially
,	in	the	order	they	appear	in	the	program.	The
execution	order	of	a	set	of	machine-code	instructions	can	be	altered	with
a	
jump
instruction,	indicating	that	control	should	pass	to	some	other	part
of	the	program,	possibly	contingent	on	the	result	of	some	test.	The
compiler	must	generate	instruction	sequences	that	build	upon	this	low-
level	mechanism	to	implement	the	control	constructs	of	C.
In	our	presentation,	we	first	cover	the	two	ways	of	implementing
conditional	operations.	We	then	describe	methods	for	presenting	loops
and	switch	statements.
3.6.1	
Condition	Codes</p>
<p>In	addition	to	the	integer	registers,	the	CPU	maintains	a	set	of	single-bit
condition	code
registers	describing	attributes	of	the	most	recent
arithmetic	or	logical	operation.	These	registers	can	then	be	tested	to
perform	conditional	branches.	These	condition	codes	are	the	most	useful:
CF
:	Carry	flag.	The	most	recent	operation	generated	a	carry	out	of	the
most	significant	bit.	Used	to	detect	overflow	for	unsigned	operations.
ZF
:	Zero	flag.	The	most	recent	operation	yielded	zero.
SF
:	Sign	flag.	The	most	recent	operation	yielded	a	negative	value.
OF
:	Overflow	flag.	The	most	recent	operation	caused	a	two's-
complement	overflow—either	negative	or	positive.
For	example,	suppose	we	used	one	of	the	
ADD</p>
<p>instructions	to	perform	the
equivalent	of	the	C	assignment	
,	where	variables	
,	and	
are
integers.	Then	the	condition	codes	would	be	set	according	to	the
following	C	expressions:
The	
instruction	does	not	alter	any	condition	codes,	since	it	is
intended	to	be	used	in	address	computations.	Otherwise,	all	of	the
instructions	listed	in	
Figure	
3.10
cause	the	condition	codes	to	be	set.
For	the	logical	operations,	such	as	
XOR
,	the	carry	and	overflow	flags	are</p>
<p>set	to	zero.	For	the	shift	operations,	the	carry	flag	is	set	to	the	last	bit
shifted	out,	while	the	overflow	flag	is	set	to	zero.	For	reasons	that	we	will
not	delve	into,	the	
INC</p>
<p>and	
DEC</p>
<p>instructions	set	the	overflow	and	zero	flags,
but	they	leave	the	carry	flag	unchanged.
In	addition	to	the	setting	of	condition	codes	by	the	instructions	of	
Figure
3.10
,	there	are	two	instruction	classes	(having	8-,	16-,	32-,	and	64-bit
forms)	that	set	condition	codes	without	altering	any	other	registers;	these
are	listed	in	
Figure	
3.13
.	The	
CMP</p>
<p>instructions	set	the	condition	codes
according	to	the	differences	of	their	two	operands.	They	behave	in	the
same	way	as	the	
SUB</p>
<p>instructions,	except	that	they	set	the	condition
codes	without	updating	their	destinations.	With	ATT	format,
Instruction
Based	on
Description
CMP
S
,	
S
S
–	
S
Compare
 
Compare	byte
 
Compare	word
 
Compare	double	word
 
Compare	quad	word
TEST
S
,	
S
S
&amp;	
S
Test
 
Test	byte
 
Test	word
 
Test	double	word
1
2
2
1
1
2
1
2</p>
<p> 
Test	quad	word
Figure	
3.13	
Comparison	and	test	instructions.
These	instructions	set	the	condition	codes	without	updating	any	other
registers.
the	operands	are	listed	in	reverse	order,	making	the	code	difficult	to	read.
These	instructions	set	the	zero	flag	if	the	two	operands	are	equal.	The
other	flags	can	be	used	to	determine	ordering	relations	between	the	two
operands.	The	
TEST</p>
<p>instructions	behave	in	the	same	manner	as	the	
AND
instructions,	except	that	they	set	the	condition	codes	without	altering	their
destinations.	Typically,	the	same	operand	is	repeated	(e.g.,	
to	see	whether	
is	negative,	zero,	or	positive),	or	one	of
the	operands	is	a	mask	indicating	which	bits	should	be	tested.
3.6.2	
Accessing	the	Condition
Codes
Rather	than	reading	the	condition	codes	directly,	there	are	three	common
ways	of	using	the	condition	codes:	(1)	we	can	set	a	single	byte	to	0	or	1
depending	on	some	combination	of	the	condition	codes,	(2)	we	can
conditionally	jump	to	some	other	part	of	the	program,	or	(3)	we	can
conditionally	transfer	data.	For	the	first	case,	the	instructions	described	in
Figure	
3.14
set	a	single	byte	to	0	or	to	1	depending	on	some
combination	of	the	condition	codes.	We	refer	to	this	entire	class	of
instructions	as	the	
SET</p>
<p>instructions;	they	differ	from	one	another	based	on
which	combinations	of	condition	codes	they	consider,	as	indicated	by	the</p>
<p>different	suffixes	for	the	instruction	names.	It	is	important	to	recognize
that	the	suffixes	for	these	instructions	denote	different	conditions	and	not
different	operand	sizes.	For	example,	instructions	
and	
denote
&quot;set	less&quot;	and	&quot;set	below,&quot;	not	&quot;set	long	word&quot;	or	&quot;set	byte.&quot;
A	
SET</p>
<p>instruction	has	either	one	of	the	low-order	single-byte	register
elements	(
Figure	
3.2
)	or	a	single-byte	memory	location	as	its
destination,	setting	this	byte	to	either	0	or	1.	To	generate	a	32-bit	or	64-bit
result,	we	must	also	clear	the	high-order	bits.	A	typical	instruction
sequence	to	compute	the	C	expression	
,	where	
and	
are	both
of	type	
,	proceeds	as	follows:
Instruction
Synonym
Effect
Set	condition</p>
<p>D
D
←	
Equal	/	zero</p>
<p>D
D
←	~	
Not	equal	/	not	zero</p>
<p>D
D
←	
Negative</p>
<p>D
D
←	←	
Nonnegative</p>
<p>D
D
←	
Greater	(signed	&gt;)</p>
<p>D
D
←	
Greater	or	equal	(signed	&gt;=)</p>
<p>D
D
←	
Less	(signed	&lt;)</p>
<p>D
D
←	
Less	or	equal	(signed	&lt;=)</p>
<p>D
D
←	
Above	(unsigned	&gt;)</p>
<p>D
D
←	
Above	or	equal	(unsigned	&gt;=)</p>
<p>D
D
←	
Below	(unsigned	&lt;)</p>
<p>D
D
←	
Below	or	equal	(unsigned	&lt;=)
Figure	
3.14	
The	
SET</p>
<p>instructions.
Each	instruction	sets	a	single	byte	to	0	or	1	based	on	some	combination
of	the	condition	codes.	Some	instructions	have	&quot;synonyms,&quot;	that	is,
alternate	names	for	the	same	machine	instruction.
Note	the	comparison	order	of	the	
instruction	(line	2).	Although	the
arguments	are	listed	in	the	order	
(b),	then	
(a),	the	comparison
is	really	between	
and	
.	Recall	also,	as	discussed	in	
Section	
3.4.2
,
that	the	
instruction	(line	4)	clears	not	just	the	high-order	3	bytes	of
,	but	the	upper	4	bytes	of	the	entire	register,	
,	as	well.
For	some	of	the	underlying	machine	instructions,	there	are	multiple
possible	names,	which	we	list	as	&quot;synonyms.&quot;	For	example,	both	</p>
<p>(for	&quot;set	greater&quot;)	and	
(for	&quot;set	not	less	or	equal&quot;)	refer	to	the
same	machine	instruction.	Compilers	and	disassemblers	make	arbitrary
choices	of	which	names	to	use.
Although	all	arithmetic	and	logical	operations	set	the	condition	codes,	the
descriptions	of	the	different	
SET</p>
<h2>instructions	apply	to	the	case	where	a
comparison	instruction	has	been	executed,	setting	the	condition	codes
according	to	the	computation	
.	More	specifically,	let	
a
,	
b
,	and	
t
be
the	integers	represented	in	two's-complement	form	by	variables	
,
and	
,	respectively,	and	so	
,	where	
w
depends	on	the	sizes
associated	with	
and	
.
Consider	the	
,	or	&quot;set	when	equal,&quot;	instruction.	When	
a
=	
b
,	we	will
have	
t
=	0,	and	hence	the	zero	flag	indicates	equality.	Similarly,	consider
testing	for	signed	comparison	with	the	
,	or	&quot;set	when	less,&quot;
instruction.	When	no	overflow	occurs	(indicated	by	having	OF	set	to	0),
we	will	have	
a
≥	
b
when	
,	indicated	by	having	SF	set	to	1,	and	
a
≥	
b
when	
,	indicated	by	having	SF	set	to	0.	On	the	other	hand,
when	overflow	occurs,	we	will	have	
a
&lt;	
b
when	
(negative
overflow)	and	
a
&gt;	
b
when	
(positive	overflow).	We	cannot	have
overflow	when	
a
=	
b
.	Thus,	when	OF	is	set	to	1,	we	will	have	
a
&lt;	
b
if	and
only	if	SF	is	set	to	0.	Combining	these	cases,	the	
EXCLUSIVE</h2>
<p>OR</p>
<h1>of	the
overflow	and	sign	bits	provides	a	test	for	whether	
a
&lt;	
b
.	The	other	signed
comparison	tests	are	based	on	other	combinations	of	
and	
.
For	the	testing	of	unsigned	comparisons,	we	now	let	
a
and	
b
be	the
integers	represented	in	unsigned	form	by	variables	
and	
.	In
performing	the	computation	
,	the	carry	flag	will	be	set	by	the	
CMP
t</h1>
<p>a
−
w
t
b
a
−
w
t
b
&lt;
0
a
−
w
t
b
≥
0
a
−
w
t
b
&gt;
0</p>
<p>a
−
w
t
b
&lt;
0</p>
<p>instruction	when	
a
−	
b
&lt;	0,	and	so	the	unsigned	comparisons	use
combinations	of	the	carry	and	zero	flags.
It	is	important	to	note	how	machine	code	does	or	does	not	distinguish
between	signed	and	unsigned	values.	Unlike	in	C,	it	does	not	associate	a
data	type	with	each	program	value.	Instead,	it	mostly	uses	the	same
instructions	for	the	two	cases,	because	many	arithmetic	operations	have
the	same	bit-level	behavior	for	unsigned	and	two's-complement
arithmetic.	Some	circumstances	require	different	instructions	to	handle
signed	and	unsigned	operations,	such	as	using	different	versions	of	right
shifts,	division	and	multiplication	instructions,	and	different	combinations
of	condition	codes.
Practice	Problem	
3.13	
(solution	page	
330
)
The	C	code
shows	a	general	comparison	between	arguments	
and	
,	where
,	the	data	type	of	the	arguments,	is	defined	(via	
)	to
be	one	of	the	integer	data	types	listed	in	
Figure	
3.1
and	either
signed	or	unsigned.	The	comparison	COMP	is	defined	via
.
Suppose	a	is	in	some	portion	of	
while	
is	in	some	portion	of
.	For	each	of	the	following	instruction	sequences,	determine</p>
<p>which	data	types	
and	which	comparisons	COMP	could
cause	the	compiler	to	generate	this	code.	(There	can	be	multiple
correct	answers;	you	should	list	them	all.)
A
.	
B
.	
C
.	
D
.	
Practice	Problem	
3.14	
(solution	page	
330
)
The	C	code</p>
<p>shows	a	general	comparison	between	argument	a	and	0,	where
we	can	set	the	data	type	of	the	argument	by	declaring	
with
a	
,	and	the	nature	of	the	comparison	by	declaring	TEST
with	a	
declaration.	The	following	instruction	sequences
implement	the	comparison,	where	a	is	held	in	some	portion	of
register	
.	For	each	sequence,	determine	which	data	types
and	which	comparisons	TEST	could	cause	the	compiler	to
generate	this	code.	(There	can	be	multiple	correct	answers;	list	all
correct	ones.)
A
.	
B
.	
C
.	</p>
<p>D
.	
3.6.3	
Jump	Instructions
Under	normal	execution,	instructions	follow	each	other	in	the	order	they
are	listed.	A	
jump
instruction	can	cause	the	execution	to	switch	to	a
completely	new	position	in	the	program.	These	jump	destinations	are
generally	indicated	in	assembly	code	by	a	
label
.	Consider	the	following
(very	contrived)	assembly-code	sequence:
Instruction
Synonym
Jump	condition
Description
Label
1
Direct	jump</p>
<ul>
<li></li>
</ul>
<p>Operand
1
Indirect	jump
Label
ZF
Equal	/	zero</p>
<p>Label
~ZF
Not	equal	/	not	zero
Label
SF
Negative
Label
~SF
Nonnegative
Label
~(SF	^	OF)	&amp;	~ZF
Greater	(signed	&gt;)
Label
~(SF	^	OF)
Greater	or	equal	(signed	&gt;=)
Label
SF	^	OF
Less	(signed	&lt;)
Label
(SF	^	OF)	|	ZF
Less	or	equal	(signed	&lt;=)
Label
~CF	&amp;	~ZF
Above	(unsigned	&gt;)
Label
~CF
Above	or	equal	(unsigned	&gt;=)
Label
CF
Below	(unsigned	&lt;)
Label
CF	|	ZF
Below	or	equal	(unsigned	&lt;=)
Figure	
3.15	
The	jump	instructions.
These	instructions	jump	to	a	labeled	destination	when	the	jump	condition
holds.	Some	instructions	have	&quot;synonyms,&quot;	alternate	names	for	the	same
machine	instruction.
The	instruction	
will	cause	the	program	to	skip	over	the	
instruction	and	instead	resume	execution	with	the	
instruction.	In
generating	the	object-code	file,	the	assembler	determines	the	addresses
of	all	labeled	instructions	and	encodes	the	
jump	targets
(the	addresses	of
the	destination	instructions)	as	part	of	the	jump	instructions.</p>
<p>Figure	
3.15
shows	the	different	jump	instructions.	The	
instruction
jumps	unconditionally.	It	can	be	either	a	
direct
jump,	where	the	jump
target	is	encoded	as	part	of	the	instruction,	or	an	
indirect
jump,	where	the
jump	target	is	read	from	a	register	or	a	memory	location.	Direct	jumps	are
written	in	assembly	code	by	giving	a	label	as	the	jump	target,	for
example,	the	label	
in	the	code	shown.	Indirect	jumps	are	written
using	`*'	followed	by	an	operand	specifier	using	one	of	the	memory
operand	formats	described	in	
Figure	
3.3
.	As	examples,	the	instruction
uses	the	value	in	register	
as	the	jump	target,	and	the	instruction
reads	the	jump	target	from	memory,	using	the	value	in	
as	the	read
address.
The	remaining	jump	instructions	in	the	table	are	
conditional
—they	either
jump	or	continue	executing	at	the	next	instruction	in	the	code	sequence,
depending	on	some	combination	of	the	condition	codes.	The	names	of
these	instructions	
and	the	conditions	under	which	they	jump	match	those
of	the	
SET</p>
<p>instructions	(see	
Figure	
3.14
).	As	with	the	
SET</p>
<p>instructions,
some	of	the	underlying	machine	instructions	have	multiple	names.
Conditional	jumps	can	only	be	direct.</p>
<p>3.6.4	
Jump	Instruction	Encodings
For	the	most	part,	we	will	not	concern	ourselves	with	the	detailed	format
of	machine	code.	On	the	other	hand,	understanding	how	the	targets	of
jump	instructions	are	encoded	will	become	important	when	we	study
linking	in	
Chapter	
7
.	In	addition,	it	helps	when	interpreting	the	output
of	a	disassembler.	In	assembly	code,	jump	targets	are	written	using
symbolic	labels.	The	assembler,	and	later	the	linker,	generate	the	proper
encodings	of	the	jump	targets.	There	are	several	different	encodings	for
jumps,	but	some	of	the	most	commonly	used	ones	are	
PC	relative
.	That
is,	they	encode	the	difference	between	the	address	of	the	target
instruction	and	the	address	of	the	instruction	immediately	following	the
jump.	These	offsets	can	be	encoded	using	1,	2,	or	4	bytes.	A	second
encoding	method	is	to	give	an	&quot;absolute&quot;	address,	using	4	bytes	to
directly	specify	the	target.	The	assembler	and	linker	select	the
appropriate	encodings	of	the	jump	destinations.
As	an	example	of	PC-relative	addressing,	the	following	assembly	code
for	a	function	was	generated	by	compiling	a	file	branch.	
It	contains
two	jumps:	the	
instruction	on	line	2	jumps	forward	to	a	higher
address,	while	the	
instruction	on	line	7	jumps	back	to	a	lower	one.</p>
<p>The	disassembled	version	of	the	
format	generated	by	the	assembler
is	as	follows:
In	the	annotations	on	the	right	generated	by	the	disassembler,	the	jump
targets	are	indicated	as	
for	the	jump	instruction	on	line	2	and	
for
the	jump	instruction	on	line	5	(the	disassembler	lists	all	numbers	in
hexadecimal).	Looking	at	the	byte	encodings	of	the	instructions,	however,
we	see	that	the	target	of	the	first	jump	instruction	is	encoded	(in	the
second	byte)	as	
.	Adding	this	to	
,	the
Aside	
What	do	the	instructions	
and
do?</p>
<p>Line	8	of	the	assembly	code	shown	on	page	207	contains	the
instruction	combination	
.	These	are	rendered	in	the
disassembled	code	(line	6)	as	
.	One	can	infer	that	
is	a	synonym	for	
,	just	as	
is	a	synonym	for	
.	Looking
at	the	Intel	and	AMD	documentation	for	the	
instruction,	we
find	that	it	is	normally	used	to	implement	a	repeating	string
operation	
[3,</p>
<p>51]
.	It	seems	completely	inappropriate	here.	The
answer	to	this	puzzle	can	be	seen	in	AMD's	guidelines	to	compiler
writers	
[1]
.	They	recommend	using	the	combination	of	
followed	by	
to	avoid	making	the	
instruction	the
destination	of	a	conditional	jump	instruction.	Without	the	
instruction,	the	
instruction	(line	7	of	the	assembly	code)	would
proceed	to	the	
instruction	when	the	branch	is	not	taken.
According	toAMD,	their	processors	cannot	properly	predict	the
destination	of	a	
instruction	when	it	is	reached	from	a	jump
instruction.	The	
instruction	serves	as	a	form	of	no-operation
here,	and	so	inserting	it	as	the	jump	destination	does	not	change
behavior	of	the	code,	except	to	make	it	faster	on	AMD	processors.
We	can	safely	ignore	any	
or	
instruction	we	see	in	the
rest	of	the	code	presented	in	this	book.
address	of	the	following	instruction,	we	get	jump	target	address	
,	the
address	of	the	instruction	on	line	4.
Similarly,	the	target	of	the	second	jump	instruction	is	encoded	as	
(decimal	−8)	using	a	single-byte	two's-complement	representation.
Adding	this	to	
(decimal	13),	the	address	of	the	instruction	on	line	6,
we	get	
,	the	address	of	the	instruction	on	line	3.</p>
<p>As	these	examples	illustrate,	the	value	of	the	program	counter	when
performing	PC-relative	addressing	is	the	address	of	the	instruction
following	the	jump,	not	that	of	the	jump	itself.	This	convention	dates	back
to	early	implementations,	when	the	processor	would	update	the	program
counter	as	its	first	step	in	executing	an	instruction.
The	following	shows	the	disassembled	version	of	the	program	after
linking:
The	instructions	have	been	relocated	to	different	addresses,	but	the
encodings	of	the	jump	targets	in	lines	2	and	5	remain	unchanged.	By
using	a	PC-relative	encoding	of	the	jump	targets,	the	instructions	can	be
compactly	encoded	(requiring	just	2	bytes),	and	the	object	code	can	be
shifted	to	different	positions	in	memory	without	alteration.
Practice	Problem	
3.15	
(solution	page	
330
)
In	the	following	excerpts	from	a	disassembled	binary,	some	of	the
information	has	been	replaced	by	X's.	Answer	the	following
questions	about	these	instructions.</p>
<p>A
.	
What	is	the	target	of	the	
instruction	below?	(You	do	not
need	to	know	anything	about	the	
instruction	here.)
B
.	
What	is	the	target	of	the	
instruction	below?
C
.	
What	is	the	address	of	the	
and	pop	instructions?
D
.	
In	the	code	that	follows,	the	jump	target	is	encoded	in	PC-
relative	form	as	a	4-byte	two's-complement	number.	The
bytes	are	listed	from	least	significant	to	most,	reflecting	the
little-endian	byte	ordering	of	x86-64.	What	is	the	address	of
the	jump	target?</p>
<p>The	jump	instructions	provide	a	means	to	implement	conditional
execution	(
),	as	well	as	several	different	loop	constructs.
3.6.5	
Implementing	Conditional
Branches	with	Conditional	Control
The	most	general	way	to	translate	conditional	expressions	and
statements	from	C	into	machine	code	is	to	use	combinations	of
conditional	and	unconditional	jumps.	(As	an	alternative,	we	will	see	in
Section	
3.6.6
that	some	conditionals	can	be	implemented	by
conditional	transfers	of	data	rather	than	control.)	For	example,	
Figure
3.16(a)
shows	the	C	code	for	a	function	that	computes	the	absolute
value	of	the	difference	of	two	numbers.
The	function	also	has	a	side
effect	of	incrementing	one	of	two	counters,	encoded	as	global	variables
Gcc	generates	the	assembly	code	shown	as	
Figure
3.16(c)
.	Our	rendition	of	the	machine	code	into	C	is	shown	as	the
function	
(
Figure	
3.16(b)
).	It	uses	the	
statement	in	C,
which	is	similar	to	the	unconditional	jump	of
3.	
Actually,	it	can	return	a	negative	value	if	one	of	the	subtractions	overflows.	Our	interest	here	is
to	demonstrate	machine	code,	not	to	implement	robust	code.
(a)	Original	C	code
3</p>
<p>(b)	Equivalent	goto	version</p>
<p>(c)	Generated	assembly	code
Figure	
3.16	
Compilation	of	conditional	statements.
(a)	C	procedure	
contains	an	if-else	statement.	The	generated
assembly	code	is	shown	(c),	along	with	(b)	a	C	procedure	
that	mimics	the	control	flow	of	the	assembly	code.</p>
<p>assembly	code.	Using	
statements	is	generally	considered	a	bad
programming	style,	since	their	use	can	make	code	very	difficult	to	read
and	debug.	We	use	them	in	our	presentation	as	a	way	to	construct	C
programs	that	describe	the	control	flow	of	machine	code.	We	call	this
style	of	programming	&quot;goto	code.&quot;
In	the	goto	code	(
Figure	
3.16(b)
),	the	statement	
on	line	5
causes	a	jump	to	the	label	
(since	it	occurs	when	
x
≥	
y
)	on	line	9.
Continuing	the
Aside	
Describing	machine	code	with	C
code
Figure	
3.16
shows	an	example	of	how	we	will	demonstrate	the
translation	of	C	language	control	constructs	into	machine	code.
The	figure	contains	an	example	C	function	(a)	and	an	annotated
version	of	the	assembly	code	generated	by	
GCC</p>
<p>(c).	It	also
contains	a	version	in	C	that	closely	matches	the	structure	of	the
assembly	code	(b).	Although	these	versions	were	generated	in	the
sequence	(a),	(c),	and	(b),	we	recommend	that	you	read	them	in
the	order	(a),	(b),	and	then	(c).	That	is,	the	C	rendition	of	the
machine	code	will	help	you	understand	the	key	points,	and	this
can	guide	you	in	understanding	the	actual	assembly	code.
execution	from	this	point,	it	completes	the	computations	specified	by	the
portion	of	function	
and	returns.	On	the	other	hand,	if	the
test	
fails,	the	program	procedure	will	carry	out	the	steps	specified
by	the	if	portion	of	
and	return.</p>
<p>The	assembly-code	implementation	(
Figure	
3.16(c)
)	first	compares
the	two	operands	(line	2),	setting	the	condition	codes.	If	the	comparison
result	indicates	that	
x
is	greater	than	or	equal	to	
y
,	it	then	jumps	to	a
block	of	code	starting	at	line	8	that	increments	global	variable	
,
computes	
as	the	return	value,	and	returns.	Otherwise,	it	continues
with	the	execution	of	code	beginning	at	line	4	that	increments	global
variable	
,	computes	
as	the	return	value,	and	returns.	We	can
see,	then,	that	the	control	flow	of	the	assembly	code	generated	for
closely	follows	the	goto	code	of	
The	general	form	of	an	if-else	statement	in	C	is	given	by	the	template
where	
test-expr
is	an	integer	expression	that	evaluates	either	to	zero
(interpreted	as	meaning	&quot;false&quot;)	or	to	a	nonzero	value	(interpreted	as
meaning	&quot;true&quot;).	Only	one	of	the	two	branch	statements	(
then-statement
or	
else-statement
)	is	executed.
For	this	general	form,	the	assembly	implementation	typically	adheres	to
the	following	form,	where	we	use	C	syntax	to	describe	the	control	flow:</p>
<p>That	is,	the	compiler	generates	separate	blocks	of	code	for	
then-
statement
and	
else-statement
.	It	inserts	conditional	and	unconditional
branches	to	make	sure	the	correct	block	is	executed.
Practice	Problem	
3.16	
(solution	page	
331
)
When	given	the	C	code
GCC</p>
<p>generates	the	following	assembly	code:</p>
<p>A
.	
Write	a	goto	version	in	C	that	performs	the	same
computation	and	mimics	the	control	flow	of	the	assembly
code,	in	the	style	shown	in	
Figure	
3.16(b)
.	You	might
find	it	helpful	to	first	annotate	the	assembly	code	as	we
have	done	in	our	examples.
B
.	
Explain	why	the	assembly	code	contains	two	conditional
branches,	even	though	the	C	code	has	only	one	if
statement.
Practice	Problem	
3.17	
(solution	page	
331
)
An	alternate	rule	for	translating	
statements	into	goto	code	is	as
follows:</p>
<p>A
.	
Rewrite	the	goto	version	of	
based	on	this
alternate	rule.
B
.	
Can	you	think	of	any	reasons	for	choosing	one	rule	over	the
other?
Practice	Problem	
3.18	
(solution	page	
332
)
Starting	with	C	code	of	the	form
GCC</p>
<p>generates	the	following	assembly	code:</p>
<p>Fill	in	the	missing	expressions	in	the	C	code.
3.6.6	
Implementing	Conditional</p>
<p>Branches	with	Conditional	Moves
The	conventional	way	to	implement	conditional	operations	is	through	a
conditional	transfer	of	
control
,	where	the	program	follows	one	execution
path	when	a	condition	holds	and	another	when	it	does	not.	This
mechanism	is	simple	and	general,	but	it	can	be	very	inefficient	on
modern	processors.
An	alternate	strategy	is	through	a	conditional	transfer	of	
data
.	This
approach	computes	both	outcomes	of	a	conditional	operation	and	then
selects	one	based	on	whether	or	not	the	condition	holds.	This	strategy
makes	sense	only	in	restricted	cases,	but	it	can	then	be	implemented	by
a	simple	
conditional	move
instruction	that	is	better	matched	to	the
performance	characteristics	of	modern	processors.	Here,	we	examine
this	strategy	and	its	implementation	with	x86-64.
Figure	
3.17(a)
shows	an	example	of	code	that	can	be	compiled	using
a	conditional	move.	The	function	computes	the	absolute	value	of	its
arguments	x	and	y,	as	did	our	earlier	example	(
Figure	
3.16
).Whereas
the	earlier	example	had	side	effects	in	the	branches,	modifying	the	value
of	either	
or	
,	this	version	simply	computes	the	value	to	be
returned	by	the	function.
(a)	Original	C	code</p>
<p>(b)	Implementation	using	conditional	assignment
(c)	Generated	assembly	code</p>
<p>Figure	
3.17	
Compilation	of	conditional	statements	using	conditional
assignment.
(a)	C	function	
contains	a	conditional	expression.	The	generated
assembly	code	is	shown	(c),	along	with	(b)	a	C	function	
that
mimics	the	operation	of	the	assembly	code.
For	this	function,	
generates	the	assembly	code	shown	in	
Figure
3.17(c)
,	having	an	approximate	form	shown	by	the	C	function	
shown	in	
Figure	
3.17(b)
.	Studying	the	C	version,	we	can	see	that	it
computes	both	
and	
,	naming	these	
and	
,	respectively.
It	then	tests	whether	
x
is	greater	than	or	equal	to	
y
,	and	if	so,	copies	
to	
before	returning	
.	The	assembly	code	in	
Figure	
3.17(c)
follows	the	same	logic.	The	key	is	that	the	single	
instruction	(line
7)	of	the	assembly	code	implements	the	conditional	assignment	(line	8)
of	
.	It	will	transfer	the	data	from	the	source	register	to	the
destination,	only	if	the	
instruction	of	line	6	indicates	that	one	value	is
greater	than	or	equal	to	the	other	(as	indicated	by	the	suffix	
).</p>
<p>To	understand	why	code	based	on	conditional	data	transfers	can
outperform	code	based	on	conditional	control	transfers	(as	in	
Figure
3.16
),	we	must	understand	something	about	how	modern	processors
operate.	As	we	will	see	in	
Chapters	
4
and	
5
,	processors	achieve
high	performance	through	
pipelining
,	where	an	instruction	is	processed
via	a	sequence	of	stages,	each	performing	one	small	portion	of	the
required	operations	(e.g.,	fetching	the	instruction	from	memory,
determining	the	instruction	type,	reading	from	memory,	performing	an
arithmetic	operation,	writing	to	memory,	and	updating	the	program
counter).	This	approach	achieves	high	performance	by	overlapping	the
steps	of	the	successive	instructions,	such	as	fetching	one	instruction
while	performing	the	arithmetic	operations	for	a	previous	instruction.	To
do	this	requires	being	able	to	determine	the	sequence	of	instructions	to
be	executed	well	ahead	of	time	in	order	to	keep	the	pipeline	full	of
instructions	to	be	executed.	When	the	machine	encounters	a	conditional
jump	(referred	to	as	a	&quot;branch&quot;),	it	cannot	determine	which	way	the
branch	will	go	until	it	has	evaluated	the	branch	condition.	Processors
employ	sophisticated	
branch	prediction	logic
to	try	to	guess	whether	or
not	each	jump	instruction	will	be	followed.	As	long	as	it	can	guess	reliably
(modern	microprocessor	designs	try	to	achieve	success	rates	on	the
order	of	90%),	the	instruction	pipeline	will	be	kept	full	of	instructions.
Mispredicting	a	jump,	on	the	other	hand,	requires	that	the	processor
discard	much	of	the	work	it	has	already	done	on	future	instructions	and
then	begin	filling	the	pipeline	with	instructions	starting	at	the	correct
location.	As	we	will	see,	such	a	misprediction	can	incur	a	serious	penalty,
say,	15–30	clock	cycles	of	wasted	effort,	causing	a	serious	degradation
of	program	performance.</p>
<h1>As	an	example,	we	ran	timings	of	the	
function	on	an	Intel
Haswell	processor	using	both	methods	of	implementing	the	conditional
operation.	In	a	typical	application,	the	outcome	of	the	test	
is	highly
unpredictable,	and	so	even	the	most	sophisticated	branch	prediction
hardware	will	guess	correctly	only	around	50%	of	the	time.	In	addition,
the	computations	performed	in	each	of	the	two	code	sequences	require
only	a	single	clock	cycle.	As	a	consequence,	the	branch	misprediction
penalty	dominates	the	performance	of	this	function.	For	x86-64	code	with
conditional	jumps,	we	found	that	the	function	requires	around	8	clock
cycles	per	call	when	the	branching	pattern	is	easily	predictable,	and
around	17.50	clock	cycles	per	call	when	the	branching	pattern	is	random.
From	this,	we	can	infer	that	the	branch	misprediction	penalty	is	around	19
clock	cycles.	That	means	time	required	by	the	function	ranges	between
around	8	and	27	cycles,	depending	on	whether	or	not	the	branch	is
predicted	correctly.
Aside	
How	did	you	determine	this
penalty?
Assume	the	probability	of	misprediction	is	
p
,	the	time	to	execute
the	code	without	misprediction	is	
T
,	and	the	misprediction
penalty	is	
.	We	are	given
T
and	
T
,	the	average	time	when	
p
=	0.5,	and	we	want	to
determine	
T
.	Substituting	into	the	equation,	we	get
.
On	the	other	hand,	the	code	compiled	using	conditional	moves	requires
around	8	clock	cycles	regardless	of	the	data	being	tested.	The	flow	of
OK
T
avg
(
P
)</h1>
<p>(
1
−
P
)
T
OK</p>
<ul>
<li></li>
</ul>
<h1>T
MP
)</h1>
<p>T
OK</p>
<ul>
<li></li>
</ul>
<h1>P
T
MP
OK
ran
MP
T
ran</h1>
<h1>T
avg
(
0.5
)</h1>
<p>T
OK</p>
<ul>
<li></li>
</ul>
<h1>0.5
T
MP
,
 
and
 
therefore
 
T
MP</h1>
<h1>2
(
T
ran
−
T
OK
)
.
 
S
o
,
 
f
o
r
 
T
OK</h1>
<h1>8
 
and
 
T
ran</h1>
<h1>17.5
,
 
we
 
get
 
T
MP</h1>
<p>19</p>
<p>control	does	not	depend	on	data,	and	this	makes	it	easier	for	the
processor	to	keep	its	pipeline	full.
Practice	Problem	
3.19	
(solution	page	
332
)
Running	on	an	older	processor	model,	our	code	required	around
16	cycles	when	the	branching	pattern	was	highly	predictable,	and
around	31	cycles	when	the	pattern	was	random.
A
.	
What	is	the	approximate	miss	penalty?
B
.	
How	many	cycles	would	the	function	require	when	the
branch	is	mispredicted?
Figure	
3.18
illustrates	some	of	the	conditional	move	instructions
available	with	x86-64.	Each	of	these	instructions	has	two	operands:	a
source	register	or	memory	location	
S
,	and	a	destination	register	
R
.	As
with	the	different	set	(
Section	
3.6.2
)	and	jump	(
Section	
3.6.3
)
instructions,	the	outcome	of	these	instructions	depends	on	the	values	of
the	condition	codes.	The	source	value	is	read	from	either	memory	or	the
source	register,	but	it	is	copied	to	the	destination	only	if	the	specified
condition	holds.
The	source	and	destination	values	can	be	16,	32,	or	64	bits	long.	Single-
byte	conditional	moves	are	not	supported.	Unlike	the	unconditional
instructions,	where	the	operand	length	is	explicitly	encoded	in	the
instruction	name	(e.g.,	
and	
),	the	assembler	can	infer	the
operand	length	of	a	conditional	move	instruction	from	the	name	of	the
destination	register,	and	so	the	same	instruction	name	can	be	used	for	all
operand	lengths.</p>
<p>Unlike	conditional	jumps,	the	processor	can	execute	conditional	move
instructions	without	having	to	predict	the	outcome	of	the	test.	The
processor	simply	reads	the	source	value	(possibly	from	memory),	checks
the	condition	code,	and	then	either	updates	the	destination	register	or
keeps	it	the	same.	We	will	explore	the	implementation	of	conditional
moves	in	
Chapter	
4
.
To	understand	how	conditional	operations	can	be	implemented	via
conditional	data	transfers,	consider	the	following	general	form	of
conditional	expression	and	assignment:
Instruction
Synonym
Move	condition
Description
S
,	
R
Equal	/	zero
S
,	
R
Not	equal	/	not	zero
S
,	
R
Negative
S
,	
R
Nonnegative
S
,	
R
Greater	(signed	&gt;)
S
,	
R
Greater	or	equal	(signed	&gt;=)
S
,	
R
Less	(signed	&lt;)
S
,	
R
Less	or	equal	(signed	&lt;=)
S
,	
R
Above	(unsigned	&gt;)
S
,	
R
Above	or	equal	(Unsigned	&gt;=)
S
,	
R
Below	(unsigned	&lt;)</p>
<p>S
,	
R
Below	or	equal	(unsigned	&lt;=)
Figure	
3.18	
The	conditional	move	instructions.
These	instructions	copy	the	source	value	
S
to	its	destination	
R
when	the
move	condition	holds.	Some	instructions	have	&quot;synonyms,&quot;	alternate
names	for	the	same	machine	instruction.
The	standard	way	to	compile	this	expression	using	conditional	control
transfer	would	have	the	following	form:
This	code	contains	two	code	sequences—one	evaluating	
then-expr
and
one	evaluating	
else-expr
.	A	combination	of	conditional	and	unconditional
jumps	is	used	to	ensure	that	just	one	of	the	sequences	is	evaluated.</p>
<p>For	the	code	based	on	a	conditional	move,	both	the	
then-expr
and	the
else-expr
are	evaluated,	with	the	final	value	chosen	based	on	the
evaluation	
test-expr
.	This	can	be	described	by	the	following	abstract
code:
The	final	statement	in	this	sequence	is	implemented	with	a	conditional
move—value	
is	copied	to	
only	if	test	condition	
does	not	hold.
Not	all	conditional	expressions	can	be	compiled	using	conditional	moves.
Most	significantly,	the	abstract	code	we	have	shown	evaluates	both	
then-
expr
and	
else-expr
regardless	of	the	test	outcome.	If	one	of	those	two
expressions	could	possibly	generate	an	error	condition	or	a	side	effect,
this	could	lead	to	invalid	behavior.	Such	is	the	case	for	our	earlier
example	(
Figure	
3.16
).	Indeed,	we	put	the	side	effects	into	this
example	specifically	to	force	
to	implement	this	function	using
conditional	transfers.
As	a	second	illustration,	consider	the	following	C	function:</p>
<p>At	first,	this	seems	like	a	good	candidate	to	compile	using	a	conditional
move	to	set	the	result	to	zero	when	the	pointer	is	null,	as	shown	in	the
following	assembly	code:
This	implementation	is	invalid,	however,	since	the	dereferencing	of	
by
the	
instruction	(line	2)	occurs	even	when	the	test	fails,	causing	a
null	pointer	dereferencing	error.	Instead,	this	code	must	be	compiled
using	branching	code.
Using	conditional	moves	also	does	not	always	improve	code	efficiency.
For	example,	if	either	the	
then-expr
or	the	
else-expr
evaluation	requires	a
significant	computation,	then	this	effort	is	wasted	when	the	corresponding
condition	does	not	hold.	Compilers	must	take	into	account	the	relative
performance	of	wasted	computation	versus	the	potential	for	performance</p>
<p>penalty	due	to	branch	misprediction.	In	truth,	they	do	not	really	have
enough	information	to	make	this	decision	reliably;	for	example,	they	do
not	know	how	well	the	branches	will	follow	predictable	patterns.	Our
experiments	with	
indicate	that	it	only	uses	conditional	moves	when
the	two	expressions	can	be	computed	very	easily,	for	example,	with
single	add	instructions.	In	our	experience,	
uses	conditional	control
transfers	even	in	many	cases	where	the	cost	of	branch	misprediction
would	exceed	even	more	complex	computations.
Overall,	then,	we	see	that	conditional	data	transfers	offer	an	alternative
strategy	to	conditional	control	transfers	for	implementing	conditional
operations.	They	can	only	be	used	in	restricted	cases,	but	these	cases
are	fairly	common	and	provide	a	much	better	match	to	the	operation	of
modern	processors.</p>
<p>Practice	Problem	
3.20	
(solution	page	
333
)
In	the	following	C	function,	we	have	left	the	definition	of	operation
incomplete:
When	compiled,	
generates	the	following	assembly	code:
A
.	
What	operation	is	
?
B
.	
Annotate	the	code	to	explain	how	it	works.
Practice	Problem	
3.21	
(solution	page	
333
)</p>
<p>Starting	with	C	code	of	the	form
generates	the	following	assembly	code:</p>
<p>Fill	in	the	missing	expressions	in	the	C	code.
3.6.7	
Loops
C	provides	several	looping	constructs—namely,	
,	and
.	No	corresponding	instructions	exist	in	machine	code.	Instead,
combinations	of	conditional	tests	and	jumps	are	used	to	implement	the
effect	of	loops.	
and	other	compilers	generate	loop	code	based	on	the
two	basic	loop	patterns.	We	will	study	the	translation	of	loops	as	a
progression,	starting	with	
and	then	working	toward	ones	with
more	complex	implementations,	covering	both	patterns.
Do-While	Loops
The	general	form	of	a	
statement	is	as	follows:</p>
<p>The	effect	of	the	loop	is	to	repeatedly	execute	
body-statement
,	evaluate
test-expr
,	and	continue	the	loop	if	the	evaluation	result	is	nonzero.
Observe	that	
body-statement
is	executed	at	least	once.
This	general	form	can	be	translated	into	conditionals	and	
statements	as	follows:
That	is,	on	each	iteration	the	program	evaluates	the	body	statement	and
then	the	test	expression.	If	the	test	succeeds,	the	program	goes	back	for
another	iteration.
(a)	C	code</p>
<p>(b)	Equivalent	goto	version
(c)	Corresponding	assembly-language	code</p>
<p>Figure	
3.19	
Code	for	
version	of	factorial	program.
A	conditional	jump	causes	the	program	to	loop.
As	an	example,	
Figure	
3.19(a)
shows	an	implementation	of	a	routine
to	compute	the	factorial	of	its	argument,	written	
n
!,	with	a	do-while	loop.
This	function	only	computes	the	proper	value	for	
n
&gt;	0.
Practice	Problem	
3.22	
(solution	page	
333
)
A
.	
What	is	the	maximum	value	of	
n
for	which	we	can	represent	
n
!
with	a	32-bit	
B
.	
What	about	for	a	64-bit	
The	goto	code	shown	in	
Figure	
3.19(b)
shows	how	the	loop	gets
turned	into	a	lower-level	combination	of	tests	and	conditional	jumps.
Following	the	initialization	of	
,	the	program	begins	looping.	First	it
executes	the	body	of	the	loop,	consisting	here	of	updates	to	variables
and	
n
.	It	then	tests	whether	
n
&gt;	1,	and,	if	so,	it	jumps	back	to	the
beginning	of	the	loop.	
Figure	
3.19(c)
shows</p>
<p>Aside	
Reverse	engineering	loops
A	key	to	understanding	how	the	generated	assembly	code	relates
to	the	original	source	code	is	to	find	a	mapping	between	program
values	and	registers.	This	task	was	simple	enough	for	the	loop	of
Figure	
3.19
,	but	it	can	be	much	more	challenging	for	more
complex	programs.	The	C	compiler	will	often	rearrange	the
computations,	so	that	some	variables	in	the	C	code	have	no
counterpart	in	the	machine	code,	and	new	values	are	introduced
into	the	machine	code	that	do	not	exist	in	the	source	code.
Moreover,	it	will	often	try	to	minimize	register	usage	by	mapping
multiple	program	values	onto	a	single	register.
The	process	we	described	for	
works	as	a	general	strategy
for	reverse	engineering	loops.	Look	at	how	registers	are	initialized
before	the	loop,	updated	and	tested	within	the	loop,	and	used	after
the	loop.	Each	of	these	provides	a	clue	that	can	be	combined	to
solve	a	puzzle.	Be	prepared	for	surprising	transformations,	some
of	which	are	clearly	cases	where	the	compiler	was	able	to
optimize	the	code,	and	others	where	it	is	hard	to	explain	why	the
compiler	chose	that	particular	strategy.
the	assembly	code	from	which	the	goto	code	was	generated.	The
conditional	jump	instruction	
(line	7)	is	the	key	instruction	in
implementing	a	loop.	It	determines	whether	to	continue	iterating	or	to	exit
the	loop.
Reverse	engineering	assembly	code,	such	as	that	of	
Figure	
3.19(c)
,
requires	determining	which	registers	are	used	for	which	program	values.
In	this	case,	the	mapping	is	fairly	simple	to	determine:	We	know	that	
n
will	be	passed	to	the	function	in	register	
.	We	can	see	register	</p>
<p>getting	initialized	to	1	(line	2).	(Recall	that,	although	the	instruction	has
as	its	destination,	it	will	also	set	the	upper	4	bytes	of	
to	0.)	We
can	see	that	this	register	is	also	updated	by	multiplication	on	line	4.
Furthermore,	since	
is	used	to	return	the	function	value,	it	is	often
chosen	to	hold	program	values	that	are	returned.	We	therefore	conclude
that	
corresponds	to	program	value	
.
Practice	Problem	
3.23	
(solution	page	
334
)
For	the	C	code
GCC</p>
<p>generates	the	following	assembly	code:</p>
<p>A
.	
Which	registers	are	used	to	hold	program	values	x,	y,	and
n?
B
.	
How	has	the	compiler	eliminated	the	need	for	pointer
variable	
and	the	pointer	dereferencing	implied	by	the
expression	
C
.	
Add	annotations	to	the	assembly	code	describing	the
operation	of	the	program,	similar	to	those	shown	in	
Figure
3.19(c)
.
While	Loops
The	general	form	of	a	
statement	is	as	follows:</p>
<p>It	differs	from	
in	that	
test-expr
is	evaluated	and	the	loop	is
potentially	terminated	before	the	first	execution	of	
body-statement
.	There
are	a	number	of	ways	to	translate	a	
into	machine	code,	two	of
which	are	used	in	code	generated	by	
.	Both	use	the	same	loop
structure	as	we	saw	for	
but	differ	in	how	to	implement	the
initial	test.
The	first	translation	method,	which	we	refer	to	as	
jump	to	middle
,
performs	the	initial	test	by	performing	an	unconditional	jump	to	the	test	at
the	end	of	the	loop.	It	can	be	expressed	by	the	following	template	for
translating	from	the	general	
form	to	goto	code:
As	an	example,	
Figure	
3.20(a)
shows	an	implementation	of	the
factorial	function	using	a	
loop.	This	function	correctly	computes	0!
=	1.	The	adjacent
(a)	C	code</p>
<p>(b)	Equivalent	goto	version</p>
<p>(c)	Corresponding	assembly-language	code
Figure	
3.20	
C	and	assembly	code	for	
version	of	factorial	using
jump-to-middle	translation.
The	C	function	
illustrates	the	operation	of	the
assembly-code	version.
function	
(
Figure	
3.20(b)
)	is	a	C	rendition	of	the
assembly	code	generated	by	
when	optimization	is	specified	with	the
command-line	option	
.	Comparing	the	goto	code	generated	for
(
Figure	
3.20(b)
)	to	that	for	
(
Figure	
3.19(b)
),	we
see	that	they	are	very	similar,	except	that	the	statement	
before
the	loop	causes	the	program	to	first	perform	the	test	of	
before</p>
<p>modifying	the	values	of	
or	
.	The	bottom	portion	of	the	figure
(
Figure	
3.20(c)
)	shows	the	actual	assembly	code	generated.
Practice	Problem	
3.24	
(solution	page	
335
)
For	C	code	having	the	general	form
GCC
,	run	with	command-line	option	
,	produces	the	following
code:</p>
<p>We	can	see	that	the	compiler	used	a	jump-to-middle	translation,
using	the	
instruction	on	line	3	to	jump	to	the	test	starting	with
label	
Fill	in	the	missing	parts	of	the	C	code.
The	second	translation	method,	which	we	refer	to	as	
guarded	do
,	first
transforms	the	code	into	a	
loop	by	using	a	conditional	branch	to
skip	over	the	loop	if	the	initial	test	fails.	
follows	this	strategy	when
compiling	with	higher	levels	of	optimization,	for	example,	with	command-
line	option	
.	This	method	can	be	expressed	by	the	following	template
for	translating	from	the	general	while	loop	form	to	a	
loop:</p>
<p>This,	in	turn,	can	be	transformed	into	goto	code	as
Using	this	implementation	strategy,	the	compiler	can	often	optimize	the
initial	test,	for	example,	determining	that	the	test	condition	will	always
hold.
As	an	example,	
Figure	
3.21
shows	the	same	C	code	for	a	factorial
function	as	in	
Figure	
3.20
,	but	demonstrates	the	compilation	that
occurs	when	
GCC</p>
<p>is	given	command-line	option	-01.	
Figure	
3.21(c)
shows	the	actual	assembly	code	generated,	while	
Figure	
3.21(b)
renders	this	assembly	code	in	a	more	readable	C	representation.
Referring	to	this	goto	code,	we	see	that	the	loop	will	be	skipped	if	
n
≤	1,
for	the	initial	value	of	
n
.	The	loop	itself	has	the	same	general	structure	as
that	generated	for	the	do-while	version	of	the	function	(
Figure	
3.19
).
One	interesting	feature,	however,	is	that	the	loop	test	(line	9	of	the
assembly	code)	has	been	changed	from	
n
&gt;	1	in	the	original	C	code	to	
n
≠	1.	The	compiler	has	determined	that	the	loop	can	only	be	entered	when</p>
<p>n
&gt;	1,	and	that	decrementing	
n
will	result	in	either	
n
&gt;	1	or	
n
=	1.
Therefore,	the	test	
n
≠	1	will	be	equivalent	to	the	test	
n
≤	1.
Practice	Problem	
3.25	
(solution	page	
335
)
For	C	code	having	the	general	form
GCC
,	run	with	command-line	option	-01,	produces	the	following
code:</p>
<p>(a)	C	code
(b)	Equivalent	goto	version</p>
<p>(c)	Corresponding	assembly-language	code
Figure	
3.21	
C	and	assembly	code	for	
version	of	factorial
using	guarded-do	translation.
The	
function	illustrates	the	operation	of	the
assembly-code	version.</p>
<p>We	can	see	that	the	compiler	used	a	guarded-do	translation,	using
the	
instruction	on	line	3	to	skip	over	the	loop	code	when	the
initial	test	fails.	Fill	in	the	missing	parts	of	the	C	code.	Note	that	the
control	structure	in	the	assembly	
code	does	not	exactly	match
what	would	be	obtained	by	a	direct	translation	of	the	C	code
according	to	our	translation	rules.	In	particular,	it	has	two	different
ret	instructions	(lines	10	and	13).	However,	you	can	fill	out	the
missing	portions	of	the	C	code	in	a	way	that	it	will	have	equivalent
behavior	to	the	assembly	code.
Practice	Problem	
3.26	
(solution	page	
336
)
A	function	
has	the	following	overall	structure:
⋮</p>
<p>The	
GCC</p>
<p>C	compiler	generates	the	following	assembly	code:
Reverse	engineer	the	operation	of	this	code	and	then	do	the
following:
A
.	
Determine	what	loop	translation	method	was	used.
B
.	
Use	the	assembly-code	version	to	fill	in	the	missing	parts	of
the	C	code.
C
.	
Describe	in	English	what	this	function	computes.
For	Loops</p>
<p>The	general	form	of	a	
is	as	follows:
The	C	language	standard	states	(with	one	exception,	highlighted	in
Problem	
3.29
)	that	the	behavior	of	such	a	loop	is	identical	to	the
following	code	using	a	while	loop:
The	program	first	evaluates	the	initialization	expression	
init-expr
.	It	enters
a	loop	where	it	first	evaluates	the	test	condition	
test-expr
,	exiting	if	the
test	fails,	then	executes	the	body	of	the	loop	
body-statement
,	and	finally
evaluates	the	update	expression	
update-expr
.
The	code	generated	by	
GCC</p>
<p>for	a	
then	follows	one	of	our	two
translation	strategies	for	
s,	depending	on	the	optimization
level.	That	is,	the	jump-to-middle	strategy	yields	the	goto	code</p>
<p>while	the	guarded-do	strategy	yields
As	examples,	consider	a	factorial	function	written	with	a	
:</p>
<p>As	shown,	the	natural	way	of	writing	a	factorial	function	with	a	
is
to	multiply	factors	from	2	up	to	
n
,	and	so	this	function	is	quite	different
from	the	code	we	showed	using	either	a	
or	a	
.
We	can	identify	the	different	components	of	the	
in	this	code	as
follows:
Substituting	these	components	into	the	template	we	have	shown	to
transform	a	
into	a	
yields	the	following:</p>
<p>Applying	the	jump-to-middle	transformation	to	the	
then	yields
the	following	version	in	goto	code:</p>
<p>Indeed,	a	close	examination	of	the	assembly	code	produced	by	
GCC</p>
<p>with
command-line	option	
closely	follows	this	template:
Practice	Problem	
3.27	
(solution	page	
336
)
Write	goto	code	for	
based	on	first	transforming	it	to	a
and	then	applying	the	guarded-do	transformation.</p>
<p>We	see	from	this	presentation	that	all	three	forms	of	loops	in	C—
,	and	
—can	be	translated	by	a	simple	strategy,
generating	code	that	contains	one	or	more	conditional	branches.
Conditional	transfer	of	control	provides	the	basic	mechanism	for
translating	loops	into	machine	code.
Practice	Problem	
3.28	
(solution	page	
336
)
A	function	
has	the	following	overall	structure:
⋮
The	
GCC</p>
<p>C	compiler	generates	the	following	assembly	code:</p>
<p>Reverse	engineer	the	operation	of	this	code	and	then	do	the
following:
A
.	
Use	the	assembly-code	version	to	fill	in	the	missing	parts	of
the	C	code.
B
.	
Explain	why	there	is	neither	an	initial	test	before	the	loop
nor	an	initial	jump	to	the	test	portion	of	the	loop.
C
.	
Describe	in	English	what	this	function	computes.
Practice	Problem	
3.29	
(solution	page	
337
)
Executing	a	
statement	in	C	causes	the	program	to	jump
to	the	end	of	the	current	loop	iteration.	The	stated	rule	for
translating	a	
into	a	
needs	some	refinement
when	dealing	with	
statements.	For	example,	consider	the
following	code:</p>
<p>A
.	
What	would	we	get	if	we	naively	applied	our	rule	for
translating	the	
into	a	
?	What	would	be
wrong	with	this	code?
B
.	
How	could	you	replace	the	
statement	with	a	
statement	to	ensure	that	the	
correctly	duplicates
the	behavior	of	the	
?
3.6.8	
Switch	Statements
A	
statement	provides	a	multiway	branching	capability	based	on
the	value	of	an	integer	index.	They	are	particularly	useful	when	dealing
with	tests	where	
there	can	be	a	large	number	of	possible	outcomes.	Not
only	do	they	make	the	C	code	more	readable,	but	they	also	allow	an
efficient	implementation	using	a	data	structure	called	a
jump	table
.A	jump
table	is	an	array	where	entry
i
is	the	address	of	a	code	segment
implementing	the	action	the	program	should	take	when	the	switch	index
equals	
i
.	The	code	performs	an	array	reference	into	the	jump	table	using
the	switch	index	to	determine	the	target	for	a	jump	instruction.	The
advantage	of	using	a	jump	table	over	a	long	sequence	of	if-else
statements	is	that	the	time	taken	to	perform	the	switch	is	independent	of</p>
<p>the	number	of	switch	cases.	G
CC</p>
<p>selects	the	method	of	translating	a
statement	based	on	the	number	of	cases	and	the	sparsity	of	the
case	values.	Jump	tables	are	used	when	there	are	a	number	of	cases
(e.g.,	four	or	more)	and	they	span	a	small	range	of	values.
Figure	
3.22(a)
shows	an	example	of	a	C	
statement.	This
example	has	a	number	of	interesting	features,	including	case	labels	that
do	not	span	a	contiguous	range	(there	are	no	labels	for	cases	101	and
105),	cases	with	multiple	labels	(cases	104	and	106),	and	cases	that	
fall
through
to	other	cases	(case	102)	because	the	code	for	the	case	does
not	end	with	a	
statement.
Figure	
3.23
shows	the	assembly	code	generated	when	compiling
The	behavior	of	this	code	is	shown	in	C	as	the	procedure
in	
Figure	
3.22(b)
.	This	code	makes	use	of	support
provided	by	
GCC</p>
<p>for	jump	tables,	as	an	extension	to	the	C	language.	The
array	
contains	seven	entries,	each	of	which	is	the	address	of	a	block
of	code.	These	locations	are	defined	by	labels	in	the	code	and	indicated
in	the	entries	in	
by	code	pointers,	consisting	of	the	labels	prefixed	by
(Recall	that	the	operator	
creates	a	pointer	for	a	data	value.	In
making	this	extension,	the	authors	of	
G
CC</p>
<p>created	a	new	operator	
to
create	a	pointer	for	a	code	location.)	We	recommend	that	you	study	the	C
procedure	
and	how	it	relates	to	the	assembly-code
version.
Our	original	C	code	has	cases	for	values	100,	102–104,	and	106,	but	the
switch	variable	
can	be	an	arbitrary	integer.	The	compiler	first	shifts	the
range	to	between	0	and	6	by	subtracting	100	from	
,	creating	a	new</p>
<p>program	variable	that	we	call	index	in	our	C	version.	It	further	simplifies
the	branching	possibilities	by	treating	
as	an	
unsigned
value,
making	use	of	the	fact	that	negative	numbers	in	a	two's-complement
representation	map	to	large	positive	numbers	in	an	unsigned
representation.	It	can	therefore	test	whether	
is	outside	of	the	range
0–6	by	testing	whether	it	is	greater	than	6.	In	the	C	and	assembly	code,
there	are	five	distinct	locations	to	jump	to,	based	on	the	value	of	
.
These	are	
(identified	in	the	assembly	code	as	
,	and	
,	where	the	latter	is	the
destination	for	the	default	case.	Each	of	these	labels	identifies	a	block	of
code	implementing	one	of	thecase	branches.	In	both	the	C	and	the
assembly	code,	the	program	compares	index	to	6	and	jumps	to	the	code
for	the	default	case	if	it	is	greater.
The	key	step	in	executing	a	
statement	is	to	access	a	code
location	through	the	jump	table.	This	occurs	in	line	16	in	the	C	code,	with
a	
statement	that	references	the	jump	table	
.	This	
computed	goto
is	supported	by	
GCC</p>
<p>as	an	extension	to	the	C	language.	In	our	assembly-
code	version,	a	similar	operation	occurs	on	line	5,	where	the	
instruction's	operand	is	prefixed	with	`*',	indicating
(a)	Switch	statement</p>
<p>(b)	Translation	into	extended	C</p>
<p>Figure	
3.22	
Example	
statement	and	its	translation	into
extended	C.
The	translation	shows	the	structure	of	
table	
and	how	it	is
accessed.	Such	tables	are	supported	by	
GCC</p>
<p>as	an	extension	to	the	C
language.
an	indirect	jump,	and	the	operand	specifies	a	memory	location	indexed
by	register	
,	which	holds	the	value	of	
.	(We	will	see	in	
Section
3.8
how	array	references	are	translated	into	machine	code.)
Our	C	code	declares	the	jump	table	as	an	array	of	seven	elements,	each
of	which	is	a	pointer	to	a	code	location.	These	elements	span	values	0–6
of</p>
<p>Figure	
3.23	
Assembly	code	for	
statement	example	in	
Figure
3.22
.
,	corresponding	to	values	100–106	of	
.	Observe	that	the	jump
table	handles	duplicate	cases	by	simply	having	the	same	code	label
for	entries	4	and	6,	and	it	handles	missing	cases	by	using	the
label	for	the	default	case	
as	entries	1	and	5.
In	the	assembly	code,	the	jump	table	is	indicated	by	the	following
declarations,	to	which	we	have	added	comments:</p>
<p>These	declarations	state	that	within	the	segment	of	the	object-code	file
called	
(for	&quot;read-only	data&quot;),	there	should	be	a	sequence	of
seven	&quot;quad&quot;	(8-byte)	words,	where	the	value	of	each	word	is	given	by
the	instruction	address	associated	with	the	indicated	assembly-code
labels	(e.g.,	
).	Label	
marks	the	start	of	this	allocation.	The
address	associated	with	this	label	serves	as	the	base	for	the	indirect
jump	(line	5).
The	different	code	blocks	(C	labels	
through	
and	
)
implement	the	different	branches	of	the	
statement.	Most	of	them
simply	compute	a	value	for	
and	then	go	to	the	end	of	the	function.
Similarly,	the	assembly-code	blocks	compute	a	value	for	register	
and	jump	to	the	position	indicated	by	label	
at	the	end	of	the	function.
Only	the	code	for	case	label	102	does	not	follow	this	pattern,	to	account
for	the	way	the	code	for	this	case	falls	through	to	the	block	with	label	103
in	the	original	C	code.	This	is	handled	in	the	assembly-code	block</p>
<p>starting	with	label	
,	by	omitting	the	
instruction	at	the	end	of	the
block,	so	that	the	code	continues	execution	of	the	next	block.	Similarly,
the	C	version	
has	no	
statement	at	the	end	of	the
block	starting	with	label	
Examining	all	of	this	code	requires	careful	study,	but	the	key	point	is	to
see	that	the	use	of	a	jump	table	allows	a	very	efficient	way	to	implement
a	multiway	branch.	In	our	case,	the	program	could	branch	to	five	distinct
locations	with	a	single	jump	table	reference.	Even	if	we	had	a	
statement	with	hundreds	of	cases,	they	could	be	handled	by	a	single
jump	table	access.
Practice	Problem	
3.30	
(solution	page	
338
)
In	the	C	function	that	follows,	we	have	omitted	the	body	of	the
statement.	In	the	C	code,	the	case	labels	did	not	span	a
contiguous	range,	and	some	cases	had	multiple	labels.
⋮</p>
<p>In	compiling	the	function,	
GCC</p>
<p>generates	the	assembly	code	that
follows	for	the	initial	part	of	the	procedure,	with	variable	
in	
It	generates	the	following	code	for	the	jump	table:
Based	on	this	information,	answer	the	following	questions:
A
.	
What	were	the	values	of	the	case	labels	in	the	
statement?</p>
<p>B
.	
What	cases	had	multiple	labels	in	the	C	code?
Practice	Problem	
3.31	
(solution	page	
338
)
For	a	C	function	
with	the	general	structure</p>
<p>GCC</p>
<p>generates	the	assembly	code	and	jump	table	shown	in	
Figure
3.24
.
Fill	in	the	missing	parts	of	the	C	code.	Except	for	the	ordering	of
case	labels	
and	
,	there	is	only	one	way	to	fit	the	different
cases	into	the	template.
(a)	Code</p>
<p>(b)	Jump	table
Figure	
3.24	
Assembly	code	and	jump	table	for	
Problem	
3.31
.</p>
<p>3.7	
Procedures
Procedures	are	a	key	abstraction	in	software.	They	provide	a	way	to
package	code	that	implements	some	functionality	with	a	designated	set
of	arguments	and	an	optional	return	value.	This	function	can	then	be
invoked	from	different	points	in	a	program.	Well-designed	software	uses
procedures	as	an	abstraction	mechanism,	hiding	the	detailed
implementation	of	some	action	while	providing	a	clear	and	concise
interface	definition	of	what	values	will	be	computed	and	what	effects	the
procedure	will	have	on	the	program	state.	Procedures	come	in	many
guises	
in	different	programming	languages—functions,	methods,
subroutines,	handlers,	and	so	on—but	they	all	share	a	general	set	of
features.
There	are	many	different	attributes	that	must	be	handled	when	providing
machine-level	support	for	procedures.	For	discussion	purposes,	suppose
procedure	
calls	procedure	
,	and	
then	executes	and	returns	back	to
.	These	actions	involve	one	or	more	of	the	following	mechanisms:
Passing	control.	
The	program	counter	must	be	set	to	the	starting
address	of	the	code	for	
upon	entry	and	then	set	to	the	instruction	in
following	the	call	to	
upon	return.
Passing	data.	
must	be	able	to	provide	one	or	more	parameters	to
,	and	
must	be	able	to	return	a	value	back	to	
.</p>
<p>Allocating	and	deallocating	memory.	
may	need	to	allocate	space
for	local	variables	when	it	begins	and	then	free	that	storage	before	it
returns.
The	x86-64	implementation	of	procedures	involves	a	combination	of
special	instructions	and	a	set	of	conventions	on	how	to	use	the	machine
resources,	such	as	the	registers	and	the	program	memory.	Great	effort
has	been	made	to	minimize	the	overhead	involved	in	invoking	a
procedure.	As	a	consequence,	it	follows	what	can	be	seen	as	a
minimalist	strategy,	implementing	only	as	much	of	the	above	set	of
mechanisms	as	is	required	for	each	particular	procedure.	In	our
presentation,	we	build	up	the	different	mechanisms	step	by	step,	first
describing	control,	then	data	passing,	and,	finally,	memory	management.
3.7.1	
The	Run-Time	Stack
A	key	feature	of	the	procedure-calling	mechanism	of	C,	and	of	most	other
languages,	is	that	it	can	make	use	of	the	last-in,	first-out	memory
management	discipline	provided	by	a	stack	data	structure.	Using	our
example	of	procedure	
calling	procedure	
,	we	can	see	that	while	
is
executing,	
,	along	with	any	of	the	procedures	in	the	chain	of	calls	up	to
P,	is	temporarily	suspended.	While	
is	running,	only	it	will	need	the
ability	to	allocate	new	storage	for	its	local	variables	or	to	set	up	a	call	to
another	procedure.	On	the	other	hand,	when	
returns,	any	local	storage
it	has	allocated	can	be	freed.	Therefore,	a	program	can	manage	the
storage	required	by	its	procedures	using	a	stack,	where	the	stack	and	the
program	registers	store	the	information	required	for	passing	control	and</p>
<p>data,	and	for	allocating	memory.	As	
calls	
,	control	and	data
information	are	added	to	the	end	of	the	stack.	This	information	gets
deallocated	when	
returns.
As	described	in	
Section	
3.4.4
,	the	x86-64	stack	grows	toward	lower
addresses	and	the	stack	pointer	
points	to	the	top	element	of	the
stack.	Data	can	be	stored	on	and	retrieved	from	the	stack	using	the
and	
instructions.	Space	for	data	with	no	specified	initial	value
can	be	allocated	on	the	stack	by	simply	decrementing	the	stack	pointer
by	an	appropriate	amount.	Similarly,	space	can	be	deallocated	by
incrementing	the	stack	pointer.
When	an	x86-64	procedure	requires	storage	beyond	what	it	can	hold	in
registers,	it	allocates	space	on	the	stack.	This	region	is	referred	to	as	the
procedure's</p>
<p>Figure	
3.25	
General	stack	frame	structure.
The	stack	can	be	used	for	passing	arguments,	for	storing	return
information,	for	saving	registers,	and	for	local	storage.	Portions	may	be
omitted	when	not	needed.
stack	frame
.	
Figure	
3.25
shows	the	overall	structure	of	the	run-time
stack,	including	its	partitioning	into	stack	frames,	in	its	most	general	form.
The	frame	for	the	currently	executing	procedure	is	always	at	the	top	of</p>
<p>the	stack.	When	procedure	
calls	procedure	
,	it	will	push	the	
return
address
onto	the	stack,	indicating	where	within	
the	program	should
resume	execution	once	
returns.	We	consider	the	return	address	to	be
part	of	
's	stack	frame,	since	it	holds	state	relevant	to	
.	The	code	for	
allocates	the	space	required	for	its	stack	frame	by	extending	the	current
stack	boundary.	Within	that	space,	it	can	save	the	values	of	registers,
allocate	
space	for	local	variables,	and	set	up	arguments	for	the
procedures	it	calls.	The	stack	frames	for	most	procedures	are	of	fixed
size,	allocated	at	the	beginning	of	the	procedure.	Some	procedures,
however,	require	variable-size	frames.	This	issue	is	discussed	in	
Section
3.10.5
.	Procedure	
can	pass	up	to	six	integral	values	(i.e.,	pointers
and	integers)	on	the	stack,	but	if	
requires	more	arguments,	these	can
be	stored	by	
within	its	stack	frame	prior	to	the	call.
In	the	interest	of	space	and	time	efficiency,	x86-64	procedures	allocate
only	the	portions	of	stack	frames	they	require.	For	example,	many
procedures	have	six	or	fewer	arguments,	and	so	all	of	their	parameters
can	be	passed	in	registers.	Thus,	parts	of	the	stack	frame	diagrammed	in
Figure	
3.25
may	be	omitted.	Indeed,	many	functions	do	not	even
require	as	tack	frame.	This	occurs	when	all	of	the	local	variables	can	be
held	in	registers	and	the	function	does	not	call	any	other	functions
(sometimes	referred	to	as	a	
leaf	procedure
,	in	reference	to	the	tree
structure	of	procedure	calls).	For	example,	none	of	the	functions	we	have
examined	thus	far	required	stack	frames.
3.7.2	
Control	Transfer</p>
<p>Passing	control	from	function	
to	function	
involves	simply	setting	the
program	counter	(PC)	to	the	starting	address	of	the	code	for	
.	However,
when	it	later	comes	time	for	
to	return,	the	processor	must	have	some
record	of	the	code	location	where	it	should	resume	the	execution	of	
.
This	information	is	recorded	in	x86-64	machines	by	invoking	procedure	
with	the	instruction	call	
.	This	instruction	pushes	an	address	
A
onto	the
stack	and	sets	the	PC	to	the	beginning	of	
.	The	pushed	address	
A
is
referred	to	as	the	
return	address
and	is	computed	as	the	address	of	the
instruction	immediately	following	the	
instruction.	The	counterpart
instruction	
pops	an	address	
A
off	the	stack	and	sets	the	PC	to	
A
.
The	general	forms	of	the	
and	
instructions	are	described	as
follows:
Instruction
Description</p>
<p>Label
Procedure	call
*
Operand
Procedure	call
Return	from	call
(These	instructions	are	referred	to	as	
and	
in	the	disassembly
outputs	generated	by	the	program	
.	The	added	suffix	`
'	simply
emphasizes	that	these	are	x86-64	versions	of	call	and	return	instructions,
not	IA32.	In	x86-64	assembly	code,	both	versions	can	be	used
interchangeably.)
The	
instruction	has	a	target	indicating	the	address	of	the	instruction
where	the	called	procedure	starts.	Like	jumps,	a	call	can	be	either	direct</p>
<p>or	indirect.	In	assembly	code,	the	target	of	a	direct	call	is	given	as	a	label,
while	the	target	of	an	indirect	call	is	given	by	`*'	followed	by	an	operand
specifier	using	one	of	the	formats	described	in	
Figure	
3.3
.
Figure	
3.26	
Illustration	of	
and	
functions.
The	
transfers	control	to	the	start	of	a	function,	while	the
instruction	returns	back	to	the	instruction	following	the	call.
Figure	
3.26
illustrates	the	execution	of	the	
and	
instructions
for	the	
and	
functions	introduced	in	
Section	
3.2.2
.	The
following	are	excerpts	of	the	disassembled	code	for	the	two	functions:</p>
<p>In	this	code,	we	can	see	that	the	
instruction	with	address	
in	
calls	function	
.	This	status	is	shown	in	
Figure
3.26(a)
,	with	the	indicated	values	for	the	stack	pointer	
and	the
program	counter	
.	The	effect	of	the	
is	to	push	the	return
address	
onto	the	stack	and	to	jump	to	the	first	instruction	in
function	
,	at	address	
(3.26(b)).	The	execution	of
multstore	continues	until	it	hits	the	
instruction	at	address
.	This	instruction	pops	the	value	
from	the	stack	and
jumps	to	this	address,	resuming	the	execution	of	
just	after	the	
instruction	(3.26(c)).
As	a	more	detailed	example	of	passing	control	to	and	from	procedures,
Figure	
3.27(a)
shows	the	disassembled	code	for	two	functions,	
and	
,	as	well	as	the	portion	of	code	in	function	
where	
gets
called.	Each	instruction	is	identified	by	labels	
(in	
),	
(in
top),	and	
in	main.	Part	(b)	of	the	figure	shows	a	detailed	trace	of
the	code	execution,	in	which	
calls	
,	causing	top	to	call
.	Function	
returns	97	to	
,	which
(a)	Disassembled	code	for	demonstrating	procedure	calls	and	returns</p>
<p>(b)	Execution	trace	of	example	code
Instruction
State	values	(at	beginning)
Label
PC
Instruction
M1
100
—
—
T1
100
—</p>
<p>T2
95
—
L1
95
—
L2
—
97
T3
—
97
T4
—
194
M2
—
194
—
Figure	
3.27	
Detailed	execution	of	program	involving	procedure	calls
and	returns.
Using	the	stack	to	store	return	addresses	makes	it	possible	to	return	to
the	right	point	in	the	procedures.
then	returns	194	to	
.	The	first	three	columns	describe	the	instruction
being	executed,	including	the	instruction	label,	the	address,	and	the
instruction	type.	The	next	four	columns	show	the	state	of	the	program
before
the	instruction	is	executed,	including	the	contents	of	registers
,	and	
,	as	well	as	the	value	at	the	top	of	the	stack.	The
contents	of	this	table	should	be	studied	carefully,	as	they	
demonstrate	the
important	role	of	the	run-time	stack	in	managing	the	storage	needed	to
support	procedure	calls	and	returns.</p>
<p>Instruction	
of	leaf	sets	
to	97,	the	value	to	be	returned.	Instruction
then	returns.	It	pops	
from	the	stack.	In	setting	the	PC	to	this
popped	value,	control	transfers	back	to	instruction	
of	
.	The
program	has	successfully	completed	the	call	to	
and	returned	to	
.
Instruction	
sets	
to	194,	the	value	to	be	returned	from	
.
Instruction	
then	returns.	It	pops	
from	the	stack,	thereby
setting	the	PC	to	instruction	
of	
.	The	program	has	successfully
completed	the	call	to	
and	returned	to	
.	We	see	that	the	stack
pointer	has	also	been	restored	to	
,	the	value	it	had	before
the	call	to	
.
We	can	see	that	this	simple	mechanism	of	pushing	the	return	address
onto	the	stack	makes	it	possible	for	the	function	to	later	return	to	the
proper	point	in	the	program.	The	standard	call/return	mechanism	of	C
(and	of	most	programming	languages)	conveniently	matches	the	last-in,
first-out	memory	management	discipline	provided	by	a	stack.
Practice	Problem	
3.32	
(solution	page	
339
)
The	disassembled	code	for	two	functions	
and	
is	shown
below,	along	with	the	code	for	a	call	of	
by	function	
:</p>
<p>⋮
Each	of	these	instructions	is	given	a	label,	similar	to	those	in
Figure	
3.27(a)
.	Starting	with	the	calling	of	
by	
,	fill
in	the	following	table	to	trace	instruction	execution	through	to	the
point	where	the	program	returns	back	to	
.
Instruction
State	values	(at	beginning)
Label
PC
Instruction</p>
<p>M1
10
—
—
F1</p>
<hr />
<hr />
<hr />
<hr />
<hr />
<p>F2</p>
<hr />
<hr />
<hr />
<hr />
<hr />
<p>F3</p>
<hr />
<hr />
<hr />
<hr />
<hr />
<p>L1</p>
<hr />
<hr />
<hr />
<hr />
<hr />
<p>L2</p>
<hr />
<hr />
<hr />
<hr />
<hr />
<p>L3</p>
<hr />
<hr />
<hr />
<hr />
<hr />
<p>F4</p>
<hr />
<hr />
<hr />
<hr />
<hr />
<p>M2</p>
<hr />
<hr />
<hr />
<hr />
<hr />
<p>3.7.3	
Data	Transfer
In	addition	to	passing	control	to	a	procedure	when	called,	and	then	back
again	when	the	procedure	returns,	procedure	calls	may	involve	passing
data	as	arguments,	and	returning	from	a	procedure	may	also	involve
returning	a	value.	With	x86-64,	most	of	these	data	passing	to	and	from
procedures	take	place	via	registers.	For	example,	we	have	already	seen
numerous	examples	of	functions	where	arguments	are	passed	in
registers	
,	and	others,	and	where	values	are	returned	in
register	
.	When	procedure	
calls	procedure	
,	the	code	for	
must
first	copy	the	arguments	into	the	proper	registers.	Similarly,	when	</p>
<p>returns	back	to	
,	the	code	for	
can	access	the	returned	value	in
register	
.	In	this	section,	we	explore	these	conventions	in	greater
detail.
With	x86-64,	up	to	six	integral	(i.e.,	integer	and	pointer)	arguments	can
be	passed	via	registers.	The	registers	are	used	in	a	specified	order,	with
the	name	used	for	a	register	depending	on	the	size	of	the	data	type	being
passed.	These	are	shown	in	
Figure	
3.28
.	Arguments	are	allocated	to
these	registers	according	to	their
Operand	size	(bits)
Argument	number
1
2
3
4
5
6
64
32
16
8
Figure	
3.28	
Registers	for	passing	function	arguments.
The	registers	are	used	in	a	specified	order	and	named	according	to	the
argument	sizes.
ordering	in	the	argument	list.	Arguments	smaller	than	64	bits	can	be
accessed	using	the	appropriate	subsection	of	the	64-bit	register.	For
example,	if	the	first	argument	is	32	bits,	it	can	be	accessed	as	
.</p>
<p>When	a	function	has	more	than	six	integral	arguments,	the	other	ones
are	passed	on	the	stack.	Assume	that	procedure	
calls	procedure	
with	
n
integral	arguments,	such	that	
n
&gt;	6.	Then	the	code	for	
must
allocate	a	stack	frame	with	enough	storage	for	arguments	7	through	
n
,	as
illustrated	in	
Figure	
3.25
.	It	copies	arguments	1–6	into	the	appropriate
registers,	and	it	puts	arguments	7	through	
n
onto	the	stack,	with
argument	7	at	the	top	of	the	stack.	When	passing	parameters	on	the
stack,	all	data	sizes	are	rounded	up	to	be	multiples	of	eight.	With	the
arguments	in	place,	the	program	can	then	execute	a	
instruction	to
transfer	control	to	procedure	
.	Procedure	
can	access	its	arguments
via	registers	and	possibly	from	the	stack.	If	
,	in	turn,	calls	some	function
that	has	more	than	six	arguments,	it	can	allocate	space	within	its	stack
frame	for	these,	as	is	illustrated	by	the	area	labeled	&quot;Argument	build
area&quot;	in	
Figure	
3.25
.
As	an	example	of	argument	passing,	consider	the	C	function	
shown
in	
Figure	
3.29(a)
.	This	function	has	eight	arguments,	including
integers	with	different	numbers	of	bytes	(8,	4,	2,	and	1),	as	well	as
different	types	of	pointers,	each	of	which	is	8	bytes.
The	assembly	code	generated	for	
is	shown	in	
Figure	
3.29(b)
.
The	first	six	arguments	are	passed	in	registers.	The	last	two	are	passed
on	the	stack,	as	documented	by	the	diagram	of	
Figure	
3.30
.	This
diagram	shows	the	state	of	the	stack	during	the	execution	of	
.	We
can	see	that	the	return	address	was	pushed	onto	the	stack	as	part	of	the
procedure	call.	The	two	arguments,	therefore,	are	at	positions	8	and	16
relative	to	the	stack	pointer.	Within	the	code,	we	can	see	that	different
versions	of	the	
ADD</p>
<p>instruction	are	used	according	to	the	sizes	of	the
operands:	
for	
for	
for	
,</p>
<p>and	
for	
(char).	Observe	that	the	
instruction	of	line	6	reads	4
bytes	from	memory;	the	following	
instruction	only	makes	use	of	the
low-order	byte.
Practice	Problem	
3.33	
(solution	page	
339
)
A	C	function	
has	four	arguments	
,	and	
.	Each
is	either	a	signed	number	or	a	pointer	to	a	signed	number,	where
the	numbers	have	different	sizes.	The	function	has	the	following
body:
It	compiles	to	the	following	x86-64	code:
(a)	C	code</p>
<p>(b)	Generated	assembly	code</p>
<p>Figure	
3.29	
Example	of	function	with	multiple	arguments	of
different	types.
Arguments	1–6	are	passed	in	registers,	while	arguments	7–8	are
passed	on	the	stack.
Figure	
3.30	
Stack	frame	structure	for	function	
.
Arguments	
4	and	
are	passed	on	the	stack.
Determine	a	valid	ordering	and	types	of	the	four	parameters.
There	are	two	correct	answers.
3.7.4	
Local	Storage	on	the	Stack
Most	of	the	procedure	examples	we	have	seen	so	far	did	not	require	any
local	storage	beyond	what	could	be	held	in	registers.	At	times,	however,</p>
<p>local	data	must	be	stored	in	memory.	Common	cases	of	this	include
these:
There	are	not	enough	registers	to	hold	all	of	the	local	data.
The	address	operator	`
'	is	applied	to	a	local	variable,	and	hence	we
must	be	able	to	generate	an	address	for	it.
Some	of	the	local	variables	are	arrays	or	structures	and	hence	must
be	accessed	by	array	or	structure	references.	We	will	discuss	this
possibility	when	we	describe	how	arrays	and	structures	are	allocated.
Typically,	a	procedure	allocates	space	on	the	stack	frame	by
decrementing	the	stack	pointer.	This	results	in	the	portion	of	the	stack
frame	labeled	&quot;Local	variables&quot;	in	
Figure	
3.25
.
As	an	example	of	the	handling	of	the	address	operator,	consider	the	two
functions	shown	in	
Figure	
3.31(a)
.	The	function	
swaps	the
two	values	designated	by	pointers	
and	
and	also	returns	the	sum	of
the	two	values.	The	function	
creates	pointers	to	local	variables
and	
and	passes	these	to	
.	
Figure	
3.31(b)
shows
how	
uses	a	stack	frame	to	implement	these	local	variables.	The
code	for	
starts	by	decrementing	the	stack	pointer	by	16;	this
effectively	allocates	16	bytes	on	the	stack.	Letting	
S
denote	the	value	of
the	stack	pointer,	we	can	see	that	the	code	computes	
as	
S
+	8
(line	5),	
as	
S
(line	6).	We	can	therefore	infer	that	local	variables
and	
are	stored	within	the	stack	frame	at	offsets	0	and	8
relative	to	the	stack	pointer.	When	the	call	to	
completes,	the
code	for	
then	retrieves	the	two	values	from	the	stack	(lines	8–9),
computes	their	difference,	and	multiplies	this	by	the	value	returned	by</p>
<pre><code>in	register	
(line	10).	Finally,	the	function	deallocates	its
</code></pre>
<p>stack	frame	by	incrementing	the	stack	pointer	by	16	(line	11.)	We	can	see
with	this	example	that	the	run-time	stack	provides	a	simple	mechanism
for	allocating	local	storage	when	it	is	required	and	deallocating	it	when
the	function	completes.
As	a	more	complex	example,	the	function	
,	shown	in	
Figure
3.32
,	illustrates	many	aspects	of	the	x86-64	stack	discipline.	Despite
the	length	of	this	example,	it	is	worth	studying	carefully.	It	shows	a
function	that	must	allocate	storage	on	the	stack	for	local	variables,	as
well	as	to	pass	values	to	the	8-argument	function	
(
Figure	
3.29
).
The	function	creates	a	stack	frame,	diagrammed	in	
Figure	
3.33
.
Looking	at	the	assembly	code	for	
(
Figure	
3.32(b)
),	we	can
see	that	a	large	portion	of	the	code	(lines	2–15)	involves	preparing	to	call
function
(a)	Code	for	swap_add	and	calling	function</p>
<p>(b)	Generated	assembly	code	for	calling	function
Figure	
3.31	
Example	of	procedure	definition	and	call.</p>
<p>The	calling	code	must	allocate	a	stack	frame	due	to	the	presence	of
address	operators.
.	This	includes	setting	up	the	stack	frame	for	the	local	variables	and
function	parameters,	and	for	loading	function	arguments	into	registers.	As
Figure	
3.33
shows,	local	variables	
are	allocated	on	the	stack
and	have	different	sizes.	Expressing	their	locations	as	offsets	relative	to
the	stack	pointer,	they	occupy	bytes	24–31	(
),	20–23	(
),	18–19	(
),
and	17	(
).	Pointers	to	these	locations	are	generated	by	
instructions	(lines	7,	10,	12,	and	14).	Arguments	7	(with	value	4)	and	8	(a
pointer	to	the	location	of	
)	are	stored	on	the	stack	at	offsets	0	and	8
relative	to	the	stack	pointer.
(a)	C	code	for	calling	function
(b)	Generated	assembly	code</p>
<p>Figure	
3.32	
Example	of	code	to	call	function	
,	defined	in	
Figure
3.29
.
This	code	creates	a	stack	frame.
Figure	
3.33	
Stack	frame	for	function	
.
The	stack	frame	contains	local	variables,	as	well	as	two	of	the	arguments
to	pass	to	function	
.
When	procedure	
is	called,	the	program	will	begin	executing	the
code	shown	in	
Figure	
3.29(b)
.	As	shown	in	
Figure	
3.30
,	arguments
7	and	8	are	now	at	offsets	8	and	16	relative	to	the	stack	pointer,	because
the	return	address	was	pushed	onto	the	stack.
When	the	program	returns	to	
,	the	code	retrieves	the	values	of
the	four	local	variables	(lines	17–20)	and	performs	the	final	computations.
It	finishes	by	incrementing	the	stack	pointer	by	32	to	deallocate	the	stack
frame.
3.7.5	
Local	Storage	in	Registers
The	set	of	program	registers	acts	as	a	single	resource	shared	by	all	of
the	procedures.	Although	only	one	procedure	can	be	active	at	a	given</p>
<p>time,	we	must	make	sure	that	when	one	procedure	(the	
caller
)	calls
another	(the	
callee
),	the	callee	does	not	overwrite	some	register	value
that	the	caller	planned	to	use	later.	For	this	reason,	x86-64	adopts	a
uniform	set	of	conventions	for	register	usage	that	must	be	respected	by
all	procedures,	including	those	in	program	libraries.
By	convention,	registers	
,	and	
are	classified	as
callee-saved
registers.	When	procedure	
calls	procedure	
,	
must
preserve
the	values	of	these	registers,	ensuring	that	they	have	the	same
values	when	
returns	to	
as	they	did	when	
was	called.	Procedure	
can	preserve	a	register	value	by	either	not	changing	it	at	all	or	by	pushing
the	original	value	on	the	stack,	altering	it,	and	then	popping	the	old	value
from	the	stack	before	returning.	The	pushing	of	register	values	has	the
effect	of	creating	the	portion	of	the	stack	frame	labeled	&quot;Saved	registers&quot;
in	
Figure	
3.25
.	With	this	convention,	the	code	for	
can	safely	store	a
value	in	a	callee-saved	register	(after	saving	the	previous	value	on	the
stack,	of	course),	call	
,	and	then	use	the	value	in	the	register	without
risk	of	it	having	been	corrupted.
All	other	registers,	except	for	the	stack	pointer	
,	are	classified	as
caller-saved
registers.	This	means	that	they	can	be	modified	by	any
function.	The	name	&quot;caller	saved&quot;	can	be	understood	in	the	context	of	a
procedure	
having	some	local	data	in	such	a	register	and	calling
procedure	
.	Since	
is	free	to	alter	this	register,	it	is	incumbent	upon	
(the	caller)	to	first	save	the	data	before	it	makes	the	call.
As	an	example,	consider	the	function	
shown	in	
Figure	
3.34(a)
.	It
calls	
twice.	During	the	first	call,	it	must	retain	the	value	of	
for	use</p>
<p>later.	Similarly,	during	the	second	call,	it	must	retain	the	value	computed
for	
(
).	In	
Figure	
3.34(b)
,
(a)	Calling	function
(b)	Generated	assembly	code	for	the	calling	function</p>
<p>Figure	
3.34	
Code	demonstrating	use	of	callee-saved	registers.
Value	
must	be	preserved	during	the	first	call,	and	value	
(
)	must	be
preserved	during	the	second.
we	can	see	that	the	code	generated	by	
GCC</p>
<p>uses	two	callee-saved
registers:	
to	hold	
,	and	
to	hold	the	computed	value	of	
(
).
At	the	beginning	of	the	function,	it	saves	the	values	of	these	two	registers
on	the	stack	(lines	2–3).	It	copies	argument	
to	
before	the	first	call
to	
(line	5).	It	copies	the	result	of	this	call	to	
before	the	second	call
to	
(line	8).	At	the	end	of	the	function	(lines	13–14),	it	restores	the
values	of	the	two	callee-saved	registers	by	popping	them	off	the	stack.
Note	how	they	are	popped	in	the	reverse	order	from	how	they	were
pushed,	to	account	for	the	last-in,	first-out	discipline	of	a	stack.
Practice	Problem	
3.34	
(solution	page	
340
)
Consider	a	function	P,	which	generates	local	values,	named	
.
It	then	calls	function	
using	these	generated	values	as
arguments.	G
CC</p>
<p>produces	the	following	code	for	the	first	part	of	
:</p>
<p>A
.	
Identify	which	local	values	get	stored	in	callee-saved
registers.
B
.	
Identify	which	local	values	get	stored	on	the	stack.
C
.	
Explain	why	the	program	could	not	store	all	of	the	local
values	in	callee-saved	registers.</p>
<p>3.7.6	
Recursive	Procedures
The	conventions	we	have	described	for	using	the	registers	and	the	stack
allow	x86-64	procedures	to	call	themselves	recursively.	Each	procedure
call	has	its	own	private	space	on	the	stack,	and	so	the	local	variables	of
the	multiple	outstanding	calls	do	not	interfere	with	one	another.
Furthermore,	the	stack	discipline	naturally	provides	the	proper	policy	for
allocating	local	storage	when	the	procedure	is	called	and	deallocating	it
before	returning.
Figure	
3.35
shows	both	the	C	code	and	the	generated	assembly	code
for	a	recursive	factorial	function.	We	can	see	that	the	assembly	code
uses	register	
to	hold	the	parameter	
,	after	first	saving	the	existing
value	on	the	stack	(line	2)	and	later	restoring	the	value	before	returning
(line	11).	Due	to	the	stack	discipline,	and	the	register-saving	conventions,
we	can	be	assured	that	when	the	recursive	call	to	
returns
(line	9)	that	(1)	the	result	of	the	call	will	be	held	in	register
(a)	C	code</p>
<p>(b)	Generated	assembly	code
Figure	
3.35	
Code	for	recursive	factorial	program.
The	standard	procedure	handling	mechanisms	suffice	for	implementing
recursive	functions.
,	and	(2)	the	value	of	argument	
will	held	in	register	
.
Multiplying	these	two	values	then	computes	the	desired	result.</p>
<p>We	can	see	from	this	example	that	calling	a	function	recursively
proceeds	just	like	any	other	function	call.	Our	stack	discipline	provides	a
mechanism	where	each	invocation	of	a	function	has	its	own	private
storage	for	state	information	(saved	values	of	the	return	location	and
callee-saved	registers).	If	need	be,	it	can	also	provide	storage	for	local
variables.	The	stack	discipline	of	allocation	and	deallocation	naturally
matches	the	call-return	ordering	of	functions.	This	method	of
implementing	function	calls	and	returns	even	works	for	more	complex
patterns,	including	mutual	recursion	(e.g.,	when	procedure	
calls	
,
which	in	turn	calls	
).
Practice	Problem	
3.35	
(solution	page	
340
)
For	a	C	function	having	the	general	structure
GCC</p>
<p>generates	the	following	assembly	code:</p>
<p>A
.	
What	value	does	
store	in	the	callee-saved	register
?
B
.	
Fill	in	the	missing	expressions	in	the	C	code	shown	above.</p>
<p>3.8	
Array	Allocation	and	Access
Arrays	in	C	are	one	means	of	aggregating	scalar	data	into	larger	data
types.	C	uses	a	particularly	simple	implementation	of	arrays,	and	hence
the	translation	into	machine	code	is	fairly	straightforward.	One	unusual
feature	of	C	is	that	we	can	generate	pointers	to	elements	within	arrays
and	perform	arithmetic	with	these	pointers.	These	are	translated	into
address	computations	in	machine	code.
Optimizing	compilers	are	particularly	good	at	simplifying	the	address
computations	used	by	array	indexing.	This	can	make	the	correspondence
between	the	C	code	and	its	translation	into	machine	code	somewhat
difficult	to	decipher.
3.8.1	
Basic	Principles
For	data	type	
T
and	integer	constant	
N
,	consider	a	declaration	of	the	form
Let	us	denote	the	starting	location	as	
x
.	The	declaration	has	two	effects.
First,	it	allocates	a	contiguous	region	of	
L
·	
N
bytes	in	memory,	where	
L
is
the	size	(in	bytes)	of	data	type	
T
.	Second,	it	introduces	an	identifier	</p>
<p>that	can	be	used	as	a	pointer	to	the	beginning	of	the	array.	The	value	of
this	pointer	will	be	
x
.	The	array	elements	can	be	accessed	using	an
integer	index	ranging	between	0	and	
N
–1.	Array	element	
i
will	be	stored
at	address	
x
+	
L
·	
i
.
As	examples,	consider	the	following	declarations:
These	declarations	will	generate	arrays	with	the	following	parameters:
Array
Element	size
Total	size
Start	address
Element	
i
1
12
x
x
+	
i
8
64
x
x
+	8
i
4
24
x
x
+	4
i
8
40
x
x
+	8
i
Array	
consists	of	12	single-byte	(char)	elements.	Array	
consists	of	6
integers,	each	requiring	4	bytes.	
and	
are	both	arrays	of	pointers,	and
hence	the	array	elements	are	8	bytes	each.</p>
<p>The	memory	referencing	instructions	of	x86-64	are	designed	to	simplify
array	access.	For	example,	suppose	
is	an	array	of	values	of	type	int
and	we	wish	to	evaluate	
,	where	the	address	of	
is	stored	in
register	
and	
i
is	stored	in	register	
.	Then	the	instruction
will	perform	the	address	computation	
x
+	4
i
,	read	that	memory	location,
and	copy	the	result	to	register	
.	The	allowed	scaling	factors	of	1,	2,
4,	and	8	cover	the	sizes	of	the	common	primitive	data	types.
Practice	Problem	
3.36	
(solution	page	
341
)
Consider	the	following	declarations:
Fill	in	the	following	table	describing	the	element	size,	the	total	size,
and	the	address	of	element	
i
for	each	of	these	arrays.
Array
Element	size
Total	size
Start	address
Element	
i</p>
<hr />
<hr />
<p>x</p>
<hr />
<hr />
<hr />
<p>x</p>
<hr />
<hr />
<hr />
<p>x</p>
<hr />
<hr />
<hr />
<p>x</p>
<hr />
<hr />
<hr />
<p>x</p>
<hr />
<p>3.8.2	
Pointer	Arithmetic
C	allows	arithmetic	on	pointers,	where	the	computed	value	is	scaled
according	to	the	size	of	the	data	type	referenced	by	the	pointer.	That	is,	if
is	a	pointer	to	data	of	type	
T
,	and	the	value	of	
is	
x
,	then	the
expression	
has	value	
x
+	
L
·	
i
,	where	
L
is	the	size	of	data	type	
T
.
The	unary	operators	<code> '	and	</code>*'	allow	the	generation	and	dereferencing	of
pointers.	That	is,	for	an	expression	
denoting	some	object,	
is	a
pointer	giving	the	address	of	the	object.	For	an	expression	
denoting	an	address,	
gives	the	value	at	that	address.	The
expressions	
and	
are	therefore	equivalent.	The	array
subscripting	operation	can	be	applied	to	both	arrays	and	pointers.	The
array	reference	
is	identical	to	the	expression	
.	It	computes
the	address	of	the	
i
th	array	element	and	then	accesses	this	memory
location.</p>
<p>Expanding	on	our	earlier	example,	suppose	the	starting	address	of
integer	array	
and	integer	index	
i
are	stored	in	registers	
and	
,
respectively.	The	following	are	some	expressions	involving	
.	We	also
show	an	assembly-code	implementation	of	each	expression,	with	the
result	being	stored	in	either	register	
(for	data)	or	register	
(for
pointers).
Expression
Type
Value
Assembly	code
In	these	examples,	we	see	that	operations	that	return	array	values	have
type	
,	and	hence	involve	4-byte	operations	(e.g.,	
)	and	registers
(e.g.,	
).	Those	that	return	pointers	have	type	
,	and	hence
involve	8-byte	operations	(e.g.,	
)	and	registers	(e.g.,	
).	The	final
example	shows	that	one	can	compute	the	difference	of	two	pointers
within	the	same	data	structure,	with	the	result	being	data	having	type
and	value	equal	to	the	difference	of	the	two	addresses	divided	by
the	size	of	the	data	type.</p>
<p>Practice	Problem	
3.37	
(solution	page	
341
)
Suppose	
x
,	the	address	of	short	integer	array	
,	and	long	integer
index	
i
are	stored	in	registers	
and	
,	respectively.	For
each	of	the	following	expressions,	give	its	type,	a	formula	for	its
value,	and	an	assembly-code	implementation.	The	result	should
be	stored	in	register	
if	it	is	a	pointer	and	register	element	
if	it	has	data	type	short.
Expression
Type
Value
Assembly	code</p>
<hr />
<hr />
<hr />
<hr />
<hr />
<hr />
<hr />
<hr />
<hr />
<hr />
<hr />
<hr />
<hr />
<hr />
<hr />
<p>3.8.3	
Nested	Arrays
The	general	principles	of	array	allocation	and	referencing	hold	even	when
we	create	arrays	of	arrays.	For	example,	the	declaration</p>
<p>is	equivalent	to	the	declaration
Data	type	
is	defined	to	be	an	array	of	three	integers.	Array	
contains	five	such	elements,	each	requiring	12	bytes	to	store	the	three
integers.	The	total	array	size	is	then	4	·	5	·	3	=	60	bytes.
Array	
can	also	be	viewed	as	a	two-dimensional	array	with	five	rows
and	three	columns,	referenced	as	
through	
.	The	array
elements	are	ordered	in	memory	in	
row-major
order,	meaning	all
elements	of	row	0,	which	can	be	written	
,	followed	by	all	elements	of
row	1	
,	and	so	on.	This	is	illustrated	in	
Figure	
3.36
.
This	ordering	is	a	consequence	of	our	nested	declaration.	Viewing	
as
an	array	of	five	elements,	each	of	which	is	an	array	of	three	
's,	we
first	have	
,	followed	by	
,	and	so	on.
Toaccess	elements	of	multidimensional	arrays,	the	compiler	generates
code	to	compute	the	off	set	of	the	desired	element	and	then	uses	one	of
the	
MOV</p>
<p>instructions	with	the	start	of	the	array	as	the	base	address	and
the	(possibly	scaled)	offset	as	an	index.	In	general,	for	an	array	declared
as</p>
<p>array	element	
is	at	memory	address
Figure	
3.36	
Elements	of	array	in	row-major	order.
where	
L
is	the	size	of	data	type	
T
in	bytes.	As	an	example,	consider	the
5×3	integer	array	
defined	earlier.	Suppose	
x
,	
,	and	
are	in
registers	
,	and	
,	respectively.	Then	array	element	
can	be	copied	to	register	
by	the	following	code:
&amp;
D
[</p>
<p>i</p>
<p>]
[</p>
<p>j</p>
<h1 id="-3"><a class="header" href="#-3">]</a></h1>
<p>x
D</p>
<ul>
<li></li>
</ul>
<p>L
(
C
⋅
i</p>
<ul>
<li></li>
</ul>
<p>j
)
(3.1)</p>
<p>As	can	be	seen,	this	code	computes	the	element's	address	as	
x
+	12
i
+
4
j
=	
x
+	4(3
i
+	
j
)	using	the	scaling	and	addition	capabilities	of	x86-64
address	arithmetic.
Practice	Problem	
3.38	
(solution	page	
341
)
Consider	the	following	source	code,	where	
M
and	
N
are	constants
declared	with	
:
In	compiling	this	program,	
GCC</p>
<p>generates	the	following	assembly
code:</p>
<p>Use	your	reverse	engineering	skills	to	determine	the	values	of	
M
and	
N
based	on	this	assembly	code.
3.8.4	
Fixed-Size	Arrays
The	C	compiler	is	able	to	make	many	optimizations	for	code	operating	on
multidimensional	arrays	of	fixed	size.	Here	we	demonstrate	some	of	the
optimizations	made	by	
GCC</p>
<p>when	the	optimization	level	is	set	with	the	flag
.	Suppose	we	declare	data	type	
to	be	16	×	16	arrays	of
integers	as	follows:
(This	example	illustrates	a	good	coding	practice.	Whenever	a	program
uses	some	constant	as	an	array	dimension	or	buffer	size,	it	is	best	to
associate	a	name	with	it	via	a	
declaration,	and	then	use	this
name	consistently,	rather	than	the	numeric	value.	That	way,	if	an
occasion	ever	arises	to	change	the	value,	it	can	be	done	by	simply</p>
<p>modifying	the	
declaration.)	The	code	in	
Figure	
3.37(a)
computes	element	
i,	k
of	the	product	of	arrays	
and	
—that	is,	the	inner
product	of	row	
i
from	
and	column	
k
from	
.	This	product	is	given	by	the
formula	
.	G
CC</p>
<p>generates	code	that	we	then	recoded	into	C,
shown	as	function	
in	
Figure	
3.37(b)
.	This	code
contains	a	number	of	clever	optimizations.	It	removes	the	integer	index	
and	converts	all	array	references	to	pointer	dereferences.	This	involves
(1)	generating	a	pointer,	which	we	have	named	
,	that	points	to
successive	elements	in	row	
i
of	
,	(2)	generating	a	pointer,	which	we
have	named	
,	that	points	to	successive	elements	in	column	
k
of	
,
and	(3)	generating	a	pointer,	which	we	have	named	Bend,	that	equals	the
value	
will	have	when	it	is	time	to	terminate	the	loop.	The	initial	value
for	
is	the	address	of	the	first	element	of	row	
i
of	
,	given	by	the	C
expression	
.	The	initial	value	for	
is	the	address	of	the	first
element	of	column	
k
of	
,	given	by	the	C	expression	
.	The	value
for	
is	the	index	of	what	would	be	the	(
n
+	1)st	element	in	column	
j
of
,	given	by	the	C	expression	
.
(a)	Original	C	code
∑
0
≤
j
&lt;
N
a
i
,
j
⋅
b
j
,
k</p>
<p>(b)	Optimized	C	code</p>
<p>Figure	
3.37	
Original	and	optimized	code	to	compute	element	
i,	k
of
matrix	product	for	fixed-length	arrays.
The	compiler	performs	these	optimizations	automatically.
The	following	is	the	actual	assembly	code	generated	by	
GCC</p>
<p>for	function
.	We	see	that	four	registers	are	used	as	follows:	
holds
result,	
holds	
holds	
,	and	
holds	
.</p>
<p>Practice	Problem	
3.39	
(solution	page	
342
)
Use	
Equation	
3.1
to	explain	how	the	computations	of	the	initial
values	for	
,	
,	and	Bend	in	the	C	code	of	
Figure	
3.37(b)
(lines	3–5)	correctly	describe	their	computations	in	the	assembly
code	generated	for	
(lines	3–5).
Practice	Problem	
3.40	
(solution	page	
342
)
The	following	C	code	sets	the	diagonal	elements	of	one	of	our
fixed-size	arrays	to	
:
When	compiled	with	optimization	level	
generates	the
following	assembly	code:</p>
<p>Create	a	C	code	program	
that	uses
optimizations	similar	to	those	in	the	assembly	code,	in	the	same
style	as	the	code	in	
Figure	
3.37(b)
.	Use	expressions	involving
the	parameter	
N
rather	than	integer	constants,	so	that	your	code
will	work	correctly	if	
N
is	redefined.
3.8.5	
Variable-Size	Arrays
Historically,	C	only	supported	multidimensional	arrays	where	the	sizes
(with	the	possible	exception	of	the	first	dimension)	could	be	determined
at	compile	time.	
Programmers	requiring	variable-size	arrays	had	to
allocate	storage	for	these	arrays	using	functions	such	as	
or
,	and	they	had	to	explicitly	encode	the	mapping	of
multidimensional	arrays	into	single-dimension	ones	via	row-major</p>
<p>indexing,	as	expressed	in	
Equation	
3.1
.	ISO	C99	introduced	the
capability	of	having	array	dimension	expressions	that	are	computed	as
the	array	is	being	allocated.
In	the	C	version	of	variable-size	arrays,	we	can	declare	an	array
either	as	a	local	variable	or	as	an	argument	to	a	function,	and	then	the
dimensions	of	the	array	are	determined	by	evaluating	the	expressions
expr1
and	
expr2
at	the	time	the	declaration	is	encountered.	So,	for
example,	we	can	write	a	function	to	access	element	
i,	j
of	an	
n
×	
n
array
as	follows:
The	parameter	
must	precede	the	parameter	
,	so	that	the
function	can	compute	the	array	dimensions	as	the	parameter	is
encountered.
G
CC</p>
<p>generates	code	for	this	referencing	function	as</p>
<p>As	the	annotations	show,	this	code	computes	the	address	of	element	
i,	j
as	
x
+	4(
n
·	
i
)	+	4
j
=	
x
+	4(
n
·	
i
+	
j
).	The	address	computation	is	similar
to	that	of	the	fixed-size	array	(
Section	
3.8.3
),	except	that	(1)	the
register	usage	changes	due	to	added	parameter	
,	and	(2)	a	multiply
instruction	is	used	(line	2)	to	compute	
n
·	
i
,	rather	than	an	
instruction
to	compute	3
i
.	We	see	therefore	that	referencing	variable-size	arrays
requires	only	a	slight	generalization	over	fixed-size	ones.	The	dynamic
version	must	use	a	multiplication	instruction	to	scale	
i
by	
n
,	rather	than	a
series	of	shifts	and	adds.	In	some	processors,	this	multiplication	can
incur	a	significant	performance	penalty,	but	it	is	unavoidable	in	this	case.
When	variable-size	arrays	are	referenced	within	a	loop,	the	compiler	can
often	optimize	the	index	computations	by	exploiting	the	regularity	of	the
access	patterns.	For	example,	
Figure	
3.38(a)
shows	C	code	to
compute	element	
i
,	
k
of	the	product	of	two	
n
×	
n
arrays	
and	
.	G
CC
generates	assembly	code,	which	we	have	recast	into	C	(
Figure</p>
<p>3.38(b)
).	This	code	follows	a	different	style	from	the	optimized	code	for
the	fixed-size	array	(
Figure	
3.37
),	but	that	is	more	an	artifact	of	the
choices	made	by	the	compiler,	rather	than	a	fundamental	requirement	for
the	two	different	functions.	The	code	of	
Figure	
3.38(b)
retains	loop
variable	
,	both	to	detect	when
(a)	Original	C	code
(b)	Optimized	C	code</p>
<p>Figure	
3.38	
Original	and	optimized	code	to	compute	element	
i,	k
of
matrix	product	for	variable-size	arrays.
The	compiler	performs	these	optimizations	automatically.
the	loop	has	terminated	and	to	index	into	an	array	consisting	of	the
elements	of	row	
i
of	
.
The	following	is	the	assembly	code	for	the	loop	of	
:</p>
<p>We	see	that	the	program	makes	use	of	both	a	scaled	value	4
n
(register
)	for	incrementing	
as	well	as	the	value	of	
n
(register	
)	to
check	the	loop	
bounds.	The	need	for	two	values	does	not	show	upin	the
C	code,	due	to	the	scaling	of	pointer	arithmetic.
We	have	seen	that,	with	optimizations	enabled,	
GCC</p>
<p>is	able	to	recognize
patterns	that	arise	when	a	program	steps	through	the	elements	of	a
multidimensional	array.	It	can	then	generate	code	that	avoids	the
multiplication	that	would	result	from	a	direct	application	of	
Equation
3.1
.	Whether	it	generates	the	pointer-based	code	of	
Figure	
3.37(b)
or	the	array-based	code	of	
Figure	
3.38(b)
,	these	optimizations	will
significantly	improve	program	performance.</p>
<p>3.9	
Heterogeneous	Data	Structures
C	provides	two	mechanisms	for	creating	data	types	by	combining	objects
of	different	types:	
structures
,	declared	using	the	keyword	
,
aggregate	multiple	objects	into	a	single	unit;	
unions
,	declared	using	the
keyword	
,	allow	an	object	to	be	referenced	using	several	different
types.
3.9.1	
Structures
The	C	
declaration	creates	a	data	type	that	groups	objects	of
possibly	different	types	into	a	single	object.	The	different	components	of
a	structure	are	referenced	by	names.	The	implementation	of	structures	is
similar	to	that	of	arrays	in	that	all	of	the	components	of	a	structure	are
stored	in	a	contiguous	region	of	memory	and	a	pointer	to	a	structure	is
the	address	of	its	first	byte.	The	compiler	maintains	information	about
each	structure	type	indicating	the	byte	offset	of	each	field.	It	generates
references	to	structure	elements	using	these	offsets	as	displacements	in
memory	referencing	instructions.
As	an	example,	consider	the	following	structure	declaration:</p>
<p>This	structure	contains	four	fields:	two	4-byte	values	of	type	
,	a	two-
element	array	of	type	
,	and	an	8-byte	integer	pointer,	giving	a	total	of
24	bytes:
Observe	that	array	a	is	embedded	within	the	structure.	The	numbers
along	the	top	of	the	diagram	give	the	byte	offsets	of	the	fields	from	the
beginning	of	the	structure.
To	access	the	fields	of	a	structure,	the	compiler	generates	code	that	adds
the	appropriate	offset	to	the	address	of	the	structure.	For	example,
suppose	variable	
New	to	C?	
Representing	an	object	as	a
The	
data	type	constructor	is	the	closest	thing	C	provides	to
the	objects	of	C++	and	Java.	It	allows	the	programmer	to	keep
information	about	some	entity	in	a	single	data	structure	and	to
reference	that	information	with	names.</p>
<p>For	example,	a	graphics	program	might	represent	a	rectangle	as	a
structure:
We	can	declare	a	variable	
of	type	
and	set	its	field
values	as	follows:
where	the	expression	
selects	field	
of	structure	
.</p>
<p>Alternatively,	we	can	both	declare	the	variable	and	initialize	its
fields	with	a	single	statement:
It	is	common	to	pass	pointers	to	structures	from	one	place	to
another	rather	than	copying	them.	For	example,	the	following
function	computes	the	area	of	a	rectangle,	where	a	pointer	to	the
rectangle	
is	passed	to	the	function:
The	expression	
.width	dereferences	the	pointer	and	selects
the	width	field	of	the	resulting	structure.	Parentheses	are	required,
because	the	compiler	would	interpret	the	expression	
as
,	which	is	not	valid.	This	combination	of	dereferencing
and	field	selection	is	so	common	that	C	provides	an	alternative
notation	using	-&gt;.	That	is,	
is	equivalent	to	the
expression	
For	example,	we	can	write	a	function	that
rotates	a	rectangle	counterclockwise	by	90	degrees	as</p>
<p>The	objects	of	C++	and	Java	are	more	elaborate	than	structures
in	C,	in	that	they	also	associate	a	set	of	
methods
with	an	object
that	can	be	invoked	to	perform	computation.	In	C,	we	would
simply	write	these	as	ordinary	functions,	such	as	the	functions
and	
shown	previously.
of	type	
is	in	register	
.	Then	the	following	code	copies
element	
to	element	
:
Since	the	offset	of	field	
is	0,	the	address	of	this	field	is	simply	the	value
of	
.	To	store	into	field	
,	the	code	adds	offset	4	to	the	address	of	
.
To	generate	a	pointer	to	an	object	within	a	structure,	we	can	simply	add
the	field's	offset	to	the	structure	address.	For	example,	we	can	generate
the	pointer	
by	adding	offset	8	+	4	·	1	=	12.	For	pointer	
in</p>
<p>register	
and	long	integer	variable	
in	register	
,	we	can
generate	the	pointer	value	
with	the	single	instruction
As	a	final	example,	the	following	code	implements	the	statement
starting	with	
in	register	
As	these	examples	show,	the	selection	of	the	different	fields	of	a
structure	is	handled	completely	at	compile	time.	The	machine	code</p>
<p>contains	no	information	about	the	field	declarations	or	the	names	of	the
fields.
Practice	Problem	
3.41	
(solution	page	
343
)
Consider	the	following	structure	declaration:
This	declaration	illustrates	that	one	structure	can	be	embedded
within	another,	just	as	arrays	can	be	embedded	within	structures
and	arrays	can	be	embedded	within	arrays.
The	following	procedure	(with	some	expressions	omitted)	operates
on	this	structure:</p>
<p>A
.	
What	are	the	offsets	(in	bytes)	of	the	following	fields?
B
.	
How	many	total	bytes	does	the	structure	require?
C
.	
The	compiler	generates	the	following	assembly	code	for
On	the	basis	of	this	information,	fill	in	the	missing
expressions	in	the	code	for	
Practice	Problem	
3.42	
(solution	page	
343
)</p>
<p>The	following	code	shows	the	declaration	of	a	structure	of	type
and	the	prototype	for	a	function	
:
When	the	code	for	
is	compiled,	
GCC</p>
<p>generates	the	following
assembly	code:
A
.	
Use	your	reverse	engineering	skills	to	write	C	code	for	
.</p>
<p>B
.	
Describe	the	data	structure	that	this	structure	implements
and	the	operation	performed	by	
.
3.9.2	
Unions
Unions	provide	a	way	to	circumvent	the	type	system	of	C,	allowing	a
single	object	to	be	referenced	according	to	multiple	types.	The	syntax	of
a	union	declaration	is	identical	to	that	for	structures,	but	its	semantics	are
very	different.	Rather	than	having	the	different	fields	reference	different
blocks	of	memory,	they	all	reference	the	same	block.
Consider	the	following	declarations:</p>
<p>When	compiled	on	an	x86-64	Linux	machine,	the	offsets	of	the	fields,	as
well	as	the	total	size	of	data	types	
and	
,	are	as	shown	in	the
following	table:
Type
Size
0
4
16
24
0
0
0
8
(We	will	see	shortly	why	
has	offset	4	in	
rather	than	1,	and	why	
has	offset	16,	rather	than	9	or	12.)	For	pointer	
of	type	union	
*,
references	
,	and	
would	all	reference	the	beginning	of
the	data	structure.	Observe	also	that	the	overall	size	of	a	union	equals
the	maximum	size	of	any	of	its	fields.
Unions	can	be	useful	in	several	contexts.	However,	they	can	also	lead	to
nasty	bugs,	since	they	bypass	the	safety	provided	by	the	C	type	system.
One	application	is	when	we	know	in	advance	that	the	use	of	two	different
fields	in	a	data	structure	will	be	mutually	exclusive.	Then,	declaring	these
two	fields	as	part	of	a	union	rather	than	a	structure	will	reduce	the	total
space	allocated.
For	example,	suppose	we	want	to	implement	a	binary	tree	data	structure
where	each	leaf	node	has	two	
data	values	and	each	internal	node
has	pointers	to	two	children	but	no	data.	If	we	declare	this	as</p>
<p>then	every	node	requires	32	bytes,	with	half	the	bytes	wasted	for	each
type	of	node.	On	the	other	hand,	if	we	declare	a	node	as
then	every	node	will	require	just	16	bytes.	If	
is	a	pointer	to	a	node	of
type	union	
,	we	would	reference	the	data	of	a	leaf	node	as	
and	
,	and	the	children	of	an	internal	node	as	
and	
With	this	encoding,	however,	there	is	no	way	to	determine	whether	a
given	node	is	a	leaf	or	an	internal	node.	A	common	method	is	to
introduce	an	enumerated	type	defining	the	different	possible	choices	for
the	union,	and	then	create	a	structure	containing	a	tag	field	and	the
union:</p>
<p>This	structure	requires	a	total	of	24	bytes:	4	for	
,	and	either	8	each
for	
and	
or	16	for	
As
we	will	discuss	shortly,	an	additional	4	bytes	of	padding	is	required
between	the	field	for	type	and	the	union	elements,	bringing	the	total
structure	size	to	4	+	4	+	16	=	24.	In	this	case,	the	savings	gain	of	using	a
union	is	small	relative	to	the	awkwardness	of	the	resulting	code.	For	data
structures	with	more	fields,	the	savings	can	be	more	compelling.
Unions	can	also	be	used	to	access	the	bit	patterns	of	different	data	types.
For	example,	suppose	we	use	a	simple	cast	to	convert	a	value	d	of	type
to	a	value	
of	type	unsigned	
:</p>
<p>Value	
will	be	an	integer	representation	of	
.	Except	for	the	case	where
is	0.0,	the	bit	representation	of	
will	be	very	different	from	that	of	
.
Now	consider	the	following	code	to	generate	a	value	of	type	
long	from	a	
:
In	this	code,	we	store	the	argument	in	the	union	using	one	data	type	and
access	it	using	another.	The	result	will	be	that	
will	have	the	same	bit
representation	as	
,	including	fields	for	the	sign	bit,	the	exponent,	and
the	significand,	as	described	in	
Section	
3.11
.	The	numeric	value	of	
will	bear	no	relation	to	that	of	
,	except	for	the	case	when	
is	0.0.
When	using	unions	to	combine	data	types	of	different	sizes,	byte-
ordering	issues	can	become	important.	For	example,	suppose	we	write	a
procedure	that	will	create	an	8-byte	
using	the	bit	patterns	given	by
two	4-byte	
values:</p>
<p>On	a	little-endian	machine,	such	as	an	x86-64	processor,	argument
will	become	the	low-order	4	bytes	of	
,	while	
will	become
the	high-order	4	bytes.	On	a	big-endian	machine,	the	role	of	the	two
arguments	will	be	reversed.
Practice	Problem	
3.43	
(solution	page	
344
)
Suppose	you	are	given	the	job	of	checking	that	a	C	compiler
generates	the	proper	code	for	structure	and	union	access.	You
write	the	following	structure	declaration:</p>
<p>You	write	a	series	of	functions	of	the	form
with	different	access	expressions	
expr
and	with	destination	data
type	
type
set	according	to	type	associated	with	
expr
.	You	then
examine	the	code	generated	when	compiling	the	functions	to	see
if	they	match	your	expectations.
Suppose	in	these	functions	that	
and	
are	loaded	into
registers	
and	
,	respectively.	Fill	in	the	following	table
with	data	type	
type
and	sequences	of	one	to	three	instructions	to
compute	the	expression	and	store	the	result	at	
.
expr
type
Code</p>
<hr />
<hr />
<hr />
<hr />
<hr />
<hr />
<hr />
<hr />
<hr />
<hr />
<hr />
<hr />
<hr />
<hr />
<hr />
<hr />
<hr />
<hr />
<hr />
<hr />
<p>3.9.3	
Data	Alignment
Many	computer	systems	place	restrictions	on	the	allowable	addresses	for
the	primitive	data	types,	requiring	that	the	address	for	some	objects	must
be	a	multiple	of	some	value	
K
(typically	2,	4,	or	8).	Such	
alignment
restrictions
simplify	the	design	of	the	hardware	forming	the	interface
between	the	processor	and	the	memory	system.	For	example,	suppose	a
processor	always	fetches	8	bytes	from	memory	with	an	address	that
must	be	a	multiple	of	8.	If	we	can	guarantee	that	any	double	will	be
aligned	to	have	its	address	be	a	multiple	of	8,	then	the	value	can	be	read
or	written	with	a	single	memory	operation.	Otherwise,	we	may	need	to</p>
<p>perform	two	memory	accesses,	since	the	object	might	be	split	across	two
8-byte	memory	blocks.
The	x86-64	hardware	will	work	correctly	regardless	of	the	alignment	of
data.	However,	Intel	recommends	that	data	be	aligned	to	improve
memory	system	performance.	Their	alignment	rule	is	based	on	the
principle	that	any	primitive	object	of	
K
bytes	must	have	an	address	that	is
a	multiple	of	
K
.	We	can	see	that	this	rule	leads	to	the	following
alignments:
K
Types
1
char
2
short
4
int,	float
8
long,	double,	char	*
Alignment	is	enforced	by	making	sure	that	every	data	type	is	organized
and	allocated	in	such	a	way	that	every	object	within	the	type	satisfies	its
alignment	restrictions.	The	compiler	places	directives	in	the	assembly
code	indicating	the	desired	alignment	for	global	data.	For	example,	the
assembly-code	declaration	of	the	jump	table	on	page	235	contains	the
following	directive	on	line	2:</p>
<p>This	ensures	that	the	data	following	it	(in	this	case	the	start	of	the	jump
table)	will	start	with	an	address	that	is	a	multiple	of	8.	Since	each	table
entry	is	8	bytes	long,	the	successive	elements	will	obey	the	8-byte
alignment	restriction.
For	code	involving	structures,	the	compiler	may	need	to	insert	gaps	in	the
field	allocation	to	ensure	that	each	structure	element	satisfies	its
alignment	requirement.	The	structure	will	then	have	some	required
alignment	for	its	starting	address.
For	example,	consider	the	structure	declaration
Suppose	the	compiler	used	the	minimal	9-byte	allocation,	diagrammed	as
follows:
Then	it	would	be	impossible	to	satisfy	the	4-byte	alignment	requirement
for	both	fields	
(offset	0)	and	
(offset	5).	Instead,	the	compiler	inserts	a
3-byte	gap	(shown	here	as	shaded	in	blue)	between	fields	
and	
:</p>
<p>As	a	result,	
has	offset	8,	and	the	overall	structure	size	is	12	bytes.
Furthermore,	the	compiler	must	ensure	that	any	pointer	
of	type	
satisfies	a	4-byte	alignment.	Using	our	earlier	notation,	let	pointer	
have	value	
x
.	Then	
x
must	be	a	multiple	of	4.	This	guarantees	that
both	
(address	
x
)	and	
(address	
x
+	8)	will	satisfy	their	4-byte
alignment	requirements.
In	addition,	the	compiler	may	need	to	add	padding	to	the	end	of	the
structure	so	that	each	element	in	an	array	of	structures	will	satisfy	its
alignment	requirement.	For	example,	consider	the	following	structure
declaration:
If	we	pack	this	structure	into	9	bytes,	we	can	still	satisfy	the	alignment
requirements	for	fields	
and	
by	making	sure	that	the	starting	address
of	the	structure	satisfies	a	4-byte	alignment	requirement.	Consider,
however,	the	following	declaration:</p>
<p>With	the	9-byte	allocation,	it	is	not	possible	to	satisfy	the	alignment
requirement	for	each	element	of	
,	because	these	elements	will	have
addresses	
x
,	
x
+	9,	
x
+	18,	and	
x
+	27.	Instead,	the	compiler
allocates	12	bytes	for	structure	
,	with	the	final	3	bytes	being	wasted
space:
That	way,	the	elements	of	
will	have	addresses	
x
,	
x
+	12,	
x
+	24,
and	
x
+	36.	As	long	as	
x
is	a	multiple	of	4,	all	of	the	alignment
restrictions	will	be	satisfied.
Practice	Problem	
3.44	
(solution	page	
345
)
For	each	of	the	following	structure	declarations,	determine	the
offset	of	each	field,	the	total	size	of	the	structure,	and	its	alignment
requirement	for	x86-64:
A
.	
B
.	
C
.	
D
.	
E
.	
Practice	Problem	
3.45	
(solution	page	
345
)
Answer	the	following	for	the	structure	declaration</p>
<p>Aside	
A	case	of	mandatory
alignment
For	most	x86-64	instructions,	keeping	data	aligned
improves	efficiency,	but	it	does	not	affect	program	behavior.
On	the	other	hand,	some	models	of	Intel	and	AMD
processors	will	not	work	correctly	with	unaligned	data	for
some	of	the	SSE	instructions	implementing	multimedia
operations.	These	instructions	operate	on	16-byte	blocks	of
data,	and	the	instructions	that	transfer	data	between	the
SSE	unit	and	memory	require	the	memory	addresses	to	be
multiples	of	16.	Any	attempt	to	access	memory	with	an
address	that	does	not	satisfy	this	alignment	will	lead	to	an
exception
(see	
Section	
8.1
),	with	the	default	behavior	for
the	program	to	terminate.
As	a	result,	any	compiler	and	run-time	system	for	an	x86-
64	processor	must	ensure	that	any	memory	allocated	to
hold	a	data	structure	that	may	be	read	from	or	stored	into
an	SSE	register	must	satisfy	a	16-byte	alignment.	This
requirement	has	the	following	two	consequences:</p>
<p>The	starting	address	for	any	block	generated	by	a
memory	allocation	function	(
,	or
)	must	be	a	multiple	of	16.
The	stack	frame	for	most	functions	must	be	aligned	on	a
16-byte	boundary.	(This	requirement	has	a	number	of
exceptions.)
More	recent	versions	of	x86-64	processors	implement	the
AVX	multimedia	instructions.	In	addition	to	providing	a
superset	of	the	SSE	instructions,	processors	supporting
AVX	also	do	not	have	a	mandatory	alignment	requirement.
A
.	
What	are	the	byte	offsets	of	all	the	fields	in	the	structure?
B
.	
What	is	the	total	size	of	the	structure?
C
.	
Rearrange	the	fields	of	the	structure	to	minimize	wasted
space,	and	then	show	the	byte	offsets	and	total	size	for	the
rearranged	structure.</p>
<p>3.10	
Combining	Control	and	Data	in
Machine-Level	Programs
So	far,	we	have	looked	separately	at	how	machine-level	code	implements
the	control	aspects	of	a	program	and	how	it	implements	different	data
structures.	In	this	section,	we	look	at	ways	in	which	data	and	control
interact	with	each	other.	We	start	by	taking	a	deep	look	into	pointers,	one
of	the	most	important	concepts	in	the	C	programming	language,	but	one
for	which	many	programmers	only	have	a	shallow	understanding.	We
review	the	use	of	the	symbolic	debugger	
GDB
for	examining	the	detailed
operation	of	machine-level	programs.	Next,	we	see	how	understanding
machine-level	programs	enables	us	to	study	buffer	overflow,	an	important
security	vulnerability	in	many	real-world	systems.	Finally,	we	examine
how	machine-level	programs	implement	cases	where	the	amount	of
stack	storage	required	by	a	function	can	vary	from	one	execution	to
another.
3.10.1	
Understanding	Pointers
Pointers	are	a	central	feature	of	the	C	programming	language.	They
serve	as	a	uniform	way	to	generate	references	to	elements	within
different	data	structures.	Pointers	are	a	source	of	confusion	for	novice
programmers,	but	the	underlying	concepts	are	fairly	simple.	Here	we</p>
<p>highlight	some	key	principles	of	pointers	and	their	mapping	into	machine
code.
Every	pointer	has	an	associated	type.	
This	type	indicates	what	kind
of	object	the	pointer	points	to.	Using	the	following	pointer	declarations
as	illustrations
variable	
is	a	pointer	to	an	object	of	type	
,	while	
is	a	pointer
to	an	object	that	itself	is	a	pointer	to	an	object	of	type	
.	In	general,
if	the	object	has	type	
T
,	then	the	pointer	has	type	*
T
.	The	special	</p>
<ul>
<li>type	represents	a	generic	pointer.	For	example,	the	
function
returns	a	generic	pointer,	which	is	converted	to	a	typed	pointer	via
either	an	explicit	cast	or	by	the	implicit	casting	of	the	assignment
operation.	Pointer	types	are	not	part	of	machine	code;	they	are	an
abstraction	provided	by	C	to	help	programmers	avoid	addressing
errors.
Every	pointer	has	a	value.	
This	value	is	an	address	of	some	object
of	the	designated	type.	The	special	
value	indicates	that	the
pointer	does	not	point	anywhere.
Pointers	are	created	with	the	<code>&amp;'	operator.	 This	operator	can	be applied	to	any	C	expression	that	is	categorized	as	an	 lvalue ,	meaning an	expression	that	can	appear	on	the	left	side	of	an	assignment. Examples	include	variables	and	the	elements	of	structures,	unions, and	arrays.	We	have	seen	that	the	machine-code	realization	of	the	</code>&amp;'
operator	often	uses	the	
instruction	to	compute	the	expression</li>
</ul>
<p>value,	since	this	instruction	is	designed	to	compute	the	address	of	a
memory	reference.
Pointers	are	dereferenced	with	the	`*'	operator.	
The	result	is	a
value	having	the	type	associated	with	the	pointer.	Dereferencing	is
implemented	by	a	memory	reference,	either	storing	to	or	retrieving
from	the	specified	address.
Arrays	and	pointers	are	closely	related.	
The	name	of	an	array
canbe	referenced	(but	not	updated)	as	if	it	were	a	pointer	variable.
Array	referencing	(e.g.,	
)	has	the	exact	same	effect	as	pointer
arithmetic	and	dereferencing	(e.g.,	
).	Both	array	referencing
and	pointer	arithmetic	require	scaling	the	offsets	by	the	object	size.
When	we	write	an	expression	
for	pointer	
with	value	
,	the
resulting	address	is	computed	as	
+	
L
·	
i
,	where	
L
is	the	size	of	the
data	type	associated	with	
.
Casting	from	one	type	of	pointer	to	another	changes	its	type	but
not	its	value.	
One	effect	of	casting	is	to	change	any	scaling	of	pointer
arithmetic.	So,	for	example,	if	
is	a	pointer	of	type	
*	having
value	
,	then	the	expression	(
computes	
+	28,	while
computes	
.	(Recall	that	casting	has	higher
precedence	than	addition.)
Pointers	can	also	point	to	functions.	
This	provides	a	powerful
capability	for	storing	and	passing	references	to	code,	which	can	be
invoked	in	some	other	part	of	the	program.	For	example,	if	we	have	a
function	defined	by	the	prototype</p>
<p>then	we	can	declare	and	assign	a	pointer	
to	this	function	by	the
following	code	sequence:
We	can	then	invoke	the	function	using	this	pointer:
The	value	of	a	function	pointer	is	the	address	of	the	first	instruction	in
the	machine-code	representation	of	the	function.
New	to	C?	
Function	pointers
The	syntax	for	declaring	function	pointers	is	especially	difficult	for
novice	programmers	to	understand.	For	a	declaration	such	as
it	helps	to	read	it	starting	from	the	inside	(starting	with	`
')	and
working	outward.	Thus,	we	see	that	
is	a	pointer,	as	indicated	by
(
).	It	is	a	pointer	to	a	function	that	has	a	single	
*	as	an
argument,	as	indicated	by	
.	Finally,	we	see	that	it	is	a</p>
<p>pointer	to	a	function	that	takes	an	
*	as	an	argument	and
returns	
.
The	parentheses	around	*
are	required,	because	otherwise	the
declaration
would	be	read	as
That	is,	it	would	be	interpreted	as	a	function	prototype,	declaring	a
function	
that	has	an	
*	as	its	argument	and	returns	an	
*.
Kernighan	and	Ritchie	
[61,
Sect.	5.12]	present	a	helpful	tutorial	on
reading	C	declarations.
3.10.2	
Life	in	the	Real	World:	Using
the	
GDB</p>
<p>Debugger
The	GNU	debugger	
GDB</p>
<p>provides	a	number	of	useful	features	to	support
the	run-time	evaluation	and	analysis	of	machine-level	programs.	With	the
examples	and	exercises	in	this	book,	we	attempt	to	infer	the	behavior	of</p>
<p>a	program	by	just	looking	at	the	code.	Using	
GDB
,	it	becomes	possible	to
study	the	behavior	by	watching	the	program	in	action	while	having
considerable	control	over	its	execution.
Figure	
3.39
shows	examples	of	some	
GDB</p>
<p>commands	that	help	when
working	with	machine-level	x86-64	programs.	It	is	very	helpful	to	first	run
OBJDUMP</p>
<p>to	get	a	disassembled	version	of	the	program.	Our	examples	are
based	on	running	
GDB</p>
<p>on	the	file	
,	described	and	disassembled	on
page	175.	We	start	
GDB</p>
<p>with	the	following	command	line:
The	general	scheme	is	to	set	breakpoints	near	points	of	interest	in	the
program.	These	can	be	set	to	just	after	the	entry	of	a	function	or	at	a
program	address.	When	one	of	the	breakpoints	is	hit	during	program
execution,	the	program	will	halt	and	return	control	to	the	user.	From	a
breakpoint,	we	can	examine	different	registers	and	memory	locations	in
various	formats.	We	can	also	single-step	the	program,	running	just	a	few
instructions	at	a	time,	or	we	can	proceed	to	the	next	breakpoint.
As	our	examples	suggest,	
GDB</p>
<p>has	an	obscure	command	syntax,	but	the
online	help	information	(invoked	within	
GDB</p>
<p>with	the	
command)
overcomes	this	shortcoming.	Rather	than	using	the	command-line
interface	to	
GDB
,	many	programmers	prefer	using	
DDD
,	an	extension	to	
GDB
that	provides	a	graphical	user	interface.</p>
<p>3.10.3	
Out-of-Bounds	Memory
References	and	Buffer	Overflow
We	have	seen	that	C	does	not	perform	any	bounds	checking	for	array
references,	and	that	local	variables	are	stored	on	the	stack	along	with
state	information	such	as	saved	register	values	and	return	addresses.
This	combination	can	lead	to	serious	program	errors,	where	the	state
stored	on	the	stack	gets	corrupted	by	a	write	to	an	out-of-bounds	array
element.	When	the	program	then	tries	to	reload	the	register	or	execute	a
instruction	with	this	corrupted	state,	things	can	go	seriously	wrong.
A	particularly	common	source	of	state	corruption	is	known	as	
buffer
overflow
.	Typically,	some	character	array	is	allocated	on	the	stack	to	hold
a	string,	but	the	size	of	the	string	exceeds	the	space	allocated	for	the
array.	This	is	demonstrated	by	the	following	program	example:
Command
Effect
Starting	and	stopping</p>
<p>Exit	
GDB
Run	your	program	(give	command-line	arguments	here)
Stop	your	program
Breakpoints
Set	breakpoint	at	entry	to	function	multstore
Set	breakpoint	at	address	
Delete	breakpoint	1
Delete	all	breakpoints
Execution
Execute	one	instruction
Execute	four	instructions
Like	
,	but	proceed	through	function	calls
Resume	execution
Run	until	current	function	returns
Examining	code
Disassemble	current	function
Disassemble	function	
Disassemble	function	around	address	
Disassemble	code	within	specified	address	range</p>
<p>Print	program	counter	in	hex
Examining	data
Print	contents	of	
in	decimal
Print	contents	of	
in	hex
Print	contents	of	
in	binary
Print	decimal	representation	of	
Print	hex	representation	of	555
Print	contents	of	
plus	8	in	hex
Print	long	integer	at	address	
Print	long	integer	at	address	
+	8
Examine	two	(8-byte)	words	starting	at	address
Examine	first	20	bytes	of	function	
Useful	information
Information	about	current	stack	frame
Values	of	all	the	registers
Get	information	about	
GDB
Figure	
3.39	
Example	
GDB</p>
<p>commands.</p>
<p>These	examples	illustrate	some	of	the	ways	
GDB</p>
<p>supports	debugging	of
machine-level	programs.
Figure	
3.40	
Stack	organization	for	
function.
Character	array	
is	just	part	of	the	saved	state.	An	out-of-bounds	write
to	
can	corrupt	the	program	state.</p>
<p>The	preceding	code	shows	an	implementation	of	the	library	function	gets
to	demonstrate	a	serious	problem	with	this	function.	It	reads	a	line	from
the	standard	input,	stopping	when	either	a	terminating	newline	character
or	some	error	condition	is	encountered.	It	copies	this	string	to	the	location
designated	by	argument	
and	terminates	the	string	with	a	null	character.
We	show	the	use	of	gets	in	the	function	
,	which	simply	reads	a	line
from	standard	input	and	echos	it	back	to	standard	output.
The	problem	with	gets	is	that	it	has	no	way	to	determine	whether
sufficient	space	has	been	allocated	to	hold	the	entire	string.	In	our	
example,	we	have	purposely	made	the	buffer	very	small—just	eight
characters	long.	Any	string	longer	than	seven	characters	will	cause	an
out-of-bounds	write.
By	examining	the	assembly	code	generated	by	
GCC</p>
<p>for	
,	we	can	infer
how	the	stack	is	organized:</p>
<p>Figure	
3.40
illustrates	the	stack	organization	during	the	execution	of
.	The	program	allocates	24	bytes	on	the	stack	by	subtracting	24	from
the	stack	pointer	(line	2).	Character	
is	positioned	at	the	top	of	the
stack,	as	can	be	seen	by	the	fact	that	
is	copied	to	
to	be	used
as	the	argument	to	the	calls	to	both	gets	and	
.	The	16	bytes	between
and	the	stored	return	pointer	are	not	used.	As	long	as	the	user	types
at	most	seven	characters,	the	string	returned	by	gets	(including	the
terminating	null)	will	fit	within	the	space	allocated	for	
.	A	longer	string,
however,	will	cause	gets	to	overwrite	some	of	the	information	stored	on
the	stack.	As	the	string	gets	longer,	the	following	information	will	get
corrupted:
Characters	typed
Additional	corrupted	state
0–7
None
9–23
Unused	stack	space
24–31
Return	address
32+
Saved	state	in	caller
No	serious	consequence	occurs	for	strings	of	up	to	23	characters,	but
beyond	that,	the	value	of	the	return	pointer,	and	possibly	additional	saved
state,	will	be	corrupted.	If	the	stored	value	of	the	return	address	is
corrupted,	then	the	
instruction	(line	8)	will	cause	the	program	to	jump
to	a	totally	unexpected	location.	None	of	these	behaviors	would	seem
possible	based	on	the	C	code.	The	impact	of	out-of-bounds	writing	to
memory	by	functions	such	as	
can	only	be	understood	by	studying
the	program	at	the	machine-code	level.</p>
<p>Our	code	for	
is	simple	but	sloppy.	A	better	version	involves	using
the	function	
,	which	includes	as	an	argument	a	count	on	the
maximum	number	of	bytes	to	read.	
Problem	
3.71
asks	you	to	write	an
echo	function	that	can	handle	an	input	string	of	arbitrary	length.	In
general,	using	
or	any	function	that	can	overflow	storage	is
considered	a	bad	programming	practice.	Unfortunately,	a	number	of
commonly	used	library	functions,	including	
,	and	
,
have	the	property	that	they	can	generate	a	byte	sequence	without	being
given	any	indication	of	the	size	of	the	destination	buffer	
[97]
.	Such
conditions	can	lead	to	vulnerabilities	to	buffer	overflow.
Practice	Problem	
3.46	
(solution	page	
346
)
Figure	
3.41
shows	a	(low-quality)	implementation	of	a	function
that	reads	a	line	from	standard	input,	copies	the	string	to	newly
allocated	storage,	and	returns	a	pointer	to	the	result.
Consider	the	following	scenario.	Procedure	
is	called	with
the	return	address	equal	to	
and	register	
equal	to
.	You	type	in	the	string
(a)	C	code</p>
<p>(b)	Disassembly	up	through	call	to	gets
Figure	
3.41	
C	and	disassembled	code	for	Practice	
Problem
3.46
.
The	program	terminates	with	a	segmentation	fault.	You	run	
GDB
and	determine	that	the	error	occurs	during	the	execution	of	the
instruction	of	</p>
<p>A
.	
Fill	in	the	diagram	that	follows,	indicating	as	much	as	you
can	about	the	stack	just	after	executing	the	instruction	at
line	3	in	the	disassembly.	Label	the	quantities	stored	on	the
stack	(e.g.,	&quot;Return	address&quot;)	on	the	right,	and	their
hexadecimal	values	(if	known)	within	the	box.	Each	box
represents	8	bytes.	Indicate	the	position	of	
.	Recall	that
the	ASCII	codes	for	characters	0–9	are	
B
.	
Modify	your	diagram	to	show	the	effect	of	the	call	to	
(line	5).
C
.	
To	what	address	does	the	program	attempt	to	return?
D
.	
What	register(s)	have	corrupted	value(s)	when	
returns?
E
.	
Besides	the	potential	for	buffer	overflow,	what	two	other
things	are	wrong	with	the	code	for	
A	more	pernicious	use	of	buffer	overflow	is	to	get	a	program	to	perform	a
function	that	it	would	otherwise	be	unwilling	to	do.	This	is	one	of	the	most
common	methods	to	attack	the	security	of	a	system	over	a	computer
network.	Typically,	the	program	is	fed	with	a	string	that	contains	the	byte
encoding	of	some	executable	code,	called	the	
exploit	code
,	plus	some
extra	bytes	that	overwrite	the	return	address	with	a	pointer	to	the	exploit
code.	The	effect	of	executing	the	
instruction	is	then	to	jump	to	the
exploit	code.</p>
<p>In	one	form	of	attack,	the	exploit	code	then	uses	a	system	call	to	start	up
a	shell	program,	providing	the	attacker	with	a	range	of	operating	system
functions.	In	another	form,	the	exploit	code	performs	some	otherwise
unauthorized	task,	repairs	the	damage	to	the	stack,	and	then	executes
a	second	time,	causing	an	(apparently)	normal	return	to	the	caller.
As	an	example,	the	famous	Internet	worm	of	November	1988	used	four
different	ways	to	gain	access	to	many	of	the	computers	across	the
Internet.	One	was	a	buffer	overflow	attack	on	the	finger	daemon	
,
which	serves	requests	by	the	
FINGER</p>
<p>command.	By	invoking	
FINGER</p>
<p>with	an
appropriate	string,	the	worm	could	make	the	daemon	at	a	remote	site
have	a	buffer	overflow	and	execute	code	that	gave	the	worm	access	to
the	remote	system.	Once	the	worm	gained	access	to	a	system,	it	would
replicate	itself	and	consume	virtually	all	of	the	machine's	computing
resources.	As	a	consequence,	hundreds	of	machines	were	effectively
paralyzed	until	security	experts	could	determine	how	to	eliminate	the
worm.	The	author	of	the	worm	was	caught	and	prosecuted.	He	was
sentenced	to	3	years	probation,	400	hours	of	community	service,	and	a
$10,500	fine.	Even	to	this	day,	however,	people	continue	to	find	security
leaks	in	systems	that	leave	them	vulnerable	to	buffer	overflow	attacks.
This	highlights	the	need	for	careful	programming.	Any	interface	to	the
external	environment	should	be	made	&quot;bulletproof&quot;	so	that	no	behavior	by
an	external	agent	can	cause	the	system	to	misbehave.
3.10.4	
Thwarting	Buffer	Overflow
Attacks</p>
<p>Buffer	overflow	attacks	have	become	so	pervasive	and	have	caused	so
many	problems	with	computer	systems	that	modern	compilers	and
operating	systems	have	implemented	mechanisms	to	make	it	more
difficult	to	mount	these	attacks	and	to	limit	the	ways	by	which	an	intruder
can	seize	control	of	a	system	via	a	buffer	overflow	attack.	In	this	section,
we	will	present	mechanisms	that	are	provided	by	recent	versions	of	
GCC
for	Linux.
Stack	Randomization
In	order	to	insert	exploit	code	into	a	system,	the	attacker	needs	to	inject
both	the	code	as	well	as	a	pointer	to	this	code	as	part	of	the	attack	string.
Generating
Aside	
Worms	and	viruses
Both	worms	and	viruses	are	pieces	of	code	that	attempt	to	spread
themselves	among	computers.	As	described	by	Spafford	
[105],
a
worm
is	a	program	that	can	run	by	itself	and	can	propagate	a	fully
working	version	of	itself	to	other	machines.	A	
virus
is	a	piece	of
code	that	adds	itself	to	other	programs,	including	operating
systems.	It	cannot	run	independently.	In	the	popular	press,	the
term	&quot;virus&quot;	is	used	to	refer	to	a	variety	of	different	strategies	for
spreading	attacking	code	among	systems,	and	so	you	will	hear
people	saying	&quot;virus&quot;	for	what	more	properly	should	be	called	a
&quot;worm.&quot;
this	pointer	requires	knowing	the	stack	address	where	the	string	will	be
located.	Historically,	the	stack	addresses	for	a	program	were	highly
predictable.	For	all	systems	running	the	same	combination	of	program</p>
<p>and	operating	system	version,	the	stack	locations	were	fairly	stable
across	many	machines.	So,	for	example,	if	an	attacker	could	determine
the	stack	addresses	used	by	a	common	Web	server,	it	could	devise	an
attack	that	would	work	on	many	machines.	Using	infectious	disease	as
an	analogy,	many	systems	were	vulnerable	to	the	exact	same	strain	of	a
virus,	a	phenomenon	often	referred	to	as	a	
security	monoculture</p>
<p>[96]
.
The	idea	of	
stack	randomization
is	to	make	the	position	of	the	stack	vary
from	one	run	of	a	program	to	another.	Thus,	even	if	many	machines	are
running	identical	code,	they	would	all	be	using	different	stack	addresses.
This	is	implemented	by	allocating	a	random	amount	of	space	between	0
and	
n
bytes	on	the	stack	at	the	start	of	a	program,	for	example,	by	using
the	allocation	function	
,	which	allocates	space	for	a	specified
number	of	bytes	on	the	stack.	This	allocated	space	is	not	used	by	the
program,	but	it	causes	all	subsequent	stack	locations	to	vary	from	one
execution	of	a	program	to	another.	The	allocation	range	
n
needs	to	be
large	enough	to	get	sufficient	variations	in	the	stack	addresses,	yet	small
enough	that	it	does	not	waste	too	much	space	in	the	program.
The	following	code	shows	a	simple	way	to	determine	a	&quot;typical&quot;	stack
address:</p>
<p>This	code	simply	prints	the	address	of	a	local	variable	in	the	main
function.	Running	the	code	10,000	times	on	a	Linux	machine	in	32-bit
mode,	the	addresses	ranged	from	
to	
,	a	range	of
around	2
.	Running	in	64-bit	mode	on	the	newer	machine,	the	addresses
ranged	from	
to	
,	a	range	of	nearly	2
.
Stack	randomization	has	become	standard	practice	in	Linux	systems.	It	is
one	of	a	larger	class	of	techniques	known	as	
address-space	layout
randomization
,	or	ASLR	
[99]
.	With	ASLR,	different	parts	of	the	program,
including	program	code,	library	code,	stack,	global	variables,	and	heap
data,	are	loaded	into	different	
regions	of	memory	each	time	a	program	is
run.	That	means	that	a	program	running	on	one	machine	will	have	very
different	address	mappings	than	the	same	program	running	on	other
machines.	This	can	thwart	some	forms	of	attack.
Overall,	however,	a	persistent	attacker	can	overcome	randomization	by
brute	force,	repeatedly	attempting	attacks	with	different	addresses.	A
common	trick	is	to	include	a	long	sequence	of	
(pronounced	&quot;no	op,&quot;
short	for	&quot;no	operation&quot;)	instructions	before	the	actual	exploit	code.
Executing	this	instruction	has	no	effect,	other	than	incrementing	the
program	counter	to	the	next	instruction.	As	long	as	the	attacker	can
guess	an	address	somewhere	within	this	sequence,	the	program	will	run
through	the	sequence	and	then	hit	the	exploit	code.	The	common	term
for	this	sequence	is	a	&quot;nop	sled&quot;	
[97]
,	expressing	the	idea	that	the
program	&quot;slides&quot;	through	the	sequence.	If	we	set	up	a	256-byte	nop	sled,
then	the	randomization	over	
n
=	2
can	be	cracked	by	enumerating	2
=
32,768	starting	addresses,	which	is	entirely	feasible	for	a	determined
attacker.	For	the	64-bit	case,	trying	to	enumerate	2
=	16,777,216	is	a	bit
more	daunting.	We	can	see	that	stack	randomization	and	other	aspects
23
32
23
15
24</p>
<p>of	ASLR	can	increase	the	effort	required	to	successfully	attack	a	system,
and	therefore	greatly	reduce	the	rate	at	which	a	virus	or	worm	can
spread,	but	it	cannot	provide	a	complete	safeguard.
Practice	Problem	
3.47	
(solution	page	
347
)
Running	our	stack-checking	code	10,000	times	on	a	system
running	Linux	version	2.6.16,	we	obtained	addresses	ranging	from
a	minimum	of	
to	a	maximum	of	
.
A
.	
What	is	the	approximate	range	of	addresses?
B
.	
If	we	attempted	a	buffer	overrun	with	a	128-byte	nop	sled,
about	how	many	attempts	would	it	take	to	test	all	starting
addresses?
Stack	Corruption	Detection
A	second	line	of	defense	is	to	be	able	to	detect	when	a	stack	has	been
corrupted.	We	saw	in	the	example	of	the	echo	function	(
Figure	
3.40
)
that	the	corruption	typically	occurs	when	the	program	overruns	the
bounds	of	a	local	buffer.	In	C,	there	is	no	reliable	way	to	prevent	writing
beyond	the	bounds	of	an	array.	Instead,	the	program	can	attempt	to
detect	when	such	a	write	has	occurred	before	it	can	have	any	harmful
effects.
Recent	versions	of	
GCC</p>
<p>incorporate	a	mechanism	known	as	a	
stack
protector
into	the	generated	code	to	detect	buffer	overruns.	The	idea	is	to
store	a	special	
canary
value
in	the	stack	frame	between	any	local	buffer
and	the	rest	of	the	stack	state,	as	illustrated	in	
Figure	
3.42</p>
<p>[26,</p>
<p>97]
.
4</p>
<p>This	canary	value,	also	referred	to	as	a	
guard	value
,	is	generated
randomly	each	time	the	program	runs,	and	so	there	is	no
4.	
The	term	&quot;canary&quot;	refers	to	the	historic	use	of	these	birds	to	detect	the	presence	of	dangerous
gases	in	coal	mines.
Figure	
3.42	
Stack	organization	for	
function	with	stack	protector
enabled.
A	special	&quot;canary&quot;	value	is	positioned	between	array	
and	the	saved
state.	The	code	checks	the	canary	value	to	determine	whether	or	not	the
stack	state	has	been	corrupted.
easy	way	for	an	attacker	to	determine	what	it	is.	Before	restoring	the
register	state	and	returning	from	the	function,	the	program	checks	if	the
canary	has	been	altered	by	some	operation	of	this	function	or	one	that	it
has	called.	If	so,	the	program	aborts	with	an	error.
Recent	versions	of	
GCC</p>
<p>try	to	determine	whether	a	function	is	vulnerable
to	a	stack	overflow	and	insert	this	type	of	overflow	detection
automatically.	In	fact,	for	our	earlier	demonstration	of	stack	overflow,	we
had	to	give	the	command-line	option	
to	prevent	
GCC
from	inserting	this	code.	Compiling	the	function	
without	this	option,
and	hence	with	the	stack	protector	enabled,	gives	the	following	assembly
code:</p>
<p>We	see	that	this	version	of	the	function	retrieves	a	value	from	memory
(line	3)	and	stores	it	on	the	stack	at	offset	8	from	
,	just	beyond	the
region	allocated	for	
.	The	instruction	argument	
is	an	indication
that	the	canary	value	is	read	from	memory	using	
segmented	addressing
,
an	addressing	mechanism	that	dates	
back	to	the	80286	and	is	seldom
found	in	programs	running	on	modern	systems.	By	storing	the	canary	in	a
special	segment,	it	can	be	marked	as	&quot;read	only,&quot;	so	that	an	attacker
cannot	overwrite	the	stored	canary	value.	Before	restoring	the	register
state	and	returning,	the	function	compares	the	value	stored	at	the	stack</p>
<p>location	with	the	canary	value	(via	the	
instruction	on	line	11).	If	the
two	are	identical,	the	
instruction	will	yield	zero,	and	the	function	will
complete	in	the	normal	fashion.	A	nonzero	value	indicates	that	the	canary
on	the	stack	has	been	modified,	and	so	the	code	will	call	an	error	routine.
Stack	protection	does	a	good	job	of	preventing	a	buffer	overflow	attack
from	corrupting	state	stored	on	the	program	stack.	It	incurs	only	a	small
performance	penalty,	especially	because	
GCC</p>
<p>only	inserts	it	when	there	is
a	local	buffer	of	type	
in	the	function.	Of	course,	there	are	other	ways
to	corrupt	the	state	of	an	executing	program,	but	reducing	the
vulnerability	of	the	stack	thwarts	many	common	attack	strategies.
Practice	Problem	
3.48	
(solution	page	
347
)
The	functions	
,	and	
provide	a	very	convoluted
way	to	compute	the	number	of	decimal	digits	required	to	represent
an	integer.	We	will	use	this	as	a	way	to	study	some	aspects	of	the
GCC</p>
<p>stack	protector	facility.</p>
<p>The	following	show	portions	of	the	code	for	
,	compiled	both
with	and	without	stack	protector:
(a)	Without	protector
(b)	With	protector</p>
<p>A
.	
For	both	versions:	What	are	the	positions	in	the	stack	frame
for	
,	and	(when	present)	the	canary	value?
B
.	
How	does	the	rearranged	ordering	of	the	local	variables	in
the	protected	code	provide	greater	security	against	a	buffer
overrun	attack?
Limiting	Executable	Code	Regions
A	final	step	is	to	eliminate	the	ability	of	an	attacker	to	insert	executable
code	into	a	system.	One	method	is	to	limit	which	memory	regions	hold
executable	code.	In	typical	programs,	only	the	portion	of	memory	holding
the	code	generated	by	the	compiler	need	be	executable.	The	other
portions	can	be	restricted	to	allow	just	reading	and	writing.	As	we	will	see
in	
Chapter	
9
,	the	virtual	memory	space	is	logically	divided	into	
pages
,
typically	with	2,048	or	4,096	bytes	per	page.	The	hardware	supports
different	forms	of	
memory	protection
,	indicating	the	forms	of	access
allowed	by	both	user	programs	and	the	operating	system	kernel.	Many
systems	allow	control	over	three	forms	of	access:	read	(reading	data
from	memory),	write	(storing	data	into	memory),	and	execute	(treating	the
memory	contents	as	machine-level	code).	Historically,	the	x86
architecture	merged	the	read	and	execute	access	controls	into	a	single	1-
bit	flag,	so	that	any	page	marked	as	readable	was	also	executable.	The
stack	had	to	be	kept	both	readable	and	writable,	and	therefore	the	bytes</p>
<p>on	the	stack	were	also	executable.	Various	schemes	were	implemented
to	be	able	to	limit	some	pages	to	being	readable	but	not	executable,	but
these	generally	introduced	significant	inefficiencies.
More	recently,	AMD	introduced	an	NX	(for	&quot;no-execute&quot;)	bit	into	the
memory	protection	for	its	64-bit	processors,	separating	the	read	and
execute	access	modes,	and	Intel	followed	suit.	With	this	feature,	the
stack	can	be	marked	as	being	readable	and	writable,	but	not	executable,
and	the	checking	of	whether	a	page	is	executable	is	performed	in
hardware,	with	no	penalty	in	efficiency.
Some	types	of	programs	require	the	ability	to	dynamically	generate	and
execute	code.	For	example,	&quot;just-in-time&quot;	compilation	techniques
dynamically	generate	code	for	programs	written	in	interpreted	languages,
such	as	Java,	to	improve	execution	performance.	Whether	or	not	the	run-
time	system	can	restrict	the	executable	code	to	just	that	part	generated
by	the	compiler	in	creating	the	original	program	depends	on	the	language
and	the	operating	system.
The	techniques	we	have	outlined—randomization,	stack	protection,	and
limiting	which	portions	of	memory	can	hold	executable	code—are	three	of
the	most	common	mechanisms	used	to	minimize	the	vulnerability	of
programs	to	buffer	overflow	attacks.	They	all	have	the	properties	that
they	require	no	special	effort	on	the	part	of	the	programmer	and	incur
very	little	or	no	performance	penalty.	Each	separately	reduces	the	level	of
vulnerability,	and	in	combination	they	become	even	more	effective.
Unfortunately,	there	are	still	ways	to	attack	computers	
[85,</p>
<p>97]
,	and	so
worms	and	viruses	continue	to	compromise	the	integrity	of	many
machines.</p>
<p>3.10.5	
Supporting	Variable-Size
Stack	Frames
We	have	examined	the	machine-level	code	for	a	variety	of	functions	so
far,	but	they	all	have	the	property	that	the	compiler	can	determine	in
advance	the	amount	of	space	that	must	be	allocated	for	their	stack
frames.	Some	functions,	however,	require	a	variable	amount	of	local
storage.	This	can	occur,	for	example,	when	the	function	calls	
,	a
standard	library	function	that	can	allocate	an	arbitrary	number	of	bytes	of
storage	on	the	stack.	It	can	also	occur	when	the	code	declares	a	local
array	of	variable	size.
Although	the	information	presented	in	this	section	should	rightfully	be
considered	an	aspect	of	how	procedures	are	implemented,	we	have
deferred	the	presentation	to	this	point,	since	it	requires	an	understanding
of	arrays	and	alignment.
The	code	of	
Figure	
3.43(a)
gives	an	example	of	a	function	containing
a	variable-size	array.	The	function	declares	local	array	
of	
n
pointers,
where	
n
is	given	by	the	first	argument.	This	requires	allocating	8
n
bytes
on	the	stack,	where	the	value	of	
n
may	vary	from	one	call	of	the	function
to	another.	The	compiler	therefore	cannot	determine	how	much	space	it
must	allocate	for	the	function's	stack	frame.	In	addition,	the	program
generates	a	reference	to	the	address	of	local	variable	
,	and	so	this
variable	must	also	be	stored	on	the	stack.	During	execution,	the	program
must	be	able	to	access	both	local	variable	
and	the	elements	of	array</p>
<p>.	On	returning,	the	function	must	deallocate	the	stack	frame	and	set	the
stack	pointer	to	the	position	of	the	stored	return	address.
To	manage	a	variable-size	stack	frame,	x86-64	code	uses	register	
to	serve	as	a	
frame	pointer
(sometimes	referred	to	as	a	
base	pointer
,	and
hence	the	letters	
in	
).	When	using	a	frame	pointer,	the	stack
frame	is	organized	as	shown	for	the	case	of	function	
in	
Figure
3.44
.	We	see	that	the	code	must	save	the	previous	version	of	
on
the	stack,	since	it	is	a	callee-saved	register.	It	then	keeps	
pointing	to
this	position	throughout	the	execution	of	the	function,	and	it	references
fixed-length	local	variables,	such	as	
,	at	offsets	relative	to	
(a)	C	code
(b)	Portions	of	generated	assembly	code</p>
<p>Figure	
3.43	
Function	requiring	the	use	of	a	frame	pointer.
The	variable-size	array	implies	that	the	size	of	the	stack	frame	cannot	be
determined	at	compile	time.
Figure	
3.44	
Stack	frame	structure	for	function	
.
The	function	uses	register	
as	a	frame	pointer.	The	annotations
along	the	right-hand	side	are	in	reference	to	Practice	
Problem	
3.49
.
Figure	
3.43(b)
shows	portions	of	the	code	
GCC</p>
<p>generates	for	function
.	At	the	beginning	of	the	function,	we	see	code	that	sets	up	the
stack	frame	and	allocates	space	for	array	
.	The	code	starts	by	pushing
the	current	value	of	
onto	the	stack	and	setting	
to	point	to	this
stack	position	(lines	2–3).	Next,	it	allocates	16	bytes	on	the	stack,	the	first
8	of	which	are	used	to	store	local	variable	
,	and	the	second	8	of	which
are	unused.	Then	it	allocates	space	for	array	
(lines	5–11).	The	details
of	how	much	space	it	allocates	and	where	it	positions	
within	this	space</p>
<p>are	explored	in	Practice	
Problem	
3.49
.	Suffice	it	to	say	that	by	the
time	the	program	reaches	line	11,	it	has	(1)	allocated	at	least	8
n
bytes	on
the	stack	and	(2)	positioned	array	
within	the	allocated	region	such	that
at	least	8
n
bytes	are	available	for	its	use.
The	code	for	the	initialization	loop	shows	examples	of	how	local	variables
and	
are	referenced.	Line	13	shows	array	element	
being	set	to
.	This	instruction	uses	the	value	in	register	
as	the	address	for	the
start	of	
.	We	can	see	instances	where	local	variable	
is	updated	(line
15)	and	read	(line	17).	The	address	of	
is	given	by	reference	
—
that	is,	at	offset	-8	relative	to	the	frame	pointer.
At	the	end	of	the	function,	the	frame	pointer	is	restored	to	its	previous
value	using	the	
instruction	(line	20).	This	instruction	takes	no
arguments.	It	is	equivalent	to	executing	the	following	two	instructions:
That	is,	the	stack	pointer	is	first	set	to	the	position	of	the	saved	value	of
,	and	then	this	value	is	popped	from	the	stack	into	
.	This
instruction	combination	has	the	effect	of	deallocating	the	entire	stack
frame.
In	earlier	versions	of	x86	code,	the	frame	pointer	was	used	with	every
function	call.	With	x86-64	code,	it	is	used	only	in	cases	where	the	stack</p>
<p>frame	may	be	of	variable	size,	as	is	the	case	for	function	
.
Historically,	most	compilers	used	frame	pointers	when	generating	IA32
code.	Recent	versions	of	
GCC</p>
<p>have	dropped	this	convention.	Observe	that
it	is	acceptable	to	mix	code	that	uses	frame	pointers	with	code	that	does
not,	as	long	as	all	functions	treat	
as	a	callee-saved	register.
Practice	Problem	
3.49	
(solution	page	
347
)
In	this	problem,	we	will	explore	the	logic	behind	the	code	in	lines
5–11	of	
Figure	
3.43(b)
,	where	space	is	allocated	for	variable-
size	array	p.	As	the	annotations	of	the	code	indicate,	let	us	let	
s
denote	the	address	of	the	stack	pointer	after	executing	the	
instruction	of	line	4.	This	instruction	allocates	the	space	for	local
variable	i.	Let	
s
denote	the	value	of	the	stack	pointer	after
executing	the	
instruction	of	line	7.	This	instruction	allocates
the	storage	for	local	array	
.	Finally,	let	
p
denote	the	value
assigned	to	registers	
and	
in	the	instructions	of	lines	10–
11.	Both	of	these	registers	are	used	to	reference	array	
.
The	right-hand	side	of	
Figure	
3.44
diagrams	the	positions	of	the
locations	indicated	by	
s
,	
s
,	and	
p
.	It	also	shows	that	there	may	be
an	offset	of	
e
bytes	between	the	values	of	
s
and	
p
.	This	space
will	not	be	used.	There	may	also	be	an	offset	of	
e
bytes	between
the	end	of	array	
and	the	position	indicated	by	
s
.
A
.	
Explain,	in	mathematical	terms,	the	logic	in	the	computation
of	
s</p>
<div style="break-before: page; page-break-before: always;"></div><pre><code>on	lines	5–7.	
</code></pre>
<p>Hint:
Think	about	the	bit-level
representation	of	–16	and	its	effect	in	the	
instruction	of
line	6.
1
2
1
2
2
1
1
1
2</p>
<p>B
.	
Explain,	in	mathematical	terms,	the	logic	in	the	computation
of	
p
on	lines	8–10.	
Hint:
You	may	want	to	refer	to	the
discussion	on	division	by	powers	of	2	in	
Section	
2.3.7
.
C
.	
For	the	following	values	of	
n
and	
s
,	trace	the	execution	of
the	code	to	determine	what	the	resulting	values	would	be
for	
s
,	
p
,	
e
,	and	
e
.
n
s
s
p
e
e
5
2,065</p>
<hr />
<hr />
<hr />
<hr />
<p>6
2,064</p>
<hr />
<hr />
<hr />
<hr />
<p>D
.	
What	alignment	properties	does	this	code	guarantee	for	the
values	of	
s
and	
p
?
1
2
1
2
1
2
1
2
2</p>
<p>3.11	
Floating-Point	Code
The	
floating-point	architecture
for	a	processor	consists	of	the	different
aspects	that	affect	how	programs	operating	on	floating-point	data	are
mapped	onto	the	machine,	including
How	floating-point	values	are	stored	and	accessed.	This	is	typically
via	some	form	of	registers.
The	instructions	that	operate	on	floating-point	data.
The	conventions	used	for	passing	floating-point	values	as	arguments
to	functions	and	for	returning	them	as	results.
The	conventions	for	how	registers	are	preserved	during	function	calls
—for	example,	with	some	registers	designated	as	caller	saved,	and
others	as	callee	saved.
To	understand	the	x86-64	floating-point	architecture,	it	is	helpful	to	have	a
brief	historical	perspective.	Since	the	introduction	of	the	Pentium/MMX	in
1997,	both	Intel	and	AMD	have	incorporated	successive	generations	of
media
instructions	to	support	graphics	and	image	processing.	These
instructions	originally	focused	on	allowing	multiple	operations	to	be
performed	in	a	parallel	mode	known	as	
single	instruction,	multiple	data
,
or	
SIMD
(pronounced	sim-dee).	In	this	mode	the	same	operation	is
performed	on	a	number	of	different	data	values	in	parallel.	Over	the
years,	there	has	been	a	progression	of	these	extensions.	The	names
have	changed	through	a	series	of	major	revisions	from	MMX	to	SSE	(for
&quot;streaming	SIMD	extensions&quot;)	and	most	recently	AVX	(for	&quot;advanced
vector	extensions&quot;).	Within	each	generation,	there	have	also	been</p>
<p>different	versions.	Each	of	these	extensions	manages
datainsetsofregisters,	referredto	as&quot;MM&quot;	registers	for	MMX,	&quot;XMM&quot;	for
SSE,	and	&quot;YMM&quot;	for	AVX,	ranging	from	64	bits	for	MM	registers,	to	128
for	XMM,	to	256	for	YMM.	So,	for	example,	each	YMM	register	can	hold
eight	32-bit	values,	or	four	64-bit	values,	where	these	values	can	be
either	integer	or	floating	point.
Starting	with	SSE2,	introduced	with	the	Pentium	4	in	2000,	the	media
instructions	have	included	ones	to	operate	on	
scalar
floating-point	data,
using	single	values	in	the	low-order	32	or	64	bits	of	XMM	or	YMM
registers.	This	scalar	mode	provides	a	set	of	registers	and	instructions
that	are	more	typical	of	the	way	other	processors	support	floating	point.
All	processors	capable	of	executing	x86-64	code	support	SSE2	or	higher,
and	hence	x86-64	floating	point	is	based	on	SSE	or	AVX,	including
conventions	for	passing	procedure	arguments	and	return	values	
[77]
.
Our	presentation	is	based	on	AVX2,	the	second	version	of	AVX,
introduced	with	the	Core	i7	Haswell	processor	in	2013.	G
CC
will	generate
AVX2	code	when	given	the	command-line	parameter	
.	Code
based	on	the	different	versions	of	SSE,	as	well	as	the	first	version	of
AVX,	is	conceptually	similar,	although	they	differ	in	the	instruction	names
and	formats.	We	present	only	instructions	that	arise	in	compiling	floating-
point	programs	with	
GCC
.	These	are,	for	the	most	part,	the	scalar	AVX
instructions,	although	we	document	occasions	where	instructions
intended	for	operating	on	entire	data	vectors	arise.	A	more	complete
coverage	of	how	to	exploit	the	SIMD	capabilities	of	SSE	and	AVX	is
presented	in	Web	Aside	
OPT
:
SIMD
on	page	546.	Readers	may	wish	to	refer
to	the	AMD	and	Intel	documentation	for	the	individual	instructions	
[4,</p>
<p>51]
.
As	with	integer	operations,	note	that	the	ATT	format	we	use	in	our</p>
<p>presentation	differs	from	the	Intel	format	used	in	these	documents.	In
particular,	the	instruction	operands	are	listed	in	a	different	order	in	these
two	versions.
Figure	
3.45	
Media	registers.
These	registers	are	used	to	hold	floating-point	data.	Each	YMM	register
holds	32	bytes.	The	low-order	16	bytes	can	be	accessed	as	an	XMM
register.
As	is	illustrated	in	
Figure	
3.45
,	the	AVX	floating-point	architecture</p>
<p>allows	data	to	be	stored	in	16	YMM	registers,	named	
.	Each
YMM	register	is	256	bits	(32	bytes)	long.	When	operating	on	scalar	data,
these	registers	only	hold	floating-point	data,	and	only	the	low-order	32
bits	(for	float)	or	64	bits	(for	double)	are	used.	The	assembly	code	refers
to	the	registers	by	their	SSE	XMM	register	names	
,	where
each	XMM	register	is	the	low-order	128	bits	(16	bytes)	of	the
corresponding	YMM	register.
Instruction
Source
Destination
Description
M
X
Move	single	precision
X
M
Move	single	precision
M
X
Move	double	precision
X
M
Move	double	precision
X
X
Move	aligned,	packed	single	precision
X
X
Move	aligned,	packed	double	precision
Figure	
3.46	
Floating-point	movement	instructions.
These	operations	transfer	values	between	memory	and	registers,	as	well
as	between	pairs	of	registers.	(
X
:	XMM	register	(e.g.,	
);	
M
:	32-bit
memory	range;	
M
:	64-bit	memory	range)
3.11.1	
Floating-Point	Movement	and
Conversion	Operations
32
32
64
64
32
64</p>
<p>Figure	
3.46
shows	a	set	of	instructions	for	transferring	floating-point
data	between	memory	and	XMM	registers,	as	well	as	from	one	XMM
register	to	another	without	any	conversions.	Those	that	reference
memory	are	
scalar
instructions,	meaning	that	they	operate	on	individual,
rather	than	packed,	data	values.	The	data	are	held	either	in	memory
(indicated	in	the	table	as	
M
and	
M
)	or	in	XMM	registers	(shown	in	the
table	as	
X
).	These	instructions	will	work	correctly	regardless	of	the
alignment	of	data,	although	the	code	optimization	guidelines	recommend
that	32-bit	memory	data	satisfy	a	4-byte	alignment	and	that	64-bit	data
satisfy	an	8-byte	alignment.	Memory	references	are	specified	in	the	same
way	as	for	the	integer	
MOV</p>
<p>instructions,	with	all	of	the	different	possible
combinations	of	displacement,	base	register,	index	register,	and	scaling
factor.
G
CC</p>
<p>uses	the	scalar	movement	operations	only	to	transfer	data	from
memory	to	an	XMM	register	or	from	an	XMM	register	to	memory.	For
transferring	data	between	two	XMM	registers,	it	uses	one	of	two	different
instructions	for	copying	the	entire	contents	of	one	XMM	register	to
another—namely,	
for	single-precision	and	
for	double-
precision	values.	For	these	cases,	whether	the	program	copies	the	entire
register	or	just	the	low-order	value	affects	neither	the	program
functionality	nor	the	execution	speed,	and	so	using	these	instructions
rather	than	ones	specific	to	scalar	data	makes	no	real	difference.	The
letter	`
'	in	these	instruction	names	stands	for	&quot;aligned.&quot;	When	used	to
read	and	write	memory,	they	will	cause	an	exception	if	the	address	does
not	satisfy	a	16-byte	alignment.	For	transferring	between	two	registers,
there	is	no	possibility	of	an	incorrect	alignment.
32
64</p>
<p>As	an	example	of	the	different	floating-point	move	operations,	consider
the	C	function
Instruction
Source
Destination
Description
X
/
M
R
Convert	with	truncation	single	precision	to
integer
X
/
M
R
Convert	with	truncation	double	precision	to
integer
X
/
M
R
Convert	with	truncation	single	precision	to	quad
word	integer
X
/
M
R
Convert	with	truncation	double	precision	to
quad	word	integer
Figure	
3.47	
Two-operand	floating-point	conversion	operations.
These	convert	floating-point	data	to	integers.	(
X
:	XMM	register	(e.g.,
);	
R
:	32-bit	general-purpose	register	(e.g.,	
);	
R
:	64-bit
general-purpose	register	(e.g.,	
);	
M
:	32-bit	memory	range;	
M
:	64-
bit	memory	range)
32
32
64
32
32
64
64
64
32
64
32
64</p>
<p>Instruction
Source
1
Source
2
Destination
Description
M
/
R
X
X
Convert	integer	to	single	precision
M
/
R
X
X
Convert	integer	to	double	precision
M
/
R
X
X
Convert	quad	word	integer	to	single
precision
M
/
R
X
X
Convert	quad	word	integer	to	double
precision
Figure	
3.48	
Three-operand	floating-point	conversion	operations.
These	instructions	convert	from	the	data	type	of	the	first	source	to	the
data	type	of	the	destination.	The	second	source	value	has	no	effect	on
the	low-order	bytes	of	the	result.	(
X
:	XMM	register	(e.g.,	
);	
M
:	32-
bit	memory	range;	
M
:	64-bit	memory	range)
and	its	associated	x86-64	assembly	code
32
32
32
32
32
64
64
32
64</p>
<p>We	can	see	in	this	example	the	use	of	the	
instruction	to	copy
data	from	one	register	to	another	and	the	use	of	the	
instruction	to
copy	data	from	memory	to	an	XMM	register	and	from	an	XMM	register	to
memory.
Figures	
3.47
and	
3.48
show	sets	of	instructions	for	converting
between	floating-point	and	integer	data	types,	as	well	as	between
different	floating-point	formats.	These	are	all	scalar	instructions	operating
on	individual	data	values.	Those	in	
Figure	
3.47
convert	from	a
floating-point	value	read	from	either	an	XMM	register	or	memory	and
write	the	result	to	a	general-purpose	register	(e.g.,	
,	etc.).
When	converting	floating-point	values	to	integers,	they	perform
truncation
,	rounding	values	toward	zero,	as	is	required	by	C	and	most
other	programming	languages.
The	instructions	in	
Figure	
3.48
convert	from	integer	to	floating	point.
They	use	an	unusual	three-operand	format,	with	two	sources	and	a
destination.	The	
first	operand	is	read	from	memory	or	from	a	general-
purpose	register.	For	our	purposes,	we	can	ignore	the	second	operand,
since	its	value	only	affects	the	upper	bytes	of	the	result.	The	destination
must	be	an	XMM	register.	In	common	usage,	both	the	second	source	and
the	destination	operands	are	identical,	as	in	the	instruction
This	instruction	reads	a	long	integer	from	register	
,	converts	it	to
data	type	double,	and	stores	the	result	in	the	lower	bytes	of	XMM	register</p>
<p>.
Finally,	for	converting	between	two	different	floating-point	formats,	current
versions	of	
GCC</p>
<p>generate	code	that	requires	separate	documentation.
Suppose	the	low-order	4	bytes	of	
hold	a	single-precision	value;
then	it	would	seem	straightforward	to	use	the	instruction
to	convert	this	to	a	double-precision	value	and	store	the	result	in	the
lower	8	bytes	of	register	
.	Instead,	we	find	the	following	code
generated	by	
GCC
:
The	
instruction	is	normally	used	to	interleave	the	values	in	two
XMM	registers	and	store	them	in	a	third.	That	is,	if	one	source	register
contains	words	[
s
,	
s
,	
s
,	
s
]	and	the	other	contains	words	[
d
,	
d
,	
d
,	
d
],
then	the	value	of	the	destination	register	will	be	[
s
,	
d
,	
s
,	
d
].	In	the	code
above,	we	see	the	same	register	being	used	for	all	three	operands,	and
3
2
1
0
3
2
1
0
1
1
0
0</p>
<p>so	if	the	original	register	held	values	[
x
,	
x
,	
x
,	
x
],	then	the	instruction	will
update	the	register	to	hold	values	[
x
,	
x
,	
x
,	
x
].	The	
instruction
expands	the	two	low-order	single-precision	values	in	the	source	XMM
register	to	be	the	two	double-precision	values	in	the	destination	XMM
register.	Applying	this	to	the	result	of	the	preceding	
instruction
would	give	values	[
dx
,	
dx
],	where	
dx
is	the	result	of	converting	
x
to
double	precision.	That	is,	the	net	effect	of	the	two	instructions	is	to
convert	the	original	single-precision	value	in	the	low-order	4	bytes	of
to	double	precision	and	store	two	copies	of	it	in	
.	It	is	unclear
why	
GCC</p>
<p>generates	this	code.	There	is	neither	benefit	nor	need	to	have
the	value	duplicated	within	the	XMM	register.
G
CC</p>
<p>generates	similar	code	for	converting	from	double	precision	to	single
precision:
Suppose	these	instructions	start	with	register	
holding	two	double-
precision	values	[
x
,	
x
].	Then	the	
instruction	will	set	it	to	[
x
,	
x
].
The	
instruction	will	convert	these	values	to	single	precision,
pack	them	into	the	low-order	half	of	the	register,	and	set	the	upper	half	to
0,	yielding	a	result	[0.0,	0.0,	
x
,	
x
]	(recall	that	floating-point	value	0.0	is
3
2
1
0
1
1
0
0
0
0
0
1
0
0
0
0
0</p>
<p>represented	by	a	bit	pattern	of	all	zeros).	Again,	there	is	no	clear	value	in
computing	the	conversion	from	one	precision	to	another	this	way,	rather
than	by	using	the	single	instruction
As	an	example	of	the	different	floating-point	conversion	operations,
consider	the	C	function
and	its	associated	x86-64	assembly	code</p>
<p>All	of	the	arguments	to	
are	passed	through	the	general-purpose
registers,	since	they	are	either	integers	or	pointers.	The	result	is	returned
in	register	
.	As	is	documented	in	
Figure	
3.45
,	this	is	the
designated	return	register	for	float	or	double	values.	In	this	code,	we	see
a	number	of	the	movement	and	conversion	instructions	of	
Figures
3.46
–
3.48
,	as	well	as	
GCC
's	preferred	method	of	converting	from
single	to	double	precision.
Practice	Problem	
3.50	
(solution	page	
347
)
For	the	following	C	code,	the	expressions	
all	map	to	the
program	values	
,	and	
:</p>
<p>Determine	the	mapping,	based	on	the	following	x86-64	code	for
the	function:</p>
<p>Practice	Problem	
3.51	
(solution	page	
348
)
The	following	C	function	converts	an	argument	of	type	
to	a
return	value	of	type	
,	where	these	two	types	are	defined
using	
:
For	execution	on	x86-64,	assume	that	argument	
is	either	in
or	in	the	appropriately	named	portion	of	register	
(i.e.,
or	
).	One	or	two	instructions	are	to	be	used	to	perform
the	type	conversion	and	to	copy	the	value	to	the	appropriately
named	portion	of	register	
(integer	result)	or	
(floating-
point	result).	Show	the	instruction(s),	including	the	source	and
destination	registers.
T
T
Instruction(s)
long
double
vcvtsi2sdq	%rdi,	%xmm0
double
int</p>
<hr />
<p>double
float</p>
<hr />
<p>long
float</p>
<hr />
<p>x
y</p>
<p>float
long</p>
<hr />
<p>3.11.2	
Floating-Point	Code	in
Procedures
With	x86-64,	the	XMM	registers	are	used	for	passing	floating-point
arguments	to	functions	and	for	returning	floating-point	values	from	them.
As	is	illustrated	in	
Figure	
3.45
,	the	following	conventions	are
observed:
Up	to	eight	floating-point	arguments	can	be	passed	in	XMM	registers
.	These	registers	are	used	in	the	order	the	arguments	are
listed.	Additional	floating-point	arguments	can	be	passed	on	the	stack.
A	function	that	returns	a	floating-point	value	does	so	in	register	
.
All	XMM	registers	are	caller	saved.	The	callee	may	overwrite	any	of
these	registers	without	first	saving	it.
When	a	function	contains	a	combination	of	pointer,	integer,	and	floating-
point	arguments,	the	pointers	and	integers	are	passed	in	general-
purpose	registers,	while	the	floating-point	values	are	passed	in	XMM
registers.	This	means	that	the	mapping	of	arguments	to	registers
depends	on	both	their	types	and	their	ordering.	Here	are	several
examples:</p>
<p>This	function	would	have	
in	
in	
and	
in	
.
This	function	would	have	the	same	register	assignment	as	function	
.
This	function	would	have	
in	
in	
,	and	
in	
Practice	Problem	
3.52	
(solution	page	
348
)
For	each	of	the	following	function	declarations,	determine	the
register	assignments	for	the	arguments:
A
.	
B
.	
C
.	
D
.	
3.11.3	
Floating-Point	Arithmetic</p>
<p>Operations
Figure	
3.49
documents	a	set	of	scalar	AVX2	floating-point	instructions
that	perform	arithmetic	operations.	Each	has	either	one	(
S
)	or	two	(
S
,
S
)	source	operands	and	a	destination	operand	
D
.	The	first	source
operand	
S
can	be	either	an	XMM	register	or	a	memory	location.	The
second	source	operand	and	the	destination	operands	must	be	XMM
registers.	Each	operation	has	an	instruction	for	single	precision	and	an
instruction	for	double	precision.	The	result	is	stored	in	the	destination
register.
As	an	example,	consider	the	following	floating-point	function:
The	x86-64	code	is	as	follows:
1
1
2
1</p>
<p>Single
Double
Effect
Description
D
←	
S
+
S
Floating-point	add
D
←	
S
-
S
Floating-point	subtract
D
←	
S
×	
S
Floating-point	multiply
D
←	
S
/
S
Floating-point	divide
D
←	max(
S
,	
S
)
Floating-point	maximum
D
←	min(
S
,	
S
)
Floating-point	minimum
Floating-point	square	root
Figure	
3.49	
Scalar	floating-point	arithmetic	operations.
These	have	either	one	or	two	source	operands	and	a	destination
operand.
2
1
2
1
2
1
2
1
2
1
2
1
D
←
S
1</p>
<p>The	three	floating-point	arguments	
,	and	
are	passed	in	XMM
registers	
,	while	integer	argument	
is	passed	in	register
.	The	standard	two-instruction	sequence	is	used	to	convert	argument
to	double	(lines	2-3).	Another	conversion	instruction	is	required	to
convert	argument	
to	double	(line	5).	The	function	value	is	returned	in
register	
.
Practice	Problem	
3.53	
(solution	page	
348
)
For	the	following	C	function,	the	types	of	the	four	arguments	are
defined	by	
:
When	compiled,	
GCC</p>
<p>generates	the	following	code:</p>
<p>Determine	the	possible	combinations	of	types	of	the	four
arguments	(there	may	be	more	than	one).
Practice	Problem	
3.54	
(solution	page	
349
)
Function	
has	the	following	prototype:
G
CC</p>
<p>generates	the	following	code	for	the	function:</p>
<p>Write	a	C	version	of	
.
3.11.4	
Defining	and	Using	Floating-
Point	Constants
Unlike	integer	arithmetic	operations,	AVX	floating-point	operations	cannot
have	immediate	values	as	operands.	Instead,	the	compiler	must	allocate
and	initialize	storage	for	any	constant	values.	The	code	then	reads	the
values	from	memory.	This	is	illustrated	by	the	following	Celsius	to
Fahrenheit	conversion	function:
The	relevant	parts	of	the	x86-64	assembly	code	are	as	follows:</p>
<p>We	see	that	the	function	reads	the	value	1.8	from	the	memory	location
labeled	
and	the	value	32.0	from	the	memory	location	labeled	
Looking	at	the	values	associated	with	these	labels,	we	see	that	each	is
specified	by	a	pair	of	.long	declarations	with	the	values	given	in	decimal.
How	should	these	be	interpreted	as	floating-point	values?	Looking	at	the
declaration	labeled	.
,	we	see	that	the	two	values	are	3435973837
(
)	and	1073532108	(
.)	Since	the	machine	uses
little-endian	byte	ordering,	the	first	value	gives	the	low-order	4	bytes,
while	the	second	gives	the	high-order	4	bytes.	From	the	high-order	bytes,
we	can	extract	an	exponent	field	of	
(1023),	from	which	we	subtract
a	bias	of	1023	to	get	an	exponent	of	0.	Concatenating	the	fraction	bits	of
the	two	values,	we	get	a	fraction	field	of	
,	which	can	be
shown	to	be	the	fractional	binary	representation	of	0.8,	to	which	we	add
the	implied	leading	one	to	get	1.8.
Single
Double
Effect
Description</p>
<h2>D
←	
S
^	
S
Bitwise	
EXCLUSIVE</h2>
<p>OR
D
←	
S
&amp;	
S
Bitwise	
AND
Figure	
3.50	
Bitwise	operations	on	packed	data.
These	instructions	perform	Boolean	operations	on	all	128	bits	in	an	XMM
register.
Practice	Problem	
3.55	
(solution	page	
349
)
Show	how	the	numbers	declared	at	label	
encode	the	number
32.0.
3.11.5	
Using	Bitwise	Operations	in
Floating-Point	Code
At	times,	we	find	
GCC</p>
<p>generating	code	that	performs	bitwise	operations	on
XMM	registers	to	implement	useful	floating-point	results.	
Figure	
3.50
shows	some	relevant	instructions,	similar	to	their	counterparts	for
operating	on	general-purpose	registers.	These	operations	all	act	on
packed	data,	meaning	that	they	update	the	entire	destination	XMM
register,	applying	the	bitwise	operation	to	all	the	data	in	the	two	source
registers.	Once	again,	our	only	interest	for	scalar	data	is	the	effect	these
instructions	have	on	the	low-order	4	or	8	bytes	of	the	destination.	These
operations	are	often	simple	and	convenient	ways	to	manipulate	floating-
point	values,	as	is	explored	in	the	following	problem.
2
1
2
1</p>
<p>Practice	Problem	
3.56	
(solution	page	
350
)
Consider	the	following	C	function,	where	
is	a	macro	defined
with	#define:
Below,	we	show	the	AVX2	code	generated	for	different	definitions
of	
,	where	value	x	is	held	in	
.	All	of	them	correspond	to
some	useful	operation	on	floating-point	values.	Identify	what	the
operations	are.	Your	answers	will	require	you	to	understand	the	bit
patterns	of	the	constant	words	being	retrieved	from	memory.
A
.	
B
.	</p>
<p>C
.	
3.11.6	
Floating-Point	Comparison
Operations
AVX2	provides	two	instructions	for	comparing	floating-point	values:
Instruction
Based	on
Description</p>
<h2>S
,	
S
S</h2>
<p>S
Compare	single	precision</p>
<h2>S
,	
S
S</h2>
<p>S
Compare	double	precision
These	instructions	are	similar	to	the	
CMP</p>
<p>instructions	(see	
Section	
3.6
),
in	that	they	compare	operands	
S
and	
S
(but	in	the	opposite	order	one
might	expect)	and	set	the	condition	codes	to	indicate	their	relative	values.
As	with	
,	they	follow	the	ATT-format	convention	of	listing	the
1
2
2
1
1
2
2
1
1
2</p>
<p>operands	in	reverse	order.	Argument	
S
must	be	in	an	XMM	register,
while	
S
can	be	either	in	an	XMM	register	or	in	memory.
The	floating-point	comparison	instructions	set	three	condition	codes:	the
zero	flag	
,	the	carry	flag	
,	and	the	parity	flag	
.	We	did	not
document	the	parity	flag	in	
Section	
3.6.1
,	because	it	is	not	commonly
found	in	
GCC
-generated	x86	code.	For	integer	operations,	this	flag	is	set
when	the	most	recent	arithmetic	or	logical	operation	yielded	a	value
where	the	least	significant	byte	has	even	parity	(i.e.,	an	even	number	of
ones	in	the	byte).	For	floating-point	comparisons,	however,	the	flag	is	set
when	either	operand	is	
NaN
.	By	convention,	any	comparison	in	C	is
considered	to	fail	when	one	of	the	arguments	is	
NaN
,	and	this	flag	is
used	to	detect	such	a	condition.	For	example,	even	the	comparison	
yields	0	when	
is	
NaN
.
The	condition	codes	are	set	as	follows:
Ordering	
S
:
S
Unordered
1
1
1
S
&lt;	
S
1
0
0
S
=	
S
0
1
0
S
&gt;	
S
0
0
0
The	
unordered
case	occurs	when	either	operand	is	
NaN
.	This	can	be
detected	with	the	parity	flag.	Commonly,	the	
(for	&quot;jump	on	parity&quot;)
instruction	is	used	to	conditionally	jump	when	a	floating-point	comparison
yields	an	unordered	result.	Except	for	this	case,	the	values	of	the	carry
2
1
2
1
2
1
2
1
2
1</p>
<p>and	zero	flags	are	the	same	as	those	for	an	unsigned	comparison:	
is
set	when	the	two	operands	are	equal,	and	
is
(a)	C	code
(b)	Generated	assembly	code</p>
<p>Figure	
3.51	
Illustration	of	conditional	branching	in	floating-point
code.</p>
<p>set	when	
S
&lt;	
S
.	Instructions	such	as	
and	
are	used	to
conditionally	jump	on	various	combinations	of	these	flags.
As	an	example	of	floating-point	comparisons,	the	C	function	of	
Figure
3.51(a)
classifies	argument	
according	to	its	relation	to	0.0,	returning
an	enumerated	type	as	the	result.	Enumerated	types	in	C	are	encoded	as
integers,	and	so	the	possible	function	values	are:	
,	and	
.	This	final	outcome	occurs	when	the	value	of	
is
NaN
.
G
CC</p>
<p>generates	the	code	shown	in	
Figure	
3.51(b)
for	
.	The
code	is	not	very	efficient—it	compares	
to	0.0	three	times,	even	though
the	required	information	could	be	obtained	with	a	single	comparison.	It
also	generates	floating	point	constant	0.0	twice—once	using	
,	and
once	by	reading	the	value	from	memory.	Let	us	trace	the	flow	of	the
function	for	the	four	possible	comparison	results:
x	&lt;	0.0	The	ja	branch	on	line	4	will	be	taken,	jumping	to	the	end	with	a
return	value	of	0.
x	=	0.0	The	
(line	4)	and	
(line	6)	branches	will	not	be	taken,	but
the	
branch	(line	8)	will,	returning	with	
equal	to	1.
x	&gt;	0.0	None	of	the	three	branches	will	be	taken.	The	set	be	(line	11)
will	yield	0,	and	this	will	be	incremented	by	the	
instruction	(line
13)	to	give	a	return	value	of	2.
x	=	
NaN
The	jp	branch	(line	6)	will	be	taken.	The	third	
instruction	(line	10)	will	set	both	the	carry	and	the	zero	flag,	and	so
the	set	be	instruction	(line	11)	and	the	following	instruction	will	set
2
1</p>
<pre><code>to	1.	This	gets	incremented	by	the	
instruction	(line	13)	to
</code></pre>
<p>give	a	return	value	of	3.
In	Homework	
Problems	
3.73
and	
3.74
,	you	are	challenged	to	hand-
generate	more	efficient	implementations	of	
.
Practice	Problem	
3.57	
(solution	page	
350
)
Function	
has	the	following	prototype:
For	this	function,	
GCC</p>
<p>generates	the	following	code:</p>
<p>Write	a	C	version	of	
.
3.11.7	
Observations	about	Floating-
Point	Code
We	see	that	the	general	style	of	machine	code	generated	for	operating
on	floating-point	data	with	AVX2	is	similar	to	what	we	have	seen	for
operating	on	integer	data.	Both	use	a	collection	of	registers	to	hold	and
operate	on	values,	and	they	use	these	registers	for	passing	function
arguments.
Of	course,	there	are	many	complexities	in	dealing	with	the	different	data
types	and	the	rules	for	evaluating	expressions	containing	a	mixture	of
data	types,	and	AVX2	code	involves	many	more	different	instructions	and
formats	than	is	usually	seen	with	functions	that	perform	only	integer
arithmetic.
AVX2	also	has	the	potential	to	make	computations	run	faster	by
performing	parallel	operations	on	packed	data.	Compiler	developers	are</p>
<p>working	on	automating	the	conversion	of	scalar	code	to	parallel	code,	but
currently	the	most	reliable	way	to	achieve	higher	performance	through
parallelism	is	to	use	the	extensions	to	the	C	language	supported	by	
GCC
for	manipulating	vectors	of	data.	See	Web	Aside	
OPT
:
SIMD</p>
<p>on	page	546	to
see	how	this	can	be	done.</p>
<p>3.12	
Summary
In	this	chapter,	we	have	peered	beneath	the	layer	of	abstraction	provided
by	the	C	language	to	get	a	view	of	machine-level	programming.	By
having	the	compiler	generate	an	assembly-code	representation	of	the
machine-level	program,	we	gain	insights	into	both	the	compiler	and	its
optimization	capabilities,	along	with	the	machine,	its	data	types,	and	its
instruction	set.	In	
Chapter	
5
,	we	will	see	that	knowing	the
characteristics	of	a	compiler	can	help	when	trying	to	write	programs	that
have	efficient	mappings	onto	the	machine.	We	have	also	gotten	amore
complete	picture	of	how	the	program	stores	data	in	different	memory
regions.	In	
Chapter	
12
,	we	will	see	many	examples	where	application
programmers	need	to	know	whether	a	program	variable	is	on	the	run-
time	stack,	in	some	dynamically	allocated	data	structure,	or	part	of	the
global	program	data.	Understanding	how	programs	map	onto	machines
makes	it	easier	to	understand	the	differences	between	these	kinds	of
storage.
Machine-level	programs,	and	their	representation	by	assembly	code,
differ	in	many	ways	from	C	programs.	There	is	minimal	distinction
between	different	data	types.	The	program	is	expressed	as	a	sequence
of	instructions,	each	of	which	performs	a	single	operation.	Parts	of	the
program	state,	such	as	registers	and	the	run-time	stack,	are	directly
visible	to	the	programmer.	Only	low-level	operations	are	provided	to
support	data	manipulation	and	program	control.	The	compiler	must	use
multiple	instructions	to	generate	and	operate	on	different	data	structures
and	to	implement	control	constructs	such	as	conditionals,	loops,	and</p>
<p>procedures.	We	have	covered	many	different	aspects	of	C	and	how	it
gets	compiled.	We	have	seen	that	the	lack	of	bounds	checking	in	C
makes	many	programs	prone	to	buffer	overflows.	This	has	made	many
systems	vulnerable	to	attacks	by	malicious	intruders,	although	recent
safeguards	provided	by	the	run-time	system	and	the	compiler	help	make
programs	more	secure.
We	have	only	examined	the	mapping	of	C	onto	x86-64,	but	much	of	what
we	have	covered	is	handled	in	a	similar	way	for	other	combinations	of
language	and	machine.	For	example,	compiling	C++	is	very	similar	to
compiling	C.	In	fact,	early	implementations	of	C++	first	performed	a
source-to-source	conversion	from	C++	to	C	and	generated	object	code
by	running	a	C	compiler	on	the	result.	C++	objects	are	represented	by
structures,	similar	to	a	C	
.	Methods	are	represented	by	pointers	to
the	code	implementing	the	methods.	By	contrast,	Java	is	implemented	in
an	entirely	different	fashion.	The	object	code	of	Java	is	a	special	binary
representation	known	as	
Java	byte	code
.	This	code	can	be	viewed	as	a
machine-level	program	for	a	
virtual	machine
.	As	its	name	suggests,	this
machine	is	not	implemented	directly	in	hardware.	Instead,	software
interpreters	process	the	byte	code,	simulating	the	behavior	of	the	virtual
machine.	Alternatively,	an	approach	known	as	
just-in-time	compilation
dynamically	translates	byte	code	sequences	into	machine	instructions.
This	approach	provides	faster	execution	when	code	is	executed	multiple
times,	such	as	in	loops.	The	advantage	of	using	byte	code	as	the	low-
level	representation	of	a	program	is	that	the	same	code	can	be
&quot;executed&quot;	on	many	different	machines,	whereas	the	machine	code	we
have	considered	runs	only	on	x86-64	machines.</p>
<p>Bibliographic	Notes
Both	Intel	and	AMD	provide	extensive	documentation	on	their
processors.	This	includes	general	descriptions	of	an	assembly-language
programmer's	view	of	the	hardware	
[2,</p>
<p>50]
,	as	well	as	detailed
references	about	the	individual	instructions	
[3,</p>
<p>51]
.	Reading	the
instruction	descriptions	is	complicated	by	the	facts	that	(1)	all
documentation	is	based	on	the	Intel	assembly-code	format,	(2)	there	are
many	variations	for	each	instruction	due	to	the	different	addressing	and
execution	modes,	and	(3)	there	are	no	illustrative	examples.	Still,	these
remain	the	authoritative	references	about	the	behavior	of	each
instruction.
The	organization	x86-64.org	has	been	responsible	for	defining	the
application	binary	interface
(ABI)	for	x86-64	code	running	on	Linux
systems	
[77]
.	This	interface	describes	details	for	procedure	linkages,
binary	code	files,	and	a	number	of	other	features	that	are	required	for
machine-code	programs	to	execute	properly.
As	we	have	discussed,	the	ATT	format	used	by	
GCC</p>
<p>is	very	different	from
the	Intel	format	used	in	Intel	documentation	and	by	other	compilers
(including	the	Microsoft	compilers).
Muchnick's	book	on	compiler	design	
[80]
is	considered	the	most
comprehensive	reference	on	code-optimization	techniques.	It	covers
many	of	the	techniques	we	discuss	here,	such	as	register	usage
conventions.</p>
<p>Much	has	been	written	about	the	use	of	buffer	overflow	to	attack	systems
over	the	Internet.	Detailed	analyses	of	the	1988	Internet	worm	have	been
published	by	Spafford	
[105]
as	well	as	by	members	of	the	team	at	MIT
who	helped	stop	its	spread	
[35]
.	Since	then	a	number	of	papers	and
projects	have	generated	ways	both	to	create	and	to	prevent	buffer
overflow	attacks.	Seacord's	book	
[97]
provides	a	wealth	of	information
about	buffer	overflow	and	other	attacks	on	code	generated	by	C
compilers.</p>
<p>Homework	Problems
3.58
For	a	function	with	prototype
GCC</p>
<p>generates	the	following	assembly	code:
Parameters	
,	and	
are	passed	in	registers	
,	and
.	The	code	stores	the	return	value	in	register	
.</p>
<p>Write	C	code	for	
that	will	have	an	effect	equivalent	to	the
assembly	code	shown.
3.59
The	following	code	computes	the	128-bit	product	of	two	64-bit
signed	values	
x
and	
y
and	stores	the	result	in	memory:
G
CC</p>
<p>generates	the	following	assembly	code	implementing	the
computation:</p>
<p>This	code	uses	three	multiplications	for	the	multiprecision
arithmetic	required	to	implement	128-bit	arithmetic	on	a	64-bit
machine.	Describe	the	algorithm	used	to	compute	the	product,	and
annotate	the	assembly	code	to	show	how	it	realizes	your
algorithm.	
Hint:
When	extending	arguments	of	
x
and	
y
to	128	bits,
they	can	be	rewritten	as	
x
=	2
·	
x
+	
x
and	
y
=	2
·	
y
+	
y
,	where
x
,	
x
,	
y
,	and	
y
are	64-bit	values.	Similarly,	the	128-bit	product	can
be	written	as	
p
=	2
·	
p
+	
p
,	where	
p
and	
p
are	64-bit	values.
Show	how	the	code	computes	the	values	of	
p
and	
p
in	terms	of
x
,	
x
,	
y
,	and	
y
.
3.60
Consider	the	following	assembly	code:
64
h
l
64
h
l
h
l
h
l
64
h
l
h
l
h
l
h
l
h
l</p>
<p>The	preceding	code	was	generated	by	compiling	C	code	that	had
the	following	overall	form:
Your	task	is	to	fill	in	the	missing	parts	of	the	C	code	to	get	a
program	equivalent	to	the	generated	assembly	code.	Recall	that
the	result	of	the	function	is	returned	in	register	
.	You	will	find	it</p>
<p>helpful	to	examine	the	assembly	code	before,	during,	and	after	the
loop	to	form	a	consistent	mapping	between	the	registers	and	the
program	variables.
A
.	
Which	registers	hold	program	values	
,	and
B
.	
What	are	the	initial	values	of	
and	
C
.	
What	is	the	test	condition	for	
D
.	
How	does	
get	updated?
E
.	
How	does	
get	updated?
F
.	
Fill	in	all	the	missing	parts	of	the	C	code.
3.61
In	
Section	
3.6.6
,	we	examined	the	following	code	as	a
candidate	for	the	use	of	conditional	data	transfer:
We	showed	a	trial	implementation	using	a	conditional	move
instruction	but	argued	that	it	was	not	valid,	since	it	could	attempt	to
read	from	a	null	address.
Write	a	C	function	
that	has	the	same	behavior	as	
,
except	that	it	can	be	compiled	to	use	conditional	data	transfer.</p>
<p>When	compiled,	the	generated	code	should	use	a	conditional
move	instruction	rather	than	one	of	the	jump	instructions.
3.62
The	code	that	follows	shows	an	example	of	branching	on	an
enumerated	type	value	in	a	switch	statement.	Recall	that
enumerated	types	in	C	are	simply	a	way	to	introduce	a	set	of
names	having	associated	integer	values.	By	default,	the	values
assigned	to	the	names	count	from	zero	upward.	In	our	code,	the
actions	associated	with	the	different	case	labels	have	been
omitted.</p>
<p>The	part	of	the	generated	assembly	code	implementing	the
different	actions	is	shown	in	
Figure	
3.52
.	The	annotations
indicate	the	argument	locations,	the	register	values,	and	the	case
labels	for	the	different	jump	destinations.
Fill	in	the	missing	parts	of	the	C	code.	It	contained	one	case	that
fell	through	to	another—try	to	reconstruct	this.
3.63
This	problem	will	give	you	a	chance	to	reverse	engineer	a	
statement	from	disassembled	machine	code.	In	the	following
procedure,	the	body	of	the	
statement	has	been	omitted:</p>
<p>Figure	
3.52	
Assembly	code	for	
Problem	
3.62
.
This	code	implements	the	different	branches	of	a	switch
statement.
Figure	
3.53
shows	the	disassembled	machine	code	for	the
procedure.
The	jump	table	resides	in	a	different	area	of	memory.	We	can	see
from	the	indirect	jump	on	line	5	that	the	jump	table	begins	at
address	
.	Using	the	
GDB	
debugger,	we	can	examine	the
six	8-byte	words	of	memory	comprising	the	jump	table	with	the
command	</p>
<p>GDB	
prints	the	following:
Fill	in	the	body	of	the	switch	statement	with	C	code	that	will	have
the	same	behavior	as	the	machine	code.</p>
<p>Figure	
3.53	
Disassembled	code	for	
Problem	
3.63
.
3.64
Consider	the	following	source	code,	where	
R
,	
S
,	and	
T
are
constants	declared	with	
:</p>
<p>In	compiling	this	program,	
GCC</p>
<p>generates	the	following	assembly
code:</p>
<p>A
.	
Extend	
Equation	
3.1
from	two	dimensions	to	three	to
provide	a	formula	for	the	location	of	array	element	
.
B
.	
Use	your	reverse	engineering	skills	to	determine	the	values
of	
R
,	
S
,	and	
T
based	on	the	assembly	code.
3.65
The	following	code	transposes	the	elements	of	an	
M
×	
M
array,
where	
M
is	a	constant	defined	by	
:
When	compiled	with	optimization	level	–01,	
GCC</p>
<p>generates	the
following	code	for	the	inner	loop	of	the	function:</p>
<p>We	can	see	that	
GCC</p>
<p>has	converted	the	array	indexing	to	pointer
code.
A
.	
Which	register	holds	a	pointer	to	array	element	
B
.	
Which	register	holds	a	pointer	to	array	element	
C
.	
What	is	the	value	of	
M
?
3.66
Consider	the	following	source	code,	where	
and	
are	macro
expressions	declared	with	
that	compute	the	dimensions	of
array	A	in	terms	of	parameter	
n
.	This	code	computes	the	sum	of
the	elements	of	column	
j
of	the	array.</p>
<p>In	compiling	this	program,	
GCC</p>
<p>generates	the	following	assembly
code:</p>
<p>Use	your	reverse	engineering	skills	to	determine	the	definitions	of
and	
.
3.67
For	this	exercise,	we	will	examine	the	code	generated	by	
GCC</p>
<p>for
functions	that	have	structures	as	arguments	and	return	values,
and	from	this	see	how	these	language	features	are	typically
implemented.
The	following	C	code	has	a	function	process	having	structures	as
argument	and	return	values,	and	a	function	eval	that	calls	process:</p>
<p>G
CC</p>
<p>generates	the	following	code	for	these	two	functions:</p>
<p>A
.	
We	can	see	on	line	2	of	function	eval	that	it	allocates	104
bytes	on	the	stack.	Diagram	the	stack	frame	for	
,
showing	the	values	that	it	stores	on	the	stack	prior	to	calling
.
B
.	
What	value	does	
pass	in	its	call	to	
C
.	
How	does	the	code	for	
access	the	elements	of
structure	arguments?
D
.	
How	does	the	code	for	
set	the	fields	of	result
structure	
E
.	
Complete	your	diagram	of	the	stack	frame	for	
,
showing	how	
accesses	the	elements	of	structure	
following	the	return	from	process.</p>
<p>F
.	
What	general	principles	can	you	discern	about	how
structure	values	are	passed	as	function	arguments	and	how
they	are	returned	as	function	results?
3.68
In	the	following	code,	
A
and	
B
are	constants	defined	with	</p>
<p>G
CC</p>
<p>generates	the	following	code	for	
What	are	the	values	of	
A
and	
B
?	(The	solution	is	unique.)
3.69
You	are	charged	with	maintaining	a	large	C	program,	and	you
come	across	the	following	code:</p>
<p>The	declarations	of	the	compile-time	constant	
and	the
structure	
are	in	a	file	for	which	you	do	not	have	the
necessary	access	privilege.	Fortunately,	you	have	a	copy	of	the
version	of	code,	which	you	are	able	to	disassemble	with	the
OBJDUMP</p>
<p>program,	yielding	the	following	disassembly:
Using	your	reverse	engineering	skills,	deduce	the	following:
A
.	
The	value	of	
B
.	
A	complete	declaration	of	structure	
Assume	that
the	only	fields	in	this	structure	are	
and	
,	and	that	both
of	these	contain	signed	values.</p>
<p>3.70
Consider	the	following	union	declaration:
This	declaration	illustrates	that	structures	can	be	embedded	within
unions.
The	following	function	(with	some	expressions	omitted)	operates
on	a	linked	list	having	these	unions	as	list	elements:
A
.	
What	are	the	offsets	(in	bytes)	of	the	following	fields:</p>
<p>B
.	
How	many	total	bytes	does	the	structure	require?
C
.	
The	compiler	generates	the	following	assembly	code	for
On	the	basis	of	this	information,	fill	in	the	missing
expressions	in	the	code	for	</p>
<p>Hint:
Some	union
references	can	have	ambiguous	interpretations.	These
ambiguities	get	resolved	as	you	see	where	the	references
lead.	There	
is	only	one	answer	that	does	not	perform	any
casting	and	does	not	violate	any	type	constraints.</p>
<p>3.71
Write	a	function	
that	reads	a	line	from	standard	input
and	writes	it	to	standard	output.	Your	implementation	should	work
for	an	input	line	of	arbitrary	length.	You	may	use	the	library
function	
,	but	you	must	make	sure	your	function	works
correctly	even	when	the	input	line	requires	more	space	than	you
have	allocated	for	your	buffer.	Your	code	should	also	check	for
error	conditions	and	return	when	one	is	encountered.	Refer	to	the
definitions	of	the	standard	I/O	functions	for	documentation	
[45,
61]
.
3.72
Figure	
3.54(a)
shows	the	code	for	a	function	that	is	similar	to
function	
(
Figure	
3.43(a)
).	We	used	
to	illustrate
the	use	of	a	frame	pointer	in	managing	variable-size	stack	frames.
The	new	function	
allocates	space	for	local
(a)	C	code</p>
<p>(b)	Portions	of	generated	assembly	code
⋮
Figure	
3.54	
Code	for	
Problem	
3.72
.
This	function	is	similar	to	that	of	
Figure	
3.43
.
array	p	by	calling	library	function	
.	This	function	is	similar	to
the	more	commonly	used	function	malloc,	except	that	it	allocates
space	on	the	run-time	stack.	The	space	is	automatically
deallocated	when	the	executing	procedure	returns.</p>
<p>Figure	
3.54(b)
shows	the	part	of	the	assembly	code	that	sets
up	the	frame	pointer	and	allocates	space	for	local	variables	
and
.	It	is	very	similar	to	the	corresponding	code	for	
.	Let	us
use	the	same	notation	as	in	
Problem	
3.49
:	The	stack	pointer	is
set	to	values	
s
at	line	4	and	
s
at	line	7.	The	start	address	of	array
is	set	to	value	
p
at	line	9.	Extra	space	
e
may	arise	between	
s
and	
p
,	and	extra	space	
e
may	arise	between	the	end	of	array	
and	
s
.
A
.	
Explain,	in	mathematical	terms,	the	logic	in	the	computation
of	
s
.
B
.	
Explain,	in	mathematical	terms,	the	logic	in	the	computation
of	
p
.
C
.	
Find	values	of	
n
and	
s
that	lead	to	minimum	and	maximum
values	of	
e
.
D
.	
What	alignment	properties	does	this	code	guarantee	for	the
values	of	
s
and	
p
?
3.73
Write	a	function	in	assembly	code	that	matches	the	behavior	of	the
function	
in	
Figure	
3.51
.	Your	code	should	contain
only	one	floating-point	comparison	instruction,	and	then	it	should
use	conditional	branches	to	generate	the	correct	result.	Test	your
code	on	all	2
possible	argument	values.	Web	Aside	
ASM:EASM
on	page	178	describes	how	to	incorporate	functions	written	in
assembly	code	into	C	programs.
1
2
2
2
1
1
2
1
1
2
32</p>
<p>3.74
Write	a	function	in	assembly	code	that	matches	the	behavior	of	the
function	
in	
Figure	
3.51
.	Your	code	should	contain
only	one	floating-point	comparison	instruction,	and	then	it	should
use	conditional	moves	to	generate	the	correct	result.	You	might
want	to	make	use	of	the	instruction	
(move	if	even	parity).
Test	your	code	on	all	2
possible	argument	values.	Web	Aside
ASM:EASM	
on	page	178	describes	how	to	incorporate	functions
written	in	assembly	code	into	C	programs.
3.75
ISO	C99	includes	extensions	to	support	complex	numbers.	Any
floating-point	type	can	be	modified	with	the	keyword	complex.
Here	are	some	sample	functions	that	work	with	complex	data	and
that	call	some	of	the	associated	library	functions:
32</p>
<p>When	compiled,	
GCC</p>
<p>generates	the	following	assembly	code	for
these	functions:
Based	on	these	examples,	determine	the	following:
A
.	
How	are	complex	arguments	passed	to	a	function?
B
.	
How	are	complex	values	returned	from	a	function?</p>
<p>Solutions	to	Practice	Problems
Solution	to	Problem	
3.1	
(page
182
)
This	exercise	gives	you	practice	with	the	different	operand	forms.
Operand
Value
Comment
Register
Absolute	address
Immediate
Address	
Address	
Address	
Address	
Address	
Address	</p>
<p>Solution	to	Problem	
3.2	
(page
185
)
As	we	have	seen,	the	assembly	code	generated	by	
GCC</p>
<p>includes
suffixes	on	the	instructions,	while	the	disassembler	does	not.	Being
able	to	switch	between	these	
two	forms	is	an	important	skill	to	learn.
One	important	feature	is	that	memory	references	in	x86-64	are	always
given	with	quad	word	registers,	such	as	
,	even	if	the	operand	is	a
byte,	single	word,	or	double	word.
Here	is	the	code	written	with	suffixes:
Solution	to	Problem	
3.3	
(page
186
)</p>
<p>Since	we	will	rely	on	
GCC</p>
<p>to	generate	most	of	our	assembly	code,
being	able	to	write	correct	assembly	code	is	not	a	critical	skill.
Nonetheless,	this	exercise	will	help	you	become	more	familiar	with	the
different	instruction	and	operand	types.
Here	is	the	code	with	explanations	of	the	errors:
Solution	to	Problem	
3.4	
(page
187
)
This	exercise	gives	you	more	experience	with	the	different	data
movement	instructions	and	how	they	relate	to	the	data	types	and
conversion	rules	of	C.	The	nuances	of	conversions	of	both</p>
<p>signedness	and	size,	as	well	as	integral	promotion,	add	challenge	to
this	problem.
Instruction
Comments
Read	8	bytes
Store	8	bytes
Convert	char	to	int
Store	4	bytes
Convert	char	to	int
Store	4	bytes
Read	byte	and	zero-
extend
Store	8	bytes
Read	4	bytes
Store	low-order	byte
Read	4	bytes
Store	low-order	byte
Read	byte	and	sign-
extend
Store	2	bytes</p>
<p>Solution	to	Problem	
3.5	
(page
189
)
Reverse	engineering	is	a	good	way	to	understand	systems.	In	this
case,	we	want	to	reverse	the	effect	of	the	C	compiler	to	determine
what	C	code	gave	rise	to	this	assembly	code.	The	best	way	is	to	run	a
&quot;simulation,&quot;	starting	with	values	
,	and	
at	the	locations
designated	by	pointers	
,	and	
,	respectively.	We	would	then
get	the	following	behavior:
From	this,	we	can	generate	the	following	C	code:</p>
<p>Solution	to	Problem	
3.6	
(page
192
)
This	exercise	demonstrates	the	versatility	of	the	
instruction	and
gives	you	more	practice	in	deciphering	the	different	operand	forms.
Although	the	operand	forms	are	classified	as	type	&quot;Memory&quot;	in	
Figure
3.3
,	no	memory	access	occurs.
Instruction
Result
6+
x
x
+
y
x
+	4
y
7	+	9
x</p>
<p>10	+	4
y
9	+
x
+	2
y
Solution	to	Problem	
3.7	
(page
193
)
Again,	reverse	engineering	proves	to	be	a	useful	way	to	learn	the
relationship	between	C	code	and	the	generated	assembly	code.
The	best	way	to	solve	problems	of	this	type	is	to	annotate	the	lines	of
assembly	code	with	information	about	the	operations	being
performed.	Here	is	a	sample:
From	this,	it	is	easy	to	generate	the	missing	expression:</p>
<p>Solution	to	Problem	
3.8	
(page
194
)
This	problem	gives	you	a	chance	to	test	your	understanding	of
operands	and	the	arithmetic	instructions.	The	instruction	sequence	is
designed	so	that	the	result	of	each	instruction	does	not	affect	the
behavior	of	subsequent	ones.
Instruction
Destination
Value
Solution	to	Problem	
3.9	
(page
195
)</p>
<p>This	exercise	gives	you	a	chance	to	generate	a	little	bit	of	assembly
code.	The	solution	code	was	generated	by	
GCC
.	By	loading	parameter
in	register	
,	it	can	then	use	byte	register	
to	specify	the	shift
amount	for	the	
instruction.	It	might	seem	odd	to	use	a	
instruction,	given	that	
is	eight	bytes	long,	but	keep	in	mind	that	only
the	least	significant	byte	is	required	to	specify	the	shift	amount.
Solution	to	Problem	
3.10	
(page
196
)
This	problem	is	fairly	straightforward,	since	the	assembly	code	follows
the	structure	of	the	C	code	closely.</p>
<p>Solution	to	Problem	
3.11	
(page
197
)
A
.	
This	instruction	is	used	to	set	register	
to	zero,	exploiting	the
property	that	
x
^	
x
=	0	for	any	
x
.	It	corresponds	to	the	C	statement
=	0.
B
.	
A	more	direct	way	of	setting	register	
to	zero	is	with	the
instruction	
C
.	
Assembling	and	disassembling	this	code,	however,	we	find	that
the	version	with	
requires	only	3	bytes,	while	the	version	with
requires	7.	Other	ways	to	set	
to	zero	rely	on	the
property	that	any	instruction	that	updates	the	lower	4	bytes	will
cause	the	high-order	bytes	to	be	set	to	zero.	Thus,	we	could	use
either	
(2	bytes)	or	
(5	bytes).
Solution	to	Problem	
3.12	
(page
200
)
We	can	simply	replace	the	
instruction	with	one	that	sets	register
to	
,	and	use	
rather	than	
as	our	division</p>
<p>instruction,	yielding	the	following	code:
Solution	to	Problem	
3.13	
(page
204
)
It	is	important	to	understand	that	assembly	code	does	not	keep	track
of	the	type	of	a	program	value.	Instead,	the	different	instructions
determine	the	operand	sizes	and	whether	they	are	signed	or
unsigned.	When	mapping	from	instruction	sequences	back	to	C	code,
we	must	do	a	bit	of	detective	work	to	infer	the	data	types	of	the
program	values.</p>
<p>A
.	
The	suffix	<code> '	and	the	register	identifiers	indicate	32-bit operands,	while	the	comparison	is	for	a	two's-complement	&lt;. We	can	infer	that	 must	be	 B .	 The	suffix	</code>
'	and	the	register	identifiers	indicate	16-bit
operands,	while	the	comparison	is	for	a	two's-complement	&gt;=.
We	can	infer	that	
must	be	
C
.	
The	suffix	<code> '	and	the	register	identifiers	indicate	8-bit operands,	while	the	comparison	is	for	an	unsigned	&lt;=.	We	can infer	that	 must	be	 D .	 The	suffix	</code>
'	and	the	register	identifiers	indicate	64-bit
operands,	while	the	comparison	is	for	!=,	which	is	the	same
whether	the	arguments	are	signed,	unsigned,	or	pointers.	We
can	infer	that	
could	be	either	
,	or
some	form	of	pointer.
Solution	to	Problem	
3.14	
(page
205
)
This	problem	is	similar	to	
Problem	
3.13
,	except	that	it	involves	
TEST
instructions	rather	than	
CMP</p>
<p>instructions.
A
.	
The	suffix	`
'	and	the	register	identifiers	indicate	a	64-bit
operand,	while	the	comparison	is	for	&gt;=,	which	must	be	signed.
We	can	infer	that	
must	be	
.</p>
<p>B
.	
The	suffix	<code> '	and	the	register	identifier	indicate	a	16-bit operand,	while	the	comparison	is	for	==,	which	is	the	same	for signed	or	unsigned.	We	can	infer	that	 must	be	either C .	 The	suffix	</code>
'	and	the	register	identifier	indicate	an	8-bit
operand,	while	the	comparison	is	for	unsigned	&gt;.	We	can	infer
that	
must	be	
D
.	
The	suffix	`
'	and	the	register	identifier	indicate	32-bit
operands,	while	the	comparison	is	for	&lt;.	We	can	infer	that
must	be	
Solution	to	Problem	
3.15	
(page
209
)
This	exercise	requires	you	to	examine	disassembled	code	in	detail
and	reason	about	the	encodings	for	jump	targets.	It	also	gives	you
practice	in	hexadecimal	arithmetic.
A
.	
The	
instruction	has	as	its	target	
As	the
original	disassembled	code	shows,	this	is	</p>
<p>B
.	
The	
instruction	has	as	its	target	
–	12	(since
is	the	1-byte	two's-complement	representation	of	–	12).
As	the	original	disassembled	code	shows,	this	is	
C
.	
According	to	the	annotation	produced	by	the	disassembler,	the
jump	target	is	at	absolute	address	
.	According	to	the
byte	encoding,	this	must	be	at	an	address	
bytes	beyond
that	of	the	pop	instruction.	Subtracting	these	gives	address
.	Noting	that	the	encoding	of	the	
instruction
requires	2	bytes,	it	must	be	located	at	address	
.	These
are	confirmed	by	examining	the	original	disassembly:
D
.	
Reading	the	bytes	in	reverse	order,	we	can	see	that	the	target
offset	is	
,	or	decimal	-141.	Adding	this	to	
(the	address	of	the	nop	instruction)	gives	address	</p>
<p>Solution	to	Problem	
3.16	
(page
212
)
Annotating	assembly	code	and	writing	C	code	that	mimics	its	control
flow	are	good	first	steps	in	understanding	assembly-language
programs.	This	problem	gives	you	practice	for	an	example	with	simple
control	flow.	It	also	gives	you	a	chance	to	examine	the	implementation
of	logical	operations.
A
.	
Here	is	the	C	code:
B
.	
The	first	conditional	branch	is	part	of	the	implementation	of	the
&amp;&amp;	expression.	If	the	test	for	
being	non-null	fails,	the	code
will	skip	the	test	of	a	&gt;	*p.</p>
<p>Solution	to	Problem	
3.17	
(page
212
)
This	is	an	exercise	to	help	you	think	about	the	idea	of	a	general
translation	rule	and	how	to	apply	it.
A
.	
Converting	to	this	alternate	form	involves	only	switching	around
a	few	lines	of	the	code:
B
.	
In	most	respects,	the	choice	is	arbitrary.	But	the	original	rule
works	better	for	the	common	case	where	there	is	no	else
statement.	For	this	case,	we	can	simply	modify	the	translation
rule	to	be	as	follows:</p>
<p>A	translation	based	on	the	alternate	rule	is	more	cumbersome.
Solution	to	Problem	
3.18	
(page
213
)
This	problem	requires	that	you	work	through	a	nested	branch
structure,	where	you	will	see	how	our	rule	for	translating	
statements	has	been	applied.	On	the	whole,	the	machine	code	is	a
straightforward	translation	of	the	C	code.</p>
<p>Solution	to	Problem	
3.19	
(page
216
)
This	problem	reinforces	our	method	of	computing	the	misprediction
penalty.
A
.	
We	can	apply	our	formula	directly	to	get	
T
=	2(31	–	16)	=	30.
B
.	
When	misprediction	occurs,	the	function	will	require	around
cycles.
Solution	to	Problem	
3.20	
(page
219
)
This	problem	provides	a	chance	to	study	the	use	of	conditional
moves.
A
.	
The	operator	is	`/'.	We	see	this	is	an	example	of	dividing	by	a
power	of	3	by	right	shifting	(see	
Section	
2.3.7
).	Before
MP
16</p>
<ul>
<li></li>
</ul>
<h1 id="30"><a class="header" href="#30">30</a></h1>
<p>46</p>
<h1>shifting	by	
,	we	must	add	a	bias	of	
when	the
dividend	is	negative.
B
.	
Here	is	an	annotated	version	of	the	assembly	code:
The	program	creates	a	temporary	value	equal	to	
,	in
anticipation	of	
x
being	negative	and	therefore	requiring	biasing.
The	
instruction	conditionally	changes	this	number	to	
x
when	
,	and	then	it	is	shifted	by	3	to	generate	
x
/8.
Solution	to	Problem	
3.21	
(page
219
)
This	problem	is	similar	to	
Problem	
3.18
,	except	that	some	of	the
conditionals	have	been	implemented	by	conditional	data	transfers.
Although	it	might	seem	daunting	to	fit	this	code	into	the	framework	of
k</h1>
<h1>3
2
k
−
1</h1>
<p>7
x</p>
<ul>
<li></li>
</ul>
<p>7
x
≥
0</p>
<p>the	original	C	code,	you	will	find	that	it	follows	the	translation	rules
fairly	closely.
Solution	to	Problem	
3.22	
(page
221
)
A
.	
If	we	build	up	a	table	of	factorials	computed	with	data	type	
,	we
get	the	following:
n
n
!
OK?
1
1
Y
2
2
Y</p>
<p>3
6
Y
4
24
Y
5
120
Y
6
720
Y
7
5,040
Y
8
40,320
Y
9
362,880
Y
10
3,628,800
Y
11
39,916,800
Y
12
479,001,600
Y
13
1,932,053,504
N
We	can	see	that	the	computation	of	13!	has	overflowed.	As	we
learned	in	
Problem	
2.35
,	when	we	get	value	
x
while	attempting
to	compute	
n
!,	we	can	test	for	overflow	by	computing	
x/n
and
seeing	whether	it	equals	(
n
-	1)!	(assuming	that	we	have	already
ensured	that	the	computation	of	(
n
-	1)	!did	not	overflow).	In	this
case	we	get	1,932,053,504/13	=	161,004,458.667.	As	a	second
test,	we	can	see	that	any	factorial	beyond	10!	must	be	a	multiple
of	100	and	therefore	have	zeros	for	the	last	two	digits.	The	correct
value	of	13!	is	6,227,020,800.
B
.	
Doing	the	computation	with	data	type	long	lets	us	go	up	to	20!,
yielding	2,432,902,008,176,640,000.</p>
<p>Solution	to	Problem	
3.23	
(page
222
)
The	code	generated	when	compiling	loops	can	be	tricky	to	analyze,
because	the	compiler	can	perform	many	different	optimizations	on
loop	code,	and	because	it	can	be	difficult	to	match	program	variables
with	registers.	This	particular	example	demonstrates	several	places
where	the	assembly	code	is	not	just	a	direct	translation	of	the	C	code.
A
.	
Although	parameter	
is	passed	to	the	function	in	register
,	we	can	see	that	the	register	is	never	referenced	once	the
loop	is	entered.	Instead,	we	can	see	that	registers	
,
and	
are	initialized	in	lines	2–5	to	
,	and	
.	We	can
conclude,	therefore,	that	these	registers	contain	the	program
variables.
B
.	
The	compiler	determines	that	pointer	
always	points	to	
,	and
hence	the	expression	(*
)++	simply	increments	
.	It	combines
this	incrementing	by	1	with	the	increment	by	
,	via	the	
instruction	of	line	7.
C
.	
The	annotated	code	is	as	follows:</p>
<p>Solution	to	Problem	
3.24	
(page
224
)
This	assembly	code	is	a	fairly	straightforward	translation	of	the	loop
using	the	jump-to-middle	method.	The	full	C	code	is	as	follows:</p>
<p>Solution	to	Problem	
3.25	
(page
226
)
While	the	generated	code	does	not	follow	the	exact	pattern	of	the
guarded-do	translation,	we	can	see	that	it	is	equivalent	to	the
following	C	code:
We	will	often	see	cases,	especially	when	compiling	with	higher	levels
of	optimization,	where	
GCC</p>
<p>takes	some	liberties	in	the	exact	form	of
the	code	it	generates,	while	preserving	the	required	functionality.
Solution	to	Problem	
3.26	
(page</p>
<p>228
)
Being	able	to	work	backward	from	assembly	code	to	C	code	is	a
prime	example	of	reverse	engineering.
A
.	
We	can	see	that	the	code	uses	the	jump-to-middle	translation,
using	the	
instruction	on	line	3.
B
.	
Here	is	the	original	C	code:
C
.	
This	code	computes	the	
parity
of	argument	
.	That	is,	it	returns
1	if	there	is	an	odd	number	of	ones	in	
and	0	if	there	is	an
even	number.
Solution	to	Problem	
3.27	
(page
231
)</p>
<p>This	exercise	is	intended	to	reinforce	your	understanding	of	how	loops
are	implemented.
Solution	to	Problem	
3.28	
(page
231
)
This	problem	is	trickier	than	
Problem	
3.26
,	since	the	code	within
the	loop	is	more	complex	and	the	overall	operation	is	less	familiar.
A
.	
Here	is	the	original	C	code:</p>
<p>B
.	
The	code	was	generated	using	the	guarded-do	transformation,
but	the	compiler	detected	that,	since	
i
is	initialized	to	64,	it	will
satisfy	the	test	
i
≠	0,	and	therefore	the	initial	test	is	not
required.
C
.	
This	code	reverses	the	bits	in	
,	creating	a	mirror	image.	It
does	this	by	shifting	the	bits	of	
from	left	to	right,	and	then
filling	these	bits	in	as	it	shifts	
from	right	to	left.</p>
<p>Solution	to	Problem	
3.29	
(page
232
)
Our	stated	rule	for	translating	a	for	loop	into	a	
loop	is	just	a	bit
too	simplistic—this	is	the	only	aspect	that	requires	special
consideration.
A
.	
Applying	our	translation	rule	would	yield	the	following	code:
This	code	has	an	infinite	loop,	since	the	continue	statement
would	prevent	index	variable	
from	being	updated.</p>
<p>B
.	
The	general	solution	is	to	replace	the	continue	statement	with	a
statement	that	skips	the	rest	of	the	loop	body	and	goes
directly	to	the	update	portion:
Solution	to	Problem	
3.30	
(page
236
)
This	problem	gives	you	a	chance	to	reason	about	the	control	flow	of	a
switch	statement.	Answering	the	questions	requires	you	to	combine
information	from	several	places	in	the	assembly	code.
Line	2	of	the	assembly	code	adds	1	to	
x
to	set	the	lower	range	of
the	cases	to	zero.	That	means	that	the	minimum	case	label	is	–1.</p>
<p>Lines	3	and	4	cause	the	program	to	jump	to	the	default	case	when
the	adjusted	case	value	is	greater	than	8.	This	implies	that	the
maximum	case	label	is	–1	+	8	=	7.
In	the	jump	table,	we	see	that	the	entry	on	lines	6	(case	value	3)
and	9	(case	value	6)	have	the	same	destination	(
)	as	the	jump
instruction	on	line	4,	indicating	the	default	case	behavior.	Thus,
case	labels	3	and	5	are	missing	in	the	switch	statement	body.
In	the	jump	table,	we	see	that	the	entries	on	lines	3	and	10	have
the	same	destination.	These	correspond	to	cases	0	and	7.
In	the	jump	table,	we	see	that	the	entries	on	lines	5	and	7	have	the
same	destination.	These	correspond	to	cases	2	and	4.
From	this	reasoning,	we	draw	the	following	conclusions:
A
.	
The	case	labels	in	the	switch	statement	body	have	values	–1,
0,	1,	2,	4,	5,	and	7.
B
.	
The	case	with	destination	
has	labels	0	and	7.
C
.	
The	case	with	destination	
has	labels	2	and	4.
Solution	to	Problem	
3.31	
(page
237
)
The	key	to	reverse	engineering	compiled	switch	statements	is	to
combine	the	information	from	the	assembly	code	and	the	jump	table
to	sort	out	the	different	cases.	We	can	see	from	the	
instruction</p>
<p>(line	3)	that	the	code	for	the	default	case	has	label	
We	can	see
that	the	only	other	repeated	label	in	the	jump	table	is	
,	and	so	this
must	be	the	code	for	the	cases	C	and	D.	We	can	see	that	the	code
falls	through	at	line	8,	and	so	label	
must	match	case	A	and	label
must	match	case	B.	That	leaves	only	label	
to	match	case	E.
The	original	C	code	is	as	follows:</p>
<p>Solution	to	Problem	
3.32	
(page
244
)
Tracing	through	the	program	execution	at	this	level	of	detail	reinforces
many	aspects	of	procedure	call	and	return.	We	can	see	clearly	how
control	is	passed	to	the	function	when	it	is	called,	and	how	the	calling
function	resumes	upon	return.	We	can	also	see	how	arguments	get
passed	through	registers	
and	
,	and	how	results	are	returned
via	register	
.
Instruction
State	values	(at	beginning)
Label
PC
Instruction
M1
10
—
—
0x7fffffffe820
—
F1
10
—
—
0x7fffffffe818
F2
10
11
—
0x7fffffffe818
F3
9
11
—
0x7fffffffe818
L1
9
11
—
0x7fffffffe810
L2
9
11
9
0x7fffffffe810</p>
<p>L3
9
11
99
0x7fffffffe810
F4
9
11
99
0x7fffffffe818
M2
9
11
99
0x7fffffffe820
—
Solution	to	Problem	
3.33	
(page
246
)
This	problem	is	a	bit	tricky	due	to	the	mixing	of	different	data	sizes.
Let	us	first	describe	one	answer	and	then	explain	the	second
possibility.	If	we	assume	the	first	addition	(line	3)	implements	*
+=	
,
while	the	second	(line	4)	implements	
,	then	we	can	see	that	a
was	passed	as	the	first	argument	in	
and	converted	from	4	bytes
to	8	before	adding	it	to	the	8	bytes	pointed	to	by	
.	This	implies
that	a	must	be	of	type	
and	
must	be	of	type	
*.	We	can	also
see	that	the	low-order	byte	of	argument	
is	added	to	the	byte
pointed	to	by	
.	This	implies	that	
must	be	of	type	
,	but	the
type	of	
is	ambiguous—it	could	be	1,	2,	4,	or	8	bytes	long.	This
ambiguity	is	resolved	by	noting	the	return	value	of	
6,	computed	as	the
sum	of	the	sizes	of	
and	
.	Since	we	know	a	is	4	bytes	long,	we	can
deduce	that	
must	be	2.
An	annotated	version	of	this	function	explains	these	details:</p>
<p>Alternatively,	we	can	see	that	the	same	assembly	code	would	be	valid
if	the	two	sums	were	computed	in	the	assembly	code	in	the	opposite
ordering	as	they	are	in	the	C	code.	This	would	result	in	interchanging
arguments	
and	
and	arguments	
and	
,	yielding	the	following
prototype:
Solution	to	Problem	
3.34	
(page
252
)
This	example	demonstrates	the	use	of	callee-saved	registers	as	well
as	the	stack	for	holding	local	data.</p>
<p>A
.	
We	can	see	that	lines	9-14	save	local	values	
into	callee-
saved	registers	
,	and	
,
respectively.
B
.	
Local	values	
and	
are	stored	on	the	stack	at	offsets	0
and	8	relative	to	the	stack	pointer	(lines	16	and	18).
C
.	
After	storing	six	local	variables,	the	program	has	used	up	the
supply	of	callee-saved	registers.	It	stores	the	remaining	two
local	values	on	the	stack.
Solution	to	Problem	
3.35	
(page
254
)
This	problem	provides	a	chance	to	examine	the	code	for	a	recursive
function.	An	important	lesson	to	learn	is	that	recursive	code	has	the
exact	same	structure	as	the	other	functions	we	have	seen.	The	stack
and	register-saving	disciplines	suffice	to	make	recursive	functions
operate	correctly.
A
.	
Register	
holds	the	value	of	parameter	
,	so	that	it	can	be
used	to	compute	the	result	expression.
B
.	
The	assembly	code	was	generated	from	the	following	C	code:</p>
<p>Solution	to	Problem	
3.36	
(page
256
)
This	exercise	tests	your	understanding	of	data	sizes	and	array
indexing.	Observe	that	a	pointer	of	any	kind	is	8	bytes	long.	Data	type
short	requires	2	bytes,	while	
requires	4.
Array
Element	size
Total	size
Start	address
Element	
i
2
14
x
x
+	2
i
8
24
x
x
+	8
i
8
48
x
x
+8
i
4
32
x
x
+	4
i
8
32
x
x
+	8
i
Solution	to	Problem	
3.37	
(page
S
S
T
T
U
U
V
V
W
W</p>
<p>258
)
This	problem	is	a	variant	of	the	one	shown	for	integer	array	E.	It	is
important	to	understand	the	difference	between	a	pointer	and	the
object	being	pointed	to.	Since	data	type	short	requires	2	bytes,	all	of
the	array	indices	are	scaled	by	a	factor	of	2.	Rather	than	using	
,
as	before,	we	now	use	
.
Expression
Type
Value
Assembly
Solution	to	Problem	
3.38	
(page
259
)
This	problem	requires	you	to	work	through	the	scaling	operations	to
determine	the	address	computations,	and	to	apply	
Equation	
3.1
for
row-major	indexing.	The	first	step	is	to	annotate	the	assembly	code	to
determine	how	the	address	references	are	computed:</p>
<p>We	can	see	that	the	reference	to	matrix	
is	at	byte	offset	8	·	(7
i
+	
j
),
while	the	reference	to	matrix	
is	at	byte	offset	8	·	(5
j
+	
i
).	From	this,
we	can	determine	that	
has	7	columns,	while	
has	5,	giving	
M
=	5
and	
N
=	7.
Solution	to	Problem	
3.39	
(page
262
)
These	computations	are	direct	applications	of	
Equation	
3.1
:
For	
L
=	4,	
C
=	16,	and	
j
=	0,	pointer	
is	computed	as	
x
+	4	·
(16
i
+	0)	=	
x
+	64
i
.
A
A</p>
<p>For	
L
=	4,	
C
=	16,	
i
=	0,	and	
j
=	
k
,	Bptr	is	computed	as	
x
+	4	·	(16	·
0	+	
k
)	=	
x
+	4
k
.
For	
L
=	4,	
C
=	16,	
i
=	16,	and	
j
=	
k
,	Bend	is	computed	as	
x
+	4	·
(16	·	16	+	
k
)	=	
x
+	1,024	+	4
k
.
Solution	to	Problem	
3.40	
(page
262
)
This	exercise	requires	that	you	be	able	to	study	compiler-generated
assembly	code	to	understand	what	optimizations	have	been
performed.	In	this	case,	the	compiler	was	clever	in	its	optimizations.
Let	us	first	study	the	following	C	code,	and	then	see	how	it	is	derived
from	the	assembly	code	generated	for	the	original	function.
B
B
B
B</p>
<p>This	function	introduces	a	variable	Abase,	of	type	
*,	pointing	to
the	start	of	array	A.	This	pointer	designates	a	sequence	of	4-byte
integers	consisting	of	elements	of	A	in	row-major	order.	We	introduce
an	integer	variable	index	that	steps	through	the	diagonal	elements	of
A,	with	the	property	that	diagonal	elements	
i
and	
i
+	1	are	spaced	
N
+
1	elements	apart	in	the	sequence,	and	that	once	we	reach	diagonal
element	
N
(index	value	
N(N
+	1)),	we	have	gone	beyond	the	end.
The	actual	assembly	code	follows	this	general	form,	but	now	the
pointer	increments	must	be	scaled	by	a	factor	of	4.	We	label	register
as	holding	a	value	
equal	to	index	in	our	C	version	but
scaled	by	a	factor	of	4.	For	
N
=	16,	we	can	see	that	our	stopping	point
for	
will	be	4.	16(16	+	1)	=	1,088.</p>
<p>Solution	to	Problem	
3.41	
(page
268
)
This	problem	gets	you	to	think	about	structure	layout	and	the	code
used	to	access	structure	fields.	The	structure	declaration	is	a	variant
of	the	example	shown	in	the	text.	It	shows	that	nested	structures	are
allocated	by	embedding	the	inner	structures	within	the	outer	ones.
A
.	
The	layout	of	the	structure	is	as	follows:
B
.	
It	uses	24	bytes.
C
.	
As	always,	we	start	by	annotating	the	assembly	code:
From	this,	we	can	generate	C	code	as	follows:</p>
<p>Solution	to	Problem	
3.42	
(page
269
)
This	problem	demonstrates	how	a	very	common	data	structure	and
operation	on	it	is	implemented	in	machine	code.	We	solve	the
problem	by	first	annotating	the	assembly	code,	recognizing	that	the
two	fields	of	the	structure	are	at	offsets	0	(for	
)	and	8	(for	
).</p>
<p>A
.	
Based	on	the	annotated	code,	we	can	generate	a	C	version:
B
.	
We	can	see	that	each	structure	is	an	element	in	a	singly	linked
list,	with	field	
being	the	value	of	the	element	and	
being	a
pointer	to	the	next	element.	Function	fun	computes	the	sum	of
the	element	values	in	the	list.
Solution	to	Problem	
3.43	
(page
272
)</p>
<p>Structures	and	unions	involve	a	simple	set	of	concepts,	but	it	takes
practice	to	be	comfortable	with	the	different	referencing	patterns	and
their	implementations.
Solution	to	Problem	
3.44	
(page</p>
<p>275
)
Understanding	structure	layout	and	alignment	is	very	important	for
understanding	how	much	storage	different	data	structures	require	and
for	understanding	the	code	generated	by	the	compiler	for	accessing
structures.	This	problem	lets	you	work	out	the	details	of	some
example	structures.
A
.	
Total
Alignment
0
4
8
12
16
4
B
.	
Total
Alignment
0
4
5
8
16
8
C
.	
C.	
Total
Alignment
0
6
10
2
D
.	
Total
Alignment
0
16
40
8
E
.	</p>
<p>Total
Alignment
0
24
40
8
Solution	to	Problem	
3.45	
(page
275
)
This	is	an	exercise	in	understanding	structure	layout	and	alignment.
A
.	
Here	are	the	object	sizes	and	byte	offsets:
Field
Size
8
2
8
1
4
1
8
4
Offset
0
8
16
24
28
32
40
48
B
.	
The	structure	is	a	total	of	56	bytes	long.	The	end	of	the
structure	must	be	padded	by	4	bytes	to	satisfy	the	8-byte
alignment	requirement.
C
.	
One	strategy	that	works,	when	all	data	elements	have	a	length
equal	to	a	power	of	2,	is	to	order	the	structure	elements	in
descending	order	of	size.	This	leads	to	a	declaration</p>
<p>with	the	following	offsets:
Field
Size
8
8
8
4
4
2
1
1
Offset
0
8
16
24
28
32
34
35
The	structure	must	be	padded	by	4	bytes	to	satisfy	the	8-byte
alignment	requirement,	giving	a	total	of	40	bytes.
Solution	to	Problem	
3.46	
(page
282
)
This	problem	covers	a	wide	range	of	topics,	such	as	stack	frames,
string	representations,	ASCII	code,	and	byte	ordering.	It	demonstrates</p>
<p>the	dangers	of	out-of-bounds	memory	references	and	the	basic	ideas
behind	buffer	overflow.
A
.	
Stack	after	line	3:
B
.	
Stack	after	line	5:
C
.	
The	program	is	attempting	to	return	to	address	
.	The
low-order	2	bytes	were	overwritten	by	the	code	for	character	`4'
and	the	terminating	null	character.
D
.	
The	saved	value	of	register	
was	set	to
.	This	value	will	be	loaded	into	the	register
before	
returns.
E
.	
The	call	to	
should	have	had	
as	its
argument,	and	the	code	should	also	check	that	the	returned
value	is	not	equal	to	
.</p>
<p>Solution	to	Problem	
3.47	
(page
286
)
A
.	
This	corresponds	to	a	range	of	around	2
addresses.
B
.	
A	128-byte	nop	sled	would	cover	2
addresses	with	each	test,	and
so	we	would	only	require	around	2
=	64	attempts.
This	example	clearly	shows	that	the	degree	of	randomization	in
this	version	of	Linux	would	provide	only	minimal	deterrence
against	an	overflow	attack.
Solution	to	Problem	
3.48	
(page
288
)
This	problem	gives	you	another	chance	to	see	how	x86-64	code
manages	the	stack,	and	to	also	better	understand	how	to	defend
against	buffer	overflow	attacks.
A
.	
For	the	unprotected	code,	we	can	see	that	lines	4	and	5
compute	the	positions	of	
and	
to	be	at	offsets	24	and	0
relative	to	
In	the	protected	code,	the	canary	is	stored	at
offset	40	(line	4),	while	
and	
are	at	offsets	8	and	16	(lines
7	and	8).
B
.	
In	the	protected	code,	local	variable	
is	positioned	closer	to
the	top	of	the	stack	than	
,	and	so	an	overrun	of	
will	not
13
7
6</p>
<p>corrupt	the	value	of	
.
Solution	to	Problem	
3.49	
(page
293
)
This	code	combines	many	of	the	tricks	we	have	seen	for	performing
bit-level	arithmetic.	It	requires	careful	study	to	make	any	sense	of	it.
A
.	
The	
instruction	of	line	5	computes	the	value	8
n
+	22,
which	is	then	rounded	down	to	the	nearest	multiple	of	16	by	the
instruction	of	line	6.	The	resulting	value	will	be	8
n
+	8
when	
n
is	odd	and	8
n
+	16	when	
n
is	even,	and	this	value	is
subtracted	from	
s
to	give	
s
.
B
.	
The	three	instructions	in	this	sequence	round	
s
up	to	the
nearest	multiple	of	8.	They	make	use	of	the	combination	of
biasing	and	shifting	that	we	saw	for	dividing	by	a	power	of	2	in
Section	
2.3.7
.
C
.	
These	two	examples	can	be	seen	as	the	cases	that	minimize
and	maximize	the	values	of	
e
and	
e
.
n
s
s
p
e
e
5
2,065
2,017
2,024
1
7
6
2,064
2,000
2,000
16
0
1
2
2
1
2
1
2
1
2</p>
<p>D
.	
We	can	see	that	
s
is	computed	in	a	way	that	preserves
whatever	offset	
s
has	with	the	nearest	multiple	of	16.	We	can
also	see	that	
p
will	be	aligned	on	a	multiple	of	8,	as	is
recommended	for	an	array	of	8-byte	elements.
Solution	to	Problem	
3.50	
(page
300
)
This	exercise	requires	that	you	step	through	the	code,	paying	careful
attention	to	which	conversion	and	data	movement	instructions	are
used.	We	can	see	the	values	being	retrieved	and	converted	as
follows:
The	value	at	
is	retrieved,	converted	to	an	
(line	4),	and	then
stored	at	
.	We	can	therefore	infer	that	
is	
.
The	value	at	
is	retrieved,	converted	to	a	
(line	6),	and
then	stored	at	
.	We	can	therefore	infer	that	
is	
.
The	value	of	
is	converted	to	a	
(line	8)	and	stored	at	
.
We	can	therefore	infer	that	
is	
.
The	value	at	
is	retrieved	on	line	3.	The	two	instructions	at	lines
10-11	convert	this	to	double	precision	as	the	value	returned	in
register	
.	We	can	therefore	infer	that	
is	
.
2
1</p>
<p>Solution	to	Problem	
3.51	
(page
300
)
These	cases	can	be	handled	by	selecting	the	appropriate	entries	from
the	tables	in	
Figures	
3.47
and	
3.48
,	or	using	one	of	the	code
sequences	for	converting	between	floating-point	formats.
T
T
Instruction(s)
Solution	to	Problem	
3.52	
(page
301
)
The	basic	rules	for	mapping	arguments	to	registers	are	fairly	simple
(although	they	become	much	more	complex	with	more	and	other
types	of	arguments	
[77]
).
x
y</p>
<p>A
.	
Registers:	
in	
in	
in	
in	
B
.	
Registers:	
in	
in	
in	
in	
C
.	
Registers:	
in	
in	
in	
in	
D
.	
Registers:	
in	
in	
in	
in	
Solution	to	Problem	
3.53	
(page
303
)
We	can	see	from	the	assembly	code	that	there	are	two	integer
arguments,	passed	in	registers	
and	
.	Let	us	name	these	
and	
.	Similarly,	there	are	two	floating-point	arguments,	passed	in
registers	
and	
,	which	we	name	
and	
.
We	can	then	annotate	the	assembly	code:</p>
<p>From	this	we	see	that	the	code	computes	the	value	
.
We	can	also	see	that	
has	type	
has	type	long,	
has	type
float,	and	
has	type	double.	The	only	ambiguity	in	matching
arguments	to	the	named	values	stems	from	the	commutativity	of
multiplication—yielding	two	possible	results:</p>
<p>Solution	to	Problem	
3.54	
(page
303
)
This	problem	can	readily	be	solved	by	stepping	through	the	assembly
code	and	determining	what	is	computed	on	each	step,	as	shown	with
the	annotations	below:
We	can	conclude	from	this	analysis	that	the	function	computes	</p>
<p>Solution	to	Problem	
3.55	
(page
305
)
This	problem	involves	the	same	reasoning	as	was	required	to	see	that
numbers	declared	at	label	
encode	1.8,	but	with	a	simpler
example.
We	see	that	the	two	values	are	0	and	1077936128	(
).	From
the	high-order	bytes,	we	can	extract	an	exponent	field	of	
(1028),	from	which	we	subtract	a	bias	of	1023	to	get	an	exponent	of	5.
Concatenating	the	fraction	bits	of	the	two	values,	we	get	a	fraction
field	of	0,	but	with	the	implied	leading	value	giving	value	1.0.	The
constant	is	therefore	1.0	×	2
=	32.0.
Solution	to	Problem	
3.56	
(page
305
)
A
.	
We	see	here	that	the	16	bytes	starting	at	address	
form	a
mask,	where	the	low-order	8	bytes	contain	all	ones,	except	for	the
most	significant	bit,	which	is	the	sign	bit	of	a	double-precision
value.	When	we	compute	the	
AND	
of	this	mask	with	
,	it	will
clear	the	sign	bit	of	
,	yielding	the	absolute	value.	In	fact,	we
generated	this	code	by	defining	
to	be	
,	where
is	defined	in	
5</p>
<p>B
.	
We	see	that	the	vxorpd	instruction	sets	the	entire	register	to	zero,
and	so	this	is	a	way	to	generate	floating-point	constant	0.0.
C
.	
We	see	that	the	16	bytes	starting	at	address	
form	a	mask
with	a	single	1	bit,	at	the	position	of	the	sign	bit	for	the	low-order
value	in	the	XMM	register.	When	we	compute	the	
EXCLUSIVE-OR
of	this	mask	with	
,	we	change	the	sign	of	
,	computing	the
expression	
.
Solution	to	Problem	
3.57	
(page
308
)
Again,	we	annotate	the	code,	including	dealing	with	the	conditional
branch:</p>
<p>From	this,	we	can	write	the	following	code	for	</p>
<p>Chapter	
4	
Processor	Architecture
4.1	
The	Y86-64	Instruction	Set	Architecture	
355
4.2	
Logic	Design	and	the	Hardware	Control	Language	HCL	
372
4.3	
Sequential	Y86-64	Implementations	
384
4.4	
General	Principles	of	Pipelining	
412
4.5	
Pipelined	Y86-64	Implementations	
421
4.6	
Summary</p>
<p>470
Bibliographic	Notes	
473
Homework	Problems	
473
Solutions	to	Practice	Problems	
480
Modern	microprocessors	are	among	the	most
complex	systems	ever	created	by	humans.	A	single
silicon	chip,	roughly	the	size	of	a	fingernail,	can
contain	several	high-performance	processors,	large
cache	memories,	and	the	logic	required	to	interface
them	to	external	devices.	In	terms	of	performance,
the	processors	implemented	on	a	single	chip	today</p>
<p>dwarf	the	room-size	supercomputers	that	cost	over
$10	million	just	20	years	ago.	Even	the	embedded
processors	found	in	everyday	appliances	such	as
cell	phones,	navigation	systems,	and	programmable
thermostats	are	far	more	powerful	than	the	early
developers	of	computers	could	ever	have
envisioned.
So	far,	we	have	only	viewed	computer	systems
down	to	the	level	of	machine-language	programs.
We	have	seen	that	a	processor	must	execute	a
sequence	of	instructions,	where	each	instruction
performs	some	primitive	operation,	such	as	adding
two	numbers.	An	instruction	is	encoded	in	binary
form	as	a	sequence	of	1	or	more	bytes.	The
instructions	supported	by	a	particular	processor	and
their	byte-level	encodings	are	known	as	its
instruction	set	architecture
(ISA).	Different	&quot;families&quot;
of	processors,	such	as	Intel	IA32	and	x86-64,
IBM/Freescale	Power,	and	the	ARM	processor
family,	have	different	ISAs.	A	program	compiled	for
one	type	of	machine	will	not	run	on	another.	On	the
other	hand,	there	are	many	different	models	of
processors	within	a	single	family.	Each
manufacturer	produces	processors	of	ever-growing
performance	and	complexity,	but	the	different
models	remain	compatible	at	the	ISA	level.	Popular
families,	such	as	x86-64,	have	processors	supplied
by	multiple	manufacturers.	Thus,	the	ISA	provides	a</p>
<p>conceptual	layer	of	abstraction	between	compiler
writers,	who	need	only	know	what	instructions	are
permitted	and	how	they	are	encoded,	and	processor
designers,	who	must	build	machines	that	execute
those	instructions.
In	this	chapter,	we	take	a	brief	look	at	the	design	of
processor	hardware.	We	study	the	way	a	hardware
system	can	execute	the	instructions	of	a	particular
ISA.	This	view	will	give	you	a	better	understanding
of	how	computers	work	and	the	technological
challenges	faced	by	computer	manufacturers.	One
important	concept	is	that	the	actual	way	a	modern
processor	operates	can	be	quite	different	from	the
model	of	computation	implied	by	the	ISA.	The	ISA
model	would	seem	to	imply	
sequential
instruction
execution,	where	each	instruction	is	fetched	and
executed	to	completion	before	the	next	one	begins.
By	executing	different	parts	of	multiple	instructions
simultaneously,	the	processor	can	achieve	higher
performance	than	if	it	executed	just	one	instruction
at	a	time.	Special	mechanisms	are	used	to	make
sure	the	processor	computes	the	same	results	as	it
would	with	sequential	execution.	This	idea	of	using
clever	tricks	to	improve	performance	while
maintaining	the	functionality	of	a	simpler	and	more
abstract	model	is	well	known	in	computer	science.
Examples	include	the	use	of	caching	in	Web</p>
<p>browsers	and	information	retrieval	data	structures
such	as	balanced	binary	trees	and	hash	tables.
Chances	are	you	will	never	design	your	own
processor.	This	is	a	task	for	experts	working	at
fewer	than	100	companies	worldwide.	Why,	then,
should	you	learn	about	processor	design?
It	is	intellectually	interesting	and	important.
There	is	an	intrinsic	value	in	learning	how	things
work.	It	is	especially	interesting	to	learn	the	inner
workings	of
Aside	
The	progress	of
computer	technology
To	get	a	sense	of	how	much	computer
technology	has	improved	over	the	past
four	decades,	consider	the	following	two
processors.
The	first	Cray	1	supercomputer	was
delivered	to	Los	Alamos	National
Laboratory	in	1976.	It	was	the	fastest
computer	in	the	world,	able	to	perform	as
many	as	250	million	arithmetic	operations
per	second.	It	came	with	8	megabytes	of
random	access	memory,	the	maximum</p>
<p>configuration	allowed	by	the	hardware.
The	machine	was	also	very	large—it
weighed	5,000	kg,	consumed	115
kilowatts,	and	cost	$9	million.	In	total,
around	80	of	them	were	manufactured.
The	Apple	ARM	A7	microprocessor	chip,
introduced	in	2013	to	power	the	iPhone
5S,	contains	two	CPUs,	each	of	which
can	perform	several	billion	arithmetic
operations	per	second,	and	1	gigabyte	of
random	access	memory.	The	entire
phone	weighs	just	112	grams,	consumes
around	1	watt,	and	costs	less	than	$800.
Over	9	million	units	were	sold	in	the	first
weekend	of	its	introduction.	In	addition	to
being	a	powerful	computer,	it	can	be	used
to	take	pictures,	to	place	phone	calls,	and
to	provide	driving	directions,	features
never	considered	for	the	Cray	1.
These	two	systems,	spaced	just	37	years
apart,	demonstrate	the	tremendous
progress	of	semiconductor	technology.
Whereas	the	Cray	l's	CPU	was
constructed	using	around	100,000
semiconductor	chips,	each	containing
less	than	20	transistors,	the	Apple	A7	has
over	1	billion	transistors	on	its	single	chip.
The	Cray	1's	8-megabyte	memory</p>
<p>required	8,192	chips,	whereas	the
iPhone's	gigabyte	memory	is	contained	in
a	single	chip.
a	system	that	is	such	a	part	of	the	daily	lives	of
computer	scientists	and	engineers	and	yet
remains	a	mystery	to	many.	Processor	design
embodies	many	of	the	principles	of	good
engineering	practice.	It	requires	creating	a
simple	and	regular	structure	to	perform	a
complex	task.
Understanding	how	the	processor	works	aids
in	understanding	how	the	overall	computer
system	works.	
In	
Chapter	
6
,	we	will	look	at
the	memory	system	and	the	techniques	used	to
create	an	image	of	a	very	large	memory	with	a
very	fast	access	time.	Seeing	the	processor	side
of	the	processor-memory	interface	will	make	this
presentation	more	complete.
Although	few	people	design	processors,
many	design	hardware	systems	that	contain
processors.	
This	has	become	commonplace	as
processors	are	embedded	into	real-world
systems	such	as	automobiles	and	appliances.
Embedded-system	designers	must	understand
how	processors	work,	because	these	systems
are	generally	designed	and	programmed	at	a
lower	level	of	abstraction	than	is	the	case	for
desktop	and	server-based	systems.
You	just	might	work	on	a	processor	design.</p>
<p>You	just	might	work	on	a	processor	design.
Although	the	number	of	companies	producing
microprocessors	is	small,	the	design	teams
working	on	those	processors	are	already	large
and	growing.	There	can	be	over	1,000	people
involved	in	the	different	aspects	of	a	major
processor	design.
In	this	chapter,	we	start	by	defining	a	simple
instruction	set	that	we	use	as	a	running	example	for
our	processor	implementations.	We	call	this	the
&quot;Y86-64&quot;	
instruction	set,	because	it	was	inspired	by
the	x86-64	instruction	set.	Compared	with	x86-64,
the	Y86-64	instruction	set	has	fewer	data	types,
instructions,	and	addressing	modes.	It	also	has	a
simple	byte-level	encoding,	making	the	machine
code	less	compact	than	the	comparable	x86-64
code,	but	also	much	easier	to	design	the	CPU's
decoding	logic.	Even	though	the	Y86-64	instruction
set	is	very	simple,	it	is	sufficiently	complete	to	allow
us	to	write	programs	manipulating	integer	data.
Designing	a	processor	to	implement	Y86-64
requires	us	to	deal	with	many	of	the	challenges
faced	by	processor	designers.
We	then	provide	some	background	on	digital
hardware	design.	We	describe	the	basic	building
blocks	used	in	a	processor	and	how	they	are
connected	together	and	operated.	This	presentation</p>
<p>builds	on	our	discussion	of	Boolean	algebra	and	bit-
level	operations	from	
Chapter	
2
.	We	also
introduce	a	simple	language,	HCL	(for	&quot;hardware
control	language&quot;),	to	describe	the	control	portions
of	hardware	systems.	We	will	later	use	this
language	to	describe	our	processor	designs.	Even	if
you	already	have	some	background	in	logic	design,
read	this	section	to	understand	our	particular
notation.
As	a	first	step	in	designing	a	processor,	we	present
a	functionally	correct,	but	somewhat	impractical,
Y86-64	processor	based	on	
sequential
operation.
This	processor	executes	a	complete	Y86-64
instruction	on	every	clock	cycle.	The	clock	must	run
slowly	enough	to	allow	an	entire	series	of	actions	to
complete	within	one	cycle.	Such	a	processor	could
be	implemented,	but	its	performance	would	be	well
below	what	could	be	achieved	for	this	much
hardware.
With	the	sequential	design	as	a	basis,	we	then	apply
a	series	of	transformations	to	create	a	
pipelined
processor.	This	processor	breaks	the	execution	of
each	instruction	into	five	steps,	each	of	which	is
handled	by	a	separate	section	or	
stage
of	the
hardware.	Instructions	progress	through	the	stages
of	the	pipeline,	with	one	instruction	entering	the
pipeline	on	each	clock	cycle.	As	a	result,	the</p>
<p>processor	can	be	executing	the	different	steps	of	up
to	five	instructions	simultaneously.	Making	this
processor	preserve	the	sequential	behavior	of	the
Y86-64	ISA	requires	handling	a	variety	of	
hazard
conditions,	where	the	location	or	operands	of	one
instruction	depend	on	those	of	other	instructions
that	are	still	in	the	pipeline.
We	have	devised	a	variety	of	tools	for	studying	and
experimenting	with	our	processor	designs.	These
include	an	assembler	for	Y86-64,	a	simulator	for
running	Y86-64	programs	on	your	machine,	and
simulators	for	two	sequential	and	one	pipelined
processor	design.	The	control	logic	for	these
designs	is	described	by	files	in	HCL	notation.	By
editing	these	files	and	recompiling	the	simulator,	you
can	alter	and	extend	the	simulator's	behavior.	A
number	of	exercises	are	provided	that	involve
implementing	new	instructions	and	modifying	how
the	machine	processes	instructions.	Testing	code	is
provided	to	help	you	evaluate	the	correctness	of
your	modifications.	These	exercises	will	greatly	aid
your	understanding	of	the	material	and	will	give	you
an	appreciation	for	the	many	different	design
alternatives	faced	by	processor	designers.
Web	Aside	
ARCH
:
VLOG</p>
<p>on	page	467	presents	a
representation	of	our	pipelined	Y86-64	processor	in
the	Verilog	hardware	description	language.	This</p>
<p>involves	creating	modules	for	the	basic	hardware
building	blocks	and	for	the	overall	processor
structure.	We	automatically	translate	the	HCL
description	of	the	control	
logic	into	Verilog.	By	first
debugging	the	HCL	description	with	our	simulators,
we	eliminate	many	of	the	tricky	bugs	that	would
otherwise	show	up	in	the	hardware	design.	Given	a
Verilog	description,	there	are	commercial	and	open-
source	tools	to	support	simulation	and	
logic
synthesis
,	generating	actual	circuit	designs	for	the
microprocessors.	So,	although	much	of	the	effort	we
expend	here	is	to	create	pictorial	and	textual
descriptions	of	a	system,	much	as	one	would	when
writing	software,	the	fact	that	these	designs	can	be
automatically	synthesized	demonstrates	that	we	are
indeed	creating	a	system	that	can	be	realized	as
hardware.</p>
<p>4.1	
The	Y86-64	Instruction	Set
Architecture
Defining	an	instruction	set	architecture,	such	as	Y86-64,	includes	defining
the	different	components	of	its	state,	the	set	of	instructions	and	their
encodings,	a	set	of	programming	conventions,	and	the	handling	of
exceptional	events.
4.1.1	
Programmer-Visible	State
As	
Figure	
4.1
illustrates,	each	instruction	in	a	Y86-64	program	can
read	and	modify	some	part	of	the	processor	state.	This	is	referred	to	as
the	
programmer-visible
state,	where	the	&quot;programmer&quot;	in	this	case	is
either	someone	writing	programs	in	assembly	code	or	a	compiler
generating	machine-level	code.	We	will	see	in	our	processor
implementations	that	we	do	not	need	to	represent	and	organize	this	state
in	exactly	the	manner	implied	by	the	ISA,	as	long	as	we	can	make	sure
that	machine-level	programs	appear	to	have	access	to	the	programmer-
visible	state.	The	state	for	Y86-64	is	similar	to	that	for	x86-64.	There	are
15	
program	registers:</p>
<pre><code>through	
(We	omit	the	x86-64	register	
15	to	simplify	the
</code></pre>
<p>instruction	encoding.)	Each	of	these	stores	a	64-bit	word.	Register	
is	used	as	a	stack	pointer	by	the	push,	pop,	call,	and	return	instructions.</p>
<p>Otherwise,	the	registers	have	no	fixed	meanings	or	values.	There	are
three	single-bit	
condition	codes
,	
,	and	
,	storing	information
Figure	
4.1	
Y86-64	programmer-visible	state.
As	with	x86-64,	programs	for	Y86-64	access	and	modify	the	program
registers,	the	condition	codes,	the	program	counter	(PC),	and	the
memory.	The	status	code	indicates	whether	the	program	is	running
normally	or	some	special	event	has	occurred.
about	the	effect	of	the	most	recent	arithmetic	or	logical	instruction.	The
program	counter	(PC)	holds	the	address	of	the	instruction	currently	being
executed.
The	
memory
is	conceptually	a	large	array	of	bytes,	holding	both	program
and	data.	Y86-64	programs	reference	memory	locations	using	
virtual
addresses.
A	combination	of	hardware	and	operating	system	software
translates	these	into	the	actual,	or	
physical
,	addresses	indicating	where
the	values	are	actually	stored	in	memory.	We	will	study	virtual	memory	in
more	detail	in	
Chapter	
9
.	For	now,	we	can	think	of	the	virtual	memory
system	as	providing	Y86-64	programs	with	an	image	of	a	monolithic	byte
array.</p>
<p>A	final	part	of	the	program	state	is	a	status	code	Stat,	indicating	the
overall	state	of	program	execution.	It	will	indicate	either	normal	operation
or	that	some	sort	of	
exception
has	occurred,	such	as	when	an	instruction
attempts	to	read	from	an	invalid	memory	address.	The	possible	status
codes	and	the	handling	of	exceptions	is	described	in	
Section	
4.1.4
.
4.1.2	
Y86-64	Instructions
Figure	
4.2
gives	a	concise	description	of	the	individual	instructions	in
the	Y86-64	ISA.	We	use	this	instruction	set	as	a	target	for	our	processor
implementations.	The	set	of	Y86-64	instructions	is	largely	a	subset	of	the
x86-64	instruction	set.	It	includes	only	8-byte	integer	operations,	has
fewer	addressing	modes,	and	includes	a	smaller	set	of	operations.	Since
we	only	use	8-byte	data,	we	can	refer	to	these	as	&quot;words&quot;	without	any
ambiguity.	In	this	figure,	we	show	the	assembly-code	representation	of
the	instructions	on	the	left	and	the	byte	encodings	on	the	right.	
Figure
4.3
shows	further	details	of	some	of	the	instructions.	The	assembly-
code	format	is	similar	to	the	ATT	format	for	x86-64.
Here	are	some	details	about	the	Y86-64	instructions.
The	x86-64	
instruction	is	split	into	four	different	instructions:
,	and	
,	explicitly	indicating	the	form	of
the	source	and	destination.	The	source	is	either	immediate	(
),
register	(
),	or	memory	(
).	It	is	designated	by	the	first	character	in
the	instruction	name.	The	destination	is	either	register	(
)	or	memory
(
).	It	is	designated	by	the	second	character	in	the	instruction	name.</p>
<p>Explicitly	identifying	the	four	types	of	data	transfer	will	prove	helpful
when	we	decide	how	to	implement	them.
The	memory	references	for	the	two	memory	movement	instructions
have	a	simple	base	and	displacement	format.	We	do	not	support	the
second	index	register	or	any	scaling	of	a	register's	value	in	the
address	computation.
As	with	x86-64,	we	do	not	allow	direct	transfers	from	one	memory
location	to	another.	In	addition,	we	do	not	allow	a	transfer	of
immediate	data	to	memory.
There	are	four	integer	operation	instructions,	shown	in	
Figure	
4.2
as	
These	are	
,	and	
.	They	operate	only	on
register	data,	whereas	x86-64	also	allows	operations	on	memory
data.	These	instructions	set	the	three	condition	codes	
,	and	
(zero,	sign,	and	overflow).
Figure	
4.2	
Y86-64	instruction	set.
Instruction	encodings	range	between	1	and	10	bytes.	An	instruction
consists	of	a	1-byte	instruction	specifier,	possibly	a	1	-byte	register
specifier,	and	possibly	an	8-byte	constant	word.	Field	
specifies	a</p>
<p>particular	integer	operation	(
),	data	movement	condition	(
),
or	branch	condition	(
).	All	numeric	values	are	shown	in
hexadecimal.
The	seven	jump	instructions	(shown	in	
Figure	
4.2
as	
)	are	
,	and	
.	Branches	are	taken	according	to	the
type	of	branch	and	the	settings	of	the	condition	codes.	The	branch
conditions	are	the	same	as	with	x86-64	(
Figure	
3.15
).
There	are	six	conditional	move	instructions	(shown	in	
Figure	
4.2
as
,	and	
.	These
have	the	same	format	as	the	register-register	move	instruction
,	but	the	destination	register	is	updated	only	if	the	condition
codes	satisfy	the	required	constraints.
The	call	instruction	pushes	the	return	address	on	the	stack	and	jumps
to	the	destination	address.	The	
instruction	returns	from	such	a
call.
The	
and	
instructions	implement	push	and	pop,	just	as
they	do	in	x86-64.
The	
instruction	stops	instruction	execution.	x86-64	has	a
comparable	instruction,	called	
.	x86-64	application	programs	are
not	permitted	to	use	
this	instruction,	since	it	causes	the	entire	system
to	suspend	operation.	For	Y86-64,	executing	the	
instruction
causes	the	processor	to	stop,	with	the	status	code	set	to	
.	(See
Section	
4.1.4
.)
4.1.3	
Instruction	Encoding</p>
<p>Figure	
4.2
also	shows	the	byte-level	encoding	of	the	instructions.
Each	instruction	requires	between	1	and	10	bytes,	depending	on	which
fields	are	required.	Every	instruction	has	an	initial	byte	identifying	the
instruction	type.	This	byte	is	split	into	two	4-bit	parts:	the	high-order,	or
code
,	part,	and	the	low-order,	or	
function
,	part.	As	can	be	seen	in	
Figure
4.2
,	code	values	range	from	
to	
.	The	function	values	are
significant	only	for	the	cases	where	a	group	of	related	instructions	share
a	common	code.	These	are	given	in	
Figure	
4.3
,	showing	the	specific
encodings	of	the	integer	operation,	branch,	and	conditional	move
instructions.	Observe	that	
has	the	same	instruction	code	as	the
conditional	moves.	It	can	be	viewed	as	an	&quot;unconditional	move&quot;	just	as
the	
instruction	is	an	unconditional	jump,	both	having	function	code	
.
As	shown	in	
Figure	
4.4
,	each	of	the	15	program	registers	has	an
associated	
register	identifier
(ID)	ranging	from	
to	
.	The	numbering
of	registers	in	Y86-64	matches	what	is	used	in	x86-64.	The	program
registers	are	stored	within	the	CPU	in	a	
register	file
,	a	small	random
access	memory	where	the	register	IDs	serve	as	addresses.	ID	value	
is	used	in	the	instruction	encodings	and	within	our	hardware	designs
when	we	need	to	indicate	that	no	register	should	be	accessed.
Some	instructions	are	just	1	byte	long,	but	those	that	require	operands
have	longer	encodings.	First,	there	can	be	an	additional	
register	specifier
byte
,	specifying	either	one	or	two	registers.	These	register	fields	are
called	rA	and	rB	in	
Figure	
4.2
.	As	the	assembly-code	versions	of	the
instructions	show,	they	can	specify	the	registers	used	for	data	sources
and	destinations,	as	well	as	the	base	register	used	in	an	address
computation,	depending	on	the	instruction	type.	Instructions	that	have	no
register	operands,	such	as	branches	and	call,	do	not	have	a	register</p>
<p>specifier	byte.	Those	that	require	just	one	register	operand	(
,	and	
)	have
Figure	
4.3	
Function	codes	for	Y86-64	instruction	set.
The	code	specifies	a	particular	integer	operation,	branch	condition,	or
data	transfer	condition.	These	instructions	are	shown	as	
,	and
in	
Figure	
4.2
.
Number
Register	name
Number
Register	name
No	register
Figure	
4.4	
Y86-64	program	register	identifiers.</p>
<p>Each	of	the	1	5	program	registers	has	an	associated	identifier	(ID)
ranging	from	
to	
.	ID	
in	a	register	field	of	an	instruction	indicates
the	absence	of	a	register	operand.
the	other	register	specifier	set	to	value	
.	This	convention	will	prove
useful	in	our	processor	implementation.
Some	instructions	require	an	additional	8-byte	
constant	word.
This	word
can	serve	as	the	immediate	data	for	
,	the	displacement	for	
and	
address	specifiers,	and	the	destination	of	branches	and	calls.
Note	that	branch	and	call	destinations	are	given	as	absolute	addresses,
rather	than	using	the	PC-relative	addressing	seen	in	x86-64.	Processors
use	PC-relative	addressing	to	give	more	compact	encodings	of	branch
instructions	and	to	allow	code	to	be	shifted	from	one	part	of	memory	to
another	without	the	need	to	update	all	of	the	branch	target	addresses.
Since	we	are	more	concerned	with	simplicity	in	our	presentation,	we	use
absolute	addressing.	As	with	x86-64,	all	integers	have	a	little-endian
encoding.	When	the	instruction	is	written	in	disassembled	form,	these
bytes	appear	in	reverse	order.
As	an	example,	let	us	generate	the	byte	encoding	of	the	instruction
in	hexadecimal.	From	
Figure	
4.2
,
we	can	see	that	
has	initial	byte	40.	We	can	also	see	that	source
register	
should	be	encoded	in	the	rA	field,	and	base	register	
should	be	encoded	in	the	rB	field.	Using	the	register	numbers	in	
Figure
4.4
,	we	get	a	register	specifier	byte	of	42.	Finally,	the	displacement	is
encoded	in	the	8-byte	constant	word.	We	first	pad	
with
leading	zeros	to	fill	out	8	bytes,	giving	a	byte	sequence	of	
.	We	write	this	in	byte-reversed	order	as	</p>
<p>.	Combining	these,	we	get	an	instruction	encoding	of
One	important	property	of	any	instruction	set	is	that	the	byte	encodings
must	have	a	unique	interpretation.	An	arbitrary	sequence	of	bytes	either
encodes	a	unique	instruction	sequence	or	is	not	a	legal	byte	sequence.
This	property	holds	for	Y86-64,	because	every	instruction	has	a	unique
combination	of	code	and	function	in	its	initial	byte,	and	given	this	byte,	we
can	determine	the	length	and	meaning	of	any	additional	bytes.	This
property	ensures	that	a	processor	can	execute	an	object-code	program
without	any	ambiguity	about	the	meaning	of	the	code.	Even	if	the	code	is
embedded	within	other	bytes	in	the	program,	we	can	readily	determine
Aside	
Comparing	x86-64	to	Y86-64
instruction	encodings
Compared	with	the	instruction	encodings	
used	in	x86-64,	the
encoding	of	Y86-64	is	much	simpler	but	also	less	compact.	The
register	fields	occur	only	in	fixed	positions	in	all	Y86-64
instructions,	whereas	they	are	packed	into	various	positions	in	the
different	x86-64	instructions.	An	x86-64	instruction	can	encode
constant	values	in	1,	2,	4,	or	8	bytes,	whereas	Y86-64	always
requires	8	bytes.
the	instruction	sequence	as	long	as	we	start	from	the	first	byte	in	the
sequence.	On	the	other	hand,	if	we	do	not	know	the	starting	position	of	a
code	sequence,	we	cannot	reliably	determine	how	to	split	the	sequence
into	individual	instructions.	This	causes	problems	for	disassemblers	and</p>
<p>other	tools	that	attempt	to	extract	machine-level	programs	directly	from
object-code	byte	sequences.
Practice	Problem	
4.1	
(solution	page	
480
)
Determine	the	byte	encoding	of	the	Y86-64	instruction	sequence
that	follows.	The	line	
indicates	that	the	starting	address
of	the	object	code	should	be	
Practice	Problem	
4.2	
(solution	page	
481
)
For	each	byte	sequence	listed,	determine	the	Y86-64	instruction
sequence	it	encodes.	If	there	is	some	invalid	byte	in	the	sequence,
show	the	instruction	sequence	up	to	that	point	and	indicate	where
the	invalid	value	occurs.	For	each	sequence,	we	show	the	starting
address,	then	a	colon,	and	then	the	byte	sequence.
A.	
B.	
C.	</p>
<p>D.	
E.	
Aside	
RISC	and	CISC	instruction	sets
x86-64	is	sometimes	labeled	as	a	&quot;complex	instruction	set
computer&quot;	(CISC—pronounced	&quot;sisk&quot;),	and	is	deemed	to	be	the
opposite	of	ISAs	that	are	classified	as	&quot;reduced	instruction	set
computers&quot;	(RISC—pronounced	&quot;risk&quot;).	Historically,	CISC
machines	came	first,	having	evolved	from	the	earliest	computers.
By	the	early	1980s,	instruction	sets	for	mainframe	and
minicomputers	had	grown	quite	large,	as	machine	designers
incorporated	new	instructions	to	support	high-level	tasks,	such	as
manipulating	circular	buffers,	performing	decimal	arithmetic,	and
evaluating	polynomials.	The	first	microprocessors	appeared	in	the
early	1970s	and	had	limited	instruction	sets,	because	the
integrated-circuit	technology	then	posed	severe	constraints	on
what	could	be	implemented	on	a	single	chip.	Microprocessors
evolved	quickly	and,	by	the	early	1980s,	were	following	the	same
path	of	increasing	instruction	set	complexity	that	had	been	the
case	for	mainframes	and	minicomputers.	The	x86	family	took	this
path,	evolving	into	IA32,	and	more	recently	into	x86-64.	The	x86
line	continues	to	evolve	as	new	classes	of	instructions	are	added
based	on	the	needs	of	emerging	applications.
The	RISC	design	philosophy	developed	in	the	early	1980s	as	an
alternative	to	these	trends.	A	group	of	hardware	and	compiler
experts	at	IBM,	strongly	influenced	by	the	ideas	of	IBM	researcher
John	Cocke,	recognized	that	they	could	generate	efficient	code	for
a	much	simpler	form	of	instruction	set.	In	fact,	many	of	the	high-</p>
<p>level	instructions	that	were	being	added	to	instruction	sets	were
very	difficult	to	generate	with	a	compiler	and	were	seldom	used.	A
simpler	instruction	set	could	be	implemented	with	much	less
hardware	and	could	be	organized	in	an	efficient	pipeline	structure,
similar	to	those	described	later	in	this	chapter.	IBM	did	not
commercialize	this	idea	until	many	years	later,	when	it	developed
the	Power	and	PowerPC	ISAs.
The	RISC	concept	was	further	developed	by	Professors	David
Patterson,	of	the	University	of	California	at	Berkeley,	and	John
Hennessy,	of	Stanford	University.	Patterson	gave	the	name	RISC
to	this	new	class	of	machines,	and	CISC	to	the	existing	class,
since	there	had	previously	been	no	need	to	have	a	special
designation	for	a	nearly	universal	form	of	instruction	set.
When	comparing	CISC	with	the	original	RISC	instruction	sets,	we
find	the	following	general	characteristics:
CISC
Early	RISC
A	large	number	of	instructions.
The	Intel	document	describing
the	complete	set	of	instructions
[51]
is	over	1,200	pages	long.
Many	fewer	instructions—typically	less	than	100.
Some	instructions	with	long
execution	times.	These	include
instructions	that	copy	an	entire
block	from	one	part	of	memory
to	another	and	others	that	copy
multiple	registers	to	and	from
memory.
No	instruction	with	a	long	execution	time.	Some
early	RISC	machines	did	not	even	have	an
integer	multiply	instruction,	requiring	compilers	to
implement	multiplication	as	a	sequence	of
additions.</p>
<p>Variable-size	encodings.	x86-64
instructions	can	range	from	1	to
15	bytes.
Fixed-length	encodings.	Typically	all	instructions
are	encoded	as	4	bytes.
Multiple	formats	for	specifying
operands.	In	x86-64,	a	memory
operand	specifier	can	have
many	different	combinations	of
displacement,	base	and	index
registers,	and	scale	factors.
Simple	addressing	formats.	Typically	just	base
and	displacement	addressing.
Arithmetic	and	logical
operations	can	be	applied	to
both	memory	and	register
operands.
Arithmetic	and	logical	operations	only	use
register	operands.	Memory	referencing	is	only
allowed	by	
load
instructions,	reading	from
memory	into	a	register,	and	
store
instructions,
writing	from	a	register	to	memory.	This
convention	is	referred	to	as	a	
load/store
architecture.
Implementation	artifacts	hidden
from	machine-level	programs.
The	ISA	provides	a	clean
abstraction	between	programs
and	how	they	get	executed.
Implementation	artifacts	exposed	to	machine-
level	programs.	Some	RISC	machines	prohibit
particular	instruction	sequences	and	have	jumps
that	do	not	take	effect	until	the	following
instruction	is	executed.	The	compiler	is	given	the
task	of	optimizing	performance	within	these
constraints.
Condition	codes.	Special	flags
are	set	as	a	side	effect	of
instructions	and	then	used	for
conditional	branch	testing.
No	condition	codes.	Instead,	explicit	test
instructions	store	the	test	results	in	normal
registers	for	use	in	conditional	evaluation.
Stack-intensive	procedure
linkage.	The	stack	is	used	for
procedure	arguments	and
return	addresses.
Register-intensive	procedure	linkage.	Registers
are	used	for	procedure	arguments	and	return
addresses.	Some	procedures	can	thereby	avoid
any	memory	references.	Typically,	the	processor</p>
<p>has	many	more	(up	to	32)	registers.
The	Y86-64	instruction	set	includes	attributes	of	both	CISC	and
RISC	instruction	sets.	On	the	CISC	side,	it	has	condition	codes
and	variable-length	instructions,	and	it	uses	the	stack	to	store
return	addresses.	On	the	RISC	side,	it	uses	a	load/store
architecture	and	a	regular	instruction	encoding,	and	it	passes
procedure	arguments	through	registers.	It	can	be	viewed	as	taking
a	CISC	instruction	set	(x86)	and	simplifying	it	by	applying	some	of
the	principles	of	RISC.
Aside	
The	RISC	versus	CISC
controversy
Through	the	1980s,	battles	raged	in	the	computer	architecture
community	regarding	the	merits	of	RISC	versus	CISC	instruction
sets.	Proponents	of	RISC	claimed	they	could	get	more	computing
power	for	a	given	amount	of	hardware	through	a	combination	of
streamlined	instruction	set	design,	advanced	compiler	technology,
and	pipelined	processor	implementation.	CISC	proponents
countered	that	fewer	CISC	instructions	were	required	to	perform	a
given	task,	and	so	their	machines	could	achieve	higher	overall
performance.
Major	companies	introduced	RISC	processor	lines,	including	Sun
Microsystems	(SPARC),	IBM	and	Motorola	(PowerPC),	and	Digital
Equipment	Corporation	(Alpha).	A	British	company,	Acorn
Computers	Ltd.,	developed	its	own	architecture,	ARM	(originally
an	acronym	for	&quot;Acorn	RISC	machine&quot;),	which	has	become	widely
used	in	embedded	applications,	such	as	cell	phones.</p>
<p>In	the	early	1990s,	the	debate	diminished	as	it	became	clear	that
neither	RISC	nor	CISC	in	their	purest	forms	were	better	than
designs	that	incorporated	the	best	ideas	of	both.	RISC	machines
evolved	and	introduced	more	instructions,	many	of	which	take
multiple	cycles	to	execute.	RISC	machines	today	have	hundreds
of	instructions	in	their	repertoire,	hardly	fitting	the	name	&quot;reduced
instruction	set	machine.&quot;	The	idea	of	exposing	implementation
artifacts	to	machine-level	programs	proved	to	be	shortsighted.	As
new	processor	models	were	developed	using	more	advanced
hardware	structures,	many	of	these	artifacts	became	irrelevant,
but	they	still	remained	part	of	the	instruction	set.	Still,	the	core	of
RISC	design	is	an	instruction	set	that	is	well	suited	to	execution	on
a	pipelined	machine.
More	recent	CISC	machines	also	take	advantage	of	high-
performance	pipeline	structures.	As	we	will	discuss	in	
Section
5.7
,	they	fetch	the	CISC	instructions	and	dynamically	translate
them	into	a	sequence	of	simpler,	RISC-like	operations.	For
example,	an	instruction	that	adds	a	register	to	memory	is
translated	into	three	operations:	one	to	read	the	original	memory
value,	one	to	perform	the	addition,	and	a	third	to	write	the	sum	to
memory.	Since	the	dynamic	translation	can	generally	be
performed	well	in	advance	of	the	actual	instruction	execution,	the
processor	can	sustain	a	very	high	execution	rate.
Marketing	issues,	apart	from	technological	ones,	have	also	played
a	major	role	in	determining	the	success	of	different	instruction
sets.	By	maintaining	compatibility	with	its	existing	processors,	Intel
with	x86	made	it	easy	to	keep	moving	from	one	generation	of
processor	to	the	next.	As	integrated-circuit	technology	improved,
Intel	and	other	x86	processor	manufacturers	could	overcome	the</p>
<p>inefficiencies	created	by	the	original	8086	instruction	set	design,
using	RISC	techniques	to	produce	performance	comparable	to	the
best	RISC	machines.	As	we	saw	in	
Section	
3.1
,	the	evolution
of	IA32	into	x86-64	provided	an	opportunity	to	incorporate	several
features	of	RISC	into	the	x86	family.	In	the	areas	of	desktop,
laptop,	and	server-based	computing,	x86	has	achieved	near	total
domination.
RISC	processors	have	done	very	well	in	the	market	for	
embedded
processors
,	controlling	such	systems	as	cellular	telephones,
automobile	brakes,	and	Internet	appliances.	In	these	applications,
saving	on	cost	and	power	is	more	important	than	maintaining
backward	compatibility.	In	terms	of	the	number	of	processors	sold,
this	is	a	very	large	and	growing	market.
4.1.4	
Y86-64	Exceptions
The	programmer-visible	state	for	Y86-64	(
Figure	
4.1
)	includes	a	status
code	Stat	describing	the	overall	state	of	the	executing	program.	The
possible	values	for	this	code	are	shown	in	
Figure	
4.5
.	Code	value	1,
named	
,	indicates	that	the	program
Value
Name
Meaning
1
Normal	operation
2
halt	instruction	encountered
3
Invalid	address	encountered</p>
<p>4
Invalid	instruction	encountered
Figure	
4.5	
Y86-64	status	codes.
In	our	design,	the	processor	halts	for	any	code	other	than	
.
is	executing	normally,	while	the	other	codes	indicate	that	some	type	of
exception
has	occurred.	Code	2,	named	
T,	indicates	that	the	processor
has	executed	a	
instruction.	Code	3,	named	
,	indicates	that	the
processor	attempted	to	read	from	or	write	to	an	invalid	memory	address,
either	while	fetching	an	instruction	or	while	reading	or	writing	data.	We
limit	the	maximum	address	(the	exact	limit	varies	by	implementation),	and
any	access	to	an	address	beyond	this	limit	will	trigger	an	
exception.
Code	4,	named	
,	indicates	that	an	invalid	instruction	code	has	been
encountered.
For	Y86-64,	we	will	simply	have	the	processor	stop	executing	instructions
when	it	encounters	any	of	the	exceptions	listed.	In	a	more	complete
design,	the	processor	would	typically	invoke	an	
exception	handler
,	a
procedure	designated	to	handle	the	specific	type	of	exception
encountered.	As	described	in	
Chapter	
8
,	exception	handlers	can	be
configured	to	have	different	effects,	such	as	aborting	the	program	or
invoking	a	user-defined	
signal	handler.
4.1.5	
Y86-64	Programs
Figure	
4.6
shows	x86-64	and	Y86-64	assembly	code	for	the	following
C	function:</p>
<p>The	x86-64	code	was	generated	by	the	
GCC</p>
<p>compiler.	The	Y86-64	code	is
similar,	but	with	the	following	differences:
The	Y86-64	code	loads	constants	into	registers	(lines	2-3),	since	it
cannot	use	immediate	data	in	arithmetic	instructions.
x86-64	code</p>
<p>Y86-64	code
Figure	
4.6	
Comparison	of	Y86-64	and	x86-64	assembly	programs.
The	sum	function	computes	the	sum	of	an	integer	array.	The	Y86-64
code	follows	the	same	general	pattern	as	the	x86-64	code.
The	Y86-64	code	requires	two	instructions	(lines	8-9)	to	read	a	value
from	memory	and	add	it	to	a	register,	whereas	the	x86-64	code	can</p>
<p>do	this	with	a	single	
instruction	(line	5).
Our	hand-coded	Y86-64	implementation	takes	advantage	of	the
property	that	the	
instruction	(line	11)	also	sets	the	condition
codes,	and	so	the	
instruction	of	the	
GCC
-generated	code	(line	9)
is	not	required.	For	this	to	work,	though,	the	Y86-64	code	must	set	the
condition	codes	prior	to	entering	the	loop	with	an	
instruction	(line
5).
Figure	
4.7
shows	an	example	of	a	complete	program	file	written	in
Y86-64	assembly	code.	The	program	contains	both	data	and	instructions.
Directives	indicate	where	to	place	code	or	data	and	how	to	align	it.	The
program	specifies	issues	such	as	stack	placement,	data	initialization,
program	initialization,	and	program	termination.
In	this	program,	words	beginning	with	`.'	are	
assembler	directives
telling
the	assembler	to	adjust	the	address	at	which	it	is	generating	code	or	to
insert	some	words	of	data.	The	directive	.
(line	2)	indicates	that	the
assembler	should	begin	generating	code	starting	at	address	0.	This	is	the
starting	address	for	all	Y86-64	programs.	The	next	instruction	(line	3)
initializes	the	stack	pointer.	We	can	see	that	the	label	stack	is	declared	at
the	end	of	the	program	(line	40),	to	indicate	address	
using	a	
directive	(line	39).	Our	stack	will	therefore	start	at	this	address	and	grow
toward	lower	addresses.	We	must	ensure	that	the	stack	does	not	grow	so
large	that	it	overwrites	the	code	or	other	program	data.
Lines	8	to	13	of	the	program	declare	an	array	of	four	words,	having	the
values</p>
<p>The	label	array	denotes	the	start	of	this	array,	and	is	aligned	on	an	8-byte
boundary	(using	the	.align	directive).	Lines	16	to	19	show	a	&quot;main&quot;
procedure	that	calls	the	function	sum	on	the	four-word	array	and	then
halts.
As	this	example	shows,	since	our	only	tool	for	creating	Y86-64	code	is	an
assembler,	the	programmer	must	perform	tasks	we	ordinarily	delegate	to
the	compiler,	linker,	and	run-time	system.	Fortunately,	we	only	do	this	for
small	programs,	for	which	simple	mechanisms	suffice.
Figure	
4.8
shows	the	result	of	assembling	the	code	shown	in	
Figure
4.7
by	an	assembler	we	call	
YAS
.	
The	assembler	output	is	in	ASCII
format	to	make	it	more	readable.	On	lines	of	the	assembly	file	that
contain	instructions	or	data,	the	object	code	contains	an	address,
followed	by	the	values	of	between	1	and	10	bytes.
We	have	implemented	an	
instruction	set	simulator
we	call	
YIS
,	the
purpose	of	which	is	to	model	the	execution	of	a	Y86-64	machine-code
program	without	attempting	to	model	the	behavior	of	any	specific
processor	implementation.	This	form	of	simulation	is	useful	for	debugging
programs	before	actual	hardware	is	available,	and	for	checking	the	result
of	either	simulating	the	hardware	or	running</p>
<p>Figure	
4.7	
Sample	program	written	in	Y86-64	assembly	code.
The	sum	function	is	called	to	compute	the	sum	of	a	four-element	array.</p>
<p>Figure	
4.8	
Output	of	
YAS</p>
<p>assembler.
Each	line	includes	a	hexadecimal	address	and	between	1	and	10	bytes
of	object	code.
the	program	on	the	hardware	itself.	Running	on	our	sample	object	code,
YIS</p>
<p>generates	the	following	output:
The	first	line	of	the	simulation	output	summarizes	the	execution	and	the
resulting	values	of	the	PC	and	program	status.	In	printing	register	and
memory	values,	it	only	prints	out	words	that	change	during	simulation,</p>
<p>either	in	registers	or	in	memory.	The	original	values	(here	they	are	all
zero)	are	shown	on	the	left,	and	the	final	values	are	shown	on	the	right.
We	can	see	in	this	output	that	register	
contains	
,
the	sum	of	the	4-element	array	passed	to	procedure	sum.	In	addition,	we
can	see	that	the	stack,	which	starts	at	address	
and	grows	toward
lower	addresses,	has	been	used,	causing	changes	to	words	of	memory
at	addresses	
.	The	maximum	address	for	executable	code	is
,	and	so	the	pushing	and	popping	of	values	on	the	stack	did	not
corrupt	the	executable	code.
Practice	Problem	
4.3	
(solution	page	
482
)
One	common	pattern	in	machine-level	programs	is	to	add	a	constant
value	to	a	register.	With	the	Y86-64	instructions	presented	thus	far,	this
requires	first	using	an	
instruction	to	set	a	register	to	the	constant,
and	then	an	
instruction	to	add	this	value	to	the	destination	register.
Suppose	we	want	to	add	a	new	instruction	
with	the	following
format:
This	instruction	adds	the	constant	value	V	to	register	rB.
Rewrite	the	Y86-64	
function	of	
Figure	
4.6
to	make	use	of	the
instruction.	In	the	original	version,	we	dedicated	registers	
and
to	hold	constant	values.	Now,	we	can	avoid	using	those	registers
altogether.</p>
<p>Practice	Problem	
4.4	
(solution	page	
482
)
Write	Y86-64	code	to	implement	a	recursive	sum	function	
,
based	on	the	following	C	code:
Use	the	same	argument	passing	and	register	saving	conventions
as	x86-64	code	does.	You	might	find	it	helpful	to	compile	the	C
code	on	an	x86-64	machine	and	then	translate	the	instructions	to
Y86-64.
Practice	Problem	
4.5	
(solution	page	
483
)
Modify	the	Y86-64	code	for	the	sum	function	(
Figure	
4.6
)	to
implement	a	function	
that	computes	the	sum	of	absolute
values	of	an	array.	Use	a	
conditional	jump
instruction	within	your
inner	loop.
Practice	Problem	
4.6	
(solution	page	
483
)
Modify	the	Y86-64	code	for	the	
function	(
Figure	
4.6
)	to
implement	a	function	
that	computes	the	sum	of	absolute</p>
<p>values	of	an	array.	Use	a	
conditional	move
instruction	within	your
inner	loop.
4.1.6	
Some	Y86-64	Instruction
Details
Most	Y86-64	instructions	transform	the	program	state	in	a	straightforward
manner,	and	so	defining	the	intended	effect	of	each	instruction	is	not
difficult.	Two	unusual	instruction	combinations,	however,	require	special
attention.
The	
instruction	both	decrements	the	stack	pointer	by	8	and	writes
a	register	value	to	memory.	It	is	therefore	not	totally	clear	what	the
processor	should	do	when	executing	the	instruction	
,	since	the
register	being	pushed	is	being	changed	by	the	same	instruction.	Two
different	conventions	are	possible:	(1)	push	the	original	value	of	
,	or
(2)	push	the	decremented	value	of	
.
For	the	Y86-64	processor,	let	us	adopt	the	same	convention	as	is	used
with	x86-64,	as	determined	in	the	following	problem.
Practice	Problem	
4.7	
(solution	page	
484
)
Let	us	determine	the	behavior	of	the	instruction	
for	an
x86-64	processor.	We	could	try	reading	the	Intel	documentation	on
this	instruction,	but	a	
simpler	approach	is	to	conduct	an</p>
<p>experiment	on	an	actual	machine.	The	C	compiler	would	not
normally	generate	this	instruction,	so	we	must	use	hand-generated
assembly	code	for	this	task.	Here	is	a	test	function	we	have	written
(Web	Aside	
ASM
:
EASM</p>
<p>on	page	178	describes	how	to	write
programs	that	combine	C	code	with	handwritten	assembly	code):
In	our	experiments,	we	find	that	function	
always	returns
0.	What	does	this	imply	about	the	behavior	of	the	instruction	
under	x86-64?
A	similar	ambiguity	occurs	for	the	instruction	
.	It	could	either	set
to	the	value	read	from	memory	or	to	the	incremented	stack	pointer.
As	with	
Problem	
4.7
,	let	us	run	an	experiment	to	determine	how	an
x86-64	machine	would	handle	this	instruction,	and	then	design	our	Y86-
64	machine	to	follow	the	same	convention.
Practice	Problem	
4.8	
(solution	page	
484
)</p>
<p>The	following	assembly-code	function	lets	us	determine	the
behavior	of	the	instruction	
for	x86-64:
We	find	this	function	always	returns	
.	What	does	this	imply
about	the	behavior	of	
What	other	Y86-64	instruction
would	have	the	exact	same	behavior?
Aside	
Getting	the	details	right:
Inconsistencies	across	x86	models
Practice	Problems	
4.7
and	
4.8
are	designed	to	help	us
devise	a	consistent	set	of	conventions	for	instructions	that	push	or
pop	the	stack	pointer.	There	seems	to	be	little	reason	why	one
would	want	to	perform	either	of	these	operations,	and	so	a	natural
question	to	ask	is,	&quot;Why	worry	about	such	picky	details?&quot;</p>
<p>Several	useful	lessons	can	be	learned	about	the	importance	of
consistency	from	the	following	excerpt	from	the	Intel
documentation	of	the	
PUSH</p>
<p>instruction	
[51]
:
For	IA-32	processors	from	the	Intel	286	on,	the	PUSH	ESP	instruction	pushes	the
value	of	the	ESP	register	as	it	existed	before	the	instruction	was	executed.	(This	is
also	true	for	Intel	64	architecture,	real-address	and	virtual-8086	modes	of	IA-32
architecture.)	For	the	Intel(r)	8086	processor,	the	PUSH	SP	instruction	pushes	the
new	value	of	the	SP	register	(that	is	the	value	after	it	has	been	decremented	by	2).
(PUSH	ESP	instruction.	Intel	Corporation.	50.)
Although	the	exact	details	of	this	note	may	be	difficult	to	follow,	we
can	see	that	it	states	that,	depending	on	what	mode	an	x86
processor	operates	under,	it	will	do	different	things	when
instructed	to	push	the	stack	pointer	register.	Some	modes	push
the	original	value,	while	others	push	the	decremented	value.
(Interestingly,	there	is	no	corresponding	ambiguity	about	popping
to	the	stack	pointer	register.)	There	are	two	drawbacks	to	this
inconsistency:
It	decreases	code	portability.	Programs	may	have	different
behavior	depending	on	the	processor	mode.	Although	the
particular	instruction	is	not	at	all	common,	even	the	potential
for	incompatibility	can	have	serious	consequences.
It	complicates	the	documentation.	As	we	see	here,	a	special
note	is	required	to	try	to	clarify	the	differences.	The
documentation	for	x86	is	already	complex	enough	without
special	cases	such	as	this	one.</p>
<p>We	conclude,	therefore,	that	working	out	details	in	advance	and
striving	for	complete	consistency	can	save	a	lot	of	trouble	in	the
long	run.</p>
<p>4.2	
Logic	Design	and	the	Hardware
Control	Language	HCL
In	hardware	design,	electronic	circuits	are	used	to	compute	functions	on
bits	and	to	store	bits	in	different	kinds	of	memory	elements.	Most
contemporary	circuit	technology	represents	different	bit	values	as	high	or
low	voltages	on	signal	wires.	In	current	technology,	logic	value	1	is
represented	by	a	high	voltage	of	around	1.0	volt,	while	logic	value	0	is
represented	by	a	low	voltage	of	around	0.0	volts.	Three	major
components	are	required	to	implement	a	digital	system:	
combinational
logic
to	compute	functions	on	the	bits,	
memory	elements
to	store	bits,
and	
clock	signals
to	regulate	the	updating	of	the	memory	elements.
In	this	section,	we	provide	a	brief	description	of	these	different
components.	We	also	introduce	HCL	(for	&quot;hardware	control	language&quot;),
the	language	that	we	use	to	describe	the	control	logic	of	the	different
processor	designs.	We	only	describe	HCL	informally	here.	A	complete
reference	for	HCL	can	be	found	in	Web	Aside	
ARCH
:
HCL</p>
<p>on	page	472.
Aside	
Modern	logic	design
At	one	time,	hardware	designers	created	circuit	designs	by
drawing	schematic	diagrams	of	logic	circuits	(first	with	paper	and
pencil,	and	later	with	computer	graphics	terminals).	Nowadays,
most	designs	are	expressed	in	a	
hardware	description	language
(HDL),	a	textual	notation	that	looks	similar	to	a	programming</p>
<p>language	but	that	is	used	to	describe	hardware	structures	rather
than	program	behaviors.	The	most	commonly	used	languages	are
Verilog,	having	a	syntax	similar	to	C,	and	VHDL,	having	a	syntax
similar	to	the	Ada	programming	language.	These	languages	were
originally	designed	for	creating	simulation	models	of	digital
circuits.	In	the	mid-1980s,	researchers	developed	
logic	synthesis
programs	that	could	generate	efficient	circuit	designs	from	HDL
descriptions.	There	are	now	a	number	of	commercial	synthesis
programs,	and	this	has	become	the	dominant	technique	for
generating	digital	circuits.	This	shift	from	hand-designed	circuits	to
synthesized	ones	can	be	likened	to	the	shift	from	writing	programs
in	assembly	code	to	writing	them	in	a	high-level	language	and
having	a	compiler	generate	the	machine	code.
Our	HCL	language	expresses	only	the	control	portions	of	a
hardware	design,	with	only	a	limited	set	of	operations	and	with	no
modularity.	As	we	will	see,	however,	the	control	logic	is	the	most
difficult	part	of	designing	a	microprocessor.	We	have	developed
tools	that	can	directly	translate	HCL	into	Verilog,	and	by	combining
this	code	with	Verilog	code	for	the	basic	hardware	units,	we	can
generate	HDL	descriptions	from	which	actual	working
microprocessors	can	be	synthesized.	By	carefully	separating	out,
designing,	and	testing	the	control	logic,	we	can	create	a	working
microprocessor	with	reasonable	effort.	Web	Aside	
ARCH
:
VLOG</p>
<p>on
page	467	describes	how	we	can	generate	Verilog	versions	of	a
Y86-64	processor.
Figure	
4.9	
Logic	gate	types.</p>
<p>Each	gate	generates	output	equal	to	some	Boolean	function	of	its	inputs.
4.2.1	
Logic	Gates
Logic	gates	are	the	basic	computing	elements	for	digital	circuits.	They
generate	an	output	equal	to	some	Boolean	function	of	the	bit	values	at
their	inputs.	
Figure	
4.9
shows	the	standard	symbols	used	for	Boolean
functions	
AND
,	
OR
,	and	
NOT
.	
HCL	expressions	are	shown	below	the	gates
for	the	operators	in	C	(
Section	
2.1.8
):	
for	
AND
,	||	for	
OR
,	and	!	for
NOT
.	
We	use	these	instead	of	the	bit-level	C	operators	
,	|,	and	~,
because	logic	gates	operate	on	single-bit	quantities,	not	entire	words.
Although	the	figure	illustrates	only	two-input	versions	of	the	
AND</p>
<p>and	
OR
gates,	it	is	common	to	see	these	being	used	as	
n
-way	operations	for	
n
&gt;
2.	We	still	write	these	in	HCL	using	binary	operators,	though,	so	the
operation	of	a	three-input	
AND</p>
<p>gate	with	inputs	a,	b,	and	c	is	described
with	the	HCL	expression	a	
b	
c.
Logic	gates	are	always	active.	If	some	input	to	a	gate	changes,	then
within	some	small	amount	of	time,	the	output	will	change	accordingly.
Figure	
4.10	
Combinational	circuit	to	test	for	bit	equality.
The	output	will	equal	1	when	both	inputs	are	0	or	both	are	1.</p>
<p>4.2.2	
Combinational	Circuits	and
HCL	Boolean	Expressions
By	assembling	a	number	of	logic	gates	into	a	network,	we	can	construct
computational	blocks	known	as	
combinational	circuits.
Several
restrictions	are	placed	on	how	the	networks	are	constructed:
Every	logic	gate	input	must	be	connected	to	exactly	one	of	the
following:	(1)	one	of	the	system	inputs	(known	as	a	
primary	input
),	(2)
the	output	connection	of	some	memory	element,	or	(3)	the	output	of
some	logic	gate.
The	outputs	of	two	or	more	logic	gates	cannot	be	connected	together.
Otherwise,	the	two	could	try	to	drive	the	wire	toward	different
voltages,	possibly	causing	an	invalid	voltage	or	a	circuit	malfunction.
The	network	must	be	
acyclic.
That	is,	there	cannot	be	a	path	through
a	series	of	gates	that	forms	a	loop	in	the	network.	Such	loops	can
cause	ambiguity	in	the	function	computed	by	the	network.
Figure	
4.10
shows	an	example	of	a	simple	combinational	circuit	that
we	will	find	useful.	It	has	two	inputs,	a	and	b.	It	generates	a	single	output
eq,	such	that	the	output	will	equal	1	if	either	a	and	b	are	both	1	(detected
by	the	upper	
AND</p>
<p>gate)	or	are	both	0	(detected	by	the	lower	
AND</p>
<p>gate).	We
write	the	function	of	this	network	in	HCL	as</p>
<h2>This	code	simply	defines	the	bit-level	(denoted	by	data	type	
)	signal
eq	as	a	function	of	inputs	a	and	b.	As	this	example	shows,	HCL	uses	C-
style	syntax,	with	`='	associating	a	signal	name	with	an	expression.
Unlike	C,	however,	we	do	not	view	this	as	performing	a	computation	and
assigning	the	result	to	some	memory	location.	Instead,	it	is	simply	a	way
to	give	a	name	to	an	expression.
Practice	Problem	
4.9	
(solution	page	
484
)
Write	an	HCL	expression	for	a	signal	
,	equal	to	the	
EXCLUSIVE</h2>
<p>OR</p>
<p>of
inputs	a	and	b.	What	is	the	relation	between	the	signals	
and	eq
defined	above?
Figure	
4.11
shows	another	example	of	a	simple	but	useful
combinational	circuit	known	as	a	
multiplexor
(commonly	referred	to	as	a
&quot;MUX&quot;).	A	multiplexor
Figure	
4.11	
Single-bit	multiplexor	circuit.
The	output	will	equal	input	a	if	the	control	signal	s	is	1	and	will	equal	input
b	when	s	is	0.
selects	a	value	from	among	a	set	of	different	data	signals,	depending	on
the	value	of	a	control	input	signal.	In	this	single-bit	multiplexor,	the	two</p>
<p>data	signals	are	the	input	bits	a	and	b,	while	the	control	signal	is	the	input
bit	s.	The	output	will	equal	a	when	s	is	1,	and	it	will	equal	b	when	s	is	0.
In	this	circuit,	we	can	see	that	the	two	
AND</p>
<p>gates	determine	whether	to
pass	their	respective	data	inputs	to	the	
OR</p>
<p>gate.	The	upper	
AND</p>
<p>gate
passes	signal	b	when	s	is	0	(since	the	other	input	to	the	gate	is	!s),	while
the	lower	
AND</p>
<p>gate	passes	signal	a	when	s	is	1.	Again,	we	can	write	an
HCL	expression	for	the	output	signal,	using	the	same	operations	as	are
present	in	the	combinational	circuit:
Our	HCL	expressions	demonstrate	a	clear	parallel	between
combinational	logic	circuits	and	logical	expressions	in	C.	They	both	use
Boolean	operations	to	compute	functions	over	their	inputs.	Several
differences	between	these	two	ways	of	expressing	computation	are	worth
noting:
Since	a	combinational	circuit	consists	of	a	series	of	logic	gates,	it	has
the	property	that	the	outputs	continually	respond	to	changes	in	the
inputs.	If	some	input	to	the	circuit	changes,	then	after	some	delay,	the
outputs	will	change	accordingly.	By	contrast,	a	C	expression	is	only
evaluated	when	it	is	encountered	during	the	execution	of	a	program.
Logical	expressions	in	C	allow	arguments	to	be	arbitrary	integers,
interpreting	0	as	
FALSE</p>
<p>and	anything	else	as	
TRUE
.	
In	contrast,	our	logic
gates	only	operate	over	the	bit	values	0	and	1.
Logical	expressions	in	C	have	the	property	that	they	might	only	be
partially	evaluated.	If	the	outcome	of	an	
AND</p>
<p>or	
OR</p>
<p>operation	can	be</p>
<p>determined	by	just	evaluating	the	first	argument,	then	the	second
argument	will	not	be	evaluated.	For	example,	with	the	C	expression
the	function	
will	not	be	called,	because	the	expression	(
)
evaluates	to	0.	In	contrast,	combinational	logic	does	not	have	any	partial
evaluation	rules.	The	gates	simply	respond	to	changing	inputs.
Figure	
4.12	
Word-level	equality	test	circuit.
The	output	will	equal	1	when	each	bit	from	word	A	equals	its	counterpart
from	word	B.	Word-level	equality	is	one	of	the	operations	in	HCL.</p>
<p>4.2.3	
Word-Level	Combinational
Circuits	and	HCL	Integer
Expressions
By	assembling	large	networks	of	logic	gates,	we	can	construct
combinational	circuits	that	compute	much	more	complex	functions.
Typically,	we	design	circuits	that	operate	on	data	
words.
These	are
groups	of	bit-level	signals	that	represent	an	integer	or	some	control
pattern.	For	example,	our	processor	designs	will	contain	numerous
words,	with	word	sizes	ranging	between	4	and	64	bits,	representing
integers,	addresses,	instruction	codes,	and	register	identifiers.
Combinational	circuits	that	perform	word-level	computations	are
constructed	using	logic	gates	to	compute	the	individual	bits	of	the	output
word,	based	on	the	individual	bits	of	the	input	words.	For	example,
Figure	
4.12
shows	a	combinational	circuit	that	tests	whether	two	64-bit
words	A	and	B	are	equal.	That	is,	the	output	will	equal	1	if	and	only	if
each	bit	of	A	equals	the	corresponding	bit	of	B.	This	circuit	is
implemented	using	64	of	the	single-bit	equality	circuits	shown	in	
Figure
4.10
.	The	outputs	of	these	single-bit	circuits	are	combined	with	an	
AND
gate	to	form	the	circuit	output.
In	HCL,	we	will	declare	any	word-level	signal	as	an	
,	without
specifying	the	word	size.	This	is	done	for	simplicity.	In	a	full-featured
hardware	description	language,	every	word	can	be	declared	to	have	a
specific	number	of	bits.	HCL	allows	words	to	be	compared	for	equality,</p>
<h2>and	so	the	functionality	of	the	circuit	shown	in	
Figure	
4.12
can	be
expressed	at	the	word	level	as
where	arguments	A	and	B	are	of	type	int.	Note	that	we	use	the	same
syntax	conventions	as	in	C,	where	<code>='	denotes	assignment	and	</code>=='
denotes	the	equality	operator.
As	is	shown	on	the	right	side	of	
Figure	
4.12
,	we	will	draw	word-level
circuits	using	medium-thickness	lines	to	represent	the	set	of	wires
carrying	the	individual	bits	of	the	word,	and	we	will	show	a	single-bit
signal	as	a	dashed	line.
Practice	Problem	
4.10	
(solution	page	
484
)
Suppose	you	want	to	implement	a	word-level	equality	circuit	using	the
EXCLUSIVE</h2>
<p>OR</p>
<h2>circuits	from	
Problem	
4.9
rather	than	from	bit-level
equality	circuits.	Design	such	a	circuit	for	a	64-bit	word	consisting	of	64
bit-level	
EXCLUSIVE</h2>
<p>OR</p>
<p>circuits	and	two	additional	logic	gates.
Figure	
4.13
shows	the	circuit	for	a	word-level	multiplexor.	This	circuit
generates	a	64-bit	word	Out	equal	to	one	of	the	two	input	words,	A	or	B,
depending	on	the	control	input	bit	s.	The	circuit	consists	of	64	identical
subcircuits,	each	having	a	structure	similar	to	the	bit-level	multiplexor
from	
Figure	
4.11
.	Rather	than	replicating	the	bit-level	multiplexor	64
times,	the	word-level	version	reduces	the	number	of	inverters	by
generating	!s	once	and	reusing	it	at	each	bit	position.</p>
<p>Figure	
4.13	
Word-level	multiplexor	circuit.
The	output	will	equal	input	word	A	when	the	control	signal	s	is	1,	and	it
will	equal	B	otherwise.	Multiplexors	are	described	in	HCL	using	case
expressions.
We	will	use	many	forms	of	multiplexors	in	our	processor	designs.	They
allow	us	to	select	a	word	from	a	number	of	sources	depending	on	some
control	condition.	Multiplexing	functions	are	described	in	HCL	using	
case
expressions.
A	case	expression	has	the	following	general	form:</p>
<p>⋮
The	expression	contains	a	series	of	cases,	where	each	case	
i
consists	of
a	Boolean	expression	
select
,	indicating	when	this	case	should	be
selected,	and	an	integer	expression	
expr
,	indicating	the	resulting	value.
Unlike	the	switch	statement	of	C,	we	do	not	require	the	different	selection
expressions	to	be	mutually	exclusive.	Logically,	the	selection	expressions
are	evaluated	in	sequence,	and	the	case	for	the	first	one	yielding	1	is
selected.	For	example,	the	word-level	multiplexor	of	
Figure	
4.13
can
be	described	in	HCL	as
In	this	code,	the	second	selection	expression	is	simply	1,	indicating	that
this	case	should	be	selected	if	no	prior	one	has	been.	This	is	the	way	to
specify	a	default	case	in	HCL.	Nearly	all	case	expressions	end	in	this
manner.
Allowing	nonexclusive	selection	expressions	makes	the	HCL	code	more
readable.	An	actual	hardware	multiplexor	must	have	mutually	exclusive
signals	controlling	which	input	word	should	be	passed	to	the	output,	such
as	the	signals	s	and	!s	in	
Figure	
4.13
.	To	translate	an	HCL	case
i
i</p>
<p>expression	into	hardware,	a	logic	synthesis	program	would	need	to
analyze	the	set	of	selection	expressions	and	resolve	any	possible
conflicts	by	making	sure	that	only	the	first	matching	case	would	be
selected.
The	selection	expressions	can	be	arbitrary	Boolean	expressions,	and
there	can	be	an	arbitrary	number	of	cases.	This	allows	case	expressions
to	describe	blocks	where	there	are	many	choices	of	input	signals	with
complex	selection	criteria.	For	example,	consider	the	diagram	of	a	4-way
multiplexor	shown	in	
Figure	
4.14
.	This	circuit	selects	from	among	the
four	input	words	A,	B,	C,	and	D	based	on	the	control	signals	s1	and	s0,
treating	the	controls	as	a	2-bit	binary	number.	We	can	express	this	in
HCL	using	Boolean	expressions	to	describe	the	different	combinations	of
control	bit	patterns:
Figure	
4.14	
Four-way	multiplexor.
The	different	combinations	of	control	signals	s1	and	s0	determine	which
data	input	is	transmitted	to	the	output.</p>
<p>The	comments	on	the	right	(any	text	starting	with	#	and	running	for	the
rest	of	the	line	is	a	comment)	show	which	combination	of	s1	and	s0	will
cause	the	case	to	be	selected.	Observe	that	the	selection	expressions
can	sometimes	be	simplified,	since	only	the	first	matching	case	is
selected.	For	example,	the	second	expression	can	be	written	
,	rather
than	the	more	complete	
,	since	the	only	other	possibility	having
equal	to	0	was	given	as	the	first	selection	expression.	Similarly,	the
third	expression	can	be	written	as	
,	while	the	fourth	can	simply	be
written	as	1.
As	a	final	example,	suppose	we	want	to	design	a	logic	circuit	that	finds
the	minimum	value	among	a	set	of	words	A,	B,	and	C,	diagrammed	as
follows:
We	can	express	this	using	an	HCL	case	expression	as</p>
<p>Practice	Problem	
4.11	
(solution	page	
484
)
The	HCL	code	given	for	computing	the	minimum	of	three	words
contains	four	comparison	expressions	of	the	form	
X
&lt;=	
Y.
Rewrite
the	code	to	compute	the	same	result,	but	using	only	three
comparisons.
Figure	
4.15	
Arithmetic/logic	unit	(ALU).
Depending	on	the	setting	of	the	function	input,	the	circuit	will	perform	one
of	four	different	arithmetic	and	logical	operations.
Practice	Problem	
4.12	
(solution	page	
484
)
Write	HCL	code	describing	a	circuit	that	for	word	inputs	A,	B,	and
C	selects	the	
median
of	the	three	values.	That	is,	the	output
equals	the	word	lying	between	the	minimum	and	maximum	of	the
three	inputs.
Combinational	logic	circuits	can	be	designed	to	perform	many	different
types	of	operations	on	word-level	data.	The	detailed	design	of	these	is</p>
<p>beyond	the	scope	of	our	presentation.	One	important	combinational
circuit,	known	as	an	
arithmetic/logic	unit
(ALU),	is	diagrammed	at	an
abstract	level	in	
Figure	
4.15
.	In	our	version,	the	circuit	has	three
inputs:	two	data	inputs	labeled	A	and	B	and	a	control	input.	Depending	on
the	setting	of	the	control	input,	the	circuit	will	perform	different	arithmetic
or	logical	operations	on	the	data	inputs.	Observe	that	the	four	operations
diagrammed	for	this	ALU	correspond	to	the	four	different	integer
operations	supported	by	the	Y86-64	instruction	set,	and	the	control
values	match	the	function	codes	for	these	instructions	(
Figure	
4.3
).
Note	also	the	ordering	of	operands	for	subtraction,	where	the	A	input	is
subtracted	from	the	B	input.	This	ordering	is	chosen	in	anticipation	of	the
ordering	of	arguments	in	the	
instruction.
4.2.4	
Set	Membership
In	our	processor	designs,	we	will	find	many	examples	where	we	want	to
compare	one	signal	against	a	number	of	possible	matching	signals,	such
as	to	test	whether	the	code	for	some	instruction	being	processed
matches	some	category	of	instruction	codes.	As	a	simple	example,
suppose	we	want	to	generate	the	signals	s1	and	s0	for	the	4-way
multiplexor	of	
Figure	
4.14
by	selecting	the	high-	and	low-order	bits
from	a	2-bit	signal	code,	as	follows:</p>
<p>In	this	circuit,	the	2-bit	signal	code	would	then	control	the	selection
among	the	four	data	words	A,	B,	C,	and	D.	We	can	express	the
generation	of	signals	s1	and	s0	using	equality	tests	based	on	the
possible	values	of	code:
A	more	concise	expression	can	be	written	that	expresses	the	property
that	s1	is	1	when	code	is	in	the	set	{2,	3},	and	s0	is	1	when	code	is	in	the
set	{1,	3}:
The	general	form	of	a	set	membership	test	is
where	the	value	being	tested	
(iexpr)
and	the	candidate	matches	(
iexpr
through	
iexpr
)	are	all	integer	expressions.
4.2.5	
Memory	and	Clocking
1
k</p>
<p>Combinational	circuits,	by	their	very	nature,	do	not	store	any	information.
Instead,	they	simply	react	to	the	signals	at	their	inputs,	generating
outputs	equal	to	some	function	of	the	inputs.	To	create	
sequential	circuits
—that	is,	systems	that	have	state	and	perform	computations	on	that	state
—we	must	introduce	devices	that	store	information	represented	as	bits.
Our	storage	devices	are	all	controlled	by	a	single	
clock
,	a	periodic	signal
that	determines	when	new	values	are	to	be	loaded	into	the	devices.	We
consider	two	classes	of	memory	devices:
Clocked	registers
(or	simply	
registers)
store	individual	bits	or	words.	The	clock	signal	controls
the	loading	of	the	register	with	the	value	at	its	input.
Random	access	memories
(or	simply	
memories
)	store	multiple	words,	using	an	address	to
select	which	word	should	be	read	or	written.	Examples	of	random	access	memories	include
(1)	the	virtual	memory	system	of	a	processor,	where	a	combination	of	hardware	and	operating
system	software	make	it	appear	to	a	processor	that	it	can	access	any	word	within	a	large
address	space;	and	(2)	the	register	file,	where	register	identifiers	serve	as	the	addresses.	In	a
Y86-64	processor,	the	register	file	holds	the	15	program	registers	(
through	
).
As	we	can	see,	the	word	&quot;register&quot;	means	two	slightly	different	things
when	speaking	of	hardware	versus	machine-language	programming.	In
hardware,	a	register	is	directly	connected	to	the	rest	of	the	circuit	by	its
input	and	output	wires.	In	machine-level	programming,	the	registers
represent	a	small	collection	of	addressable	words	in	the	CPU,	where	the
addresses	consist	of	register	IDs.	These	words	are	generally	stored	in
the	register	file,	although	we	will	see	that	the	hardware	can	sometimes
pass	a	word	directly	from	one	instruction	to	another	to</p>
<p>Figure	
4.16	
Register	operation.
The	register	outputs	remain	held	at	the	current	register	state	until	the
clock	signal	rises.	When	the	clock	rises,	the	values	at	the	register	inputs
are	captured	to	become	the	new	register	state.
avoid	the	delay	of	first	writing	and	then	reading	the	register	file.	When
necessary	to	avoid	ambiguity,	we	will	call	the	two	classes	of	registers
&quot;hardware	registers&quot;	and	&quot;program	registers,&quot;	respectively.
Figure	
4.16
gives	a	more	detailed	view	of	a	hardware	register	and
how	it	operates.	For	most	of	the	time,	the	register	remains	in	a	fixed	state
(shown	as	x),	generating	an	output	equal	to	its	current	state.	Signals
propagate	through	the	combinational	logic	preceding	the	register,
creating	a	new	value	for	the	register	input	(shown	as	y),	but	the	register
output	remains	fixed	as	long	as	the	clock	is	low.	As	the	clock	rises,	the
input	signals	are	loaded	into	the	register	as	its	next	state	(y),	and	this
becomes	the	new	register	output	until	the	next	rising	clock	edge.	A	key
point	is	that	the	registers	serve	as	barriers	between	the	combinational
logic	in	different	parts	of	the	circuit.	Values	only	propagate	from	a	register
input	to	its	output	once	every	clock	cycle	at	the	rising	clock	edge.	Our
Y86-64	processors	will	use	clocked	registers	to	hold	the	program	counter
(PC),	the	condition	codes	(CC),	and	the	program	status	(Stat).
The	following	diagram	shows	a	typical	register	file:</p>
<p>This	register	file	has	two	
read	ports
,	named	A	and	B,	and	one	
write	port
,
named	W.	Such	a	
multiported
random	access	memory	allows	multiple
read	and	write	operations	to	take	place	simultaneously.	In	the	register	file
diagrammed,	the	circuit	can	read	the	values	of	two	program	registers	and
update	the	state	of	a	third.	Each	port	has	an	address	input,	indicating
which	program	register	should	be	selected,	and	a	data	output	or	input
giving	a	value	for	that	program	register.	The	addresses	are	register
identifiers,	using	the	encoding	shown	in	
Figure	
4.4
.	The	two	read	ports
have	address	inputs	srcA	and	srcB	(short	for	&quot;source	A&quot;	and	&quot;source	B&quot;)
and	data	
outputs	valA	and	valB	(short	for	&quot;value	A&quot;	and	&quot;value	B&quot;).	The
write	port	has	address	input	dstW	(short	for	&quot;destination	W&quot;)	and	data
input	valW	(short	for	&quot;value	W&quot;).
The	register	file	is	not	a	combinational	circuit,	since	it	has	internal
storage.	In	our	implementation,	however,	data	can	be	read	from	the
register	file	as	if	it	were	a	block	of	combinational	logic	having	addresses
as	inputs	and	the	data	as	outputs.	When	either	srcA	or	srcB	is	set	to
some	register	ID,	then,	after	some	delay,	the	value	stored	in	the
corresponding	program	register	will	appear	on	either	valA	or	valB.	For
example,	setting	srcA	to	3	will	cause	the	value	of	program	register	
to	be	read,	and	this	value	will	appear	on	output	valA.
The	writing	of	words	to	the	register	file	is	controlled	by	the	clock	signal	in
a	manner	similar	to	the	loading	of	values	into	a	clocked	register.	Every</p>
<p>time	the	clock	rises,	the	value	on	input	valW	is	written	to	the	program
register	indicated	by	the	register	ID	on	input	dstW.	When	dstW	is	set	to
the	special	ID	value	
,	no	program	register	is	written.	Since	the	register
file	can	be	both	read	and	written,	a	natural	question	to	ask	is,	&quot;What
happens	if	the	circuit	attempts	to	read	and	write	the	same	register
simultaneously?&quot;	The	answer	is	straightforward:	if	the	same	register	ID	is
used	for	both	a	read	port	and	the	write	port,	then,	as	the	clock	rises,	there
will	be	a	transition	on	the	read	port's	data	output	from	the	old	value	to	the
new.	When	we	incorporate	the	register	file	into	our	processor	design,	we
will	make	sure	that	we	take	this	property	into	consideration.
Our	processor	has	a	random	access	memory	for	storing	program	data,
as	illustrated	below:
This	memory	has	a	single	address	input,	a	data	input	for	writing,	and	a
data	output	for	reading.	Like	the	register	file,	reading	from	our	memory
operates	in	a	manner	similar	to	combinational	logic:	If	we	provide	an
address	on	the	address	input	and	set	the	write	control	signal	to	0,	then
after	some	delay,	the	value	stored	at	that	address	will	appear	on	data	out.
The	error	signal	will	be	set	to	1	if	the	address	is	out	of	range,	and	to	0
otherwise.	Writing	to	the	memory	is	controlled	by	the	clock:	We	set
address	to	the	desired	address,	data	in	to	the	desired	value,	and	write	to</p>
<ol>
<li>When	we	then	operate	the	clock,	the	specified	location	in	the	memory
will	be	updated,	as	long	as	the	address	is	valid.	As	with	the	read</li>
</ol>
<p>operation,	the	error	signal	will	be	set	to	1	if	the	address	is	invalid.	This
signal	is	generated	by	combinational	logic,	since	the	required	bounds
checking	is	purely	a	function	of	the	address	input	and	does	not	involve
saving	any	state.
Aside	
Real-life	memory	design
The	memory	system	in	a	full-scale	microprocessor	is	far	more
complex	than	the	simple	one	we	assume	in	our	design.	It	consists
of	several	forms	of	hardware	memories,	including	several	random
access	memories,	plus	nonvolatile	memory	or	magnetic	disk,	as
well	as	a	variety	of	hardware	and	software	mechanisms	for
managing	these	devices.	The	design	and	characteristics	of	the
memory	system	are	described	in	
Chapter	
6
.
Nonetheless,	our	simple	memory	design	can	be	used	for	smaller
systems,	and	it	provides	us	with	an	abstraction	of	the	interface
between	the	processor	and	memory	for	more	complex	systems.
Our	processor	includes	an	additional	read-only	memory	for	reading
instructions.	In	most	actual	systems,	these	memories	are	merged	into	a
single	memory	with	two	ports:	one	for	reading	instructions,	and	the	other
for	reading	or	writing	data.</p>
<p>4.3	
Sequential	Y86-64
Implementations
Now	we	have	the	components	required	to	implement	a	Y86-64
processor.	As	a	first	step,	we	describe	a	processor	called	SEQ	(for
&quot;sequential&quot;	processor).	On	each	clock	cycle,	SEQ	performs	all	the	steps
required	to	process	a	complete	instruction.	This	would	require	a	very	long
cycle	time,	however,	and	so	the	clock	rate	would	be	unacceptably	low.
Our	purpose	in	developing	SEQ	is	to	provide	a	first	step	toward	our
ultimate	goal	of	implementing	an	efficient	pipelined	processor.
4.3.1	
Organizing	Processing	into
Stages
In	general,	processing	an	instruction	involves	a	number	of	operations.
We	organize	them	in	a	particular	sequence	of	stages,	attempting	to	make
all	instructions	follow	a	uniform	sequence,	even	though	the	instructions
differ	greatly	in	their	actions.	The	detailed	processing	at	each	step
depends	on	the	particular	instruction	being	executed.	Creating	this
framework	will	allow	us	to	design	a	processor	that	makes	best	use	of	the
hardware.	The	following	is	an	informal	description	of	the	stages	and	the
operations	performed	within	them:</p>
<p>Fetch.	
The	fetch	stage	reads	the	bytes	of	an	instruction	from	memory,
using	the	program	counter	(PC)	as	the	memory	address.	From	the
instruction	it	extracts	the	two	4-bit	portions	of	the	instruction	specifier
byte,	referred	to	as	icode	(the	instruction	code)	and	ifun	(the
instruction	function).	It	possibly	fetches	a	register	specifier	byte,	giving
one	or	both	of	the	register	operand	specifiers	rA	and	rB.	It	also
possibly	fetches	an	8-byte	constant	word	valC.	It	computes	valP	to	be
the	address	of	the	instruction	following	the	current	one	in	sequential
order.	That	is,	valP	equals	the	value	of	the	PC	plus	the	length	of	the
fetched	instruction.
Decode.	
The	decode	stage	reads	up	to	two	operands	from	the
register	file,	giving	values	valA	and/or	valB.	Typically,	it	reads	the
registers	designated	by	instruction	fields	rA	and	rB,	but	for	some
instructions	it	reads	register	
.
Execute.	
In	the	execute	stage,	the	arithmetic/logic	unit	(ALU)	either
performs	the	operation	specified	by	the	instruction	(according	to	the
value	of	ifun),	computes	the	effective	address	of	a	memory	reference,
or	increments	or	decrements	the	stack	pointer.	We	refer	to	the
resulting	value	as	valE.	The	condition	codes	are	possibly	set.	For	a
conditional	move	instruction,	the	stage	will	evaluate	the	condition
codes	and	move	condition	(given	by	ifun)	and	enable	the	updating	of
the	destination	register	only	if	the	condition	holds.	Similarly,	for	a	jump
instruction,	it	determines	whether	or	not	the	branch	should	be	taken.
Memory.	
The	memory	stage	may	write	data	to	memory,	or	it	may	read
data	from	memory.	We	refer	to	the	value	read	as	valM.
Write	back.	
The	write-back	stage	writes	up	to	two	results	to	the
register	file.</p>
<p>PC	update.	
The	PC	is	set	to	the	address	of	the	next	instruction.
The	processor	loops	indefinitely,	performing	these	stages.	In	our
simplified	implementation,	the	processor	will	stop	when	any	exception
occurs—that	is,	when	it	executes	a	
or	invalid	instruction,	or	it
attempts	to	read	or	write	an	invalid	address.	In	a	more	complete	design,
the	processor	would	enter	an	exception-handling	mode	and	begin
executing	special	code	determined	by	the	type	of	exception.
As	can	be	seen	by	the	preceding	description,	there	is	a	surprising
amount	of	processing	required	to	execute	a	single	instruction.	Not	only
must	we	perform	the	stated	operation	of	the	instruction,	we	must	also
compute	addresses,	update	stack	pointers,	and	determine	the	next
instruction	address.	Fortunately,	the	overall	flow	can	be	similar	for	every
instruction.	Using	a	very	simple	and	uniform	structure	is	important	when
designing	hardware,	since	we	want	to	minimize	the	total	amount	of
hardware	and	we	must	ultimately	map	it	onto	the	two-dimensional
surface	of	an	integrated-circuit	chip.	One	way	to	minimize	the	complexity
is	to	have	the	different	instructions	share	as	much	of	the	hardware	as
possible.	For	example,	each	of	our	processor	designs	contains	a	single
arithmetic/logic	unit	that	is	used	in	different	ways	depending	on	the	type
of	instruction	being	executed.	The	cost	of	duplicating	blocks	of	logic	in
hardware	is	much	higher	than	the	cost	of	having	multiple	copies	of	code
in	software.	It	is	also	more	difficult	to	deal	with	many	special	cases	and
idiosyncrasies	in	a	hardware	system	than	with	software.
Our	challenge	is	to	arrange	the	computing	required	for	each	of	the
different	instructions	to	fit	within	this	general	framework.	We	will	use	the
code	shown	in	
Figure	
4.17
to	illustrate	the	processing	of	different	Y86-</p>
<p>64	instructions.	
Figures	
4.18
through	
4.21
contain	tables	describing
how	the	different	Y86-64	instructions	proceed	through	the	stages.	It	is
worth	the	effort	to	study	these	tables	carefully.	They	are	in	a	form	that
enables	a	straightforward	mapping	into	the	hardware.	Each	line	in	these
tables	describes	an	assignment	to	some	signal	or	stored	state</p>
<p>Figure	
4.17	
Sample	Y86-64	instruction	sequence.
We	will	trace	the	processing	of	these	instructions	through	the	different
stages.
(indicated	by	the	assignment	operation	‘←’).	These	should	be	read	as	if
they	were	evaluated	in	sequence	from	top	to	bottom.	When	we	later	map
the	computations	to	hardware,	we	will	find	that	we	do	not	need	to	perform
these	evaluations	in	strict	sequential	order.
Figure	
4.18
shows	the	processing	required	for	instruction	types	
(integer	and	logical	operations),	
(register-register	move),	and
(immediate-register	move).	Let	us	first	consider	the	integer
operations.	Examining	
Figure	
4.2
,	we	can	see	that	we	have	carefully
chosen	an	encoding	of	instructions	so	that	the	four	integer	operations
(
,	and	
)	all	have	the	same	value	of	
.	We	can
handle	them	all	by	an	identical	sequence	of	steps,	except	that	the	ALU
computation	must	be	set	according	to	the	particular	instruction	operation,
encoded	in	ifun.
The	processing	of	an	integer-operation	instruction	follows	the	general
pattern	listed	above.	In	the	fetch	stage,	we	do	not	require	a	constant
word,	and	so	valP	is	computed	as	PC	+	2.	During	the	decode	stage,	we
read	both	operands.	These	are	supplied	to	the	ALU	in	the	execute	stage,
along	with	the	function	specifier	ifun,	so	that	valE	becomes	the	instruction
result.	This	computation	is	shown	as	the	expression	valB	OP	valA,	where
OP	indicates	the	operation	specified	by	ifun.	Note	the	ordering	of	the	two
arguments—this	order	is	consistent	with	the	conventions	of	Y86-64	(and
x86-64).	For	example,	the	instruction	
is	supposed	to
compute	the	value	
.	Nothing	happens	in	the	memory</p>
<p>stage	for	these	instructions,	but	valE	is	written	to	register	rB	in	the	write-
back	stage,	and	the	PC	is	set	to	valP	to	complete	the	instruction
execution.
Executing	an	
instruction	proceeds	much	like	an	arithmetic
operation.	We	do	not	need	to	fetch	the	second	register	operand,
however.	Instead,	we	set	the	second	ALU	input	to	zero	and	add	this	to
the	first,	giving	valE	=	valA,	which	is
Stage
rA,	rB
rA,	rB
V,	rB
Fetch
icode:ifun	←	M
[PC]
rA:rB	←	M
[PC	+	1]
icode:ifun	←	M
[PC]
rA:rB	←	M
[PC	+	1]
icode:ifun	←	M
[PC]
rA:rB	←	M
[PC	+1]
valC	←	M
[PC	+	2]
valP	←	PC+	2
valP	←	PC+	2
valP	←	PC+	10
Decode
valA	←	R[rA]	valB	←	R[rB]
valA	←	R[rA]
Execute
valE	←	valBOPvalA	SetCC
valE	←	0	+	valA
valE	←	0	+	valC
Memory
Write	back
R[rB]	←	valE
R[rB]	←	valE
R[rB]	←	valE
PC	update
PC	←	valP
PC	←	valP
PC	←	valP
Figure	
4.18	
Computations	in	sequential	implementation	of	Y86-64
instructions	
,	and	
.
These	instructions	compute	a	value	and	store	the	result	in	a	register.	The
notation	
:	ifun	indicates	the	two	components	of	the	instruction	byte,
while	rA	:	rB	indicates	the	two	components	of	the	register	specifier	byte.
1
1
1
1
1
1
8</p>
<p>The	notation	M
[
x
]	indicates	accessing	(either	reading	or	writing)	1	byte	at
memory	location	
x
,	while	M
[
x
]	indicates	accessing	8	bytes.
then	written	to	the	register	file.	Similar	processing	occurs	for	
,
except	that	we	use	constant	value	valC	for	the	first	ALU	input.	In	addition,
we	must	increment	the	program	counter	by	10	for	
due	to	the	long
instruction	format.	Neither	of	these	instructions	changes	the	condition
codes.
Practice	Problem	
4.13	
(solution	page	
485
)
Fill	in	the	right-hand	column	of	the	following	table	to	describe	the
processing	of	the	
instruction	on	line	4	of	the	object	code	in
Figure	
4.17
:
Stage
Generic	
V,	rB
Specific	
Fetch
icode:ifun	←	M
[PC]
rA:rB	←	M
[PC	+	1]
valC	←	M
[PC	+	2]
valP	←	PC+	10
Decode
Execute
valE	←	0	+	valC
Aside	
Tracing	the	execution	of	a	
1
8
1
1
8</p>
<p>instruction
As	an	example,	let	us	follow	the	processing	of	the	
instruction
on	line	3	of	the	object	code	shown	in	
Figure	
4.17
.	We	can	see
that	the	previous	two	instructions	initialize	registers	
and	
to	9	and	21,	respectively.	We	can	also	see	that	the	instruction	is
located	at	address	
and	consists	of	2	bytes,	having	values
and	
.	The	stages	would	proceed	as	shown	in	the
following	table,	which	lists	the	generic	rule	for	processing	an	
instruction	(
Figure	
4.18
)	on	the	left,	and	the	computations	for
this	specific	instruction	on	the	right.
Stage
rA,	rB
Fetch
icode:ifun	←	M
[PC]
rA:rB	←	MT.PC	+	1]
icode:ifun	←	M
[
]	=	6:1
rA:rB	←	M
[
]	=	2:3
valP	←	PC+	2
valP	←	
Decode
valA	←	R[rA]
valB	←	R[rB]
valA	←	R[
]	=	9
va	IB	←	R[
]	=	21
Execute
valE	←	valBOPvalA
SetCC
valE	←	21	-	9=12
ZF	←	0,	SF	←	0,	OF	←	0
Memory
Write	back
R[rB]	←	valE
R[%rbx]	←	valE	=	12
PC	update
PC	←	valP
PC	←	valP	=	
As	this	trace	shows,	we	achieve	the	desired	effect	of	setting
register	
to	12,	setting	all	three	condition	codes	to	zero,	and
1
1
1</p>
<p>incrementing	the	PC	by	2.
Stage
Generic	
V,	rB
Specific	
Memory
Writeback
R[rB]	←	valE
PC	update
PC	←	va	IP
How	does	this	instruction	execution	modify	the	registers	and	the	PC?
Figure	
4.19
shows	the	processing	required	for	the	memory	write	and
read	instructions	
and	
.	We	see	the	same	basic	flow	as
before,	but	using	the	ALU	to	add	valC	to	valB,	giving	the	effective
address	(the	sum	of	the	displacement	and	the	base	register	value)	for	the
memory	operation.	In	the	memory	stage,	we	either	write	the	register
value	valA	to	memory	or	read	valM	from	memory.
Stage
rA,	D(rB)
D	(rB),	rA
Fetch
icode:ifun	←	M
[PC]
rA:rB	←	M
[PC	+	1]
valC	←	M
[PC	+	2]
valP	←	PC+	10
icode:ifun	←	M
[PC]
rA:rB	←	M
[PC	+	1]
valC	←	M
[PC	+	2]
valP	←	PC+	10
Decode
valA	←	R[rA]
valB	←	R[rB]
valB	←	R[rB]
Execute
valE	←	valB	+	valC
valE	←	valB	+	valC
Memory
M
[valE]	←	valA
valM	←	M
[valE]
Write	back
1
1
8
1
1
8
8
8</p>
<p>R[rA]	←	valM
PC	update
PC	←	valP
PC	←	valP
Figure	
4.19	
Computations	in	sequential	implementation	of	Y86-64
instructions	</p>
<p>and</p>
<p>.
These	instructions	read	or	write	memory.
Figure	
4.20
shows	the	steps	required	to	process	
and	
instructions.	These	are	among	the	most	difficult	Y86-64	instructions	to
implement,	because	they	involve	both	accessing	memory	and
incrementing	or	decrementing	the	stack	pointer.	Although	the	two
instructions	have	similar	flows,	they	have	important	differences.
The	
instruction	starts	much	like	our	previous	instructions,	but	in	the
decode	stage	we	use	
as	the	identifier	for	the	second	register
operand,	giving	the	stack	pointer	as	value	valB.	In	the	execute	stage,	we
use	the	ALU	to	decrement	the	stack	pointer	by	8.	This	decremented
value	is	used	for	the	memory	write	address	and	is	also	stored	back	to
in	the	write-back	stage.	By	using	valE	as	the	address	for	the	write
operation,	we	adhere	to	the	Y86-64	(and	x86-64)	convention	that	
should	decrement	the	stack	pointer	before	writing,	even	though	the	actual
updating	of	the	stack	pointer	does	not	occur	until	after	the	memory
operation	has	completed.
The	
instruction	proceeds	much	like	
,	except	that	we	read	two
copies	of	the	stack	pointer	in	the	decode	stage.	This	is	clearly	redundant,
but	we	will	see	that	having	the	stack	pointer	as	both	valA	and	valB	makes</p>
<p>the	subsequent	flow	more	similar	to	that	of	other	instructions,	enhancing
the	overall	uniformity	of	the	design.	We	use	the	ALU	to	increment	the
stack	pointer	by	8	in	the	execute	stage,	but	use	the	unincremented	value
as	the	address	for	the	memory	operation.	In	the	write-back	stage,	we
update	both	the	stack	pointer	register	with	the	incremented	stack	pointer
and	register	rA	with	the	value	read	from	memory.	Using	the
unincremented	stack	pointer	as	the	memory	read	address	preserves	the
Y86-64
Aside	
Tracing	the	execution	of	an
instruction
Let	us	trace	the	processing	of	the	
instruction	on	line	5	of
the	object	code	shown	in	
Figure	
4.17
.	We	can	see	that	the
previous	instruction	initialized	register	
to	128,	while	
still
holds	12,	as	computed	by	the	
instruction	(line	3).	We	can
also	see	that	the	instruction	is	located	at	address	
and
consists	of	10	bytes.	The	first	2	bytes	have	values	
and	
,
while	the	final	8	bytes	are	a	byte-reversed	version	of	the	number
(decimal	100).	The	stages	would	proceed	as
follows:
Stage
Generic	
rA,	D(rB)
Specific	</p>
<p>,	100(
)
Fetch
icode:ifun	←	M
[PC]
rA:rB	←	M
[PC	+	1]
valC	←	M
[PC	+	2]
valP	←	PC+	10
icode:ifun	←	M
[
]	=	4:0
rA:rB	←	M
[
]	=	4:3
valC	←	M
[
]	=	100
valP	←	
Decode
valA	←	R[rA]
valA	←	R[
]	=	128
1
1
8
1
1
8</p>
<p>Decode
valA	←	R[rA]
valB	←	R[rB]
valA	←	R[
]	=	128
va	IB	←	R[
]	=	12
Execute
valE	←	valB	+	valC
valE	←	12	+	100	=	112
Memory
M
[valE]	←	valA
M
[112]	←	128
Write	back
PC	update
PC	←	valP
PC	←	
As	this	trace	shows,	the	instruction	has	the	effect	of	writing	128	to
memory	address	112	and	incrementing	the	PC	by	10.
(and	x86-64)	convention	that	popq	should	first	read	memory	and	then
increment	the	stack	pointer.
Practice	Problem	
4.14	
(solution	page	
486
)
Fill	in	the	right-hand	column	of	the	following	table	to	describe	the
processing	of	the	popq	instruction	on	line	7	of	the	object	code	in
Figure	
4.17
.
Stage
Generic	popq	rA
Specific	
Fetch
icode:ifun	←	M
[PC]	
rA:rB	←	M
[PC	+	1]	
valP	←	PC+	2
Stage
rA
rA
Fetch
icode:ifun	←	M
[PC]
rA:rB	←	M
[PC	+	1]
icode:ifun	←	M
[PC]
rA:rB	←	M
[PC	+	1]
8
8
1
1
1
1
1
1</p>
<p>valP	←	PC+	2
valP	←	PC+	2
Decode
valA	←	R[rA]
valB	←	R[
]
valA	←	R[
]
va	IB	←	R[
]
Execute
valE	←	valB+(-8)
valE	←	valB	+	8
Memory
M
[valE]	←	valA
va	IM	←	M
[valA]
Write	back
R[
]	←	valE
R[
]	←	valE	
R[rA]	←	valM
PC	update
PC	←	valP
PC	←	valP
Figure	
4.20	
Computations	in	sequential	implementation	of	Y86-64
instructions	</p>
<p>and</p>
<p>.
These	instructions	push	and	pop	the	stack.
Stage
Generic	
rA
Specific	
Decode
valA	←	R[
]
valB	←	R[
]
Execute
valE	←	valB	+	8
Memory
valM	←	M
[valA]
Write	back
R[
]	←	valE
R[rA]	←	valM
PC	update
PC	←	valP
8
8
8</p>
<p>What	effect	does	this	instruction	execution	have	on	the	registers	and	the
PC?
Practice	Problem	
4.15	
(solution	page	
486
)
What	would	be	the	effect	of	the	instruction	
according	to
the	steps	listed	in	
Figure	
4.20
?	Does	this	conform	to	the
desired	behavior	for	Y86-64,	as	determined	in	
Problem	
4.7
?
Aside	
Tracing	the	execution	of	a	
instruction
Let	us	trace	the	processing	of	the	
instruction	on	line	6	of	the
object	code	shown	in	
Figure	
4.17
.	At	this	point,	we	have	9	in
register	
and	128	in	register	
.	We	can	also	see	that	the
instruction	is	located	at	address	
and	consists	of	2	bytes
having	values	
and	
.	The	stages	would	proceed	as
follows:
Stage
Generic	
rA
Specific	
Fetch
icode:ifun	←	M
[PC]
rA:rB	←	MT.PC	+	1]
icode:ifun	←	M
[
rA:rB	←	M
[
valP	←	PC+	2
valP	←	
Decode
valA	←	R[rA]
valB	←	R[
]
valA	←	R[
]	=	9
valB	←	R[
]	=	128
1
1
1</p>
<p>Execute
valE	←	valB	+	(-8)
valE	←	128+	(-8)	=	120
Memory
M
[valE]	←	valA
M
[120]	←	9
Write	back
R[
]	←	valE
R[
]	←	120
PC	update
PC	←	valP
PC	←	
As	this	trace	shows,	the	instruction	has	the	effect	of	setting	
to	120,	writing	9	to	address	120,	and	incrementing	the	PC	by	2.
Practice	Problem	
4.16	
(solution	page	
486
)
Assume	the	two	register	writes	in	the	write-back	stage	for	
occur	in	the	order	listed	in	
Figure	
4.20
.	What	would	be	the
effect	of	executing	
?	Does	this	conform	to	the	desired
behavior	for	Y86-64,	as	determined	in	
Problem	
4.8
?
Figure	
4.21
indicates	the	processing	of	our	three	control	transfer
instructions:	the	different	jumps,	
,	and	
.	We	see	that	we	can
implement	these	instructions	with	the	same	overall	flow	as	the	preceding
ones.
As	with	integer	operations,	we	can	process	all	of	the	jumps	in	a	uniform
manner,	since	they	differ	only	when	determining	whether	or	not	to	take
the	branch.	A	jump	instruction	proceeds	through	fetch	and	decode	much
like	the	previous	instructions,	except	that	it	does	not	require	a	register
specifier	byte.	In	the	execute	stage,	we	check	the	condition	codes	and
the	jump	condition	to	determine	whether	or	not	to	take	the	branch,
yielding	a	1-bit	signal	
.	During	the	PC	update	stage,	we	test	this	flag
and	set	the	PC	to	valC	(the	jump	target)	if	the	flag	is	1	and	to	valP	(the
8
8</p>
<p>address	of	the	following	instruction)	if	the	flag	is	0.	Our	notation	
x
?	
a
:	
b
is	similar	to	the	conditional	expression	in	C—it	yields	
a
when	
x
is	1	and	
b
when	
x
is	0.
Stage
Dest
Dest
Fetch
icode:ifun	←	M
[PC]
valC	←	M
[PC	+	1]
valP	←	PC+	9
icode:ifun	←	M
[PC]
valC	←	M
[PC	+	1]
valP	←	PC+	9
icode:ifun	←	M
[PC]
valP	←	PC	+	1
Decode
valB	←	R[
]
valA	←	R[
]
valB	←	R[
]
Execute
Cnd	←	Cond(CC,	ifun)
valE	←	valB	+	(-8)
valE	←	valB	+	8
Memory
M
[valE]	←	valP
valM	←	M
[valA]
Write	back
R[
]	←	valE
R[
]	←	valE
PC	update
PC	←	Cnd?valC:valP
PC	←	valC
PC	←	valM
Figure	
4.21	
Computations	in	sequential	implementation	of	Y86-64
instructions	
,	and	
.
These	instructions	cause	control	transfers.
Practice	Problem	
4.17	
(solution	page	
486
)
We	can	see	by	the	instruction	encodings	(
Figures	
4.2
and
4.3
)	that	the	
instruction	is	the	unconditional	version	of	a
more	general	class	of	instructions	that	include	the	conditional
1
8
1
8
1
8
8</p>
<p>moves.	Show	how	you	would	modify	the	steps	for	the	
instruction	below	to	also	handle	the	six	conditional	move
instructions.	You	may	find	it	useful	to	see	how	the	implementation
of	the	
instructions	(
Figure	
4.21
)	handles	conditional
behavior.
Stage
rA,	rB
Fetch
icode:ifun	←	M
[PC]
rA:rB	←	M
[PC	+	1]
valP	←	PC	+	2
Decode
valA	←	R[rA]
Execute
valE	←	0	+	valA
Memory
Write	back
R[rB]	←	valE
PC	update
PC	←	valP
Aside	
Tracing	the	execution	of	a	
instruction
Let	us	trace	the	processing	of	the	je	instruction	on	line	8	of	the
object	code	shown	in	
Figure	
4.17
.	The	condition	codes	were	all
set	to	zero	by	the	
instruction	(line	3),	and	so	the	branch	will
not	be	taken.	The	instruction	is	located	at	address	
and
1
1</p>
<p>consists	of	9	bytes.	The	first	has	value	
,	while	the	remaining	8
bytes	are	a	byte-reversed	version	of	the	number
,	the	jump	target.	The	stages	would	proceed	as
follows:
Stage
Generic	
Dest
Specific	
Fetch
icode:ifun	←	M
[PC]
icode:ifun	←	M
[
valC	←	M
[PC	+	1]
valC	←	M
valP	←	PC+	9
valP	←	
Decode
Execute
Cnd	←	Cond(CC,	ifun)
Cnd	←	Cond
Memory
Write	back
PC	update
PC	←	Cnd?valC:valP
PC	←	0	?	
As	this	trace	shows,	the	instruction	has	the	effect	of	incrementing
the	PC	by	9.
Instructions	call	and	ret	bear	some	similarity	to	instructions	
and
,	except	that	we	push	and	pop	program	counter	values.	With
instruction	call,	we	push	valP,	the	address	of	the	instruction	that	follows
the	call	instruction.	During	the	PC	update	stage,	we	set	the	PC	to	valC,
the	call	destination.	With	instruction	ret,	we	assign	valM,	the	value
popped	from	the	stack,	to	the	PC	in	the	PC	update	stage.
1
1
8
8</p>
<p>Practice	Problem	
4.18	
(solution	page	
487
)
Fill	in	the	right-hand	column	of	the	following	table	to	describe	the
processing	of	the	call	instruction	on	line	9	of	the	object	code	in	
Figure
4.17
:
Stage
Generic	
Dest
Specific	
Fetch
icode:ifun	←	M
[PC]
valC	←	M
[PC	+	1]
valP	←	PC+	9
Aside	
Tracing	the	execution	of	a	ret
instruction
Let	us	trace	the	processing	of	the	ret	instruction	on	line	13	of	the
object	code	shown	in	
Figure	
4.17
.	The	instruction	address	is
and	is	encoded	by	a	single	byte	
.	The	previous	call
instruction	set	
to	120	and	stored	the	return	address	
at
memory	address	120.	The	stages	would	proceed	as	follows:
Stage
Generic	
Specific	
Fetch
icode:ifun	←	M
[PC]
icode:ifun	←	M
valP	←	PC	+	1
valP	←	
Decode
valA	←	R[
]
valA	←	R[
]	=	120
valB	←	R[
]
valB	←	R[
]	=	120
1
8
1
1</p>
<p>Execute
valE	←	valB	+	8
valE	←	120	+	8=128
Memory
valM	←	M
[valA]
valM	←	M
[120]	=	
Write	back
R[
]	←	valE
R[
]	←	128
PC	update
PC	←	valM
PC	←	
As	this	trace	shows,	the	instruction	has	the	effect	of	setting	the	PC
to	
,	the	address	of	the	
instruction.	It	also	sets	
to
128.
Stage
Generic	
Dest
Specific	
Decode
valB	←	R[
]
Execute
valE	←	valB+(-8)
Memory
M
[valE]	←	valP
Write	back
R[
]	←	valE
PC	update
PC	←	valC
What	effect	would	this	instruction	execution	have	on	the	registers,	the
PC,	and	the	memory?
We	have	created	a	uniform	framework	that	handles	all	of	the	different
types	of	Y86-64	instructions.	Even	though	the	instructions	have	widely
varying	behavior,	we	can	organize	the	processing	into	six	stages.	Our
task	now	is	to	create	a	hardware	design	that	implements	the	stages	and
connects	them	together.
8
8
8</p>
<p>4.3.2	
SEQ	Hardware	Structure
The	computations	required	to	implement	all	of	the	Y86-64	instructions
can	be	organized	as	a	series	of	six	basic	stages:	fetch,	decode,	execute,
memory,	write	back,	and	PC	update.	
Figure	
4.22
shows	an	abstract
view	of	a	hardware	structure	that	can	perform	these	computations.	The
program	counter	is	stored	in	a	register,	shown	in	the	lower	left-hand
corner	(labeled	&quot;PC&quot;).	Information	then	flows	along	wires	(shown	grouped
together	as	a	heavy	gray	line),	first	upward	and	then	around	to	the	right.
Processing	is	performed	by	
hardware	units
associated	with	the	different
stages.	The	feedback	paths	coming	back	down	on	the	right-hand	side
contain	the	updated	values	to	write	to	the	register	file	and	the	updated
program	counter.	In	SEQ,	all	of	the	processing	by	the	hardware	units
occurs	within	a	single	clock	cycle,	as	is	discussed	in	
Section	
4.3.3
.
This	diagram	omits	some	small	blocks	of	combinational	logic	as	well	as
all	of	the	control	logic	needed	to	operate	the	different	hardware	units	and
to	route	the	appropriate	values	to	the	units.	We	will	add	this	detail	later.
Our	method	of	drawing	processors	with	the	flow	going	from	bottom	to	top
is	unconventional.	We	will	explain	the	reason	for	this	convention	when	we
start	designing	pipelined	processors.
The	hardware	units	are	associated	with	the	different	processing	stages:
Fetch.	
Using	the	program	counter	register	as	an	address,	the
instruction	memory	reads	the	bytes	of	an	instruction.	The	PC
incrementer	computes	valP,	the	incremented	program	counter.
Decode.	
The	register	file	has	two	read	ports,	A	and	B,	via	which
register	values	valA	and	valB	are	read	simultaneously.</p>
<p>Execute.	
The	execute	stage	uses	the	arithmetic/logic	(ALU)	unit	for
different	purposes	according	to	the	instruction	type.	For	integer
operations,	it	performs	the	specified	operation.	For	other	instructions,
it	serves	as	an	adder	to	compute	an	incremented	or	decremented
stack	pointer,	to	compute	an	effective	address,	or	simply	to	pass	one
of	its	inputs	to	its	outputs	by	adding	zero.
The	condition	code	register	(CC)	holds	the	three	condition	code	bits.
New	values	for	the	condition	codes	are	computed	by	the	ALU.	When
executing	a	conditional	move	instruction,	the	decision	as	to	whether
or	not	to	update	the	destination	register	is	computed	based	on	the
condition	codes	and	move	condition.	Similarly,	when	executing	a	jump
instruction,	the	branch	signal	Cnd	is	computed	based	on	the	condition
codes	and	the	jump	type.
Memory.	
The	data	memory	reads	or	writes	a	word	of	memory	when
executing	a	memory	instruction.	The	instruction	and	data	memories
access	the	same	memory	locations,	but	for	different	purposes.
Write	back.	
The	register	file	has	two	write	ports.	Port	E	is	used	to
write	values	computed	by	the	ALU,	while	port	M	is	used	to	write
values	read	from	the	data	memory.</p>
<p>Figure	
4.22	
Abstract	view	of	SEQ,	a	sequential	implementation.
The	information	processed	during	execution	of	an	instruction	follows	a
clockwise	flow	starting	with	an	instruction	fetch	using	the	program
counter	(PC),	shown	in	the	lower	left-hand	corner	of	the	figure.</p>
<p>PC	update.	
The	new	value	of	the	program	counter	is	selected	to	be
either	valP,	the	address	of	the	next	instruction,	valC,	the	destination
address	specified	by	a	call	or	jump	instruction,	or	valM,	the	return
address	read	from	memory.
Figure	
4.23
gives	a	more	detailed	view	of	the	hardware	required	to
implement	SEQ	(although	we	will	not	see	the	complete	details	until	we
examine	the	individual	stages).	We	see	the	same	set	of	hardware	units
as	earlier,	but	now	the	wires	are	shown	explicitly.	In	this	figure,	as	well	as
in	our	other	hardware	diagrams,	we	use	the	following	drawing
conventions:
Clocked	registers	are	shown	as	white	rectangles.	
The	program
counter	PC	is	the	only	clocked	register	in	SEQ.
Hardware	units	are	shown	as	light	blue	boxes.	
These	include	the
memories,	the	ALU,	and	so	forth.	We	will	use	the	same	basic	set	of
units	for	all	of	our	processor	implementations.	We	will	treat	these	units
as	&quot;black	boxes&quot;	and	not	go	into	their	detailed	designs.
Control	logic	blocks	are	drawn	as	gray	rounded	rectangles.
These	blocks	serve	to	select	from	among	a	set	of	signal	sources	or	to
compute	some	Boolean	function.	We	will	examine	these	blocks	in
complete	detail,	including	developing	HCL	descriptions.
Wire	names	are	indicated	in	white	circles.	
These	are	simply	labels
on	the	wires,	not	any	kind	of	hardware	element.
Word-wide	data	connections	are	shown	as	medium	lines.	
Each	of
these	lines	actually	represents	a	bundle	of	64	wires,	connected	in
parallel,	for	transferring	a	word	from	one	part	of	the	hardware	to
another.</p>
<p>Byte	and	narrower	data	connections	are	shown	as	thin	lines.
Each	of	these	lines	actually	represents	a	bundle	of	four	or	eight	wires,
depending	on	what	type	of	values	must	be	carried	on	the	wires.
Single-bit	connections	are	shown	as	dotted	lines.	
These
represent	control	values	passed	between	the	units	and	blocks	on	the
chip.
All	of	the	computations	we	have	shown	in	
Figures	
4.18
through
4.21
have	the	property	that	each	line	represents	either	the
computation	of	a	specific	value,	such	as	valP,	or	the	activation	of	some
hardware	unit,	such	as	the	memory.	These	computations	and	actions	are
listed	in	the	second	column	of	
Figure	
4.24
.	In	addition	to	the	signals
we	have	already	described,	this	list	includes	four	register	ID	signals:
srcA,	the	source	of	valA;	srcB,	the	source	of	valB;	dstE,	the	register	to
which	valE	gets	written;	and	dstM,	the	register	to	which	valM	gets	written.
The	two	right-hand	columns	of	this	figure	show	the	computations	for	the
and	
instructions	to	illustrate	the	values	being	computed.	To
map	the	computations	into	hardware,	we	want	to	implement	control	logic
that	will	transfer	the	data	between	the	different	hardware	units	and
operate	these	units	in	such	a	way	that	the	specified	operations	are
performed	for	each	of	the	different	instruction	types.	That	is	the	purpose
of	the	control	logic	blocks,	shown	as	gray	rounded	boxes</p>
<p>Figure	
4.23	
Hardware	structure	of	SEQ,	a	sequential
implementation.
Some	of	the	control	signals,	as	well	as	the	register	and	control	word
connections,	are	not	shown.</p>
<p>Stage
Computation
rA,	rB
D(rB),	rA
Fetch
icode,	ifun
icode:ifun	←	M
[PC]
icode:ifun	←	M
[PC]
rA,	rB
rA:rB	←	M
[PC	+	1]
rA:rB	←	M
[PC	+1]
valC
valC	←	M
[PC	+	2]
valP
valP	←	PC	+	2
valP	←	PC+	10
Decode
valA,	srcA
valA	←	R[rA]
valB,	srcB
valB	←	R[rB]
valB	←	R[rB]
Execute
valE	Cond.	codes
valE	←	valB	OP	valA	Set	CC
valE	←	valB	+	valC
Memory
Read/write
valM	←	M
[valE]
Write	back
E	port,	dstE
R[rB]	←	valE
M	port,	dstM
R[rA]	←	valM
PC	update
PC
PC	←	valP
PC	←	valP
Figure	
4.24	
Identifying	the	different	computation	steps	in	the
sequential	implementation.
The	second	column	identifies	the	value	being	computed	or	the	operation
being	performed	in	the	stages	of	SEQ.	The	computations	for	instructions
and	
are	shown	as	examples	of	the	computations.
in	
Figure	
4.23
.	Our	task	is	to	proceed	through	the	individual	stages
and	create	detailed	designs	for	these	blocks.
1
1
1
1
8
8</p>
<p>4.3.3	
SEQ	Timing
In	introducing	the	tables	of	
Figures	
4.18
through	
4.21
,	we	stated
that	they	should	be	read	as	if	they	were	written	in	a	programming
notation,	with	the	assignments	performed	in	sequence	from	top	to
bottom.	On	the	other	hand,	the	hardware	structure	of	
Figure	
4.23
operates	in	a	fundamentally	different	way,	with	a	single	clock	transition
triggering	a	flow	through	combinational	logic	to	execute	an	entire
instruction.	Let	us	see	how	the	hardware	can	implement	the	behavior
listed	in	these	tables.
Our	implementation	of	SEQ	consists	of	combinational	logic	and	two
forms	of	memory	devices:	clocked	registers	(the	program	counter	and
condition	code	register)	and	random	access	memories	(the	register	file,
the	instruction	memory,	and	the	data	memory).	Combinational	logic	does
not	require	any	sequencing	or	control—values	propagate	through	a
network	of	logic	gates	whenever	the	inputs	change.	As	we	have
described,	we	also	assume	that	reading	from	a	random	access	memory
operates	much	like	combinational	logic,	with	the	output	word	generated
based	on	the	address	input.	This	is	a	reasonable	assumption	for	smaller
memories	(such	as	the	register	file),	and	we	can	mimic	this	effect	for
larger	circuits	using	special	clock	circuits.	Since	our	instruction	memory	is
only	used	to	read	instructions,	we	can	therefore	treat	this	unit	as	if	it	were
combinational	logic.
We	are	left	with	just	four	hardware	units	that	require	an	explicit	control
over	their	sequencing—the	program	counter,	the	condition	code	register,
the	data	memory,	and	the	register	file.	These	are	controlled	via	a	single</p>
<p>clock	signal	that	triggers	the	loading	of	new	values	into	the	registers	and
the	writing	of	values	to	the	random	access	memories.	The	program
counter	is	loaded	with	a	new	instruction	address	every	clock	cycle.	The
condition	code	register	is	loaded	only	when	an	integer	operation
instruction	is	executed.	The	data	memory	is	written	only	when	an	
,	or	
instruction	is	executed.	The	two	write	ports	of	the	register
file	allow	two	program	registers	to	be	updated	on	every	cycle,	but	we	can
use	the	special	register	ID	
as	a	port	address	to	indicate	that	no	write
should	be	performed	for	this	port.
This	clocking	of	the	registers	and	memories	is	all	that	is	required	to
control	the	sequencing	of	activities	in	our	processor.	Our	hardware
achieves	the	same	effect	as	would	a	sequential	execution	of	the
assignments	shown	in	the	tables	of	
Figures	
4.18
through	
4.21
,
even	though	all	of	the	state	updates	actually	occur	simultaneously	and
only	as	the	clock	rises	to	start	the	next	cycle.	This	equivalence	holds
because	of	the	nature	of	the	Y86-64	instruction	set,	and	because	we
have	organized	the	computations	in	such	a	way	that	our	design	obeys
the	following	principle:</p>
<p>Principle:
No	reading	back
The	processor	never	needs	to	read	back	the	state	updated
by	an	instruction	in	order	to	complete	the	processing	of	this
instruction.
This	principle	is	crucial	to	the	success	of	our	implementation.	As	an
illustration,	suppose	we	implemented	the	
instruction	by	first
decrementing	
by	8	and	then	using	the	updated	value	of	
as	the
address	of	a	write	operation.	This	approach	would	violate	the	principle
stated	above.	It	would	require	reading	the	updated	stack	pointer	from	the
register	file	in	order	to	perform	the	memory	operation.	Instead,	our
implementation	(
Figure	
4.20
)	generates	the	decremented	value	of	the
stack	pointer	as	the	signal	valE	and	then	uses	this	signal	both	as	the	data
for	the	register	write	and	the	address	for	the	memory	write.	As	a	result,	it
can	perform	the	register	and	memory	writes	simultaneously	as	the	clock
rises	to	begin	the	next	clock	cycle.
As	another	illustration	of	this	principle,	we	can	see	that	some	instructions
(the	integer	operations)	set	the	condition	codes,	and	some	instructions
(the	conditional	move	and	jump	instructions)	read	these	condition	codes,
but	no	instruction	must	both	set	and	then	read	the	condition	codes.	Even
though	the	condition	codes	are	not	set	until	the	clock	rises	to	begin	the</p>
<p>next	clock	cycle,	they	will	be	updated	before	any	instruction	attempts	to
read	them.
Figure	
4.25
shows	how	the	SEQ	hardware	would	process	the
instructions	at	lines	3	and	4	in	the	following	code	sequence,	shown	in
assembly	code	with	the	instruction	addresses	listed	on	the	left:
Each	of	the	diagrams	labeled	1	through	4	shows	the	four	state	elements
plus	the	combinational	logic	and	the	connections	among	the	state
elements.	We	show	the	combinational	logic	as	being	wrapped	around	the
condition	code	register,	because	some	of	the	combinational	logic	(such
as	the	ALU)	generates	the	input	to	the	condition	code	register,	while
other	parts	(such	as	the	branch	computation	and	the	PC	selection	logic)
have	the	condition	code	register	as	input.	We	show	the	register	file	and
the	data	memory	as	having	separate	connections	for	reading	and	writing,
since	the	read	operations	propagate	through	these	units	as	if	they	were
combinational	logic,	while	the	write	operations	are	controlled	by	the	clock.
The	color	coding	in	
Figure	
4.25
indicates	how	the	circuit	signals	relate
to	the	different	instructions	being	executed.	We	assume	the	processing</p>
<p>starts	with	the	condition	codes,	listed	in	the	order	
,	and	
,	set	to
.	At	the	beginning	of	clock	cycle	3	(point	1),	the	state	elements	hold
the	state	as	updated	by	the	second	
instruction	(line	2	of	the
listing),	shown	in	light	gray.	The	combinational	logic	is	shown	in	white,
indicating	that	it	has	not	yet	had	time	to	react	to	the	changed	state.	The
clock	cycle	begins	with	address	
loaded	into	the	program	counter.
This	causes	the	
instruction	(line	3	of	the	listing),	shown	in	blue,	to
be	fetched	and	processed.	Values	flow	through	the	combinational	logic,
including	the	reading	of	the	random	access	memories.	By	the	end	of	the
cycle	(point	2),	the	combinational	logic	has	generated	new	values	(
)
for	the	condition	codes,	an	update	for	program	register	
,	and	a	new
value	(
)	for	the	program	counter.	At	this	point,	the	combinational
logic	has	been	updated	according	to	the	
instruction	(shown	in	blue),
but	the	state	still	holds	the	values	set	by	the	second	
instruction
(shown	in	light	gray).
As	the	clock	rises	to	begin	cycle	4	(point	3),	the	updates	to	the	program
counter,	the	register	file,	and	the	condition	code	register	occur,	and	so	we
show	these	in	blue,	but	the	combinational	logic	has	not	yet	reacted	to
these	changes,	and	so	we	show	this	in	white.	In	this	cycle,	the	
instruction	(line	4	in	the	listing),	shown	in	dark	gray,	is	fetched	and
executed.	Since	condition	code	
is	0,	the	branch	is	not	taken.	By	the
end	of	the	cycle	(point	4),	a	new	value	of	
has	been	generated	for
the	program	counter.	The	combinational	logic	has	been	updated
according	to	the	
instruction	(shown	in	dark	gray),	but	the	state	still
holds	the	values	set	by	the	
instruction	(shown	in	blue)	until	the	next
cycle	begins.</p>
<p>As	this	example	illustrates,	the	use	of	a	clock	to	control	the	updating	of
the	state	elements,	combined	with	the	propagation	of	values	through
combinational	logic,	suffices	to	control	the	computations	performed	for
each	instruction	in	our	implementation	of	SEQ.	Every	time	the	clock
transitions	from	low	to	high,	the	processor	begins	executing	a	new
instruction.</p>
<p>Figure	
4.25	
Tracing	two	cycles	of	execution	by	SEQ.
Each	cycle	begins	with	the	state	elements	(program	counter,	condition
code	register,	register	file,	and	data	memory)	set	according	to	the
previous	instruction.	Signals	propagate	through	the	combinational	logic,
creating	new	values	for	the	state	elements.	These	values	are	loaded	into
the	state	elements	to	start	the	next	cycle.</p>
<p>4.3.4	
SEQ	Stage	Implementations
In	this	section,	we	devise	HCL	descriptions	for	the	control	logic	blocks
required	to	implement	SEQ.	A	complete	HCL	description	for	SEQ	is	given
in	Web	Aside	
ARCH
:
HCL</p>
<p>on	page	472.	We	show	some	example	blocks
here,	and	others	are	given	as	practice	problems.	We	recommend	that
you	work	these	problems	as	a	way	to	check	your	understanding	of	how
the	blocks	relate	to	the	computational	requirements	of	the	different
instructions.
Part	of	the	HCL	description	of	SEQ	that	we	do	not	include	here	is	a
definition	of	the	different	integer	and	Boolean	signals	that	can	be	used	as
arguments	to	the	HCL	operations.	These	include	the	names	of	the
different	hardware	signals,	as	well	as	constant	values	for	the	different
instruction	codes,	function	codes,	register	names,	ALU	operations,	and
status	codes.	Only	those	that	must	be	explicitly
Name
Value	(hex)
Meaning
Code	for	
instruction
Code	for	
instruction
Code	for	
instruction
Code	for	
instruction
Code	for	
instruction</p>
<p>Code	for	
instruction
Code	for	integer	operation	instructions
Code	for	jump	instructions
Code	for	
instruction
Code	for	
instruction
Code	for	
instruction
Code	for	
instruction
Default	function	code
Register	ID	for	
Indicates	no	register	file	access
Function	for	addition	operation
Status	code	for	normal	operation
Status	code	for	address	exception
Status	code	for	illegal	instruction	exception
Status	code	for	
Figure	
4.26	
Constant	values	used	in	HCL	descriptions.
These	values	represent	the	encodings	of	the	instructions,	function	codes,
register	IDs,	ALU	operations,	and	status	codes.</p>
<p>Figure	
4.27	
SEQ	fetch	stage.
Six	bytes	are	read	from	the	instruction	memory	using	the	PC	as	the
starting	address.	From	these	bytes,	we	generate	the	different	instruction
fields.	The	PC	increment	block	computes	signal	valP.
referenced	in	the	control	logic	are	shown.	The	constants	we	use	are
documented	in	
Figure	
4.26
.	By	convention,	we	use	uppercase	names
for	constant	values.
In	addition	to	the	instructions	shown	in	
Figures	
4.18
to	
4.21
,	we
include	the	processing	for	the	
and	
instructions.	The	
instruction	simply	flows	through	stages	without	much	processing,	except
to	increment	the	PC	by	1.	The	
instruction	causes	the	processor
status	to	be	set	to	
,	causing	it	to	
operation.
Fetch	Stage</p>
<p>As	shown	in	
Figure	
4.27
,	the	fetch	stage	includes	the	instruction
memory	hardware	unit.	This	unit	reads	10	bytes	from	memory	at	a	time,
using	the	PC	as	the	address	of	the	first	byte	(byte	0).	This	byte	is
interpreted	as	the	instruction	byte	and	is	split	(by	the	unit	labeled	&quot;Split&quot;)
into	two	4-bit	quantities.	The	control	logic	blocks	labeled	&quot;icode&quot;	and
&quot;ifun&quot;	then	compute	the	instruction	and	function	codes	as	equaling	either
the	values	read	from	memory	or,	in	the	event	that	the	instruction	address
is	not	valid	(as	indicated	by	the	signal	
),	the	values
corresponding	to	a	
instruction.	Based	on	the	value	of	icode,	we	can
compute	three	1-bit	signals	(shown	as	dashed	lines):
instr_valid.	
Does	this	byte	correspond	to	a	legal	Y86-64	instruction?
This	signal	is	used	to	detect	an	illegal	instruction.
need_regids.	
Does	this	instruction	include	a	register	specifier	byte?
need_valC.	
Does	this	instruction	include	a	constant	word?
The	signals	instr_valid	and	imem_error	(generated	when	the	instruction
address	is	out	of	bounds)	are	used	to	generate	the	status	code	in	the
memory	stage.
As	an	example,	the	HCL	description	for	need_regids	simply	determines
whether	the	value	of	icode	is	one	of	the	instructions	that	has	a	register
specifier	byte:</p>
<p>Practice	Problem	
4.19	
(solution	page	
487
)
Write	HCL	code	for	the	signal	
in	the	SEQ	implementation.
As	
Figure	
4.27
shows,	the	remaining	9	bytes	read	from	the	instruction
memory	encode	some	combination	of	the	register	specifier	byte	and	the
constant	word.	These	bytes	are	processed	by	the	hardware	unit	labeled
&quot;Align&quot;	into	the	register	fields	and	the	constant	word.	Byte	1	is	split	into
register	specifiers	rA	and	rB	when	the	computed	signal	need_regids	is	1.
If	need_regids	is	0,	both	register	specifiers	are	set	to	
,
indicating	there	are	no	registers	specified	by	this	instruction.	Recall	also
(
Figure	
4.2
)	that	for	any	instruction	having	only	one	register	operand,
the	other	field	of	the	register	specifier	byte	will	be	
.	Thus,	we
can	assume	that	the	signals	rA	and	rB	either	encode	registers	we	want	to
access	or	indicate	that	register	access	is	not	required.	The	unit	labeled
&quot;Align&quot;	also	generates	the	constant	word	valC.	This	will	either	be	bytes	1-
8	or	bytes	2-9,	depending	on	the	value	of	signal	need_regids.
The	PC	incrementer	hardware	unit	generates	the	signal	valP,	based	on
the	current	value	of	the	PC,	and	the	two	signals	need_regids	and
need_valC.	For	PC	value	
p
,	need_regids	value	
r
,	and	need_valC	value	
i
,
the	incrementer	generates	the	value	
p
+	1	+	
r
+	8
i.
Decode	and	Write-Back	Stages
Figure	
4.28
provides	a	detailed	view	of	logic	that	implements	both	the
decode	and	write-back	stages	in	SEQ.	These	two	stages	are	combined</p>
<p>because	they	both	access	the	register	file.
The	register	file	has	four	ports.	It	supports	up	to	two	simultaneous	reads
(on	ports	A	and	B)	and	two	simultaneous	writes	(on	ports	E	and	M).	Each
port	has	both	an	address	connection	and	a	data	connection,	where	the
address	connection	is	a	register	ID,	and	the	data	connection	is	a	set	of
64	wires	serving	as	either	an	output	word	(for	a	read	port)	or	an	input
word	(for	a	write	port)	of	the	register	file.	The	two	read	ports	have
address	inputs	srcA	and	srcB,	while	the	two	write	ports	have	address
inputs	dstE	and	dstM.	The	special	identifier	
on	an	address
port	indicates	that	no	register	should	be	accessed.
The	four	blocks	at	the	bottom	of	
Figure	
4.28
generate	the	four	different
register	IDs	for	the	register	file,	based	on	the	instruction	code	icode,	the
register	specifiers	rA	and	rB,	and	possibly	the	condition	signal	Cnd
computed	in	the	execute	stage.	Register	ID	srcA	indicates	which	register
should	be	read	to	generate	valA.
Figure	
4.28	
SEQ	decode	and	write-back	stage.
The	instruction	fields	are	decoded	to	generate	register	identifiers	for	four
addresses	(two	read	and	two	write)	used	by	the	register	file.	The	values</p>
<p>read	from	the	register	file	become	the	signals	valA	and	valB.	The	two
write-back	values	valE	and	valM	serve	as	the	data	for	the	writes.
The	desired	value	depends	on	the	instruction	type,	as	shown	in	the	first
row	for	the	decode	stage	in	
Figures	
4.18
to	
4.21
.	Combining	all	of
these	entries	into	a	single	computation	gives	the	following	HCL
description	of	srcA	(recall	that	
is	the	register	ID	of	
):
Practice	Problem	
4.20	
(solution	page	
488
)
The	register	signal	srcB	indicates	which	register	should	be	read	to
generate	the	signal	valB.	The	desired	value	is	shown	as	the	second	step
in	the	decode	stage	in	
Figures	
4.18
to	
4.21
.	Write	HCL	code	for
srcB.
Register	ID	dstE	indicates	the	destination	register	for	write	port	E,	where
the	computed	value	valE	is	stored.	This	is	shown	in	
Figures	
4.18
to
4.21
as	the	first	step	in	the	write-back	stage.	If	we	ignore	for	the
moment	the	conditional	move	instructions,	then	we	can	combine	the</p>
<p>destination	registers	for	all	of	the	different	instructions	to	give	the
following	HCL	description	of	dstE:
We	will	revisit	this	signal	and	how	to	implement	conditional	moves	when
we	examine	the	execute	stage.
Practice	Problem	
4.21	
(solution	page	
488
)
Register	ID	dstM	indicates	the	destination	register	for	write	port	M,	where
valM,	the	value	read	from	memory,	is	stored.	This	is	shown	in	
Figures
4.18
to	
4.21
as	the	second	step	in	the	write-back	stage.	Write	HCL
code	for	dstM.
Practice	Problem	
4.22	
(solution	page	
488
)
Only	the	
instruction	uses	both	register	file	write	ports
simultaneously.	For	the	instruction	
,	the	same	address	will	be
used	for	both	the	E	and	M	write	ports,	but	with	different	data.	To	handle
this	conflict,	we	must	establish	a	
priority
among	the	two	write	ports	so</p>
<h2>that	when	both	attempt	to	write	the	same	register	on	the	same	cycle,	only
the	write	from	the	higher-priority	port	takes	place.	Which	of	the	two	ports
should	be	given	priority	in	order	to	implement	the	desired	behavior,	as
determined	in	
Practice	Problem	
4.8
?
Execute	Stage
The	execute	stage	includes	the	arithmetic/logic	unit	(ALU).	This	unit
performs	the	operation	
ADD
,	
SUBTRACT
,	
AND
,	or	
EXCLUSIVE</h2>
<p>OR</p>
<p>on	inputs	aluA
and	aluB	based	on	the	setting	of	the	alufun	signal.	These	data	and
control	signals	are	generated	by	three	control	blocks,	as	diagrammed	in
Figure	
4.29
.	The	ALU	output	becomes	the	signal	valE.
In	
Figures	
4.18
to	
4.21
,	the	ALU	computation	for	each	instruction	is
shown	as	the	first	step	in	the	execute	stage.	The	operands	are	listed	with
aluB	first,	followed	by	aluA	to	make	sure	the	
instruction	subtracts
valA	from	valB.	We	can	see	that	the	value	of	aluA	can	be	valA,	valC,	or
either	-8	or	+8,	depending	on	the	instruction	type.	We	can	therefore
express	the	behavior	of	the	control	block	that	generates	aluA	as	follows:</p>
<p>Figure	
4.29	
SEQ	execute	stage.
The	ALU	either	performs	the	operation	for	an	integer	operation	instruction
or	acts	as	an	adder.	The	condition	code	registers	are	set	according	to	the
ALU	value.	The	condition	code	values	are	tested	to	determine	whether	a
branch	should	be	taken.
Practice	Problem	
4.23	
(solution	page	
488
)
Based	on	the	first	operand	of	the	first	step	of	the	execute	stage	in
Figures	
4.18
to	
4.21
,	write	an	HCL	description	for	the	signal	aluB	in
SEQ.
Looking	at	the	operations	performed	by	the	ALU	in	the	execute	stage,	we
can	see	that	it	is	mostly	used	as	an	adder.	For	the	
instructions,</p>
<p>however,	we	want	it	to	use	the	operation	encoded	in	the	ifun	field	of	the
instruction.	We	can	therefore	write	the	HCL	description	for	the	ALU
control	as	follows:
The	execute	stage	also	includes	the	condition	code	register.	Our	ALU
generates	the	three	signals	on	which	the	condition	codes	are	based—
zero,	sign,	and	overflow—every	time	it	operates.	However,	we	only	want
to	set	the	condition	codes	when	an	
instruction	is	executed.	We
therefore	generate	a	signal	set_cc	that	controls	whether	or	not	the
condition	code	register	should	be	updated:
The	hardware	unit	labeled	&quot;cond&quot;	uses	a	combination	of	the	condition
codes	and	the	function	code	to	determine	whether	a	conditional	branch
or	data	transfer	should	take	place	(
Figure	
4.3
).	It	generates	the	Cnd
signal	used	both	for	the	setting	of	dstE	with	conditional	moves	and	in	the
next	PC	logic	for	conditional	branches.	For	other	instructions,	the	Cnd
signal	may	be	set	to	either	1	or	0,	depending	on	the	instruction's	function
code	and	the	setting	of	the	condition	codes,	but	it	will	be	ignored	by	the
control	logic.	We	omit	the	detailed	design	of	this	unit.</p>
<p>Practice	Problem	
4.24	
(solution	page	
488
)
The	conditional	move	instructions,	abbreviated	
,	have	instruction
code	
.	As	
Figure	
4.28
shows,	we	can	implement	these
instructions	by	making	use	of	the	Cnd	signal,	generated	in	the	execute
stage.	Modify	the	HCL	code	for	dstE	to	implement	these	instructions.
Memory	Stage
The	memory	stage	has	the	task	of	either	reading	or	writing	program	data.
As	shown	in	
Figure	
4.30
,	two	control	blocks	generate	the	values	for
the	memory
Figure	
4.30	
SEQ	memory	stage.
The	data	memory	can	either	write	or	read	memory	values.	The	value
read	from	memory	forms	the	signal	valM.
address	and	the	memory	input	data	(for	write	operations).	Two	other
blocks	generate	the	control	signals	indicating	whether	to	perform	a	read</p>
<p>or	a	write	operation.	When	a	read	operation	is	performed,	the	data
memory	generates	the	value	valM.
The	desired	memory	operation	for	each	instruction	type	is	shown	in	the
memory	stage	of	
Figures	
4.18
to	
4.21
.	Observe	that	the	address	for
memory	reads	and	writes	is	always	valE	or	valA.	We	can	describe	this
block	in	HCL	as	follows:
Practice	Problem	
4.25	
(solution	page	
488
)
Looking	at	the	memory	operations	for	the	different	instructions	shown	in
Figures	
4.18
to	
4.21
,	we	can	see	that	the	data	for	memory	writes
are	always	either	valA	or	valP.	Write	HCL	code	for	the	signal	mem_data
in	SEQ.
We	want	to	set	the	control	signal	mem_read	only	for	instructions	that
read	data	from	memory,	as	expressed	by	the	following	HCL	code:</p>
<p>Practice	Problem	
4.26	
(solution	page	
489
)
We	want	to	set	the	control	signal	mem_write	only	for	instructions	that
write	data	to	memory.	Write	HCL	code	for	the	signal	mem_write	in	SEQ.
Figure	
4.31	
SEQ	PC	update	stage.
The	next	value	of	the	PC	is	selected	from	among	the	signals	valC,	valM,
and	valP,	depending	on	the	instruction	code	and	the	branch	flag.
A	final	function	for	the	memory	stage	is	to	compute	the	status	code	Stat
resulting	from	the	instruction	execution	according	to	the	values	of	icode,
imem_error,	and	instr_valid	generated	in	the	fetch	stage	and	the	signal
dmem_error	generated	by	the	data	memory.
Practice	Problem	
4.27	
(solution	page	
489
)
Write	HCL	code	for	Stat,	generating	the	four	status	codes	
,	and	
(see	
Figure	
4.26
).
PC	Update	Stage
The	final	stage	in	SEQ	generates	the	new	value	of	the	program	counter
(see	
Figure	
4.31
).	As	the	final	steps	in	
Figures	
4.18
to	
4.21
show,	the	new	PC	will	be	valC,	valM,	or	valP,	depending	on	the</p>
<p>instruction	type	and	whether	or	not	a	branch	should	be	taken.	This
selection	can	be	described	in	HCL	as	follows:
Surveying	SEQ
We	have	now	stepped	through	a	complete	design	for	a	Y86-64
processor.	We	have	seen	that	by	organizing	the	steps	required	to
execute	each	of	the	different	instructions	into	a	uniform	flow,	we	can
implement	the	entire	processor	with	a	small	number	of	different	hardware
units	and	with	a	single	clock	to	control	the	sequencing	of	computations.
The	control	logic	must	then	route	the	signals	between	these	units	and
generate	the	proper	control	signals	based	on	the	instruction	types	and
the	branch	conditions.
The	only	problem	with	SEQ	is	that	it	is	too	slow.	The	clock	must	run
slowly	enough	so	that	signals	can	propagate	through	all	of	the	stages</p>
<p>within	a	single	cycle.	As	an	example,	consider	the	processing	of	a	
instruction.	Starting	with	an	updated	program	counter	at	the	beginning	of
the	clock	cycle,	the	instruction	must	be	read	from	the	instruction	memory,
the	stack	pointer	must	be	read	from	the	register	file,	the	ALU	must
increment	the	stack	pointer	by	8,	and	the	return	address	must	be	read
from	the	memory	in	order	to	determine	the	next	value	for	the	program
counter.	All	of	these	must	be	completed	by	the	end	of	the	clock	cycle.
This	style	of	implementation	does	not	make	very	good	use	of	our
hardware	units,	since	each	unit	is	only	active	for	a	fraction	of	the	total
clock	cycle.	We	will	see	that	we	can	achieve	much	better	performance	by
introducing	pipelining.</p>
<p>4.4	
General	Principles	of	Pipelining
Before	attempting	to	design	a	pipelined	Y86-64	processor,	let	us	consider
some	general	properties	and	principles	of	pipelined	systems.	Such
systems	are	familiar	to	anyone	who	has	been	through	the	serving	line	at
a	cafeteria	or	run	a	car	through	an	automated	car	wash.	In	a	pipelined
system,	the	task	to	be	performed	is	divided	into	a	series	of	discrete
stages.	In	a	cafeteria,	this	involves	supplying	salad,	a	main	dish,	dessert,
and	beverage.	In	a	car	wash,	this	involves	spraying	water	and	soap,
scrubbing,	applying	wax,	and	drying.	Rather	than	having	one	customer
run	through	the	entire	sequence	from	beginning	to	end	before	the	next
can	begin,	we	allow	multiple	customers	to	proceed	through	the	system	at
once.	In	a	traditional	cafeteria	line,	the	customers	maintain	the	same
order	in	the	pipeline	and	pass	through	all	stages,	even	if	they	do	not	want
some	of	the	courses.	In	the	case	of	the	car	wash,	a	new	car	is	allowed	to
enter	the	spraying	stage	as	the	preceding	car	moves	from	the	spraying
stage	to	the	scrubbing	stage.	In	general,	the	cars	must	move	through	the
system	at	the	same	rate	to	avoid	having	one	car	crash	into	the	next.
A	key	feature	of	pipelining	is	that	it	increases	the	
throughput
of	the
system	(i.e.,	the	number	of	customers	served	per	unit	time),	but	it	may
also	slightly	increase	the	
latency
(i.e.,	the	time	required	to	service	an
individual	customer).	For	example,	a	customer	in	a	cafeteria	who	only
wants	a	dessert	could	pass	through	a	nonpipelined	system	very	quickly,
stopping	only	at	the	dessert	stage.	A	customer	in	a	pipelined	system	who
attempts	to	go	directly	to	the	dessert	stage	risks	incurring	the	wrath	of
other	customers.</p>
<p>4.4.1	
Computational	Pipelines
Shifting	our	focus	to	computational	pipelines,	the	&quot;customers&quot;	are
instructions	and	the	stages	perform	some	portion	of	the	instruction
execution.	
Figure	
4.32
(a)	shows	an	example	of	a	simple	nonpipelined
hardware	system.	It	consists	of	some	logic	that	performs	a	computation,
followed	by	a	register	to	hold	the	results	of	this	computation.	A	clock
signal	controls	the	loading	of	the	register	at	some	regular	time	interval.
An	example	of	such	a	system	is	the	decoder	in	a	compact	disk	(CD)
player.	The	incoming	signals	are	the	bits	read	from	the	surface	of	the	CD,
and
Figure	
4.32	
Unpipelined	computation	hardware.
On	each	320	ps	cycle,	the	system	spends	300	ps	evaluating	a
combinational	logic	function	and	20	ps	storing	the	results	in	an	output
register.
the	logic	decodes	these	to	generate	audio	signals.	The	computational
block	in	the	figure	is	implemented	as	combinational	logic,	meaning	that</p>
<h1>the	signals	will	pass	through	a	series	of	logic	gates,	with	the	outputs
becoming	some	function	of	the	inputs	after	some	time	delay.
In	contemporary	logic	design,	we	measure	circuit	delays	in	units	of
picoseconds
(abbreviated	&quot;ps&quot;),	or	10
seconds.	In	this	example,	we
assume	the	combinational	logic	requires	300	ps,	while	the	loading	of	the
register	requires	20	ps.	
Figure	
4.32
shows	a	form	of	timing	diagram
known	as	a	
pipeline	diagram
.	In	this	diagram,	time	flows	from	left	to	right.
A	series	of	instructions	(here	named	
,	and	
)	are	written	from	top
to	bottom.	The	solid	rectangles	indicate	the	times	during	which	these
instructions	are	executed.	In	this	implementation,	we	must	complete	one
instruction	before	beginning	the	next.	Hence,	the	boxes	do	not	overlap
one	another	vertically.	The	following	formula	gives	the	maximum	rate	at
which	we	could	operate	the	system:
We	express	throughput	in	units	of	giga-instructions	per	second
(abbreviated	GIPS),	or	billions	of	instructions	per	second.	The	total	time
required	to	perform	a	single	instruction	from	beginning	to	end	is	known	as
the	
latency
.	In	this	system,	the	latency	is	320	ps,	the	reciprocal	of	the
throughput.
Suppose	we	could	divide	the	computation	performed	by	our	system	into
three	stages,	A,	B,	and	C,	where	each	requires	100	ps,	as	illustrated	in
Figure	
4.33
.	Then	we	could	put	
pipeline	registers
between	the	stages
so	that	each	instruction	moves	through	the	system	in	three	steps,
requiring	three	complete	clock	cycles	from	beginning	to	end.	As	the
pipeline	diagram	in	
Figure	
4.33
illustrates,	we	could	allow	
to	enter
-12
T
h
r
o
u
g
h
p
u
t</h1>
<p>1
 
instruction
(
20</p>
<ul>
<li></li>
</ul>
<p>300
)
 
picoseconds
.
1
,
000
 
picoseconds
1
 
nanosecond</p>
<p>stage	A	as	soon	as	
moves	from	A	to	B,	and	so	on.	In	steady	state,	all
three	stages	would	be	active,	with	one	instruction	leaving	and	a	new	one
entering	the	system	every	clock	cycle.	We	can	see	this	during	the	third
clock	cycle	in	the	pipeline	diagram	where	
is	in	stage	C,	
is	in	stage
B,	and	
is	in	stage	A.	In
Figure	
4.33	
Three-stage	pipelined	computation	hardware.
The	computation	is	split	into	stages	A,	B,	and	C.	On	each	120	ps	cycle,
each	instruction	progresses	through	one	stage.
Figure	
4.34	
Three-stage	pipeline	timing.
The	rising	edge	of	the	clock	signal	controls	the	movement	of	instructions
from	one	pipeline	stage	to	the	next.</p>
<p>this	system,	we	could	cycle	the	clocks	every	100	+	20	=	120
picoseconds,	giving	a	throughput	of	around	8.33	GIPS.	Since	processing
a	single	instruction	requires	3	clock	cycles,	the	latency	of	this	pipeline	is	3
×	120	=	360	ps.	We	have	increased	the	throughput	of	the	system	by	a
factor	of	8.33/3.12	=	2.67	at	the	expense	of	some	added	hardware	and	a
slight	increase	in	the	latency	(360/320	=	1.12).	The	increased	latency	is
due	to	the	time	overhead	of	the	added	pipeline	registers.
4.4.2	
A	Detailed	Look	at	Pipeline
Operation
To	better	understand	how	pipelining	works,	let	us	look	in	some	detail	at
the	timing	and	operation	of	pipeline	computations.	
Figure	
4.34
shows
the	pipeline	diagram	for	the	three-stage	pipeline	we	have	already	looked
at	(
Figure	
4.33
).	The	transfer	of	the	instructions	between	pipeline
stages	is	controlled	by	a	clock	signal,	as	shown	above	the	pipeline
diagram.	Every	120	ps,	this	signal	rises	from	0	to	1,	initiating	the	next	set
of	pipeline	stage	evaluations.
Figure	
4.35
traces	the	circuit	activity	between	times	240	and	360,	as
instruction	
(shown	in	dark	gray)	propagates	through	stage	C,	
(shown	in	blue)</p>
<p>Figure	
4.35	
One	clock	cycle	of	pipeline	operation.
Just	before	the	clock	rises	at	time	240	(point	1),	instructions	
(shown	in
dark	gray)	and	
(shown	in	blue)	have	completed	stages	B	and	A.	After
the	clock	rises,	these	instructions	begin	propagating	through	stages	C
and	B,	while	instruction	
(shown	in	light	gray)	begins	propagating</p>
<p>through	stage	A	(points	2	and	3).	Just	before	the	clock	rises	again,	the
results	for	the	instructions	have	propagated	to	the	inputs	of	the	pipeline
registers	(point	4).
propagates	through	stage	B,	and	
(shown	in	light	gray)	propagates
through	stage	A.	Just	before	the	rising	clock	at	time	240	(point	1),	the
values	computed	in	stage	A	for	instruction	
have	reached	the	input	of
the	first	pipeline	register,	but	its	state	and	output	remain	set	to	those
computed	during	stage	A	for	instruction	
.	The	values	computed	in
stage	B	for	instruction	
have	reached	the	input	of	the	second	pipeline
register.	As	the	clock	rises,	these	inputs	are	loaded	into	the	pipeline
registers,	becoming	the	register	outputs	(point	2).	In	addition,	the	input	to
stage	A	is	set	to	initiate	the	computation	of	instruction	
.	The	signals
then	propagate	through	the	combinational	logic	for	the	different	stages
(point	3).	As	the	curved	wave	fronts	in	the	diagram	at	point	3	suggest,
signals	can	propagate	through	different	sections	at	different	rates.	Before
time	360,	the	result	values	reach	the	inputs	of	the	pipeline	registers	(point
4).	When	the	clock	rises	at	time	360,	each	of	the	instructions	will	have
progressed	through	one	pipeline	stage.
We	can	see	from	this	detailed	view	of	pipeline	operation	that	slowing
down	the	clock	would	not	change	the	pipeline	behavior.	The	signals
propagate	to	the	pipeline	register	inputs,	but	no	change	in	the	register
states	will	occur	until	the	clock	rises.	On	the	other	hand,	we	could	have
disastrous	effects	if	the	clock	were	run	too	fast.	The	values	would	not
have	time	to	propagate	through	the	combinational	logic,	and	so	the
register	inputs	would	not	yet	be	valid	when	the	clock	rises.</p>
<p>As	with	our	discussion	of	the	timing	for	the	SEQ	processor	(
Section
4.3.3
),	we	see	that	the	simple	mechanism	of	having	clocked	registers
between	blocks	of	combinational	logic	suffices	to	control	the	flow	of
instructions	in	the	pipeline.	As	the	clock	rises	and	falls	repeatedly,	the
different	instructions	flow	through	the	stages	of	the	pipeline	without
interfering	with	one	another.
4.4.3	
Limitations	of	Pipelining
The	example	of	
Figure	
4.33
shows	an	ideal	pipelined	system	in	which
we	are	able	to	divide	the	computation	into	three	independent	stages,
each	requiring	one-third	of	the	time	required	by	the	original	logic.
Unfortunately,	other	factors	often	arise	that	diminish	the	effectiveness	of
pipelining.
Nonuniform	Partitioning
Figure	
4.36
shows	a	system	in	which	we	divide	the	computation	into
three	stages	as	before,	but	the	delays	through	the	stages	range	from	50
to	150	ps.	The	sum	of	the	delays	through	all	of	the	stages	remains	300
ps.	However,	the	rate	at	which	we	can	operate	the	clock	is	limited	by	the
delay	of	the	slowest	stage.	As	the	pipeline	diagram	in	this	figure	shows,
stage	A	will	be	idle	(shown	as	a	white	box)	for	100	ps	every	clock	cycle,
while	stage	C	will	be	idle	for	50	ps	every	clock	cycle.	Only	stage	B	will	be
continuously	active.	We	must	set	the	clock	cycle	to	150	+	20	=	170
picoseconds,	giving	a	throughput	of	5.88	GIPS.	In	addition,	the	latency
would	increase	to	510	ps	due	to	the	slower	clock	rate.</p>
<p>Devising	a	partitioning	of	the	system	computation	into	a	series	of	stages
having	uniform	delays	can	be	a	major	challenge	for	hardware	designers.
Often,
Figure	
4.36	
Limitations	of	pipelining	due	to	nonuniform	stage
delays.
The	system	throughput	is	limited	by	the	speed	of	the	slowest	stage.
some	of	the	hardware	units	in	a	processor,	such	as	the	ALU	and	the
memories,	cannot	be	subdivided	into	multiple	units	with	shorter	delay.
This	makes	it	difficult	to	create	a	set	of	balanced	stages.	We	will	not
concern	ourselves	with	this	level	of	detail	in	designing	our	pipelined	Y86-
64	processor,	but	it	is	important	to	appreciate	the	importance	of	timing
optimization	in	actual	system	design.
Practice	Problem	
4.28	
(solution	page	
489
)
Suppose	we	analyze	the	combinational	logic	of	
Figure	
4.32
and
determine	that	it	can	be	separated	into	a	sequence	of	six	blocks,</p>
<p>named	A	to	F,	having	delays	of	80,	30,	60,	50,	70,	and	10	ps,
respectively,	illustrated	as	follows:
We	can	create	pipelined	versions	of	this	design	by	inserting
pipeline	registers	between	pairs	of	these	blocks.	Different
combinations	of	pipeline	depth	(how	many	stages)	and	maximum
throughput	arise,	depending	on	where	we	insert	the	pipeline
registers.	Assume	that	a	pipeline	register	has	a	delay	of	20	ps.
A
.	
Inserting	a	single	register	gives	a	two-stage	pipeline.	Where
should	the	register	be	inserted	to	maximize	throughput?
What	would	be	the	throughput	and	latency?
B
.	
Where	should	two	registers	be	inserted	to	maximize	the
throughput	of	a	three-stage	pipeline?	What	would	be	the
throughput	and	latency?
C
.	
Where	should	three	registers	be	inserted	to	maximize	the
throughput	of	a	4-stage	pipeline?	What	would	be	the
throughput	and	latency?
D
.	
What	is	the	minimum	number	of	stages	that	would	yield	a
design	with	the	maximum	achievable	throughput?	Describe
this	design,	its	throughput,	and	its	latency.
Diminishing	Returns	of	Deep	Pipelining
Figure	
4.37
illustrates	another	limitation	of	pipelining.	In	this	example,
we	have	divided	the	computation	into	six	stages,	each	requiring	50	ps.</p>
<p>Inserting	a	pipeline	register	between	each	pair	of	stages	yields	a	six-
stage	pipeline.	The	minimum	clock	period	for	this	system	is	50	+	20	=	70
picoseconds,	giving	a	throughput	of	14.29	GIPS.	Thus,	in	doubling	the
number	of	pipeline	stages,	we	improve	the	performance	by	a	factor	of
14.29/8.33	=	1.71.	Even	though	we	have	cut	the	time	required	for	each
computation	block	by	a	factor	of	2,	we	do	not	get	a	doubling	of	the
throughput,	due	to	the	delay	through	the	pipeline	registers.	This	delay
becomes	a	limiting	factor	in	the	throughput	of	the	pipeline.	In	our	new
design,	this	delay	consumes	28.6%	of	the	total	clock	period.
Modern	processors	employ	very	deep	pipelines	(15	or	more	stages)	in	an
attempt	to	maximize	the	processor	clock	rate.	The	processor	architects
divide	the	instruction	execution	into	a	large	number	of	very	simple	steps
so	that	each	stage	can	have	a	very	small	delay.	The	circuit	designers
carefully	design	the	pipeline	registers	to	minimize	their	delay.	The	chip
designers	must	also	carefully	design	the	clock	distribution	network	to
ensure	that	the	clock	changes	at	the	exact	same	time	across	the	entire
chip.	All	of	these	factors	contribute	to	the	challenge	of	designing	high-
speed	microprocessors.
Practice	Problem	
4.29	
(solution	page	
490
)
Suppose	we	could	take	the	system	of	
Figure	
4.32
and	divide	it
into	an	arbitrary	number	of	pipeline	stages	
k
,	each	having	a	delay
of	300/
k
,	and	with	each	pipeline	register	having	a	delay	of	20	ps.</p>
<p>Figure	
4.37	
Limitations	of	pipelining	due	to	overhead.
As	the	combinational	logic	is	split	into	shorter	blocks,	the	delay
due	to	register	updating	becomes	a	limiting	factor.
A
.	
What	would	be	the	latency	and	the	throughput	of	the
system,	as	functions	of	
k
?
B
.	
What	would	be	the	ultimate	limit	on	the	throughput?
4.4.4	
Pipelining	a	System	with
Feedback
Up	to	this	point,	we	have	considered	only	systems	in	which	the	objects
passing	through	the	pipeline—whether	cars,	people,	or	instructions—are
completely	independent	of	one	another.	For	a	system	that	executes
machine	programs	such	as	x86-64	or	Y86-64,	however,	there	are
potential	dependencies	between	successive	instructions.	For	example,
consider	the	following	Y86-64	instruction	sequence:
In	this	three-instruction	sequence,	there	is	a	
data	dependency
between
each	successive	pair	of	instructions,	as	indicated	by	the	circled	register
names	and	the	arrows	between	them.	The	
instruction	(line	1)
stores	its	result	in	
,	which	then	must	be	read	by	the	
instruction</p>
<p>(line	2);	and	this	instruction	stores	its	result	in	
,	which	must	then	be
read	by	the	
instruction	(line	3).
Another	source	of	sequential	dependencies	occurs	due	to	the	instruction
control	flow.	Consider	the	following	Y86-64	instruction	sequence:
The	
instruction	(line	3)	creates	a	
control	dependency
since	the
outcome	of	the	conditional	test	determines	whether	the	next	instruction	to
execute	will	be	the	
instruction	(line	4)	or	the	halt	instruction	(line
7).	In	our	design	for	SEQ,	these	dependencies	were	handled	by	the
feedback	paths	shown	on	the	right-hand	side	of	
Figure	
4.22
.	This
feedback	brings	the	updated	register	values	down	to	the	register	file	and
the	new	PC	value	down	to	the	PC	register.
Figure	
4.38
illustrates	the	perils	of	introducing	pipelining	into	a	system
containing	feedback	paths.	In	the	original	system	(
Figure	
4.38
(a)),	the
result	of	each</p>
<p>Figure	
4.38	
Limitations	of	pipelining	due	to	logical	dependencies.
In	going	from	an	unpipelined	system	with	feedback	(a)	to	a	pipelined	one
(c),	we	change	its	computational	behavior,	as	can	be	seen	by	the	two
pipeline	diagrams	(b	and	d).
instruction	is	fed	back	around	to	the	next	instruction.	This	is	illustrated	by
the	pipeline	diagram	(
Figure	
4.38
(b)),	where	the	result	of	
becomes</p>
<p>an	input	to	
,	and	so	on.	If	we	attempt	to	convert	this	to	a	three-stage
pipeline	in	the	most	straightforward	manner	(
Figure	
4.38
(c)),	we
change	the	behavior	of	the	system.	As	
Figure	
4.38
(c)	shows,	the
result	of	
becomes	an	input	to	
.	In	attempting	to	speed	up	the
system	via	pipelining,	we	have	changed	the	system	behavior.
When	we	introduce	pipelining	into	a	Y86-64	processor,	we	must	deal	with
feedback	effects	properly.	Clearly,	it	would	be	unacceptable	to	alter	the
system	behavior	as	occurred	in	the	example	of	
Figure	
4.38
.	Somehow
we	must	deal	with	the	data	and	control	dependencies	between
instructions	so	that	the	resulting	behavior	matches	the	model	defined	by
the	ISA.</p>
<p>4.5	
Pipelined	Y86-64
Implementations
We	are	finally	ready	for	the	major	task	of	this	chapter—designing	a
pipelined	Y86-64	processor.	We	start	by	making	a	small	adaptation	of	the
sequential	processor	SEQ	to	shift	the	computation	of	the	PC	into	the
fetch	stage.	We	then	add	pipeline	registers	between	the	stages.	Our	first
attempt	at	this	does	not	handle	the	different	data	and	control
dependencies	properly.	By	making	some	modifications,	however,	we
achieve	our	goal	of	an	efficient	pipelined	processor	that	implements	the
Y86-64	ISA.
4.5.1	
SEQ+:	Rearranging	the
Computation	Stages
As	a	transitional	step	toward	a	pipelined	design,	we	must	slightly
rearrange	the	order	of	the	five	stages	in	SEQ	so	that	the	PC	update
stage	comes	at	the	beginning	of	the	clock	cycle,	rather	than	at	the	end.
This	transformation	requires	only	minimal	change	to	the	overall	hardware
structure,	and	it	will	work	better	with	the	sequencing	of	activities	within
the	pipeline	stages.	We	refer	to	this	modified	design	as	SEQ+.</p>
<p>We	can	move	the	PC	update	stage	so	that	its	logic	is	active	at	the
beginning	of	the	clock	cycle	by	making	it	compute	the	PC	value	for	the
current
instruction.	
Figure	
4.39
shows	how	SEQ	and	SEQ+	differ	in
their	PC	computation.	With	SEQ	(
Figure	
4.39
(a)),	the	PC	computation
takes	place	at	the	end	of	the	clock	cycle,	computing	the	new	value	for	the
PC	register	based	on	the	values	of	signals	computed	during	the	current
clock	cycle.	With	SEQ+	(
Figure	
4.39
(b)),	we	create	state	registers	to
hold	the	signals	computed	during	an	instruction.	Then,	as	a	new	clock
cycle	begins,	the	values	propagate	through	the	exact	same	logic	to
compute	the	PC	for	the	now-current	instruction.	We	label	the	registers
“pIcode,”
Figure	
4.39	
Shifting	the	timing	of	the	PC	computation.
With	SEQ+,	we	compute	the	value	of	the	program	counter	for	the	current
state	as	the	first	step	in	instruction	execution.
Aside	
Where	is	the	PC	in	SEQ+?
One	curious	feature	of	SEQ+	is	that	there	is	no	hardware	register
storing	the	program	counter.	Instead,	the	PC	is	computed
dynamically	based	on	some	state	information	stored	from	the
previous	instruction.	This	is	a	small	illustration	of	the	fact	that	we
can	implement	a	processor	in	a	way	that	differs	from	the</p>
<p>conceptual	model	implied	by	the	ISA,	as	long	as	the	processor
correctly	executes	arbitrary	machine-language	programs.	We
need	not	encode	the	state	in	the	form	indicated	by	the
programmer-visible	state,	as	long	as	the	processor	can	generate
correct	values	for	any	part	of	the	programmer-visible	state	(such
as	the	program	counter).	We	will	exploit	this	principle	even	more
in	creating	a	pipelined	design.	Out-of-order	processing
techniques,	as	described	in	
Section	
5.7
,	take	this	idea	to	an
extreme	by	executing	instructions	in	a	completely	different	order
than	they	occur	in	the	machine-level	program.
“pCnd,”	and	so	on,	to	indicate	that	on	any	given	cycle,	they	hold	the
control	signals	generated	during	the	previous	cycle.
Figure	
4.40
shows	a	more	detailed	view	of	the	SEQ+	hardware.	We
can	see	that	it	contains	the	exact	same	hardware	units	and	control	blocks
that	we	had	in	SEQ	(
Figure	
4.23
),	but	with	the	PC	logic	shifted	from
the	top,	where	it	was	active	at	the	end	of	the	clock	cycle,	to	the	bottom,
where	it	is	active	at	the	beginning.
The	shift	of	state	elements	from	SEQ	to	SEQ+	is	an	example	of	a	general
transformation	known	as	
circuit	retiming</p>
<p>[68]
.	Retiming	changes	the	state
representation	for	a	system	without	changing	its	logical	behavior.	It	is
often	used	to	balance	the	delays	between	the	different	stages	of	a
pipelined	system.
4.5.2	
Inserting	Pipeline	Registers</p>
<p>In	our	first	attempt	at	creating	a	pipelined	Y86-64	processor,	we	insert
pipeline	registers	between	the	stages	of	SEQ+	and	rearrange	signals
somewhat,	yielding	the	PIPE—	processor,	where	the	&quot;-&quot;	in	the	name
signifies	that	this	processor	has	somewhat	less	performance	than	our
ultimate	processor	design.	The	structure	of	PIPE—	is	illustrated	in	
Figure
4.41
.	The	pipeline	registers	are	shown	in	this	figure	as	blue	boxes,
each	containing	different	fields	that	are	shown	as	white	boxes.	As
indicated	by	the	multiple	fields,	each	pipeline	register	holds	multiple	bytes
and	words.	Unlike	the	labels	shown	in	rounded	boxes	in	the	hardware
structure	of	the	two	sequential	processors	(
Figures	
4.23
and	
4.40
),
these	white	boxes	represent	actual	hardware	components.
Observe	that	PIPE—	uses	nearly	the	same	set	of	hardware	units	as	our
sequential	design	SEQ	(
Figure	
4.40
),	but	with	the	pipeline	registers
separating	the	stages.	The	differences	between	the	signals	in	the	two
systems	is	discussed	in	
Section	
4.5.3
.
The	pipeline	registers	are	labeled	as	follows:
F	
holds	a	
predicted
value	of	the	program	counter,	as	will	be	discussed
shortly.
D	
sits	between	the	fetch	and	decode	stages.	It	holds	information
about	the	most	recently	fetched	instruction	for	processing	by	the
decode	stage.</p>
<p>Figure	
4.40	
SEQ+	hardware	structure.
Shifting	the	PC	computation	from	the	end	of	the	clock	cycle	to	the
beginning	makes	it	more	suitable	for	pipelining.</p>
<p>Figure	
4.41	
Hardware	structure	of	PIPE—,	an	initial	pipelined
implementation.
By	inserting	pipeline	registers	into	SEQ+	(
Figure	
4.40
),	we	create	a
five-stage	pipeline.	There	are	several	shortcomings	of	this	version
that	we	will	deal	with	shortly.</p>
<p>E	
sits	between	the	decode	and	execute	stages.	It	holds	information
about	the	most	recently	decoded	instruction	and	the	values	read	from
the	register	file	for	processing	by	the	execute	stage.
M	
sits	between	the	execute	and	memory	stages.	It	holds	the	results	of
the	most	recently	executed	instruction	for	processing	by	the	memory
stage.	It	also	holds	information	about	branch	conditions	and	branch
targets	for	processing	conditional	jumps.
W	
sits	between	the	memory	stage	and	the	feedback	paths	that	supply
the	computed	results	to	the	register	file	for	writing	and	the	return
address	to	the	PC	selection	logic	when	completing	a	
instruction.
Figure	
4.42
shows	how	the	following	code	sequence	would	flow
through	our	five-stage	pipeline,	where	the	comments	identify	the
instructions	as	
to	
for	reference:</p>
<p>Figure	
4.42	
Example	of	instruction	flow	through	pipeline.
The	right	side	of	the	figure	shows	a	pipeline	diagram	for	this	instruction
sequence.	As	with	the	pipeline	diagrams	for	the	simple	pipelined
computation	units	of	
Section	
4.4
,	this	diagram	shows	the	progression
of	each	instruction	through	the	pipeline	stages,	with	time	increasing	from
left	to	right.	The	numbers	along	the	top	identify	the	clock	cycles	at	which
the	different	stages	occur.	For	example,	in	cycle	1,	instruction	
is
fetched,	and	it	then	proceeds	through	the	pipeline	stages,	with	its	result
being	written	to	the	register	file	after	the	end	of	cycle	5.	Instruction	
is
fetched	in	cycle	2,	and	its	result	is	written	back	after	the	end	of	cycle	6,
and	so	on.	At	the	bottom,	we	show	an	expanded	view	of	the	pipeline	for
cycle	5.	At	this	point,	there	is	an	instruction	in	each	of	the	pipeline	stages.
From	
Figure	
4.42
,	we	can	also	justify	our	convention	of	drawing
processors	so	that	the	instructions	flow	from	bottom	to	top.	The</p>
<p>expanded	view	for	cycle	5	shows	the	pipeline	stages	with	the	fetch	stage
on	the	bottom	and	the	write-back	stage	on	the	top,	just	as	do	our
diagrams	of	the	pipeline	hardware	(
Figure	
4.41
).	If	we	look	at	the
ordering	of	instructions	in	the	pipeline	stages,	we	see	that	they	appear	in
the	same	order	as	they	do	in	the	program	listing.	Since	normal	program
flow	goes	from	top	to	bottom	of	a	listing,	we	preserve	this	ordering	by
having	the	pipeline	flow	go	from	bottom	to	top.	This	convention	is
particularly	useful	when	working	with	the	simulators	that	accompany	this
text.
4.5.3	
Rearranging	and	Relabeling
Signals
Our	sequential	implementations	SEQ	and	SEQ+	only	process	one
instruction	at	a	time,	and	so	there	are	unique	values	for	signals	such	as
valC,	srcA,	and	valE.	In	our	pipelined	design,	there	will	be	multiple
versions	of	these	values	associated	with	the	different	instructions	flowing
through	the	system.	For	example,	in	the	detailed	structure	of	PIPE—,
there	are	four	white	boxes	labeled	&quot;Stat&quot;	that	hold	the	status	codes	for
four	different	instructions	(see	
Figure	
4.41
).	We	need	to	take	great
care	to	make	sure	we	use	the	proper	version	of	a	signal,	or	else	we	could
have	serious	errors,	such	as	storing	the	result	computed	for	one
instruction	at	the	destination	register	specified	by	another	instruction.	We
adopt	a	naming	scheme	where	a	signal	stored	in	a	pipeline	register	can
be	uniquely	identified	by	prefixing	its	name	with	that	of	the	pipe	register
written	in	uppercase.	For	example,	the	four	status	codes	are	named
D_stat,	E_stat,	M_stat,	and	W_stat.	We	also	need	to	refer	to	some</p>
<p>signals	that	have	just	been	computed	within	a	stage.	These	are	labeled
by	prefixing	the	signal	name	with	the	first	character	of	the	stage	name,
written	in	lowercase.	Using	the	status	codes	as	examples,	we	can	see
control	logic	blocks	labeled	&quot;Stat&quot;	in	the	fetch	and	memory	stages.	The
outputs	of	these	blocks	are	therefore	named	f_stat	and	m_stat.	We	can
also	see	that	the	actual	status	of	the	overall	processor	Stat	is	computed
by	a	block	in	the	write-back	stage,	based	on	the	status	value	in	pipeline
register	W.
The	decode	stages	of	SEQ+	and	PIPE—	both	generate	signals	dstE	and
dstM	indicating	the	destination	register	for	values	valE	and	valM.	In
SEQ+,	we	could	connect	these	signals	directly	to	the	address	inputs	of
the	register	file	write	ports.	With	PIPE-,	these	signals	are	carried	along	in
the	pipeline	through	the	execute	and	memory	stages	and	are	directed	to
the	register	file	only	once	they	reach
Aside	
What	is	the	difference	between
signals	M_stat	and	m_stat?
With	our	naming	system,	the	uppercase	prefixes	<code>D',	</code>E',	<code>M',	and </code>W	refer	to	pipeline	
registers
,	and	so	M_stat	refers	to	the	status
code	field	of	pipeline	register	M.	The	lowercase	prefixes	<code>f',	</code>d',	<code>e', </code>m',	and	`w'	refer	to	the	pipeline	
stages
,	and	so	m_stat	refers	to
the	status	signal	generated	in	the	memory	stage	by	a	control	logic
block.
Understanding	this	naming	convention	is	critical	to	understanding
the	operation	of	our	pipelined	processors.</p>
<p>the	write-back	stage	(shown	in	the	more	detailed	views	of	the	stages).
We	do	this	to	make	sure	the	write	port	address	and	data	inputs	hold
values	from	the	same	instruction.	Otherwise,	the	write	back	would	be
writing	the	values	for	the	instruction	in	the	write-back	stage,	but	with
register	IDs	from	the	instruction	in	the	decode	stage.	As	a	general
principle,	we	want	to	keep	all	of	the	information	about	a	particular
instruction	contained	within	a	single	pipeline	stage.
One	block	of	PIPE—	that	is	not	present	in	SEQ+	in	the	exact	same	form
is	the	block	labeled	&quot;Select	A&quot;	in	the	decode	stage.	We	can	see	that	this
block	generates	the	value	valA	for	the	pipeline	register	E	by	choosing
either	valP	from	pipeline	register	D	or	the	value	read	from	the	A	port	of
the	register	file.	This	block	is	included	to	reduce	the	amount	of	state	that
must	be	carried	forward	to	pipeline	registers	E	and	M.	Of	all	the	different
instructions,	only	the	
requires	valP	in	the	memory	stage.	Only	the
jump	instructions	require	the	value	of	valP	in	the	execute	stage	(in	the
event	the	jump	is	not	taken).	None	of	these	instructions	requires	a	value
read	from	the	register	file.	Therefore,	we	can	reduce	the	amount	of
pipeline	register	state	by	merging	these	two	signals	and	carrying	them
through	the	pipeline	as	a	single	signal	valA.	This	eliminates	the	need	for
the	block	labeled	&quot;Data&quot;	in	SEQ	(
Figure	
4.23
)	and	SEQ+	(
Figure
4.40
),	which	served	a	similar	purpose.	In	hardware	design,	it	is
common	to	carefully	identify	how	signals	get	used	and	then	reduce	the
amount	of	register	state	and	wiring	by	merging	signals	such	as	these.
As	shown	in	
Figure	
4.41
,	our	pipeline	registers	include	a	field	for	the
status	code	stat,	initially	computed	during	the	fetch	stage	and	possibly
modified	during	the	memory	stage.	We	will	discuss	how	to	implement	the
processing	of	exceptional	events	in	
Section	
4.5.6
,	after	we	have</p>
<p>covered	the	implementation	of	normal	instruction	execution.	Suffice	it	to
say	at	this	point	that	the	most	systematic	approach	is	to	associate	a
status	code	with	each	instruction	as	it	passes	through	the	pipeline,	as	we
have	indicated	in	the	figure.
4.5.4	
Next	PC	Prediction
We	have	taken	some	measures	in	the	design	of	PIPE—	to	properly
handle	control	dependencies.	Our	goal	in	the	pipelined	design	is	to	
issue
a	new	instruction	on	every	clock	cycle,	meaning	that	on	each	clock	cycle,
a	new	instruction	proceeds	into	the	execute	stage	and	will	ultimately	be
completed.	Achieving	this	goal	would
Aside	
Other	branch	prediction
strategies
Our	design	uses	an	
always	taken
branch	prediction	strategy.
Studies	show	this	strategy	has	around	a	60%	success	rate	
[44,
122]
.	Conversely,	a	
never	taken
(NT)	strategy	has	around	a	40%
success	rate.	A	slightly	more	sophisticated	strategy,	known	as
backward	taken,	forward	not	taken
(BTFNT),	predicts	that
branches	to	lower	addresses	than	the	next	instruction	will	be
taken,	while	those	to	higher	addresses	will	not	be	taken.	This
strategy	has	a	success	rate	of	around	65%.	This	improvement
stems	from	the	fact	that	loops	are	closed	by	backward	branches
and	loops	are	generally	executed	multiple	times.	Forward
branches	are	used	for	conditional	operations,	and	these	are	less</p>
<p>likely	to	be	taken.	In	Problems	4.55	and	4.56,	you	can	modify	the
Y86-64	pipeline	processor	to	implement	the	NT	and	BTFNT
branch	prediction	strategies.
As	we	saw	in	
Section	
3.6.6
,	mispredicted	branches	can
degrade	the	performance	of	a	program	considerably,	thus
motivating	the	use	of	conditional	data	transfer	rather	than
conditional	control	transfer	when	possible.
yield	a	throughput	of	one	instruction	per	cycle.	To	do	this,	we	must
determine	the	location	of	the	next	instruction	right	after	fetching	the
current	instruction.	Unfortunately,	if	the	fetched	instruction	is	a	conditional
branch,	we	will	not	know	whether	or	not	the	branch	should	be	taken	until
several	cycles	later,	after	the	instruction	has	passed	through	the	execute
stage.	Similarly,	if	the	fetched	instruction	is	a	ret,	we	cannot	determine
the	return	location	until	the	instruction	has	passed	through	the	memory
stage.
With	the	exception	of	conditional	jump	instructions	and	ret,	we	can
determine	the	address	of	the	next	instruction	based	on	information
computed	during	the	fetch	stage.	For	
and	
(unconditional	jump),
it	will	be	valC,	the	constant	word	in	the	instruction,	while	for	all	others	it
will	be	valP,	the	address	of	the	next	instruction.	We	can	therefore	achieve
our	goal	of	issuing	a	new	instruction	every	clock	cycle	in	most	cases	by
predicting
the	next	value	of	the	PC.	For	most	instruction	types,	our
prediction	will	be	completely	reliable.	For	conditional	jumps,	we	can
predict	either	that	a	jump	will	be	taken,	so	that	the	new	PC	value	would
be	valC,	or	that	it	will	not	be	taken,	so	that	the	new	PC	value	would	be
valP.	In	either	case,	we	must	somehow	deal	with	the	case	where	our
prediction	was	incorrect	and	therefore	we	have	fetched	and	partially</p>
<p>executed	the	wrong	instructions.	We	will	return	to	this	matter	in	
Section
4.5.8
.
This	technique	of	guessing	the	branch	direction	and	then	initiating	the
fetching	of	instructions	according	to	our	guess	is	known	as	
branch
prediction
.	It	is	used	in	some	form	by	virtually	all	processors.	Extensive
experiments	have	been	conducted	on	effective	strategies	for	predicting
whether	or	not	branches	will	be	taken	
[46,</p>
<p>Section	
2.3
].	Some
systems	devote	large	amounts	of	hardware	to	this	task.	In	our	design,	we
will	use	the	simple	strategy	of	predicting	that	conditional	branches	are
always	taken,	and	so	we	predict	the	new	value	of	the	PC	to	be	valC.
We	are	still	left	with	predicting	the	new	PC	value	resulting	from	a	
instruction.	Unlike	conditional	jumps,	we	have	a	nearly	unbounded	set	of
possible
Aside	
Return	address	prediction	with	a
stack
With	most	programs,	it	is	very	easy	to	predict	return	addresses,
since	procedure	calls	and	returns	occur	in	matched	pairs.	Most	of
the	time	that	a	procedure	is	called,	it	returns	to	the	instruction
following	the	call.	This	property	is	exploited	in	high-performance
processors	by	including	a	hardware	stack	within	the	instruction
fetch	unit	that	holds	the	return	address	generated	by	procedure
call	instructions.	Every	time	a	procedure	call	instruction	is
executed,	its	return	address	is	pushed	onto	the	stack.	When	a
return	instruction	is	fetched,	the	top	value	is	popped	from	this</p>
<p>stack	and	used	as	the	predicted	return	address.	Like	branch
prediction,	a	mechanism	must	be	provided	to	recover	when	the
prediction	was	incorrect,	since	there	are	times	when	calls	and
returns	do	not	match.	In	general,	the	prediction	is	highly	reliable.
This	hardware	stack	is	not	part	of	the	programmer-visible	state.
results,	since	the	return	address	will	be	whatever	word	is	on	the	top	of
the	stack.	In	our	design,	we	will	not	attempt	to	predict	any	value	for	the
return	address.	Instead,	we	will	simply	hold	off	processing	any	more
instructions	until	the	
instruction	passes	through	the	write-back	stage.
We	will	return	to	this	part	of	the	implementation	in	
Section	
4.5.8
.
The	PIPE—	fetch	stage,	diagrammed	at	the	bottom	of	
Figure	
4.41
,	is
responsible	for	both	predicting	the	next	value	of	the	PC	and	selecting	the
actual	PC	for	the	instruction	fetch.	We	can	see	the	block	labeled	&quot;Predict
PC&quot;	can	choose	either	valP	(as	computed	by	the	PC	incrementer)	or	valC
(from	the	fetched	instruction).	This	value	is	stored	in	pipeline	register	F	as
the	
predicted
value	of	the	program	counter.	The	block	labeled	&quot;Select
PC&quot;	is	similar	to	the	block	labeled	&quot;PC&quot;	in	the	SEQ+	PC	selection	stage
(
Figure	
4.40
).	It	chooses	one	of	three	values	to	serve	as	the	address
for	the	instruction	memory:	the	predicted	PC,	the	value	of	valP	for	a	not-
taken	branch	instruction	that	reaches	pipeline	register	M	(stored	in
register	M_valA),	or	the	value	of	the	return	address	when	a	
instruction	reaches	pipeline	register	W	(stored	in	W_valM).
4.5.5	
Pipeline	Hazards
Our	structure	PIPE—	is	a	good	start	at	creating	a	pipelined	Y86-64
processor.	Recall	from	our	discussion	in	
Section	
4.4.4
,	however,	that</p>
<p>introducing	pipelining	into	a	system	with	feedback	can	lead	to	problems
when	there	are	dependencies	between	successive	instructions.	We	must
resolve	this	issue	before	we	can	complete	our	design.	These
dependencies	can	take	two	forms:	(1)	
data
dependencies,	where	the
results	computed	by	one	instruction	are	used	as	the	data	for	a	following
instruction,	and	(2)	
control
dependencies,	where	one	instruction
determines	the	location	of	the	following	instruction,	such	as	when
executing	a	jump,	call,	or	return.	When	such	dependencies	have	the
potential	to	cause	an	erroneous	computation	by	the	pipeline,	they	are
called	
hazards
.	Like	dependencies,	hazards	can	be	classified	as	either
data	hazards
or	
control	hazards
.	We	first	concern	ourselves	with	data
hazards	and	then	consider	control	hazards.
Figure	
4.43	
Pipelined	execution	of	
without	special	pipeline
control.</p>
<p>In	cycle	6,	the	second	
writes	its	result	to	program	register	
.
The	
instruction	reads	its	source	operands	in	cycle	7,	so	it	gets
correct	values	for	both	
and	
.
Figure	
4.43
illustrates	the	processing	of	a	sequence	of	instructions	we
refer	to	as	
by	the	PIPE—	processor.	Let	us	assume	in	this	example
and	successive	ones	that	the	program	registers	initially	all	have	value	0.
The	code	loads	values	10	and	3	into	program	registers	
and	
,
executes	three	
instructions,	and	then	adds	register	
to	
.	We
focus	our	attention	on	the	potential	data	hazards	resulting	from	the	data
dependencies	between	the	two	
instructions	and	the	
instruction.	On	the	right-hand	side	of	the	figure,	we	show	a	pipeline
diagram	for	the	instruction	sequence.	The	pipeline	stages	for	cycles	6
and	7	are	shown	highlighted	in	the	pipeline	diagram.	Below	this,	we	show
an	expanded	view	of	the	write-back	activity	in	cycle	6	and	the	decode
activity	during	cycle	7.	After	the	start	of	cycle	7,	both	of	the	
instructions	have	passed	through	the	write	back	stage,	and	so	the
register	file	holds	the	updated	values	of	
and	
.	As	the	
instruction	passes	through	the	decode	stage	during	cycle	7,	it	will
therefore	read	the	correct	values	for	its	source	operands.	The	data
dependencies	between	the	two	
instructions	and	the	
instruction	have	not	created	data	hazards	in	this	example.
We	saw	that	
will	flow	through	our	pipeline	and	get	the	correct
results,	because	the	three	
instructions	create	a	delay	between
instructions	with	data</p>
<p>Figure	
4.44	
Pipelined	execution	of	
without	special	pipeline
control.
The	write	to	program	register	
does	not	occur	until	the	start	of	cycle
7,	and	so	the	
instruction	gets	the	incorrect	value	for	this	register	in
the	decode	stage.
dependencies.	Let	us	see	what	happens	as	these	
instructions	are
removed.	
Figure	
4.44
illustrates	the	pipeline	flow	of	a	program,	named
,	containing	two	
instructions	between	the	two	
instructions	generating	values	for	registers	
and	
and	the	
instruction	having	these	two	registers	as	operands.	In	this	case,	the
crucial	step	occurs	in	cycle	6,	when	the	
instruction	reads	its
operands	from	the	register	file.	An	expanded	view	of	the	pipeline
activities	during	this	cycle	is	shown	at	the	bottom	of	the	figure.	The	first
instruction	has	passed	through	the	write-back	stage,	and	so</p>
<p>program	register	
has	been	updated	in	the	register	file.	The	second
instruction	is	in	the	write-back	stage	during	this	cycle,	and	so	the
write	to	program	register	
only	occurs	at	the	start	of	cycle	7	as	the
clock	rises.	As	a	result,	the	incorrect	value	zero	would	be	read	for	register
(recall	that	we	assume	all	registers	are	initially	zero),	since	the
pending	write	for	this	register	has	not	yet	occurred.	Clearly,	we	will	have
to	adapt	our	pipeline	to	handle	this	hazard	properly.
Figure	
4.45
shows	what	happens	when	we	have	only	one	
instruction	between	the	
instructions	and	the	
instruction,
yielding	a	program	
.	Now	we	must	examine	the	behavior	of	the
pipeline	during	cycle	5	as	the	
instruction	passes	through	the	decode
stage.	Unfortunately,	the	pending</p>
<p>Figure	
4.45	
Pipelined	execution	of	
without	special	pipeline
control.
In	cycle	5,	the	
instruction	reads	its	source	operands	from	the
register	file.	The	pending	write	to	register	
is	still	in	the	write-back
stage,	and	the	pending	write	to	register	
is	still	in	the	memory	stage.
Both	operands	valA	and	valB	get	incorrect	values.
write	to	register	
is	still	in	the	write-back	stage,	and	the	pending	write
to	
is	still	in	the	memory	stage.	Therefore,	the	
instruction	would
get	the	incorrect	values	for	both	operands.
Figure	
4.46
shows	what	happens	when	we	remove	all	of	the	
instructions	between	the	
instructions	and	the	
instruction,
yielding	a	program	
.	Now	we	must	examine	the	behavior	of	the
pipeline	during	cycle	4	as	the	
instruction	passes	through	the	decode
stage.	Unfortunately,	the	pending	write	to	register	
is	still	in	the
memory	stage,	and	the	new	value	for	
is	just	being	computed	in	the
execute	stage.	Therefore,	the	
instruction	would	get	the	incorrect
values	for	both	operands.
These	examples	illustrate	that	a	data	hazard	can	arise	for	an	instruction
when	one	of	its	operands	is	updated	by	any	of	the	three	preceding
instructions.	These	hazards	occur	because	our	pipelined	processor	reads
the	operands	for	an	instruction	from	the	register	file	in	the	decode	stage
but	does	not	write	the	results	for	the	instruction	to	the	register	file	until
three	cycles	later,	after	the	instruction	passes	through	the	write-back
stage.</p>
<p>Figure	
4.46	
Pipelined	execution	of	</p>
<p>without	special	pipeline
control.
In	cycle	4,	the	
instruction	reads	its	source	operands	from	the
register	file.	The	pending	write	to	register	
is	still	in	the	memory
stage,	and	the	new	value	for	register	
is	just	being	computed	in	the
execute	stage.	Both	operands	valA	and	valB	get	incorrect	values.
Avoiding	Data	Hazards	by	Stalling
One	very	general	technique	for	avoiding	hazards	involves	
stalling
,	where
the	processor	holds	back	one	or	more	instructions	in	the	pipeline	until	the
hazard	condition	no	longer	holds.	Our	processor	can	avoid	data	hazards
by	holding	back	an	instruction	in	the	decode	stage	until	the	instructions
generating	its	source	operands	have	passed	through	the	write-back
stage.	The	details	of	this	mechanism	will	be	discussed	in	
Section</p>
<p>4.5.8
.	It	involves	simple	enhancements	to	the	pipeline	control	logic.
The	effect	of	stalling	is	diagrammed	in	
Figure	
4.47
(
)	and	
Figure
4.48
(
).	(We	omit	
from	this	discussion,	since	it	operates
similarly	to	the	other	two	examples.)	When	the	
instruction	is	in	the
decode	stage,	the	pipeline	control	logic	detects	that	at	least	one	of	the
instructions	in	the	execute,	memory,	or	write-back	stage	will	update	either
register	
or	register	
.	Rather	than	letting	the	
instruction
pass	through	the	stage	with	the	incorrect	results,	it	stalls	the	instruction,
holding	it	back	in	the	decode	stage	for	either	one	(for	
)	or	three	(for
)	extra	cycles.	For	all	three	programs,	the	
instruction	finally
gets	correct	values	for	its	two	source	operands	in	cycle	7	and	then
proceeds	down	the	pipeline.</p>
<p>Figure	
4.47	
Pipelined	execution	of	
using	stalls.
After	decoding	the	
instruction	in	cycle	6,	the	stall	control	logic
detects	a	data	hazard	due	to	the	pending	write	to	register	
in	the
write-back	stage.	It	injects	a	bubble	into	the	execute	stage	and	repeats
the	decoding	of	the	
instruction	in	cycle	7.	In	effect,	the	machine	has
dynamically	inserted	a	
instruction,	giving	a	flow	similar	to	that	shown
for	
(
Figure	
4.43
).
Figure	
4.48	
Pipelined	execution	of	
using	stalls.
After	decoding	the	
instruction	in	cycle	4,	the	stall	control	logic
detects	data	hazards	for	both	source	registers.	It	injects	a	bubble	into	the
execute	stage	and	repeats	the	decoding	of	the	
instruction	on	cycle
5.	It	again	detects	hazards	for	both	source	registers,	injects	a	bubble	into
the	execute	stage,	and	repeats	the	decoding	of	the	
instruction	on
cycle	6.	Still,	it	detects	a	hazard	for	source	register	
,	injects	a	bubble
into	the	execute	stage,	and	repeats	the	decoding	of	the	
instruction
on	cycle	7.	In	effect,	the	machine	has	dynamically	inserted	three	
instructions,	giving	a	flow	similar	to	that	shown	for	
(
Figure	
4.43
).</p>
<p>In	holding	back	the	
instruction	in	the	decode	stage,	we	must	also
hold	back	the	halt	instruction	following	it	in	the	fetch	stage.	We	can	do
this	by	keeping	the	program	counter	at	a	fixed	value,	so	that	the	halt
instruction	will	be	fetched	repeatedly	until	the	stall	has	completed.
Stalling	involves	holding	back	one	group	of	instructions	in	their	stages
while	allowing	other	instructions	to	continue	flowing	through	the	pipeline.
What	then	should	we	do	in	the	stages	that	would	normally	be	processing
the	
instruction?	We	handle	these	by	injecting	a	
bubble
into	the
execute	stage	each	time	we	hold	an	instruction	back	in	the	decode	stage.
A	bubble	is	like	a	dynamically	generated	
instruction—it	does	not
cause	any	changes	to	the	registers,	the	memory,	the
Aside	
Enumerating	classes	of	data
hazards
Hazards	can	potentially	occur	when	one	instruction	updates	part
of	the	program	state	that	will	be	read	by	a	later	instruction.	For
Y86-64,	the	program	state	includes	the	program	registers,	the
program	counter,	the	memory,	the	condition	code	register,	and	the
status	register.	Let	us	look	at	the	hazard	possibilities	in	our
proposed	design	for	each	of	these	forms	of	state.
Program	registers.	
These	are	the	hazards	we	have	already
identified.	They	arise	because	the	register	file	is	read	in	one
stage	and	written	in	another,	leading	to	possible	unintended
interactions	between	different	instructions.</p>
<p>Program	counter.	
Conflicts	between	updating	and	reading	the
program	counter	give	rise	to	control	hazards.	No	hazard	arises
when	our	fetch-stage	logic	correctly	predicts	the	new	value	of
the	program	counter	before	fetching	the	next	instruction.
Mispredicted	branches	and	
instructions	require	special
handling,	as	will	be	discussed	in	
Section	
4.5.5
.
Memory.	
Writes	and	reads	of	the	data	memory	both	occur	in
the	memory	stage.	By	the	time	an	instruction	reading	memory
reaches	this	stage,	any	preceding	instructions	writing	memory
will	have	already	done	so.	On	the	other	hand,	there	can	be
interference	between	instructions	writing	data	in	the	memory
stage	and	the	reading	of	instructions	in	the	fetch	stage,	since
the	instruction	and	data	memories	reference	a	single	address
space.	This	can	only	happen	with	programs	containing	
self-
modifying	code
,	where	instructions	write	to	a	portion	of
memory	from	which	instructions	are	later	fetched.	Some
systems	have	complex	mechanisms	to	detect	and	avoid	such
hazards,	while	others	simply	mandate	that	programs	should
not	use	self-modifying	code.	We	will	assume	for	simplicity	that
programs	do	not	modify	themselves,	and	therefore	we	do	not
need	to	take	special	measures	to	update	the	instruction
memory	based	on	updates	to	the	data	memory	during	program
execution.
Condition	code	register.	
These	are	written	by	integer
operations	in	the	execute	stage.	They	are	read	by	conditional
moves	in	the	execute	stage	and	by	conditional	jumps	in	the
memory	stage.	By	the	time	a	conditional	move	or	jump</p>
<p>reaches	the	execute	stage,	any	preceding	integer	operation
will	have	already	completed	this	stage.	No	hazards	can	arise.
Status	register.	
The	program	status	can	be	affected	by
instructions	as	they	flow	through	the	pipeline.	Our	mechanism
of	associating	a	status	code	with	each	instruction	in	the
pipeline	enables	the	processor	to	come	to	an	orderly	halt	when
an	exception	occurs,	as	will	be	discussed	in	
Section	
4.5.6
.
This	analysis	shows	that	we	only	need	to	deal	with	register	data
hazards,	control	hazards,	and	making	sure	exceptions	are
handled	properly.	A	systematic	analysis	of	this	form	is	important
when	designing	a	complex	system.	It	can	identify	the	potential
difficulties	in	implementing	the	system,	and	it	can	guide	the
generation	of	test	programs	to	be	used	in	checking	the
correctness	of	the	system.
condition	codes,	or	the	program	status.	These	are	shown	as	white	boxes
in	the	pipeline	diagrams	of	
Figures	
4.47
and	
4.48
.	In	these	figures
the	arrow	between	the	box	labeled	&quot;D&quot;	for	the	
instruction	and	the
box	labeled	&quot;E&quot;	for	one	of	the	pipeline	bubbles	indicates	that	a	bubble
was	injected	into	the	execute	stage	in	place	of	the	
instruction	that
would	normally	have	passed	from	the	decode	to	
the	execute	stage.	We
will	look	at	the	detailed	mechanisms	for	making	the	pipeline	stall	and	for
injecting	bubbles	in	
Section	
4.5.8
.
In	using	stalling	to	handle	data	hazards,	we	effectively	execute	programs
and	
by	dynamically	generating	the	pipeline	flow	seen	for
(
Figure	
4.43
).	Injecting	one	bubble	for	
and	three	for	
has	the	same	effect	as	having	three	
instructions	between	the	second</p>
<pre><code>instruction	and	the	
instruction.	This	mechanism	can	be
</code></pre>
<p>implemented	fairly	easily	(see	
Problem	
4.53
),	but	the	resulting
performance	is	not	very	good.	There	are	numerous	cases	in	which	one
instruction	updates	a	register	and	a	closely	following	instruction	uses	the
same	register.	This	will	cause	the	pipeline	to	stall	for	up	to	three	cycles,
reducing	the	overall	throughput	significantly.
Avoiding	Data	Hazards	by	Forwarding
Our	design	for	PIPE—	reads	source	operands	from	the	register	file	in	the
decode	stage,	but	there	can	also	be	a	pending	write	to	one	of	these
source	registers	in	the	write-back	stage.	Rather	than	stalling	until	the
write	has	completed,	it	can	simply	pass	the	value	that	is	about	to	be
written	to	pipeline	register	E	as	the	source	operand.	
Figure	
4.49
shows	this	strategy	with	an	expanded	view	of	the	pipeline	diagram	for
cycle	6	of	
.	The	decode-stage	logic	detects	that	register</p>
<p>Figure	
4.49	
Pipelined	execution	of	</p>
<p>using	forwarding.
In	cycle	6,	the	decode-stage	logic	detects	the	presence	of	a	pending
write	to	register	
in	the	write-back	stage.	It	uses	this	value	for	source
operand	valB	rather	than	the	value	read	from	the	register	file.
Figure	
4.50	
Pipelined	execution	of	</p>
<p>using	forwarding.
In	cycle	5,	the	decode-stage	logic	detects	a	pending	write	to	register	
in	the	write-back	stage	and	to	register	
in	the	memory	stage.	It	uses
these	as	the	values	for	valA	and	valB	rather	than	the	values	read	from
the	register	file.
is	the	source	register	for	operand	valB,	and	that	there	is	also	a
pending	write	to	
on	write	port	E.	It	can	therefore	avoid	stalling	by</p>
<p>simply	using	the	data	word	supplied	to	port	E	(signal	W_valE)	as	the
value	for	operand	valB.	This	technique	of	passing	a	result	value	directly
from	one	pipeline	stage	to	an	earlier	one	is	commonly	known	as	
data
forwarding
(or	simply	
forwarding
,	and	sometimes	
bypassing
).	It	allows	the
instructions	of	
to	proceed	through	the	pipeline	without	any	stalling.
Data	forwarding	requires	adding	additional	data	connections	and	control
logic	to	the	basic	hardware	structure.
As	
Figure	
4.50
illustrates,	data	forwarding	can	also	be	used	when
there	is	a	pending	write	to	a	register	in	the	memory	stage,	avoiding	the
need	to	stall	for	program	
.	In	cycle	5,	the	decode-stage	logic
detects	a	pending	write	to	register	
on	port	E	in	the	write-back	stage,
as	well	as	a	pending	write	to	register	
that	is	on	its	way	to	port	E	but
is	still	in	the	memory	stage.	Rather	than	stalling	until	the	writes	have
occurred,	it	can	use	the	value	in	the	write-back	stage	(signal	W_valE)	for
operand	valA	and	the	value	in	the	memory	stage	(signal	M_valE)	for
operand	valB.</p>
<p>Figure	
4.51	
Pipelined	execution	of	</p>
<p>using	forwarding.
In	cycle	4,	the	decode-stage	logic	detects	a	pending	write	to	register	
in	the	memory	stage.	It	also	detects	that	a	new	value	is	being	computed
for	register	
in	the	execute	stage.	It	uses	these	as	the	values	for	valA
and	valB	rather	than	the	values	read	from	the	register	file.
To	exploit	data	forwarding	to	its	full	extent,	we	can	also	pass	newly
computed	values	from	the	execute	stage	to	the	decode	stage,	avoiding
the	need	to	stall	for	program	
,	as	illustrated	in	
Figure	
4.51
.	In
cycle	4,	the	decode-stage	logic	detects	a	pending	write	to	register	
in
the	memory	stage,	and	also	that	the	value	being	computed	by	the	ALU	in
the	execute	stage	will	later	be	written	to	register	
.	It	can	use	the
value	in	the	memory	stage	(signal	M_valE)	for	operand	valA.	It	can	also
use	the	ALU	output	(signal	e_valE)	for	operand	valB.	Note	that	using	the
ALU	output	does	not	introduce	any	timing	problems.	The	decode	stage
only	needs	to	generate	signals	valA	and	valB	by	the	end	of	the	clock</p>
<p>cycle	so	that	pipeline	register	E	can	be	loaded	with	the	results	from	the
decode	stage	as	the	clock	rises	to	start	the	next	cycle.	The	ALU	output
will	be	valid	before	this	point.
The	uses	of	forwarding	illustrated	in	programs	
to	
all	involve
the	forwarding	of	values	generated	by	the	ALU	and	destined	for	write	port
E.	Forwarding	can	also	be	used	with	values	read	from	the	memory	and
destined	for	write	port	M.	From	the	memory	stage,	we	can	forward	the
value	that	has	just	been	read	from	the	data	memory	(signal	m_valM).
From	the	write-back	stage,	we	can	forward	the	pending	write	to	port	M
(signal	W_valM).	This	gives	a	total	of	five	different	forwarding	sources
(e_valE,	m_valM,	M_valE,	W_valM,	and	W_valE)	and	two	different
forwarding	destinations	(valA	and	valB).
The	expanded	diagrams	of	
Figures	
4.49
to	
4.51
also	show	how	the
decode-stage	logic	can	determine	whether	to	use	a	value	from	the
register	file	or	to	use	a	forwarded	value.	Associated	with	every	value	that
will	be	written	back	to	the	register	file	is	the	destination	register	ID.	The
logic	can	compare	these	IDs	with	the	source	register	IDs	srcA	and	srcB
to	detect	a	case	for	forwarding.	It	is	possible	to	have	multiple	destination
register	IDs	match	one	of	the	source	IDs.	We	must	establish	a	priority
among	the	different	forwarding	sources	to	handle	such	cases.	This	will	be
discussed	when	we	look	at	the	detailed	design	of	the	forwarding	logic.
Figure	
4.52
shows	the	structure	of	PIPE,	an	extension	of	PIPE—	that
can	handle	data	hazards	by	forwarding.	Comparing	this	to	the	structure
of	PIPE—(
Figure	
4.41
),	we	can	see	that	the	values	from	the	five
forwarding	sources	are	fed	back	to	the	two	blocks	labeled	&quot;Sel+Fwd	A&quot;
and	&quot;Fwd	B&quot;	in	the	decode	stage.	The	block	labeled	&quot;Sel+Fwd	A&quot;</p>
<p>combines	the	role	of	the	block	labeled	&quot;Select	A&quot;	in	PIPE—	with	the
forwarding	logic.	It	allows	valA	for	pipeline	register	E	to	be	either	the
incremented	program	counter	valP,	the	value	read	from	the	A	port	of	the
register	file,	or	one	of	the	forwarded	values.	The	block	labeled	&quot;Fwd	B&quot;
implements	the	forwarding	logic	for	source	operand	valB.
Load/Use	Data	Hazards
One	class	of	data	hazards	cannot	be	handled	purely	by	forwarding,
because	memory	reads	occur	late	in	the	pipeline.	
Figure	
4.53
illustrates	an	example	of	a	
load/use	hazard
,	where	one	instruction	(the
at	address	
)	reads	a	value	from	memory	for	register	
while	the	next	instruction	(the	
at	address	
)	needs	this	value	as
a	source	operand.	Expanded	views	of	cycles	7	and	8	are	shown	in	the
lower	part	of	the	figure,	where	we	assume	all	program	registers	initially
have	value	0.	The	
instruction	requires	the	value	of	the	register	in
cycle	7,	but	it	is	not	generated	by	the	
instruction	until	cycle	8.	In
order	to	&quot;forward&quot;	from	the	
to	the	
,	the	forwarding	logic	would
have	to	make	the	value	go	backward	in	time!	Since	this	is	clearly
impossible,	we	must	find	some	other	mechanism	for	handling	this	form	of
data	hazard.	(The	data	hazard	for	register	
,	with	the	value	being
generated	by	the	
instruction	at	address	
and	used	by	the
instruction	at	address	
,	can	be	handled	by	forwarding.)
As	
Figure	
4.54
demonstrates,	we	can	avoid	a	load/use	data	hazard
with	a	combination	of	stalling	and	forwarding.	This	requires	modifications
of	the	control	logic,	but	it	can	use	existing	bypass	paths.	As	the	
instruction	passes	through	the	execute	stage,	the	pipeline	control	logic</p>
<p>detects	that	the	instruction	in	the	decode	stage	(the	
)	requires	the
result	read	from	memory.	It	stalls	the	instruction	in	the	decode	stage	for
one	cycle,	causing	a	bubble	to	be	injected	into	the	execute	stage.	As	the
expanded	view	of	cycle	8	shows,	the	value	read	from	memory	can	then
be	forwarded	from	the	memory	stage	to	the	
instruction	in	the
decode	stage.	The	value	for	register	
is	also	forwarded	from	the
write-back	to	the	memory	stage.	As	indicated	in	the	pipeline	diagram	by
the	arrow	from	the	box	labeled	&quot;D&quot;	in	cycle	7	to	the	box	labeled	&quot;E&quot;	in
cycle	8,	the	injected	bubble	replaces	the	
instruction	that	would
normally	continue	flowing	through	the	pipeline.</p>
<p>Figure	
4.52	
Hardware	structure	of	PIPE,	our	final	pipelined
implementation.
The	additional	bypassing	paths	enable	forwarding	the	results	from	the
three	preceding	instructions.	This	allows	us	to	handle	most	forms	of	data
hazards	without	stalling	the	pipeline.</p>
<p>Figure	
4.53	
Example	of	load/use	data	hazard.
The	
instruction	requires	the	value	of	register	
during	the
decode	stage	in	cycle	7.	The	preceding	
reads	a	new	value	for	this
register	during	the	memory	stage	in	cycle	8,	which	is	too	late	for	the	
instruction.
This	use	of	a	stall	to	handle	a	load/use	hazard	is	called	a	
load	interlock
.
Load	interlocks	combined	with	forwarding	suffice	to	handle	all	possible
forms	of	data	hazards.	Since	only	load	interlocks	reduce	the	pipeline
throughput,	we	can	nearly	achieve	our	throughput	goal	of	issuing	one
new	instruction	on	every	clock	cycle.
Avoiding	Control	Hazards</p>
<p>Control	hazards	arise	when	the	processor	cannot	reliably	determine	the
address	of	the	next	instruction	based	on	the	current	instruction	in	the
fetch	stage.	As	was	discussed	in	
Section	
4.5.4
,	control	hazards	can
only	occur	in	our	pipelined	processor	for	
and	jump	instructions.
Moreover,	the	latter	case	only	causes	difficulties	when	the	direction	of	a
conditional	jump	is	mispredicted.	In	this	section,	we	provide	a	high-level
view	of	how	these	hazards	can	be	handled.	The	detailed	implementation
will	be	presented	in	
Section	
4.5.8
as	part	of	a	more	general	discussion
of	the	pipeline	control.
For	the	
instruction,	consider	the	following	example	program.	This
program	is	shown	in	assembly	code,	but	with	the	addresses	of	the
different	instructions	on	the	left	for	reference:</p>
<p>Figure	
4.54	
Handling	a	load/use	hazard	by	stalling.
By	stalling	the	
instruction	for	one	cycle	in	the	decode	stage,	the
value	for	valB	can	be	forwarded	from	the	
instruction	in	the
memory	stage	to	the	
instruction	in	the	decode	stage.</p>
<p>Figure	
4.55
shows	how	we	want	the	pipeline	to	process	the	
instruction.	As	with	our	earlier	pipeline	diagrams,	this	figure	shows	the
pipeline	activity	with
Figure	
4.55	
Simplified	view	of	
instruction	processing.
The	pipeline	should	stall	while	the	
passes	through	the	decode,
execute,	and	memory	stages,	injecting	three	bubbles	in	the	process.	The
PC	selection	logic	will	choose	the	return	address	as	the	instruction	fetch
address	once	the	
reaches	the	write-back	stage	(cycle	7).
time	growing	to	the	right.	Unlike	before,	the	instructions	are	not	listed	in
the	same	order	they	occur	in	the	program,	since	this	program	involves	a
control	flow	where	instructions	are	not	executed	in	a	linear	sequence.	It	is
useful	to	look	at	the	instruction	addresses	to	identify	the	different
instructions	in	the	program.</p>
<p>As	this	diagram	shows,	the	
instruction	is	fetched	during	cycle	3	and
proceeds	down	the	pipeline,	reaching	the	write-back	stage	in	cycle	7.
While	it	passes	through	the	decode,	execute,	and	memory	stages,	the
pipeline	cannot	do	any	useful	activity.	Instead,	we	want	to	inject	three
bubbles	into	the	pipeline.	Once	the	
instruction	reaches	the	write-back
stage,	the	PC	selection	logic	will	set	the	program	counter	to	the	return
address,	and	therefore	the	fetch	stage	will	fetch	the	
instruction	at
the	return	point	(address	
).
To	handle	a	mispredicted	branch,	consider	the	following	program,	shown
in	assembly	code	but	with	the	instruction	addresses	shown	on	the	left	for
reference:
Figure	
4.56
shows	how	these	instructions	are	processed.	As	before,
the	instructions	are	listed	in	the	order	they	enter	the	pipeline,	rather	than
the	order	they	occur	in	the	program.	Since	the	jump	instruction	is
predicted	as	being	taken,	the	instruction	at	the	jump	target	will	be	fetched
in	cycle	3,	and	the	instruction	following	this	one	will	be	fetched	in	cycle	4.</p>
<p>By	the	time	the	branch	logic	detects	that	the	jump	should	not	be	taken
during	cycle	4,	two	instructions	have	been	fetched	that	should	not
continue	being	executed.	Fortunately,	neither	of	these	instructions	has
caused	a	change	in	the	programmer-visible	state.	That	can	only	occur
when	an	instruction
Figure	
4.56	
Processing	mispredicted	branch	instructions.
The	pipeline	predicts	branches	will	be	taken	and	so	starts	fetching
instructions	at	the	jump	target.	Two	instructions	are	fetched	before	the
misprediction	is	detected	in	cycle	4	when	the	jump	instruction	flows
through	the	execute	stage.	In	cycle	5,	the	pipeline	
cancels
the	two	target
instructions	by	injecting	bubbles	into	the	decode	and	execute	stages,	and
it	also	fetches	the	instruction	following	the	jump.
reaches	the	execute	stage,	where	it	can	cause	the	condition	codes	to
change.	At	this	point,	the	pipeline	can	simply	
cancel
(sometimes	called
instruction	squashing
)	the	two	misfetched	instructions	by	injecting
bubbles	into	the	decode	and	execute	stages	on	the	following	cycle	while
also	fetching	the	instruction	following	the	jump	instruction.	The	two
misfetched	instructions	will	then	simply	disappear	from	the	pipeline	and
therefore	not	have	any	effect	on	the	programmer-visible	state.	The	only</p>
<p>drawback	is	that	two	clock	cycles'	worth	of	instruction	processing
capability	have	been	wasted.
This	discussion	of	control	hazards	indicates	that	they	can	be	handled	by
careful	consideration	of	the	pipeline	control	logic.	Techniques	such	as
stalling	and	injecting	bubbles	into	the	pipeline	dynamically	adjust	the
pipeline	flow	when	special	conditions	arise.	As	we	will	discuss	in	
Section
4.5.8
,	a	simple	extension	to	the	basic	clocked	register	design	will
enable	us	to	stall	stages	and	to	inject	bubbles	into	pipeline	registers	as
part	of	the	pipeline	control	logic.
4.5.6	
Exception	Handling
As	we	will	discuss	in	
Chapter	
8
,	a	variety	of	activities	in	a	processor
can	lead	to	
exceptional	control	flow
,	where	the	normal	chain	of	program
execution	gets	broken.	Exceptions	can	be	generated	either	
internally
,	by
the	executing	program,	or	
externally
,	by	some	outside	signal.	Our
instruction	set	architecture	includes	three	different	internally	generated
exceptions,	caused	by	(1)	a	halt	instruction,	(2)	an	instruction	with	an
invalid	combination	of	instruction	and	function	code,	and	(3)	an	attempt	to
access	an	invalid	address,	either	for	instruction	fetch	or	data	read	or
write.	A	more	complete	processor	design	would	also	handle	external
exceptions,	such	as	when	the	processor	receives	a	signal	that	the
network	interface	has	received	a	new	packet	or	the	user	has	clicked	a
mouse	button.	Handling	
exceptions	correctly	is	a	challenging	aspect	of
any	microprocessor	design.	They	can	occur	at	unpredictable	times,	and
they	require	creating	a	clean	break	in	the	flow	of	instructions	through	the
processor	pipeline.	Our	handling	of	the	three	internal	exceptions	gives</p>
<p>just	a	glimpse	of	the	true	complexity	of	correctly	detecting	and	handling
exceptions.
Let	us	refer	to	the	instruction	causing	the	exception	as	the	
excepting
instruction.
In	the	case	of	an	invalid	instruction	address,	there	is	no	actual
excepting	instruction,	but	it	is	useful	to	think	of	there	being	a	sort	of
&quot;virtual	instruction&quot;	at	the	invalid	address.	In	our	simplified	ISA	model,	we
want	the	processor	to	halt	when	it	reaches	an	exception	and	to	set	the
appropriate	status	code,	as	listed	in	
Figure	
4.5
.	It	should	appear	that
all	instructions	up	to	the	excepting	instruction	have	completed,	but	none
of	the	following	instructions	should	have	any	effect	on	the	programmer-
visible	state.	In	a	more	complete	design,	the	processor	would	continue	by
invoking	an	exception	handler,	a	procedure	that	is	part	of	the	operating
system,	but	implementing	this	part	of	exception	handling	is	beyond	the
scope	of	our	presentation.
In	a	pipelined	system,	exception	handling	involves	several	subtleties.
First,	it	is	possible	to	have	exceptions	triggered	by	multiple	instructions
simultaneously.	For	example,	during	one	cycle	of	pipeline	operation,	we
could	have	a	halt	instruction	in	the	fetch	stage,	and	the	data	memory
could	report	an	out-of-bounds	data	address	for	the	instruction	in	the
memory	stage.	We	must	determine	which	of	these	exceptions	the
processor	should	report	to	the	operating	system.	The	basic	rule	is	to	put
priority	on	the	exception	triggered	by	the	instruction	that	is	furthest	along
the	pipeline.	In	the	example	above,	this	would	be	the	out-of-bounds
address	attempted	by	the	instruction	in	the	memory	stage.	In	terms	of	the
machine-language	program,	the	instruction	in	the	memory	stage	should
appear	to	execute	before	one	in	the	fetch	stage,	and	therefore	only	this
exception	should	be	reported	to	the	operating	system.</p>
<p>A	second	subtlety	occurs	when	an	instruction	is	first	fetched	and	begins
execution,	causes	an	exception,	and	later	is	canceled	due	to	a
mispredicted	branch.	The	following	is	an	example	of	such	a	program	in
its	object-code	form:
In	this	program,	the	pipeline	will	predict	that	the	branch	should	be	taken,
and	so	it	will	fetch	and	attempt	to	use	a	byte	with	value	
as	an
instruction	(generated	in	the	assembly	code	using	the	
directive).
The	decode	stage	will	therefore	detect	an	invalid	instruction	exception.
Later,	the	pipeline	will	discover	that	the	branch	should	not	be	taken,	and
so	the	instruction	at	address	
should	never	even	have	been
fetched.	The	pipeline	control	logic	will	cancel	this	instruction,	but	we	want
to	avoid	raising	an	exception.
A	third	subtlety	arises	because	a	pipelined	processor	updates	different
parts	of	the	system	state	in	different	stages.	It	is	possible	for	an
instruction	following	one	causing	an	exception	to	alter	some	part	of	the
state	before	the	excepting	instruction	completes.	For	example,	consider</p>
<p>the	following	code	sequence,	in	which	we	assume	that	user	programs	are
not	allowed	to	access	addresses	at	the	upper	end	of	the	64-bit	range:
The	
instruction	causes	an	address	exception,	because
decrementing	the	stack	pointer	causes	it	to	wrap	around	to
.	This	exception	is	detected	in	the	memory	stage.	On
the	same	cycle,	the	
instruction	is	in	the	execute	stage,	and	it	will
cause	the	condition	codes	to	be	set	to	new	values.	This	would	violate	our
requirement	that	none	of	the	instructions	following	the	excepting
instruction	should	have	had	any	effect	on	the	system	state.
In	general,	we	can	both	correctly	choose	among	the	different	exceptions
and	avoid	raising	exceptions	for	instructions	that	are	fetched	due	to
mispredicted	branches	by	merging	the	exception-handling	logic	into	the
pipeline	structure.	That	is	the	motivation	for	us	to	include	a	status	code
stat	in	each	of	our	pipeline	registers	(
Figures	
4.41
and	
4.52
).	If	an
instruction	generates	an	exception	at	some	stage	in	its	processing,	the
status	field	is	set	to	indicate	the	nature	of	the	exception.	The	exception
status	propagates	through	the	pipeline	with	the	rest	of	the	information	for
that	instruction,	until	it	reaches	the	write-back	stage.	At	this	point,	the</p>
<p>pipeline	control	logic	detects	the	occurrence	of	the	exception	and	stops
execution.
To	avoid	having	any	updating	of	the	programmer-visible	state	by
instructions	beyond	the	excepting	instruction,	the	pipeline	control	logic
must	disable	any	updating	of	the	condition	code	register	or	the	data
memory	when	an	instruction	in	the	memory	or	write-back	stages	has
caused	an	exception.	In	the	example	program	above,	the	control	logic	will
detect	that	the	
in	the	memory	stage	has	caused	an	exception,	and
therefore	the	updating	of	the	condition	code	register	by	the	
instruction	in	the	execute	stage	will	be	disabled.
Let	us	consider	how	this	method	of	handling	exceptions	deals	with	the
subtleties	we	have	mentioned.	When	an	exception	occurs	in	one	or	more
stages	of	a	pipeline,	the	information	is	simply	stored	in	the	status	fields	of
the	pipeline	registers.	The	event	has	no	effect	on	the	flow	of	instructions
in	the	pipeline	until	an	excepting	instruction	reaches	the	final	pipeline
stage,	except	to	disable	any	updating	of	the	programmer-visible	state
(the	condition	code	register	and	the	memory)	by	later	instructions	in	the
pipeline.	Since	instructions	reach	the	write-back	stage	in	the	same	order
as	they	would	be	executed	in	a	nonpipelined	processor,	we	are
guaranteed	that	the	first	instruction	encountering	an	exception	will	arrive
first	in	the	write-back	stage,	at	which	point	program	execution	can	stop
and	the	status	code	in	pipeline	register	W	can	be	recorded	as	the
program	status.	If	some	instruction	is	fetched	but	later	canceled,	any
exception	status	information	about	the	
instruction	gets	canceled	as	well.
No	instruction	following	one	that	causes	an	exception	can	alter	the
programmer-visible	state.	The	simple	rule	of	carrying	the	exception	status
together	with	all	other	information	about	an	instruction	through	the</p>
<p>pipeline	provides	a	simple	and	reliable	mechanism	for	handling
exceptions.
4.5.7	
PIPE	Stage	Implementations
We	have	now	created	an	overall	structure	for	PIPE,	our	pipelined	Y86-64
processor	with	forwarding.	It	uses	the	same	set	of	hardware	units	as	the
earlier	sequential	designs,	with	the	addition	of	pipeline	registers,	some
reconfigured	logic	blocks,	and	additional	pipeline	control	logic.	In	this
section,	we	go	through	the	design	of	the	different	logic	blocks,	deferring
the	design	of	the	pipeline	control	logic	to	the	next	section.	Many	of	the
logic	blocks	are	identical	to	their	counterparts	in	SEQ	and	SEQ+,	except
that	we	must	choose	proper	versions	of	the	different	signals	from	the
pipeline	registers	(written	with	the	pipeline	register	name,	written	in
uppercase,	as	a	prefix)	or	from	the	stage	computations	(written	with	the
first	character	of	the	stage	name,	written	in	lowercase,	as	a	prefix).
As	an	example,	compare	the	HCL	code	for	the	logic	that	generates	the
srcA	signal	in	SEQ	to	the	corresponding	code	in	PIPE:</p>
<p>They	differ	only	in	the	prefixes	added	to	the	PIPE	signals:	
for	the
source	values,	to	indicate	that	the	signals	come	from	pipeline	register	D,
and	
for	the	result	value,	to	indicate	that	it	is	generated	in	the	decode
stage.	To	avoid	repetition,	we	will	not	show	the	HCL	code	here	for	blocks
that	only	differ	from	those	in	SEQ	because	of	the	prefixes	on	names.	As
a	reference,	the	complete	HCL	code	for	PIPE	is	given	in	Web	Aside
ARCH
:
HCL</p>
<p>on	page	472.
PC	Selection	and	Fetch	Stage
Figure	
4.57
provides	a	detailed	view	of	the	PIPE	fetch	stage	logic.	As
discussed	earlier,	this	stage	must	also	select	a	current	value	for	the
program	counter	and	predict	the	next	PC	value.	The	hardware	units	for
reading	the	instruction	from</p>
<p>Figure	
4.57	
PIPE	PC	selection	and	fetch	logic.
Within	the	one	cycle	time	limit,	the	processor	can	only	predict	the
address	of	the	next	instruction.
memory	and	for	extracting	the	different	instruction	fields	are	the	same	as
those	we	considered	for	SEQ	(see	the	fetch	stage	in	
Section	
4.3.4
).
The	PC	selection	logic	chooses	between	three	program	counter	sources.
As	a	mispredicted	branch	enters	the	memory	stage,	the	value	of	valP	for
this	instruction	(indicating	the	address	of	the	following	instruction)	is	read
from	pipeline	register	M	(signal	M_valA).	When	a	
instruction	enters
the	write-back	stage,	the	return	address	is	read	from	pipeline	register	W</p>
<p>(signal	W_valM).	All	other	cases	use	the	predicted	value	of	the	PC,
stored	in	pipeline	register	F	(signal	F_predPC):
The	PC	prediction	logic	chooses	valC	for	the	fetched	instruction	when	it
is	either	a	call	or	a	jump,	and	valP	otherwise:
The	logic	blocks	labeled	&quot;Instr	valid,&quot;	&quot;Need	regids,&quot;	and	&quot;Need	valC&quot;	are
the	same	as	for	SEQ,	with	appropriately	named	source	signals.
Unlike	in	SEQ,	we	must	split	the	computation	of	the	instruction	status	into
two	parts.	In	the	fetch	stage,	we	can	test	for	a	memory	error	due	to	an
out-of-range	instruction	address,	and	we	can	detect	an	illegal	instruction</p>
<p>or	a	halt	instruction.	Detecting	an	invalid	data	address	must	be	deferred
to	the	memory	stage.
Practice	Problem	
4.30	
(solution	page	
490
)
Write	HCL	code	for	the	signal	f_stat,	providing	the	provisional	status	for
the	fetched	instruction.
Decode	and	Write-Back	Stages
Figure	
4.58
gives	a	detailed	view	of	the	decode	and	write-back	logic
for	PIPE.	The	blocks	labeled	dstE,	dstM,	srcA,	and	srcB	are	very	similar
to	their	counterparts	in	the	implementation	of	SEQ.	Observe	that	the
register	IDs	supplied	to	the	write	ports	come	from	the	write-back	stage
(signals	W_dstE	and	W_dstM),	rather	than	from	the	decode	stage.	This	is
because	we	want	the	writes	to	occur	to	the	destination	registers	specified
by	the	instruction	in	the	write-back	stage.</p>
<p>Practice	Problem	
4.31	
(solution	page	
490
)
The	block	labeled	&quot;dstE&quot;	in	the	decode	stage	generates	the	register	ID
for	the	E	port	of	the	register	file,	based	on	fields	from	the	fetched
instruction	in	pipeline	register	D.	The	resulting	signal	is	named	d_dstE	in
the	HCL	description	of	PIPE.	Write	HCL	code	for	this	signal,	based	on	the
HCL	description	of	the	SEQ	signal	dstE.	(See	the	decode	stage	for	SEQ
in	
Section	
4.3.4
.)	Do	not	concern	yourself	with	the	logic	to	implement
conditional	moves	yet.
Most	of	the	complexity	of	this	stage	is	associated	with	the	forwarding
logic.	As	mentioned	earlier,	the	block	labeled	&quot;Sel+Fwd	A&quot;	serves	two
roles.	It	merges	the	valP	signal	into	the	valA	signal	for	later	stages	in
order	to	reduce	the	amount	of	state	in	the	pipeline	register.	It	also
implements	the	forwarding	logic	for	source	operand	valA.
The	merging	of	signals	valA	and	valP	exploits	the	fact	that	only	the	
and	jump	instructions	need	the	value	of	valP	in	later	stages,	and	these
instructions</p>
<p>Figure	
4.58	
PIPE	decode	and	write-back	stage	logic.
No	instruction	requires	both	valP	and	the	value	read	from	register	port	A,
and	so	these	two	can	be	merged	to	form	the	signal	valA	for	later	stages.
The	block	labeled	&quot;Sel+Fwd	A&quot;	performs	this	task	and	also	implements
the	forwarding	logic	for	source	operand	valA.	The	block	labeled	&quot;Fwd	B&quot;
implements	the	forwarding	logic	for	source	operand	valB.	The	register
write	locations	are	specified	by	the	dstE	and	dstM	signals	from	the	write-
back	stage	rather	than	from	the	decode	stage,	since	it	is	writing	the
results	of	the	instruction	currently	in	the	write-back	stage.</p>
<p>do	not	need	the	value	read	from	the	A	port	of	the	register	file.	This
selection	is	controlled	by	the	icode	signal	for	this	stage.	When	signal
D_icode	matches	the	instruction	code	for	either	
or	
,	this	block
should	select	D_valP	as	its	output.
As	mentioned	in	
Section	
4.5.5
,	there	are	five	different	forwarding
sources,	each	with	a	data	word	and	a	destination	register	ID:
Data	word
Register	ID
Source	description
e_valE
e_dstE
ALU	output
m_valM
M_dstM
Memory	output
M_valE
M_dstE
Pending	write	to	port	E	in	memory	stage
W_valM
W_dstM
Pending	write	to	port	M	in	write-back	stage
W_valE
W_dstE
Pending	write	to	port	E	in	write-back	stage
If	none	of	the	forwarding	conditions	hold,	the	block	should	select	d_rvalA,
the	value	read	from	register	port	A,	as	its	output.
Putting	all	of	this	together,	we	get	the	following	HCL	description	for	the
new	value	of	valA	for	pipeline	register	E:</p>
<p>The	priority	given	to	the	five	forwarding	sources	in	the	above	HCL	code	is
very	important.	This	priority	is	determined	in	the	HCL	code	by	the	order	in
which	the	five	destination	register	IDs	are	tested.	If	any	order	other	than
the	one	shown	were	chosen,	the	pipeline	would	behave	incorrectly	for
some	programs.	
Figure	
4.59	
shows	an	example	of	a	program	that
requires	a	correct	setting	of	priority	among	the	forwarding	sources	in	the
execute	and	memory	stages.	In	this	program,	the	first	two	instructions
write	to	register	
,	while	the	third	uses	this	register	as	its	source
operand.	When	the	
instruction	reaches	the	decode	stage	in	cycle
4,	the	forwarding	logic	must	choose	between	two	values	destined	for	its
source	register.	Which	one	should	it	choose?	To	set	the	priority,	we	must
consider	the	behavior	of	the	machine-language	program	when	it	is
executed	one	instruction	at	a	time.	The	first	
instruction	would	set
register	
to	10,	the	second	would	set	the	register	to	3,	and	then	the
instruction	would	read	3	from	
.	To	imitate	this	behavior,	our
pipelined	implementation	should	always	give	priority	to	the	forwarding
source	in	the	earliest	pipeline	stage,	since	it	holds	the	latest	instruction	in
the	program	sequence	setting	the	register.	Thus,	the	logic	in	the	HCL
code	above	first	tests	the	forwarding	source	in	the	execute	stage,	then
those	in	the	memory	stage,	and	finally	the	sources	in	the	write-back
stage.	The	forwarding	priority	between	the	two	sources	in	either	the</p>
<p>memory	or	the	write-back	stages	is	only	a	concern	for	the	instruction
popq	
,	since	only	this	instruction	can	attempt	two	simultaneous
writes	to	the	same	register.
Figure	
4.59	
Demonstration	of	forwarding	priority.
In	cycle	4,	values	for	
are	available	from	both	the	execute	and
memory	stages.	The	forwarding	logic	should	choose	the	one	in	the
execute	stage,	since	it	represents	the	most	recently	generated	value	for
this	register.
Practice	Problem	
4.32	
(solution	page	
490
)
Suppose	the	order	of	the	third	and	fourth	cases	(the	two	forwarding
sources	from	the	memory	stage)	in	the	HCL	code	for	d_valA	were
reversed.	Describe	the	resulting	behavior	of	the	
instruction	(line	5)
for	the	following	program:</p>
<p>Practice	Problem	
4.33	
(solution	page	
491
)
Suppose	the	order	of	the	fifth	and	sixth	cases	(the	two	forwarding
sources	from	the	write-back	stage)	in	the	HCL	code	for	d_valA	were
reversed.	Write	a	Y86-64	program	that	would	be	executed	incorrectly.
Describe	how	the	error	would	occur	and	its	effect	on	the	program
behavior.
Practice	Problem	
4.34	
(solution	page	
491
)
Write	HCL	code	for	the	signal	d_valB,	giving	the	value	for	source	operand
valB	supplied	to	pipeline	register	E.
One	small	part	of	the	write-back	stage	remains.	As	shown	in	
Figure
4.52
,	the	overall	processor	status	Stat	is	computed	by	a	block	based
on	the	status	value	in	pipeline	registerW.	Recall	from	
Section	
4.1.1
that	the	code	should	indicate	either	normal	operation	(
)	or	one	of	the
three	exception	conditions.	Since	pipeline	registerWholds	the	state	of	the
most	recently	completed	instruction,	it	is	natural	to	use	this	value	as	an
indication	of	the	overall	processor	status.	The	only	special	case	to
consider	is	when	there	is	a	bubble	in	the	write-back	stage.	This	is	part	of</p>
<p>normal	operation,	and	so	we	want	the	status	code	to	be	
for	this	case
as	well:
Execute	Stage
Figure	
4.60
shows	the	execute	stage	logic	for	PIPE.	The	hardware
units	and	the	logic	blocks	are	identical	to	those	in	SEQ,	with	an
appropriate	renaming	of	signals.	We	can	see	the	signals	e_valE	and
e_dstE	directed	toward	the	decode	stage	as	one	of	the	forwarding
sources.	One	difference	is	that	the	logic	labeled	&quot;Set	CC,&quot;	which
determineswhether	or	not	to	update	the	condition	codes,	has
signalsm_stat	and	W_stat	as	inputs.	These	signals	are	used	to	detect
cases	where	an	instruction</p>
<p>Figure	
4.60	
PIPE	execute	stage	logic.
This	part	of	the	design	is	very	similar	to	the	logic	in	the	SEQ
implementation.
Figure	
4.61	
PIPE	memory	stage	logic.
Many	of	the	signals	from	pipeline	registers	M	and	W	are	passed	down	to
earlier	stages	to	provide	write-back	results,	instruction	addresses,	and
forwarded	results.
causing	an	exception	is	passing	through	later	pipeline	stages,	and
therefore	any	updating	of	the	condition	codes	should	be	suppressed.	This
aspect	of	the	design	is	discussed	in	
Section	
4.5.8
.
Practice	Problem	
4.35	
(solution	page	
491
)
Our	second	case	in	the	HCL	code	for	d_valA	uses	signal	e_dstE	to	see
whether	to	select	the	ALU	output	e_valE	as	the	forwarding	source.
Suppose	instead	that	we	use	signal	E_dstE,	the	destination	register	ID	in</p>
<p>pipeline	register	E	for	this	selection.	Write	a	Y86-64	program	that	would
give	an	incorrect	result	with	this	modified	forwarding	logic.
Memory	Stage
Figure	
4.61
shows	the	memory	stage	logic	for	PIPE.	Comparing	this
to	the	memory	stage	for	SEQ	(
Figure	
4.30
),	we	see	that,	as	noted
before,	the	block	labeled	&quot;Mem.	data&quot;	in	SEQ	is	not	present	in	PIPE.	This
block	served	to	select	between	data	sources	valP	(for	
instructions)
and	valA,	but	this	selection	is	now	performed	by	the	block	labeled
&quot;Sel+Fwd	A&quot;	in	the	decode	stage.	Most	other	blocks	in	this	stage	are
identical	to	their	counterparts	in	SEQ,	with	an	appropriate	renaming	of
the	signals.	In	this	figure,	you	can	also	see	that	many	of	the	values	in
pipeline	registers	and	M	and	W	are	supplied	to	other	parts	of	the	circuit
as	part	of	the	forwarding	and	pipeline	control	logic.
Practice	Problem	
4.36	
(solution	page	
492
)
In	this	stage,	we	can	complete	the	computation	of	the	status	code	Stat	by
detecting	the	case	of	an	invalid	address	for	the	data	memory.	Write	HCL
code	for	the	signal	m_stat.
4.5.8	
Pipeline	Control	Logic
We	are	now	ready	to	complete	our	design	for	PIPE	by	creating	the
pipeline	control	logic.	This	logic	must	handle	the	following	four	control</p>
<p>cases	for	which	other	mechanisms,	such	as	data	forwarding	and	branch
prediction,	do	not	suffice:
Load/use	hazards.	
The	pipeline	must	stall	for	one	cycle	between	an
instruction	that	reads	a	value	from	memory	and	an	instruction	that
uses	this	value.
Processing	ret.	
The	pipeline	must	stall	until	the	
instruction
reaches	the	write-back	stage.
Mispredicted	branches.	
By	the	time	the	branch	logic	detects	that	a
jump	should	not	have	been	taken,	several	instructions	at	the	branch
target	will	have	started	down	the	pipeline.	These	instructions	must	be
canceled,	and	fetching	should	begin	at	the	instruction	following	the
jump	instruction.
Exceptions.	
When	an	instruction	causes	an	exception,	we	want	to
disable	the	updating	of	the	programmer-visible	state	by	later
instructions	and	halt	execution	once	the	excepting	instruction	reaches
the	write-back	stage.
We	will	go	through	the	desired	actions	for	each	of	these	cases	and	then
develop	control	logic	to	handle	all	of	them.
Desired	Handling	of	Special	Control	Cases
For	a	load/use	hazard,	we	have	described	the	desired	pipeline	operation
in	
Section	
4.5.5
,	as	illustrated	by	the	example	of	
Figure	
4.54
.	Only
the	
and	popq	instructions	read	data	from	memory.	When	(1)	either
of	these	is	in	the	execute	stage	and	(2)	an	instruction	requiring	the
destination	register	is	in	the	decode	stage,	we	want	to	hold	back	the</p>
<p>second	instruction	in	the	decode	stage	and	inject	a	bubble	into	the
execute	stage	on	the	next	cycle.	After	this,	the	forwarding	logic	will
resolve	the	data	hazard.	The	pipeline	can	hold	back	an	instruction	in	the
decode	stage	by	keeping	pipeline	register	D	in	a	fixed	state.	In	doing	so,
it	should	also	keep	pipeline	register	F	in	a	fixed	state,	so	that	the	next
instruction	will	be	fetched	a	second	time.	In	summary,	implementing	this
pipeline	flow	requires	detecting	the	hazard	condition,	keeping	pipeline
registers	F	and	D	fixed,	and	injecting	a	bubble	into	the	execute	stage.
For	the	processing	of	a	
instruction,	we	have	described	the	desired
pipeline	operation	in	
Section	
4.5.5
.	The	pipeline	should	stall	for	three
cycles	until	the	return	address	is	read	as	the	
instruction	passes
through	the	memory	stage.
This	was	illustrated	by	a	simplified	pipeline	diagram	in	
Figure	
4.55
for
processing	the	following	program:</p>
<p>Figure	
4.62
provides	a	detailed	view	of	the	processing	of	the	
instruction	for	the	example	program.	The	key	observation	here	is	that
there	is	no	way	to	inject	a	bubble	into	the	fetch	stage	of	our	pipeline.	On
every	cycle,	the	fetch	stage	reads	
some
instruction	from	the	instruction
memory.	Looking	at	the	HCL	code	for	implementing	the	PC	prediction
logic	in	
Section	
4.5.7
,	we	can	see	that	for	the	
instruction,	the	new
value	of	the	PC	is	predicted	to	be	valP,	the	address	of	the	following
instruction.	In	our	example	program,	this	would	be	
,	the	address	of
the	
instruction	following	the	ret.	This	prediction	is	not	correct	for
this	example,	nor	would	it	be	for	most	cases,	but	we	are	not	attempting	to
predict	return	addresses	correctly	in	our	design.	For	three	clock	cycles,
the	fetch	stage	stalls,	causing	the	
instruction	to	be	fetched	but
then	replaced	by	a	bubble	in	the	decode	stage.	This	process	is	illustrated
in	
Figure	
4.62
by	the	three	fetches,	with	an	arrow	leading	down	to	the
bubbles	passing	through	the	remaining	pipeline	stages.	Finally,	the
instruction	is	fetched	on	cycle	7.	Comparing	
Figure	
4.62
with
Figure	
4.62	
Detailed	processing	of	the	
instruction.
The	fetch	stage	repeatedly	fetches	the	
instruction	following	the
instruction,	but	then	the	pipeline	control	logic	injects	a	bubble	into	the</p>
<p>decode	stage	rather	than	allowing	the	
instruction	to	proceed.	The
resulting	behavior	is	equivalent	to	that	shown	in	
Figure	
4.55
.
Figure	
4.55
,	we	see	that	our	implementation	achieves	the	desired
effect,	but	with	a	slightly	peculiar	fetching	of	an	incorrect	instruction	for
three	consecutive	cycles.
When	a	mispredicted	branch	occurs,	we	have	described	the	desired
pipeline	operation	in	
Section	
4.5.5
and	illustrated	it	in	
Figure	
4.56
.
The	misprediction	will	be	detected	as	the	jump	instruction	reaches	the
execute	stage.	The	control	logic	then	injects	bubbles	into	the	decode	and
execute	stages	on	the	next	cycle,	causing	the	two	incorrectly	fetched
instructions	to	be	canceled.	On	the	same	cycle,	the	pipeline	reads	the
correct	instruction	into	the	fetch	stage.
For	an	instruction	that	causes	an	exception,	we	must	make	the	pipelined
implementation	match	the	desired	ISA	behavior,	with	all	prior	instructions
completing	and	with	none	of	the	following	instructions	having	any	effect
on	the	program	state.	Achieving	these	effects	is	complicated	by	the	facts
that	(1)	exceptions	are	detected	during	two	different	stages	(fetch	and
memory)	of	program	execution,	and	(2)	the	program	state	is	updated	in
three	different	stages	(execute,	memory,	and	write-back).
Our	stage	designs	include	a	status	code	stat	in	each	pipeline	register	to
track	the	status	of	each	instruction	as	it	passes	through	the	pipeline
stages.	When	an	exception	occurs,	we	record	that	information	as	part	of
the	instruction's	status	and	continue	fetching,	decoding,	and	executing
instructions	as	if	nothing	were	amiss.	As	the	excepting	instruction</p>
<p>reaches	the	memory	stage,	we	take	steps	to	prevent	later	instructions
from	modifying	the	programmer-visible	state	by	(1)	disabling	the	setting
of	condition	codes	by	instructions	in	the	execute	stage,	(2)	injecting
bubbles	into	the	memory	stage	to	disable	any	writing	to	the	data	memory,
and	(3)	stalling	the	write-back	stage	when	it	has	an	excepting	instruction,
thus	bringing	the	pipeline	to	a	halt.
The	pipeline	diagram	in	
Figure	
4.63
illustrates	how	our	pipeline	control
handles	the	situation	where	an	instruction	causing	an	exception	is
followed	by	one	that	would	change	the	condition	codes.	On	cycle	6,	the
instruction	reaches	the	memory	stage	and	generates	a	memory
error.	On	the	same	cycle,	the	
instruction	in	the	execute	stage
generates	new	values	for	the	condition	codes.	We	disable	the	setting	of
condition	codes	when	an	excepting	instruction	is	in	the	memory	or	write-
back	stage	(by	examining	the	signals	m_stat	and	W_stat	and	then	setting
the	signal	set_cc	to	zero).	We	can	also	see	the	combination	of	inj	ecting
bubbles	into	the	memory	stage	and	stalling	the	excepting	instruction	in
the	write-back	stage	in	the	example	of	
Figure	
4.63
—the	
instruction	remains	stalled	in	the	write-back	stage,	and	none	of	the
subsequent	instructions	get	past	the	execute	stage.
By	this	combination	of	pipelining	the	status	signals,	controlling	the	setting
of	condition	codes,	and	controlling	the	pipeline	stages,	we	achieve	the
desired	behavior	for	exceptions:	all	instructions	prior	to	the	excepting
instruction	are	completed,	while	none	of	the	following	instructions	has
any	effect	on	the	programmer-visible	state.
Detecting	Special	Control	Conditions</p>
<p>Figure	
4.64
summarizes	the	conditions	requiring	special	pipeline
control.	It	gives	expressions	describing	the	conditions	under	which	the
three	special	cases	arise.
Figure	
4.63	
Processing	invalid	memory	reference	exception.
On	cycle	6,	the	invalid	memory	reference	by	the	
instruction	causes
the	updating	of	the	condition	codes	to	be	disabled.	The	pipeline	starts
injecting	bubbles	into	the	memory	stage	and	stalling	the	excepting
instruction	in	the	write-back	stage.
Condition
Trigger
Processing	ret
IRET	
∊
{D_icode,	E_icode,	M_icode}
Load/use	hazard
E_icode	
∊
{IMRMOVQ,	IPOPQ}	&amp;&amp;	E_dstM	
∊
{d_srcA,	d_srcB}
Mispredicted	branch
E_icode	=	IJXX&amp;&amp;	!e_Cnd
Exception
m_stat	
∊
{SADR,	SINS,	SHLT}	||	W_stat	
∊
{SADR,	SINS,	SHLT}
Figure	
4.64	
Detection	conditions	for	pipeline	control	logic.</p>
<p>Four	different	conditions	require	altering	the	pipeline	flow	by	either
stalling	the	pipeline	or	canceling	partially	executed	instructions.
These	expressions	are	implemented	by	simple	blocks	of	combinational
logic	that	must	generate	their	results	before	the	end	of	the	clock	cycle	in
order	to	control	the	action	of	the	pipeline	registers	as	the	clock	rises	to
start	the	next	cycle.	During	a	clock	cycle,	pipeline	registers	D,	E,	and	M
hold	the	states	of	the	instructions	that	are	in	the	decode,	execute,	and
memory	pipeline	stages,	respectively.	As	we	approach	the	end	of	the
clock	cycle,	signals	d_srcA	and	d_srcB	will	be	set	to	the	register	IDs	of
the	source	operands	for	the	instruction	in	the	decode	stage.	Detecting	a
instruction	as	it	passes	through	the	pipeline	simply	involves	checking
the	instruction	codes	of	the	instructions	in	the	decode,	execute,	and
memory	stages.	Detecting	a	load/use	hazard	involves	checking	the
instruction	type	(
or	
)	of	the	instruction	in	the	execute	stage
and	comparing	its	destination	register	with	the	source	registers	of	the
instruction	in	the	decode	stage.	The	pipeline	control	logic	should	detect	a
mispredicted	branch	while	the	jump	
instruction	is	in	the	execute	stage,	so
that	it	can	set	up	the	conditions	required	to	recover	from	the
misprediction	as	the	instruction	enters	the	memory	stage.	When	a	jump
instruction	is	in	the	execute	stage,	the	signal	e_Cnd	indicates	whether	or
not	the	jump	should	be	taken.	We	detect	an	excepting	instruction	by
examining	the	instruction	status	values	in	the	memory	and	write-back
stages.	For	the	memory	stage,	we	use	the	signal	m_stat,	computed
within	the	stage,	rather	than	M_stat	from	the	pipeline	register.	This
internal	signal	incorporates	the	possibility	of	a	data	memory	address
error.</p>
<p>Pipeline	Control	Mechanisms
Figure	
4.65
shows	low-level	mechanisms	that	allow	the	pipeline
control	logic	to	hold	back	an	instruction	in	a	pipeline	register	or	to	inject	a
bubble	into	the	pipeline.	These	mechanisms	involve	small	extensions	to
the	basic	clocked	register	described
Figure	
4.65	
Additional	pipeline	register	operations,
(a)	Under	normal	conditions,	the	state	and	output	of	the	register	are	set
to	the	value	at	the	input	when	the	clock	rises,	(b)	When	operated	in	
stall
mode,	the	state	is	held	fixed	at	its	previous	value,	(c)	When	operated	in
bubble
mode,	the	state	is	overwritten	with	that	of	a	
operation.</p>
<p>Pipeline	resister
Condition
F
D
E
M
W
Processing	ret
stall
bubble
normal
normal
normal
Load/use	hazard
stall
stall
bubble
normal
normal
Mispredicted	branch
normal
bubble
bubble
normal
normal
Figure	
4.66	
Actions	for	pipeline	control	logic.
The	different	conditions	require	altering	the	pipeline	flow	by	either	stalling
the	pipeline	or	canceling	partially	executed	instructions.
in	
Section	
4.2.5
.	Suppose	that	each	pipeline	register	has	two	control
inputs	stall	and	bubble.	The	settings	of	these	signals	determine	how	the
pipeline	register	is	updated	as	the	clock	rises.	Under	normal	operation
(
Figure	
4.65
(a)),	both	of	these	inputs	are	set	to	0,	causing	the	register
to	load	its	input	as	its	new	state.	When	the	stall	signal	is	set	to	1	(
Figure
4.65
(b)),	the	updating	of	the	state	is	disabled.	Instead,	the	register	will
remain	in	its	previous	state.	This	makes	it	possible	to	hold	back	an
instruction	in	some	pipeline	stage.	When	the	bubble	signal	is	set	to	1
(
Figure	
4.65
(c)),	the	state	of	the	register	will	be	set	to	some	fixed	
reset
configuration
,	giving	a	state	equivalent	to	that	of	a	
instruction.	The
particular	pattern	of	ones	and	zeros	for	a	pipeline	register's	reset
configuration	depends	on	the	set	of	fields	in	the	pipeline	register.	For
example,	to	inject	a	bubble	into	pipeline	register	D,	we	want	the	icode
field	to	be	set	to	the	constant	value	
(
Figure	
4.26
).	To	inject	a
bubble	into	pipeline	register	E,	we	want	the	icode	field	to	be	set	to	</p>
<p>and	the	dstE,	dstM,	srcA,	and	srcB	fields	to	be	set	to	the	constant	
.
Determining	the	reset	configuration	is	one	of	the	tasks	for	the	hardware
designer	in	designing	a	pipeline	register.	We	will	not	concern	ourselves
with	the	details	here.	We	will	consider	it	an	error	to	set	both	the	bubble
and	the	stall	signals	to	1.
The	table	in	
Figure	
4.66
shows	the	actions	the	different	pipeline
stages	should	take	for	each	of	the	three	special	conditions.	Each	involves
some	combination	of	normal,	stall,	and	bubble	operations	for	the	pipeline
registers.	In	terms	of	timing,	the	stall	and	bubble	control	signals	for	the
pipeline	registers	are	generated	by	blocks	of	combinational	logic.	These
values	must	be	valid	as	the	clock	rises,	causing	each	of	the	pipeline
registers	to	either	load,	stall,	or	bubble	as	the	next	clock	cycle	begins.
With	this	small	extension	to	the	pipeline	register	designs,	we	can
implement	a	complete	pipeline,	including	all	of	its	control,	using	the	basic
building	blocks	of	combinational	logic,	clocked	registers,	and	random
access	memories.
Combinations	of	Control	Conditions
In	our	discussion	of	the	special	pipeline	control	conditions	so	far,	we
assumed	that	at	most	one	special	case	could	arise	during	any	single
clock	cycle.	A	common	bug	in	designing	a	system	is	to	fail	to	handle
instances	where	multiple	special	conditions	arise	simultaneously.	Let	us
analyze	such	possibilities.	We	need	not	worry	about	combinations
involving	program	exceptions,	since	we	have	carefully	designed	our
exception-handling	mechanism	to	consider	other	instructions	in	the
pipeline.	
Figure	
4.67
diagrams	the	pipeline	states	that	cause	the	other
three	special	control</p>
<p>Figure	
4.67	
Pipeline	states	for	special	control	conditions.
The	two	pairs	indicated	can	arise	simultaneously.
conditions.	These	diagrams	show	blocks	for	the	decode,	execute,	and
memory	stages.	The	shaded	boxes	represent	particular	constraints	that
must	be	satisfied	for	the	condition	to	arise.	A	load/use	hazard	requires
that	the	instruction	in	the	execute	stage	reads	a	value	from	memory	into
a	register,	and	that	the	instruction	in	the	decode	stage	has	this	register	as
a	source	operand.	A	mispredicted	branch	requires	the	instruction	in	the
execute	stage	to	have	a	jump	instruction.	There	are	three	possible	cases
for	
—the	instruction	can	be	in	either	the	decode,	execute,	or	memory
stage.	As	the	
instruction	moves	through	the	pipeline,	the	earlier
pipeline	stages	will	have	bubbles.
We	can	see	by	these	diagrams	that	most	of	the	control	conditions	are
mutually	exclusive.	For	example,	it	is	not	possible	to	have	a	load/use
hazard	and	a	mispredicted	branch	simultaneously,	since	one	requires	a
load	instruction	(
or	
)	in	the	execute	stage,	while	the	other</p>
<div style="break-before: page; page-break-before: always;"></div><p>requires	a	jump.	Similarly,	the	second	and	third	
combinations	cannot
occur	at	the	same	time	as	a	load/use	hazard	or	a	mispredicted	branch.
Only	the	two	combinations	indicated	by	arrows	can	arise	simultaneously.
Combination	A	involves	a	not-taken	jump	instruction	in	the	execute	stage
and	a	
instruction	in	the	decode	stage.	Setting	up	this	combination</p>
<p>requires	the	
to	be	at	the	target	of	a	not-taken	branch.	The	pipeline
control	logic	should	detect	that	the	branch	was	mispredicted	and
therefore	cancel	the	
instruction.
Practice	Problem	
4.37	
(solution	page	
492
)
Write	a	Y86-64	assembly-language	program	that	causes	combination	A
to	arise	and	determines	whether	the	control	logic	handles	it	correctly.
Combining	the	control	actions	for	the	combination	A	conditions	(
Figure
4.66
),	we	get	the	following	pipeline	control	actions	(assuming	that
either	a	bubble	or	a	stall	overrides	the	normal	case):
Pipeline	resister
Condition
F
D
E
M
W
Processing	
stall
bubble
normal
normal
normal
Mispredicted	branch
normal
bubble
bubble
normal
normal
Combination
stall
bubble
bubble
normal
normal
That	is,	it	would	be	handled	like	a	mispredicted	branch,	but	with	a	stall	in
the	fetch	stage.	Fortunately,	on	the	next	cycle,	the	PC	selection	logic	will
choose	the	address	of	the	instruction	following	the	jump,	rather	than	the
predicted	program	counter,	and	so	it	does	not	matter	what	happens	with
the	pipeline	register	F.	We	conclude	that	the	pipeline	will	correctly	handle
this	combination.</p>
<p>Combination	B	involves	a	load/use	hazard,	where	the	loading	instruction
sets	register	
and	the	
instruction	then	uses	this	register	as	a
source	operand,	since	it	must	pop	the	return	address	from	the	stack.	The
pipeline	control	logic	should	hold	back	the	
instruction	in	the	decode
stage.
Practice	Problem	
4.38	
(solution	page	
492
)
Write	a	Y86-64	assembly-language	program	that	causes	combination	B
to	arise	and	completes	with	a	halt	instruction	if	the	pipeline	operates
correctly.
Combining	the	control	actions	for	the	combination	B	conditions	(
Figure
4.66
),	we	get	the	following	pipeline	control	actions:
Pipeline	resister
Condition
F
D
E
M
W
Processing	
stall
bubble
normal
normal
normal
Load/use	hazard
stall
stall
bubble
normal
normal
Combination
stall
bubble+stall
bubble
normal
normal
Desired
stall
stall
bubble
normal
normal
If	both	sets	of	actions	were	triggered,	the	control	logic	would	try	to	stall
the	
instruction	to	avoid	the	load/use	hazard	but	also	inject	a	bubble
into	the	decode	stage	due	to	the	
instruction.	Clearly,	we	do	not	want
the	pipeline	to	perform	both	sets	of	actions.	Instead,	we	want	it	to	just</p>
<p>take	the	actions	for	the	load/use	hazard.	The	actions	for	processing	the
instruction	should	be	delayed	for	one	cycle.
This	analysis	shows	that	combination	B	requires	special	handling.	In	fact,
our	original	implementation	of	the	PIPE	control	logic	did	not	handle	this
combination	correctly.	Even	though	the	design	had	passed	many
simulation	tests,	it	had	a	subtle	bug	that	was	uncovered	only	by	the
analysis	we	have	just	shown.	When	a	program	having	combination	B	was
executed,	the	control	logic	would	set	both	the	bubble	and	the	stall	signals
for	pipeline	register	D	to	1.	This	example	shows	the	importance	of
systematic	analysis.	It	would	be	unlikely	to	uncover	this	bug	by	just
running	normal	programs.	If	left	undetected,	the	pipeline	would	not
faithfully	implement	the	ISA	behavior.
Control	Logic	Implementation
Figure	
4.68
shows	the	overall	structure	of	the	pipeline	control	logic.
Based	on	signals	from	the	pipeline	registers	and	pipeline	stages,	the
control	logic	generates</p>
<p>Figure	
4.68	
PIPE	pipeline	control	logic.
This	logic	overrides	the	normal	flow	of	instructions	through	the	pipeline	to
handle	special	conditions	such	as	procedure	returns,	mispredicted
branches,	load/use	hazards,	and	program	exceptions.
stall	and	bubble	control	signals	for	the	pipeline	registers	and	also
determines	whether	the	condition	code	registers	should	be	updated.	We
can	combine	the	detection	conditions	of	
Figure	
4.64
with	the	actions
of	
Figure	
4.66
to	create	HCL	descriptions	for	the	different	pipeline
control	signals.
Pipeline	register	F	must	be	stalled	for	either	a	load/use	hazard	or	a	
instruction:</p>
<p>Practice	Problem	
4.39	
(solution	page	
493
)
Write	HCL	code	for	the	signal	D_stall	in	the	PIPE	implementation.
Pipeline	register	D	must	be	set	to	bubble	for	a	mispredicted	branch	or	a
instruction.	As	the	analysis	in	the	preceding	section	shows,	however,
it	should	
not	inject	a	bubble	when	there	is	a	load/use	hazard	in
combination	with	a	
instruction:
Practice	Problem	
4.40	
(solution	page	
493
)
Write	HCL	code	for	the	signal	E_bubble	in	the	PIPE	implementation.</p>
<p>Practice	Problem	
4.41	
(solution	page	
493
)
Write	HCL	code	for	the	signal	set_cc	in	the	PIPE	implementation.	This
should	only	occur	for	
instructions,	and	should	consider	the	effects	of
program	exceptions.
Practice	Problem	
4.42	
(solution	page	
493
)
Write	HCL	code	for	the	signals	M_bubble	and	W_stall	in	the	PIPE
implementation.	The	latter	signal	requires	modifying	the	exception
condition	listed	in	
Figure	
4.64
.
This	covers	all	of	the	special	pipeline	control	signal	values.	In	the
complete	HCL	code	for	PIPE,	all	other	pipeline	control	signals	are	set	to
zero.
4.5.9	
Performance	Analysis
We	can	see	that	the	conditions	requiring	special	action	by	the	pipeline
control	logic	all	cause	our	pipeline	to	fall	short	of	the	goal	of	issuing	a
new	instruction	on	every	clock	cycle.	We	can	measure	this	inefficiency	by
determining	how	often	a	bubble	gets	injected	into	the	pipeline,	since
these	cause	unused	pipeline	cycles.	A	return	instruction	generates	three
bubbles,	a	load/use	hazard	generates	one,	and	a	mispredicted	branch
generates	two.	We	can	quantify	the	effect	these	penalties	have	on	the
overall	performance	by	computing	an	estimate	of	the	average	number	of
clock	cycles	PIPE	would	require	per	instruction	it	executes,	a	measure
known	as	the	CPI	(for	&quot;cycles	per	instruction&quot;).	This	measure	is	the</p>
<p>reciprocal	of	the	average	throughput	of	the	pipeline,	but	with	time
measured	in	clock	cycles	rather	than	picoseconds.	It	is	a	useful	measure
of	the	architectural	efficiency	of	a	design.
If	we	ignore	the	performance	implications	of	exceptions	(which,	by
definition,	will	only	occur	rarely),	another	way	to	think	about	CPI	is	to
imagine	we	run	the
Aside	
Testing	the	design
As	we	have	seen,	there	are	many	ways	to	introduce	bugs	into	a
design,	even	for	a	simple	microprocessor.	With	pipelining,	there
are	many	subtle	interactions	between	the	instructions	at	different
pipeline	stages.	We	have	seen	that	many	of	the	design	challenges
involve	unusual	instructions	(such	as	popping	to	the	stack	pointer)
or	unusual	instruction	combinations	(such	as	a	not-taken	jump
followed	by	a	
).	We	also	see	that	exception	handling	adds	an
entirely	new	dimension	to	the	possible	pipeline	behaviors.	How,
then,	can	we	be	sure	that	our	design	is	correct?	For	hardware
manufacturers,	this	is	a	dominant	concern,	since	they	cannot
simply	report	an	error	and	have	users	download	code	patches
over	the	Internet.	Even	a	simple	logic	design	error	can	have
serious	consequences,	especially	as	microprocessors	are
increasingly	used	to	operate	systems	that	are	critical	to	our	lives
and	health,	such	as	automotive	antilock	braking	systems,	heart
pacemakers,	and	aircraft	control	systems.
Simply	simulating	a	design	while	running	a	number	of	&quot;typical&quot;
programs	is	not	a	sufficient	means	of	testing	a	system.	Instead,
thorough	testing	requires	devising	ways	of	systematically</p>
<p>generating	many	tests	that	will	exercise	as	many	different
instructions	and	instruction	combinations	as	possible.	In	creating
our	Y86-64	processor	designs,	we	also	devised	a	number	of
testing	scripts,	each	of	which	generates	many	different	tests,	runs
simulations	of	the	processor,	and	compares	the	resulting	register
and	memory	values	to	those	produced	by	our	
YIS</p>
<p>instruction	set
simulator.	Here	is	a	brief	description	of	the	scripts:
optest.	
Runs	49	tests	of	different	Y86-64	instructions	with
different	source	and	destination	registers
jtest.	
Runs	64	tests	of	the	different	jump	and	call	instructions,
with	different	combinations	of	whether	or	not	the	branches	are
taken
erntest.	
Runs	28	tests	of	the	different	conditional	move
instructions,	with	different	control	combinations
htest.	
Runs	600	tests	of	different	data	hazard	possibilities,	with
different	combinations	of	source	and	destination	instructions,
and	with	different	numbers	of	
instructions	between	the
instruction	pairs
ctest.	
Tests	22	different	control	combinations,	based	on	an
analysis	similar	to	what	we	did	in	
Section	
4.5.8
etest.	
Tests	12	different	combinations	where	an	instruction
causes	an	exception	and	the	instructions	following	it	could	alter
the	programmer-visible	state
The	key	idea	of	this	testing	method	is	that	we	want	to	be	as
systematic	as	possible,	generating	tests	that	create	the	different</p>
<p>conditions	that	are	likely	to	cause	pipeline	errors.
processor	on	some	benchmark	program	and	observe	the	operation	of	the
execute	stage.	On	each	cycle,	the	execute	stage	either	(1)	processes	an
instruction	and	this	instruction	continues	through	the	remaining	stages	to
completion,	or	(2)	processes	a	bubble	injected	due	to	one	of	the	three
special	cases.	If	the	stage	processes	a	total	of	
C
instructions	and	
C
bubbles,	then	the	processor	has	required	around	
C
+	
C
total	clock
cycles	to	execute	
C
instructions.	We	say	&quot;around&quot;	because	we	ignore
Aside	
Formally	verifying	our	design
Even	when	a	design	passes	an	extensive	set	of	tests,	we	cannot
be	certain	that	it	will	operate	correctly	for	all	possible	programs.
The	number	of	possible	programs	we	could	test	is	unimaginably
large,	even	if	we	only	consider	tests	consisting	of	short	code
segments.	Newer	methods	of	
formal	verification
,	however,	hold
the	promise	that	we	can	have	tools	that	rigorously	consider	all
possible	behaviors	of	a	system	and	determine	whether	or	not
there	are	any	design	errors.
We	were	able	to	apply	formal	verification	to	an	earlier	version	of
our	Y86-64	processors	
[13]
.	We	set	up	a	framework	to	compare
the	behavior	of	the	pipelined	design	PIPE	to	the	unpipelined
version	SEQ.	That	is,	it	was	able	to	prove	that	for	an	arbitrary
machine-language	program,	the	two	processors	would	have
identical	effects	on	the	programmer-visible	state.	Of	course,	our
verifier	cannot	actually	run	all	possible	programs,	since	there	are
an	infinite	number	of	them.	Instead,	it	uses	a	form	of	proof	by
induction,	showing	a	consistency	between	the	two	processors	on
a	cycle-by-cycle	basis.	Carrying	out	this	analysis	requires
i
b
i
b
i</p>
<p>reasoning	about	the	hardware	using	
symbolic	methods
in	which
we	consider	all	program	values	to	be	arbitrary	integers,	and	we
abstract	the	ALU	as	a	sort	of	&quot;black	box,&quot;	computing	some
unspecified	function	over	its	arguments.	We	assume	only	that	the
ALUs	for	SEQ	and	PIPE	compute	identical	functions.
We	used	the	HCL	descriptions	of	the	control	logic	to	generate	the
control	logic	for	our	symbolic	processor	models,	and	so	we	could
catch	any	bugs	in	the	HCL	code.	Being	able	to	show	that	SEQ	and
PIPE	are	identical	does	not	guarantee	that	either	of	them	faithfully
implements	the	instruction	set	architecture.	However,	it	would
uncover	any	bug	due	to	an	incorrect	pipeline	design,	and	this	is
the	major	source	of	design	errors.
In	our	experiments,	we	verified	not	only	a	version	of	PIPE	similar
to	the	one	we	have	presented	in	this	chapter	but	also	several
variants	that	we	give	as	homework	problems,	in	which	we	add
more	instructions,	modify	the	hardware	capabilities,	or	use
different	branch	prediction	strategies.	Interestingly,	we	found	only
one	bug	in	all	of	our	designs,	involving	control	combination	B
(described	in	
Section	
4.5.8
)	for	our	solution	to	the	variant
described	in	
Problem	
4.58
.	This	exposed	a	weakness	in	our
testing	regime	that	caused	us	to	add	additional	cases	to	the	ctest
testing	script.
Formal	verification	is	still	in	an	early	stage	of	development.	The
tools	are	often	difficult	to	use,	and	they	do	not	have	the	capacity	to
verify	large-scale	designs.	We	were	able	to	verify	our	processors
in	part	because	of	their	relative	simplicity.	Even	then,	it	required
several	weeks	of	effort	and	multiple	runs	of	the	tools,	each
requiring	up	to	8	hours	of	computer	time.	This	is	an	active	area	of</p>
<p>research,	with	some	tools	becoming	commercially	available	and
some	in	use	at	companies	such	as	Intel,	AMD,	and	IBM.
the	cycles	required	to	start	the	instructions	flowing	through	the	pipeline.
We	can	then	compute	the	CPI	for	this	benchmark	as	follows:
That	is,	the	CPI	equals	1.0	plus	a	penalty	term	
C
/
C
indicating	the
average	number	of	bubbles	injected	per	instruction	executed.	Since	only
three	different	instruction	types	can	cause	a	bubble	to	be	injected,	we
can	break	this	penalty	term	into	three	components:
Web	Aside	ARCH:VLOG	
Verilog
implementation	of	a	pipelined	Y86-64
processor
As	we	have	mentioned,	modern	logic	design	involves	writing
textual	representations	of	hardware	designs	in	a	
hardware
description	language.
The	design	can	then	be	tested	by	both
simulation	and	a	variety	of	formal	verification	tools.	Once	we	have
confidence	in	the	design,	we	can	use	
logic	synthesis
tools	to
translate	the	design	into	actual	logic	circuits.
We	have	developed	models	of	our	Y86-64	processor	designs	in
the	Verilog	hardware	description	language.	These	designs
combine	modules	implementing	the	basic	building	blocks	of	the
processor,	along	with	control	logic	generated	directly	from	the	HCL
descriptions.	We	have	been	able	to	synthesize	some	of	these
CPI=
C
i</p>
<ul>
<li></li>
</ul>
<h1>C
b
C
i</h1>
<p>1.0</p>
<ul>
<li></li>
</ul>
<p>C
b
C
i
b
i</p>
<p>designs,	download	the	logic	circuit	descriptions	onto	field-
programmable	gate	array	(FPGA)	hardware,	and	run	the
processors	on	actual	programs.
where	
lp
(for	&quot;load	penalty&quot;)	is	the	average	frequency	with	which	bubbles
are	injected	while	stalling	for	load/use	hazards,	
mp
(for	&quot;mispredicted
branch	penalty&quot;)	is	the	average	frequency	with	which	bubbles	are
injected	when	canceling	instructions	due	to	mispredicted	branches,	and
rp
(for	&quot;return	penalty&quot;)	is	the	average	frequency	with	which	bubbles	are
injected	while	stalling	for	
instructions.	Each	of	these	penalties
indicates	the	total	number	of	bubbles	injected	for	the	stated	reason
(some	portion	of	
C
)	divided	by	the	total	number	of	instructions	that	were
executed	(
C
.)
To	estimate	each	of	these	penalties,	we	need	to	know	how	frequently	the
relevant	instructions	(load,	conditional	branch,	and	return)	occur,	and	for
each	of	these	how	frequently	the	particular	condition	arises.	Let	us	pick
the	following	set	of	frequencies	for	our	CPI	computation	(these	are
comparable	to	measurements	reported	in	
[44]
and	
[46]
):
Load	instructions	(
and	
)	account	for	25%	of	all
instructions	executed.	Of	these,	20%	cause	load/use	hazards.
Conditional	branches	account	for	20%	of	all	instructions	executed.	Of
these,	60%	are	taken	and	40%	are	not	taken.
Return	instructions	account	for	2%	of	all	instructions	executed.
We	can	therefore	estimate	each	of	our	penalties	as	the	product	of	the
frequency	of	the	instruction	type,	the	frequency	the	condition	arises,	and
CPI=1
.0+
l
p</p>
<ul>
<li></li>
</ul>
<p>m
p</p>
<ul>
<li></li>
</ul>
<p>r
p
b
i</p>
<p>the	number	of	bubbles	that	get	injected	when	the	condition	occurs:
Cause
Name
Instruction
frequency
Condition
frequency
Bubbles
Product
Load/use
lp
0.25
0.20
1
0.05
Mispredict
mp
0.20
0.40
2
0.16
Return
rp
0.02
1.00
3
0.06<br />
Total
penalty
0.27
The	sum	of	the	three	penalties	is	0.27,	giving	a	CPI	of	1.27.
Our	goal	was	to	design	a	pipeline	that	can	issue	one	instruction	per
cycle,	giving	a	CPI	of	1.0.	We	did	not	quite	meet	this	goal,	but	the	overall
performance	is	still	quite	good.	We	can	also	see	that	any	effort	to	reduce
the	CPI	further	should	focus	on	mispredicted	branches.	They	account	for
0.16	of	our	total	penalty	of	0.27,	because	conditional	branches	are
common,	our	prediction	strategy	often	fails,	and	we	cancel	two
instructions	for	every	misprediction.
Practice	Problem	
4.43	
(solution	page	
494
)
Suppose	we	use	a	branch	prediction	strategy	that	achieves	a	success
rate	of	65%,	such	as	backward	taken,	forward	not	taken	(BTFNT),	as
described	in	
Section	
4.5.4
.	What	would	be	the	impact	on	CPI,
assuming	all	of	the	other	frequencies	are	not	affected?</p>
<p>Practice	Problem	
4.44	
(solution	page	
494
)
Let	us	analyze	the	relative	performance	of	using	conditional	data
transfers	versus	conditional	control	transfers	for	the	programs	you	wrote
for	Problems	4.5	and	4.6.	Assume	that	we	are	using	these	programs	to
compute	the	sum	of	the	absolute	values	of	a	very	long	array,	and	so	the
overall	performance	is	determined	largely	by	the	number	of	cycles
required	by	the	inner	loop.	Assume	that	our	jump	instructions	are
predicted	as	being	taken,	and	that	around	50%	of	the	array	values	are
positive.
A
.	
On	average,	how	many	instructions	are	executed	in	the	inner
loops	of	the	two	programs?
B
.	
On	average,	how	many	bubbles	would	be	injected	into	the	inner
loops	of	the	two	programs?
C
.	
What	is	the	average	number	of	clock	cycles	required	per	array
element	for	the	two	programs?
4.5.10	
Unfinished	Business
We	have	created	a	structure	for	the	PIPE	pipelined	microprocessor,
designed	the	control	logic	blocks,	and	implemented	pipeline	control	logic
to	handle	special	cases	where	normal	pipeline	flow	does	not	suffice.	Still,
PIPE	lacks	several	key	features	that	would	be	required	in	an	actual
microprocessor	design.	We	highlight	a	few	of	these	and	discuss	what
would	be	required	to	add	them.</p>
<p>Multicycle	Instructions
All	of	the	instructions	in	the	Y86-64	instruction	set	involve	simple
operations	such	as	adding	numbers.	These	can	be	processed	in	a	single
clock	cycle	within	the	execute	stage.	In	a	more	complete	instruction	set,
we	would	also	need	to	implement	instructions	requiring	more	complex
operations	such	as	integer	multiplication	and	
division	and	floating-point
operations.	In	a	medium-performance	processor	such	as	PIPE,	typical
execution	times	for	these	operations	range	from	3	or	4	cycles	for	floating-
point	addition	up	to	64	cycles	for	integer	division.	To	implement	these
instructions,	we	require	both	additional	hardware	to	perform	the
computations	and	a	mechanism	to	coordinate	the	processing	of	these
instructions	with	the	rest	of	the	pipeline.
One	simple	approach	to	implementing	multicycle	instructions	is	to	simply
expand	the	capabilities	of	the	execute	stage	logic	with	integer	and
floating-point	arithmetic	units.	An	instruction	remains	in	the	execute	stage
for	as	many	clock	cycles	as	it	requires,	causing	the	fetch	and	decode
stages	to	stall.	This	approach	is	simple	to	implement,	but	the	resulting
performance	is	not	very	good.
Better	performance	can	be	achieved	by	handling	the	more	complex
operations	with	special	hardware	functional	units	that	operate
independently	of	the	main	pipeline.	Typically,	there	is	one	functional	unit
for	performing	integer	multiplication	and	division,	and	another	for
performing	floating-point	operations.	As	an	instruction	enters	the	decode
stage,	it	can	be	
issued
to	the	special	unit.	While	the	unit	performs	the
operation,	the	pipeline	continues	processing	other	instructions.	Typically,</p>
<p>the	floating-point	unit	is	itself	pipelined,	and	thus	multiple	operations	can
execute	concurrently	in	the	main	pipeline	and	in	the	different	units.
The	operations	of	the	different	units	must	be	synchronized	to	avoid
incorrect	behavior.	For	example,	if	there	are	data	dependencies	between
the	different	operations	being	handled	by	different	units,	the	control	logic
may	need	to	stall	one	part	of	the	system	until	the	results	from	an
operation	handled	by	some	other	part	of	the	system	have	been
completed.	Often,	different	forms	of	forwarding	are	used	to	convey
results	from	one	part	of	the	system	to	other	parts,	just	as	we	saw
between	the	different	stages	of	PIPE.	The	overall	design	becomes	more
complex	than	we	have	seen	with	PIPE,	but	the	same	techniques	of
stalling,	forwarding,	and	pipeline	control	can	be	used	to	make	the	overall
behavior	match	the	sequential	ISA	model.
Interfacing	with	the	Memory	System
In	our	presentation	of	PIPE,	we	assumed	that	both	the	instruction	fetch
unit	and	the	data	memory	could	read	or	write	any	memory	location	in	one
clock	cycle.	We	also	ignored	the	possible	hazards	caused	by	self-
modifying	code	where	one	instruction	writes	to	the	region	of	memory	from
which	later	instructions	are	fetched.	Furthermore,	we	reference	memory
locations	according	to	their	virtual	addresses,	and	these	require	a
translation	into	physical	addresses	before	the	actual	read	or	write
operation	can	be	performed.	Clearly,	it	is	unrealistic	to	do	all	of	this
processing	in	a	single	clock	cycle.	Even	worse,	the	memory	values	being
accessed	may	reside	on	disk,	requiring	millions	of	clock	cycles	to	read
into	the	processor	memory.</p>
<p>As	will	be	discussed	in	
Chapters	
6
and	
9
,	the	memory	system	of	a
processor	uses	a	combination	of	multiple	hardware	memories	and
operating	system	software	to	manage	the	virtual	memory	system.	The
memory	system	is	organized	as	a	hierarchy,	with	faster	but	smaller
memories	holding	a	subset	of	the	memory	being	
backed	up	by	slower
and	larger	memories.	At	the	level	closest	to	the	processor,	the	
cache
memories	provide	fast	access	to	the	most	heavily	referenced	memory
locations.	A	typical	processor	has	two	first-level	caches—one	for	reading
instructions	and	one	for	reading	and	writing	data.	Another	type	of	cache
memory,	known	as	a	
translation	look-aside	buffer
,	or	TLB,	provides	a	fast
translation	from	virtual	to	physical	addresses.	Using	a	combination	of
TLBs	and	caches,	it	is	indeed	possible	to	read	instructions	and	read	or
write	data	in	a	single	clock	cycle	most	of	the	time.	Thus,	our	simplified
view	of	memory	referencing	by	our	processors	is	actually	quite
reasonable.
Although	the	caches	hold	the	most	heavily	referenced	memory	locations,
there	will	be	times	when	a	cache	
miss
occurs,	where	some	reference	is
made	to	a	location	that	is	not	held	in	the	cache.	In	the	best	case,	the
missing	data	can	be	retrieved	from	a	higher-level	cache	or	from	the	main
memory	of	the	processor,	requiring	3	to	20	clock	cycles.	Meanwhile,	the
pipeline	simply	stalls,	holding	the	instruction	in	the	fetch	or	memory	stage
until	the	cache	can	perform	the	read	or	write	operation.	In	terms	of	our
pipeline	design,	this	can	be	implemented	by	adding	more	stall	conditions
to	the	pipeline	control	logic.	A	cache	miss	and	the	consequent
synchronization	with	the	pipeline	is	handled	completely	by	hardware,
keeping	the	time	required	down	to	a	small	number	of	clock	cycles.
In	some	cases,	the	memory	location	being	referenced	is	actually	stored
in	the	disk	or	nonvolatile	memory.	When	this	occurs,	the	hardware</p>
<p>signals	a	
page	fault
exception.	Like	other	exceptions,	this	will	cause	the
processor	to	invoke	the	operating	system's	exception	handler	code.	This
code	will	then	set	up	a	transfer	from	the	disk	to	the	main	memory.	Once
this	completes,	the	operating	system	will	return	to	the	original	program,
where	the	instruction	causing	the	page	fault	will	be	re-executed.	This
time,	the	memory	reference	will	succeed,	although	it	might	cause	a	cache
miss.	Having	the	hardware	invoke	an	operating	system	routine,	which
then	returns	control	back	to	the	hardware,	allows	the	hardware	and
system	software	to	cooperate	in	the	handling	of	page	faults.	Since
accessing	a	disk	can	require	millions	of	clock	cycles,	the	several
thousand	cycles	of	processing	performed	by	the	OS	page	fault	handler
has	little	impact	on	performance.
From	the	perspective	of	the	processor,	the	combination	of	stalling	to
handle	short-duration	cache	misses	and	exception	handling	to	handle
long-duration	page	faults	takes	care	of	any	unpredictability	in	memory
access	times	due	to	the	structure	of	the	memory	hierarchy.</p>
<p>4.6	
Summary
We	have	seen	that	the	instruction	set	architecture,	or	ISA,	provides	a
layer	of	abstraction	between	the	behavior	of	a	processor—in	terms	of	the
set	of	instructions	and	their	encodings—and	how	the	processor	is
implemented.	The	ISA	provides	a	very	sequential	view	of	program
execution,	with	one	instruction	executed	to	completion	before	the	next
one	begins.
Aside	
State-of-the-art	microprocessor
design
A	five-stage	pipeline,	such	as	we	have	shown	with	the	PIPE
processor,	represented	the	state	of	the	art	in	processor	design	in
the	mid-1980s.	The	prototype	RISC	processor	developed	by
Patterson's	research	group	at	Berkeley	formed	the	basis	for	the
first	SPARC	processor,	developed	by	Sun	Microsystems	in	1987.
The	processor	developed	by	Hennessy's	research	group	at
Stanford	was	commercialized	by	MIPS	Technologies	(a	company
founded	by	Hennessy)	in	1986.	Both	of	these	used	five-stage
pipelines.	The	Intel	i486	processor	also	uses	a	five-stage	pipeline,
although	with	a	different	partitioning	of	responsibilities	among	the
stages,	with	two	decode	stages	and	a	combined	execute/memory
stage	
[27]
.</p>
<p>These	pipelined	designs	are	limited	to	a	throughput	of	at	most	one
instruction	per	clock	cycle.	The	CPI	(for	&quot;cycles	per	instruction&quot;)
measure	described	in	
Section	
4.5.9
can	never	be	less	than	1.0.
The	different	stages	can	only	process	one	instruction	at	a	time.
More	recent	processors	support	
superscalar
operation,	meaning
that	they	can	achieve	a	CPI	less	than	1.0	by	fetching,	decoding,
and	executing	multiple	instructions	in	parallel.	As	superscalar
processors	have	become	widespread,	the	accepted	performance
measure	has	shifted	from	CPI	to	its	reciprocal—the	average
number	of	instructions	executed	per	cycle,	or	IPC.	It	can	exceed
1.0	for	superscalar	processors.	The	most	advanced	designs	use	a
technique	known	as	
out-of-order
execution	to	execute	multiple
instructions	in	parallel,	possibly	in	a	totally	different	order	than
they	occur	in	the	program,	while	preserving	the	overall	behavior
implied	by	the	sequential	ISA	model.	This	form	of	execution	is
described	in	
Chapter	
5
as	part	of	our	discussion	of	program
optimization.
Pipelined	processors	are	not	just	historical	artifacts,	however.	The
majority	of	processors	sold	are	used	in	embedded	systems,
controlling	automotive	functions,	consumer	products,	and	other
devices	where	the	processor	is	not	directly	visible	to	the	system
user.	In	these	applications,	the	simplicity	of	a	pipelined	processor,
such	as	the	one	we	have	explored	in	this	chapter,	reduces	its	cost
and	power	requirements	compared	to	higher-performance	models.
More	recently,	as	multicore	processors	have	gained	a	following,
some	have	argued	that	we	could	get	more	overall	computing
power	by	integrating	many	simple	processors	on	a	single	chip
rather	than	a	smaller	number	of	more	complex	ones.	This	strategy
is	sometimes	referred	to	as	&quot;many-core&quot;	processors	
[10]
.</p>
<p>We	defined	the	Y86-64	instruction	set	by	starting	with	the	x86-64
instructions	and	simplifying	the	data	types,	address	modes,	and
instruction	encoding	considerably.	The	resulting	ISA	has	attributes	of	both
RISC	and	CISC	instruction	sets.	We	then	organized	the	processing
required	for	the	different	instructions	into	a	series	of	five	stages,	where
the	operations	at	each	stage	vary	according	to	the	instruction	being
executed.	From	this,	we	constructed	the	SEQ	processor,	in	which	an
entire	instruction	is	executed	every	clock	cycle	by	having	it	flow	through
all	five	stages.
Pipelining	improves	the	throughput	performance	of	a	system	by	letting
the	different	stages	operate	concurrently.	At	any	given	time,	multiple
operations	are	being	processed	by	the	different	stages.	In	introducing	this
concurrency,	we	must	be	careful	to	provide	the	same	program-level
behavior	as	would	a	sequential	execution	of	the	program.	We	introduced
pipelining	by	reordering	parts	of	SEQ	to	get	SEQ+	and	then	adding
pipeline	registers	to	create	the	PIPE—	pipeline.
Web	Aside	ARCH:HCL	
HCL
descriptions	of	Y86-64	processors
In	this	chapter,	we	have	looked	at	portions	of	the	HCL	code	for
several	simple	logic	designs	and	for	the	control	logic	for	Y86-64
processors	SEQ	and	PIPE.	For	reference,	we	provide
documentation	of	the	HCL	language	and	complete	HCL
descriptions	for	the	control	logic	of	the	two	processors.	Each	of
these	descriptions	requires	only	five	to	seven	pages	of	HCL	code,
and	it	is	worthwhile	to	study	them	in	their	entirety.</p>
<p>We	enhanced	the	pipeline	performance	by	adding	forwarding	logic	to
speed	the	sending	of	a	result	from	one	instruction	to	another.	Several
special	cases	require	additional	pipeline	control	logic	to	stall	or	cancel
some	of	the	pipeline	stages.
Our	design	included	rudimentary	mechanisms	to	handle	exceptions,
where	we	make	sure	that	only	instructions	up	to	the	excepting	instruction
affect	the	programmer-visible	state.	Implementing	a	complete	handling	of
exceptions	would	be	significantly	more	challenging.	Properly	handling
exceptions	gets	even	more	complex	in	systems	that	employ	greater
degrees	of	pipelining	and	parallelism.
In	this	chapter,	we	have	learned	several	important	lessons	about
processor	design:
Managing	complexity	is	a	top	priority.	
We	want	to	make	optimum
use	of	the	hardware	resources	to	get	maximum	performance	at
minimum	cost.	We	did	this	by	creating	a	very	simple	and	uniform
framework	for	processing	all	of	the	different	instruction	types.	With
this	framework,	we	could	share	the	hardware	units	among	the	logic
for	processing	the	different	instruction	types.
We	do	not	need	to	implement	the	ISA	directly.	
A	direct
implementation	of	the	ISA	would	imply	a	very	sequential	design.	To
achieve	higher	performance,	we	want	to	exploit	the	ability	in	hardware
to	perform	many	operations	simultaneously.	This	led	to	the	use	of	a
pipelined	design.	By	careful	design	and	analysis,	we	can	handle	the
various	pipeline	hazards,	so	that	the	overall	effect	of	running	a
program	exactly	matches	what	would	be	obtained	with	the	ISA	model.
Hardware	designers	must	be	meticulous.	
Once	a	chip	has	been
fabricated,	it	is	nearly	impossible	to	correct	any	errors.	It	is	very</p>
<p>important	to	get	the	design	right	on	the	first	try.	This	means	carefully
analyzing	different	instruction	types	and	combinations,	even	ones	that
do	not	seem	to	make	sense,	such	as	popping	to	the	stack	pointer.
Designs	must	be	thoroughly	tested	with	systematic	simulation	test
programs.	In	developing	the	control	logic	for	PIPE,	our	design	had	a
subtle	bug	that	was	uncovered	only	after	a	careful	and	systematic
analysis	of	control	combinations.
4.6.1	
Y86-64	Simulators
The	lab	materials	for	this	chapter	include	simulators	for	the	SEQ	and
PIPE	processors.	Each	simulator	has	two	versions:
The	GUI	(graphic	user	interface)	version	displays	the	memory,
program	code,	and	processor	state	in	graphic	windows.	This	provides
a	way	to	readily	see	how	the	instructions	flow	through	the	processors.
The	control	panel	also	allows	you	to	reset,	single-step,	or	run	the
simulator	interactively.
The	text	version	runs	the	same	simulator,	but	it	only	displays
information	by	printing	to	the	terminal.	This	version	is	not	as	useful	for
debugging,	but	it	allows	automated	testing	of	the	processor.
The	control	logic	for	the	simulators	is	generated	by	translating	the	HCL
declarations	of	the	logic	blocks	into	C	code.	This	code	is	then	compiled
and	linked	with	the	rest	of	the	simulation	code.	This	combination	makes	it
possible	for	you	to	test	out	variants	of	the	original	designs	using	the
simulators.	Testing	scripts	are	also	available	that	thoroughly	exercise	the
different	instructions	and	the	different	hazard	possibilities.</p>
<p>Bibliographic	Notes
For	those	interested	in	learning	more	about	logic	design,	the	Katz	and
Borriello	logic	design	textbook	
[58]
is	a	standard	introductory	text,
emphasizing	the	use	of	hardware	description	languages.	Hennessy	and
Patterson's	computer	architecture	textbook	
[46]
provides	extensive
coverage	of	processor	design,	including	both	simple	pipelines,	such	as
the	one	we	have	presented	here,	and	advanced	processors	that	execute
more	instructions	in	parallel.	Shriver	and	Smith	
[101]
give	a	very
thorough	presentation	of	an	Intel-compatible	x86-64	processor
manufactured	by	AMD.</p>
<p>Homework	Problems
4.45
In	
Section	
3.4.2
,	the	x86-64	
instruction	was	described	as
decrementing	the	stack	pointer	and	then	storing	the	register	at	the	stack
pointer	location.	So,	if	we	had	an	instruction	of	the	form	</p>
<p>REG
,	for
some	register	
REG
,	it	would	be	equivalent	to	the	code	sequence
A
.	
In	light	of	analysis	done	in	
Practice	Problem	
4.7
,	does	this
code	sequence	correctly	describe	the	behavior	of	the	instruction</p>
<p>?	Explain.
B
.	
How	could	you	rewrite	the	code	sequence	so	that	it	correctly
describes	both	the	cases	where	
REG
is	
as	well	as	any	other
register?
4.46</p>
<p>In	
Section	
3.4.2
,	the	x86-64	
instruction	was	described	as
copying	the	result	from	the	top	of	the	stack	to	the	destination	register	and
then	incrementing	the	stack	pointer.	So,	if	we	had	an	instruction	of	the
form	</p>
<p>REG
,	it	would	be	equivalent	to	the	code	sequence
A
.	
In	light	of	analysis	done	in	
Practice	Problem	
4.8
,	does	this
code	sequence	correctly	describe	the	behavior	of	the	instruction</p>
<p>?	Explain.
B
.	
How	could	you	rewrite	the	code	sequence	so	that	it	correctly
describes	both	the	cases	where	
REG
is	
as	well	as	any	other
register?
4.47
Your	assignment	will	be	to	write	a	Y86-64	program	to	perform	bubblesort.
For	reference,	the	following	C	function	implements	bubblesort	using	array
referencing:</p>
<p>A
.	
Write	and	test	a	C	version	that	references	the	array	elements	with
pointers,	rather	than	using	array	indexing.
B
.	
Write	and	test	a	Y86-64	program	consisting	of	the	function	and
test	code.	You	may	find	it	useful	to	pattern	your	implementation
after	x86-64	code	generated	by	compiling	your	C	code.	Although
pointer	comparisons	are	normally	done	using	unsigned	arithmetic,
you	can	use	signed	arithmetic	for	this	exercise.
4.48
Modify	the	code	you	wrote	for	
Problem	
4.47
to	implement	the	test	and
swap	in	the	bubblesort	function	(lines	6-11)	using	no	jumps	and	at	most
three	conditional	moves.
4.49</p>
<p>Modify	the	code	you	wrote	for	
Problem	
4.47
to	implement	the	test	and
swap	in	the	bubblesort	function	(lines	6-11)	using	no	jumps	and	just	one
conditional	move.
4.50
In	
Section	
3.6.8
,	we	saw	that	a	common	way	to	implement	
statements	is	to	create	a	set	of	code	blocks	and	then	index	those	blocks
using	a	jump	table.	Consider</p>
<p>Figure	
4.69	
Switch	statements	can	be	translated	into	Y86-64	code.
This	requires	implementation	of	a	jump	table.
the	C	code	shown	in	
Figure	
4.69
for	a	function	
,	along	with
associated	test	code.
Implement	
in	Y86-64	using	a	jump	table.	Although	the	Y86-64
instruction	set	does	not	include	an	indirect	jump	instruction,	you	can	get
the	same	effect	by	pushing	a	computed	address	onto	the	stack	and	then</p>
<p>executing	the	</p>
<p>instruction.	Implement	test	code	similar	to	what	is
shown	in	C	to	demonstrate	that	your	implementation	of	
will
handle	both	the	cases	handled	explicitly	as	well	as	those	that	trigger	the
case.
4.51
Practice	Problem	
4.3
introduced	the	
instruction	to	add
immediate	data	to	a	register.	Describe	the	computations	performed	to
implement	this	instruction.	Use	the	computations	for	
and	
(
Figure	
4.18
)	as	a	guide.
4.52
The	file	
contains	the	HCL	description	for	SEQ,	along	with
the	declaration	of	a	constant	
having	hexadecimal	value	C,	the
instruction	code	for	
.	Modify	the	HCL	descriptions	of	the	control
logic	blocks	to	implement	the	
instruction,	as	described	in	
Practice
Problem	
4.3
and	
Problem	
4.51
.	See	the	lab	material	for	directions
on	how	to	generate	a	simulator	for	your	solution	and	how	to	test	it.
4.53</p>
<p>Suppose	we	wanted	to	create	a	lower-cost	pipelined	processor	based	on
the	structure	we	devised	for	PIPE—	(
Figure	
4.41
),	without	any
bypassing.	This	design	would	handle	all	data	dependencies	by	stalling
until	the	instruction	generating	a	needed	value	has	passed	through	the
write-back	stage.
The	file	
contains	a	modified	version	of	the	HCL	code	for
PIPE	in	which	the	bypassing	logic	has	been	disabled.	That	is,	the	signals
and	
are	simply	declared	as	follows:
Modify	the	pipeline	control	logic	at	the	end	of	this	file	so	that	it	correctly
handles	all	possible	control	and	data	hazards.	As	part	of	your	design
effort,	you	should	analyze	the	different	combinations	of	control	cases,	as
we	did	in	the	design	of	the	pipeline	control	logic	for	PIPE.	You	will	find
that	many	different	combinations	can	occur,	since	many	more	conditions
require	the	pipeline	to	stall.	Make	sure	your	control	logic	handles	each
combination	correctly.	See	the	lab	material	for	directions	on	how	to
generate	a	simulator	for	your	solution	and	how	to	test	it.</p>
<p>4.54
The	
contains	a	copy	of	the	PIPE	HCL	description,
along	with	a	declaration	of	the	constant	value	
.	Modify	this	file	to
implement	the	
instruction,	as	described	in	
Practice	Problem	
4.3
and	
Problem	
4.51
.	See	the	lab	
material	for	directions	on	how	to
generate	a	simulator	for	your	solution	and	how	to	test	it.
4.55
The	file	
contains	a	copy	of	the	HCL	code	for	PIPE,	plus	a
declaration	of	the	constant	
with	value	0,	the	function	code	for	an
unconditional	jump	instruction.	Modify	the	branch	prediction	logic	so	that
it	predicts	conditional	jumps	as	being	not	taken	while	continuing	to	predict
unconditional	jumps	and	
as	being	taken.	You	will	need	to	devise	a
way	to	get	valC,	the	jump	target	address,	to	pipeline	register	M	to	recover
from	mispredicted	branches.	See	the	lab	material	for	directions	on	how	to
generate	a	simulator	for	your	solution	and	how	to	test	it.
4.56
The	file	
contains	a	copy	of	the	HCL	code	for	PIPE,	plus	a
declaration	of	the	constant	
with	value	0,	the	function	code	for	an
unconditional	jump	instruction.	Modify	the	branch	prediction	logic	so	that</p>
<p>it	predicts	conditional	jumps	as	being	taken	when	
(backward
branch)	and	as	being	not	taken	when	
(forward	branch).
(Since	Y86-64	does	not	support	unsigned	arithmetic,	you	should
implement	this	test	using	a	signed	comparison.)	Continue	to	predict
unconditional	jumps	and	
as	being	taken.	You	will	need	to	devise	a
way	to	get	both	
and	
to	pipeline	register	M	to	recover	from
mispredicted	branches.	See	the	lab	material	for	directions	on	how	to
generate	a	simulator	for	your	solution	and	how	to	test	it.
4.57
In	our	design	of	PIPE,	we	generate	a	stall	whenever	one	instruction
performs	a	
load
,	reading	a	value	from	memory	into	a	register,	and	the
next	instruction	has	this	register	as	a	source	operand.	When	the	source
gets	used	in	the	execute	stage,	this	stalling	is	the	only	way	to	avoid	a
hazard.	For	cases	where	the	second	instruction	stores	the	source
operand	to	memory,	such	as	with	an	
or	
instruction,	this
stalling	is	not	necessary.	Consider	the	following	code	examples:</p>
<p>In	lines	1	and	2,	the	
instruction	reads	a	value	from	memory	into
,	and	the	
instruction	then	pushes	this	value	onto	the	stack.
Our	design	for	PIPE	would	stall	the	
instruction	to	avoid	a	load/use
hazard.	Observe,	however,	that	the	value	of	
is	not	required	by	the
instruction	until	it	reaches	the	memory	stage.	We	can	add	an
additional	bypass	path,	as	diagrammed	in	
Figure	
4.70
,	to	forward	the
memory	output	(signal	m_valM)	to	the	valA	field	in	pipeline	register	M.	On
the	next	clock	cycle,	this	forwarded	value	can	then	be	written	to	memory.
This	technique	is	known	as	
load	forwarding.
Note	that	the	second	example	(lines	4	and	5)	in	the	code	sequence
above	cannot	make	use	of	load	forwarding.	The	value	loaded	by	the	
instruction	is</p>
<p>Figure	
4.70	
Execute	and	memory	stages	capable	of	load	forwarding.
By	adding	a	bypass	path	from	the	memory	output	to	the	source	of	valA	in
pipeline	register	M,	we	can	use	forwarding	rather	than	stalling	for	one
form	of	load/use	hazard.	This	is	the	subject	of	
Problem	
4.57
.
used	as	part	of	the	address	computation	by	the	next	instruction,	and	this
value	is	required	in	the	execute	stage	rather	than	the	memory	stage.
A
.	
Write	a	logic	formula	describing	the	detection	condition	for	a
load/use	hazard,	similar	to	the	one	given	in	
Figure	
4.64
,	except
that	it	will	not	cause	a	stall	in	cases	where	load	forwarding	can	be
used.
B
.	
The	file	
contains	a	modified	version	of	the	control
logic	for	PIPE.	It	contains	the	definition	of	a	signal	
to
implement	the	block	labeled	&quot;Fwd	A&quot;	in	
Figure	
4.70
.	It	also	has
the	conditions	for	a	load/use	hazard	in	the	pipeline	control	logic	set
to	zero,	and	so	the	pipeline	control	logic	will	not	detect	any	forms
of	load/use	hazards.	Modify	this	HCL	description	to	implement
load	forwarding.	See	the	lab	material	for	directions	on	how	to
generate	a	simulator	for	your	solution	and	how	to	test	it.
4.58
Our	pipelined	design	is	a	bit	unrealistic	in	that	we	have	two	write	ports	for
the	register	file,	but	only	the	
instruction	requires	two	simultaneous
writes	to	the	register	file.	The	other	instructions	could	therefore	use	a</p>
<p>single	write	port,	sharing	this	for	writing	
and	
.	The	following
figure	shows	a	modified	version	of	the	write-back	logic,	in	which	we
merge	the	write-back	register	IDs	(
and	
)	into	a	single	signal
and	the	write-back	values	(
and	
)	into	a	single	signal
:
The	logic	for	performing	the	merges	is	written	in	HCL	as	follows:</p>
<p>The	control	for	these	multiplexors	is	determined	by	
—when	it
indicates	there	is	some	register,	then	it	selects	the	value	for	port	E,	and
otherwise	it	selects	the	value	for	port	M.
In	the	simulation	model,	we	can	then	disable	register	port	M,	as	shown	by
the	following	HCL	code:
The	challenge	then	becomes	to	devise	a	way	to	handle	
.	One
method	is	to	use	the	control	logic	to	dynamically	process	the	instruction
rA	so	that	it	has	the	same	effect	as	the	two-instruction	sequence
(See	
Practice	Problem	
4.3
for	a	description	of	the	
instruction.)
Note	the	ordering	of	the	two	instructions	to	make	sure	</p>
<pre><code>works
</code></pre>
<p>properly.	You	can	do	this	by	having	the	logic	in	the	decode	stage	treat
the	same	as	it	would	the	
listed	above,	except	that	it	predicts
the	next	PC	to	be	equal	to	the	current	PC.	On	the	next	cycle,	the	</p>
<p>instruction	is	refetched,	but	the	instruction	code	is	converted	to	a	special
value	
.	This	is	treated	as	a	special	instruction	that	has	the	same
behavior	as	the	
instruction	listed	above.
The	file	
contains	the	modified	write	port	logic	described
above.	It	contains	a	declaration	of	the	constant	
having
hexadecimal	value	E.	It	also	contains	the	definition	of	a	signal	f_icode
that	generates	the	icode	field	for	pipeline	register	D.	This	definition	can
be	modified	to	insert	the	instruction	code	
the	second	time	the	
instruction	is	fetched.	The	HCL	file	also	contains	a	declaration	of	the
signal	f_pc,	the	value	of	the	program	counter	generated	in	the	fetch	stage
by	the	block	labeled	&quot;Select	PC&quot;	(
Figure	
4.57
).
Modify	the	control	logic	in	this	file	to	process	
instructions	in	the
manner	we	have	described.	See	the	lab	material	for	directions	on	how	to
generate	a	simulator	for	your	solution	and	how	to	test	it.
4.59
Compare	the	performance	of	the	three	versions	of	bubblesort	(
Problems
4.47
,	
4.48
,	and	
4.49
).	Explain	why	one	version	performs	better
than	the	other.</p>
<p>Solutions	to	Practice	Problems
Solution	to	Problem	
4.1	
(page
360
)
Encoding	instructions	by	hand	is	rather	tedious,	but	it	will	solidify	your
understanding	of	the	idea	that	assembly	code	gets	turned	into	byte
sequences	by	the	assembler.	In	the	following	output	from	our	Y86-64
assembler,	each	line	shows	an	address	and	a	byte	sequence	that	starts
at	that	address:
Several	features	of	this	encoding	are	worth	noting:</p>
<p>Decimal	15	(line	2)	has	hex	representation	
.
Writing	the	bytes	in	reverse	order	gives	
.
Decimal	-3	(line	5)	has	hex	representation	
.	Writing
the	bytes	in	reverse	order	gives	
.
The	code	starts	at	address	
.	The	first	instruction	requires	10
bytes,	while	the	second	requires	2.	Thus,	the	loop	target	will	be
.	Writing	these	bytes	in	reverse	order	gives	
.
Solution	to	Problem	
4.2	
(page
360
)
Decoding	a	byte	sequence	by	hand	helps	you	understand	the	task	faced
by	a	processor.	It	must	read	byte	sequences	and	determine	what
instructions	are	to	be	executed.	In	the	following,	we	show	the	assembly
code	used	to	generate	each	of	the	byte	sequences.	To	the	left	of	the
assembly	code,	you	can	see	the	address	and	byte	sequence	for	each
instruction.
A
.	
Some	operations	with	immediate	data	and	address	displacements:</p>
<p>B
.	
Code	including	a	function	call:
C
.	
Code	containing	illegal	instruction	specifier	byte	
:
D
.	
Code	containing	a	jump	operation:
E
.	
Code	containing	an	invalid	second	byte	in	a	
instruction:</p>
<p>Solution	to	Problem	
4.3	
(page
369
)
Using	the	
instruction,	we	can	rewrite	the	
function	as</p>
<p>Solution	to	Problem	
4.4	
(page
370
)
G
CC
,	running	on	an	x86-64	machine,	produces	the	following	code	for
:</p>
<p>This	can	easily	be	adapted	to	produce	Y86-64	code:
Solution	to	Problem	
4.5	
(page
370
)</p>
<p>This	problem	gives	you	a	chance	to	try	your	hand	at	writing	assembly
code.</p>
<p>Solution	to	Problem	
4.6	
(page
370
)
This	problem	gives	you	a	chance	to	try	your	hand	at	writing	assembly
code	with	conditional	moves.	We	show	only	the	code	for	the	loop.	The
rest	is	the	same	as	for	
Problem	
4.5
:
Solution	to	Problem	
4.7	
(page
370
)</p>
<h2>Although	it	is	hard	to	imagine	any	practical	use	for	this	particular
instruction,	it	is	important	when	designing	a	system	to	avoid	any
ambiguities	in	the	specification.	We	want	to	determine	a	reasonable
convention	for	the	instruction's	behavior	and	to	make	sure	each	of	our
implementations	adheres	to	this	convention.
The	
instruction	in	this	test	compares	the	starting	value	of	
to
the	value	pushed	onto	the	stack.	The	fact	that	the	result	of	this
subtraction	is	zero	implies	that	the	old	value	of	
gets	pushed.
Solution	to	Problem	
4.8	
(page
371
)
It	is	even	more	difficult	to	imagine	why	anyone	would	want	to	pop	to	the
stack	pointer.	Still,	we	should	decide	on	a	convention	and	stick	with	it.
This	code	sequence	pushes	
onto	the	stack,	pops	to	
,	and
returns	the	popped	value.	Since	the	result	equals	
,	we	can	deduce
that	
sets	the	stack	pointer	to	the	value	read	from	memory.	It	is
therefore	equivalent	to	the	instruction	
(
),
.
Solution	to	Problem	
4.9	
(page
374
)
The	
EXCLUSIVE</h2>
<p>OR</p>
<p>function	requires	that	the	2	bits	have	opposite	values:</p>
<h2>In	general,	the	signals	
and	
will	be	complements	of	each	other.
That	is,	one	will	equal	1	whenever	the	other	is	0.
Solution	to	Problem	
4.10	
(page
377
)
The	outputs	of	the	
EXCLUSIVE</h2>
<p>OR</p>
<p>circuits	will	be	the	complements	of	the	bit
equality	values.	Using	DeMorgan's	laws	(Web	Aside	
DATA
:
BOOL</p>
<p>on	page
52),	we	can	implement	
AND</p>
<p>using	
OR</p>
<p>and	
NOT
,	yielding	the	circuit	shown	in
Figure	
4.71
.
Solution	to	Problem	
4.11	
(page
379
)
We	can	see	that	the	second	part	of	the	case	expression	can	be	written	as
Since	the	first	line	will	detect	the	case	where	A	is	the	minimum	element,
the	second	line	need	only	determine	whether	B	or	C	is	minimum.</p>
<p>Solution	to	Problem	
4.12	
(page
380
)
This	design	is	a	variant	of	the	one	to	find	the	minimum	of	the	three	inputs:
Figure	
4.71	
Solution	for	
Problem	
4.10
.</p>
<p>Solution	to	Problem	
4.13	
(page
387
)
These	exercises	help	make	the	stage	computations	more	concrete.	We
can	see	from	the	object	code	that	this	instruction	is	located	at	address
.	It	consists	of	10	bytes,	with	the	first	two	being	
and	
.	The
last	8	bytes	are	a	byte-reversed	version	of	
(decimal
128).
Stage
Generic	
V,	rB
Specific	
$128,	
Fetch
icode:ifun	←	M
[PC]
icode:ifun	←	M
rA:rB	←	MfiTC	+	1]
rA:rB	←	M
[
valC	←	M
[PC	+	2]
valC	←	M
valP	←	PC	+	10
valP	←	
Decode
Execute
valE	←	0	+	valC
valE	←	
Memory
Write	back
R[rB]	←	valE
R[
]	←	valE=128
PC	update
PC	←	valP
PC	←	valP	=	
This	instruction	sets	register	
to	128	and	increments	the	PC	by	10.
1
1
1
8
8</p>
<p>Solution	to	Problem	
4.14	
(page
390
)
We	can	see	that	the	instruction	is	located	at	address	
and	consists
of	2	bytes	with	values	
and	
.	Register	
was	set	to	120	by
the	
instruction	(line	6),	which	also	stored	9	at	this	memory	location.
Stage
Generic	
rA
Specific	
Fetch
icode:ifun	←	M
[PC]
rA:rB	←	M
[PC	+	1]
icode:ifun	←	M
rA:rB	←	M
valP	←	PC	+	2
valP	←	
Decode
valA	←	R[
]
valB	←	R[
]
valA	←	R[
]	=	</p>
<p>valB	←	R[
]	=	
Execute
valE	←	valB	+	8
valE	←	
Memory
valM	←	M
[valA]
valM	←	M
Write	back
R[
]	←	valE	
R[rA]	←	valM
R[
]	←	128	
R[
]	←	
PC	update
PC	←	valP
PC	←	
The	instruction	sets	
to	9,	sets	
to	128,	and	increments	the	PC
by	2.
1
1
1
1
8
8</p>
<p>Solution	to	Problem	
4.15	
(page
391
)
Tracing	the	steps	listed	in	
Figure	
4.20
with	rA	equal	to	
,	we	can
see	that	in	the	memory	stage	the	instruction	will	store	valA,	the	original
value	of	the	stack	pointer,	to	memory,	just	as	we	found	for	x86-64.
Solution	to	Problem	
4.16	
(page
392
)
Tracing	the	steps	listed	in	
Figure	
4.20
with	rA	equal	to	
,	we	can
see	that	both	of	the	write-back	operations	will	update	
.	Since	the	one
writing	valM	would	occur	last,	the	net	effect	of	the	instruction	will	be	to
write	the	value	read	from	memory	to	
,	just	as	we	saw	for	x86-64.
Solution	to	Problem	
4.17	
(page
393
)
Implementing	conditional	moves	requires	only	minor	changes	from
register-to-register	moves.	We	simply	condition	the	write-back	step	on
the	outcome	of	the	conditional	test:</p>
<p>Stage
rA,	rB
Fetch
icode:ifun	←	M
[PC]
rA:rB	←	M
[PC	+	1]
valP	←	PC	+	2
Decode
valA	←	R[rA]
Execute
valE	←	0	+	valA
Cnd	←	Cond(CC,	ifun)
Memory
Write	back
(Cnd)	R[rB]	←	valE
PC	update
PC	←	valP
Solution	to	Problem	
4.18	
(page
394
)
We	can	see	that	this	instruction	is	located	at	address	
and	is	9
bytes	long.	The	first	byte	has	value	
,	while	the	last	8	bytes	are	a
byte-reversed	version	of	
,	the	call	target.	The	stack
pointer	was	set	to	128	by	the	
instruction	(line	7).
Stage
Generic	
Dest
Specific	
1
1</p>
<p>Fetch
icode:ifun	←	M
[PC]
icode:ifun	←	M
valC	←	M
[PC	+	1]
valP	←	PC	+	9
valC	←	M
valP	←	
Decode
valB	←	R[
]
valB	←	R[
]	=	
Execute
valE	←	valB	+	-8
valE	←	
Memory
M
[valE]	←	valP
M
[
]	←	
Write	back
R[
]	←	valE
R[
]	←	
PC	update
PC	←	valC
PC	←	
The	effect	of	this	instruction	is	to	set	
to	120,	to	store	
(the
return	address)	at	this	memory	address,	and	to	set	the	PC	to	
(the
call	target).
Solution	to	Problem	
4.19	
(page
406
)
All	of	the	HCL	code	in	this	and	other	practice	problems	is	straightforward,
but	trying	to	generate	it	yourself	will	help	you	think	about	the	different
instructions	and	how	they	are	processed.	For	this	problem,	we	can	simply
look	at	the	set	of	Y86-64	instructions	(
Figure	
4.2
)	and	determine	which
have	a	constant	field.
1
1
8
8
8
8</p>
<p>Solution	to	Problem	
4.20	
(page
407
)
This	code	is	similar	to	the	code	for	srcA.
Solution	to	Problem	
4.21	
(page
408
)
This	code	is	similar	to	the	code	for	dstE.</p>
<p>Solution	to	Problem	
4.22	
(page
408
)
As	we	found	in	
Practice	Problem	
4.16
,	we	want	the	write	via	the	M
port	to	take	priority	over	the	write	via	the	E	port	in	order	to	store	the	value
read	from	memory	into	
.
Solution	to	Problem	
4.23	
(page
409
)
This	code	is	similar	to	the	code	for	aluA.</p>
<p>Solution	to	Problem	
4.24	
(page
409
)
Implementing	conditional	moves	is	surprisingly	simple:	we	disable	writing
to	the	register	file	by	setting	the	destination	register	to	
when	the
condition	does	not	hold.
Solution	to	Problem	
4.25	
(page
410
)</p>
<p>This	code	is	similar	to	the	code	for	mem_addr.
Solution	to	Problem	
4.26	
(page
410
)
This	code	is	similar	to	the	code	for	mem_read.
Solution	to	Problem	
4.27	
(page
411
)</p>
<p>Computing	the	Stat	field	requires	collecting	status	information	from
several	stages:
Solution	to	Problem	
4.28	
(page
417
)
This	problem	is	an	interesting	exercise	in	trying	to	find	the	optimal
balance	among	a	set	of	partitions.	It	provides	a	number	of	opportunities
to	compute	throughputs	and	latencies	in	pipelines.
A
.	
For	a	two-stage	pipeline,	the	best	partition	would	be	to	have
blocks	A,	B,	and	C	in	the	first	stage	and	D,	E,	and	F	in	the	second.
The	first	stage	has	a	delay	of	170	ps,	giving	a	total	cycle	time	of
170	+	20	=	190	ps.	We	therefore	have	a	throughput	of	5.26	GIPS
and	a	latency	of	380	ps.</p>
<p>B
.	
For	a	three-stage	pipeline,	we	should	have	blocks	A	and	B	in	the
first	stage,	blocks	C	and	D	in	the	second,	and	blocks	E	and	F	in
the	third.	The	first	two	stages	have	a	delay	of	110	ps,	giving	a	total
cycle	time	of	130	ps	and	a	throughput	of	7.69	GIPS.	The	latency	is
390	ps.
C
.	
For	a	four-stage	pipeline,	we	should	have	block	A	in	the	first	stage,
blocks	B	and	C	in	the	second,	block	D	in	the	third,	and	blocks	E
and	F	in	the	fourth.	The	second	stage	requires	90	ps,	giving	a	total
cycle	time	of	110	ps	and	a	throughput	of	9.09	GIPS.	The	latency	is
440	ps.
D
.	
The	optimal	design	would	be	a	five-stage	pipeline,	with	each	block
in	its	own	stage,	except	that	the	fifth	stage	has	blocks	E	and	F	The
cycle	time	is	80	+	20	=	100	ps,	for	a	throughput	of	around	10.00
GIPS	and	a	latency	of	
500	ps.	Adding	more	stages	would	not	help,
since	we	cannot	run	the	pipeline	any	faster	than	one	cycle	every
100	ps.
Solution	to	Problem	
4.29	
(page
418
)
Each	stage	would	have	combinational	logic	requiring	300/
k
ps	and	a
pipeline	register	requiring	20	ps.
A
.	
The	total	latency	would	be	300	+	20
k
ps,	while	the	throughput	(in
GIPS)	would	be
1
,
000
300
k</p>
<ul>
<li></li>
</ul>
<h1 id="20"><a class="header" href="#20">20</a></h1>
<p>1
,
000
k
300</p>
<ul>
<li></li>
</ul>
<p>20
k</p>
<p>B
.	
As	we	let	
k
go	to	infinity,	the	throughput	becomes	1,000/20	=	50
GIPS.	Of	course,	the	latency	would	approach	infinity	as	well.
This	exercise	quantifies	the	diminishing	returns	of	deep	pipelining.	As	we
try	to	subdivide	the	logic	into	many	stages,	the	latency	of	the	pipeline
registers	becomes	a	limiting	factor.
Solution	to	Problem	
4.30	
(page
449
)
This	code	is	very	similar	to	the	corresponding	code	for	SEQ,	except	that
we	cannot	yet	determine	whether	the	data	memory	will	generate	an	error
signal	for	this	instruction.
Solution	to	Problem	
4.31	
(page</p>
<p>449
)
This	code	simply	involves	prefixing	the	signal	names	in	the	code	for	SEQ
with	
and	
.
Solution	to	Problem	
4.32	
(page
452
)
The	
instruction	(line	5)	would	stall	for	one	cycle	due	to	a	load/use
hazard	caused	by	the	
instruction	(line	4).	As	it	enters	the	decode
stage,	the	
instruction	would	be	in	the	memory	stage,	giving	both
M_dstE	and	M_dstM	equal	to	
.	If	the	two	cases	were	reversed,	then
the	write	back	from	M_valE	would	take	priority,	causing	the	incremented
stack	pointer	to	be	passed	as	the	argument	
to	the	
instruction.	This
would	not	be	consistent	with	the	convention	for	handling	
determined	in	
Practice	Problem	
4.8
.</p>
<p>Solution	to	Problem	
4.33	
(page
452
)
This	problem	lets	you	experience	one	of	the	important	tasks	in	processor
design—devising	test	programs	for	a	new	processor.	In	general,	we
should	have	test	programs	that	will	exercise	all	of	the	different	hazard
possibilities	and	will	generate	incorrect	results	if	some	dependency	is	not
handled	properly.
For	this	example,	we	can	use	a	slightly	modified	version	of	the	program
shown	in	
Practice	Problem	
4.32
:
The	two	
instructions	will	cause	the	
instruction	to	be	in	the	write-
back	stage	when	the	
instruction	is	in	the	decode	stage.	If	the	two
forwarding	sources	in	the	write-back	stage	are	given	the	wrong	priority,
then	register	
will	be	set	to	the	incremented	program	counter	rather
than	the	value	read	from	memory.</p>
<p>Solution	to	Problem	
4.34	
(page
453
)
This	logic	only	needs	to	check	the	five	forwarding	sources:
Solution	to	Problem	
4.35	
(page
454
)
This	change	would	not	handle	the	case	where	a	conditional	move	fails	to
satisfy	the	condition,	and	therefore	sets	the	dstE	value	to	
.	The</p>
<p>resulting	value	could	get	forwarded	to	the	next	instruction,	even	though
the	conditional	transfer	does	not	occur.
This	code	initializes	register	
to	
.	The	conditional	data	transfer
does	not	take	place,	and	so	the	final	
instruction	should	double	the
value	in	
to	
.	With	the	altered	design,	however,	the	conditional
move	source	value	
gets	forwarded	into	ALU	input	valA,	while	input
valB	correctly	gets	operand	value	
.	These	inputs	get	added	to
produce	result	
.
Solution	to	Problem	
4.36	
(page
455
)
This	code	completes	the	computation	of	the	status	code	for	this
instruction.</p>
<p>Solution	to	Problem	
4.37	
(page
461
)
The	following	test	program	is	designed	to	set	up	control	combination	A
(
Figure	
4.67
)	and	detect	whether	something	goes	wrong:</p>
<p>This	program	is	designed	so	that	if	something	goes	wrong	(for	example,	if
the	
instruction	is	actually	executed),	then	the	program	will	execute
one	of	the	extra	
instructions	and	then	halt.	Thus,	an	error	in	the
pipeline	would	cause	some	register	to	be	updated	incorrectly.	This	code
illustrates	the	care	required	to	implement	a	test	program.	It	must	set	up	a
potential	error	condition	and	then	detect	whether	or	not	an	error	occurs.
Solution	to	Problem	
4.38	
(page
462
)
The	following	test	program	is	designed	to	set	up	control	combination	B
(
Figure	
4.67
).	The	simulator	will	detect	a	case	where	the	bubble	and
stall	control	signals	for	a	pipeline	register	are	both	set	to	zero,	and	so	our
test	program	need	only	set	up	the	combination	for	it	to	be	detected.	The
biggest	challenge	is	to	make	the	program	do	something	sensible	when
handled	correctly.</p>
<p>This	program	uses	two	initialized	words	in	memory.	The	first	word	(M
)
holds	the	address	of	the	second	(
--the	desired	stack	pointer).	The
second	word	holds	the	address	of	the	desired	return	point	for	the	
instruction.	The	program	loads	the	stack	pointer	into	
and	executes
the	
instruction.
Solution	to	Problem	
4.39	
(page
463
)
From	
Figure	
4.66
,	we	can	see	that	pipeline	register	D	must	be	stalled
for	a	load/use	hazard:</p>
<p>Solution	to	Problem	
4.40	
(page
464
)
From	
Figure	
4.66
,	we	can	see	that	pipeline	register	E	must	be	set	to
bubble	for	a	load/use	hazard	or	for	a	mispredicted	branch:
Solution	to	Problem	
4.41	
(page
464
)</p>
<p>This	control	requires	examining	the	code	of	the	executing	instruction	and
checking	for	exceptions	further	down	the	pipeline.
Solution	to	Problem	
4.42	
(page
464
)
Injecting	a	bubble	into	the	memory	stage	on	the	next	cycle	involves
checking	for	an	exception	in	either	the	memory	or	the	write-back	stage
during	the	current	cycle.
For	stalling	the	write-back	stage,	we	check	only	the	status	of	the
instruction	in	this	stage.	If	we	also	stalled	when	an	excepting	instruction</p>
<p>was	in	the	memory	stage,	then	this	instruction	would	not	be	able	to	enter
the	write-back	stage.
Solution	to	Problem	
4.43	
(page
468
)
We	would	then	have	a	misprediction	frequency	of	0.35,	giving	
mp
=	0.20
×	0.35	×	2	=	0.14,	giving	an	overall	CPI	of	1.25.	This	seems	like	a	fairly
marginal	gain,	but	it	would	be	worthwhile	if	the	cost	of	implementing	the
new	branch	prediction	strategy	were	not	too	high.
Solution	to	Problem	
4.44	
(page
468
)
This	simplified	analysis,	where	we	focus	on	the	inner	loop,	is	a	useful
way	to	estimate	program	performance.	As	long	as	the	array	is	sufficiently
large,	the	time	spent	in	other	parts	of	the	code	will	be	negligible.
A
.	
The	inner	loop	of	the	code	using	the	conditional	jump	has	11
instructions,	all	of	which	are	executed	when	the	array	element	is
zero	or	negative,	and	10	of	which	are	executed	when	the	array</p>
<p>element	is	positive.	The	average	is	10.5.	The	inner	loop	of	the
code	using	the	conditional	move	has	10	instructions,	all	of	which
are	executed	every	time.
B
.	
The	loop-closing	jump	will	be	predicted	correctly,	except	when	the
loop	terminates.	For	a	very	long	array,	this	one	misprediction	will
have	a	negligible	effect	on	the	performance.	The	only	other	source
of	bubbles	for	the	jump-based	code	is	the	conditional	jump,
depending	on	whether	or	not	the	array	element	is	positive.	This
will	cause	two	bubbles,	but	it	only	occurs	50%	of	the	time,	so	the
average	is	1.0.	There	are	no	bubbles	in	the	conditional	move
code.
C
.	
Our	conditional	jump	code	requires	an	average	of	10.5	+	1.0	=
11.5	cycles	per	array	element	(11	cycles	in	the	best	case	and	12
cycles	in	the	worst),	while	our	conditional	move	code	requires	10.0
cycles	in	all	cases.
Our	pipeline	has	a	branch	misprediction	penalty	of	only	two	cycles—far
better	than	those	for	the	deep	pipelines	of	higher-performance
processors.	As	a	result,	using	conditional	moves	does	not	affect	program
performance	very	much.</p>
<p>Chapter	
5	
Optimizing	Program
Performance
5.1	
Capabilities	and	Limitations	of	Optimizing	Compilers	
498
5.2	
Expressing	Program	Performance	
502
5.3	
Program	Example	
504
5.4	
Eliminating	Loop	Inefficiencies	
508
5.5	
Reducing	Procedure	Calls	
512
5.6	
Eliminating	Unneeded	Memory	References	
514
5.7	
Understanding	Modern	Processors	
517
5.8	
Loop	Unrolling	
531
5.9	
Enhancing	Parallelism	
536
5.10	
Summary	of	Results	for	Optimizing	Combining	Code	
547
5.11	
Some	Limiting	Factors	
548
5.12	
Understanding	Memory	Performance	
553
5.13	
Life	in	the	Real	World:	Performance	Improvement	Techniques
561</p>
<p>5.14	
Identifying	and	Eliminating	Performance	Bottlenecks	
562
5.15	
Summary</p>
<p>568
Bibliographic	Notes	
569
Homework	Problems	
570
Solutions	to	Practice	Problems	
573
The	primary	objective	in	writing	a	program	must	be
to	make	it	work	correctly	under	all	possible
conditions.	A	program	that	runs	fast	but	gives
incorrect	results	serves	no	useful	purpose.
Programmers	must	write	clear	and	concise	code,
not	only	so	that	they	can	make	sense	of	it,	but	also
so	that	others	can	read	and	understand	the	code
during	code	reviews	and	when	modifications	are
required	later.
On	the	other	hand,	there	are	many	occasions	when
making	a	program	run	fast	is	also	an	important
consideration.	If	a	program	must	process	video
frames	or	network	packets	in	real	time,	then	a	slow-
running	program	will	not	provide	the	needed
functionality.	When	a	computational	task	is	so
demanding	that	it	requires	days	or	weeks	to
execute,	then	making	it	run	just	20%	faster	can
have	significant	impact.	In	this	chapter,	we	will
explore	how	to	make	programs	run	faster	via
several	different	types	of	program	optimization.</p>
<p>Writing	an	efficient	program	requires	several	types
of	activities.	First,	we	must	select	an	appropriate	set
of	algorithms	and	data	structures.	Second,	we	must
write	source	code	that	the	compiler	can	effectively
optimize	to	turn	into	efficient	executable	code.	For
this	second	part,	it	is	important	to	understand	the
capabilities	and	limitations	of	optimizing	compilers.
Seemingly	minor	changes	in	how	a	program	is
written	can	make	large	differences	in	how	well	a
compiler	can	optimize	it.	Some	programming
languages	are	more	easily	optimized	than	others.
Some	features	of	C,	such	as	the	ability	to	perform
pointer	arithmetic	and	casting,	make	it	challenging
for	a	compiler	to	optimize.	Programmers	can	often
write	their	programs	in	ways	that	make	it	easier	for
compilers	to	generate	efficient	code.	A	third
technique	for	dealing	with	especially	demanding
computations	is	to	divide	a	task	into	portions	that
can	be	computed	in	parallel,	on	some	combination
of	multiple	cores	and	multiple	processors.	We	will
defer	this	aspect	of	performance	enhancement	to
Chapter	
12
.	Even	when	exploiting	parallelism,	it	is
important	that	each	parallel	thread	execute	with
maximum	performance,	and	so	the	material	of	this
chapter	remains	relevant	in	any	case.
In	approaching	program	development	and
optimization,	we	must	consider	how	the	code	will	be
used	and	what	critical	factors	affect	it.	In	general,</p>
<p>programmers	must	make	a	trade-off	between	how
easy	a	program	is	to	implement	and	maintain,	and
how	fast	it	runs.	At	an	algorithmic	level,	a	simple
insertion	sort	can	be	programmed	in	a	matter	of
minutes,	whereas	a	highly	efficient	sort	routine	may
take	a	day	or	more	to	implement	and	optimize.	At
the	coding	level,	many	low-level	optimizations	tend
to	reduce	code	readability	and	modularity,	making
the	programs	more	susceptible	to	bugs	and	more
difficult	to	modify	or	extend.	For	code	that	will	be
executed	repeatedly	in	a	performance-critical
environment,	extensive	optimization	may	be
appropriate.	One	challenge	is	to	maintain	some
degree	of	elegance	and	readability	in	the	code
despite	extensive	transformations.
We	describe	a	number	of	techniques	for	improving
code	performance.	Ideally,	a	compiler	would	be	able
to	take	whatever	code	we	write	and	generate	the
most	efficient	possible	machine-level	program
having	the	specified	behavior.	Modern	compilers
employ	sophisticated	forms	of	analysis	and
optimization,	and	they	keep	getting	better.	Even	the
best	compilers,	however,	can	be	thwarted	by
optimization	blockers
—aspects	of	the	program's
behavior	that	depend	strongly	on	the	execution
environment.	Programmers	must	assist	the	compiler
by	writing	code	that	can	be	optimized	readily.</p>
<p>The	first	step	in	optimizing	a	program	is	to	eliminate
unnecessary	work,	making	the	code	perform	its
intended	task	as	efficiently	as	possible.	This
includes	eliminating	unnecessary	function	calls,
conditional	tests,	and	memory	references.	These
optimizations	do	not	depend	on	any	specific
properties	of	the	target	machine.
To	maximize	the	performance	of	a	program,	both
the	programmer	and	the	compiler	require	a	model	of
the	target	machine,	specifying	how	instructions	are
processed	and	the	timing	characteristics	of	the
different	operations.	For	example,	the	compiler	must
know	timing	information	to	be	able	to	decide
whether	it	should	use	a	multiply	instruction	or	some
combination	of	shifts	and	adds.	Modern	computers
use	sophisticated	techniques	to	process	a	machine-
level	program,	executing	many	instructions	in
parallel	and	possibly	in	a	different	order	than	they
appear	in	the	program.	Programmers	must
understand	how	these	processors	work	to	be	able	to
tune	their	programs	for	maximum	speed.	We
present	a	high-level	model	of	such	a	machine	based
on	recent	designs	of	Intel	and	AMD	processors.	We
also	devise	a	graphical	
data-flow
notation	to
visualize	the	execution	of	instructions	by	the
processor,	with	which	we	can	predict	program
performance.</p>
<p>With	this	understanding	of	processor	operation,	we
can	take	a	second	step	in	program	optimization,
exploiting	the	capability	of	processors	to	provide
instruction-level	parallelism
,	executing	multiple
instructions	simultaneously.	We	cover	several
program	transformations	that	reduce	the	data
dependencies	between	different	parts	of	a
computation,	increasing	the	degree	of	parallelism
with	which	they	can	be	executed.
We	conclude	the	chapter	by	discussing	issues
related	to	optimizing	large	programs.	We	describe
the	use	of	code	
profilers
—tools	that	measure	the
performance	of	different	parts	of	a	program.	This
analysis	can	help	find	inefficiencies	in	the	code	and
identify	the	parts	of	the	program	on	which	we	should
focus	our	optimization	efforts.
In	this	presentation,	we	make	code	optimization	look
like	a	simple	linear	process	of	applying	a	series	of
transformations	to	the	code	in	a	particular	order.	In
fact,	the	task	is	not	nearly	so	straightforward.	A	fair
amount	of	trial-and-error	experimentation	is
required.	This	is	especially	true	as	we	approach	the
later	optimization	stages,	where	seemingly	small
changes	can	cause	major	changes	in	performance
and	some	very	promising	techniques	prove
ineffective.	As	we	will	see	in	the	examples	that
follow,	it	can	be	difficult	to	explain	exactly	why	a</p>
<p>particular	code	sequence	has	a	particular	execution
time.	Performance	can	depend	on	many	detailed
features	of	the	processor	design	for	which	we	have
relatively	little	documentation	or	understanding.	This
is	another	reason	to	try	a	number	of	different
variations	and	combinations	of	techniques.
Studying	the	assembly-code	representation	of	a
program	is	one	of	the	most	effective	means	for
gaining	an	understanding	of	the	compiler	and	how
the	generated	code	will	run.	A	good	strategy	is	to
start	by	looking	carefully	at	the	code	for	the	inner
loops,	identifying	performance-reducing	attributes
such	as	excessive	memory	references	and	poor	use
of	registers.	Starting	with	the	assembly	code,	we
can	also	predict	what	operations	will	be	performed
in	parallel	and	how	well	they	will	use	the	processor
resources.	As	we	will	see,	we	can	often	determine
the	time	(or	at	least	a	lower	bound	on	the	time)
required	to	execute	a	loop	by	identifying	
critical
paths
,	chains	of	data	dependencies	that	form	during
repeated	executions	of	a	loop.	We	can	then	go	back
and	modify	the	source	code	to	try	to	steer	the
compiler	toward	more	efficient	implementations.
Most	major	compilers,	including	
GCC
,	are	continually
being	updated	and	improved,	especially	in	terms	of
their	optimization	abilities.	One	useful	strategy	is	to
do	only	as	much	rewriting	of	a	program	as	is</p>
<p>required	to	get	it	to	the	point	where	the	compiler	can
then	generate	efficient	code.	By	this	means,	we
avoid	compromising	the	readability,	modularity,	and
portability	of	the	code	as	much	as	if	we	had	to	work
with	a	compiler	of	only	minimal	capabilities.	Again,	it
helps	to	iteratively	modify	the	code	and	analyze	its
performance	both	through	measurements	and	by
examining	the	generated	assembly	code.
To	novice	programmers,	it	might	seem	strange	to
keep	modifying	the	source	code	in	an	attempt	to
coax	the	compiler	into	generating	efficient	code,	but
this	is	indeed	how	many	high-performance
programs	are	written.	Compared	to	the	alternative	of
writing	code	in	assembly	language,	this	indirect
approach	has	the	advantage	that	the	resulting	code
will	still	run	on	other	machines,	although	perhaps
not	with	peak	performance.</p>
<p>5.1	
Capabilities	and	Limitations	of
Optimizing	Compilers
Modern	compilers	employ	sophisticated	algorithms	to	determine	what
values	are	computed	in	a	program	and	how	they	are	used.	They	can	then
exploit	opportunities	to	simplify	expressions,	to	use	a	single	computation
in	several	different	places,	and	to	reduce	the	number	of	times	a	given
computation	must	be	performed.	Most	compilers,	including	
GCC
,	provide
users	with	some	control	over	which	optimizations	they	apply.	As
discussed	in	
Chapter	
3
,	the	simplest	control	is	to	specify	the
optimization	level.
For	example,	invoking	
GCC</p>
<p>with	the	command-line
option	
specifies	that	it	should	apply	a	basic	set	of	optimizations.
Invoking	
GCC</p>
<p>with	option	
or	higher	(e.g.,	
or	
)	will	cause	it	to
apply	more	extensive	optimizations.	These	can	further	improve	program
performance,	but	they	may	expand	the	program	size	and	they	may	make
the	program	more	difficult	to	debug	using	standard	debugging	tools.	For
our	presentation,	we	will	mostly	consider	code	compiled	with	optimization
level	
,	even	though	level	
has	become	the	accepted	standard	for
most	software	projects	that	use	
GCC
.	We	purposely	limit	the	level	of
optimization	to	demonstrate	how	different	ways	of	writing	a	function	in	C
can	affect	the	efficiency	of	the	code	generated	by	a	compiler.	We	will	find
that	we	can	write	C	code	that,	when	compiled	just	with	option	
,	vastly
outperforms	a	more	naive	version	compiled	with	the	highest	possible
optimization	levels.</p>
<p>Compilers	must	be	careful	to	apply	only	
safe
optimizations	to	a	program,
meaning	that	the	resulting	program	will	have	the	exact	same	behavior	as
would	an	unoptimized	version	for	all	possible	cases	the	program	may
encounter,	up	to	the	limits	of	the	guarantees	provided	by	the	C	language
standards.	Constraining	
the	compiler	to	perform	only	safe	optimizations
eliminates	possible	sources	of	undesired	run-time	behavior,	but	it	also
means	that	the	programmer	must	make	more	of	an	effort	to	write
programs	in	a	way	that	the	compiler	can	then	transform	into	efficient
machine-level	code.	To	appreciate	the	challenges	of	deciding	which
program	transformations	are	safe	or	not,	consider	the	following	two
procedures:
At	first	glance,	both	procedures	seem	to	have	identical	behavior.	They
both	add	twice	the	value	stored	at	the	location	designated	by	pointer	
to	that	designated	by	pointer	
.	On	the	other	hand,	function	
is
more	efficient.	It	requires	only	three	memory	references	(read	
,	read</p>
<p>,	write	
),	whereas	
requires	six	(two	reads	of	
,	two
reads	of	
,	and	two	writes	of	
).	Hence,	if	a	compiler	is	given
procedure	
to	compile,	one	might	think	it	could	generate	more
efficient	code	based	on	the	computations	performed	by	
.
Consider,	however,	the	case	in	which	
and	
are	equal.	Then	function
will	perform	the	following	computations:
The	result	will	be	that	the	value	at	
will	be	increased	by	a	factor	of	4.
On	the	other	hand,	function	
will	perform	the	following
computation:
The	result	will	be	that	the	value	at	
will	be	increased	by	a	factor	of	3.
The	compiler	knows	nothing	about	how	
will	be	called,	and	so	it
must	assume	that	arguments	
and	
can	be	equal.	It	therefore	cannot
generate	code	in	the	style	of	
as	an	optimized	version	of
.</p>
<p>The	case	where	two	pointers	may	designate	the	same	memory	location
is	known	as	
memory	aliasing.
In	performing	only	safe	optimizations,	the
compiler	must	assume	that	different	pointers	may	be	aliased.	As	another
example,	for	a	program	with	pointer	variables	
and	
,	consider	the
following	code	sequence:
The	value	computed	for	
depends	on	whether	or	not	pointers	
and	
are	aliased—if	not,	it	will	equal	3,000,	but	if	so	it	will	equal	1,000.	This
leads	to	one	of	the	major	
optimization	blockers
,	aspects	of	programs	that
can	severely	limit	the	opportunities	for	a	compiler	to	generate	optimized
code.	If	a	compiler	cannot	determine	whether	or	not	two	pointers	may	be
aliased,	it	must	assume	that	either	case	is	possible,	limiting	the	set	of
possible	optimizations.
Practice	Problem	
5.1	
(solution	page
573
)
The	following	problem	illustrates	the	way	memory	aliasing	can
cause	unexpected	program	behavior.	Consider	the	following</p>
<p>procedure	to	swap	two	values:
If	this	procedure	is	called	with	
equal	to	
,	what	effect	will	it
have?
A	second	optimization	blocker	is	due	to	function	calls.	As	an	example,
consider	the	following	two	procedures:</p>
<p>It	might	seem	at	first	that	both	compute	the	same	result,	but	with	
calling	
only	once,	whereas	
calls	it	four	times.	It	is	tempting	to
generate	code	in	the	style	of	
when	given	
as	the	source.
Consider,	however,	the	following	code	for	f:
This	function	has	a	
side	effect
—it	modifies	some	part	of	the	global
program	state.	Changing	the	number	of	times	it	gets	called	changes	the
program	behavior.	In
Aside	
Optimizing	function	calls	by	inline
substitution
Code	involving	function	calls	can	be	optimized	by	a	process
known	as	
inline	substitution
(or	simply	&quot;inlining&quot;),	where	the
function	call	is	replaced	by	the	code	for	the	body	of	the	function.
For	example,	we	can	expand	the	code	for	
by	substituting
four	instantiations	of	function	
:</p>
<p>This	transformation	both	reduces	the	overhead	of	the	function
calls	and	allows	further	optimization	of	the	expanded	code.	For
example,	the	compiler	can	consolidate	the	updates	of	global
variable	
in	
in	to	generate	an	optimized	version	of
the	function:
This	code	faithfully	reproduces	the	behavior	of	
for	this
particular	definition	of	function	
.</p>
<p>Recent	versions	of	
GCC</p>
<p>attempt	this	form	of	optimization,	either
when	directed	to	with	the	command-line	option	
or	for
optimization	level	
and	higher.	Unfortunately,	
GCC</p>
<p>only	attempts
inlining	for	functions	defined	within	a	single	file.	That	means	it	will
not	be	applied	in	the	common	case	where	a	set	of	library	functions
is	defined	in	one	file	but	invoked	by	functions	in	other	files.
There	are	times	when	it	is	best	to	prevent	a	compiler	from
performing	inline	substitution.	One	is	when	the	code	will	be
evaluated	using	a	symbolic	debugger,	such	as	
GDB
,	as	described
in	
Section	
3.10.2
.	If	a	function	call	has	been	optimized	away
via	inline	substitution,	then	any	attempt	to	trace	or	set	a	breakpoint
for	that	call	will	fail.	The	second	is	when	evaluating	the
performance	of	a	program	by	profiling,	as	is	discussed	in	
Section
5.14.1
.	Calls	to	functions	that	have	been	eliminated	by	inline
substitution	will	not	be	profiled	correctly.
particular,	a	call	to	
would	return	0	+	1	+	2	+	3	=	6,	whereas	a	call	to
would	return	4	·	0	=	0,	assuming	both	started	with	global	variable
counter	set	to	zero.
Most	compilers	do	not	try	to	determine	whether	a	function	is	free	of	side
effects	and	hence	is	a	candidate	for	optimizations	such	as	those
attempted	in	
.	Instead,	the	compiler	assumes	the	worst	case	and
leaves	function	calls	intact.
Among	compilers,	
GCC</p>
<p>is	considered	adequate,	but	not	exceptional,	in
terms	of	its	optimization	capabilities.	It	performs	basic	optimizations,	but	it
does	not	perform	the	radical	transformations	on	programs	that	more
&quot;aggressive&quot;	compilers	do.	As	a	consequence,	programmers	using	
GCC</p>
<p>must	put	more	effort	into	writing	programs	in	a	way	that	simplifies	the
compiler's	task	of	generating	efficient	code.</p>
<p>5.2	
Expressing	Program
Performance
We	introduce	the	metric	
cycles	per	element
,	abbreviated	CPE,	to	express
program	performance	in	a	way	that	can	guide	us	in	improving	the	code.
CPE	measurements	help	us	understand	the	loop	performance	of	an
iterative	program	at	a	detailed	level.	It	is	appropriate	for	programs	that
perform	a	repetitive	computation,	such	as	processing	the	pixels	in	an
image	or	computing	the	elements	in	a	matrix	product.
The	sequencing	of	activities	by	a	processor	is	controlled	by	a	clock
providing	a	regular	signal	of	some	frequency,	usually	expressed	in
gigahertz
(GHz),	billions	of	cycles	per	second.	For	example,	when
product	literature	characterizes	a	system	as	a	&quot;4	GHz&quot;	processor,	it
means	that	the	processor	clock	runs	at	4.0	×	10
cycles	per	second.	The
time	required	for	each	clock	cycle	is	given	by	the	reciprocal	of	the	clock
frequency.	These	typically	are	expressed	in	
nanoseconds
(1	nanosecond
is	10
seconds)	or	
picoseconds
(1	picosecond	is	10
seconds).	For
example,	the	period	of	a	4	GHz	clock	can	be	expressed	as	either	0.25
nanoseconds	or	250	picoseconds.	From	a	programmer's	perspective,	it	is
more	instructive	to	express	measurements	in	clock	cycles	rather	than
nanoseconds	or	picoseconds.	That	way,	the	measurements	express	how
many	instructions	are	being	executed	rather	than	how	fast	the	clock	runs.
Many	procedures	contain	a	loop	that	iterates	over	a	set	of	elements.	For
example,	functions	
and	
in	
Figure	
5.1
both	compute	the
−9
−9
−12</p>
<h1>prefix	sum
of	a	vector	of	length	
n
.	For	a	vector	
,	the
prefix	sum	
is	defined	as
Function	
computes	one	element	of	the	result	vector	per	iteration.
Function	
uses	a	technique	known	as	
loop	unrolling
to	compute	two
elements	per	iteration.	We	will	explore	the	benefits	of	loop	unrolling	later
in	this	chapter.	(See	Problems	5.11,5.12,	and	5.19	for	more	about
analyzing	and	optimizing	the	prefix-sum	computation.)
The	time	required	by	such	a	procedure	can	be	characterized	as	a
constant	plus	a	factor	proportional	to	the	number	of	elements	processed.
For	example,	
Figure	
5.2
shows	a	plot	of	the	number	of	clock	cycles
required	by	the	two	functions	for	a	range	of	values	of	
n
.	Using	a	
least
squares	fit
,	we	find	that	the	run	times	(in	clock	cycles)	for	
and
can	be	approximated	by	the	equations	368	+	9.0
n
and	368	+	6.0
n
,
respectively.	These	equations	indicate	an	overhead	of	368	cycles	due	to
the	timing	code	and	to	initiate	the	procedure,	set	up	the	loop,	and
complete	the
a
→</h1>
<h1>〈
a
0
,
a
1
,
…
 
,
a
n
−
1
〉
p
→</h1>
<p>〈
p
0
,
p
1
,
…
 
,
p
n
−
1
〉</p>
<h1>p
0</h1>
<h1>a
0
p
i</h1>
<h2>p
i</h2>
<p>1</p>
<ul>
<li></li>
</ul>
<p>a
i
,
 
1
&lt;
i
&lt;
n
(5.1)</p>
<p>Figure	
5.1	
Prefix-sum	functions.
These	functions	provide	examples	for	how	we	express	program
performance.</p>
<p>Figure	
5.2	
Performance	of	prefix-sum	functions.
The	slope	of	the	lines	indicates	the	number	of	clock	cycles	per	element
(CPE).
Aside	
What	is	a	least	squares	fit?
For	a	set	of	data	points	(
x
,	
y
),.	.	.	(
x
,	
y
),	we	often	try	to	draw	a
line	that	best	approximates	the	X-Y	trend	represented	by	these
data.	With	a	least	squares	fit,	we	look	for	a	line	of	the	form	
y	=	mx</p>
<ul>
<li></li>
</ul>
<h1>b
that	minimizes	the	following	error	measure:
An	algorithm	for	computing	
m
and	
b
can	be	derived	by	finding	the
derivatives	of	
E(m,	b)
with	respect	to	
m
and	
b
and	setting	them	to
0.
procedure,	plus	a	linear	factor	of	6.0	or	9.0	cycles	per	element.	For	large
values	of	
n
(say,	greater	than	200),	the	run	times	will	be	dominated	by	the
linear	factors.	We	refer	to	the	coefficients	in	these	terms	as	the	effective
number	of	cycles	per	element.	We	prefer	measuring	the	number	of	cycles
1
1
n
n
E
(
m
,
b
)</h1>
<h1>∑
i</h1>
<p>1
,
n
(
m
x
i</p>
<ul>
<li></li>
</ul>
<h2 id="b"><a class="header" href="#b">b</a></h2>
<p>y
i
)
2</p>
<p>per	
element
rather	than	the	number	of	cycles	per	
iteration
,	because
techniques	such	as	loop	unrolling	allow	us	to	use	fewer	iterations	to
complete	the	computation,	but	our	ultimate	concern	is	how	fast	the
procedure	will	run	for	a	given	vector	length.	We	focus	our	efforts	on
minimizing	the	CPE	for	our	computations.	By	this	measure,	
,	with	a
CPE	of	6.0,	is	superior	to	
,	with	a	CPE	of	9.0.
Practice	Problem	
5.2	
(solution	page
573
)
Later	in	this	chapter	we	will	start	with	a	single	function	and
generate	many	different	variants	that	preserve	the	function's
behavior,	but	with	different	performance	characteristics.	For	three
of	these	variants,	we	found	that	the	run	times	(in	clock	cycles)	can
be	approximated	by	the	following	functions:
Version	1:	60	+	35
n
Version	2:	136	+	4
n
Version	3:	157	+	1.25
n
For	what	values	of	
n
would	each	version	be	the	fastest	of	the
three?	Remember	that	
n
will	always	be	an	integer.</p>
<p>5.3	
Program	Example
To	demonstrate	how	an	abstract	program	can	be	systematically
transformed	into	more	efficient	code,	we	will	use	a	running	example
based	on	the	vector	data	structure	shown	in	
Figure	
5.3
.	A	vector	is
represented	with	two	blocks	of	memory:	the	header	and	the	data	array.
The	header	is	a	structure	declared	as	follows:
Figure	
5.3	
Vector	abstract	data	type.
A	vector	is	represented	by	header	information	plus	an	array	of	designated
length.</p>
<p>The	declaration	uses	
to	designate	the	data	type	of	the	underlying
elements.	In	our	evaluation,	we	measured	the	performance	of	our	code
for	integer	(C	
and	
),	and	floating-point	(C	
and	
)
data.	We	do	this	by	compiling	and	running	the	program	separately	for
different	type	declarations,	such	as	the	following	for	data	type	
:
We	allocate	the	data	array	block	to	store	the	vector	elements	as	an	array
of	
objects	of	type	
.
Figure	
5.4
shows	some	basic	procedures	for	generating	vectors,
accessing	vector	elements,	and	determining	the	length	of	a	vector.	An
important	feature	to	note	is	that	
,	the	vector	access
routine,	performs	bounds	checking	for	every	vector	reference.	This	code
is	similar	to	the	array	representations	used	in	many	other	languages,
including	Java.	Bounds	checking	reduces	the	chances	of	program	error,
but	it	can	also	slow	down	program	execution.
As	an	optimization	example,	consider	the	code	shown	in	
Figure	
5.5
,
which	combines	all	of	the	elements	in	a	vector	into	a	single	value
according	to	some	operation.	By	using	different	definitions	of	compile-
time	constants	
and	OP,	the	code	can	be	recompiled	to	perform
different	operations	on	the	data.	In	particular,	using	the	declarations</p>
<p>it	sums	the	elements	of	the	vector.	Using	the	declarations
it	computes	the	product	of	the	vector	elements.
In	our	presentation,	we	will	proceed	through	a	series	of	transformations
of	the	code,	writing	different	versions	of	the	combining	function.	To	gauge
progress,</p>
<p>Figure	
5.4	
Implementation	of	vector	abstract	data	type.
In	the	actual	program,	data	type	
is	declared	to	be	
Figure	
5.5	
Initial	implementation	of	combining	operation.
Using	different	declarations	of	identity	element	
and	combining
operation	
,	we	can	measure	the	routine	for	different	operations.
we	measured	the	CPE	performance	of	the	functions	on	a	machine	with
an	Intel	Core	i7	Haswell	processor,	which	we	refer	to	as	our	
reference
machine.
Some	characteristics	of	this	processor	were	given	in	
Section
3.1
.	These	measurements	characterize	performance	in	terms	of	how</p>
<p>the	programs	run	on	just	one	particular	machine,	and	so	there	is	no
guarantee	of	comparable	performance	on	other	combinations	of	machine
and	compiler.	However,	we	have	compared	the	results	with	those	for	a
number	of	different	compiler/processor	combinations,	and	we	have	found
them	generally	consistent	with	those	presented	here.
As	we	proceed	through	a	set	of	transformations,	we	will	find	that	many
lead	to	only	minimal	performance	gains,	while	others	have	more	dramatic
effects.	Determining	which	combinations	of	transformations	to	apply	is
indeed	part	of	the	&quot;black	art&quot;	of	writing	fast	code.	Some	combinations	that
do	not	provide	measurable	benefits	are	indeed	ineffective,	while	others
are	important	as	ways	to	enable	further	optimizations	by	the	compiler.	In
our	experience,	the	best	approach	involves	a	combination	of
experimentation	and	analysis:	repeatedly	attempting	different
approaches,	performing	measurements,	and	examining	the	assembly-
code	representations	to	identify	underlying	performance	bottlenecks.
As	a	starting	point,	the	following	table	shows	CPE	measurements	for
running	on	our	reference	machine,	with	different	combinations
of	operation	(addition	or	multiplication)	and	data	type	(long	integer	and
double-precision	floating-point).	Our	experiments	with	many	different
programs	showed	that	operations	on	32-bit	and	64-bit	integers	have
identical	performance,	with	the	exception	of	code	involving	division
operations.	Similarly,	we	found	identical	performance	for	programs
operating	on	single-	or	double-precision	floating-point	data.	In	our	tables,
we	will	therefore	show	only	separate	results	for	integer	data	and	for
floating-point	data.
Integer
Floating	point</p>
<p>Function
Page
Method</p>
<ul>
<li></li>
</ul>
<ul>
<li></li>
</ul>
<ul>
<li></li>
</ul>
<ul>
<li></li>
</ul>
<p>507
Abstract	unoptimized
22.68
20.02
19.98
20.18
507
Abstract	
10.12
10.12
10.17
11.14
We	can	see	that	our	measurements	are	somewhat	imprecise.	The	more
likely	CPE	number	for	integer	sum	is	23.00,	rather	than	22.68,	while	the
number	for	integer	product	is	likely	20.0	instead	of	20.02.	Rather	than
&quot;fudging&quot;	our	numbers	to	make	them	look	good,	we	will	present	the
measurements	we	actually	obtained.	There	are	many	factors	that
complicate	the	task	of	reliably	measuring	the	precise	number	of	clock
cycles	required	by	some	code	sequence.	It	helps	when	examining	these
numbers	to	mentally	round	the	results	up	or	down	by	a	few	hundredths	of
a	clock	cycle.
The	unoptimized	code	provides	a	direct	translation	of	the	C	code	into
machine	code,	often	with	obvious	inefficiencies.	By	simply	giving	the
command-line	option	
,	we	enable	a	basic	set	of	optimizations.	As	can
be	seen,	this	significantly	improves	the	program	performance—more	than
a	factor	of	2—with	no	effort	on	behalf	of	the	programmer.	In	general,	it	is
good	to	get	into	the	habit	of	enabling	some	level	of	optimization.	(Similar
performance	results	were	obtained	with	optimization	level	
.)	For	the
remainder	of	our	measurements,	we	use	optimization	levels	
and	
when	generating	and	measuring	our	programs.</p>
<p>5.4	
Eliminating	Loop	Inefficiencies
Observe	that	procedure	
,	as	shown	in	
Figure	
5.5
,	calls
function	
as	the	test	condition	of	the	
loop.	Recall	from	our
discussion	of	how	to	translate	code	containing	loops	into	machine-level
programs	(
Section	
3.6.7
)	that	the	test	condition	must	be	evaluated	on
every	iteration	of	the	loop.	On	the	other	hand,	the	length	of	the	vector
does	not	change	as	the	loop	proceeds.	We	could	therefore	compute	the
vector	length	only	once	and	use	this	value	in	our	test	condition.
Figure	
5.6
shows	a	modified	version	called	
.	It	calls
at	the	beginning	and	assigns	the	result	to	a	local	variable
length.	This	transformation	has	noticeable	effect	on	the	overall
performance	for	some	data	types	and	operations,	and	minimal	or	even
none	for	others.	In	any	case,	this	transformation	is	required	to	eliminate
inefficiencies	that	would	become	bottlenecks	as	we	attempt	further
optimizations.
Integer
Floating
point
Function
Page
Method</p>
<ul>
<li></li>
</ul>
<ul>
<li></li>
</ul>
<ul>
<li></li>
</ul>
<ul>
<li></li>
</ul>
<p>507	Abstract	
10.12
10.12
10.17
11.14
509
Move	
7.02
9.03
9.02
11.03</p>
<p>This	optimization	is	an	instance	of	a	general	class	of	optimizations	known
as	
code	motion.
They	involve	identifying	a	computation	that	is	performed
multiple
Figure	
5.6	
Improving	the	efficiency	of	the	loop	test.
By	moving	the	call	to	
out	of	the	loop	test,	we	eliminate	the
need	to	execute	it	on	every	iteration.
times,	(e.g.,	within	a	loop),	but	such	that	the	result	of	the	computation	will
not	change.	We	can	therefore	move	the	computation	to	an	earlier	section
of	the	code	that	does	not	get	evaluated	as	often.	In	this	case,	we	moved
the	call	to	
from	within	the	loop	to	just	before	the	loop.</p>
<p>Optimizing	compilers	attempt	to	perform	code	motion.	Unfortunately,	as
discussed	previously,	they	are	typically	very	cautious	about	making
transformations	that	change	where	or	how	many	times	a	procedure	is
called.	They	cannot	reliably	detect	whether	or	not	a	function	will	have
side	effects,	and	so	they	assume	that	it	might.	For	example,	if	
had	some	side	effect,	then	
and	
could	have	different
behaviors.	To	improve	the	code,	the	programmer	must	often	help	the
compiler	by	explicitly	performing	code	motion.
As	an	extreme	example	of	the	loop	inefficiency	seen	in	
,
consider	the	procedure	
shown	in	
Figure	
5.7
.	This	procedure	is
styled	after	routines	submitted	by	several	students	as	part	of	a	network
programming	project.	Its	purpose	is	to	convert	all	of	the	uppercase	letters
in	a	string	to	lowercase.	The	procedure	steps	through	the	string,
converting	each	uppercase	character	to	lowercase.	The	case	conversion
involves	shifting	characters	in	the	range	<code>A'	to	</code>Z'	to	the	range	<code>a'	to	</code>z'.
The	library	function	
is	called	as	part	of	the	loop	test	of	
.
Although	
is	typically	implemented	with	special	x86	string-
processing	instructions,	its	overall	execution	is	similar	to	the	simple
version	that	is	also	shown	in	
Figure	
5.7
.	Since	strings	in	C	are	null-
terminated	character	sequences,	
can	only	determine	the	length	of
a	string	by	stepping	through	the	sequence	until	it	hits	a	null	character.	For
a	string	of	length	
n
,	
takes	time	proportional	to	
n.
Since	
is
called	in	each	of	the	
n
iterations	of	
,	the	overall	run	time	of	
is	quadratic	in	the	string	length,	proportional	to	
n
.
2</p>
<p>Figure	
5.7	
Lowercase	conversion	routines.
The	two	procedures	have	radically	different	performance.
This	analysis	is	confirmed	by	actual	measurements	of	the	functions	for
different	length	strings,	as	shown	in	
Figure	
5.8
(and	using	the	library
version	of	
).	The	graph	of	the	run	time	for	
rises	steeply	as
the	string	length	increases	(
Figure	
5.8(a)
).	
Figure	
5.8(b)
shows	the
run	times	for	seven	different	lengths	(not	the	same	as	shown	in	the
graph),	each	of	which	is	a	power	of	2.	Observe	that	for	
each
doubling	of	the	string	length	causes	a	quadrupling	of	the	run	time.	This	is
a	clear	indicator	of	a	quadratic	run	time.	For	a	string	of	length	1,048,576,
requires	over	17	minutes	of	CPU	time.
String	length
Function
16,384
32,768
65,536
131,072
262,144
524,288
1,048,576
0.26
1.03
4.10
16.41
65.62
262.48
1,049.89</p>
<p>0.0000
0.0001
0.0001
0.0003
0.0005
0.0010
0.0020
(b)
Figure	
5.8	
Comparative	performance	of	lowercase	conversion
routines.
The	original	code	
has	a	quadratic	run	time	due	to	an	inefficient
loop	structure.	The	modified	code	
has	a	linear	run	time.
Function	
shown	in	
Figure	
5.7
is	identical	to	that	of	
,
except	that	we	have	moved	the	call	to	
out	of	the	loop.	The
performance	improves	dramatically.	For	a	string	length	of	1,048,576,	the
function	requires	just	2.0	milliseconds—over	500,000	times	faster	than
.	Each	doubling	of	the	string	length	causes	a	doubling	of	the	run
time—a	clear	indicator	of	linear	run	time.	For	longer	strings,	the	run-time
improvement	will	be	even	greater.
In	an	ideal	world,	a	compiler	would	recognize	that	each	call	to	
in
the	loop	test	will	return	the	same	result,	and	thus	the	call	could	be	moved
out	of	the	loop.	This	would	require	a	very	sophisticated	analysis,	since
checks	the	elements	of	the	string	and	these	values	are	changing
as	
proceeds.	The	compiler	would	need	to	detect	that	even	though
the	characters	within	the	string	are	changing,	none	are	being	set	from
nonzero	to	zero,	or	vice	versa.	Such	an	analysis	is	well	beyond	the	ability
of	even	the	most	sophisticated	compilers,	even	if	they	employ	inlining,
and	so	programmers	must	do	such	transformations	themselves.
This	example	illustrates	a	common	problem	in	writing	programs,	in	which
a	seemingly	trivial	piece	of	code	has	a	hidden	asymptotic	inefficiency.</p>
<p>One	would	not	expect	a	lowercase	conversion	routine	to	be	a	limiting
factor	in	a	program's	performance.	Typically,	programs	are	tested	and
analyzed	on	small	data	sets,	for	which	the	performance	of	
is
adequate.	When	the	program	is	ultimately	
deployed,	however,	it	is
entirely	possible	that	the	procedure	could	be	applied	to	strings	of	over
one	million	characters.	All	of	a	sudden	this	benign	piece	of	code	has
become	a	major	performance	bottleneck.	By	contrast,	the	performance	of
will	be	adequate	for	strings	of	arbitrary	length.	Stories	abound	of
major	programming	projects	in	which	problems	of	this	sort	occur.	Part	of
the	job	of	a	competent	programmer	is	to	avoid	ever	introducing	such
asymptotic	inefficiency.
Practice	Problem	
5.3	
(solution	page
573
)
Consider	the	following	functions:
The	following	three	code	fragments	call	these	functions:
A
.	</p>
<p>B
.	</p>
<p>C
.	</p>
<p>Assume	
equals	10	and	
equals	100.	Fill	in	the	following	table
indicating	the	number	of	times	each	of	the	four	functions	is	called
in	code	fragments	A–C:
Code
A.</p>
<hr />
<hr />
<hr />
<hr />
<p>B.</p>
<hr />
<hr />
<hr />
<hr />
<p>C.</p>
<hr />
<hr />
<hr />
<hr />
<h2>5.5	
Reducing	Procedure	Calls
As	we	have	seen,	procedure	calls	can	incur	overhead	and	also	block
most	forms	of	program	optimization.	We	can	see	in	the	code	for	
(
Figure	
5.6
)	that	
is	called	on	every	loop	iteration	to
retrieve	the	next	vector	element.	This	function	checks	the	vector	index	
against	the	loop	bounds	with	every	vector	reference,	a	clear	source	of
inefficiency.	Bounds	checking	might	be	a	useful	feature	when	dealing
with	arbitrary	array	accesses,	but	a	simple	analysis	of	the	code	for
shows	that	all	references	will	be	valid.</h2>
<h2 id="codeoptvecc"><a class="header" href="#codeoptvecc">code/opt/vec.c</a></h2>
<p>code/opt/vec.c</p>
<p>Figure	
5.9	
Eliminating	function	calls	within	the	loop.
The	resulting	code	does	not	show	a	performance	gain,	but	it	enables
additional	optimizations.
Suppose	instead	that	we	add	a	function	
to	our	abstract
data	type.	This	function	returns	the	starting	address	of	the	data	array,	as
shown	in	
Figure	
5.9
.	We	could	then	write	the	procedure	shown	as
in	this	figure,	having	no	function	calls	in	the	inner	loop.	Rather
than	making	a	function	call	to	retrieve	each	vector	element,	it	accesses
the	array	directly.	A	purist	might	say	that	this	transformation	seriously
impairs	the	program	modularity.	In	principle,	the	user	of	the	vector
abstract	data	type	should	not	even	need	to	know	that	the	vector	contents
are	stored	as	an	array,	rather	than	as	some	other	data	structure	such	as
a	linked	list.	A	more	pragmatic	programmer	would	argue	that	this
transformation	is	a	necessary	step	toward	achieving	high-performance
results.
Integer
Floating	point</p>
<p>Function
Page
Method</p>
<ul>
<li></li>
</ul>
<ul>
<li></li>
</ul>
<ul>
<li></li>
</ul>
<ul>
<li></li>
</ul>
<p>509
Move	
7.02
9.03
9.02
11.03
513
Direct	data	access
7.17
9.02
9.02
11.03
Surprisingly,	there	is	no	apparent	performance	improvement.	Indeed,	the
performance	for	integer	sum	has	gotten	slightly	worse.	Evidently,	other
operations	in	the	inner	loop	are	forming	a	bottleneck	that	limits	the
performance	more	than	the	call	to	
.	We	will	return	to	this
function	later	(
Section	
5.11.2
)	and	see	why	the	repeated	bounds
checking	by	
does	not	incur	a	performance	penalty.	For	now,	we
can	view	this	transformation	as	one	of	a	series	of	steps	that	will	ultimately
lead	to	greatly	improved	performance.</p>
<p>5.6	
Eliminating	Unneeded	Memory
References
The	code	for	
accumulates	the	value	being	computed	by	the
combining	operation	at	the	location	designated	by	the	pointer	
.	This
attribute	can	be	seen	by	examining	the	assembly	code	generated	for	the
inner	loop	of	the	compiled	code.	We	show	here	the	x86-64	code
generated	for	data	type	
and	with	multiplication	as	the	combining
operation:
We	see	in	this	loop	code	that	the	address	corresponding	to	pointer	
is	held	in	register	
.	It	has	also	transformed	the	code	to	maintain	a</p>
<p>pointer	to	the	
i
th	data	element	in	register	
,	shown	in	the	annotations
as	
.	This	pointer	is	incremented	by	8	on	every	iteration.	The	loop
termination	is	detected	by	comparing	this	pointer	to	one	stored	in	register
.	We	can	see	that	the	accumulated	value	is	read	from	and	written	to
memory	on	each	iteration.	This	reading	and	writing	is	wasteful,	since	the
value	read	from	
at	the	beginning	of	each	iteration	should	simply	be
the	value	written	at	the	end	of	the	previous	iteration.
We	can	eliminate	this	needless	reading	and	writing	of	memory	by
rewriting	the	code	in	the	style	of	
in	
Figure	
5.10
.	We	introduce
a	temporary	variable	
that	is	used	in	the	loop	to	accumulate	the
computed	value.	The	result	is	stored	at	
only	after	the	loop	has	been
completed.	As	the	assembly	code	that	follows	shows,	the	compiler	can
now	use	register	
to	hold	the	accumulated	value.	Compared	to	the
loop	in	
,	we	have	reduced	the	memory	operations	per	iteration
from	two	reads	and	one	write	to	just	a	single	read.</p>
<p>We	see	a	significant	improvement	in	program	performance,	as	shown	in
the	following	table:
Figure	
5.10	
Accumulating	result	in	temporary.
Holding	the	accumulated	value	in	local	variable	
(short	for
&quot;accumulator&quot;)	eliminates	the	need	to	retrieve	it	from	memory	and	write
back	the	updated	value	on	every	loop	iteration.
Integer
Floating	point
Function
Page
Method</p>
<ul>
<li></li>
</ul>
<ul>
<li></li>
</ul>
<ul>
<li></li>
</ul>
<ul>
<li></li>
</ul>
<p>513
Direct	data	access
7.17
9.02
9.02
11.03</p>
<p>515
Accumulate	in	temporary
1.27
3.01
3.01
5.01
All	of	our	times	improve	by	factors	ranging	from	2.2×	to	5.7×,	with	the
integer	addition	case	dropping	to	just	1.27	clock	cycles	per	element.
Again,	one	might	think	that	a	compiler	should	be	able	to	automatically
transform	the	
code	shown	in	
Figure	
5.9
to	accumulate	the
value	in	a	register,	as	it	does	with	the	code	for	
shown	in	
Figure
5.10
.	In	fact,	however,	the	two	functions	can	have	different	behaviors
due	to	memory	aliasing.	Consider,	for	example,	the	case	of	integer	data
with	multiplication	as	the	operation	and	1	as	the	identity	element.	Let	
=
[2,	3,	5]	be	a	vector	of	three	elements	and	consider	the	following	two
function	calls:
That	is,	we	create	an	alias	between	the	last	element	of	the	vector	and	the
destination	for	storing	the	result.	The	two	functions	would	then	execute
as	follows:
Function
Initial
Before	loop
=0
=1
=2
Final
[2,	3,	5]
[2,	3,	1]
[2,	3,	2]
[2,	3,	6]
[2,	3,	36]
[2,	3,	36]
[2,	3,	5]
[2,	3,	5]
[2,	3,	5]
[2,	3,	5]
[2,	3,	5]
[2,	3,	30]
As	shown	previously,	
accumulates	its	result	at	the	destination,
which	in	this	case	is	the	final	vector	element.	This	value	is	therefore	set</p>
<p>first	to	1,	then	to	2	·	1	=	2,	and	then	to	3	·	2	=	6.	On	the	last	iteration,	this
value	is	then	multiplied	by	itself	to	yield	a	final	value	of	36.	For	the	case
of	
,	the	vector	remains	unchanged	until	the	end,	when	the	final
element	is	set	to	the	computed	result	1	·	2	·	3	·	5	=	30.
Of	course,	our	example	showing	the	distinction	between	
and
is	highly	contrived.	One	could	argue	that	the	behavior	of
more	closely	matches	the	intention	of	the	function	description.
Unfortunately,	a	compiler	cannot	make	a	judgment	about	the	conditions
under	which	a	function	might	be	used	and	what	the	programmer's
intentions	might	be.	Instead,	when	given	
to	compile,	the
conservative	approach	is	to	keep	reading	and	writing	memory,	even
though	this	is	less	efficient.
Practice	Problem	
5.4	
(solution	page
574
)
When	we	use	
GCC</p>
<p>to	compile	
with	command-line	option
,	we	get	code	with	substantially	better	CPE	performance	than
with	
:
Integer
Floating	point
Function
Page
Method</p>
<ul>
<li></li>
</ul>
<ul>
<li></li>
</ul>
<ul>
<li></li>
</ul>
<ul>
<li></li>
</ul>
<p>513
Compiled	
7.17
9.02
9.02
11.03
513
Compiled	
1.60
3.01
3.01
5.01</p>
<p>515
Accumulate	in	temporary
1.27
3.01
3.01
5.01
We	achieve	performance	comparable	to	that	for	
,	except
for	the	case	of	integer	sum,	but	even	it	improves	significantly.	On
examining	the	assembly	code	generated	by	the	compiler,	we	find
an	interesting	variant	for	the	inner	loop:
We	can	compare	this	to	the	version	created	with	optimization	level
1:</p>
<p>We	see	that,	besides	some	reordering	of	instructions,	the	only
difference	is	that	the	more	optimized	version	does	not	contain	the
implementing	the	read	from	the	location	designated	by
(line	2).
A
.	
How	does	the	role	of	register	
differ	in	these	two
loops?
B
.	
Will	the	more	optimized	version	faithfully	implement	the	C
code	of	
,	including	when	there	is	memory	aliasing
between	
and	the	vector	data?
C
.	
Either	explain	why	this	optimization	preserves	the	desired
behavior,	or	give	an	example	where	it	would	produce
different	results	than	the	less	optimized	code.
With	this	final	transformation,	we	reached	a	point	where	we	require	just
1.25-5	clock	cycles	for	each	element	to	be	computed.	This	is	a
considerable	improvement	over	the	original	9-11	cycles	when	we	first
enabled	optimization.	We	would	now	like	to	see	just	what	factors	are
constraining	the	performance	of	our	code	and	how	we	can	improve	things
even	further.</p>
<p>5.7	
Understanding	Modern
Processors
Up	to	this	point,	we	have	applied	optimizations	that	did	not	rely	on	any
features	of	the	target	machine.	They	simply	reduced	the	overhead	of
procedure	calls	and	eliminated	some	of	the	critical	&quot;optimization	blockers&quot;
that	cause	difficulties	for	optimizing	compilers.	As	we	seek	to	push	the
performance	further,	we	must	consider	optimizations	that	exploit	the
microarchitecture
of	the	processor—that	is,	the	underlying	system	design
by	which	a	processor	executes	instructions.	Getting	every	last	bit	of
performance	requires	a	detailed	analysis	of	the	program	as	well	as	code
generation	tuned	for	the	target	processor.	Nonetheless,	we	can	apply
some	basic	optimizations	that	will	yield	an	overall	performance
improvement	on	a	large	class	of	processors.	The	detailed	performance
results	we	report	here	may	not	hold	for	other	machines,	but	the	general
principles	of	operation	and	optimization	apply	to	a	wide	variety	of
machines.
To	understand	ways	to	improve	performance,	we	require	a	basic
understanding	of	the	microarchitectures	of	modern	processors.	Due	to
the	large	number	of	transistors	that	can	be	integrated	onto	a	single	chip,
modern	microprocessors	employ	complex	hardware	that	attempts	to
maximize	program	performance.	One	result	is	that	their	actual	operation
is	far	different	from	the	view	that	is	perceived	by	looking	at	machine-level
programs.	At	the	code	level,	it	appears	as	if	instructions	are	executed
one	at	a	time,	where	each	instruction	involves	fetching	values	from</p>
<p>registers	or	memory,	performing	an	operation,	and	storing	results	back	to
a	register	or	memory	location.	In	the	actual	processor,	a	number	of
instructions	
are	evaluated	simultaneously,	a	phenomenon	referred	to	as
instruction-level	parallelism
.	In	some	designs,	there	can	be	100	or	more
instructions	&quot;in	flight.&quot;	Elaborate	mechanisms	are	employed	to	make	sure
the	behavior	of	this	parallel	execution	exactly	captures	the	sequential
semantic	model	required	by	the	machine-level	program.	This	is	one	of
the	remarkable	feats	of	modern	microprocessors:	they	employ	complex
and	exotic	microarchitectures,	in	which	multiple	instructions	can	be
executed	in	parallel,	while	presenting	an	operational	view	of	simple
sequential	instruction	execution.
Although	the	detailed	design	of	a	modern	microprocessor	is	well	beyond
the	scope	of	this	book,	having	a	general	idea	of	the	principles	by	which
they	operate	suffices	to	understand	how	they	achieve	instruction-level
parallelism.	We	will	find	that	two	different	lower	bounds	characterize	the
maximum	performance	of	a	program.	The	
latency	bound
is	encountered
when	a	series	of	operations	must	be	performed	in	strict	sequence,
because	the	result	of	one	operation	is	required	before	the	next	one	can
begin.	This	bound	can	limit	program	performance	when	the	data
dependencies	in	the	code	limit	the	ability	of	the	processor	to	exploit
instruction-level	parallelism.	The	
throughput	bound
characterizes	the	raw
computing	capacity	of	the	processor's	functional	units.	This	bound
becomes	the	ultimate	limit	on	program	performance.
5.7.1	
Overall	Operation</p>
<p>Figure	
5.11
shows	a	very	simplified	view	of	a	modern	microprocessor.
Our	hypothetical	processor	design	is	based	loosely	on	the	structure	of
recent	Intel	processors.	These	processors	are	described	in	the	industry
as	being	
superscalar
,	which	means	they	can	perform	multiple	operations
on	every	clock	cycle	and	
out	of	order
,	meaning	that	the	order	in	which
instructions	execute	need	not	correspond	to	their	ordering	in	the
machine-level	program.	The	overall	design	has	two	main	parts:	the
instruction	control	unit
(ICU),	which	is	responsible	for	reading	a	sequence
of	instructions	from	memory	and	generating	from	these	a	set	of	primitive
operations	to	perform	on	program	data,	and	the	
execution	unit
(EU),
which	then	executes	these	operations.	Compared	to	the	simple	
in-order
pipeline	we	studied	in	
Chapter	
4
,	out-of-order	processors	require	far
greater	and	more	complex	hardware,	but	they	are	better	at	achieving
higher	degrees	of	instruction-level	parallelism.
The	ICU	reads	the	instructions	from	an	
instruction	cache
—a	special	high-
speed	memory	containing	the	most	recently	accessed	instructions.	In
general,	the	ICU	fetches	well	ahead	of	the	currently	executing
instructions,	so	that	it	has	enough	time	to	decode	these	and	send
operations	down	to	the	EU.	One	problem,	however,	is	that	when	a
program	hits	a	branch,
there	are	two	possible	directions	the	program
might	go.	The	branch	can	be	
taken
,	with	control	passing	to	the	branch
target.	Alternatively,	the	branch	can	be	
not	taken
,	with	control	passing	to
the	next</p>
<ol>
<li></li>
</ol>
<p>We	use	the	term	&quot;branch&quot;	specifically	to	refer	to	conditional	jump	instructions.	Other	instructions
that	can	transfer	control	to	multiple	destinations,	such	as	procedure	return	and	indirect	jumps,
provide	similar	challenges	for	the	processor.
1</p>
<p>Figure	
5.11	
Block	diagram	of	an	out-of-order	processor.
The	instruction	control	unit	is	responsible	for	reading	instructions	from
memory	and	generating	a	sequence	of	primitive	operations.	The
execution	unit	then	performs	the	operations	and	indicates	whether	the
branches	were	correctly	predicted.
instruction	in	the	instruction	sequence.	Modern	processors	employ	a
technique	known	as	
branch	prediction
,	in	which	they	guess	whether	or
not	a	branch	will	be	taken	and	also	predict	the	target	address	for	the
branch.	Using	a	technique	known	as	
speculative	execution
,	the
processor	begins	fetching	and	decoding	instructions	at	where	it	predicts
the	branch	will	go,	and	even	begins	executing	these	operations	before	it
has	been	determined	whether	or	not	the	branch	prediction	was	correct.	If
it	later	determines	that	the	branch	was	predicted	incorrectly,	it	resets	the
state	to	that	at	the	branch	point	and	begins	fetching	and	executing</p>
<p>instructions	in	the	other	direction.	The	block	labeled	&quot;Fetch	control&quot;
incorporates	branch	prediction	to	perform	the	task	of	determining	which
instructions	to	fetch.
The	
instruction	decoding
logic	takes	the	actual	program	instructions	and
converts	them	into	a	set	of	primitive	
operations
(sometimes	referred	to	as
micro-operations
).	Each	of	these	operations	performs	some	simple
computational	task	such	as	adding	two	numbers,	reading	data	from
memory,	or	writing	data	to	memory.	For	machines	with	complex
instructions,	such	as	x86	processors,	an	instruction	
can	be	decoded	into
multiple	operations.	The	details	of	how	instructions	are	decoded	into
sequences	of	operations	varies	between	machines,	and	this	information
is	considered	highly	proprietary.	Fortunately,	we	can	optimize	our
programs	without	knowing	the	low-level	details	of	a	particular	machine
implementation.
In	a	typical	x86	implementation,	an	instruction	that	only	operates	on
registers,	such	as
is	converted	into	a	single	operation.	On	the	other	hand,	an	instruction
involving	one	or	more	memory	references,	such	as</p>
<p>yields	multiple	operations,	separating	the	memory	references	from	the
arithmetic	operations.	This	particular	instruction	would	be	decoded	as
three	operations:	one	to	
load
a	value	from	memory	into	the	processor,
one	to	add	the	loaded	value	to	the	value	in	register	
,	and	one	to
store
the	result	back	to	memory.	The	decoding	splits	instructions	to	allow
a	division	of	labor	among	a	set	of	dedicated	hardware	units.	These	units
can	then	execute	the	different	parts	of	multiple	instructions	in	parallel.
The	EU	receives	operations	from	the	instruction	fetch	unit.	Typically,	it
can	receive	a	number	of	them	on	each	clock	cycle.	These	operations	are
dispatched	to	a	set	of	
functional	units
that	perform	the	actual	operations.
These	functional	units	are	specialized	to	handle	different	types	of
operations.
Reading	and	writing	memory	is	implemented	by	the	load	and	store	units.
The	load	unit	handles	operations	that	read	data	from	the	memory	into	the
processor.	This	unit	has	an	adder	to	perform	address	computations.
Similarly,	the	store	unit	handles	operations	that	write	data	from	the
processor	to	the	memory.	It	also	has	an	adder	to	perform	address
computations.	As	shown	in	the	figure,	the	load	and	store	units	access
memory	via	a	
data	cache
,	a	high-speed	memory	containing	the	most
recently	accessed	data	values.
With	speculative	execution,	the	operations	are	evaluated,	but	the	final
results	are	not	stored	in	the	program	registers	or	data	memory	until	the
processor	can	be	certain	that	these	instructions	should	actually	have
been	executed.	Branch	operations	are	sent	to	the	EU,	not	to	determine
where	the	branch	should	go,	but	rather	to	determine	whether	or	not	they
were	predicted	correctly.	If	the	prediction	was	incorrect,	the	EU	will</p>
<p>discard	the	results	that	have	been	computed	beyond	the	branch	point.	It
will	also	signal	the	branch	unit	that	the	prediction	was	incorrect	and
indicate	the	correct	branch	destination.	In	this	case,	the	branch	unit
begins	fetching	at	the	new	location.	As	we	saw	in	
Section	
3.6.6
,	such
a	
misprediction
incurs	a	significant	cost	in	performance.	It	takes	a	while
before	the	new	instructions	can	be	fetched,	decoded,	and	sent	to	the
functional	units.
Figure	
5.11
indicates	that	the	different	functional	units	are	designed	to
perform	different	operations.	Those	labeled	as	performing	&quot;arithmetic
operations&quot;	are	typically	specialized	to	perform	different	combinations	of
integer	and	floating-point	operations.	As	the	number	of	transistors	that
can	be	integrated	onto	a	single	
microprocessor	chip	has	grown	over	time,
successive	models	of	microprocessors	have	increased	the	total	number
of	functional	units,	the	combinations	of	operations	each	unit	can	perform,
and	the	performance	of	each	of	these	units.	The	arithmetic	units	are
intentionally	designed	to	be	able	to	perform	a	variety	of	different
operations,	since	the	required	operations	vary	widely	across	different
programs.	For	example,	some	programs	might	involve	many	integer
operations,	while	others	require	many	floating-point	operations.	If	one
functional	unit	were	specialized	to	perform	integer	operations	while
another	could	only	perform	floating-point	operations,	then	none	of	these
programs	would	get	the	full	benefit	of	having	multiple	functional	units.
For	example,	our	Intel	Core	i7	Has	well	reference	machine	has	eight
functional	units,	numbered	0−7.	Here	is	a	partial	list	of	each	one's
capabilities:
0
.	
Integer	arithmetic,	floating-point	multiplication,	integer	and	floating-
point	division,	branches</p>
<p>1
.	
Integer	arithmetic,	floating-point	addition,	integer	multiplication,
floating-point	multiplication
2
.	
Load,	address	computation
3
.	
Load,	address	computation
4
.	
Store
5
.	
Integer	arithmetic
6
.	
Integer	arithmetic,	branches
7
.	
Store	address	computation
In	the	above	list,	&quot;integer	arithmetic&quot;	refers	to	basic	operations,	such	as
addition,	bitwise	operations,	and	shifting.	Multiplication	and	division
require	more	specialized	resources.	We	see	that	a	store	operation
requires	two	functional	units—one	to	compute	the	store	address	and	one
to	actually	store	the	data.	We	will	discuss	the	mechanics	of	store	(and
load)	operations	in	
Section	
5.12
.
We	can	see	that	this	combination	of	functional	units	has	the	potential	to
perform	multiple	operations	of	the	same	type	simultaneously.	It	has	four
units	capable	of	performing	integer	operations,	two	that	can	perform	load
operations,	and	two	that	can	perform	floating-point	multiplication.	We	will
later	see	the	impact	these	resources	have	on	the	maximum	performance
our	programs	can	achieve.
Within	the	ICU,	the	
retirement	unit
keeps	track	of	the	ongoing	processing
and	makes	sure	that	it	obeys	the	sequential	semantics	of	the	machine-
level	program.	Our	figure	shows	a	
register	file
containing	the	integer,
floating-point,	and,	more	recently,	SSE	and	AVX	registers	as	part	of	the
retirement	unit,	because	this	unit	controls	the	updating	of	these	registers.
As	an	instruction	is	decoded,	information	about	it	is	placed	into	a	first-in,
first-out	queue.	This	information	remains	in	the	queue	until	one	of	two</p>
<p>outcomes	occurs.	First,	once	the	operations	for	the	instruction	have
completed	and	any	branch	points	leading	to	this	instruction	are	confirmed
as	having	been	correctly	predicted,	the	instruction	can	be	
retired
,	with
any	updates	to	the	program	registers	being	made.	If	some	branch	point
leading	to	this	instruction	was	mispredicted,	on	the	other	hand,	the
instruction	will	be
Aside	
The	history	of	out-of-order
processing
Out-of-order	processing	was	first	implemented	in	the	Control	Data
Corporation	6600	processor	in	1964.	Instructions	were	processed
by	10	different	functional	units,	each	of	which	could	be	operated
independently.	In	its	day,	this	machine,	with	a	clock	rate	of	10
MHz,	was	considered	the	premium	machine	for	scientific
computing.
IBM	first	implemented	out-of-order	processing	with	the	IBM	360/91
processor	in	1966,	but	just	to	execute	the	floating-point
instructions.	For	around	25	years,	out-of-order	processing	was
considered	an	exotic	technology,	found	only	in	machines	striving
for	the	highest	possible	performance,	until	IBM	reintroduced	it	in
the	RS/6000	line	of	workstations	in	1990.	This	design	became	the
basis	for	the	IBM/Motorola	PowerPC	line,	with	the	model	601,
introduced	in	1993,	becoming	the	first	single-chip	microprocessor
to	use	out-of-order	processing.	Intel	introduced	out-of-order
processing	with	its	PentiumPro	model	in	1995,	with	an	underlying
microarchitecture	similar	to	that	of	our	reference	machine.</p>
<p>flushed
,	discarding	any	results	that	may	have	been	computed.	By	this
means,	mispredictions	will	not	alter	the	program	state.
As	we	have	described,	any	updates	to	the	program	registers	occur	only
as	instructions	are	being	retired,	and	this	takes	place	only	after	the
processor	can	be	certain	that	any	branches	leading	to	this	instruction
have	been	correctly	predicted.	To	expedite	the	communication	of	results
from	one	instruction	to	another,	much	of	this	information	is	exchanged
among	the	execution	units,	shown	in	the	figure	as	&quot;Operation	results.&quot;	As
the	arrows	in	the	figure	show,	the	execution	units	can	send	results
directly	to	each	other.	This	is	a	more	elaborate	form	of	the	data-
forwarding	techniques	we	incorporated	into	our	simple	processor	design
in	
Section	
4.5.5
.
The	most	common	mechanism	for	controlling	the	communication	of
operands	among	the	execution	units	is	called	
register	renaming
.	When
an	instruction	that	updates	register	
r
is	decoded,	a	
tag	t
is	generated
giving	a	unique	identifier	to	the	result	of	the	operation.	An	entry	
(r,	t)
is
added	to	a	table	maintaining	the	association	between	program	register	
r
and	tag	
t
for	an	operation	that	will	update	this	register.	When	a
subsequent	instruction	using	register	
r
as	an	operand	is	decoded,	the
operation	sent	to	the	execution	unit	will	contain	
t
as	the	source	for	the
operand	value.	When	some	execution	unit	completes	the	first	operation,
it	generates	a	result	
(v,	t)
,	indicating	that	the	operation	with	tag	
t
produced	value	
v
.	Any	operation	waiting	for	
t
as	a	source	will	then	use	
v
as	the	source	value,	a	form	of	data	forwarding.	By	this	mechanism,
values	can	be	forwarded	directly	from	one	operation	to	another,	rather
than	being	written	to	and	read	from	the	register	file,	enabling	the	second
operation	to	begin	as	soon	as	the	first	has	completed.	The	renaming
table	only	contains	entries	for	registers	having	pending	write	operations.</p>
<p>When	a	decoded	instruction	requires	a	register	
r
,	and	there	is	no	tag
associated	with	this	register,	the	operand	is	retrieved	directly	from	the
register	file.	With	register	renaming,	an	entire	sequence	of	operations	can
be	performed	speculatively,	even	though	the	registers	are	updated	only
after	the	processor	is	certain	of	the	branch	outcomes.
Integer
Floating	point
Operation
Latency
Issue
Capacity
Latency
Issue
Capacity
Addition
1
1
4
3
1
1
Multiplication
3
1
1
5
1
2
Division
3−30
3−30
1
3−15
3−15
1
Figure	
5.12	
Latency,	issue	time,	and	capacity	characteristics	of
reference	machine	operations.
Latency	indicates	the	total	number	of	clock	cycles	required	to	perform	the
actual	operations,	while	issue	time	indicates	the	minimum	number	of
cycles	between	two	independent	operations.	The	capacity	indicates	how
many	of	these	operations	can	be	issued	simultaneously.	The	times	for
division	depend	on	the	data	values.
5.7.2	
Functional	Unit	Performance
Figure	
5.12
documents	the	performance	of	some	of	the	arithmetic
operations	for	our	Intel	Core	i7	Haswell	reference	machine,	determined
by	both	measurements	and	by	reference	to	Intel	literature	[
49
].	These</p>
<p>timings	are	typical	for	other	processors	as	well.	Each	operation	is
characterized	by	its	
latency
,	meaning	the	total	time	required	to	perform
the	operation,	the	
issue	time
,	meaning	the	minimum	number	of	clock
cycles	between	two	independent	operations	of	the	same	type,	and	the
capacity
,	indicating	the	number	of	functional	units	capable	of	performing
that	operation.
We	see	that	the	latencies	increase	in	going	from	integer	to	floating-point
operations.	We	see	also	that	the	addition	and	multiplication	operations	all
have	issue	times	of	1,	meaning	that	on	each	clock	cycle,	the	processor
can	start	a	new	one	of	these	operations.	This	short	issue	time	is	achieved
through	the	use	of	
pipelining
.	A	pipelined	function	unit	is	implemented	as
a	series	of	
stages
,	each	of	which	performs	part	of	the	operation.	For
example,	a	typical	floating-point	adder	contains	three	stages	(and	hence
the	three-cycle	latency):	one	to	process	the	exponent	values,	one	to	add
the	fractions,	and	one	to	round	the	result.	The	arithmetic	operations	can
proceed	through	the	stages	in	close	succession	rather	than	waiting	for
one	operation	to	complete	before	the	next	begins.	This	capability	can	be
exploited	only	if	there	are	successive,	logically	independent	operations	to
be	performed.	Functional	units	with	issue	times	of	1	cycle	are	said	to	be
fully	pipelined:
they	can	start	a	new	operation	every	clock	cycle.
Operations	with	capacity	greater	than	1	arise	due	to	the	capabilities	of
the	multiple	functional	units,	as	was	described	earlier	for	the	reference
machine.
We	see	also	that	the	divider	(used	for	integer	and	floating-point	division,
as	well	as	floating-point	square	root)	is	not	pipelined—its	issue	time
equals	its	latency.	What	this	means	is	that	the	divider	must	perform	a
complete	division	before	it	can	begin	anew	one.	We	also	see	that	the
latencies	and	issue	times	for	division	are	given	as	ranges,	because	some</p>
<p>combinations	of	dividend	and	divisor	require	more	steps	than	others.	The
long	latency	and	issue	times	of	division	make	it	a	comparatively	costly
operation.
A	more	common	way	of	expressing	issue	time	is	to	specify	the	maximum
throughput
of	the	unit,	defined	as	the	reciprocal	of	the	issue	time.	A	fully
pipelined	functional	unit	has	a	maximum	throughput	of	1	operation	per
clock	cycle,	while	units	with	higher	issue	times	have	lower	maximum
throughput.	Having	multiple	functional	units	can	increase	throughput
even	further.	For	an	operation	with	capacity	
C
and	issue	time	
I
,	the
processor	can	potentially	achieve	a	throughput	of	
C/I
operations	per
clock	cycle.	For	example,	our	reference	machine	is	capable	of	performing
floating-point	multiplication	operations	at	a	rate	of	2	per	clock	cycle.	We
will	see	how	this	capability	can	be	exploited	to	increase	program
performance.
Circuit	designers	can	create	functional	units	with	wide	ranges	of
performance	characteristics.	Creating	a	unit	with	short	latency	or	with
pipelining	requires	more	hardware,	especially	for	more	complex	functions
such	as	multiplication	and	floating-point	operations.	Since	there	is	only	a
limited	amount	of	space	for	these	units	on	the	microprocessor	chip,	CPU
designers	must	carefully	balance	the	number	of	functional	units	and	their
individual	performance	to	achieve	optimal	overall	performance.	They
evaluate	many	different	benchmark	programs	and	dedicate	the	most
resources	to	the	most	critical	operations.	As	
Figure	
5.12
indicates,
integer	multiplication	and	floating-point	multiplication	and	addition	were
considered	important	operations	in	the	design	of	the	Core	i7	Haswell
processor,	even	though	a	significant	amount	of	hardware	is	required	to
achieve	the	low	latencies	and	high	degree	of	pipelining	shown.	On	the</p>
<p>other	hand,	division	is	relatively	infrequent	and	difficult	to	implement	with
either	short	latency	or	full	pipelining.
The	latencies,	issue	times,	and	capacities	of	these	arithmetic	operations
can	affect	the	performance	of	our	combining	functions.	We	can	express
these	effects	in	terms	of	two	fundamental	bounds	on	the	CPE	values:
Integer
Floating	point
Bound</p>
<ul>
<li></li>
</ul>
<ul>
<li></li>
</ul>
<ul>
<li></li>
</ul>
<ul>
<li></li>
</ul>
<p>Latency
1.00
3.00
3.00
5.00
Throughput
0.50
1.00
1.00
0.50
The	
latency	bound
gives	a	minimum	value	for	the	CPE	for	any	function
that	must	perform	the	combining	operation	in	a	strict	sequence.	The
throughput	bound
gives	a	minimum	bound	for	the	CPE	based	on	the
maximum	rate	at	which	the	functional	units	can	produce	results.	For
example,	since	there	is	only	one	integer	multiplier,	and	it	has	an	issue
time	of	1	clock	cycle,	the	processor	cannot	possibly	sustain	a	rate	of
more	than	1	multiplication	per	clock	cycle.	On	the	other	hand,	with	four
functional	units	capable	of	performing	integer	addition,	the	processor	can
potentially	sustain	a	rate	of	4	operations	per	cycle.	Unfortunately,	the
need	to	read	elements	from	memory	creates	an	additional	throughput
bound.	The	two	load	units	limit	the	processor	to	reading	at	most	2	data
values	per	clock	cycle,	yielding	a	throughput	bound	of	0.50.	We	will
demonstrate	the	effect	of	both	the	latency	and	throughput	bounds	with
different	versions	of	the	combining	functions.</p>
<p>5.7.3	
An	Abstract	Model	of
Processor	Operation
As	a	tool	for	analyzing	the	performance	of	a	machine-level	program
executing	on	a	modern	processor,	we	will	use	a	
data-flow
representation
of	programs,	a	graphical	notation	showing	how	the	data	dependencies
between	the	different	operations	constrain	the	order	in	which	they	are
executed.	These	constraints	then	lead	to	
critical	paths
in	the	graph,
putting	a	lower	bound	on	the	number	of	clock	cycles	required	to	execute
a	set	of	machine	instructions.
Before	proceeding	with	the	technical	details,	it	is	instructive	to	examine
the	CPE	measurements	obtained	for	function	
,	our	fastest	code
up	to	this	point:
Integer
Floating	point
Function
Page
Method</p>
<ul>
<li></li>
</ul>
<ul>
<li></li>
</ul>
<ul>
<li></li>
</ul>
<ul>
<li></li>
</ul>
<p>515
Accumulate	in	temporary
1.27
3.01
3.01
5.01
Latency	bound
1.00
3.00
3.00
5.00
Throughput	bound
0.50
1.00
1.00
0.50
We	can	see	that	these	measurements	match	the	latency	bound	for	the
processor,	except	for	the	case	of	integer	addition.	This	is	not	a
coincidence—it	indicates	that	the	performance	of	these	functions	is
dictated	by	the	latency	of	the	sum	or	product	computation	being</p>
<p>performed.	Computing	the	product	or	sum	of	
n
elements	requires	around
L	·	n
+	
K
clock	cycles,	where	
L
is	the	latency	of	the	combining	operation
and	
K
represents	the	overhead	of	calling	the	function	and	initiating	and
terminating	the	loop.	The	CPE	is	therefore	equal	to	the	latency	bound	
L
.
From	Machine-Level	Code	to	Data-Flow
Graphs
Our	data-flow	representation	of	programs	is	informal.	We	use	it	as	a	way
to	visualize	how	the	data	dependencies	in	a	program	dictate	its
performance.	We	present	the	data-flow	notation	by	working	with	
(
Figure	
5.10
)	as	an	example.	We	focus	just	on	the	computation
performed	by	the	loop,	since	this	is	the	dominating	factor	in	performance
for	large	vectors.	We	consider	the	case	of	data	type	
with
multiplication	as	the	combining	operation.	Other	combinations	of	data
type	and	operation	yield	similar	code.	The	compiled	code	for	this	loop
consists	of	four	instructions,	with	registers	
holding	a	pointer	to	the
i
th	element	of	array	data,	
holding	a	pointer	to	the	end	of	the	array,
and	
holding	the	accumulated	value	
.</p>
<p>Figure	
5.13	
Graphical	representation	of	inner-loop	code	for	
Instructions	are	dynamically	translated	into	one	or	two	operations,	each
of	which	receives	values	from	other	operations	or	from	registers	and
produces	values	for	other	operations	and	for	registers.	We	show	the
target	of	the	final	instruction	as	the	label	loop.	It	jumps	to	the	first
instruction	shown.
As	
Figure	
5.13
indicates,	with	our	hypothetical	processor	design,	the
four	instructions	are	expanded	by	the	instruction	decoder	into	a	series	of
five	
operations
,	with	the	initial	multiplication	instruction	being	expanded
into	a	load	operation	to	read	the	source	operand	from	memory,	and	a	mul
operation	to	perform	the	multiplication.
As	a	step	toward	generating	a	data-flow	graph	representation	of	the
program,	the	boxes	and	lines	along	the	left-hand	side	of	
Figure	
5.13
show	how	the	registers	are	used	and	updated	by	the	different	operations,
with	the	boxes	along	the	top	representing	the	register	values	at	the
beginning	of	the	loop,	and	those	along	the	bottom	representing	the
values	at	the	end.	For	example,	register	
is	only	used	as	a	source</p>
<p>value	by	the	
operation,	and	so	the	register	has	the	same	value	at	the
end	of	the	loop	as	at	the	beginning.	Register	
,	on	the	other	hand,	is
both	used	and	updated	within	the	loop.	Its	initial	value	is	used	by	the	load
and	add	operations;	its	new	value	is	generated	by	the	add	operation,
which	is	then	used	by	the	
operation.	Register	
is	also	updated
within	the	loop	by	the	mul	operation,	which	first	uses	the	initial	value	as	a
source	value.
Some	of	the	operations	in	
Figure	
5.13
produce	values	that	do	not
correspond	to	registers.	We	show	these	as	arcs	between	operations	on
the	right-hand	side.	The	load	operation	reads	a	value	from	memory	and
passes	it	directly	to	the	
operation.	Since	these	two	operations	arise
from	decoding	a	single	
instruction,	there	is	no	register	associated
with	the	intermediate	value	passing	between	them.	The	
operation
updates	the	condition	codes,	and	these	are	then	tested	by	the	
operation.
For	a	code	segment	forming	a	loop,	we	can	classify	the	registers	that	are
accessed	into	four	categories:</p>
<p>Figure	
5.14	
Abstracting	
operations	as	a	data-flow	graph.
We	rearrange	the	operators	of	
Figure	
5.13
to	more	clearly	show	the
data	dependencies	(a),	and	then	further	show	only	those	operations	that
use	values	from	one	iteration	to	produce	new	values	for	the	next	(b).
Read-only.	
These	are	used	as	source	values,	either	as	data	or	to
compute	memory	addresses,	but	they	are	not	modified	within	the
loop.	The	only	read	only	register	for	the	loop	in	
is	
.
Write-only.	
These	are	used	as	the	destinations	of	data-movement
operations.	There	are	no	such	registers	in	this	loop.
Local.	
These	are	updated	and	used	within	the	loop,	but	there	is	no
dependency	from	one	iteration	to	another.	The	condition	code
registers	are	examples	for	this	loop:	they	are	updated	by	the	
operation	and	used	by	the	
operation,	but	this	dependency	is
contained	within	individual	iterations.
Loop.	
These	are	used	both	as	source	values	and	as	destinations	for
the	loop,	with	the	value	generated	in	one	iteration	being	used	in
another.	We	can	see	that	
and	
are	loop	registers	for
,	corresponding	to	program	values	
and	
.
As	we	will	see,	the	chains	of	operations	between	loop	registers	determine
the	performance-limiting	data	dependencies.
Figure	
5.14
shows	further	refinements	of	the	graphical	representation
of	
Figure	
5.13
,	with	a	goal	of	showing	only	those	operations	and	data
dependencies	that	affect	the	program	execution	time.	We	see	in	
Figure
5.14(a)
that	we	rearranged	the	operators	to	show	more	clearly	the	flow
of	data	from	the	source	registers	at	the	top	(both	read-only	and	loop</p>
<p>registers)	and	to	the	destination	registers	at	the	bottom	(both	write-only
and	loop	registers).
In	
Figure	
5.14(a)
,	we	also	color	operators	white	if	they	are	not	part	of
some	chain	of	dependencies	between	loop	registers.	For	this	example,
the	comparison	(cmp)	and	branch	(jne)	operations	do	not	directly	affect
the	flow	of	data	in	the	program.	We	assume	that	the	instruction	control
unit	predicts	that	branch	will	be	taken,	and	hence	the	program	will
continue	looping.	The	purpose	of	the	compare	and	branch	operations	is
to	test	the	branch	condition	and	notify	the	ICU	if	it	is	
not	taken.	We
assume	this	checking	can	be	done	quickly	enough	that	it	does	not	slow
down	the	processor.
In	
Figure	
5.14(b)
,	we	have	eliminated	the	operators	that	were	colored
white	on	the	left,	and	we	have	retained	only	the	loop	registers.	What	we
have	left	is	an	abstract	template	showing	the	data	dependencies	that
form	among	loop	registers	due	to	one	iteration	of	the	loop.	We	can	see	in
this	diagram	that	there	are	two	data	dependencies	from	one	iteration	to
the	next.	Along	one	side,	we	see	the	dependencies	between	successive
values	of	program	value	
,	stored	in	register	
.	The	loop	computes
a	new	value	for	
by	multiplying	the	old	value	by	a	data	element,
generated	by	the	load	operation.	Along	the	other	side,	we	see	the
dependencies	between	successive	values	of	the	pointer	to	the	
i
th	data
element.	On	each	iteration,	the	old	value	is	used	as	the	address	for	the
load	operation,	and	it	is	also	incremented	by	the	add	operation	to
compute	its	new	value.
Figure	
5.15
shows	the	data-flow	representation	of	
n
iterations	by	the
inner	loop	of	function	
.	This	graph	was	obtained	by	simply</p>
<p>replicating	the	template	shown	in	
Figure	
5.14(b)
n
times.Wecan	see
that	the	program	has	two	chains	of	data
Figure	
5.15	
Data-flow	representation	of	computation	by	
n
iterations
of	the	inner	loop	of	
.
The	sequence	of	multiplication	operations	forms	a	critical	path	that	limits
program	performance.
dependencies,	corresponding	to	the	updating	of	program	values	
and
with	operations	mul	and	add,	respectively.	Given	that	floating-
point	multiplication	has	a	latency	of	5	cycles,	while	integer	addition	has	a
latency	of	1	cycle,	we	can	see	that	the	chain	on	the	left	will	form	a	
critical
path
,	requiring	5
n
cycles	to	execute.	The	chain	on	the	right	would	require</p>
<p>only	
n
cycles	to	execute,	and	so	it	does	not	limit	the	program
performance.
Figure	
5.15
demonstrates	why	we	achieved	a	CPE	equal	to	the
latency	bound	of	5	cycles	for	
,	when	performing	floating-point
multiplication.	When	executing	the	function,	the	floating-point	multiplier
becomes	the	limiting	resource.	The	other	operations	required	during	the
loop—manipulating	and	testing	pointer	value	
and	reading	data
from	memory—proceed	in	parallel	with	the	multiplication.	As	each
successive	value	of	
is	computed,	it	is	fed	back	around	to	compute
the	next	value,	but	this	will	not	occur	until	5	cycles	later.
The	flow	for	other	combinations	of	data	type	and	operation	are	identical
to	those	shown	in	
Figure	
5.15
,	but	with	a	different	data	operation
forming	the	chain	of	data	dependencies	shown	on	the	left.	For	all	of	the
cases	where	the	operation	has	a	latency	
L
greater	than	1,	we	see	that
the	measured	CPE	is	simply	
L
,	indicating	that	this	chain	forms	the
performance-limiting	critical	path.
Other	Performance	Factors
For	the	case	of	integer	addition,	on	the	other	hand,	our	measurements	of
show	a	CPE	of	1.27,	slower	than	the	CPE	of	1.00	we	would
predict	based	on	the	chains	of	dependencies	formed	along	either	the	left-
or	the	right-hand	side	of	the	graph	of	
Figure	
5.15
.	This	illustrates	the
principle	that	the	critical	paths	in	a	data-flow	representation	provide	only
a	
lower
bound	on	how	many	cycles	a	program	will	require.	Other	factors
can	also	limit	performance,	including	the	total	number	of	functional	units
available	and	the	number	of	data	values	that	can	be	passed	among	the</p>
<p>functional	units	on	any	given	step.	For	the	case	of	integer	addition	as	the
combining	operation,	the	data	operation	is	sufficiently	fast	that	the	rest	of
the	operations	cannot	supply	data	fast	enough.	Determining	exactly	why
the	program	requires	1.27	cycles	per	element	would	require	a	much	more
detailed	knowledge	of	the	hardware	design	than	is	publicly	available.
To	summarize	our	performance	analysis	of	
:	our	abstract	data-
flow	representation	of	program	operation	showed	that	
has	a
critical	path	of	length	
L	·	n
caused	by	the	successive	updating	of	program
value	
,	and	this	path	limits	the	CPE	to	at	least	
L
.	This	is	indeed	the
CPE	we	measure	for	all	cases	except	integer	addition,	which	has	a
measured	CPE	of	1.27	rather	than	the	CPE	of	1.00	we	would	expect	from
the	critical	path	length.
It	may	seem	that	the	latency	bound	forms	a	fundamental	limit	on	how	fast
our	combining	operations	can	be	performed.	Our	next	task	will	be	to
restructure	the	operations	to	enhance	instruction-level	parallelism.	We
want	to	transform	the	program	in	such	a	way	that	our	only	limitation
becomes	the	throughput	bound,	yielding	CPEs	below	or	close	to	1.00.
Practice	Problem	
5.5	
(solution	page	
575
)
Supposewewishtowriteafunctiontoevaluateapolynomial,	where	a
polynomial	of	degree	
n
is	defined	to	have	a	set	of	coefficients	
a
,
a
,	
a
,	.	.	.,	
a
.	For	a	value	
x
,	we	evaluate	the	polynomial	by
computing
0
1
2
n
a
0</p>
<ul>
<li></li>
</ul>
<p>a
1
x</p>
<ul>
<li></li>
</ul>
<p>a
2
x
2</p>
<ul>
<li></li>
</ul>
<p>…</p>
<ul>
<li></li>
</ul>
<p>a
n
x
n
(5.2)</p>
<p>This	evaluation	can	be	implemented	by	the	following	function,
having	as	arguments	an	array	of	coefficients	a,	a	value	
,	and	the
polynomial	degree	
(the	value	
n
in	Equation	5.2).	In	this
function,	we	compute	both	the	successive	terms	of	the	equation
and	the	successive	powers	of	
x
within	a	single	loop:
⁁
A
.	
For	degree	
n
,	how	many	additions	and	how	many
multiplications	does	this	code	perform?
B
.	
On	our	reference	machine,	with	arithmetic	operations
having	the	latencies	shown	in	
Figure	
5.12
,	we	measure
the	CPE	for	this	function	to	be	5.00.	Explain	how	this	CPE
arises	based	on	the	data	dependencies	formed	between
iterations	due	to	the	operations	implementing	lines	7-8	of
the	function.</p>
<p>Practice	Problem	
5.6	
(solution	page	
575
)
Let	us	continue	exploring	ways	to	evaluate	polynomials,	as
described	in	
Practice	Problem	
5.5
.	We	can	reduce	the	number
of	multiplications	in	evaluating	a	polynomial	by	applying	
Horner's
method
,	named	after	British	mathematician	William	G.	Horner
(1786-1837).	The	idea	is	to	repeatedly	factor	out	the	powers	of	
x
to	get	the	following	evaluation:
Using	Horner's	method,	we	can	implement	polynomial	evaluation
using	the	following	code:
A
.	
For	degree	
n
,	how	many	additions	and	how	many
multiplications	does	this	code	perform?
B
.	
On	our	reference	machine,	with	the	arithmetic	operations
having	the	latencies	shown	in	
Figure	
5.12
,	we	measure
the	CPE	for	this	function	to	be	8.00.	Explain	how	this	CPE
a
0</p>
<ul>
<li></li>
</ul>
<p>x
(
a
1</p>
<ul>
<li></li>
</ul>
<p>x
(
a
2</p>
<ul>
<li></li>
</ul>
<p>…</p>
<ul>
<li></li>
</ul>
<h2>x
(
a
n</h2>
<p>1</p>
<ul>
<li></li>
</ul>
<p>x
a
n
)
…
)
)
(5.3)</p>
<p>arises	based	on	the	data	dependencies	formed	between
iterations	due	to	the	operations	implementing	line	7	of	the
function.
C
.	
Explain	how	the	function	shown	in	
Practice	Problem	
5.5
can	run	faster,	even	though	it	requires	more	operations.</p>
<p>5.8	
Loop	Unrolling
Loop	unrolling	is	a	program	transformation	that	reduces	the	number	of
iterations	for	a	loop	by	increasing	the	number	of	elements	computed	on
each	iteration.	We	saw	an	example	of	this	with	the	function	
(
Figure
5.1
),	where	each	iteration	computes	two	elements	of	the	prefix	sum,
thereby	halving	the	total	number	of	iterations	required.	Loop	unrolling	can
improve	performance	in	two	ways.	First,	it	reduces	the	number	of
operations	that	do	not	contribute	directly	to	the	program	result,	such	as
loop	indexing	and	conditional	branching.	Second,	it	exposes	ways	in
which	we	can	further	transform	the	code	to	reduce	the	number	of
operations	in	the	critical	paths	of	the	overall	computation.	In	this	section,
we	will	examine	simple	loop	unrolling,	without	any	further
transformations.
Figure	
5.16
shows	a	version	of	our	combining	code	using	what	we	will
refer	to	as	&quot;2	×	1	loop	unrolling.&quot;	The	first	loop	steps	through	the	array
two	elements	at	a	time.	That	is,	the	loop	index	
is	incremented	by	2	on
each	iteration,	and	the	combining	operation	is	applied	to	array	elements	
i
and	
i
+	1	in	a	single	iteration.
In	general,	the	vector	length	will	not	be	a	multiple	of	2.	We	want	our	code
to	work	correctly	for	arbitrary	vector	lengths.	We	account	for	this
requirement	in	two	ways.	First,	we	make	sure	the	first	loop	does	not
overrun	the	array	bounds.	For	a	vector	of	length	
n
,	we	set	the	loop	limit	to
be	
n
−	1.	We	are	then	assured	that	the	loop	will	only	be	executed	when</p>
<p>the	loop	index	
i
satisfies	
i
&lt;	
n
−	1,	and	hence	the	maximum	array	index	
i</p>
<ul>
<li>1	will	satisfy	
i
<ul>
<li>1	&lt;	(
n
−	1)	+	1	=	
n
.
We	can	generalize	this	idea	to	unroll	a	loop	by	any	factor	
k
,	yielding	
k
×	1
loop	unrolling.
To	do	so,	we	set	the	upper	limit	to	be	
n
−	
k</li>
<li>1	and	within
the	loop	apply	the	combining	operation	to	elements	
i
through	
i</li>
<li></li>
</ul>
</li>
</ul>
<p>k
−	1.
Loop	index	
is	incremented	by	
k
in	each	iteration.	The	maximum	array
index	
i
+	
k
−	1	will	then	be	less	than	
n
.	We	include	the	second	loop	to
step	through	the	final	few	elements	of	the	vector	one	at	a	time.	The	body
of	this	loop	will	be	executed	between	0	and	
k
−	1	times.	For	
k
=	2,	we
could	use	a	simple	conditional	statement</p>
<p>Figure	
5.16	
Applying	2	×	1	loop	unrolling.
This	transformation	can	reduce	the	effect	of	loop	overhead.
to	optionally	add	a	final	iteration,	as	we	did	with	the	function	
(
Figure	
5.1
).	For	
k
&gt;	2,	the	finishing	cases	are	better	expressed	with	a
loop,	and	so	we	adopt	this	programming	convention	for	
k
=	2	as	well.	We
refer	to	this	transformation	as	&quot;
k
×	1	loop	unrolling,&quot;	since	we	unroll	by	a
factor	of	
k
but	accumulate	values	in	a	single	variable	
.
Practice	Problem	
5.7	
(solution	page
575
)
Modify	the	code	for	
to	unroll	the	loop	by	a	factor	
k
=	5.
When	we	measure	the	performance	of	unrolled	code	for	unrolling	factors
k
=	2	(
)	and	
k
=	3,	we	get	the	following	results:
Integer
Floating	point
Function
Page
Method</p>
<ul>
<li></li>
</ul>
<ul>
<li></li>
</ul>
<ul>
<li></li>
</ul>
<ul>
<li></li>
</ul>
<p>515
No	unrolling
1.27
3.01
3.01
5.01</p>
<p>532
2	×	1	unrolling
1.01
3.01
3.01
5.01
3	×	1	unrolling
1.01
3.01
3.01
5.01
Latency	bound
1.00
3.00
3.00
5.00
Throughput	bound
0.50
1.00
1.00
0.50
Figure	
5.17	
CPE	performance	for	different	degrees	of	
k
×	1	loop
unrolling.
Only	integer	addition	improves	with	this	transformation.
We	see	that	the	CPE	for	integer	addition	improves,	achieving	the	latency
bound	of	1.00.	This	result	can	be	attributed	to	the	benefits	of	reducing
loop	overhead	operations.	By	reducing	the	number	of	overhead
operations	relative	to	the	number	of	additions	required	to	compute	the
vector	sum,	we	can	reach	the	point	where	the	1-cycle	latency	of	integer
addition	becomes	the	performance-limiting	factor.	On	the	other	hand,
none	of	the	other	cases	improve—they	are	already	at	their	latency
bounds.	
Figure	
5.17
shows	CPE	measurements	when	unrolling	the
loop	by	up	to	a	factor	of	10.	We	see	that	the	trends	we	observed	for
unrolling	by	2	and	3	continue—none	go	below	their	latency	bounds.
To	understand	why	
k
×	1	unrolling	cannot	improve	performance	beyond
the	latency	bound,	let	us	examine	the	machine-level	code	for	the	inner</p>
<p>loop	of	
,	having	
k
=	2.	The	following	code	gets	generated	when
type	
is	double,	and	the	operation	is	multiplication:
We	can	see	that	
GCC</p>
<p>uses	a	more	direct	translation	of	the	array
referencing	seen	in	the	C	code,	compared	to	the	pointer-based	code
generated	for	
.
Loop	index	
is	held	in	register	
,	and	the
address	of	data	is	held	in	register	
.	As	before,	the	accumulated	value
is	held	in	vector	register	
.	The	loop	unrolling	leads	to	two
instructions—one	to	add	
to	
,	and
2.	
The	
GCC</p>
<p>optimizer	operates	by	generating	multiple	variants	of	a	function	and	then	choosing	one
that	it	predicts	will	yield	the	best	performance	and	smallest	code	size.	As	a	consequence,	small
changes	in	the	source	code	can	yield	widely	varying	forms	of	machine	code.	We	have	found	that
the	choice	of	pointer-based	or	array-based	code	has	no	impact	on	the	performance	of	programs
running	on	our	reference	machine.
2</p>
<p>Figure	
5.18	
Graphical	representation	of	inner-loop	code	for	
.
Each	iteration	has	two	
instructions,	each	of	which	is	translated
into	a	load	and	a	mul	operation.
Figure	
5.19	
Abstracting	
operations	as	a	data-flow	graph.
We	rearrange,	simplify,	and	abstract	the	representation	of	
Figure	
5.18
to	show	the	data	dependencies	between	successive	iterations	(a).	We
see	that	each	iteration	must	perform	two	multiplications	in	sequence	(b).</p>
<p>the	second	to	add	
to	
.	
Figure	
5.18
shows	a	graphical
representation	of	this	code.	The	
instructions	each	get	translated
into	two	operations:	one	to	load	an	array	element	from	memory	and	one
to	multiply	this	value	by	the	accumulated	value.	We	see	here	that	register
gets	read	and	written	twice	in	each	execution	of	the	loop.	We	can
rearrange,	simplify,	and	abstract	this	graph,	following	the	process	shown
in	
Figure	
5.19(a)
,	to	obtain	the	template	shown	in	
Figure	
5.19(b)
.
We	then	replicate	this	template	
n
/2	times	to	show	the	computation	for	a
vector	of	length	
n
,	obtaining	the	data-flow	representation</p>
<p>Figure	
5.20	
Data-flow	representation	of	
operating	on	a
vector	of	length	
n
.
Even	though	the	loop	has	been	unrolled	by	a	factor	of	2,	there	are	still	
n
mul	operations	along	the	critical	path.
shown	in	
Figure	
5.20
.	We	see	here	that	there	is	still	a	critical	path	of	
n
mul	operations	in	this	graph—there	are	half	as	many	iterations,	but	each
iteration	has	two	multiplication	operations	in	sequence.	Since	the	critical</p>
<p>path	was	the	limiting	factor	for	the	performance	of	the	code	without	loop
unrolling,	it	remains	so	with	
k
×	1	loop	unrolling.
Aside	
Getting	the	compiler	to	unroll
loops
Loop	unrolling	can	easily	be	performed	by	a	compiler.	Many
compilers	do	this	as	part	of	their	collection	of	optimizations.	
GCC
will	perform	some	forms	of	loop	unrolling	when	invoked	with
optimization	level	3	or	higher.</p>
<p>5.9	
Enhancing	Parallelism
At	this	point,	our	functions	have	hit	the	bounds	imposed	by	the	latencies
of	the	arithmetic	units.	As	we	have	noted,	however,	the	functional	units
performing	addition	and	multiplication	are	all	fully	pipelined,	meaning	that
they	can	start	new	operations	every	clock	cycle,	and	some	of	the
operations	can	be	performed	by	multiple	functional	units.	The	hardware
has	the	potential	to	perform	multiplications	and	additions	at	a	much
higher	rate,	but	our	code	cannot	take	advantage	of	this	capability,	even
with	loop	unrolling,	since	we	are	accumulating	the	value	as	a	single
variable	
.	We	cannot	compute	a	new	value	for	
until	the	preceding
computation	has	completed.	Even	though	the	functional	unit	computing	a
new	value	for	
can	start	a	new	operation	every	clock	cycle,	it	will	only
start	one	every	
L
cycles,	where	
L
is	the	latency	of	the	combining
operation.	We	will	now	investigate	ways	to	break	this	sequential
dependency	and	get	performance	better	than	the	latency	bound.
5.9.1	
Multiple	Accumulators
For	a	combining	operation	that	is	associative	and	commutative,	such	as
integer	addition	or	multiplication,	we	can	improve	performance	by	splitting
the	set	of	combining	operations	into	two	or	more	parts	and	combining	the
results	at	the	end.	For	example,	let	
P
denote	the	product	of	elements	
a
,
a
,	.	.	.,	
a
:
n
0
1
n
−1</p>
<h1>Assuming	
n
is	even,	we	can	also	write	this	as	
P
=	
PE
×	
PO
,	where	
PE
is	the	product	of	the	elements	with	even	indices,	and	
PO
is	the	product
of	the	elements	with	odd	indices:
Figure	
5.21
shows	code	that	uses	this	method.	It	uses	both	two-way
loop	unrolling,	to	combine	more	elements	per	iteration,	and	two-way
parallelism,	accumulating	elements	with	even	indices	in	variable	
and	elements	with	odd	indices	in	variable	
.	We	therefore	refer	to	this
as	&quot;2	×	2	loop	unrolling.&quot;	As	before,	we	include	a	second	loop	to
accumulate	any	remaining	array	elements	for	the	case	where	the	vector
length	is	not	a	multiple	of	2.	We	then	apply	the	combining	operation	to
and	
to	compute	the	final	result.
Comparing	loop	unrolling	alone	to	loop	unrolling	with	two-way	parallelism,
we	obtain	the	following	performance:
p
n</h1>
<h1>∏
i</h1>
<h2>0
n</h2>
<h1>1
a
i
n
n
n
n
n
P
E
n</h1>
<h1>∏
i</h1>
<h1>0
n
/
2
−
1
a
2
i
P
O
n</h1>
<h1>∏
i</h1>
<p>0
n
/
2
−
1
a
2
i</p>
<ul>
<li></li>
</ul>
<p>1</p>
<p>Figure	
5.21	
Applying	2	×	2	loop	unrolling.
By	maintaining	multiple	accumulators,	this	approach	can	make	better	use
of	the	multiple	functional	units	and	their	pipelining	capabilities.
Integer
Floating	point
Function
Page
Method</p>
<ul>
<li></li>
</ul>
<ul>
<li></li>
</ul>
<ul>
<li></li>
</ul>
<ul>
<li></li>
</ul>
<p>515
Accumulate	in	temporary
1.27
3.01
3.01
5.01
532
2	×	1	unrolling
1.01
3.01
3.01
5.01
537
2	×	2	unrolling
0.81
1.51
1.51
2.51
Latency	bound
1.00
3.00
3.00
5.00</p>
<p>Throughput	bound
0.50
1.00
1.00
0.50
We	see	that	we	have	improved	the	performance	for	all	cases,	with
integer	product,	floating-point	addition,	and	floating-point	multiplication
improving	by	a	factor	of	around	2,	and	integer	addition	improving
somewhat	as	well.	Most	significantly,	we	have	broken	through	the	barrier
imposed	by	the	latency	bound.	The	processor	no	longer	needs	to	delay
the	start	of	one	sum	or	product	operation	until	the	previous	one	has
completed.
To	understand	the	performance	of	
,	we	start	with	the	code	and
operation	sequence	shown	in	
Figure	
5.22
.	We	can	derive	a	template
showing	the
Figure	
5.22	
Graphical	representation	of	inner-loop	code	for	
.
Each	iteration	has	two	
instructions,	each	of	which	is	translated
into	a	load	and	a	mul	operation.</p>
<p>Figure	
5.23	
Abstracting	
operations	as	a	data-flow	graph.
We	rearrange,	simplify,	and	abstract	the	representation	of	
Figure	
5.22
to	show	the	data	dependencies	between	successive	iterations	(a).	We
see	that	there	is	no	dependency	between	the	two	mul	operations	(b).
data	dependencies	between	iterations	through	the	process	shown	in
Figure	
5.23
.	As	with	
,	the	inner	loop	contains	two	
operations,	but	these	instructions	translate	into	mul	operations	that	read
and	write	separate	registers,	with	no	data	dependency	between	them
(
Figure	
5.23(b)
).	We	then	replicate	this	template	
n
/2	times	(
Figure
5.24
),	modeling	the	execution	of	the	function	on	a	vector	of	length	
n
.
We	see	that	we	now	have	two	critical	paths,	one	corresponding	to
computing	the	product	of	even-numbered	elements	(program	value	
)
and</p>
<p>Figure	
5.24	
Data-flow	representation	of	
operating	on	a
vector	of	length	
n
.
We	now	have	two	critical	paths,	each	containing	
n
/2	operations.
one	for	the	odd-numbered	elements	(program	value	
).	Each	of	these
critical	paths	contains	only	
n
/2	operations,	thus	leading	to	a	CPE	of
around	5.00/2	=	2.50.	A	similar	analysis	explains	our	observed	CPE	of
around	
L
/2	for	operations	with	latency	
L
for	the	different	combinations	of
data	type	and	combining	operation.	Operationally,	the	programs	are
exploiting	the	capabilities	of	the	functional	units	to	increase	their</p>
<p>utilization	by	a	factor	of	2.	The	only	exception	is	for	integer	addition.	We
have	reduced	the	CPE	to	below	1.0,	but	there	is	still	too	much	loop
overhead	to	achieve	the	theoretical	limit	of	0.50.
We	can	generalize	the	multiple	accumulator	transformation	to	unroll	the
loop	by	a	factor	of	
k
and	accumulate	
k
values	in	parallel,	yielding	
k
×	
k
loop	unrolling.</p>
<p>Figure	
5.25
demonstrates	the	effect	of	applying	this
transformation	for	values	up	to	
k
=	10.	We	can	see	that,	for	sufficiently
large	values	of	
k
,	the	program	can
Figure	
5.25	
CPE	performance	of	
k
×	
k
loop	unrolling.
All	of	the	CPEs	improve	with	this	transformation,	achieving	near	or	at
their	throughput	bounds.
achieve	nearly	the	throughput	bounds	for	all	cases.	Integer	addition
achieves	a	CPE	of	0.54	with	
k
=	7,	close	to	the	throughput	bound	of	0.50
caused	by	the	two	load	units.	Integer	multiplication	and	floating-point
addition	achieve	CPEs	of	1.01	when	
k
≥	3,	approaching	the	throughput
bound	of	1.00	set	by	their	functional	units.	Floating-point	multiplication
achieves	a	CPE	of	0.51	for	
k
≥	10,	approaching	the	throughput	bound	of
0.50	set	by	the	two	floating-point	multipliers	and	the	two	load	units.	It	is
worth	noting	that	our	code	is	able	to	achieve	nearly	twice	the	throughput</p>
<p>with	floating-point	multiplication	as	it	can	with	floating-point	addition,	even
though	multiplication	is	a	more	complex	operation.
In	general,	a	program	can	achieve	the	throughput	bound	for	an	operation
only	when	it	can	keep	the	pipelines	filled	for	all	of	the	functional	units
capable	of	performing	that	operation.	For	an	operation	with	latency	
L
and
capacity	
C
,	this	requires	an	unrolling	factor	
k
≥	
C	·	L.
For	example,
floating-point	multiplication	has	
C
=	2	and	
L
=	5,	necessitating	an
unrolling	factor	of	
k
≥	10.	Floating-point	addition	has	
C
=	1	and	
L
=	3,
achieving	maximum	throughput	with	
k
≥	3.
In	performing	the	
k
×	
k
unrolling	transformation,	we	must	consider
whether	it	preserves	the	functionality	of	the	original	function.	We	have
seen	in	
Chapter	
2
that	two's-complement	arithmetic	is	commutative
and	associative,	even	when	overflow	occurs.	Hence,	for	an	integer	data
type,	the	result	computed	by	
will	be	identical	to	that	computed
by	
under	all	possible	conditions.	Thus,	an	optimizing	compiler
could	potentially	convert	the	code	shown	in	
first	to	a	two-way
unrolled	variant	of	
by	loop	unrolling,	and	then	to	that	of	
by	introducing	parallelism.	Some	compilers	do	either	this	or	similar
transformations	to	improve	performance	for	integer	data.
On	the	other	hand,	floating-point	multiplication	and	addition	are	not
associative.	Thus,	
and	
could	produce	different	results
due	to	rounding	or	overflow.	Imagine,	for	example,	a	product	computation
in	which	all	of	the	elements	with	even	indices	are	numbers	with	very	large
absolute	values,	while	those	with	odd	indices	are	very	close	to	0.0.	In
such	a	case,	product	
PE
might	overflow,	or	
PO
might	underflow,	even
though	computing	product	
P
proceeds	
normally.	In	most	real-life
n
n
n</p>
<p>applications,	however,	such	patterns	are	unlikely.	Since	most	physical
phenomena	are	continuous,	numerical	data	tend	to	be	reasonably
smooth	and	well	behaved.	Even	when	there	are	discontinuities,	they	do
not	generally	cause	periodic	patterns	that	lead	to	a	condition	such	as	that
sketched	earlier.	It	is	unlikely	that	multiplying	the	elements	in	strict	order
gives	fundamentally	better	accuracy	than	does	multiplying	two	groups
independently	and	then	multiplying	those	products	together.	For	most
applications,	achieving	a	performance	gain	of	2×	outweighs	the	risk	of
generating	different	results	for	strange	data	patterns.	Nevertheless,	a
program	developer	should	check	with	potential	users	to	see	if	there	are
particular	conditions	that	may	cause	the	revised	algorithm	to	be
unacceptable.	Most	compilers	do	not	attempt	such	transformations	with
floating-point	code,	since	they	have	no	way	to	judge	the	risks	of
introducing	transformations	that	can	change	the	program	behavior,	no
matter	how	small.
5.9.2	
Reassociation	Transformation
We	now	explore	another	way	to	break	the	sequential	dependencies	and
thereby	improve	performance	beyond	the	latency	bound.	We	saw	that	the
k
×	1	loop	unrolling	of	
did	not	change	the	set	of	operations
performed	in	combining	the	vector	elements	to	form	their	sum	or	product.
By	a	very	small	change	in	the	code,	however,	we	can	fundamentally
change	the	way	the	combining	is	performed,	and	also	greatly	increase
the	program	performance.
Figure	
5.26
shows	a	function	
that	differs	from	the	unrolled
code	of	
(
Figure	
5.16
)	only	in	the	way	the	elements	are</p>
<p>combined	in	the	inner	loop.	In	
,	the	combining	is	performed	by
the	statement
while	in	
it	is	performed	by	the	statement
differing	only	in	how	two	parentheses	are	placed.	We	call	this	a
reassociation	transformation
,	because	the	parentheses	shift	the	order	in
which	the	vector	elements	are	combined	with	the	accumulated	value	acc,
yielding	a	form	of	loop	unrolling	we	refer	to	as	&quot;2	×	1
a
.&quot;
To	an	untrained	eye,	the	two	statements	may	seem	essentially	the	same,
but	when	we	measure	the	CPE,	we	get	a	surprising	result:
Integer
Floating	point
Function
Page
Method</p>
<ul>
<li></li>
</ul>
<ul>
<li></li>
</ul>
<ul>
<li></li>
</ul>
<ul>
<li></li>
</ul>
<p>515
Accumulate	in	temporary
1.27
3.01
3.01
5.01
532
2	×	1	unrolling
1.01
3.01
3.01
5.01
537
2	×	2	unrolling
0.81
1.51
1.51
2.51</p>
<p>542
2	×	1
a
unrolling
1.01
1.51
1.51
2.51
Latency	bound
1.00
3.00
3.00
5.00
Throughput	bound
0.50
1.00
1.00
0.50
Figure	
5.26	
Applying	2	×	1
a
unrolling.
By	reassociating	the	arithmetic,	this	approach	increases	the	number	of
operations	that	can	be	performed	in	parallel.</p>
<p>The	integer	addition	case	matches	the	performance	of	
k
×	1	unrolling
(
),	while	the	other	three	cases	match	the	performance	of	the
versions	with	parallel	accumulators	(
),	doubling	the	performance
relative	to	
k
×	1	unrolling.	These	cases	have	broken	through	the	barrier
imposed	by	the	latency	bound.
Figure	
5.27
illustrates	how	the	code	for	the	inner	loop	of	
(for
the	case	of	multiplication	as	the	combining	operation	and	double	as	data
type)	gets	decoded	into	operations	and	the	resulting	data	dependencies.
We	see	that	the	load	operations	resulting	from	the	
and	the	first
instructions	load	vector	elements	
i
and	
i
+	1	from	memory,	and	the
first	mul	operation	multiplies	them	together.	The	second	mul	operation
then	multiples	this	result	by	the	accumulated	value	
.	
Figure	
5.28(a)
shows	how	we	rearrange,	refine,	and	abstract	the	operations	of	
Figure
5.27
to	get	a	template	representing	the	data	dependencies	for	one
iteration	(
Figure	
5.28(b)
).	As	with	the	templates	for	
and
,	we	have	two	load	and	two	mul	operations,	but	only	one	of	the
mul	operations	forms	a	data-dependency	chain	between	loop	registers.
When	we	then	replicate	this	template	
n
/2	times	to	show	the	computations
performed	in	multiplying	
n
vector	elements	(
Figure	
5.29
),	we	see	that
we	only	have	
n
/2	operations	along	the	critical	path.	The	first	multiplication
within	each	iteration	can	be	performed	without	waiting	for	the
accumulated	value	from	the	previous	iteration.	Thus,	we	reduce	the
minimum	possible	CPE	by	a	factor	of	around	2.</p>
<p>Figure	
5.27	
Graphical	representation	of	inner-loop	code	for	
.
Each	iteration	gets	decoded	into	similar	operations	as	for	
or
,	but	with	different	data	dependencies.
Figure	
5.28	
Abstracting	
operations	as	a	data-flow	graph.
We	rearrange,	simplify,	and	abstract	the	representation	of	
Figure	
5.27
to	show	the	data	dependencies	between	successive	iterations.	The	upper
mul	operation	multiplies	two	2-vector	elements	with	each	other,	while	the
lower	one	multiplies	the	result	by	loop	variable	
.</p>
<p>Figure	
5.29	
Data-flow	representation	of	
operating	on	a
vector	of	length	
n
.
We	have	a	single	critical	path,	but	it	contains	only	
n
/2	operations.
Figure	
5.30
demonstrates	the	effect	of	applying	the	reassociation
transformation	to	achieve	what	we	refer	to	as	
k
×	1
a	loop	unrolling
for
values	up	to	
k
=	10.	We	can	see	that	this	transformation	yields
performance	results	similar	to	what	is	achieved	by	maintaining	
k
separate</p>
<p>accumulators	with	
k
×	
k
unrolling.	In	all	cases,	we	come	close	to	the
throughput	bounds	imposed	by	the	functional	units.
In	performing	the	reassociation	transformation,	we	once	again	change
the	order	in	which	the	vector	elements	will	be	combined	together.	For
integer	addition	and	multiplication,	the	fact	that	these	operations	are
associative	implies	that	this	reordering	will	have	no	effect	on	the	result.
For	the	floating-point	cases,	we	must	once	again	assess	whether	this
reassociation	is	likely	to	significantly	affect
Figure	
5.30	
CPE	performance	for	
k
×	1
a
loop	unrolling.
All	of	the	CPEs	improve	with	this	transformation,	nearly	approaching	their
throughput	bounds.
the	outcome.	We	would	argue	that	the	difference	would	be	immaterial	for
most	applications.
In	summary,	a	reassociation	transformation	can	reduce	the	number	of
operations	along	the	critical	path	in	a	computation,	resulting	in	better
performance	by	better	utilizing	the	multiple	functional	units	and	their
pipelining	capabilities.	Most	compilers	will	not	attempt	any	reassociations
of	floating-point	operations,	since	these	operations	are	not	guaranteed	to</p>
<p>be	associative.	Current	versions	of	
GCC</p>
<p>do	perform	reassociations	of
integer	operations,	but	not	always	with	good	effects.	In	general,	we	have
found	that	unrolling	a	loop	and	accumulating	multiple	values	in	parallel	is
a	more	reliable	way	to	achieve	improved	program	performance.
Practice	Problem	
5.8	
(solution	page	
576
)
Consider	the	following	function	for	computing	the	product	of	an
array	of	
n
double-precision	numbers.	We	have	unrolled	the	loop	by
a	factor	of	3.
For	the	line	labeled	&quot;Product	computation,&quot;	we	can	use
parentheses	to	create	five	different	associations	of	the
computation,	as	follows:</p>
<p>Assume	we	run	these	functions	on	a	machine	where	floating-point
multiplication	has	a	latency	of	5	clock	cycles.	Determine	the	lower
bound	on	the	CPE	set	by	the	data	dependencies	of	the
multiplication.	(
Hint:
It	helps	to	draw	a	data-flow	representation	of
how	
is	computed	on	every	iteration.)
Web	Aside	OPT:SIMD	
Achieving
greater	parallelism	with	vector
instructions
As	described	in	
Section	
3.1
,	Intel	introduced	the	SSE
instructions	in	1999,	where	SSE	is	the	acronym	for	&quot;streaming
SIMD	extensions&quot;	and,	in	turn,	SIMD	(pronounced	&quot;sim-dee&quot;)	is
the	acronym	for	&quot;single	instruction,	multiple	data.&quot;	The	SSE
capability	has	gone	through	multiple	generations,	with	more	recent
versions	being	named	
advanced	vector	extensions
,	or	AVX.	The
SIMD	execution	model	involves	operating	on	entire	vectors	of	data
within	single	instructions.	These	vectors	are	held	in	a	special	set
of	
vector	registers
,	named	
.	Current	AVX	vector</p>
<p>registers	are	32	bytes	long,	and	therefore	each	can	hold	eight	32-
bit	numbers	or	four	64-bit	numbers,	where	the	numbers	can	be
either	integer	or	floating-point	values.	AVX	instructions	can	then
perform	vector	operations	on	these	registers,	such	as	adding	or
multiplying	eight	or	four	sets	of	values	in	parallel.	For	example,	if
YMM	register	
contains	eight	single-precision	floating-point
numbers,	which	we	denote	
a
,	.	.	.,	
a
,	and	
contains	the
memory	address	of	a	sequence	of	eight	single-precision	floating-
point	numbers,	which	we	denote	
b
,	.	.	.,	
b
,	then	the	instruction
will	read	the	eight	values	from	memory	and	perform	eight
multiplications	in	parallel,	computing	
a
←	
a
·	b
,	for	0	&lt;	
i
≤	7	and
storing	the	resulting	eight	products	in	vector	register	
.	We
see	that	a	single	instruction	is	able	to	generate	a	computation	over
multiple	data	values,	hence	the	term	&quot;SIMD.&quot;
GCC</p>
<p>supports	extensions	to	the	C	language	that	let	programmers
express	a	program	in	terms	of	vector	operations	that	can	be
compiled	into	the	vector	instructions	of	AVX	(as	well	as	code
based	on	the	earlier	SSE	instructions).	This	coding	style	is
preferable	to	writing	code	directly	in	assembly	language,	since	
GCC
can	also	generate	code	for	the	vector	instructions	found	on	other
processors.
Using	a	combination	of	
GCC</p>
<p>instructions,	loop	unrolling,	and
multiple	accumulators,	we	are	able	to	achieve	the	following
0
7
0
7
i
i
i</p>
<p>performance	for	our	combining	functions:
Integer
Floating	point
int
long
int
long
Method</p>
<ul>
<li></li>
</ul>
<ul>
<li></li>
</ul>
<ul>
<li></li>
</ul>
<ul>
<li></li>
</ul>
<ul>
<li></li>
</ul>
<ul>
<li></li>
</ul>
<ul>
<li></li>
</ul>
<ul>
<li></li>
</ul>
<p>Scalar	10	×	10
0.54
1.01
0.55
1.00
1.01
0.51
1.01
0.52
Scalar	throughput
bound
0.50
0.50
1.00
1.00
1.00
1.00
0.50
0.50
Vector	8	×	8
0.05
0.24
0.13
1.51
0.12
0.08
0.25
0.16
Vector	throughput
bound
0.06
0.12
0.12
—
0.12
0.06
0.25
0.12
In	this	chart,	the	first	set	of	numbers	is	for	conventional,	
scalar
code	written	in	the	style	of	
,	unrolling	by	a	factor	of	10
and	maintaining	10	accumulators.	The	second	set	of	numbers	is
for	code	written	in	a	form	that	
GCC</p>
<p>can	compile	into	AVX	vector
code.	In	addition	to	using	vector	operations,	this	version	unrolls
the	main	loop	by	a	factor	of	8	and	maintains	eight	separate	vector
accumulators.	We	show	results	for	both	32-bit	and	64-bit
numbers,	since	the	vector	instructions	achieve	8-way	parallelism
in	the	first	case,	but	only	4-way	parallelism	in	the	second.
We	can	see	that	the	vector	code	achieves	almost	an	eightfold
improvement	on	the	four	32-bit	cases,	and	a	fourfold	improvement
on	three	of	the	four	64-bit	cases.	Only	the	long	integer
multiplication	code	does	not	perform	well	when	we	attempt	to
express	it	in	vector	code.	The	AVX	instruction	set	does	not	include</p>
<p>one	to	do	parallel	multiplication	of	64-bit	integers,	and	so	
GCC
cannot	generate	vector	code	for	this	case.	Using	vector
instructions	creates	a	new	throughput	bound	for	the	combining
operations.	These	are	eight	times	lower	for	32-bit	operations	and
four	times	lower	for	64-bit	operations	than	the	scalar	limits.	Our
code	comes	close	to	achieving	these	bounds	for	several
combinations	of	data	type	and	operation.</p>
<p>5.10	
Summary	of	Results	for
Optimizing	Combining	Code
Our	efforts	at	maximizing	the	performance	of	a	routine	that	adds	or
multiplies	the	elements	of	a	vector	have	clearly	paid	off.	The	following
summarizes	the	results	we	obtain	with	
scalar
code,	not	making	use	of	the
vector	parallelism	provided	by	AVX	vector	instructions:
Integer
Floating	point
Function
Page
Method</p>
<ul>
<li></li>
</ul>
<ul>
<li></li>
</ul>
<ul>
<li></li>
</ul>
<ul>
<li></li>
</ul>
<p>507
Abstract	
10.12
10.12
10.17
11.14
537
2	×	2	unrolling
0.81
1.51
1.51
2.51
10	×	10	unrolling
0.55
1.00
1.01
0.52
Latency	bound
1.00
3.00
3.00
5.00
Throughput	bound
0.50
1.00
1.00
0.50
By	using	multiple	optimizations,	we	have	been	able	to	achieve	CPEs
close	to	the	throughput	bounds	of	0.50	and	1.00,	limited	only	by	the
capacities	of	the	functional	units.	These	represent	10−20×	improvements
on	the	original	code.	This	has	all	been	done	using	ordinary	C	code	and	a
standard	compiler.	Rewriting	the	code	to	take	advantage	of	the	newer
SIMD	instructions	yields	additional	performance	gains	of	nearly	4×	or	8×.
For	example,	for	single-precision	multiplication,	the	CPE	drops	from	the</p>
<p>original	value	of	11.14	down	to	0.06,	an	overall	performance	gain	of	over
180×.	This	example	demonstrates	that	modern	processors	have
considerable	amounts	of	computing	power,	but	we	may	need	to	coax	this
power	out	of	them	by	writing	our	programs	in	very	stylized	ways.</p>
<p>5.11	
Some	Limiting	Factors
We	have	seen	that	the	critical	path	in	a	data-flow	graph	representation	of
a	program	indicates	a	fundamental	lower	bound	on	the	time	required	to
execute	a	program.	That	is,	if	there	is	some	chain	of	data	dependencies
in	a	program	where	the	sum	of	all	of	the	latencies	along	that	chain	equals
T
,	then	the	program	will	require	at	least	
T
cycles	to	execute.
We	have	also	seen	that	the	throughput	bounds	of	the	functional	units
also	impose	a	lower	bound	on	the	execution	time	for	a	program.	That	is,
assume	that	a	program	requires	a	total	of	
N
computations	of	some
operation,	that	the	microprocessor	has	
C
functional	units	capable	of
performing	that	operation,	and	that	these	units	have	an	issue	time	of	
I
.
Then	the	program	will	require	at	least	
N	·	I/C
cycles	to	execute.
In	this	section,	we	will	consider	some	other	factors	that	limit	the
performance	of	programs	on	actual	machines.
5.11.1	
Register	Spilling
The	benefits	of	loop	parallelism	are	limited	by	the	ability	to	express	the
computation	in	assembly	code.	If	a	program	has	a	degree	of	parallelism
P
that	exceeds	the	number	of	available	registers,	then	the	compiler	will
resort	to	
spilling
,	storing	some	of	the	temporary	values	in	memory,
typically	by	allocating	space	on	the	run-time	stack.	As	an	example,	the</p>
<p>following	measurements	compare	the	result	of	extending	the	multiple
accumulator	scheme	of	
to	the	cases	of	
k
=	10	and	
k
=	20:
Integer
Floating	point
Function
Page
Method</p>
<ul>
<li></li>
</ul>
<ul>
<li></li>
</ul>
<ul>
<li></li>
</ul>
<ul>
<li></li>
</ul>
<p>537
10	×	10	unrolling
0.55
1.00
1.01
0.52
20	×	20	unrolling
0.83
1.03
1.02
0.68
Throughput	bound
0.50
1.00
1.00
0.50
We	can	see	that	none	of	the	CPEs	improve	with	this	increased	unrolling,
and	some	even	get	worse.	Modern	x86-64	processors	have	16	integer
registers	and	can	make	use	of	the	16	YMM	registers	to	store	floating-
point	data.	Once	the	number	of	loop	variables	exceeds	the	number	of
available	registers,	the	program	must	allocate	some	on	the	stack.
As	an	example,	the	following	snippet	of	code	shows	how	accumulator
is	updated	in	the	inner	loop	of	the	code	with	10	×	10	unrolling:
We	can	see	that	the	accumulator	is	kept	in	register	
,	and	so	the
program	can	simply	read	
from	memory	and	multiply	it	by	this</p>
<p>register.
The	comparable	part	of	the	code	for	20	×	20	unrolling	has	a	much
different	form:
The	accumulator	is	kept	as	a	local	variable	on	the	stack,	at	offset	40	from
the	stack	pointer.	The	program	must	read	both	its	value	and	the	value	of
from	memory,	multiply	them,	and	store	the	result	back	to
memory.
Once	a	compiler	must	resort	to	register	spilling,	any	advantage	of
maintaining	multiple	accumulators	will	most	likely	be	lost.	Fortunately,
x86-64	has	enough	registers	that	most	loops	will	become	throughput
limited	before	this	occurs.
5.11.2	
Branch	Prediction	and
Misprediction	Penalties
We	demonstrated	via	experiments	in	
Section	
3.6.6
that	a	conditional
branch	can	incur	a	significant	
misprediction	penalty
when	the	branch</p>
<p>prediction	logic	does	not	correctly	anticipate	whether	or	not	a	branch	will
be	taken.	Now	that	we	have	learned	something	about	how	processors
operate,	we	can	understand	where	this	penalty	arises.
Modern	processors	work	well	ahead	of	the	currently	executing
instructions,	reading	new	instructions	from	memory	and	decoding	them	to
determine	what	operations	to	perform	on	what	operands.	This	
instruction
pipelining
works	well	as	long	as	the	instructions	follow	in	a	simple
sequence.	When	a	branch	is	encountered,	the	processor	must	guess
which	way	the	branch	will	go.	For	the	case	of	a	conditional	jump,	this
means	predicting	whether	or	not	the	branch	will	be	taken.	For	an
instruction	such	as	an	indirect	jump	(as	we	saw	in	the	code	to	jump	to	an
address	specified	by	a	jump	table	entry)	or	a	procedure	return,	this
means	predicting	the	target	address.	In	this	discussion,	we	focus	on
conditional	branches.
In	a	processor	that	employs	
speculative	execution
,	the	processor	begins
executing	the	instructions	at	the	predicted	branch	target.	It	does	this	in	a
way	that	avoids	modifying	any	actual	register	or	memory	locations	until
the	actual	outcome	has	been	determined.	If	the	prediction	is	correct,	the
processor	can	then	
&quot;commit&quot;	the	results	of	the	speculatively	executed
instructions	by	storing	them	in	registers	or	memory.	If	the	prediction	is
incorrect,	the	processor	must	discard	all	of	the	speculatively	executed
results	and	restart	the	instruction	fetch	process	at	the	correct	location.
The	misprediction	penalty	is	incurred	in	doing	this,	because	the
instruction	pipeline	must	be	refilled	before	useful	results	are	generated.
We	saw	in	
Section	
3.6.6
that	recent	versions	of	x86	processors,
including	all	processors	capable	of	executing	x86-64	programs,	have
conditional	move
instructions.	
GCC</p>
<p>can	generate	code	that	uses	these</p>
<p>instructions	when	compiling	conditional	statements	and	expressions,
rather	than	the	more	traditional	realizations	based	on	conditional
transfers	of	control.	The	basic	idea	for	translating	into	conditional	moves
is	to	compute	the	values	along	both	branches	of	a	conditional	expression
or	statement	and	then	use	conditional	moves	to	select	the	desired	value.
We	saw	in	
Section	
4.5.7
that	conditional	move	instructions	can	be
implemented	as	part	of	the	pipelined	processing	of	ordinary	instructions.
There	is	no	need	to	guess	whether	or	not	the	condition	will	hold,	and
hence	no	penalty	for	guessing	incorrectly.
How,	then,	can	a	C	programmer	make	sure	that	branch	misprediction
penalties	do	not	hamper	a	program's	efficiency?	Given	the	19-cycle
misprediction	penalty	we	measured	for	the	reference	machine,	the	stakes
are	very	high.	There	is	no	simple	answer	to	this	question,	but	the
following	general	principles	apply.
Do	Not	Be	Overly	Concerned	about
Predictable	Branches
We	have	seen	that	the	effect	of	a	mispredicted	branch	can	be	very	high,
but	that	does	not	mean	that	all	program	branches	will	slow	a	program
down.	In	fact,	the	branch	prediction	logic	found	in	modern	processors	is
very	good	at	discerning	regular	patterns	and	long-term	trends	for	the
different	branch	instructions.	For	example,	the	loop-closing	branches	in
our	combining	routines	would	typically	be	predicted	as	being	taken,	and
hence	would	only	incur	a	misprediction	penalty	on	the	last	time	around.
As	another	example,	consider	the	results	we	observed	when	shifting	from
to	
,	when	we	took	the	function	
out	of</p>
<p>the	inner	loop	of	the	function,	as	is	reproduced	below:
Integer
Floating	point
Function
Page
Method</p>
<ul>
<li></li>
</ul>
<ul>
<li></li>
</ul>
<ul>
<li></li>
</ul>
<ul>
<li></li>
</ul>
<p>509
Move	
7.02
9.03
9.02
11.03
513
Direct	data	access
7.17
9.02
9.02
11.03
The	CPE	did	not	improve,	even	though	the	transformation	eliminated	two
conditionals	on	each	iteration	that	check	whether	the	vector	index	is
within	bounds.	For	this	function,	the	checks	always	succeed,	and	hence
they	are	highly	predictable.
As	a	way	to	measure	the	performance	impact	of	bounds	checking,
consider	the	following	combining	code,	where	we	have	modified	the	inner
loop	of	
by	replacing	the	access	to	the	data	element	with	the
result	of	performing	an	inline	substitution	of	the	code	for	
.
We	will	call	this	new	version	
.	This	code	performs	bounds
checking	and	also	references	the	vector	elements	through	the	vector
data	structure.</p>
<p>We	can	then	directly	compare	the	CPE	for	the	functions	with	and	without
bounds	checking:
Integer
Floating	point
Function
Page
Method</p>
<ul>
<li></li>
</ul>
<ul>
<li></li>
</ul>
<ul>
<li></li>
</ul>
<ul>
<li></li>
</ul>
<p>515
No	bounds	checking
1.27
3.01
3.01
5.01
515
Bounds	checking
2.02
3.01
3.01
5.01
The	version	with	bounds	checking	is	slightly	slower	for	the	case	of	integer
addition,	but	it	achieves	the	same	performance	for	the	other	three	cases.
The	performance	of	these	cases	is	limited	by	the	latencies	of	their
respective	combining	operations.	The	additional	computation	required	to
perform	bounds	checking	can	take	place	in	parallel	with	the	combining
operations.	The	processor	is	able	to	predict	the	outcomes	of	these
branches,	and	so	none	of	this	evaluation	has	much	effect	on	the	fetching
and	processing	of	the	instructions	that	form	the	critical	path	in	the
program	execution.</p>
<p>Write	Code	Suitable	for	Implementation	with
Conditional	Moves
Branch	prediction	is	only	reliable	for	regular	patterns.	Many	tests	in	a
program	are	completely	unpredictable,	dependent	on	arbitrary	features	of
the	data,	such	as	whether	a	number	is	negative	or	positive.	For	these,
the	branch	prediction	logic	will	do	very	poorly.	For	inherently
unpredictable	cases,	program	performance	can	be	greatly	enhanced	if
the	compiler	is	able	to	generate	code	using	conditional	data	transfers
rather	than	conditional	control	transfers.	This	cannot	be	controlled	directly
by	the	C	programmer,	but	some	ways	of	expressing	conditional	behavior
can	be	more	directly	translated	into	conditional	moves	than	others.
We	have	found	that	
GCC</p>
<p>is	able	to	generate	conditional	moves	for	code
written	in	a	more	&quot;functional&quot;	style,	where	we	use	conditional	operations
to	compute	
values	and	then	update	the	program	state	with	these	values,
as	opposed	to	a	more	&quot;imperative&quot;	style,	where	we	use	conditionals	to
selectively	update	program	state.
There	are	no	strict	rules	for	these	two	styles,	and	so	we	illustrate	with	an
example.	Suppose	we	are	given	two	arrays	of	integers	
a
and	
b
,	and	at
each	position	
i
,	we	want	to	set	
to	the	minimum	of	
and	
,
and	
to	the	maximum.
An	imperative	style	of	implementing	this	function	is	to	check	at	each
position	
i
and	swap	the	two	elements	if	they	are	out	of	order:</p>
<p>Our	measurements	for	this	function	show	a	CPE	of	around	13.5	for
random	data	and	2.5-3.5	for	predictable	data,	an	indication	of	a
misprediction	penalty	of	around	20	cycles.
A	functional	style	of	implementing	this	function	is	to	compute	the
minimum	and	maximum	values	at	each	position	
i
and	then	assign	these
values	to	
and	
,	respectively:</p>
<p>Our	measurements	for	this	function	show	a	CPE	of	around	4.0	regardless
of	whether	the	data	are	arbitrary	or	predictable.	(We	also	examined	the
generated	assembly	code	to	make	sure	that	it	indeed	uses	conditional
moves.)
As	discussed	in	
Section	
3.6.6
,	not	all	conditional	behavior	can	be
implemented	with	conditional	data	transfers,	and	so	there	are	inevitably
cases	where	programmers	cannot	avoid	writing	code	that	will	lead	to
conditional	branches	for	which	the	processor	will	do	poorly	with	its	branch
prediction.	But,	as	we	have	shown,	a	little	cleverness	on	the	part	of	the
programmer	can	sometimes	make	code	more	amenable	to	translation
into	conditional	data	transfers.	This	requires	some	amount	
of
experimentation,	writing	different	versions	of	the	function	and	then
examining	the	generated	assembly	code	and	measuring	performance.
Practice	Problem	
5.9	
(solution	page	
576
)
The	traditional	implementation	of	the	merge	step	of	mergesort
requires	three	loops	[
98
]:</p>
<p>The	branches	caused	by	comparing	variables	
and	
to	n	have
good	prediction	performance—the	only	mispredictions	occur	when
they	first	become	false.	The	comparison	between	values	
and	
(line	6),	on	the	other	hand,	is	highly	unpredictable	for
typical	data.	This	comparison	controls	a	conditional	branch,
yielding	a	CPE	(where	the	number	of	elements	is	2
n
)	of	around
15.0	when	run	on	random	data.
Rewrite	the	code	so	that	the	effect	of	the	conditional	statement	in
the	first	loop	(lines	6-9)	can	be	implemented	with	a	conditional
move.</p>
<p>5.12	
Understanding	Memory
Performance
All	of	the	code	we	have	written	thus	far,	and	all	the	tests	we	have	run,
access	relatively	small	amounts	of	memory.	For	example,	the	combining
routines	were	measured	over	vectors	of	length	less	than	1,000	elements,
requiring	no	more	than	8,000	bytes	of	data.	All	modern	processors
contain	one	or	more	
cache
memories	to	provide	fast	access	to	such
small	amounts	of	memory.	In	this	section,	we	will	further	investigate	the
performance	of	programs	that	involve	load	(reading	from	memory	into
registers)	and	store	(writing	from	registers	to	memory)	operations,
considering	only	the	cases	where	all	data	are	held	in	cache.	In	
Chapter
6
,	we	go	into	much	more	detail	about	how	caches	work,	their
performance	characteristics,	and	how	to	write	code	that	makes	best	use
of	caches.
As	
Figure	
5.11
shows,	modern	processors	have	dedicated	functional
units	to	perform	load	and	store	operations,	and	these	units	have	internal
buffers	to	hold	sets	of	outstanding	requests	for	memory	operations.	For
example,	our	reference	machine	has	two	load	units,	each	of	which	can
holdup	to	72	pending	read	requests.	It	has	a	single	store	unit	with	a	store
buffer	containing	up	to	42	write	requests.	Each	of	these	units	can	initiate
1	operation	every	clock	cycle.
5.12.1	
Load	Performance</p>
<p>The	performance	of	a	program	containing	load	operations	depends	on
both	the	pipelining	capability	and	the	latency	of	the	load	unit.	In	our
experiments	with	combining	operations	using	our	reference	machine,	we
saw	that	the	CPE	never	got	below	0.50	for	any	combination	of	data	type
and	combining	operation,	except	when	using	SIMD	operations.	One
factor	limiting	the	CPE	for	our	examples	is	that	they	all	require	reading
one	value	from	memory	for	each	element	computed.	With	two	load	units,
each	able	to	initiate	at	most	1	load	operation	every	clock	cycle,	the	CPE
cannot	be	less	than	0.50.	For	applications	where	we	must	load	
k
values
for	every	element	computed,	we	can	never	achieve	a	CPE	lower	than	
k
/2
(see,	for	example,	
Problem	
5.15
).
In	our	examples	so	far,	we	have	not	seen	any	performance	effects	due	to
the	latency	of	load	operations.	The	addresses	for	our	load	operations
depended	only	on	the	loop	index	
i
,	and	so	the	load	operations	did	not
form	part	of	a	performance-limiting	critical	path.
To	determine	the	latency	of	the	load	operation	on	a	machine,	we	can	set
up	a	computation	with	a	sequence	of	load	operations,	where	the	outcome
of	one	determines	the	address	for	the	next.	As	an	example,	consider	the
function	
in	
Figure	
5.31
,	which	computes	the	length	of	a
linked	list.	In	the	loop	of	this	function,	each	successive	value	of	variable
depends	on	the	value	read	by	the	pointer	reference	
.	Our
measurements	show	that	function	
has</p>
<p>Figure	
5.31	
Linked	list	function.
Its	performance	is	limited	by	the	latency	of	the	load	operation.
a	CPE	of	4.00,	which	we	claim	is	a	direct	indication	of	the	latency	of	the
load	operation.	To	see	this,	consider	the	assembly	code	for	the	loop:
The	
instruction	on	line	3	forms	the	critical	bottleneck	in	this	loop.
Each	successive	value	of	register	
depends	on	the	result	of	a	load</p>
<p>operation	having	the	value	in	
as	its	address.	Thus,	the	load
operation	for	one	iteration	cannot	begin	until	the	one	for	the	previous
iteration	has	completed.	The	CPE	of	4.00	for	this	function	is	determined
by	the	latency	of	the	load	operation.	Indeed,	this	measurement	matches
the	documented	access	time	of	4	cycles	for	the	reference	machine's	L1
cache,	as	is	discussed	in	
Section	
6.4
.
5.12.2	
Store	Performance
In	all	of	our	examples	thus	far,	we	analyzed	only	functions	that	reference
memory	mostly	with	load	operations,	reading	from	a	memory	location	into
a	register.	Its	counterpart,	the	
store
operation,	writes	a	register	value	to
memory.	The	performance	of	this	operation,	particularly	in	relation	to	its
interactions	with	load	operations,	involves	several	subtle	issues.
As	with	the	load	operation,	in	most	cases,	the	store	operation	can
operate	in	a	fully	pipelined	mode,	beginning	a	new	store	on	every	cycle.
For	example,	consider	the	function	shown	in	
Figure	
5.32
that	sets	the
elements	of	an	array	
of	length	
to	zero.	Our	measurements	show
a	CPE	of	1.0.	This	is	the	best	we	can	achieve	on	a	machine	with	a	single
store	functional	unit.
Unlike	the	other	operations	we	have	considered	so	far,	the	store
operation	does	not	affect	any	register	values.	Thus,	by	their	very	nature,
a	series	of	store	operations	cannot	create	a	data	dependency.	Only	a
load	operation	is	affected	by	the	result	of	a	store	operation,	since	only	a
load	can	read	back	the	memory	value	that	has	been	written	by	the	store.
The	function	
shown	in	
Figure	
5.33</p>
<p>Figure	
5.32	
Function	to	set	array	elements	to	0.
This	code	achieves	a	CPE	of	1.0.</p>
<p>Figure	
5.33	
Code	to	write	and	read	memory	locations,	along	with
illustrative	executions.
This	function	highlights	the	interactions	between	stores	and	loads	when
arguments	
and	
are	equal.
illustrates	the	potential	interactions	between	loads	and	stores.	This	figure
also	shows	two	example	executions	of	this	function,	when	it	is	called	for	a
two-element	array	a,	with	initial	contents	−10	and	17,	and	with	argument
equal	to	3.	These	executions	illustrate	some	subtleties	of	the	load
and	store	operations.
In	Example	A	of	
Figure	
5.33
,	argument	
is	a	pointer	to	array
element	
,	while	
is	a	pointer	to	array	element	
.	In	this	case,
each	load	by	the	pointer	reference	
will	yield	the	value	−10.	Hence,
after	two	iterations,	the	array	elements	will	remain	fixed	at	−10	and	−9,
respectively.	The	result	of	the	read	from	
is	not	affected	by	the	write	to
.	Measuring	this	example	over	a	larger	number	of	iterations	gives	a
CPE	of	1.3.</p>
<p>In	Example	B	of	
Figure	
5.33
,	both	arguments	
and	
are
pointers	to	array	element	
.	In	this	case,	each	load	by	the	pointer
reference	
will	yield	the	value	stored	by	the	previous	execution	of	the
pointer	reference	
.
Figure	
5.34	
Detail	of	load	and	store	units.
The	store	unit	maintains	a	buffer	of	pending	writes.	The	load	unit	must
check	its	address	with	those	in	the	store	unit	to	detect	a	write/read
dependency.
As	a	consequence,	a	series	of	ascending	values	will	be	stored	in	this
location.	In	general,	if	function	
is	called	with	arguments	
and	
pointing	to	the	same	memory	location,	and	with	argument	
having	some	value	
n
&gt;	0,	the	net	effect	is	to	set	the	location	to	
n
−	1.	This
example	illustrates	a	phenomenon	we	will	call	a	
write/read	dependency
—
the	outcome	of	a	memory	read	depends	on	a	recent	memory	write.	Our
performance	measurements	show	that	Example	B	has	a	CPE	of	7.3.	The
write/read	dependency	causes	a	slowdown	in	the	processing	of	around	6
clock	cycles.</p>
<p>To	see	how	the	processor	can	distinguish	between	these	two	cases	and
why	one	runs	slower	than	the	other,	we	must	take	a	more	detailed	look	at
the	load	and	store	execution	units,	as	shown	in	
Figure	
5.34
.	The	store
unit	includes	a	
store	buffer
containing	the	addresses	and	data	of	the
store	operations	that	have	been	issued	to	the	store	unit,	but	have	not	yet
been	completed,	where	completion	involves	updating	the	data	cache.
This	buffer	is	provided	so	that	a	series	of	store	operations	can	be
executed	without	having	to	wait	for	each	one	to	update	the	cache.	When
a	load	operation	occurs,	it	must	check	the	entries	in	the	store	buffer	for
matching	addresses.	If	it	finds	a	match	(meaning	that	any	of	the	bytes
being	written	have	the	same	address	as	any	of	the	bytes	being	read),	it
retrieves	the	corresponding	data	entry	as	the	result	of	the	load	operation.
GCC</p>
<p>generates	the	following	code	for	the	inner	loop	of	
:</p>
<p>Figure	
5.35	
Graphical	representation	of	inner-loop	code	for
.
The	first	
instruction	is	decoded	into	separate	operations	to	compute
the	store	address	and	to	store	the	data	to	memory.
Figure	
5.35
shows	a	data-flow	representation	of	this	loop	code.	The
instruction	
is	translated	into	two	operations:	The	s_addr
instruction	computes	the	address	for	the	store	operation,	creates	an	entry
in	the	store	buffer,	and	sets	the	address	field	for	that	entry.	The	s_data
operation	sets	the	data	field	for	the	entry.	As	we	will	see,	the	fact	that
these	two	computations	are	performed	independently	can	be	important	to
program	performance.	This	motivates	the	separate	functional	units	for
these	operations	in	the	reference	machine.
In	addition	to	the	data	dependencies	between	the	operations	caused	by
the	writing	and	reading	of	registers,	the	arcs	on	the	right	of	the	operators
denote	a	set	of	implicit	dependencies	for	these	operations.	In	particular,
the	address	computation	of	the	s_addr	operation	must	clearly	precede
the	s_data	operation.	In	addition,	the	load	operation	generated	by
decoding	the	instruction	</p>
<pre><code>must	check	the	addresses	of
</code></pre>
<p>any	pending	store	operations,	creating	a	data	dependency	between	it</p>
<p>and	the	s_addr	operation.	The	figure	shows	a	dashed	arc	between	the
s_data	and	load	operations.	This	dependency	is	conditional:	if	the	two
addresses	match,	the	load	operation	must	wait	until	the	s_data	has
deposited	its	result	into	the	store	buffer,	but	if	the	two	addresses	differ,
the	two	operations	can	proceed	independently.
Figure	
5.36
illustrates	the	data	dependencies	between	the	operations
for	the	inner	loop	of	
.	In	
Figure	
5.36(a)
,	we	have	rearranged
the	operations	to	allow	the	dependencies	to	be	seen	more	clearly.	We
have	labeled	the	three	dependencies	involving	the	load	and	store
operations	for	special	attention.	The	arc	labeled	&quot;1&quot;	represents	the
requirement	that	the	store	address	must	be	computed	before	the	data
can	be	stored.	The	arc	labeled	&quot;2&quot;	represents	the	need	for	the	load
operation	to	compare	its	address	with	that	for	any	pending	store
operations.	Finally,	the	dashed	arc	labeled	&quot;3&quot;	represents	the	conditional
data	dependency	that	arises	when	the	load	and	store	addresses	match.
Figure	
5.36(b)
illustrates	what	happens	when	we	take	away	those
operations	that	do	not	directly	affect	the	flow	of	data	from	one	iteration	to
the	next.	The	data-flow	graph	shows	just	two	chains	of	dependencies:	the
one	on	the	left,	with	data	values	being	stored,	loaded,	and	incremented
(only	for	the	case	of	matching	addresses);	and	the	one	on	the	right,
decrementing	variable	
.</p>
<p>Figure	
5.36	
Abstracting	the	operations	for	
.
We	first	rearrange	the	operators	of	
Figure	
5.35(a)
and	then	show	only
those	operations	that	use	values	from	one	iteration	to	produce	new
values	for	the	next	(b).
We	can	now	understand	the	performance	characteristics	of	function
.	
Figure	
5.37
illustrates	the	data	dependencies	formed	by
multiple	iterations	of	its	inner	loop.	For	the	case	of	Example	A	in	
Figure
5.33
,	with	differing	source	and	destination	addresses,	the	load	and
store	operations	can	proceed	independently,	and	hence	the	only	critical
path	is	formed	by	the	decrementing	of	variable	
,	resulting	in	a	CPE
bound	of	1.0.	For	the	case	of	Example	B	with	matching	source	and
destination	addresses,	the	data	dependency	between	the	s_data	and
load	instructions	causes	a	critical	path	to	form	involving	data	being
stored,	loaded,	and	incremented.	We	found	that	these	three	operations	in
sequence	require	a	total	of	around	7	clock	cycles.
As	these	two	examples	show,	the	implementation	of	memory	operations
involves	many	subtleties.	With	operations	on	registers,	the	processor	can
determine	which	instructions	will	affect	which	others	as	they	are	being
decoded	into	operations.	With	memory	operations,	on	the	other	hand,	the</p>
<p>processor	cannot	predict	which	will	affect	which	others	until	the	load	and
store	addresses	have	been	computed.	Efficient	handling	of	memory
operations	is	critical	to	the	performance	of	many	programs.	The	memory
subsystem	makes	use	of	many	optimizations,	such	as	the	potential
parallelism	when	operations	can	proceed	independently.
Practice	Problem	
5.10	
(solution	page	
577
)
As	another	example	of	code	with	potential	load-store	interactions,
consider	the	following	function	to	copy	the	contents	of	one	array	to
another:</p>
<p>Figure	
5.37	
Data-flow	representation	of	function	
.
When	the	two	addresses	do	not	match,	the	only	critical	path	is
formed	by	the	decrementing	of	
cnt
(Example	A).	When	they	do
match,	the	chain	of	data	being	stored,	loaded,	and	incremented
forms	the	critical	path	(Example	B).
Suppose	
is	an	array	of	length	1,000	initialized	so	that	each
element	
equals	
i.
A
.	
What	would	be	the	effect	of	the	call	
B
.	
What	would	be	the	effect	of	the	call	</p>
<p>C
.	
Our	performance	measurements	indicate	that	the	call	of
part	A	has	a	CPE	of	1.2	(which	drops	to	1.0	when	the	loop
is	unrolled	by	a	factor	of	4),	while	the	call	of	part	B	has	a
CPE	of	5.0.	To	what	factor	do	you	attribute	this	performance
difference?
D
.	
What	performance	would	you	expect	for	the	call	
Practice	Problem	
5.11	
(solution	page	
577
)
We	saw	that	our	measurements	of	the	prefix-sum	function	
(
Figure	
5.1
)	yield	a	CPE	of	9.00	on	a	machine	where	the	basic
operation	to	be	performed,	floating-point	addition,	has	a	latency	of
just	3	clock	cycles.	Let	us	try	to	understand	why	our	function
performs	so	poorly.
The	following	is	the	assembly	code	for	the	inner	loop	of	the
function:</p>
<p>Perform	an	analysis	similar	to	those	shown	for	
(
Figure
5.14
)	and	for	
(
Figure	
5.36
)	to	diagram	the	data
dependencies	created	by	this	loop,	and	hence	the	critical	path	that
forms	as	the	computation	proceeds.	Explain	why	the	CPE	is	so
high.
Practice	Problem	
5.12	
(solution	page	
577
)
Rewrite	the	code	for	
(
Figure	
5.1
)	so	that	it	does	not	need
to	repeatedly	retrieve	the	value	of	
from	memory.	You	do	not
need	to	use	loop	unrolling.	We	measured	the	resulting	code	to
have	a	CPE	of	3.00,	limited	by	the	latency	of	floating-point
addition.</p>
<p>5.13	
Life	in	the	Real	World:
Performance	Improvement
Techniques
Although	we	have	only	considered	a	limited	set	of	applications,	we	can
draw	important	lessons	on	how	to	write	efficient	code.	We	have
described	a	number	of	basic	strategies	for	optimizing	program
performance:
High-level	design.	
Choose	appropriate	algorithms	and	data
structures	for	the	problem	at	hand.	Be	especially	vigilant	to	avoid
algorithms	or	coding	techniques	that	yield	asymptotically	poor
performance.
Basic	coding	principles.	
Avoid	optimization	blockers	so	that	a
compiler	can	generate	efficient	code.
Eliminate	excessive	function	calls.	Move	computations	out	of	loops
when	possible.	Consider	selective	compromises	of	program
modularity	to	gain	greater	efficiency.
Eliminate	unnecessary	memory	references.	Introduce	temporary
variables	to	hold	intermediate	results.	Store	a	result	in	an	array	or
global	variable	only	when	the	final	value	has	been	computed.
Low-level	optimizations.	
Structure	code	to	take	advantage	of	the
hardware	capabilities.</p>
<p>Unroll	loops	to	reduce	overhead	and	to	enable	further
optimizations.
Find	ways	to	increase	instruction-level	parallelism	by	techniques
such	as	multiple	accumulators	and	reassociation.
Rewrite	conditional	operations	in	a	functional	style	to	enable
compilation	via	conditional	data	transfers.
A	final	word	of	advice	to	the	reader	is	to	be	vigilant	to	avoid	introducing
errors	as	you	rewrite	programs	in	the	interest	of	efficiency.	It	is	very	easy
to	make	mistakes	when	introducing	new	variables,	changing	loop
bounds,	and	making	the	code	more	complex	overall.	One	useful
technique	is	to	use	checking	code	to	test	each	version	of	a	function	as	it
is	being	optimized,	to	ensure	no	bugs	are	introduced	during	this	process.
Checking	code	applies	a	series	of	tests	to	the	new	versions	of	a	function
and	makes	sure	they	yield	the	same	results	as	the	original.	The	set	of
test	cases	must	become	more	extensive	with	highly	optimized	code,
since	there	are	more	cases	to	consider.	For	example,	checking	code	that
uses	loop	unrolling	requires	testing	for	many	different	loop	bounds	to
make	sure	it	handles	all	of	the	different	possible	numbers	of	single-step
iterations	required	at	the	end.</p>
<p>5.14	
Identifying	and	Eliminating
Performance	Bottlenecks
Up	to	this	point,	we	have	only	considered	optimizing	small	programs,
where	there	is	some	clear	place	in	the	program	that	limits	its	performance
and	therefore	should	be	the	focus	of	our	optimization	efforts.	When
working	with	large	programs,	even	knowing	where	to	focus	our
optimization	efforts	can	be	difficult.	In	this	section,	we	describe	how	to
use	
code	profilers
,	analysis	tools	that	collect	performance	data	about	a
program	as	it	executes.	We	also	discuss	some	general	principles	of	code
optimization,	including	the	implications	of	Amdahl's	law,	introduced	in
Section	
1.9.1
.
5.14.1	
Program	Profiling
Program	
profiling
involves	running	a	version	of	a	program	in	which
instrumentation	code	has	been	incorporated	to	determine	how	much	time
the	different	parts	of	the	program	require.	It	can	be	very	useful	for
identifying	the	parts	of	a	program	we	should	focus	on	in	our	optimization
efforts.	One	strength	of	profiling	is	that	it	can	be	performed	while	running
the	actual	program	on	realistic	benchmark	data.
Unix	systems	provide	the	profiling	program	
GPROF
.	This	program
generates	two	forms	of	information.	First,	it	determines	how	much	CPU</p>
<p>time	was	spent	for	each	of	the	functions	in	the	program.	Second,	it
computes	a	count	of	how	many	times	each	function	gets	called,
categorized	by	which	function	performs	the	call.	Both	forms	of	information
can	be	quite	useful.	The	timings	give	a	sense	of	
the	relative	importance
of	the	different	functions	in	determining	the	overall	run	time.	The	calling
information	allows	us	to	understand	the	dynamic	behavior	of	the
program.
Profiling	with	
GPROF</p>
<p>requires	three	steps,	as	shown	for	a	C	program
,	which	runs	with	command-line	argument	
:
1
.	
The	program	must	be	compiled	and	linked	for	profiling.	With	
GCC
(and	other	C	compilers),	this	involves	simply	including	the	run-time
flag	
on	the	command	line.	It	is	important	to	ensure	that	the
compiler	does	not	attempt	to	perform	any	optimizations	via	inline
substitution,	or	else	the	calls	to	functions	may	not	be	tabulated
accurately.	We	use	optimization	flag	
,	guaranteeing	that
function	calls	will	be	tracked	properly.
2
.	
The	program	is	then	executed	as	usual:
It	runs	slightly	(around	a	factor	of	2)	slower	than	normal,	but
otherwise	the	only	difference	is	that	it	generates	a	file	
.
3
.	
GPROF</p>
<p>is	invoked	to	analyze	the	data	in	
:</p>
<p>The	first	part	of	the	profile	report	lists	the	times	spent	executing	the
different	functions,	sorted	in	descending	order.	As	an	example,	the
following	listing	shows	this	part	of	the	report	for	the	three	most	time-
consuming	functions	in	a	program:
Each	row	represents	the	time	spent	for	all	calls	to	some	function.	The	first
column	indicates	the	percentage	of	the	overall	time	spent	on	the	function.
The	second	shows	the	cumulative	time	spent	by	the	functions	up	to	and
including	the	one	on	this	row.	The	third	shows	the	time	spent	on	this
particular	function,	and	the	fourth	shows	how	many	times	it	was	called
(not	counting	recursive	calls).	In	our	example,	the	function	
was	called	only	once,	but	this	single	call	required	203.66	seconds,	while
the	function	
was	called	965,027	times	(not	including
recursive	calls),	requiring	a	total	of	4.85	seconds.	Function	
computes	the	length	of	a	string	by	calling	the	library	function	
.
Library	function	calls	are	normally	not	shown	in	the	results	by	
GPROF
.
Their	times	are	usually	reported	as	part	of	the	function	calling	them.	By</p>
<p>creating	the	&quot;wrapper	function&quot;	
,	we	can	reliably	track	the	calls	to
,	showing	that	it	was	called	12,511,031	times	but	only	requiring	a
total	of	0.30	seconds.
The	second	part	of	the	profile	report	shows	the	calling	history	of	the
functions.	The	following	is	the	history	for	a	recursive	function
:
This	history	shows	both	the	functions	that	called	
,	as	well	as
the	functions	that	it	called.	The	first	two	lines	show	the	calls	to	the
function:	158,655,725	calls	by	itself	recursively,	and	965,027	calls	by
function	
(which	is	itself	called	965,027	times).	Function
,	in	turn,	called	two	other	functions,	
and
,	each	a	total	of	363,039	times.
From	these	call	data,	we	can	often	infer	useful	information	about	the
program	behavior.	For	example,	the	function	
is	a	recursive
procedure	that	scans	the	linked	list	for	a	hash	bucket	looking	for	a</p>
<p>particular	string.	For	this	function,	comparing	the	number	of	recursive
calls	with	the	number	of	top-level	calls	provides	statistical	information
about	the	lengths	of	the	traversals	through	these	lists.	Given	that	their
ratio	is	164.4:1,	we	can	infer	that	the	program	scanned	an	average	of
around	164	elements	each	time.
Some	properties	of	
GPROF</p>
<p>are	worth	noting:
The	timing	is	not	very	precise.	It	is	based	on	a	simple	
interval
counting
scheme	in	which	the	compiled	program	maintains	a	counter
for	each	function	recording	the	time	spent	executing	that	function.	The
operating	system	causes	the	program	to	be	interrupted	at	some
regular	time	interval	
δ
.	Typical	values	of	
δ
range	between	1.0	and
10.0	milliseconds.	It	then	determines	what	function	the	program	was
executing	when	the	interrupt	occurred	and	increments	the	counter	for
that	function	by	
δ
.	Of	course,	it	may	happen	that	this	function	just
started	executing	and	will	shortly	be	completed,	but	it	is	assigned	the
full	cost	of	the	execution	since	the	previous	interrupt.	Some	other
function	may	run	between	two	interrupts	and	therefore	not	be	charged
any	time	at	all.
Over	a	long	duration,	this	scheme	works	reasonably	well.	Statistically,
every	function	should	be	charged	according	to	the	relative	time	spent
executing	it.	For	programs	that	run	for	less	than	around	1	second,
however,	the	numbers	should	be	viewed	as	only	rough	estimates.
The	calling	information	is	quite	reliable,	assuming	no	inline
substitutions	have	been	performed.	The	compiled	program	maintains
a	counter	for	each	combination	of	caller	and	callee.	The	appropriate
counter	is	incremented	every	time	a	procedure	is	called.</p>
<p>By	default,	the	timings	for	library	functions	are	not	shown.	Instead,
these	times	are	incorporated	into	the	times	for	the	calling	functions.
5.14.2	
Using	a	Profiler	to	Guide
Optimization
As	an	example	of	using	a	profiler	to	guide	program	optimization,	we
created	an	application	that	involves	several	different	tasks	and	data
structures.	This	application	analyzes	the	
n-gram
statistics	of	a	text
document,	where	an	
n
-gram	is	a	sequence	of	
n
words	occurring	in	a
document.	For	
n
=	1,	we	collect	statistics	on	individual	words,	for	
n
=	2	on
pairs	of	words,	and	so	on.	For	a	given	value	of	
n
,	our	program	reads	a
text	file,	creates	a	table	of	unique	
n
-grams	and	how	many	times	each	one
occurs,	then	sorts	the	
n
-grams	in	descending	order	of	occurrence.
As	a	benchmark,	we	ran	it	on	a	file	consisting	of	the	complete	works	of
William	Shakespeare,	totaling	965,028	words,	of	which	23,706	are
unique.	We	found	that	for	
n
=	1,	even	a	poorly	written	analysis	program
can	readily	process	the	entire	file	in	under	1	second,	and	so	we	set	
n
=	2
to	make	things	more	challenging.	For	the	case	of	
n
=	2,	
n
-grams	are
referred	to	as	
bigrams
(pronounced	&quot;bye-grams&quot;).	We	determined	that
Shakespeare's	works	contain	363,039	unique	bigrams.	The	most
common	is	&quot;I	am,&quot;	occurring	1,892	times.	Perhaps	his	most	famous
bigram,	&quot;to	be,&quot;	occurs	1,020	times.	Fully	266,018	of	the	bigrams	occur
only	once.</p>
<h2>Our	program	consists	of	the	following	parts.	We	created	multiple
versions,	starting	with	simple	algorithms	for	the	different	parts	and	then
replacing	them	with	more	sophisticated	ones:
1
.	
Each	word	is	read	from	the	file	and	converted	to	lowercase.	Our
initial	version	used	the	function	
(
Figure	
5.7
),	which	we
know	to	have	quadratic	run	time	due	to	repeated	calls	to	
.
2
.	
A	hash	function	is	applied	to	the	string	to	create	a	number
between	0	and	
s
−	1,	for	a	hash	table	with	
s
buckets.	Our	initial
function	simply	summed	the	ASCII	codes	for	the	characters
modulo	
s.
3
.	
Each	hash	bucket	is	organized	as	a	linked	list.	The	program	scans
down	this	list	looking	for	a	matching	entry.	If	one	is	found,	the
frequency	for	this	
n
-gram	is	incremented.	Otherwise,	a	new	list
element	is	created.	Our	initial	version	performed	this	operation
recursively,	inserting	new	elements	at	the	end	of	the	list.
4
.	
Once	the	table	has	been	generated,	we	sort	all	of	the	elements
according	to	the	frequencies.	Our	initial	version	used	insertion
sort.
Figure	
5.38
shows	the	profile	results	for	six	different	versions	of	our	
n</h2>
<p>gram-frequency	analysis	program.	For	each	version,	we	divide	the	time
into	the	following	categories:
Sort.	
Sorting	
n
-grams	by	frequency
List.	
Scanning	the	linked	list	for	a	matching	
n
-gram,	inserting	a	new
element	if	necessary
Lower.	
Converting	strings	to	lowercase</p>
<p>Strlen.	
Computing	string	lengths
Figure	
5.38	
Profile	results	for	different	versions	of	bigram-
frequency	counting	program.
Time	is	divided	according	to	the	different	major	operations	in	the
program.
Hash.	
Computing	the	hash	function
Rest.	
The	sum	of	all	other	functions
As	part	(a)	of	the	figure	shows,	our	initial	version	required	3.5	minutes,
with	most	of	the	time	spent	sorting.	This	is	not	surprising,	since	insertion
sort	has	quadratic	run	time	and	the	program	sorted	363,039	values.</p>
<p>In	our	next	version,	we	performed	sorting	using	the	library	function	
,
which	is	based	on	the	quicksort	algorithm	[
98
].	It	has	an	expected	run
time	of	
O
(
n
log	
n
).	This	version	is	labeled	&quot;Quicksort&quot;	in	the	figure.	The
more	efficient	sorting	algorithm	reduces	the	time	spent	sorting	to	become
negligible,	and	the	overall	run	time	to	around	5.4	seconds.	Part	(b)	of	the
figure	shows	the	times	for	the	remaining	version	on	a	scale	where	we	can
see	them	more	clearly.
With	improved	sorting,	we	now	find	that	list	scanning	becomes	the
bottleneck.	Thinking	that	the	inefficiency	is	due	to	the	recursive	structure
of	the	function,	we	replaced	it	by	an	iterative	one,	shown	as	&quot;Iter	first.&quot;
Surprisingly,	the	run	time	increases	to	around	7.5	seconds.	On	closer
study,	we	find	a	subtle	difference	between	the	two	list	functions.	The
recursive	version	inserted	new	elements	at	the	end	of	the	list,	while	the
iterative	one	inserted	them	at	the	front.	To	maximize	performance,	we
want	the	most	frequent	
n
-grams	to	occur	near	the	beginning	of	the	lists.
That	way,	the	function	will	quickly	locate	the	common	cases.	Assuming
that	
n
-grams	are	spread	uniformly	throughout	the	document,	we	would
expect	the	first	occurrence	of	a	frequent	one	to	come	before	that	of	a	less
frequent	one.	By	inserting	new	
n
-grams	at	the	end,	the	first	function
tended	to	order	
n
-grams	in	descending	order	of	frequency,	while	the
second	function	tended	to	do	just	the	opposite.	We	therefore	created	a
third	list-scanning	function	that	uses	iteration	but	inserts	new	elements	at
the	end	of	this	list.	With	this	version,	shown	as	&quot;Iter	last,&quot;	the	time
dropped	to	around	5.3	seconds,	slightly	better	than	with	the	recursive
version.	These	measurements	demonstrate	the	importance	of	running
experiments	on	a	program	as	part	of	an	optimization	effort.	We	initially
assumed	that	converting	recursive	code	to	iterative	code	would	improve</p>
<h2>its	performance	and	did	not	consider	the	distinction	between	adding	to
the	end	or	to	the	beginning	of	a	list.
Next,	we	consider	the	hash	table	structure.	The	initial	version	had	only
1,021	buckets	(typically,	the	number	of	buckets	is	chosen	to	be	a	prime
number	to	enhance	the	ability	of	the	hash	function	to	distribute	keys
uniformly	among	the	buckets).	For	a	table	with	363,039	entries,	this
would	imply	an	average	
load
of	363,039/1,021	=	355.6.	That	explains
why	so	much	of	the	time	is	spent	performing	list	operations—the
searches	involve	testing	a	significant	number	of	candidate	
n
-grams.	It
also	explains	why	the	performance	is	so	sensitive	to	the	list	ordering.	We
then	increased	the	number	of	buckets	to	199,999,	reducing	the	average
load	to	1.8.	Oddly	enough,	however,	our	overall	run	time	only	drops	to	5.1
seconds,	a	difference	of	only	0.2	seconds.
On	further	inspection,	we	can	see	that	the	minimal	performance	gain	with
a	larger	table	was	due	to	a	poor	choice	of	hash	function.	Simply	summing
the	character	codes	for	a	string	does	not	produce	a	very	wide	range	of
values.	In	particular,	the	maximum	code	value	for	a	letter	is	122,	and	so	a
string	of	
n
characters	will	generate	a	sum	of	at	most	122
n
.	The	longest
bigram	in	our	document,	&quot;honorificabilitudinitatibus***	thou&quot;	sums	to	just
3,371,	and	so	most	of	the	buckets	in	our	hash	table	will	go	unused.	In
addition,	a	commutative	hash	function,	such	as	addition,	does	not
differentiate	among	the	different	possible	orderings	of	characters	with	a
string.	For	example,	the	words	&quot;rat&quot;	and	&quot;tar&quot;	will	generate	the	same
sums.
We	switched	to	a	hash	function	that	uses	shift	and	
EXCLUSIVE</h2>
<p>OR
operations.	With	this	version,	shown	as	&quot;Better	hash,&quot;	the	time	drops	to
0.6	seconds.	A	more	systematic	approach	would	be	to	study	the</p>
<p>distribution	of	keys	among	the	buckets	more	carefully,	making	sure	that	it
comes	close	to	what	one	would	expect	if	the	hash	function	had	a	uniform
output	distribution.
Finally,	we	have	reduced	the	run	time	to	the	point	where	most	of	the	time
is	spent	in	
,	and	most	of	the	calls	to	
occur	as	part	of	the
lowercase	conversion.	We	have	already	seen	that	function	
has
quadratic	performance,	especially	for	long	strings.	The	words	in	this
document	are	short	enough	to	avoid	the	disastrous	consequences	of
quadratic	performance;	the	longest	bigram	is	just	32	characters.	Still,
switching	to	
,	shown	as	&quot;Linear	lower,&quot;	yields	a	significant
improvement,	with	the	overall	time	dropping	to	around	0.2	seconds.
With	this	exercise,	we	have	shown	that	code	profiling	can	help	drop	the
time	required	for	a	simple	application	from	3.5	minutes	down	to	0.2
seconds,	yielding	a	performance	gain	of	around	1,000×.	The	profiler
helps	us	focus	our	attention	on	the	most	time-consuming	parts	of	the
program	and	also	provides	useful	information	about	the	procedure	call
structure.	Some	of	the	bottlenecks	in	our	code,	such	as	using	a	quadratic
sort	routine,	are	easy	to	anticipate,	while	others,	such	as	whether	to
append	to	the	beginning	or	end	of	a	list,	emerge	only	through	a	careful
analysis.
We	can	see	that	profiling	is	a	useful	tool	to	have	in	the	toolbox,	but	it
should	not	be	the	only	one.	The	timing	measurements	are	imperfect,
especially	for	shorter	(less	than	1	second)	run	times.	More	significantly,
the	results	apply	only	to	the	particular	data	tested.	For	example,	if	we	had
run	the	original	function	on	data	consisting	of	a	smaller	number	of	longer
strings,	we	would	have	found	that	the	lowercase	conversion	routine	was</p>
<p>the	major	performance	bottleneck.	Even	worse,	if	it	only	profiled
documents	with	short	words,	we	might	never	detect	hidden	bottlenecks
such	as	the	quadratic	performance	of	
.	In	general,	profiling	can
help	us	optimize	for	
typical
cases,	assuming	we	run	the	program	on
representative	data,	but	we	should	also	make	sure	the	program	will	have
respectable	performance	for	all	possible	cases.	This	mainly	involves
avoiding	algorithms	(such	as	insertion	sort)	and	bad	programming
practices	(such	as	
)	that	yield	poor	asymptotic	performance.
Amdahl's	law,	described	in	
Section	
1.9.1
,	provides	some	additional
insights	into	the	performance	gains	that	can	be	obtained	by	targeted
optimizations.	For	our	
n
-gram	code,	we	saw	the	total	execution	time	drop
from	209.0	to	5.4	seconds	when	we	replaced	insertion	sort	by	quicksort.
The	initial	version	spent	203.7	of	its	209.0	seconds	performing	insertion
sort,	giving	
α
=	0.974,	the	fraction	of	time	subject	to	speedup.	With
quicksort,	the	time	spent	sorting	becomes	negligible,	giving	a	predicted
speedup	of	209/
α
=	39.0,	close	to	the	measured	speedup	of	38.5.	We
were	able	to	gain	a	large	speedup	because	sorting	constituted	a	very
large	fraction	of	the	overall	execution	time.	However,	when	one
bottleneck	is	eliminated,	a	new	one	arises,	and	so	gaining	additional
speedup	required	focusing	on	other	parts	of	the	program.</p>
<p>5.15	
Summary
Although	most	presentations	on	code	optimization	describe	how
compilers	can	generate	efficient	code,	much	can	be	done	by	an
application	programmer	to	assist	the	compiler	in	this	task.	No	compiler
can	replace	an	inefficient	algorithm	or	data	
structure	by	a	good	one,	and
so	these	aspects	of	program	design	should	remain	a	primary	concern	for
programmers.	We	also	have	seen	that	optimization	blockers,	such	as
memory	aliasing	and	procedure	calls,	seriously	restrict	the	ability	of
compilers	to	perform	extensive	optimizations.	Again,	the	programmer
must	take	primary	responsibility	for	eliminating	these.	These	should
simply	be	considered	parts	of	good	programming	practice,	since	they
serve	to	eliminate	unneeded	work.
Tuning	performance	beyond	a	basic	level	requires	some	understanding
of	the	processor's	microarchitecture,	describing	the	underlying
mechanisms	by	which	the	processor	implements	its	instruction	set
architecture.	For	the	case	of	out-of-order	processors,	just	knowing
something	about	the	operations,	capabilities,	latencies,	and	issue	times
of	the	functional	units	establishes	a	baseline	for	predicting	program
performance.
We	have	studied	a	series	of	techniques—including	loop	unrolling,
creating	multiple	accumulators,	and	reassociation—that	can	exploit	the
instruction-level	parallelism	provided	by	modern	processors.	As	we	get
deeper	into	the	optimization,	it	becomes	important	to	study	the	generated
assembly	code	and	to	try	to	understand	how	the	computation	is	being</p>
<p>performed	by	the	machine.	Much	can	be	gained	by	identifying	the	critical
paths	determined	by	the	data	dependencies	in	the	program,	especially
between	the	different	iterations	of	a	loop.	We	can	also	compute	a
throughput	bound	for	a	computation,	based	on	the	number	of	operations
that	must	be	computed	and	the	number	and	issue	times	of	the	units	that
perform	those	operations.
Programs	that	involve	conditional	branches	or	complex	interactions	with
the	memory	system	are	more	difficult	to	analyze	and	optimize	than	the
simple	loop	programs	we	first	considered.	The	basic	strategy	is	to	try	to
make	branches	more	predictable	or	make	them	amenable	to
implementation	using	conditional	data	transfers.	We	must	also	watch	out
for	the	interactions	between	store	and	load	operations.	Keeping	values	in
local	variables,	allowing	them	to	be	stored	in	registers,	can	often	be
helpful.
When	working	with	large	programs,	it	becomes	important	to	focus	our
optimization	efforts	on	the	parts	that	consume	the	most	time.	Code
profilers	and	related	tools	can	help	us	systematically	evaluate	and
improve	program	performance.	We	described	
GPROF
,	a	standard	Unix
profiling	tool.	More	sophisticated	profilers	are	available,	such	as	the	
VTUNE
program	development	system	from	Intel,	and	
VALGRIND
,	commonly
available	on	Linux	systems.	These	tools	can	break	down	the	execution
time	below	the	procedure	level	to	estimate	the	performance	of	each	
basic
block
of	the	program.	(A	basic	block	is	a	sequence	of	instructions	that
has	no	transfers	of	control	out	of	its	middle,	and	so	the	block	is	always
executed	in	its	entirety.)</p>
<p>Bibliographic	Notes
Our	focus	has	been	to	describe	code	optimization	from	the	programmer's
perspective,	demonstrating	how	to	write	code	that	will	make	it	easier	for
compilers	to	generate	efficient	code.	An	extended	paper	by	Chellappa,
Franchetti,	and	P$uUschel	[
19
]	
takes	a	similar	approach	but	goes	into
more	detail	with	respect	to	the	processor's	characteristics.
Many	publications	describe	code	optimization	from	a	compiler's
perspective,	formulating	ways	that	compilers	can	generate	more	efficient
code.	Muchnick's	book	is	considered	the	most	comprehensive	[
80
].
Wadleigh	and	Crawford's	book	on	software	optimization	[
115
]	covers
some	of	the	material	we	have	presented,	but	it	also	describes	the
process	of	getting	high	performance	on	parallel	machines.	An	early	paper
by	Mahlke	et	al.	[
75
]	describes	how	several	techniques	developed	for
compilers	that	map	programs	onto	parallel	machines	can	be	adapted	to
exploit	the	instruction-level	parallelism	of	modern	processors.	This	paper
covers	the	code	transformations	we	presented,	including	loop	unrolling,
multiple	accumulators	(which	they	refer	to	as	
accumulator	variable
expansion
),	and	reassociation	(which	they	refer	to	as	
tree	height
reduction
).
Our	presentation	of	the	operation	of	an	out-of-order	processor	is	fairly
brief	and	abstract.	More	complete	descriptions	of	the	general	principles
can	be	found	in	advanced	computer	architecture	textbooks,	such	as	the
one	by	Hennessy	and	Patterson	[
46
,	Ch.	2−3].	Shen	and	Lipasti's	book
[
100
]	provides	an	in-depth	treatment	of	modern	processor	design.</p>
<p>Homework	Problems
5.13	
♦♦
Suppose	we	wish	to	write	a	procedure	that	computes	the	inner	product	of
two	vectors	u	and	v.	An	abstract	version	of	the	function	has	a	CPE	of
14−18	with	x86-64	for	different	types	of	integer	and	floating-point	data.	By
doing	the	same	sort	of	transformations	we	did	to	transform	the	abstract
program	
into	the	more	efficient	
,	we	get	the	following
code:</p>
<p>Our	measurements	show	that	this	function	has	CPEs	of	1.50	for	integer
data	and	3.00	for	floating-point	data.	For	data	type	double,	the	x86-64
assembly	code	for	the	inner	loop	is	as	follows:
Assume	that	the	functional	units	have	the	characteristics	listed	in	
Figure
5.12
.
A
.	
Diagram	how	this	instruction	sequence	would	be	decoded	into
operations	and	show	how	the	data	dependencies	between	them
would	create	a	critical	path	of	operations,	in	the	style	of	
Figures
5.13
and	
5.14
.</p>
<p>B
.	
For	data	type	double,	what	lower	bound	on	the	CPE	is	determined
by	the	critical	path?
C
.	
Assuming	similar	instruction	sequences	for	the	integer	code	as
well,	what	lower	bound	on	the	CPE	is	determined	by	the	critical
path	for	integer	data?
D
.	
Explain	how	the	floating-point	versions	can	have	CPEs	of	3.00,
even	though	the	multiplication	operation	requires	5	clock	cycles.
5.14	
♦
Write	a	version	of	the	inner	product	procedure	described	in	
Problem
5.13
that	uses	6	×	1	loop	unrolling.	For	x86-64,	our	measurements	of
the	unrolled	version	give	a	CPE	of	1.07	for	integer	data	but	still	3.01	for
both	floating-point	data.
A
.	
Explain	why	any	(scalar)	version	of	an	inner	product	procedure
running	on	an	Intel	Core	i7	Haswell	processor	cannot	achieve	a
CPE	less	than	1.00.
B
.	
Explain	why	the	performance	for	floating-point	data	did	not
improve	with	loop	unrolling.
5.15	
♦
Write	a	version	of	the	inner	product	procedure	described	in	
Problem
5.13
that	uses	6	×	6	loop	unrolling.	Our	measurements	for	this	function</p>
<p>with	x86-64	give	a	CPE	of	1.06	for	integer	data	and	1.01	for	floating-point
data.
What	factor	limits	the	performance	to	a	CPE	of	1.00?
5.16	
♦
Write	a	version	of	the	inner	product	procedure	described	in	
Problem
5.13
that	uses	6	×	1
a
loop	unrolling	to	enable	greater	parallelism.	Our
measurements	for	this	function	give	a	CPE	of	1.10	for	integer	data	and
1.05	for	floating-point	data.
5.17	
♦♦
The	library	function	
has	the	following	prototype:
This	function	fills	
bytes	of	the	memory	area	starting	at	
with	copies	of
the	low-order	byte	of	
.	For	example,	it	can	be	used	to	zero	out	a	region
of	memory	by	giving	argument	0	for	
,	but	other	values	are	possible.
The	following	is	a	straightforward	implementation	of	
:</p>
<p>Implement	a	more	efficient	version	of	the	function	by	using	a	word	of	data
type	
to	pack	eight	copies	of	
,	and	then	step	through	the
region	using	word-level	writes.	You	might	find	it	helpful	to	do	additional
loop	unrolling	as	well.	On	our	reference	machine,	we	were	able	to	reduce
the	CPE	from	1.00	for	the	straightforward	implementation	to	0.127.	That
is,	the	program	is	able	to	write	8	bytes	every	clock	cycle.
Here	are	some	additional	guidelines.	To	ensure	portability,	let	
K
denote
the	value	of	
for	the	machine	on	which	you	run
your	program.
You	may	not	call	any	library	functions.
Your	code	should	work	for	arbitrary	values	of	
,	including	when	it	is
not	a	multiple	of	
K.
You	can	do	this	in	a	manner	similar	to	the	way	we
finish	the	last	few	iterations	with	loop	unrolling.</p>
<p>You	should	write	your	code	so	that	it	will	compile	and	run	correctly	on
any	machine	regardless	of	the	value	of	
K.
Make	use	of	the	operation
to	do	this.
On	some	machines,	unaligned	writes	can	be	much	slower	than
aligned	ones.	(On	some	non-x86	machines,	they	can	even	cause
segmentation	faults.)	Write	your	code	so	that	it	starts	with	byte-level
writes	until	the	destination	address	is	a	multiple	of	
K
,	then	do	word-
level	writes,	and	then	(if	necessary)	finish	with	byte-level	writes.
Beware	of	the	case	where	
is	small	enough	that	the	upper	bounds
on	some	of	the	loops	become	negative.	With	expressions	involving
the	
operator,	the	testing	may	be	performed	with	unsigned
arithmetic.	(See	
Section	
2.2.8
and	
Problem	
2.72
.)
5.18	
♦♦♦
We	considered	the	task	of	polynomial	evaluation	in	Practice	Problems
5.5	and	5.6,	with	both	a	direct	evaluation	and	an	evaluation	by	Horner's
method.	Try	to	write	
faster	versions	of	the	function	using	the	optimization
techniques	we	have	explored,	including	loop	unrolling,	parallel
accumulation,	and	reassociation.	You	will	find	many	different	ways	of
mixing	together	Horner's	scheme	and	direct	evaluation	with	these
optimization	techniques.
Ideally,	you	should	be	able	to	reach	a	CPE	close	to	the	throughput	limit	of
your	machine.	Our	best	version	achieves	a	CPE	of	1.07	on	our	reference
machine.</p>
<p>5.19	
♦♦♦
In	
Problem	
5.12
,	we	were	able	to	reduce	the	CPE	for	the	prefix-sum
computation	to	3.00,	limited	by	the	latency	of	floating-point	addition	on
this	machine.	Simple	loop	unrolling	does	not	improve	things.
Using	a	combination	of	loop	unrolling	and	reassociation,	write	code	for	a
prefix	sum	that	achieves	a	CPE	less	than	the	latency	of	floating-point
addition	on	your	machine.	Doing	this	requires	actually	increasing	the
number	of	additions	performed.	For	example,	our	version	with	two-way
unrolling	requires	three	additions	per	iteration,	while	our	version	with	four-
way	unrolling	requires	five.	Our	best	implementation	achieves	a	CPE	of
1.67	on	our	reference	machine.
Determine	how	the	throughput	and	latency	limits	of	your	machine	limit	the
minimum	CPE	you	can	achieve	for	the	prefix-sum	operation.</p>
<p>Solutions	to	Practice	Problems
Solution	to	Problem	
5.1	
(page
500
)
This	problem	illustrates	some	of	the	subtle	effects	of	memory	aliasing.
As	the	following	commented	code	shows,	the	effect	will	be	to	set	the
value	at	
to	zero:
This	example	illustrates	that	our	intuition	about	program	behavior	can
often	be	wrong.	We	naturally	think	of	the	case	where	
and	
are
distinct	but	overlook	the	possibility	that	they	might	be	equal.	Bugs	often
arise	due	to	conditions	the	programmer	does	not	anticipate.
Solution	to	Problem	
5.2	
(page</p>
<p>504
)
This	problem	illustrates	the	relationship	between	CPE	and	absolute
performance.	It	can	be	solved	using	elementary	algebra.	We	find	that	for
n
≤	2,	version	1	is	the	fastest.	Version	2	is	fastest	for	3	≤	
n
≤	7,	and
version	3	is	fastest	for	
n
≥	8.
Solution	to	Problem	
5.3	
(page
512
)
This	is	a	simple	exercise,	but	it	is	important	to	recognize	that	the	four
statements	of	a	
loop—initial,	test,	update,	and	body—get	executed
different	numbers	of	times.
Code
A.
1
91
90
90
B.
91
1
90
90
C.
1
1
90
90
Solution	to	Problem	
5.4	
(page
516
)</p>
<p>This	assembly	code	demonstrates	a	clever	optimization	opportunity
detected	by	
GCC
.	It	is	worth	studying	this	code	carefully	to	better
understand	the	subtleties	of	code	optimization.
A
.	
In	the	less	optimized	code,	register	
is	simply	used	as	a
temporary	value,	both	set	and	used	on	each	loop	iteration.	In	the
more	optimized	code,	it	is	used	more	in	the	manner	of	variable
in	
,	accumulating	the	product	of	the	vector	elements.
The	difference	with	
,	however,	is	that	location	
is
updated	on	each	iteration	by	the	second	
instruction.
We	can	see	that	this	optimized	version	operates	much	like	the
following	C	code:</p>
<p>B
.	
The	two	versions	of	
will	have	identical	functionality,	even
with	memory	aliasing.
C
.	
This	transformation	can	be	made	without	changing	the	program
behavior,	because,	with	the	exception	of	the	first	iteration,	the
value	read	from	
at	the	beginning	of	each	iteration	will	be	the
same	value	written	to	this	register	
at	the	end	of	the	previous
iteration.	Therefore,	the	combining	instruction	can	simply	use	the
value	already	in	
at	the	beginning	of	the	loop.
Solution	to	Problem	
5.5	
(page
530
)
Polynomial	evaluation	is	a	core	technique	for	solving	many	problems.	For
example,	polynomial	functions	are	commonly	used	to	approximate
trigonometric	functions	in	math	libraries.
A
.	
The	function	performs	2
n
multiplications	and	
n
additions.
B
.	
We	can	see	that	the	performance-limiting	computation	here	is	the
repeated	computation	of	the	expression	
.	This
requires	a	floating-point	multiplication	(5	clock	cycles),	and	the
computation	for	one	iteration	cannot	begin	until	the	one	for	the
previous	iteration	has	completed.	The	updating	of	
only
requires	a	floating-point	addition	(3	clock	cycles)	between
successive	iterations.</p>
<p>Solution	to	Problem	
5.6	
(page
530
)
This	problem	demonstrates	that	minimizing	the	number	of	operations	in	a
computation	may	not	improve	its	performance.
A
.	
The	function	performs	
n
multiplications	and	
n
additions,	half	the
number	of	multiplications	as	the	original	function	
.
B
.	
We	can	see	that	the	performance-limiting	computation	here	is	the
repeated	computation	of	the	expression	
Starting	from	the	value	of	
from	the	previous	iteration,	we
must	first	multiply	it	by	
(5	clock	cycles)	and	then	add	it	to	
(3	cycles)	before	we	have	the	value	for	this	iteration.	Thus,	each
iteration	imposes	a	minimum	latency	of	8	cycles,	exactly	our
measured	CPE.
C
.	
Although	each	iteration	in	function	
requires	two
multiplications	rather	than	one,	only	a	single	multiplication	occurs
along	the	critical	path	per	iteration.
Solution	to	Problem	
5.7	
(page
532
)
The	following	code	directly	follows	the	rules	we	have	stated	for	unrolling
a	loop	by	some	factor	
k:</p>
<p>Solution	to	Problem	
5.8	
(page
545
)</p>
<p>This	problem	demonstrates	how	small	changes	in	a	program	can	yield
dramatic	performance	differences,	especially	on	a	machine	with	out-of-
order	execution.	
Figure	
5.39
diagrams	the	three	multiplication
operations	for	a	single	iteration	of	the	function.	In	this	figure,	the
operations	shown	as	blue	boxes	are	along	the	critical	path—they	need	to
be	computed	in	sequence	to	compute	a	new	value	for	loop	variable	
.
The	operations	shown	as	light	boxes	can	be	computed	in	parallel	with	the
critical	path	operations.	For	a	loop	with	
P
operations	along	the	critical
path,	each	iteration	will	require	a	minimum	of	5
P
clock	cycles	and	will
compute	the	product	for	three	elements,	giving	a	lower	bound	on	the
CPE	of	5
P
/3.	This	implies	lower	bounds	of	5.00	for	Al,	3.33	for	A2	and
A5,	and	1.67	for	A3	and	A4.	We	ran	these	functions	on	an	Intel	Core	i7
Haswell	processor	and	found	that	it	could	achieve	these	CPE	values.
Solution	to	Problem	
5.9	
(page
553
)
This	is	another	demonstration	that	a	slight	change	in	coding	style	can
make	it	much	easier	for	the	compiler	to	detect	opportunities	to	use
conditional	moves:</p>
<p>Figure	
5.39	
Data	dependencies	among	multiplication	operations	for
cases	in	
Problem	
5.8
.
The	operations	shown	as	blue	boxes	form	the	critical	paths	for	the
iterations.
We	measured	a	CPE	of	around	12.0	for	this	version	of	the	code,	a
modest	improvement	over	the	original	CPE	of	15.0.
Solution	to	Problem	
5.10	
(page
559
)</p>
<p>This	problem	requires	you	to	analyze	the	potential	load-store	interactions
in	a	program.
A
.	
It	will	set	each	element	
to	
i
+	1,	for	0	≤	
i
≤	998.
B
.	
It	will	set	each	element	
to	0,	for	1	≤	
i
≤	999.
C
.	
In	the	second	case,	the	load	of	one	iteration	depends	on	the	result
of	the	store	from	the	previous	iteration.	Thus,	there	is	a	write/read
dependency	between	successive	iterations.
D
.	
It	will	give	a	CPE	of	1.2,	the	same	as	for	Example	A,	since	there
are	no	dependencies	between	stores	and	subsequent	loads.
Solution	to	Problem	
5.11	
(page
561
)
We	can	see	that	this	function	has	a	write/read	dependency	between
successive	iterations—the	destination	value	
on	one	iteration
matches	the	source	value	
on	the	next.	A	critical	path	is	therefore
formed	for	each	iteration	consisting	of	a	store	(from	the	previous
iteration),	a	load,	and	a	floating-point	addition.	The	CPE	measurement	of
9.0	is	consistent	with	our	measurement	of	7.3	for	the	CPE	of	
when	there	is	a	data	dependency,	since	
involves	an	integer
addition	(1	clock-cycle	latency),	while	
involves	a	floating-point
addition	(3	clock-cycle	latency).
Solution	to	Problem	
5.12	
(page</p>
<p>561
)
Here	is	a	revised	version	of	the	function:
We	introduce	a	local	variable	
.	At	the	start	of	iteration	
,	it	holds
the	value	of	
.	We	then	compute	
to	be	the	value	of	
and	to
be	the	new	value	for	
.
This	version	compiles	to	the	following	assembly	code:</p>
<p>This	code	holds	
in	
,	avoiding	the	need	to	read	
from
memory	and	thus	eliminating	the	write/read	dependency	seen	in	
.</p>
<p>Chapter	
6	
The	Memory	Hierarchy
6.1	
Storage	Technologies	
581
6.2	
Locality	
604
6.3	
The	Memory	Hierarchy	
609
6.4	
Cache	Memories	
614
6.5	
Writing	Cache-Friendly	Code	
633
6.6	
Putting	It	Together:	The	Impact	of	Caches	on	Program
Performance	
639
6.7	
Summary</p>
<p>648
Bibliographic	Notes	
648
Homework	Problems	
649
Solutions	to	Practice	Problems	
660
To	this	point	in	our	study	of	systems,	we	have	relied
on	a	simple	model	of	a	computer	system	as	a	CPU
that	executes	instructions	and	a	memory	system
that	holds	instructions	and	data	for	the	CPU.	In	our</p>
<p>simple	model,	the	memory	system	is	a	linear	array
of	bytes,	and	the	CPU	can	access	each	memory
location	in	a	constant	amount	of	time.	While	this	is
an	effective	model	up	to	a	point,	it	does	not	reflect
the	way	that	modern	systems	really	work.
In	practice,	a	
memory	system
is	a	hierarchy	of
storage	devices	with	different	capacities,	costs,	and
access	times.	CPU	registers	hold	the	most
frequently	used	data.	Small,	fast	
cache	memories
nearby	the	CPU	act	as	staging	areas	for	a	subset	of
the	data	and	instructions	stored	in	the	relatively	slow
main	memory.	The	main	memory	stages	data	stored
on	large,	slow	disks,	which	in	turn	often	serve	as
staging	areas	for	data	stored	on	the	disks	or	tapes
of	other	machines	connected	by	networks.
Memory	hierarchies	work	because	well-written
programs	tend	to	access	the	storage	at	any
particular	level	more	frequently	than	they	access	the
storage	at	the	next	lower	level.	So	the	storage	at	the
next	level	can	be	slower,	and	thus	larger	and
cheaper	per	bit.	The	overall	effect	is	a	large	pool	of
memory	that	costs	as	much	as	the	cheap	storage
near	the	bottom	of	the	hierarchy	but	that	serves
data	to	programs	at	the	rate	of	the	fast	storage	near
the	top	of	the	hierarchy.</p>
<p>As	a	programmer,	you	need	to	understand	the
memory	hierarchy	because	it	has	a	big	impact	on
the	performance	of	your	applications.	If	the	data
your	program	needs	are	stored	in	a	CPU	register,
then	they	can	be	accessed	in	0	cycles	during	the
execution	of	the	instruction.	If	stored	in	a	cache,	4	to
75	cycles.	If	stored	in	main	memory,	hundreds	of
cycles.	And	if	stored	in	disk,	tens	of	millions	of
cycles!
Here,	then,	is	a	fundamental	and	enduring	idea	in
computer	systems:	if	you	understand	how	the
system	moves	data	up	and	down	the	memory
hierarchy,	then	you	can	write	your	application
programs	so	that	their	data	items	are	stored	higher
in	the	hierarchy,	where	the	CPU	can	access	them
more	quickly.
This	idea	centers	around	a	fundamental	property	of
computer	programs	known	as	
locality.
Programs
with	good	locality	tend	to	access	the	same	set	of
data	items	over	and	over	again,	or	they	tend	to
access	sets	of	nearby	data	items.	Programs	with
good	locality	tend	to	access	more	data	items	from
the	upper	levels	of	the	memory	hierarchy	than
programs	with	poor	locality,	and	thus	run	faster.	For
example,	on	our	Core	i7	system,	the	running	times
of	different	matrix	multiplication	kernels	that	perform
the	same	number	of	arithmetic	operations,	but	have</p>
<p>different	degrees	of	locality,	can	vary	by	a	factor	of
almost	40!
In	this	chapter,	we	will	look	at	the	basic	storage
technologies—SRAM	memory,	DRAM	memory,
ROM	memory,	and	rotating	and	solid	state	disks—
and	describe	how	they	are	organized	into
hierarchies.	In	particular,	we	focus	on	the	cache
memories	that	act	as	staging	areas	between	the
CPU	and	main	memory,	because	they	have	the
most	impact	on	application	program	performance.
We	show	you	how	to	analyze	your	C	programs	for
locality,	and	we	introduce	techniques	for	improving
the	locality	in	your	programs.	You	will	also	learn	an
interesting	way	to	characterize	the	performance	of
the	memory	hierarchy	on	a	particular	machine	as	a
&quot;memory	mountain&quot;	that	shows	read	access	times
as	a	function	of	locality.</p>
<p>6.1	
Storage	Technologies
Much	of	the	success	of	computer	technology	stems	from	the	tremendous
progress	in	storage	technology.	Early	computers	had	a	few	kilobytes	of
random	access	memory.	The	earliest	IBM	PCs	didn't	even	have	a	hard
disk.	That	changed	with	the	introduction	of	the	IBM	PC-XT	in	1982,	with
its	10-megabyte	disk.	By	the	year	2015,	typical	machines	had	300,000
times	as	much	disk	storage,	and	the	amount	of	storage	was	increasing
by	a	factor	of	2	every	couple	of	years.
6.1.1	
Random	Access	Memory
Random	access	memory	(RAM)
comes	in	two	varieties—static	and
dynamic.	
Static	RAM	(SRAM)
is	faster	and	significantly	more	expensive
than	
dynamic	RAM	(DRAM).
SRAM	is	used	for	cache	memories,	both	on
and	off	the	CPU	chip.	DRAM	is	used	for	the	main	memory	plus	the	frame
buffer	of	a	graphics	system.	Typically,	a	desktop	system	will	have	no
more	than	a	few	tens	of	megabytes	of	SRAM,	but	hundreds	or	thousands
of	megabytes	of	DRAM.
Static	RAM
SRAM	stores	each	bit	in	a	
bistable
memory	cell.	Each	cell	is
implemented	with	a	six-transistor	circuit.	This	circuit	has	the	property	that
it	can	stay	indefinitely	in	either	of	two	different	voltage	configurations,	or</p>
<p>states.
Any	other	state	will	be	unstable—starting	from	there,	the	circuit
will	quickly	move	toward	one	of	the	stable	states.	Such	a	memory	cell	is
analogous	to	the	inverted	pendulum	illustrated	in	
Figure	
6.1
.
The	pendulum	is	stable	when	it	is	tilted	either	all	the	way	to	the	left	or	all
the	way	to	the	right.	From	any	other	position,	the	pendulum	will	fall	to	one
side	or	the	other.	In	principle,	the	pendulum	could	also	remain	balanced
in	a	vertical	position	indefinitely,	but	this	state	is	
metastable
—the	smallest
disturbance	would	make	it	start	to	fall,	and	once	it	fell	it	would	never
return	to	the	vertical	position.
Due	to	its	bistable	nature,	an	SRAM	memory	cell	will	retain	its	value
indefinitely,	as	long	as	it	is	kept	powered.	Even	when	a	disturbance,	such
as	electrical	noise,	perturbs	the	voltages,	the	circuit	will	return	to	the
stable	value	when	the	disturbance	is	removed.
Figure	
6.1	
Inverted	pendulum.
Like	an	SRAM	cell,	the	pendulum	has	only	two	stable	configurations,	or
states.
Transistors
per	bit
Relative
access
time
Persistent?
Sensitive?
Relative
cost
Applications
SRAM
6
1×
Yes
No
1,000×
Cache
memory</p>
<p>DRAM
1
10×
No
Yes
1×
Main
memory,
frame	buffers
Figure	
6.2	
Characteristics	of	DRAM	and	SRAM	memory.
Dynamic	RAM
DRAM	stores	each	bit	as	charge	on	a	capacitor.	This	capacitor	is	very
small—	typically	around	30	femtofarads—that	is,	30	×	10
farads.
Recall,	however,	that	a	farad	is	a	very	large	unit	of	measure.	DRAM
storage	can	be	made	very	dense—each	cell	consists	of	a	capacitor	and	a
single	access	transistor.	Unlike	SRAM,	however,	a	DRAM	memory	cell	is
very	sensitive	to	any	disturbance.	When	the	capacitor	voltage	is
disturbed,	it	will	never	recover.	Exposure	to	light	rays	will	cause	the
capacitor	voltages	to	change.	In	fact,	the	sensors	in	digital	cameras	and
camcorders	are	essentially	arrays	of	DRAM	cells.
Various	sources	of	leakage	current	cause	a	DRAM	cell	to	lose	its	charge
within	a	time	period	of	around	10	to	100	milliseconds.	Fortunately,	for
computers	operating	with	clock	cycle	times	measured	in	nanoseconds,
this	retention	time	is	quite	long.	The	memory	system	must	periodically
refresh	every	bit	of	memory	by	reading	it	out	and	then	rewriting	it.	Some
systems	also	use	error-correcting	codes,	where	the	computer	words	are
encoded	using	a	few	more	bits	(e.g.,	a	64-bit	word	might	be	encoded
using	72	bits),	such	that	circuitry	can	detect	and	correct	any	single
erroneous	bit	within	a	word.
−15</p>
<p>Figure	
6.2
summarizes	the	characteristics	of	SRAM	and	DRAM
memory.	SRAM	is	persistent	as	long	as	power	is	applied.	Unlike	DRAM,
no	refresh	is	necessary.	SRAM	can	be	accessed	faster	than	DRAM.
SRAM	is	not	sensitive	to	disturbances	such	as	light	and	electrical	noise.
The	trade-off	is	that	SRAM	cells	use	more	transistors	than	DRAM	cells
and	thus	have	lower	densities,	are	more	expensive,	and	consume	more
power.
Conventional	DRAMs
The	cells	(bits)	in	a	DRAM	chip	are	partitioned	into	
d	supercells
,	each
consisting	of	
w
DRAM	cells.	
Ad
×	
w
DRAM	stores	a	total	of	
dw
bits	of
information.	The	supercells	are	organized	as	a	rectangular	array	with	
r
rows	and	
c
columns,	where	
rc
=	
d
.	Each	supercell	has	an	address	of	the
form	
(i,	j)
,	where	
i
denotes	the	row	and	
j
denotes	the	column.
For	example,	
Figure	
6.3
shows	the	organization	of	a	16	×	8	DRAM
chip	with	
d
=	16	supercells,	
w
=	8	bits	per	supercell,	
r
=	4	rows,	and	
c
=	4
columns.	The	shaded	box	denotes	the	supercell	at	address	(2,1).
Information	flows	in	and	out	of	the	chip	via	external	connectors	called
pins.
Each	pin	carries	a	1-bit	signal.	
Figure	
6.3
shows	two	of	these
sets	of	pins:	eight	data	pins	that	can	transfer	1	byte
Aside	
A	note	on	terminology
The	storage	community	has	never	settled	on	a	standard	name	for
a	DRAM	array	element.	Computer	architects	tend	to	refer	to	it	as	a
&quot;cell,&quot;	overloading	the	term	with	the	DRAM	storage	cell.	Circuit
designers	tend	to	refer	to	it	as	a	&quot;word,&quot;	overloading	the	term	with</p>
<p>a	word	of	main	memory.	To	avoid	confusion,	we	have	adopted	the
unambiguous	term	&quot;supercell.&quot;
Figure	
6.3	
High-level	view	of	a	128-bit	16	×	8	DRAM	chip.
in	or	out	of	the	chip,	and	two	
pins	that	carry	two-bit	row	and	column
supercell	addresses.	Other	pins	that	carry	control	information	are	not
shown.
Each	DRAM	chip	is	connected	to	some	circuitry,	known	as	the	
memory
controller
,	that	can	transfer	
w
bits	at	a	time	to	and	from	each	DRAM	chip.
To	read	the	contents	of	supercell	
(i,	j)
,	the	memory	controller	sends	the
row	address	
i
to	the	DRAM,	followed	by	the	column	address	
j.
The	DRAM
responds	by	sending	the	contents	of	supercell	
(i,	j)
back	to	the	controller.
The	row	address	
i
is	called	a	
RAS	(row	access	strobe)	request.
The
column	address	
j
is	called	a	
CAS	(column	access	strobe)	request.
Notice
that	the	RAS	and	CAS	requests	share	the	same	DRAM	address	pins.
For	example,	to	read	supercell	(2,1)	from	the	16	×	8	DRAM	in	
Figure
6.3
,	the	memory	controller	sends	row	address	2,	as	shown	in	
Figure
6.4(a)
.	The	DRAM	responds	by	copying	the	entire	contents	of	row	2
into	an	internal	row	buffer.	Next,	the	memory	controller	sends	column
address	1,	as	shown	in	
Figure	
6.4(b)
.	The	DRAM	responds	by</p>
<p>copying	the	8	bits	in	supercell	(2,1)	from	the	row	buffer	and	sending	them
to	the	memory	controller.
One	reason	circuit	designers	organize	DRAMs	as	two-dimensional	arrays
instead	of	linear	arrays	is	to	reduce	the	number	of	address	pins	on	the
chip.	For	example,	if	our	example	128-bit	DRAM	were	organized	as	a
linear	array	of	16	supercells	with	addresses	0	to	15,	then	the	chip	would
need	four	address	pins	instead	of	two.	The	disadvantage	of	the	two-
dimensional	array	organization	is	that	addresses	must	be	sent	in	two
distinct	steps,	which	increases	the	access	time.
Figure	
6.4	
Reading	the	contents	of	a	DRAM	supercell.
Memory	Modules
DRAM	chips	are	packaged	in	
memory	modules
that	plug	into	expansion
slots	on	the	main	system	board	(motherboard).	Core	i7	systems	use	the
240-pin	
dual	inline	memory	module	(DIMM)
,	which	transfers	data	to	and
from	the	memory	controller	in	64-bit	chunks.</p>
<p>Figure	
6.5
shows	the	basic	idea	of	a	memory	module.	The	example
module	stores	a	total	of	64	MB	(megabytes)	using	eight	64-Mbit	8M	×	8
DRAM	chips,	numbered	0	to	7.	Each	supercell	stores	1	byte	of	
main
memory
,	and	each	64-bit	word	at	byte	address	A	in	main	memory	is
represented	by	the	eight	supercells	whose	corresponding	supercell
address	is	
(i,	j).
In	the	example	in	
Figure	
6.5
,	DRAM	0	stores	the	first
(lower-order)	byte,	DRAM	1	stores	the	next	byte,	and	so	on.
To	retrieve	the	word	at	memory	address	
A
,	the	memory	controller
converts	A	to	a	supercell	address	
(i,	j)
and	sends	it	to	the	memory
module,	which	then	broadcasts	
i
and	
j
to	each	DRAM.	In	response,	each
DRAM	outputs	the	8-bit	contents	of	its	
(i,	j)
supercell.	Circuitry	in	the
module	collects	these	outputs	and	forms	them	into	a	64-bit	word,	which	it
returns	to	the	memory	controller.
Main	memory	can	be	aggregated	by	connecting	multiple	memory
modules	to	the	memory	controller.	In	this	case,	when	the	controller
receives	an	address	
A
,	the	controller	selects	the	module	
k
that	contains
A
,	converts	
A
to	its	
(i,	j)
form,	and	sends	
(i,	j)
to	module	
k.
Practice	Problem	
6.1	
(solution	page	
660
)
In	the	following,	let	
r
be	the	number	of	rows	in	a	DRAM	array,	
c
the
number	of	columns,	
b
the	number	of	bits	needed	to	address	the
rows,	and	
b
the	number	of	bits	needed	to	address	the	columns.
For	each	of	the	following	DRAMs,	determine	the	power-of-2	array
dimensions	that	minimize	max(
b
,	b
),	the	maximum	number	of	bits
needed	to	address	the	rows	or	columns	of	the	array.
r
c
r
c</p>
<p>Figure	
6.5	
Reading	the	contents	of	a	memory	module.
Organization
r
c
b
b
max
(
b
,	b
)
16	×	1</p>
<hr />
<hr />
<hr />
<hr />
<hr />
<p>16	×	4</p>
<hr />
<hr />
<hr />
<hr />
<hr />
<p>128	×	8</p>
<hr />
<hr />
<hr />
<hr />
<hr />
<p>512	×	4</p>
<hr />
<hr />
<hr />
<hr />
<hr />
<p>1,024	×	4</p>
<hr />
<hr />
<hr />
<hr />
<hr />
<p>Enhanced	DRAMs
There	are	many	kinds	of	DRAM	memories,	and	new	kinds	appear	on	the
market	with	regularity	as	manufacturers	attempt	to	keep	up	with	rapidly
increasing	processor	speeds.	Each	is	based	on	the	conventional	DRAM
r
c
r
c</p>
<p>cell,	with	optimizations	that	improve	the	speed	with	which	the	basic
DRAM	cells	can	be	accessed.
Fast	page	mode	DRAM	(FPM	DRAM).	
A	conventional	DRAM	copies
an	entire	row	of	supercells	into	its	internal	row	buffer,	uses	one,	and
then	discards	the	rest.	FPM	DRAM	improves	on	this	by	allowing
consecutive	accesses	to	the	same	row	to	be	served	directly	from	the
row	buffer.	For	example,	to	read	four	supercells	from	row	
i
of	a
conventional	DRAM,	the	memory	controller	must	send	four	RAS/CAS
requests,	even	though	the	row	address	
i
is	identical	in	each	case.	To
read	supercells	from	the	same	row	of	an	FPM	DRAM,	the	memory
controller	sends	an	initial	RAS/CAS	request,	followed	by	three	CAS
requests.	The	initial	RAS/CAS	request	copies	row	
i
into	the	row	buffer
and	returns	the	supercell	addressed	by	the	
CAS.	The	next	three
supercells	are	served	directly	from	the	row	buffer,	and	thus	are
returned	more	quickly	than	the	initial	supercell.
Extended	data	out	DRAM	(EDO	DRAM).	
An	enhanced	form	of	FPM
DRAM	that	allows	the	individual	CAS	signals	to	be	spaced	closer
together	in	time.
Synchronous	DRAM	(SDRAM).	
Conventional,	FPM,	and	EDO
DRAMs	are	asynchronous	in	the	sense	that	they	communicate	with
the	memory	controller	using	a	set	of	explicit	control	signals.	SDRAM
replaces	many	of	these	control	signals	with	the	rising	edges	of	the
same	external	clock	signal	that	drives	the	memory	controller.	Without
going	into	detail,	the	net	effect	is	that	an	SDRAM	can	output	the
contents	of	its	supercells	at	a	faster	rate	than	its	asynchronous
counterparts.</p>
<p>Double	Data-Rate	Synchronous	DRAM	(DDR	SDRAM).	
DDR
SDRAM	is	an	enhancement	of	SDRAM	that	doubles	the	speed	of	the
DRAM	by	using	both	clock	edges	as	control	signals.	Different	types	of
DDR	SDRAMs	are	characterized	by	the	size	of	a	small	prefetch	buffer
that	increases	the	effective	bandwidth:	DDR	(2	bits),	DDR2	(4	bits),
and	DDR3	(8	bits).
Video	RAM	(VRAM).	
Used	in	the	frame	buffers	of	graphics	systems.
VRAM	is	similar	in	spirit	to	FPM	DRAM.	Two	major	differences	are
that	(1)	VRAM	output	is	produced	by	shifting	the	entire	contents	of	the
internal	buffer	in	sequence	and	(2)	VRAM	allows	concurrent	reads
and	writes	to	the	memory.	Thus,	the	system	can	be	painting	the
screen	with	the	pixels	in	the	frame	buffer	(reads)	while	concurrently
writing	new	values	for	the	next	update	(writes).
Nonvolatile	Memory
DRAMs	and	SRAMs	are	
volatile
in	the	sense	that	they	lose	their
information	if	the	supply	voltage	is	turned	off.	
Nonvolatile	memories
,	on
the	other	hand,	retain	their	information	even	when	they	are	powered	off.
There	are	a	variety	of	nonvolatile	memories.	For	historical	reasons,	they
are	referred	to	collectively	as	
read-only	memories
(ROMs),	even	though
some	types	of	ROMs	can	be	written	to	as	well	as	read.	ROMs	are
distinguished	by	the	number	of	times	they	can	be	reprogrammed	(written
to)	and	by	the	mechanism	for	reprogramming	them.
Aside	
Historical	popularity	of	DRAM
technologies</p>
<p>Until	1995,	most	PCs	were	built	with	FPM	DRAMs.	From	1996	to
1999,	EDO	DRAMs	dominated	the	market,	while	FPM	DRAMs	all
but	disappeared.	SDRAMs	first	appeared	in	1995	in	high-end
systems,	and	by	2002	most	PCs	were	built	with	SDRAMs	and
DDR	SDRAMs.	By	2010,	most	server	and	desktop	systems	were
built	with	DDR3	SDRAMs.	In	fact,	the	Intel	Core	i7	supports	only
DDR3	SDRAM.
A	
programmable	ROM	(PROM)
can	be	programmed	exactly	once.
PROMs	include	a	sort	of	fuse	with	each	memory	cell	that	can	be	blown
once	by	zapping	it	with	a	high	current.
An	
erasable	programmable	ROM	(EPROM)
has	a	transparent	quartz
window	that	permits	light	to	reach	the	storage	cells.	The	EPROM	cells
are	cleared	to	zeros	by	shining	ultraviolet	light	through	the	window.
Programming	an	EPROM	is	done	by	using	a	special	device	to	write	ones
into	the	EPROM.	An	EPROM	can	be	erased	and	reprogrammed	on	the
order	of	1,000	times.	An	
electrically	erasable	PROM	(EEPROM)
is	akin	to
an	EPROM,	but	it	does	not	require	a	physically	separate	programming
device,	and	thus	can	be	reprogrammed	in-place	on	printed	circuit	cards.
An	EEPROM	can	be	reprogrammed	on	the	order	of	10
times	before	it
wears	out.
Flash	memory
is	a	type	of	nonvolatile	memory,	based	on	EEPROMs,	that
has	become	an	important	storage	technology.	Flash	memories	are
everywhere,	providing	fast	and	durable	nonvolatile	storage	for	a	slew	of
electronic	devices,	including	digital	cameras,	cell	phones,	and	music
players,	as	well	as	laptop,	desktop,	and	server	computer	systems.	In
Section	
6.1.3
,	we	will	look	in	detail	at	a	new	form	of	flash-based	disk
5</p>
<p>drive,	known	as	a	
solid	state	disk	(SSD)
,	that	provides	a	faster,	sturdier,
and	less	power-hungry	alternative	to	conventional	rotating	disks.
Programs	stored	in	ROM	devices	are	often	referred	to	as	
firmware.
When
a	computer	system	is	powered	up,	it	runs	firmware	stored	in	a	ROM.
Some	systems	provide	a	small	set	of	primitive	input	and	output	functions
in	firmware—for	example,	a	PC's	BIOS	(basic	input/output	system)
routines.	Complicated	devices	such	as	graphics	cards	and	disk	drive
controllers	also	rely	on	firmware	to	translate	I/O	(input/output)	requests
from	the	CPU.
Accessing	Main	Memory
Data	flows	back	and	forth	between	the	processor	and	the	DRAM	main
memory	over	shared	electrical	conduits	called	
buses.
Each	transfer	of
data	between	the	CPU	and	memory	is	accomplished	with	a	series	of
steps	called	a	
bus	transaction.
A	
read	transaction
transfers	data	from	the
main	memory	to	the	CPU.	A	
write	transaction
transfers	data	from	the
CPU	to	the	main	memory.
A	
bus
is	a	collection	of	parallel	wires	that	carry	address,	data,	and	control
signals.	Depending	on	the	particular	bus	design,	data	and	address
signals	can	share	the	same	set	of	wires	or	can	use	different	sets.	Also,
more	than	two	devices	can	share	the	same	bus.	The	control	wires	carry
signals	that	synchronize	the	transaction	and	identify	what	kind	of
transaction	is	currently	being	performed.	For	example,	is	this	transaction
of	interest	to	the	main	memory,	or	to	some	other	I/O	device	such	as	a
disk	controller?	Is	the	transaction	a	read	or	a	write?	Is	the	information	on
the	bus	an	address	or	a	data	item?</p>
<p>Figure	
6.6
shows	the	configuration	of	an	example	computer	system.
The	main	components	are	the	CPU	chip,	a	chipset	that	we	will	call	an	
I/O
bridge
(which	includes	the	memory	controller),	and	the	DRAM	memory
modules	that	make	up	main	memory.	These	components	are	connected
by	a	pair	of	buses:	a	
system	bus
that	connects	the	CPU	to	the	I/O	bridge,
and	a	
memory	bus
that	connects	the	I/O
Aside	
A	note	on	bus	designs
Bus	design	is	a	complex	and	rapidly	changing	aspect	of	computer
systems.	Different	vendors	develop	different	bus	architectures	as
a	way	to	differentiate	their	products.	For	example,	some	Intel
systems	use	chipsets	known	as	the	
northbridge
and	the
southbridge
to	connect	the	CPU	to	memory	and	I/O	devices,
respectively.	In	older	Pentium	and	Core	2	systems,	a	
front	side
bus
(FSB)	connects	the	CPU	to	the	northbridge.	Systems	from
AMD	replace	the	FSB	with	the	
HyperTransport
interconnect,	while
newer	Intel	Core	i7	systems	use	the	
QuickPath
interconnect.	The
details	of	these	different	bus	architectures	are	beyond	the	scope
of	this	text.	Instead,	we	will	use	the	high-level	bus	architecture
from	
Figure	
6.6
as	a	running	example	throughout.	It	is	a	simple
but	useful	abstraction	that	allows	us	to	be	concrete.	It	captures	the
main	ideas	without	being	tied	too	closely	to	the	detail	of	any
proprietary	designs.</p>
<p>Figure	
6.6	
Example	bus	structure	that	connects	the	CPU	and	main
memory.
bridge	to	the	main	memory.	The	I/O	bridge	translates	the	electrical
signals	of	the	system	bus	into	the	electrical	signals	of	the	memory	bus.
As	we	will	see,	the	I/O	bridge	also	connects	the	system	bus	and	memory
bus	to	an	
I/O	bus
that	is	shared	by	I/O	devices	such	as	disks	and
graphics	cards.	For	now,	though,	we	will	focus	on	the	memory	bus.
Consider	what	happens	when	the	CPU	performs	a	load	operation	such
as
where	the	contents	of	address	
A
are	loaded	into	register	
.	Circuitry
on	the	CPU	chip	called	the	
bus	interface
initiates	a	read	transaction	on
the	bus.	The	read	transaction	consists	of	three	steps.	First,	the	CPU
places	the	address	
A
on	the	system	bus.	The	I/O	bridge	passes	the
signal	along	to	the	memory	bus	(
Figure	
6.7(a)
).	Next,	the	main
memory	senses	the	address	signal	on	the	memory	bus,	reads	the
address	from	the	memory	bus,	fetches	the	data	from	the	DRAM,	and
writes	the	data	to	the	memory	bus.	The	I/O	bridge	translates	the	memory
bus	signal	into	a	system	bus	signal	and	passes	it	along	to	the	system	bus
(
Figure	
6.7(b)
).	Finally,	the	CPU	senses	the	data	on	the	system	bus,
reads	the	data	from	the	bus,	and	copies	the	data	to	register	
(
Figure
6.7(c)
).</p>
<p>Conversely,	when	the	CPU	performs	a	store	operation	such	as
Figure	
6.7	
Memory	read	transaction	for	a	load	operation:	
where	the	contents	of	register	
are	written	to	address	
A
,	the	CPU
initiates	a	write	transaction.	Again,	there	are	three	basic	steps.	First,	the
CPU	places	the	address	on	the	system	bus.	The	memory	reads	the</p>
<p>address	from	the	memory	bus	and	waits	for	the	data	to	arrive	(
Figure
6.8(a)
).	Next,	the	CPU	copies	the	data	in	
to	the	system	bus
(
Figure	
6.8(b)
).	Finally,	the	main	memory	reads	the	data	from	the
memory	bus	and	stores	the	bits	in	the	DRAM	(
Figure	
6.8(c)
).
6.1.2	
Disk	Storage
Disks
are	workhorse	storage	devices	that	hold	enormous	amounts	of
data,	on	the	order	of	hundreds	to	thousands	of	gigabytes,	as	opposed	to
the	hundreds	or	thousands	of	megabytes	in	a	RAM-based	memory.
However,	it	takes	on	the	order	of	milliseconds	to	read	information	from	a
disk,	a	hundred	thousand	times	longer	than	from	DRAM	and	a	million
times	longer	than	from	SRAM.</p>
<p>Figure	
6.8	
Memory	write	transaction	for	a	store	operation:	
Disk	Geometry
Disks	are	constructed	from	
platters.
Each	platter	consists	of	two	sides,	or
surfaces
,	that	are	coated	with	magnetic	recording	material.	A	rotating
spindle
in	the	center	of	the	platter	spins	the	platter	at	a	fixed	
rotational
rate
,	typically	between	5,400	and	15,000	
revolutions	per	minute	(RPM).
A
disk	will	typically	contain	one	or	more	of	these	platters	encased	in	a
sealed	container.</p>
<p>Figure	
6.9(a)
shows	the	geometry	of	a	typical	disk	surface.	Each
surface	consists	of	a	collection	of	concentric	rings	called	
tracks.
Each
track	is	partitioned	into	a	collection	of	
sectors.
Each	sector	contains	an
equal	number	of	data	bits	(typically	512	bytes)	encoded	in	the	magnetic
material	on	the	sector.	Sectors	are	separated	by	
gaps
where	no	data	bits
are	stored.	Gaps	store	formatting	bits	that	identify	sectors.
Figure	
6.9	
Disk	geometry.
A	disk	consists	of	one	or	more	platters	stacked	on	top	of	each	other	and
encased	in	a	sealed	package,	as	shown	in	
Figure	
6.9(b)
.	The	entire
assembly	is	often	referred	to	as	a	
disk	drive
,	although	we	will	usually
refer	to	it	as	simply	a	
disk.
We	will	sometimes	refer	to	disks	as	
rotating
disks
to	distinguish	them	from	flash-based	solid	state	disks	(SSDs),
which	have	no	moving	parts.
Disk	manufacturers	describe	the	geometry	of	multiple-platter	drives	in
terms	of	
cylinders
,	where	a	cylinder	is	the	collection	of	tracks	on	all	the
surfaces	that	are	equidistant	from	the	center	of	the	spindle.	For	example,
if	a	drive	has	three	platters	and	six	surfaces,	and	the	tracks	on	each
surface	are	numbered	consistently,	then	cylinder	
k
is	the	collection	of	the
six	instances	of	track	
k.</p>
<p>Disk	Capacity
The	maximum	number	of	bits	that	can	be	recorded	by	a	disk	is	known	as
its	
maximum	capacity
,	or	simply	
capacity.
Disk	capacity	is	determined	by
the	following	technology	factors:
Recording	density
(bits/in).	The	number	of	bits	that	can	be	squeezed
into	a	1-inch	segment	of	a	track.
Track	density
(tracks/in).	The	number	of	tracks	that	can	be	squeezed
into	a	l-inch	segment	of	the	radius	extending	from	the	center	of	the
platter.
Areal	density
(bits/in
).	The	product	of	the	recording	density	and	the
track	density.
Disk	manufacturers	work	tirelessly	to	increase	areal	density	(and	thus
capacity),	and	this	is	doubling	every	couple	of	years.	The	original	disks,
designed	in	an	age	of	low	areal	density,	partitioned	every	track	into	the
same	number	of	sectors,	which	was	determined	by	the	number	of	sectors
that	could	be	recorded	on	the	innermost	track.	To	maintain	a	fixed
number	of	sectors	per	track,	the	sectors	were	spaced	farther	apart	on	the
outer	tracks.	This	was	a	reasonable	approach
Aside	
How	much	is	a	gigabyte?
Unfortunately,	the	meanings	of	prefixes	such	as	kilo	(K),	mega
(M),	giga	(G),	and	tera	(T)	depend	on	the	context.	For	measures
that	relate	to	the	capacity	of	DRAMs	and	SRAMs,	typically	K	=	2
,
M	=	2
,	G	=	2
,	and	T	=	2
.	For	measures	related	to	the	capacity
2
10
20
30
40
3</p>
<h1>of	I/O	devices	such	as	disks	and	networks,	typically	K	=	10
,	M	=
10
,	G	=	10
,	and	T	=	10
.	Rates	and	throughputs	usually	use
these	prefix	values	as	well.
Fortunately,	for	the	back-of-the-envelope	estimates	that	we
typically	rely	on,	either	assumption	works	fine	in	practice.	For
example,	the	relative	difference	between	2
and	10
is	not	that
large:	(2
−	10
)/10
≈	7%.	Similarly,	(2
−	10
)/10
≈	10%.
when	areal	densities	were	relatively	low.	However,	as	areal	densities
increased,	the	gaps	between	sectors	(where	no	data	bits	were	stored)
became	unacceptably	large.	Thus,	modern	high-capacity	disks	use	a
technique	known	as	
multiple	zone	recording
,	where	the	set	of	cylinders	is
partitioned	into	disjoint	subsets	known	as	
recording	zones.
Each	zone
consists	of	a	contiguous	collection	of	cylinders.	Each	track	in	each
cylinder	in	a	zone	has	the	same	number	of	sectors,	which	is	determined
by	the	number	of	sectors	that	can	be	packed	into	the	innermost	track	of
the	zone.
The	capacity	of	a	disk	is	given	by	the	following	formula:
For	example,	suppose	we	have	a	disk	with	five	platters,	512	bytes	per
sector,	20,000	tracks	per	surface,	and	an	average	of	300	sectors	per
track.	Then	the	capacity	of	the	disk	is
Notice	that	manufacturers	express	disk	capacity	in	units	of	gigabytes
(GB)	or	terabytes	(TB),	where	1	GB	=	10
bytes	and	1	TB	=	10
bytes.
3
6
9
12
30
9
30
9
9
40
12
12
Capacity</h1>
<h1 id="-4"><a class="header" href="#-4"></a></h1>
<p> 
bytes
sector
×
verage
 </p>
<h1 id="-5"><a class="header" href="#-5"></a></h1>
<p> 
sectors
track
×</p>
<h1 id="-6"><a class="header" href="#-6"></a></h1>
<p> 
tracks
surface
×</p>
<h1 id="-7"><a class="header" href="#-7"></a></h1>
<h1> 
surfaces
Capacity</h1>
<p>512
 
bytes
sector
×
300
 
sectors
track
×
20
,
000
 
tracks
surface
×
2
 
surfaces
9
12</p>
<p>Practice	Problem	
6.2	
(solution	page	
661
)
What	is	the	capacity	of	a	disk	with	2	platters,	10,000	cylinders,	an
average	of	400	sectors	per	track,	and	512	bytes	per	sector?
Disk	Operation
Disks	read	and	write	bits	stored	on	the	magnetic	surface	using	a
read/write	head
connected	to	the	end	of	an	
actuator	arm
,	as	shown	in
Figure	
6.10(a)
.	By	moving
Figure	
6.10	
Disk	dynamics.
the	arm	back	and	forth	along	its	radial	axis,	the	drive	can	position	the
head	over	any	track	on	the	surface.	This	mechanical	motion	is	known	as
a	
seek.
Once	the	head	is	positioned	over	the	desired	track,	then,	as	each
bit	on	the	track	passes	underneath,	the	head	can	either	sense	the	value
of	the	bit	(read	the	bit)	or	alter	the	value	of	the	bit	(write	the	bit).	Disks
with	multiple	platters	have	a	separate	read/write	head	for	each	surface,
as	shown	in	
Figure	
6.10(b)
.	The	heads	are	lined	up	vertically	and</p>
<p>move	in	unison.	At	any	point	in	time,	all	heads	are	positioned	on	the
same	cylinder.
The	read/write	head	at	the	end	of	the	arm	flies	(literally)	on	a	thin	cushion
of	air	over	the	disk	surface	at	a	height	of	about	0.1	microns	and	a	speed
of	about	80	km/h.	This	is	analogous	to	placing	a	skyscraper	on	its	side
and	flying	it	around	the	world	at	a	height	of	2.5	cm	(1	inch)	above	the
ground,	with	each	orbit	of	the	earth	taking	only	8	seconds!	At	these
tolerances,	a	tiny	piece	of	dust	on	the	surface	is	like	a	huge	boulder.	If
the	head	were	to	strike	one	of	these	boulders,	the	head	would	cease
flying	and	crash	into	the	surface	(a	so-called	head	crash).	For	this
reason,	disks	are	always	sealed	in	airtight	packages.
Disks	read	and	write	data	in	sector-size	blocks.	The	
access	time
for	a
sector	has	three	main	components:	
seek	time,	rotational	latency
,	and
transfer	time:
Seek	time.	
To	read	the	contents	of	some	target	sector,	the	arm	first
positions	the	head	over	the	track	that	contains	the	target	sector.	The
time	required	to	move	the	arm	is	called	the	
seek	time.
The	seek	time,
T
,	depends	on	the	previous	position	of	the	head	and	the	speed	that
the	arm	moves	across	the	surface.	The	average	seek	time	in	modern
drives,	
T
,	measured	by	taking	the	mean	of	several	thousand
seeks	to	random	sectors,	is	typically	on	the	order	of	3	to	9	ms.	The
maximum	time	for	a	single	seek,	
T
,	can	be	as	high	as	20	ms.
Rotational	latency.	
Once	the	head	is	in	position	over	the	track,	the
drive	waits	for	the	first	bit	of	the	target	sector	to	pass	under	the	head.
The	performance	of	this	step	depends	on	both	the	position	of	the
surface	when	the	head	arrives	at	the	target	track	and	the	rotational
seek
avg	seek
max	seek</p>
<h1>speed	of	the	disk.	In	the	worst	case,	the	head	just	misses	the	target
sector	and	waits	for	the	disk	to	make	a	full	rotation.	Thus,	the
maximum	rotational	latency,	in	seconds,	is	given	by
The	average	rotational	latency,	
T
,	is	simply	half	of	
T
.
Transfer	time.	
When	the	first	bit	of	the	target	sector	is	under	the
head,	the	drive	can	begin	to	read	or	write	the	contents	of	the	sector.
The	transfer	time	for	one	sector	depends	on	the	rotational	speed	and
the	number	of	sectors	per	track.	Thus,	we	can	roughly	estimate	the
average	transfer	time	for	one	sector	in	seconds	as
We	can	estimate	the	average	time	to	access	the	contents	of	a	disk	sector
as	the	sum	of	the	average	seek	time,	the	average	rotational	latency,	and
the	average	transfer	time.	For	example,	consider	a	disk	with	the	following
parameters:
Parameter
Value
Rotational	rate
7,200	RPM
T
9	ms
Average	number	of	sectors/track
400
For	this	disk,	the	average	rotational	latency	(in	ms)	is
T
max
 
rotation</h1>
<h1>1
RPM
×
60
 
secs
1
 
min
avg	rotation
max	rotation
T
avg
 
transfer</h1>
<p>1
RPM
×
1
(
average
 </p>
<h1 id="-8"><a class="header" href="#-8"></a></h1>
<h1> 
sectors/track
)
×
60
 
secs
1
 
min
avg	seek
T
avg
 
rotation</h1>
<h1>1
/
2
×
T
max
 
rotation</h1>
<p>1
/
2
×
(
60
 
sec
s
/
7
,
200
 
RPM
)
×
1
,
000
 
ms/sec</p>
<h1>The	average	transfer	time	is
Putting	it	all	together,	the	total	estimated	access	time	is
This	example	illustrates	some	important	points:
The	time	to	access	the	512	bytes	in	a	disk	sector	is	dominated	by	the
seek	time	and	the	rotational	latency.	Accessing	the	first	byte	in	the
sector	takes	a	long	time,	but	the	remaining	bytes	are	essentially	free.
Since	the	seek	time	and	rotational	latency	are	roughly	the	same,	twice
the	seek	time	is	a	simple	and	reasonable	rule	for	estimating	disk
access	time.
The	access	time	for	a	64-bit	word	stored	in	SRAM	is	roughly	4	ns,	and
60	ns	for	DRAM.	Thus,	the	time	to	read	a	512-byte	sector-size	block
from	memory	is	roughly	256	ns	for	SRAM	and	4,000	ns	for	DRAM.
The	disk	access	time,	roughly	10	ms,	is	about	40,000	times	greater
than	SRAM,	and	about	2,500	times	greater	than	DRAM.
Practice	Problem	
6.3	
(solution	page	
661
)
Estimate	the	average	time	(in	ms)	to	access	a	sector	on	the
following	disk:
Parameter
Value
Rotational	rate
15,000	RPM
T
avg
 
rotation</h1>
<h1>60
/
7
,
200
 
RPM
×
1
/
400
 
sectors/track
×
1
,
000
 
ms/sec
≈
0.02
 
ms
T
access</h1>
<p>T
avg
 
seek</p>
<ul>
<li></li>
</ul>
<p>T
avg
 
rotation</p>
<ul>
<li></li>
</ul>
<h1>T
avg
 
transfer</h1>
<p>9
 
ms</p>
<ul>
<li></li>
</ul>
<p>4
 
ms</p>
<ul>
<li></li>
</ul>
<h1>0.02
 
ms</h1>
<p>13.02</p>
<p>T
8	ms
Average	number	of	sectors/track
500
Logical	Disk	Blocks
As	we	have	seen,	modern	disks	have	complex	geometries,	with	multiple
surfaces	and	different	recording	zones	on	those	surfaces.	To	hide	this
complexity	from	the	operating	system,	modern	disks	present	a	simpler
view	of	their	geometry	as	a	sequence	of	
B
sector-size	
logical	blocks
,
numbered	0,	1,	...,	
B
−	1.	A	small	hardware/firmware	device	in	the	disk
package,	called	the	
disk	controller
,	maintains	the	mapping	between
logical	block	numbers	and	actual	(physical)	disk	sectors.
When	the	operating	system	wants	to	perform	an	I/O	operation	such	as
reading	a	disk	sector	into	main	memory,	it	sends	a	command	to	the	disk
controller	asking	it	to	read	a	particular	logical	block	number.	Firmware	on
the	controller	performs	a	fast	table	lookup	that	translates	the	logical	block
number	into	a	
(surface,	track,	sector)
triple	that	uniquely	identifies	the
corresponding	physical	sector.	Hardware	on	the	controller	interprets	this
triple	to	move	the	heads	to	the	appropriate	cylinder,	waits	for	the	sector	to
pass	under	the	head,	gathers	up	the	bits	sensed	by	the	head	into	a	small
memory	buffer	on	the	controller,	and	copies	them	into	main	memory.
Practice	Problem	
6.4	
(solution	page	
661
)
Suppose	that	a	1	MB	file	consisting	of	512-byte	logical	blocks	is
stored	on	a	disk	drive	with	the	following	characteristics:
abg	seek</p>
<p>Aside	
Formatted	disk	capacity
Before	a	disk	can	be	used	to	store	data,	it	must	be
formatted
by	the	disk	controller.	This	involves	filling	in	the
gaps	between	sectors	with	information	that	identifies	the
sectors,	identifying	any	cylinders	with	surface	defects	and
taking	them	out	of	action,	and	setting	aside	a	set	of
cylinders	in	each	zone	as	spares	that	can	be	called	into
action	if	one	or	more	cylinders	in	the	zone	goes	bad	during
the	lifetime	of	the	disk.	The	
formatted	capacity
quoted	by
disk	manufacturers	is	less	than	the	maximum	capacity
because	of	the	existence	of	these	spare	cylinders.
Parameter
Value
Rotational	rate
10,000	RPM
T
5	ms
Average	number	of	sectors/track
1,000
Surfaces
4
Sector	size
512	bytes
For	each	case	below,	suppose	that	a	program	reads	the	logical
blocks	of	the	file	sequentially,	one	after	the	other,	and	that	the	time
to	position	the	head	over	the	first	block	is	
T
+	
T
.
A
.	
Best	case:	
Estimate	the	optimal	time	(in	ms)	required	to
read	the	file	given	the	best	possible	mapping	of	logical
blocks	to	disk	sectors	(i.e.,	sequential).
B
.	
Random	case:	
Estimate	the	time	(in	ms)	required	to	read
the	file	if	blocks	are	mapped	randomly	to	disk	sectors.
avg	seek
avg	seek
avg	rotation</p>
<p>Connecting	I/O	Devices
Input/output	(I/O)	devices	such	as	graphics	cards,	monitors,	mice,
keyboards,	and	disks	are	connected	to	the	CPU	and	main	memory	using
an	
I/O	bus.
Unlike	the	system	bus	and	memory	buses,	which	are	CPU-
specific,	I/O	buses	are	designed	to	be	independent	of	the	underlying
CPU.	
Figure	
6.11
shows	a	representative	I/O	bus	structure	that
connects	the	CPU,	main	memory,	and	I/O	devices.
Although	the	I/O	bus	is	slower	than	the	system	and	memory	buses,	it	can
accommodate	a	wide	variety	of	third-party	I/O	devices.	For	example,	the
bus	in	
Figure	
6.11
has	three	different	types	of	devices	attached	to	it.
A	
Universal	Serial	Bus	(USB)
controller	is	a	conduit	for	devices
attached	to	a	USB	bus,	which	is	a	wildly	popular	standard	for
connecting	a	variety	of	peripheral	I/O	devices,	including	keyboards,
mice,	modems,	digital	cameras,	game	controllers,	printers,	external
disk	drives,	and	solid	state	disks.	USB	3.0	buses	have	a	maximum
bandwidth	of	625	MB/s.	USB	3.1	buses	have	a	maximum	bandwidth
of	1,250	MB/s.</p>
<p>Figure	
6.11	
Example	bus	structure	that	connects	the	CPU,	main
memory,	and	I/O	devices.
A	
graphics	card
(or	
adapter
)	contains	hardware	and	software	logic
that	is	responsible	for	painting	the	pixels	on	the	display	monitor	on
behalf	of	the	CPU.
A	
host	bus	adapter
that	connects	one	or	more	disks	to	the	I/O	bus
using	a	communication	protocol	defined	by	a	particular	
host	bus
interface.
The	two	most	popular	such	interfaces	for	disks	are	
SCSI
(pronounced	&quot;scuzzy&quot;)	and	
SATA
(pronounced	&quot;sat-uh&quot;).	SCSI	disks
are	typically	faster	and	more	expensive	than	SATA	drives.	A	SCSI
host	bus	adapter	(often	called	a	
SCSI	controller)
can	support	multiple
disk	drives,	as	opposed	to	SATA	adapters,	which	can	only	support
one	drive.</p>
<p>Additional	devices	such	as	
network	adapters
can	be	attached	to	the	I/O
bus	by	plugging	the	adapter	into	empty	
expansion	slots
on	the
motherboard	that	provide	a	direct	electrical	connection	to	the	bus.
Accessing	Disks
While	a	detailed	description	of	how	I/O	devices	work	and	how	they	are
programmed	is	outside	our	scope	here,	we	can	give	you	a	general	idea.
For	example,	
Figure	
6.12
summarizes	the	steps	that	take	place	when
a	CPU	reads	data	from	a	disk.
Aside	
Advances	in	I/O	bus	designs
The	I/O	bus	in	
Figure	
6.11
is	a	simple	abstraction	that	allows
us	to	be	concrete,	without	being	tied	too	closely	to	the	details	of
any	specific	system.	It	is	based	on	the	
peripheral	component
interconnect	(PCI)
bus,	which	was	popular	until	around	2010.	In
the	PCI	model,	each	device	in	the	system	shares	the	bus,	and
only	one	device	at	a	time	can	access	these	wires.	In	modern
systems,	the	shared	PCI	bus	has	been	replaced	by	a	
PCI	express
(PCIe)	bus,	which	is	a	set	of	high-speed	serial,	point-to-point	links
connected	by	switches,	akin	to	the	switched	Ethernets	that	you
will	learn	about	in	
Chapter	
11
.	A	PCIe	bus,	with	a	maximum
throughput	of	16	GB/s,	is	an	order	of	magnitude	faster	than	a	PCI
bus,	which	has	a	maximum	throughput	of	533	MB/s.	Except	for
measured	I/O	performance,	the	differences	between	the	different
bus	designs	are	not	visible	to	application	programs,	so	we	will	use
the	simple	shared	bus	abstraction	throughout	the	text.</p>
<p>The	CPU	issues	commands	to	I/O	devices	using	a	technique	called
memory-mapped	I/O
(
Figure	
6.12(a)
).	In	a	system	with	memory-
mapped	I/O,	a	block	of	addresses	in	the	address	space	is	reserved	for
communicating	with	I/O	devices.	Each	of	these	addresses	is	known	as	an
I/O	port.
Each	device	is	associated	with	(or	mapped	to)	one	or	more	ports
when	it	is	attached	to	the	bus.
As	a	simple	example,	suppose	that	the	disk	controller	is	mapped	to	port
.	Then	the	CPU	might	initiate	a	disk	read	by	executing	three	store
instructions	to	address	
:	The	first	of	these	instructions	sends	a
command	word	that	tells	the	disk	to	initiate	a	read,	along	with	other
parameters	such	as	whether	to	interrupt	the	CPU	when	the	read	is
finished.	(We	will	discuss	interrupts	in	
Section	
8.1
.)	The	second
instruction	indicates	the	logical	block	number	that	should	be	read.	The
third	instruction	indicates	the	main	memory	address	where	the	contents
of	the	disk	sector	should	be	stored.
After	it	issues	the	request,	the	CPU	will	typically	do	other	work	while	the
disk	is	performing	the	read.	Recall	that	a	1	GHz	processor	with	a	1	ns
clock	cycle	can	potentially	execute	16	million	instructions	in	the	16	ms	it
takes	to	read	the	disk.	Simply	waiting	and	doing	nothing	while	the
transfer	is	taking	place	would	be	enormously	wasteful.
After	the	disk	controller	receives	the	read	command	from	the	CPU,	it
translates	the	logical	block	number	to	a	sector	address,	reads	the
contents	of	the	sector,	and	transfers	the	contents	directly	to	main
memory,	without	any	intervention	from	the	CPU	(
Figure	
6.12(b)
).	This
process,	whereby	a	device	performs	a	read	or	write	bus	transaction	on	its</p>
<p>own,	without	any	involvement	of	the	CPU,	is	known	as	
direct	memory
access
(DMA).	The	transfer	of	data	is	known	as	a	
DMA	transfer.
After	the	DMA	transfer	is	complete	and	the	contents	of	the	disk	sector	are
safely	stored	in	main	memory,	the	disk	controller	notifies	the	CPU	by
sending	an	interrupt	signal	to	the	CPU	(
Figure	
6.12(c)
).	The	basic
idea	is	that	an	interrupt	signals	an	external	pin	on	the	CPU	chip.	This
causes	the	CPU	to	stop	what	it	is	currently	working	on	and	jump	to	an
operating	system	routine.	The	routine	records	the	fact	that	the	I/O	has
finished	and	then	returns	control	to	the	point	where	the	CPU	was
interrupted.</p>
<p>Figure	
6.12	
Reading	a	disk	sector.
Aside	
Characteristics	of	a	commercial
disk	drive
Disk	manufacturers	publish	a	lot	of	useful	high-level	technical
information	on	their	Web	sites.	For	example,	the	Seagate	Web	site
contains	the	following	information	(and	much	more!)	about	one	of
their	popular	drives,	the	Barracuda	7400.	(
Seagate.com
)
Geometry	characteristic
Value
Surface	diameter
3.5	in
Formatted	capacity
3	TB
Platters
3
Surfaces
6
Logical	blocks
5,860,533,168
Logical	block	size
512	bytes
Rotational	rate
7,200	RPM
Average	rotational	latency
4.16	ms
Average	seek	time
8.5	ms
Track-to-track	seek	time
1.0	ms
Average	transfer	rate
156	MB/s</p>
<p>Maximum	sustained	transfer	rate
210	MB/s
Figure	
6.13	
Solid	state	disk	(SSD).
6.1.3	
Solid	State	Disks
A	solid	state	disk	(SSD)	is	a	storage	technology,	based	on	flash	memory
(
Section	
6.1.1
),	that	in	some	situations	is	an	attractive	alternative	to
the	conventional	rotating	disk.	
Figure	
6.13
shows	the	basic	idea.	An
SSD	package	plugs	into	a	standard	disk	slot	on	the	I/O	bus	(typically
USB	or	SATA)	and	behaves	like	any	other	disk,	processing	requests	from
the	CPU	to	read	and	write	logical	disk	blocks.	An	SSD	package	consists
of	one	or	more	flash	memory	chips,	which	replace	the	mechanical	drive
in	a	conventional	rotating	disk,	and	a	
flash	translation	layer
,	which	is	a
hardware/firmware	device	that	plays	the	same	role	as	a	disk	controller,
translating	requests	for	logical	blocks	into	accesses	of	the	underlying
physical	device.
Figure	
6.14
shows	the	performance	characteristics	of	a	typical	SSD.
Notice	that	reading	from	SSDs	is	faster	than	writing.	The	difference
between	random	reading	and	writing	performance	is	caused	by	a</p>
<p>fundamental	property	of	the	underlying	flash	memory.	As	shown	in
Figure	
6.13
,	a	flash	memory	consists	of	a	sequence	of	
B	blocks
,
where	each	block	consists	of	
P
pages.	Typically,	pages	are	512	bytes	to
4	KB	in	size,	and	a	block	consists	of	32−128	pages,	with	total	block	sizes
ranging	from	16
Reads
Writes
Sequential	read	throughput
550	MB/s
Sequential	write	throughput
470	MB/s
Random	read	throughput
(IOPS)
89,000
IOPS
Random	write	throughput
(IOPS)
74,000
IOPS
Random	read	throughput
(MB/s)
365	MB/s
Random	write	throughput
(MB/s)
303	MB/s
Avg.	sequential	read	access
time
50	
μ
s
Avg.	sequential	write	access
time
60	
μ
s
Figure	
6.14	
Performance	characteristics	of	a	commercial	solid	state
disk.
Source:	
Intel	SSD	730	product	specification	[
53
].	
IOPS
is	I/O	operations	per	second.	Throughput	numbers	are	based	on
reads	and	writes	of	4	KB	blocks.	(Intel	SSD	730	product	specification.	Intel	Corporation.	52.)
KB	to	512	KB.	Data	are	read	and	written	in	units	of	pages.	A	page	can	be
written	only	after	the	entire	block	to	which	it	belongs	has	been	
erased
(typically,	this	means	that	all	bits	in	the	block	are	set	to	1).	However,	once
a	block	is	erased,	each	page	in	the	block	can	be	written	once	with	no
further	erasing.	A	block	wears	out	after	roughly	100,000	repeated	writes.
Once	a	block	wears	out,	it	can	no	longer	be	used.</p>
<p>Random	writes	are	slower	for	two	reasons.	First,	erasing	a	block	takes	a
relatively	long	time,	on	the	order	of	1	ms,	which	is	more	than	an	order	of
magnitude	longer	than	it	takes	to	access	a	page.	Second,	if	a	write
operation	attempts	to	modify	a	page	
p
that	contains	existing	data	(i.e.,	not
all	ones),	then	any	pages	in	the	same	block	with	useful	data	must	be
copied	to	a	new	(erased)	block	before	the	write	to	page	
p
can	occur.
Manufacturers	have	developed	sophisticated	logic	in	the	flash	translation
layer	that	attempts	to	amortize	the	high	cost	of	erasing	blocks	and	to
minimize	the	number	of	internal	copies	on	writes,	but	it	is	unlikely	that
random	writing	will	ever	perform	as	well	as	reading.
SSDs	have	a	number	of	advantages	over	rotating	disks.	They	are	built	of
semiconductor	memory,	with	no	moving	parts,	and	thus	have	much	faster
random	access	times	than	rotating	disks,	use	less	power,	and	are	more
rugged.	However,	there	are	some	disadvantages.	First,	because	flash
blocks	wear	out	after	repeated	writes,	SSDs	have	the	potential	to	wear
out	as	well.	
Wear-leveling
logic	in	the	flash	translation	layer	attempts	to
maximize	the	lifetime	of	each	block	by	spreading	erasures	evenly	across
all	blocks.	In	practice,	the	wear-leveling	logic	is	so	good	that	it	takes
many	years	for	SSDs	to	wear	out	(see	
Practice	Problem	
6.5
).
Second,	SSDs	are	about	30	times	more	expensive	per	byte	than	rotating
disks,	and	thus	the	typical	storage	capacities	are	significantly	less	than
rotating	disks.	However,	SSD	prices	are	decreasing	rapidly	as	they
become	more	popular,	and	the	gap	between	the	two	is	decreasing.
SSDs	have	completely	replaced	rotating	disks	in	portable	music	devices,
are	popular	as	disk	replacements	in	laptops,	and	have	even	begun	to
appear	in	desktops	and	servers.	While	rotating	disks	are	here	to	stay,	it	is
clear	that	SSDs	are	an	important	alternative.</p>
<p>Practice	Problem	
6.5	
(solution	page	
662
)
As	we	have	seen,	a	potential	drawback	of	SSDs	is	that	the
underlying	flash	memory	can	wear	out.	For	example,	for	the	SSD
in	
Figure	
6.14
,	Intel	guarantees	about	
128	petabytes	(128	×
10
bytes)	of	writes	before	the	drive	wears	out.	Given	this
assumption,	estimate	the	lifetime	(in	years)	of	this	SSD	for	the
following	workloads:
A
.	
Worst	case	for	sequential	writes:
The	SSD	is	written	to
continuously	at	a	rate	of	470	MB/s	(the	average	sequential
write	throughput	of	the	device).
B
.	
Worst	case	for	random	writes:
The	SSD	is	written	to
continuously	at	a	rate	of	303	MB/s	(the	average	random
write	throughput	of	the	device).
C
.	
Average	case:
The	SSD	is	written	to	at	a	rate	of	20	GB/day
(the	average	daily	write	rate	assumed	by	some	computer
manufacturers	in	their	mobile	computer	workload
simulations).
6.1.4	
Storage	Technology	Trends
There	are	several	important	concepts	to	take	away	from	our	discussion	of
storage	technologies.
Different	storage	technologies	have	different	price	and	performance
trade-offs.
SRAM	is	somewhat	faster	than	DRAM,	and	DRAM	is	much
faster	than	disk.	On	the	other	hand,	fast	storage	is	always	more
expensive	than	slower	storage.	SRAM	costs	more	per	byte	than	DRAM.
15</p>
<p>DRAM	costs	much	more	than	disk.	SSDs	split	the	difference	between
DRAM	and	rotating	disk.
The	price	and	performance	properties	of	different	storage	technologies
are	changing	at	dramatically	different	rates.</p>
<p>Figure	
6.15
summarizes
the	price	and	performance	properties	of	storage	technologies	since	1985,
shortly	after	the	first	PCs	were	introduced.	The	numbers	were	culled	from
back	issues	of	trade	magazines	and	the	Web.	Although	they	were
collected	in	an	informal	survey,	the	numbers	reveal	some	interesting
trends.
Since	1985,	both	the	cost	and	performance	of	SRAM	technology	have
improved	at	roughly	the	same	rate.	Access	times	and	cost	per	megabyte
have	decreased	by	a	factor	of	about	100	(
Figure	
6.15(a)
).	However,
the	trends	for	DRAM	and	disk	are	much	more	dramatic	and	divergent.
While	the	cost	per	megabyte	of	DRAM	has	decreased	by	a	factor	of
44,000	(more	than	four	orders	of	magnitude!),	DRAM	access	times	have
decreased	by	only	a	factor	of	10	(
Figure	
6.15(b)
).	Disk	technology	has
followed	the	same	trend	as	DRAM	and	in	even	more	dramatic	fashion.
While	the	cost	of	a	megabyte	of	disk	storage	has	plummeted	by	a	factor
of	more	than	3,000,000	(more	than	six	orders	of	magnitude!)	since	1980,
access	times	have	improved	much	more	slowly,	by	only	a	factor	of	25
(
Figure	
6.15(c)
).	These	startling	long-term	trends	highlight	a	basic
truth	of	memory	and	disk	technology:	it	is	much	easier	to	increase	density
(and	thereby	reduce	cost)	than	to	decrease	access	time.
DRAM	and	disk	performance	are	lagging	behind	CPU	performance.
As
we	see	in	
Figure	
6.15(d)
,	CPU	cycle	times	improved	by	a	factor	of	500
between	1985	and	2010.	If	we	look	at	the	
effective	cycle	time
—which	we
define	to	be	the	cycle	time	of	an	individual	CPU	(processor)	divided	by</p>
<p>the	number	of	its	processor	cores—then	the	improvement	between	1985
and	2010	is	even	greater,	a	factor	of	2,000.
Metric
1985
1990
1995
2000
2005
2010
2015
2015:1985
$/MB
2,900
320
256
100
75
60
25
116
Access	(ns)
150
35
15
3
2
1.5
1.3
115
(a)	SRAM	trends
Metric
1985
1990
1995
2000
2005
2010
2015
2015:1985
$/MB
880
100
30
1
0.1
0.06
0.02
44,000
Access	(ns)
200
100
70
60
50
40
20
10
Typical	size
(MB)
0.256
4
16
64
2,000
8,000
16,000
62,500
(b)	DRAM	trends
Metric
1985
1990
1995
2000
2005
2010
2015
2015:1985
$/GB
100,000
8,000
300
10
5
0.3
0.03
3,333,333
Min.	seek	time
(ms)
75
28
10
8
5
3
3
25
Typical	size
(GB)
0.01
0.16
1
20
160
1,500
3,000
300,000
(c)	Rotating	disk	trends
Metric
1985
1990
1995
2000
2003
2005
2010
2015
2015:1985</p>
<p>Intel
CPU
80286
80386
Pent.
P-III
Pent.
4
Core
2
Core
i7	(n)
Core
i7	(h)
—
Clock
rate
(MHz)
6
20
150
600
3,300
2,000
2,500
3,000
500
Cycle
time
(ns)
166
50
6
1.6
0.3
0.5
0.4
0.33
500
Cores
1
1
1
1
1
2
4
4
4
Effective
cycle
time
(ns)
166
50
6
1.6
0.30
0.25
0.10
0.08
2,075
(d)	CPU	trends
Figure	
6.15	
Storage	and	processing	technology	trends.
The	Core	i7	circa	201	0	uses	the	Nehalem	processor	core.	The	Core	i7
circa	201	5	uses	the	Haswell	core.
The	split	in	the	CPU	performance	curve	around	2003	reflects	the
introduction	of	multi-core	processors	(see	aside	on	page	605).	After	this
split,	cycle	times	of	individual	cores	actually	increased	a	bit	before
starting	to	decrease	again,	albeit	at	a	slower	rate	than	before.
Note	that	while	SRAM	performance	lags,	it	is	roughly	keeping	up.
However,	the	gap	between	DRAM	and	disk	performance	and	CPU
performance	is	actually	widening.	Until	the	advent	of	multi-core
processors	around	2003,	this	performance	gap	was	a	function	of	latency,</p>
<p>with	DRAM	and	disk	access	times	decreasing	more	slowly	than	the	cycle
time	of	an	individual	processor.	However,	with	the	introduction	of	multiple
cores,	this	performance	gap	is	increasingly	a	function	of
Figure	
6.16	
The	gap	between	disk,	DRAM,	and	CPU	speeds.
throughput,	with	multiple	processor	cores	issuing	requests	to	the	DRAM
and	disk	in	parallel.
The	various	trends	are	shown	quite	clearly	in	
Figure	
6.16
,	which	plots
the	access	and	cycle	times	from	
Figure	
6.15
on	a	semi-log	scale.
As	we	will	see	in	
Section	
6.4
,	modern	computers	make	heavy	use	of
SRAM-based	caches	to	try	to	bridge	the	processor-memory	gap.	This
approach	works	because	of	a	fundamental	property	of	application
programs	known	as	
locality
,	which	we	discuss	next.
Practice	Problem	
6.6	
(solution	page	
662
)</p>
<p>Using	the	data	from	the	years	2005	to	2015	in	
Figure	
6.15(c)
,
estimate	the	year	when	you	will	be	able	to	buy	a	petabyte	(10
bytes)	of	rotating	disk	storage	for	$500.	Assume	actual	dollars	(no
inflation).
15</p>
<p>6.2	
Locality
Well-written	computer	programs	tend	to	exhibit	good	
locality.
That	is,	they
tend	to	reference	data	items	that	are	near	other	recently	referenced	data
items	or	that	were	recently	referenced	themselves.	This	tendency,	known
as	the	
principle	of	locality
,	is	an	enduring	concept	that	has	enormous
impact	on	the	design	and	performance	of	hardware	and	software
systems.
Locality	is	typically	described	as	having	two	distinct	forms:	
temporal
locality
and	
spatial	locality.
In	a	program	with	good	temporal	locality,	a
memory	location	that	is	referenced	once	is	likely	to	be	referenced	again
multiple	times	in	the	near	future.	In	a	program	with	good	spatial	locality,	if
a	memory	location	is	referenced
Aside	
When	cycle	time	stood	still:	The
advent	of	multi-core	processors
The	history	of	computers	is	marked	by	some	singular	events	that
caused	profound	changes	in	the	industry	and	the	world.
Interestingly,	these	inflection	points	tend	to	occur	about	once	per
decade:	the	development	of	Fortran	in	the	1950s,	the	introduction
of	the	IBM	360	in	the	early	1960s,	the	dawn	of	the	Internet	(then
called	ARPANET)	in	the	early	1970s,	the	introduction	of	the	IBM
PC	in	the	early	1980s,	and	the	creation	of	the	World	Wide	Web	in
the	early	1990s.</p>
<p>The	most	recent	such	event	occurred	early	in	the	21st	century,
when	computer	manufacturers	ran	headlong	into	the	so-called
power	wall,	discovering	that	they	could	no	longer	increase	CPU
clock	frequencies	as	quickly	because	the	chips	would	then
consume	too	much	power.	The	solution	was	to	improve
performance	by	replacing	a	single	large	processor	with	multiple
smaller	processor	
cores
,	each	a	complete	processor	capable	of
executing	programs	independently	and	in	parallel	with	the	other
cores.	This	
multi-core
approach	works	in	part	because	the	power
consumed	by	a	processor	is	proportional	to	
P
=	
fCV
,	where	
f
is
the	clock	frequency,	
C
is	the	capacitance,	and	
V
is	the	voltage.
The	capacitance	
C
is	roughly	proportional	to	the	area,	so	the
power	drawn	by	multiple	cores	can	be	held	constant	as	long	as
the	total	area	of	the	cores	is	constant.	As	long	as	feature	sizes
continue	to	shrink	at	the	exponential	Moore's	Law	rate,	the
number	of	cores	in	each	processor,	and	thus	its	effective
performance,	will	continue	to	increase.
From	this	point	forward,	computers	will	get	faster	not	because	the
clock	frequency	increases	but	because	the	number	of	cores	in
each	processor	increases,	and	because	architectural	innovations
increase	the	efficiency	of	programs	running	on	those	cores.	We
can	see	this	trend	clearly	in	
Figure	
6.16
.	CPU	cycle	time
reached	its	lowest	point	in	2003	and	then	actually	started	to	rise
before	leveling	off	and	starting	to	decline	again	at	a	slower	rate
than	before.	However,	because	of	the	advent	of	multi-core
processors	(dual-core	in	2004	and	quad-core	in	2007),	the
effective	cycle	time	continues	to	decrease	at	close	to	its	previous
rate.
2</p>
<p>once,	then	the	program	is	likely	to	reference	a	nearby	memory	location	in
the	near	future.
Programmers	should	understand	the	principle	of	locality	because,	in
general,	
programs	with	good	locality	run	faster	than	programs	with	poor
locality.
All	levels	of	modern	computer	systems,	from	the	hardware,	to	the
operating	system,	to	application	programs,	are	designed	to	exploit
locality.	At	the	hardware	level,	the	principle	of	locality	allows	computer
designers	to	speed	up	main	memory	accesses	by	introducing	small	fast
memories	known	as	
cache	memories
that	hold	blocks	of	the	most
recently	referenced	instructions	and	data	items.	At	the	operating	system
level,	the	principle	of	locality	allows	the	system	to	use	the	main	memory
as	a	cache	of	the	most	recently	referenced	chunks	of	the	virtual	address
space.	Similarly,	the	operating	system	uses	main	memory	to	cache	the
most	recently	used	disk	blocks	in	the	disk	file	system.	The	principle	of
locality	also	plays	a	crucial	role	in	the	design	of	application	programs.	For
example,	Web	browsers	exploit	temporal	locality	by	caching	recently
referenced	documents	on	a	local	disk.	High-volume	Web	servers	hold
recently	requested	documents	in	front-end	disk	caches	that	satisfy
requests	for	these	documents	without	requiring	any	intervention	from	the
server.</p>
<p>(a)
Address
0
4
8
12
16
20
24
28
Contents
v
v
v
v
v
v
v
v
Access	order
1
2
3
4
5
6
7
8
(b)
Figure	
6.17	
(a)	A	function	with	good	locality,	(b)	Reference	pattern
for	vector	
(
N
=	8).
Notice	how	the	vector	elements	are	accessed	in	the	same	order	that	they
are	stored	in	memory.
6.2.1	
Locality	of	References	to
Program	Data
Consider	the	simple	function	in	
Figure	
6.17(a)
that	sums	the	elements
of	a	vector.	Does	this	function	have	good	locality?	To	answer	this
question,	we	look	at	the	reference	pattern	for	each	variable.	In	this
example,	the	
variable	is	referenced	once	in	each	loop	iteration,	and
thus	there	is	good	temporal	locality	with	respect	to	
.	On	the	other
hand,	since	
is	a	scalar,	there	is	no	spatial	locality	with	respect	to	
.
0
1
2
3
4
5
6
7</p>
<p>As	we	see	in	
Figure	
6.17(b)
,	the	elements	of	vector	
are	read
sequentially,	one	after	the	other,	in	the	order	they	are	stored	in	memory
(we	assume	for	convenience	that	the	array	starts	at	address	0).	Thus,
with	respect	to	variable	v,	the	function	has	good	spatial	locality	but	poor
temporal	locality	since	each	vector	element	is	accessed	exactly	once.
Since	the	function	has	either	good	spatial	or	temporal	locality	with
respect	to	each	variable	in	the	loop	body,	we	can	conclude	that	the
function	enjoys	good	locality.
A	function	such	as	
that	visits	each	element	of	a	vector
sequentially	is	said	to	have	a	
stride-1	reference	pattern
(with	respect	to
the	element	size).	We	will	sometimes	refer	to	stride-1	reference	patterns
as	
sequential	reference	patterns.
Visiting	every	
k
th	element	of	a
contiguous	vector	is	called	a	
stride-k	reference	pattern.
Stride-1
reference	patterns	are	a	common	and	important	source	of	spatial	locality
in	programs.	In	general,	as	the	stride	increases,	the	spatial	locality
decreases.
Stride	is	also	an	important	issue	for	programs	that	reference
multidimensional	arrays.	For	example,	consider	the	
function
in	
Figure	
6.18(a)
that	sums	the	elements	of	a	two-dimensional	array.
The	doubly	nested	loop	reads	the	elements	of	the	array	in	
row-major
order.
That	is,	the	inner	loop	reads	the	elements	of	the	first	row,	then	the
second	row,	and	so	on.	The	
function	enjoys	good	spatial
locality	because	it	references	the	array	in	the	same	row-major	order	that
the	array	is	stored	(
Figure	
6.18(b)
).	The	result	is	a	nice	stride-1
reference	pattern	with	excellent	spatial	locality.</p>
<p>(a)
Address
0
4
8
12
16
20
Contents
a
a
a
a
a
a
Access	order
1
2
3
4
5
6
(b)
Figure	
6.18	
(a)	Another	function	with	good	locality,	(b)	Reference
pattern	for	array	
a
(
M
=	2,	
N
=	3).
There	is	good	spatial	locality	because	the	array	is	accessed	in	the	same
row-major	order	in	which	it	is	stored	in	memory.
00
01
02
10
11
12</p>
<p>(a)
Address
0
4
8
12
16
20
Contents
a
a
a
a
a
a
Access	order
1
3
5
2
4
6
(b)
Figure	
6.19	
(a)	A	function	with	poor	spatial	locality,	(b)	Reference
pattern	for	array	a	(
M
=	2,	
N
=	3).
The	function	has	poor	spatial	locality	because	it	scans	memory	with	a
stride-N	reference	pattern.
Seemingly	trivial	changes	to	a	program	can	have	a	big	impact	on	its
locality.	For	example,	the	
function	in	
Figure	
6.19(a)
computes	the	same	result	as	the	
function	in	
Figure
6.18(a)
.	The	only	difference	is	that	we	have	interchanged	the	
i
and	
j
loops.	What	impact	does	interchanging	the	loops	have	on	its	locality?
00
01
02
10
11
12</p>
<p>The	
function	suffers	from	poor	spatial	locality	because	it
scans	the	array	column-wise	instead	of	row-wise.	Since	C	arrays	are	laid
out	in	memory	row-wise,	the	result	is	a	stride-
N
reference	pattern,	as
shown	in	
Figure	
6.19(b)
.
6.2.2	
Locality	of	Instruction	Fetches
Since	program	instructions	are	stored	in	memory	and	must	be	fetched
(read)	by	the	CPU,	we	can	also	evaluate	the	locality	of	a	program	with
respect	to	its	instruction	fetches.	For	example,	in	
Figure	
6.17
the
instructions	in	the	body	of	the	
loop	are	executed	in	sequential
memory	order,	and	thus	the	loop	enjoys	good	spatial	locality.	Since	the
loop	body	is	executed	multiple	times,	it	also	enjoys	good	temporal
locality.
An	important	property	of	code	that	distinguishes	it	from	program	data	is
that	it	is	rarely	modified	at	run	time.	While	a	program	is	executing,	the
CPU	reads	its	instructions	from	memory.	The	CPU	rarely	overwrites	or
modifies	these	instructions.
6.2.3	
Summary	of	Locality
In	this	section,	we	have	introduced	the	fundamental	idea	of	locality	and
have	identified	some	simple	rules	for	qualitatively	evaluating	the	locality
in	a	program:</p>
<p>Programs	that	repeatedly	reference	the	same	variables	enjoy	good
temporal	locality.
For	programs	with	stride-
k
reference	patterns,	the	smaller	the	stride,
the	better	the	spatial	locality.	Programs	with	stride-1	reference
patterns	have	good	spatial	locality.	Programs	that	hop	around
memory	with	large	strides	have	poor	spatial	locality.
Loops	have	good	temporal	and	spatial	locality	with	respect	to
instruction	fetches.	The	smaller	the	loop	body	and	the	greater	the
number	of	loop	iterations,	the	better	the	locality.
Later	in	this	chapter,	after	we	have	learned	about	cache	memories	and
how	they	work,	we	will	show	you	how	to	quantify	the	idea	of	locality	in
terms	of	cache	hits	and	misses.	It	will	also	become	clear	to	you	why
programs	with	good	locality	typically	run	faster	than	programs	with	poor
locality.	Nonetheless,	knowing	how	to	glance	at	a	source	code	and
getting	a	high-level	feel	for	the	locality	in	the	program	is	a	useful	and
important	skill	for	a	programmer	to	master.
Practice	Problem	
6.7	
(solution	page	
662
)
Permute	the	loops	in	the	following	function	so	that	it	scans	the
three-dimensional	array	
a
with	a	stride-1	reference	pattern.</p>
<p>(a)	An	array	of	
(b)	The	
function</p>
<p>(c)	The	
function
(d)	The	
function</p>
<p>Figure	
6.20	
Code	examples	for	
Practice	Problem	
6.8
.
Practice	Problem	
6.8	
(solution	page	
663
)
The	three	functions	in	
Figure	
6.20
perform	the	same	operation
with	varying	degrees	of	spatial	locality.	Rank-order	the	functions
with	respect	to	the	spatial	locality	enjoyed	by	each.	Explain	how
you	arrived	at	your	ranking.</p>
<p>6.3	
The	Memory	Hierarchy
Section	
6.1
and	
6.2
described	some	fundamental	and	enduring
properties	of	storage	technology	and	computer	software:
Storage	technology.	
Different	storage	technologies	have	widely
different	access	times.	Faster	technologies	cost	more	per	byte	than
slower	ones	and	have	less	capacity.	The	gap	between	CPU	and	main
memory	speed	is	widening.
Computer	software.	
Well-written	programs	tend	to	exhibit	good
locality.
Figure	
6.21	
The	memory	hierarchy.
In	one	of	the	happier	coincidences	of	computing,	these	fundamental
properties	of	hardware	and	software	complement	each	other	beautifully.</p>
<p>Their	complementary	nature	suggests	an	approach	for	organizing
memory	systems,	known	as	the	
memory	hierarchy
,	that	is	used	in	all
modern	computer	systems.	
Figure	
6.21
shows	a	typical	memory
hierarchy.
In	general,	the	storage	devices	get	slower,	cheaper,	and	larger	as	we
move	from	higher	to	lower	
levels.
At	the	highest	level	(L0)	are	a	small
number	of	fast	CPU	registers	that	the	CPU	can	access	in	a	single	clock
cycle.	Next	are	one	or	more	small	to	moderate-size	SRAM-based	cache
memories	that	can	be	accessed	in	a	few	CPU	clock	cycles.	These	are
followed	by	a	large	DRAM-based	main	memory	that	can	be	accessed	in
tens	to	hundreds	of	clock	cycles.	Next	are	slow	but	enormous	local	disks.
Finally,	some	systems	even	include	an	additional	level	of	disks	on	remote
servers	that	can	be	accessed	over	a	network.	For	example,	distributed
file	systems	such	as	the	Andrew	File	System	(AFS)	or	the	Network	File
System	(NFS)	allow	a	program	to	access	files	that	are	stored	on	remote
network-connected	servers.	Similarly,	the	World	Wide	Web	allows
programs	to	access	remote	files	stored	on	Web	servers	anywhere	in	the
world.</p>
<div style="break-before: page; page-break-before: always;"></div><p>6.3.1	
Caching	in	the	Memory
Hierarchy
In	general,	a	
cache
(pronounced	&quot;cash&quot;)	is	a	small,	fast	storage	device
that	acts	as	a	staging	area	for	the	data	objects	stored	in	a	larger,	slower
device.	The	process	of	using	a	cache	is	known	as	
caching
(pronounced
&quot;cashing&quot;).</p>
<p>The	central	idea	of	a	memory	hierarchy	is	that	for	each	
k
,	the	faster	and
smaller	storage	device	at	level	
k
serves	as	a	cache	for	the	larger	and
slower	storage	device
Aside	
Other	memory	hierarchies
We	have	shown	you	one	example	of	a	memory	hierarchy,	but
other	combinations	are	possible,	and	indeed	common.	For
example,	many	sites,	including	Google	datacenters,	back	up	local
disks	onto	archival	magnetic	tapes.	At	some	of	these	sites,	human
operators	manually	mount	the	tapes	onto	tape	drives	as	needed.
At	other	sites,	tape	robots	handle	this	task	automatically.	In	either
case,	the	collection	of	tapes	represents	a	level	in	the	memory
hierarchy,	below	the	local	disk	level,	and	the	same	general
principles	apply.	Tapes	are	cheaper	per	byte	than	disks,	which
allows	sites	to	archive	multiple	snapshots	of	their	local	disks.	The
trade-off	is	that	tapes	take	longer	to	access	than	disks.	As	another
example,	solid	state	disks	are	playing	an	increasingly	important
role	in	the	memory	hierarchy,	bridging	the	gulf	between	DRAM
and	rotating	disk.
Figure	
6.22	
The	basic	principle	of	caching	in	a	memory	hierarchy.</p>
<p>at	level	
k
+	1.	In	other	words,	each	level	in	the	hierarchy	caches	data
objects	from	the	next	lower	level.	For	example,	the	local	disk	serves	as	a
cache	for	files	(such	as	Web	pages)	retrieved	from	remote	disks	over	the
network,	the	main	memory	serves	as	a	cache	for	data	on	the	local	disks,
and	so	on,	until	we	get	to	the	smallest	cache	of	all,	the	set	of	CPU
registers.
Figure	
6.22
shows	the	general	concept	of	caching	in	a	memory
hierarchy.	The	storage	at	level	
k
+	1	is	partitioned	into	contiguous	chunks
of	data	objects	called	
blocks.
Each	block	has	a	unique	address	or	name
that	distinguishes	it	from	other	blocks.	Blocks	can	be	either	fixed	size	(the
usual	case)	or	variable	size	(e.g.,	the	remote	HTML	files	stored	on	Web
servers).	For	example,	the	level	
k
+	1	storage	in	
Figure	
6.22
is
partitioned	into	16	fixed-size	blocks,	numbered	0	to	15.
Similarly,	the	storage	at	level	
k
is	partitioned	into	a	smaller	set	of	blocks
that	are	the	same	size	as	the	blocks	at	level	
k
+	1.	At	any	point	in	time,
the	cache	at	level	
k
contains	copies	of	a	subset	of	the	blocks	from	level	
k</p>
<ul>
<li>
<ol>
<li>For	example,	in	
Figure	
6.22
,	the	cache	at	level	
k
has	room	for
four	blocks	and	currently	contains	copies	of	blocks	4,	9,14,	and	3.
Data	are	always	copied	back	and	forth	between	level	
k
and	level	
k</li>
</ol>
<ul>
<li>1	in
block-size	
transfer	units.
It	is	important	to	realize	that	while	the	block	size
is	fixed	between	any	particular	pair	of	adjacent	levels	in	the	hierarchy,
other	pairs	of	levels	can	have	different	block	sizes.	For	example,	in
Figure	
6.21
,	transfers	between	L1	and	L0	typically	use	word-size
blocks.	Transfers	between	L2	and	L1	(and	L3	and	L2,	and	L4	and	L3)
typically	use	blocks	of	tens	of	bytes.	And	transfers	between	L5	and	L4
use	blocks	with	hundreds	or	thousands	of	bytes.	In	general,	devices
lower	in	the	hierarchy	(further	from	the	CPU)	have	longer	access	times,</li>
</ul>
</li>
</ul>
<p>and	thus	tend	to	use	larger	block	sizes	in	order	to	amortize	these	longer
access	times.
Cache	Hits
When	a	program	needs	a	particular	data	object	
d
from	level	
k
+	1,	it	first
looks	for	
d
in	one	of	the	blocks	currently	stored	at	level	
k.
If	
d
happens	to
be	cached	at	level	
k
,	then	we	have	what	is	called	a	
cache	hit.
The
program	reads	
d
directly	from	level	
k
,	which	by	the	nature	of	the	memory
hierarchy	is	faster	than	reading	
d
from	level	
k	+	1.
For	example,	a
program	with	good	temporal	locality	might	read	a	data	object	from	block
14,	resulting	in	a	cache	hit	from	level	
k.
Cache	Misses
If,	on	the	other	hand,	the	data	object	
d
is	not	cached	at	level	
k
,	then	we
have	what	is	called	a	
cache	miss.
When	there	is	a	miss,	the	cache	at
level	
k
fetches	the	block	containing	
d
from	the	cache	at	level	
k
+	1,
possibly	overwriting	an	existing	block	if	the	level	
k
cache	is	already	full.
This	process	of	overwriting	an	existing	block	is	known	as	
replacing
or
evicting
the	block.	The	block	that	is	evicted	is	sometimes	referred	to	as	a
victim	block.
The	decision	about	which	block	to	replace	is	governed	by
the	cache's	
replacement	policy.
For	example,	a	cache	with	a	
random
replacement	policy
would	choose	a	random	victim	block.	A	cache	with	a
least	recently	used	(LRU)
replacement	policy	would	choose	the	block	that
was	last	accessed	the	furthest	in	the	past.</p>
<p>After	the	cache	at	level	
k
has	fetched	the	block	from	level	
k
+	1,	the
program	can	read	
d
from	level	
k
as	before.	For	example,	in	
Figure
6.22
,	reading	a	data	object	from	block	12	in	the	level	
k
cache	would
result	in	a	cache	miss	because	block	12	is	not	currently	stored	in	the
level	
k
cache.	Once	it	has	been	copied	from	level	
k
+	1	to	level	
k
,	block
12	will	remain	there	in	expectation	of	later	accesses.
Kinds	of	Cache	Misses
It	is	sometimes	helpful	to	distinguish	between	different	kinds	of	cache
misses.	If	the	cache	at	level	
k
is	empty,	then	any	access	of	any	data
object	will	miss.	An	empty	cache	is	sometimes	referred	to	as	a	
cold
cache
,	and	misses	of	this	kind	are	called	
compulsory	misses
or	
cold
misses.
Cold	misses	are	important	because	they	are	often	transient
events	that	might	not	occur	in	steady	state,	after	the	cache	has	been
warmed	up
by	repeated	memory	accesses.
Whenever	there	is	a	miss,	the	cache	at	level	
k
must	implement	some
placement	policy
that	determines	where	to	place	the	block	it	has	retrieved
from	level	
k
+	1.	The	most	flexible	placement	policy	is	to	allow	any	block
from	level	
k
+	1	to	be	stored	in	any	block	at	level	
k.
For	caches	high	in
the	memory	hierarchy	(close	to	the	CPU)	that	are	implemented	in
hardware	and	where	speed	is	at	a	premium,	this	policy	is	usually	too
expensive	to	implement	because	randomly	placed	blocks	are	expensive
to	locate.
Thus,	hardware	caches	typically	implement	a	simpler	placement	policy
that	restricts	a	particular	block	at	level	
k
+	1	to	a	small	subset	(sometimes
a	singleton)	of	the	blocks	at	level	
k.
For	example,	in	
Figure	
6.22
,	we</p>
<p>might	decide	that	a	block	
i
at	level	
k
+	1	must	be	placed	in	block	(
i
mod	4)
at	level	
k.
For	example,	blocks	0,	4,	8,	and	12	at	level	
k
+	1	would	map	to
block	0	at	level	
k;
blocks	1,	5,	9,	and	13	would	map	to	block	1;	and	so	on.
Notice	that	our	example	cache	in	
Figure	
6.22
uses	this	policy.
Restrictive	placement	policies	of	this	kind	lead	to	a	type	of	miss	known	as
a	
conflict	miss
,	in	which	the	cache	is	large	enough	to	hold	the	referenced
data	objects,	but	because	they	map	to	the	same	cache	block,	the	cache
keeps	missing.	For	example,	in	
Figure	
6.22
,	if	the	program	requests
block	0,	then	block	8,	then	block	0,	then	block	8,	and	so	on,	each	of	the
references	to	these	two	blocks	would	miss	in	the	cache	at	level	
k
,	even
though	this	cache	can	hold	a	total	of	four	blocks.
Programs	often	run	as	a	sequence	of	phases	(e.g.,	loops)	where	each
phase	accesses	some	reasonably	constant	set	of	cache	blocks.	For
example,	a	nested	loop	might	access	the	elements	of	the	same	array
over	and	over	again.	This	set	of	blocks	is	called	the	
working	set
of	the
phase.	When	the	size	of	the	working	set	exceeds	the	size	of	the	cache,
the	cache	will	experience	what	are	known	as	
capacity	misses.
In	other
words,	the	cache	is	just	too	small	to	handle	this	particular	working	set.
Cache	Management
As	we	have	noted,	the	essence	of	the	memory	hierarchy	is	that	the
storage	device	at	each	level	is	a	cache	for	the	next	lower	level.	At	each
level,	some	form	of	logic	must	
manage
the	cache.	By	this	we	mean	that
something	has	to	partition	the	cache	storage	into	blocks,	transfer	blocks
between	different	levels,	decide	when	there	are	hits	and	misses,	and</p>
<p>then	deal	with	them.	The	logic	that	manages	the	cache	can	be	hardware,
software,	or	a	combination	of	the	two.
For	example,	the	compiler	manages	the	register	file,	the	highest	level	of
the	cache	hierarchy.	It	decides	when	to	issue	loads	when	there	are
misses,	and	determines	which	register	to	store	the	data	in.	The	caches	at
levels	L1,	L2,	and	L3	are	managed	entirely	by	hardware	logic	built	into
the	caches.	In	a	system	with	virtual	memory,	the	DRAM	main	memory
serves	as	a	cache	for	data	blocks	stored	on	disk,	and	is	managed	by	a
combination	of	operating	system	software	and	address	translation
hardware	on	the	CPU.	For	a	machine	with	a	distributed	file	system	such
as	AFS,	the	local	disk	serves	as	a	cache	that	is	managed	by	the	AFS
client	process	running	on	the	local	machine.	In	most	cases,	caches
operate	automatically	and	do	not	require	any	specific	or	explicit	actions
from	the	program.
Type
What	cached
Where	cached
Latency
(cycles)
Managed	by
CPU
registers
4-byte	or	8-byte
words
On-chip	CPU
registers
0
Compiler
TLB
Address
translations
On-chip	TLB
0
Hardware	MMU
L1	cache
64-byte	blocks
On-chip	L1	cache
4
Hardware
L2	cache
64-byte	blocks
On-chip	L2	cache
10
Hardware
L3	cache
64-byte	blocks
On-chip	L3	cache
50
Hardware
Virtual
4-KB	pages
Main	memory
200
Hardware	+	OS</p>
<p>memory
Buffer
cache
Parts	of	files
Main	memory
200
OS
Disk	cache
Disk	sectors
Disk	controller
100,000
Controller
firmware
Network
cache
Parts	of	files
Local	disk
10,000,000
NFS	client
Browser
cache
Web	pages
Local	disk
10,000,000
Web	browser
Web	cache
Web	pages
Remote	server
disks
1,000,000,000
Web	proxy
server
Figure	
6.23	
The	ubiquity	of	caching	in	modern	computer	systems.
Acronyms:	TLB:	translation	lookaside	buffer;	MMU:	memory
management	unit;	OS:	operating	system;	NFS:	network	file	system.
6.3.2	
Summary	of	Memory
Hierarchy	Concepts
To	summarize,	memory	hierarchies	based	on	caching	work	because
slower	storage	is	cheaper	than	faster	storage	and	because	programs
tend	to	exhibit	locality:
Exploiting	temporal	locality.	
Because	of	temporal	locality,	the	same
data	objects	are	likely	to	be	reused	multiple	times.	Once	a	data	object</p>
<p>has	been	copied	into	the	cache	on	the	first	miss,	we	can	expect	a
number	of	subsequent	hits	on	that	object.	Since	the	cache	is	faster
than	the	storage	at	the	next	lower	level,	these	subsequent	hits	can	be
served	much	faster	than	the	original	miss.
Exploiting	spatial	locality.	
Blocks	usually	contain	multiple	data
objects.	Because	of	spatial	locality,	we	can	expect	that	the	cost	of
copying	a	block	after	a	miss	will	be	amortized	by	subsequent
references	to	other	objects	within	that	block.
Caches	are	used	everywhere	in	modern	systems.	As	you	can	see	from
Figure	
6.23
,	caches	are	used	in	CPU	chips,	operating	systems,
distributed	file	systems,	and	on	the	World	Wide	Web.	They	are	built	from
and	managed	by	various	combinations	of	hardware	and	software.	Note
that	there	are	a	number	of	terms	and	acronyms	in	
Figure	
6.23
that	we
haven't	covered	yet.	We	include	them	here	to	demonstrate	how	common
caches	are.</p>
<p>6.4	
Cache	Memories
The	memory	hierarchies	of	early	computer	systems	consisted	of	only
three	levels:	CPU	registers,	main	memory,	and	disk	storage.	However,
because	of	the	increasing	gap	between	CPU	and	main	memory,	system
designers	were	compelled	to	insert
Figure	
6.24	
Typical	bus	structure	for	cache	memories.
a	small	SRAM	
cache	memory
,	called	an	
L1	cache
(level	1	cache)
between	the	CPU	register	file	and	main	memory,	as	shown	in	
Figure
6.24
.	The	L1	cache	can	be	accessed	nearly	as	fast	as	the	registers,
typically	in	about	4	clock	cycles.
As	the	performance	gap	between	the	CPU	and	main	memory	continued
to	increase,	system	designers	responded	by	inserting	an	additional	larger
cache,	called	an	
L2	cache
,	between	the	L1	cache	and	main	memory,	that
can	be	accessed	in	about	10	clock	cycles.	Many	modern	systems	include
an	even	larger	cache,	called	an	
L3	cache
,	which	sits	between	the	L2
cache	and	main	memory	in	the	memory	hierarchy	and	can	be	accessed
in	about	50	cycles.	While	there	is	considerable	variety	in	the</p>
<p>arrangements,	the	general	principles	are	the	same.	For	our	discussion	in
the	next	section,	we	will	assume	a	simple	memory	hierarchy	with	a	single
L1	cache	between	the	CPU	and	main	memory.
6.4.1	
Generic	Cache	Memory
Organization
Consider	a	computer	system	where	each	memory	address	has	
m
bits
that	form	
M
=	2
unique	addresses.	As	illustrated	in	
Figure	
6.25(a)
,	a
cache	for	such	a	machine	is	organized	as	an	array	of	
S
=	2</p>
<p>cache	sets
.
Each	set	consists	of	
E	cache	lines.
Each	line	consists	of	a	data	
block
of	
B
=	2
bytes,	a	
valid	bit
that	indicates	whether	or	not	the	line	contains
meaningful	information,	and	
t
=	
m
−	(
b
+	
s
)	
tag	bits
(a	subset	of	the	bits
from	the	current	block's	memory	address)	that	uniquely	identify	the	block
stored	in	the	cache	line.
In	general,	a	cache's	organization	can	be	characterized	by	the	tuple	(
S,
E,	B,	m
).	The	size	(or	capacity)	of	a	cache,	
C
,	is	stated	in	terms	of	the
aggregate	size	of	all	the	blocks.	The	tag	bits	and	valid	bit	are	not
included.	Thus,	
C	=	S
×	
E
×	
B.
When	the	CPU	is	instructed	by	a	load	instruction	to	read	a	word	from
address	
A
of	main	memory,	it	sends	address	
A
to	the	cache.	If	the	cache
is	holding	a	copy	of	the	word	at	address	
A
,	it	sends	the	word	immediately
back	to	the	CPU.	So	how	does	the	cache	know	whether	it	contains	a
copy	of	the	word	at	address	
A
?	The	cache	is	organized	so	that	it	can	find
the	requested	word	by	simply	inspecting	the	bits	of	the	address,	similar	to
m
s
b</p>
<p>a	hash	table	with	an	extremely	simple	hash	function.	Here	is	how	it
works:
The	parameters	
S
and	
B
induce	a	partitioning	of	the	
m
address	bits	into
the	three	fields	shown	in	
Figure	
6.25(b)
.	The	
s	set	index	bits
in	A	form
an	index	into
Figure	
6.25	
General	organization	of	cache	
(S,	E,	B,	m).
(a)	A	cache	is	an	array	of	sets.	Each	set	contains	one	or	more	lines.	Each
line	contains	a	valid	bit,	some	tag	bits,	and	a	block	of	data,	(b)	The	cache
organization	induces	a	partition	of	the	
m
address	bits	into	
t
tag	bits,	
s
set
index	bits,	and	
b
block	offset	bits.
the	array	of	
S
sets.	The	first	set	is	set	0,	the	second	set	is	set	1,	and	so
on.	When	interpreted	as	an	unsigned	integer,	the	set	index	bits	tell	us
which	set	the	word	must	be	stored	in.	Once	we	know	which	set	the	word</p>
<p>must	be	contained	in,	the	
t
tag	bits	in	
A
tell	us	which	line	(if	any)	in	the	set
contains	the	word.	A	line	in	the	set	contains	the	word	if	and	only	if	the
valid	bit	is	set	and	the	tag	bits	in	the	line	match	the	tag	bits	in	the	address
A.
Once	we	have	located	the	line	identified	by	the	tag	in	the	set	identified
by	the	set	index,	then	the	
b	block	offset	bits
give	us	the	offset	of	the	word
in	the	B-byte	data	block.
As	you	may	have	noticed,	descriptions	of	caches	use	a	lot	of	symbols.
Figure	
6.26
summarizes	these	symbols	for	your	reference.
Practice	Problem	
6.9	
(solution	page	
663
)
The	following	table	gives	the	parameters	for	a	number	of	different
caches.	For	each	cache,	determine	the	number	of	cache	sets	
(S)
,
tag	bits	
(t)
,	set	index	bits	
(s)
,	and	block	offset	bits	
(b).
Cache
m
C
B
E
S
t
s
b</p>
<ol>
<li></li>
</ol>
<p>32
1,024
4
1</p>
<hr />
<hr />
<hr />
<hr />
<ol start="2">
<li></li>
</ol>
<p>32
1,024
8
4</p>
<hr />
<hr />
<hr />
<hr />
<ol start="3">
<li></li>
</ol>
<p>32
1,024
32
32</p>
<hr />
<hr />
<hr />
<hr />
<p>Parameter
Description
Fundamental	parameters
S
=	2
Number	of	sets
E
Number	of	lines	per	set
s
b</p>
<p>B
=	2
Block	size	(bytes)
m
=	log
(
M
)
Number	of	physical	(main	memory)	address	bits
Derived	quantities
M
=	2
Maximum	number	of	unique	memory	addresses
s
=	log
(S)
Number	of	
set	index	bits
b
=	log
(B)
Number	of	
block	offset	bits
t
=	
m
—	(
s
+	
b
)
Number	of	
tag	bits
C
=	
B
×	
E
×	
S
Cache	size	(bytes),	not	including	overhead	such	as	the	valid	and	tag	bits
Figure	
6.26	
Summary	of	cache	parameters.
Figure	
6.27	
Direct-mapped	cache	(
E
=	1).
There	is	exactly	one	line	per	set.
6.4.2	
Direct-Mapped	Caches
Caches	are	grouped	into	different	classes	based	on	
E
,	the	number	of
cache	lines	per	set.	A	cache	with	exactly	one	line	per	set	(
E
=	1)	is	known
as	a	
direct-mapped
cache	(see	
Figure	
6.27
).	Direct-mapped	caches
b
2
m
2
2</p>
<p>are	the	simplest	both	to	implement	and	to	understand,	so	we	will	use
them	to	illustrate	some	general	concepts	about	how	caches	work.
Suppose	we	have	a	system	with	a	CPU,	a	register	file,	an	L1	cache,	and
a	main	memory.	When	the	CPU	executes	an	instruction	that	reads	a
memory	word	
w
,	it	requests	the	word	from	the	L1	cache.	If	the	L1	cache
has	a	cached	copy	of	
w
,	then	we	have	an	L1	cache	hit,	and	the	cache
quickly	extracts	
w
and	returns	it	to	the	CPU.	Otherwise,	we	have	a	cache
miss,	and	the	CPU	must	wait	while	the	L1	cache	requests	a	copy	of	the
block	containing	
w
from	the	main	memory.	When	the	requested	block
finally	arrives	from	memory,	the	L1	cache	stores	the	block	in	one	of	its
cache	lines,	extracts	word	
w
from	the	stored	block,	and	returns	it	to	the
CPU.	The	process	that	a	cache	goes	through	of	determining	whether	a
request	is	a	hit	or	a	miss	and	then	extracting	the	requested	word	consists
of	three	steps:	(1)	
set	selection
,	(2)	
line	matching
,	and	(3)	
word
extraction.
Figure	
6.28	
Set	selection	in	a	direct-mapped	cache.</p>
<p>Figure	
6.29	
Line	matching	and	word	selection	in	a	direct-mapped
cache.
Within	the	cache	block,	
w
denotes	the	low-order	byte	of	the	word	
w,	w
the	next	byte,	and	so	on.
Set	Selection	in	Direct-Mapped	Caches
In	this	step,	the	cache	extracts	the	
s
set	index	bits	from	the	middle	of	the
address	for	
w.
These	bits	are	interpreted	as	an	unsigned	integer	that
corresponds	to	a	set	number.	In	other	words,	if	we	think	of	the	cache	as	a
one-dimensional	array	of	sets,	then	the	set	index	bits	form	an	index	into
this	array.	
Figure	
6.28
shows	how	set	selection	works	for	a	direct-
mapped	cache.	In	this	example,	the	set	index	bits	00001
are	interpreted
as	an	integer	index	that	selects	set	1.
Line	Matching	in	Direct-Mapped	Caches
Now	that	we	have	selected	some	set	
i
in	the	previous	step,	the	next	step
is	to	determine	if	a	copy	of	the	word	
w
is	stored	in	one	of	the	cache	lines
contained	in	set	
i.
In	a	direct-mapped	cache,	this	is	easy	and	fast
because	there	is	exactly	one	line	per	set.	A	copy	of	
w
is	contained	in	the
line	if	and	only	if	the	valid	bit	is	set	and	the	tag	in	the	cache	line	matches
the	tag	in	the	address	of	
w.
Figure	
6.29
shows	how	line	matching	works	in	a	direct-mapped	cache.
In	this	example,	there	is	exactly	one	cache	line	in	the	selected	set.	The
valid	bit	for	this	line	is	set,	so	we	know	that	the	bits	in	the	tag	and	block
are	meaningful.	Since	the	tag	bits	in	the	cache	line	match	the	tag	bits	in
the	address,	we	know	that	a	copy	of	the	word	we	want	is	indeed	stored	in
0
1
2</p>
<p>the	line.	In	other	words,	we	have	a	cache	hit.	On	the	other	hand,	if	either
the	valid	bit	were	not	set	or	the	tags	did	not	match,	then	we	would	have
had	a	cache	miss.
Word	Selection	in	Direct-Mapped	Caches
Once	we	have	a	hit,	we	know	that	
w
is	somewhere	in	the	block.	This	last
step	determines	where	the	desired	word	starts	in	the	block.	As	shown	in
Figure	
6.29
,	the	block	offset	bits	provide	us	with	the	offset	of	the	first
byte	in	the	desired	word.	Similar	to	our	view	of	a	cache	as	an	array	of
lines,	we	can	think	of	a	block	as	an	array	of	bytes,	and	the	byte	offset	as
an	index	into	that	array.	In	the	example,	the	block	offset	bits	of	100
indicate	that	the	copy	of	
w
starts	at	byte	4	in	the	block.	(We	are	assuming
that	words	are	4	bytes	long.)
Line	Replacement	on	Misses	in	Direct-
Mapped	Caches
If	the	cache	misses,	then	it	needs	to	retrieve	the	requested	block	from	the
next	level	in	the	memory	hierarchy	and	store	the	new	block	in	one	of	the
cache	lines	of	the	set	indicated	by	the	set	index	bits.	In	general,	if	the	set
is	full	of	valid	cache	lines,	then	one	of	the	existing	lines	must	be	evicted.
For	a	direct-mapped	cache,	where	each	set	contains	exactly	one	line,	the
replacement	policy	is	trivial:	the	current	line	is	replaced	by	the	newly
fetched	line.
Putting	It	Together:	A	Direct-Mapped	Cache
2</p>
<h1>in	Action
The	mechanisms	that	a	cache	uses	to	select	sets	and	identify	lines	are
extremely	simple.	They	have	to	be,	because	the	hardware	must	perform
them	in	a	few	nanoseconds.	However,	manipulating	bits	in	this	way	can
be	confusing	to	us	humans.	A	concrete	example	will	help	clarify	the
process.	Suppose	we	have	a	direct-mapped	cache	described	by
In	other	words,	the	cache	has	four	sets,	one	line	per	set,	2	bytes	per
block,	and	4-bit	addresses.	We	will	also	assume	that	each	word	is	a
single	byte.	Of	course,	these	assumptions	are	totally	unrealistic,	but	they
will	help	us	keep	the	example	simple.
When	you	are	first	learning	about	caches,	it	can	be	very	instructive	to
enumerate	the	entire	address	space	and	partition	the	bits,	as	we've	done
in	
Figure	
6.30
for	our	4-bit	example.	There	are	some	interesting	things
to	notice	about	this	enumerated	space:
The	concatenation	of	the	tag	and	index	bits	uniquely	identifies	each
block	in	memory.	For	example,	block	0	consists	of	addresses	0	and	1,
block	1	consists	of	addresses	2	and	3,	block	2	consists	of	addresses
4	and	5,	and	so	on.
Since	there	are	eight	memory	blocks	but	only	four	cache	sets,
multiple	blocks	map	to	the	same	cache	set	(i.e.,	they	have	the	same
set	index).	For	example,	blocks	0	and	4	both	map	to	set	0,	blocks	1
and	5	both	map	to	set	1,	and	so	on.
(
S
,
 
E
,
 
B
,
 
m
)</h1>
<p>(
4
,
 
1
,
 
2
,
 
4
)</p>
<p>Blocks	that	map	to	the	same	cache	set	are	uniquely	identified	by	the
tag.	For	example,	block	0	has	a	tag	bit	of	0	while	block	4	has	a	tag	bit
of	1,	block	1	has	a	tag	bit	of	0	while	block	5	has	a	tag	bit	of	1,	and	so
on.
Address	bits
Address
(decimal)
Tag	bits	(
t
=	1)
Index	bits	(
s
=	2)
Offset	bits	(
b
=	1)
Block	number
(decimal)
0
0
00
0
0
1
0
00
1
0
2
0
01
0
1
3
0
01
1
1
4
0
10
0
2
5
0
10
1
2
6
0
11
0
3
7
0
11
1
3
8
1
00
0
4
9
1
00
1
4
10
1
01
0
5
11
1
01
1
5
12
1
10
0
6</p>
<p>13
1
10
1
6
14
1
11
0
7
15
1
11
1
7
Figure	
6.30	
4-bit	address	space	for	example	direct-mapped	cache.
Let	us	simulate	the	cache	in	action	as	the	CPU	performs	a	sequence	of
reads.	Remember	that	for	this	example	we	are	assuming	that	the	CPU
reads	1-byte	words.	While	this	kind	of	manual	simulation	is	tedious	and
you	may	be	tempted	to	skip	it,	in	our	experience	students	do	not	really
understand	how	caches	work	until	they	work	their	way	through	a	few	of
them.
Initially,	the	cache	is	empty	(i.e.,	each	valid	bit	is	0):
Set
Valid
Tag
block[0]
block[1]
0
0
1
0
2
0
3
0
Each	row	in	the	table	represents	a	cache	line.	The	first	column	indicates
the	set	that	the	line	belongs	to,	but	keep	in	mind	that	this	is	provided	for
convenience	and	is	not	really	part	of	the	cache.	The	next	four	columns
represent	the	actual	bits	in	each	cache	line.	Now,	let's	see	what	happens
when	the	CPU	performs	a	sequence	of	reads:</p>
<p>1
.	
Read	word	at	address	0.	
Since	the	valid	bit	for	set	0	is	0,	this	is	a
cache	miss.	The	cache	fetches	block	0	from	memory	(or	a	lower-
level	cache)	and	stores	the	
block	in	set	0.	Then	the	cache	returns
m[0]	(the	contents	of	memory	location	0)	from	block[0]	of	the	newly
fetched	cache	line.
Set
Valid
Tag
block[0]
block[1]
0
1
0
m[0]
m[1]
1
0
2
0
3
0
2
.	
Read	word	at	address	1.	
This	is	a	cache	hit.	The	cache
immediately	returns	m[1]	from	block[1]	of	the	cache	line.	The	state
of	the	cache	does	not	change.
3
.	
Read	word	at	address	13.	
Since	the	cache	line	in	set	2	is	not
valid,	this	is	a	cache	miss.	The	cache	loads	block	6	into	set	2	and
returns	m[13]	from	block[1]	of	the	new	cache	line.
Set
Valid
Tag
block[0]
block[1]
0
1
0
m[0]
m[1]
1
0
2
1
1
m[12]
m[13]
3
0
4
.	
Read	word	at	address	8.	
This	is	a	miss.	The	cache	line	in	set	0	is
indeed	valid,	but	the	tags	do	not	match.	The	cache	loads	block	4</p>
<p>into	set	0	(replacing	the	line	that	was	there	from	the	read	of
address	0)	and	returns	m[8]	from	block[0]	of	the	new	cache	line.
Set
Valid
Tag
block[0]
block[1]
0
1
1
m[8]
m[9]
1
0
2
1
1
m[12]
m[13]
3
0
5
.	
Read	word	at	address	0.	
This	is	another	miss,	due	to	the
unfortunate	fact	that	we	just	replaced	block	0	during	the	previous
reference	to	address	8.	This	kind	of	miss,	where	we	have	plenty	of
room	in	the	cache	but	keep	alternating	references	to	blocks	that
map	to	the	same	set,	is	an	example	of	a	conflict	miss.
Set
Valid
Tag
block[0]
block[1]
0
1
0
m[0]
m[1]
1
0
2
1
1
m[12]
m[13]
3
0
Conflict	Misses	in	Direct-Mapped	Caches
Conflict	misses	are	common	in	real	programs	and	can	cause	baffling
performance	problems.	Conflict	misses	in	direct-mapped	caches	typically
occur	when	programs	access	arrays	whose	sizes	are	a	power	of	2.	For</p>
<p>example,	consider	a	function	that	computes	the	dot	product	of	two
vectors:
This	function	has	good	spatial	locality	with	respect	to	
and	
,	and	so	we
might	expect	it	to	enjoy	a	good	number	of	cache	hits.	Unfortunately,	this
is	not	always	true.
Suppose	that	floats	are	4	bytes,	that	
is	loaded	into	the	32	bytes	of
contiguous	memory	starting	at	address	0,	and	that	
starts	immediately
after	
at	address	32.	For	simplicity,	suppose	that	a	block	is	16	bytes	(big
enough	to	hold	four	floats)	and	that	the	cache	consists	of	two	sets,	for	a
total	cache	size	of	32	bytes.	We	will	assume	that	the	variable	
is
actually	stored	in	a	CPU	register	and	thus	does	not	require	a	memory
reference.	Given	these	assumptions,	each	
and	
will	map	to	the
identical	cache	set:</p>
<p>Element
Address
Set	index
0
0
4
0
8
0
12
0
16
1
20
1
24
1
28
1
32
0
36
0
40
0
44
0
48
1
52
1
56
1
60
1
At	run	time,	the	first	iteration	of	the	loop	references	
,	a	miss	that
causes	the	block	containing	
to	be	loaded	into	set	0.	The	next</p>
<h2>reference	is	to	
,	another	miss	that	causes	the	block	containing	
to	be	copied	into	set	0,	overwriting	the	values	of	
that	were
copied	in	by	the	previous	reference.	During	the	next	iteration,	the
reference	to	
misses,	which	causes	the	
block	to	be
loaded	back	into	set	0,	overwriting	the	
block.	So	now	we	have
a	conflict	miss,	and	in	fact	each	subsequent	reference	to	
and	
will
result	in	a	conflict	miss	as	we	thrash	back	and	forth	between	blocks	of	
and	
.	The	term	
thrashing
describes	any	situation	where	a	cache	is
repeatedly	loading	and	evicting	the	same	sets	of	cache	blocks.
Aside	
Why	index	with	the	middle	bits?
You	may	be	wondering	why	caches	use	the	middle	bits	for	the	set
index	instead	of	the	high-order	bits.	There	is	a	good	reason	why
the	middle	bits	are	better.	
Figure	
6.31
shows	why.	If	the	high-
order	bits	are	used	as	an	index,	then	some	contiguous	memory
blocks	will	map	to	the	same	cache	set.	For	example,	in	the	figure,
the	first	four	blocks	map	to	the	first	cache	set,	the	second	four
blocks	map	to	the	second	set,	and	so	on.	If	a	program	has	good
spatial	locality	and	scans	the	elements	of	an	array	sequentially,
then	the	cache	can	only	hold	a	block-size	chunk	of	the	array	at
any	point	in	time.	This	is	an	inefficient	use	of	the	cache.	Contrast
this	with	middle-bit	indexing,	where	adjacent	blocks	always	map	to
different	cache	sets.	In	this	case,	the	cache	can	hold	an	entire	
C</h2>
<p>size	chunk	of	the	array,	where	
C
is	the	cache	size.</p>
<p>Figure	
6.31	
Why	caches	index	with	the	middle	bits.
The	bottom	line	is	that	even	though	the	program	has	good	spatial	locality
and	we	have	room	in	the	cache	to	hold	the	blocks	for	both	
and
,	each	reference	results	in	a	conflict	miss	because	the	blocks	map	to
the	same	cache	set.	It	is	not	unusual	for	this	kind	of	thrashing	to	result	in
a	slowdown	by	a	factor	of	2	or	3.	Also,	be	aware	that	even	though	our
example	is	extremely	simple,	the	problem	is	real	for	larger	and	more
realistic	direct-mapped	caches.
Luckily,	thrashing	is	easy	for	programmers	to	fix	once	they	recognize
what	is	going	on.	One	easy	solution	is	to	put	
B
bytes	of	padding	at	the
end	of	each	array.	
For	example,	instead	of	defining	
to	be	float	
,	we
define	it	to	be	float	
.	Assuming	
starts	immediately	after	
in
memory,	we	have	the	following	mapping	of	array	elements	to	sets:</p>
<p>Element
Address
Set	index
0
0
4
0
8
0
12
0
16
1
20
1
24
1
28
1
48
1
52
1
56
1
60
1
64
0
68
0
72
0
76
0
With	the	padding	at	the	end	of	
and	
now	map	to	different
sets,	which	eliminates	the	thrashing	conflict	misses.</p>
<p>Practice	Problem	
6.10	
(solution	page	
663
)
In	the	previous	
example,	what	fraction	of	the	total
references	to	
and	
will	be	hits	once	we	have	padded	array	
?
Practice	Problem	
6.11	
(solution	page	
663
)
Imagine	a	hypothetical	cache	that	uses	the	high-order	
s
bits	of	an
address	as	the	set	index.	For	such	a	cache,	contiguous	chunks	of
memory	blocks	are	mapped	to	the	same	cache	set.
A
.	
How	many	blocks	are	in	each	of	these	contiguous	array
chunks?
B
.	
Consider	the	following	code	that	runs	on	a	system	with	a
cache	of	the	form	
(S,	E,	B,	m)
=	(512,	1,	32,	32):
What	is	the	maximum	number	of	array	blocks	that	are
stored	in	the	cache	at	any	point	in	time?
6.4.3	
Set	Associative	Caches
The	problem	with	conflict	misses	in	direct-mapped	caches	stems	from	the
constraint	that	each	set	has	exactly	one	line	(or	in	our	terminology,	
E
=</p>
<h2>1).	A	
set	associative	cache
relaxes	this	constraint	so	that	each	set	holds
more	than	one	cache	line.	A	cache	with	1	&lt;	
E
&lt;	
C/B
is	often	called	an	
E</h2>
<p>way	set	associative	cache.	We
Figure	
6.32	
Set	associative	cache	(1	&lt;	
E
&lt;	
C/B
).
In	a	set	associative	cache,	each	set	contains	more	than	one	line.	This
particular	example	shows	a	two-way	set	associative	cache.
Figure	
6.33	
Set	selection	in	a	set	associative	cache.
will	discuss	the	special	case,	where	
E
=	
C/B
,	in	the	next	section.	
Figure
6.32
shows	the	organization	of	a	two-way	set	associative	cache.
Set	Selection	in	Set	Associative	Caches</p>
<p>Set	selection	is	identical	to	a	direct-mapped	cache,	with	the	set	index	bits
identifying	the	set.	
Figure	
6.33
summarizes	this	principle.
Line	Matching	and	Word	Selection	in	Set
Associative	Caches
Line	matching	is	more	involved	in	a	set	associative	cache	than	in	a
direct-mapped	cache	because	it	must	check	the	tags	and	valid	bits	of
multiple	lines	in	order	to	determine	if	the	requested	word	is	in	the	set.	A
conventional	memory	is	an	array	of	values	that	takes	an	address	as	input
and	returns	the	value	stored	at	that	address.	An	
associative	memory
,	on
the	other	hand,	is	an	array	of	(key,	value)	pairs	that	takes	as	input	the	key
and	returns	a	value	from	one	of	the	(key,	value)	pairs	that	matches	the
input	key.	Thus,	we	can	think	of	each	set	in	a	set	associative	cache	as	a
small	associative	memory	where	the	keys	are	the	concatenation	of	the
tag	and	valid	bits,	and	the	values	are	the	contents	of	a	block.
Figure	
6.34	
Line	matching	and	word	selection	in	a	set	associative
cache.
Figure	
6.34
shows	the	basic	idea	of	line	matching	in	an	associative
cache.	An	important	idea	here	is	that	any	line	in	the	set	can	contain	any</p>
<p>of	the	memory	blocks	that	map	to	that	set.	So	the	cache	must	search
each	line	in	the	set	for	a	valid	line	whose	tag	matches	the	tag	in	the
address.	If	the	cache	finds	such	a	line,	then	we	have	a	hit	and	the	block
offset	selects	a	word	from	the	block,	as	before.
Line	Replacement	on	Misses	in	Set
Associative	Caches
If	the	word	requested	by	the	CPU	is	not	stored	in	any	of	the	lines	in	the
set,	then	we	have	a	cache	miss,	and	the	cache	must	fetch	the	block	that
contains	the	word	from	memory.	However,	once	the	cache	has	retrieved
the	block,	which	line	should	it	replace?	Of	course,	if	there	is	an	empty
line,	then	it	would	be	a	good	candidate.	But	if	there	are	no	empty	lines	in
the	set,	then	we	must	choose	one	of	the	nonempty	lines	and	hope	that
the	CPU	does	not	reference	the	replaced	line	anytime	soon.
It	is	very	difficult	for	programmers	to	exploit	knowledge	of	the	cache
replacement	policy	in	their	codes,	so	we	will	not	go	into	much	detail	about
it	here.	The	simplest	replacement	policy	is	to	choose	the	line	to	replace	at
random.	Other	more	sophisticated	policies	draw	on	the	principle	of
locality	to	try	to	minimize	the	probability	that	the	replaced	line	will	be
referenced	in	the	near	future.	For	example,	a	
least	frequently	used	(LFU)
policy	will	replace	the	line	that	has	been	referenced	the	fewest	times	over
some	past	time	window.	A	
least	recently	used	(LRU)
policy	will	replace
the	line	that	was	last	accessed	the	furthest	in	the	past.	All	of	these
policies	require	additional	time	and	hardware.	But	as	we	move	further
down	the	memory	hierarchy,	away	from	the	CPU,	the	cost	of	a	miss
becomes	more	expensive	and	it	becomes	more	worthwhile	to	minimize
misses	with	good	replacement	policies.</p>
<p>6.4.4	
Fully	Associative	Caches
A	fully	associative	cache
consists	of	a	single	set	(i.e.,	
E
=	
C/B
)	that
contains	all	of	the	cache	lines.	
Figure	
6.35
shows	the	basic
organization.
Figure	
6.35	
Fully	associative	cache	(
E
=	
C/B
).
In	a	fully	associative	cache,	a	single	set	contains	all	of	the	lines.
Figure	
6.36	
Set	selection	in	a	fully	associative	cache.
Notice	that	there	are	no	set	index	bits.</p>
<p>Figure	
6.37	
Line	matching	and	word	selection	in	a	fully	associative
cache.
Set	Selection	in	Fully	Associative	Caches
Set	selection	in	a	fully	associative	cache	is	trivial	because	there	is	only
one	set,	summarized	in	
Figure	
6.36
.	Notice	that	there	are	no	set	index
bits	in	the	address,	which	is	partitioned	into	only	a	tag	and	a	block	offset.
Line	Matching	and	Word	Selection	in	Fully
Associative	Caches
Line	matching	and	word	selection	in	a	fully	associative	cache	work	the
same	as	with	a	set	associative	cache,	as	we	show	in	
Figure	
6.37
.	The
difference	is	mainly	a	question	of	scale.
Because	the	cache	circuitry	must	search	for	many	matching	tags	in
parallel,	it	is	difficult	and	expensive	to	build	an	associative	cache	that	is
both	large	and	fast.	As	a	result,	fully	associative	caches	are	only
appropriate	for	small	caches,	such	
as	the	translation	lookaside	buffers
(TLBs)	in	virtual	memory	systems	that	cache	page	table	entries	(
Section
9.6.2
).
Practice	Problem	
6.12	
(solution	page	
663
)
The	problems	that	follow	will	help	reinforce	your	understanding	of
how	caches	work.	Assume	the	following:
The	memory	is	byte	addressable.</p>
<p>Memory	accesses	are	to	1-byte	words	(not	to	4-byte	words).
Addresses	are	13	bits	wide.
The	cache	is	two-way	set	associative	(
E
=	2),	with	a	4-byte
block	size	(
B
=	4)	and	eight	sets	(
S
=	8).
The	contents	of	the	cache	are	as	follows,	with	all	numbers	given	in
hexadecimal	notation.
2-way	set	associative	cache
Set
index
Line	0
Line	1
Tag
Valid
Byte
0
Byte
1
Byte
2
Byte
3
Tag
Valid
Byte
0
Byte
1
0
09
1
86
30
3F
10
00
0
—
—
1
45
1
60
4F
E0
23
38
1
00
BC
2
EB
0
—
—
—
—
0B
0
—
—
3
06
0
—
—
—
—
32
1
12
08
4
C7
1
06
78
07
C5
05
1
40
67
5
71
1
OB
DE
18
4B
6E
0
—
—
6
91
1
A0
B7
26
2D
F0
0
—
—
7
46
0
—
—
—
—
DE
1
12
CO
The	following	figure	shows	the	format	of	an	address	(1	bit	per
box).	Indicate	(by	labeling	the	diagram)	the	fields	that	would	be
used	to	determine	the	following:</p>
<p>CO.	The	cache	block	offset
CI.	The	cache	set	index
CT.	The	cache	tag
Practice	Problem	
6.13	
(solution	page	
664
)
Suppose	a	program	running	on	the	machine	in	
Problem	
6.12
references	the	1-byte	word	at	address	
.	Indicate	the	cache
entry	accessed	and	the	cache	byte	
value	returned	in	hexadecimal
notation.	Indicate	whether	a	cache	miss	occurs.	If	there	is	a	cache
miss,	enter	&quot;—&quot;	for	&quot;Cache	byte	returned.&quot;
A
.	
Address	format	(1	bit	per	box):
B
.	
Memory	reference:
Parameter
Value
Cache	block	offset	(CO)</p>
<hr />
<p>Cache	set	index	(CI)</p>
<hr />
<p>Cache	tag	(CT)</p>
<hr />
<p>Cache	hit?	(Y/N)</p>
<hr />
<p>Cache	byte	returned</p>
<hr />
<p>Practice	Problem	
6.14	
(solution	page	
664
)
Repeat	
Problem	
6.13
for	memory	address	
.
A
.	
Address	format	(1	bit	per	box):
B
.	
Memory	reference:
Parameter
Value
Cache	block	offset	(CO)</p>
<hr />
<p>Cache	set	index	(CI)</p>
<hr />
<p>Cache	tag	(CT)</p>
<hr />
<p>Cache	hit?	(Y/N)</p>
<hr />
<p>Cache	byte	returned</p>
<hr />
<p>Practice	Problem	
6.15	
(solution	page	
664
)
Repeat	
Problem	
6.13
for	memory	address	
.
A
.	
Address	format	(1	bit	per	box):
B
.	
Memory	reference:
Parameter
Value
Cache	block	offset	(CO)</p>
<hr />
<p>Cache	set	index	(CI)</p>
<hr />
<p>Cache	tag	(CT)</p>
<hr />
<p>Cache	hit?	(Y/N)</p>
<hr />
<p>Cache	byte	returned</p>
<hr />
<p>Practice	Problem	
6.16	
(solution	page	
665
)
For	the	cache	in	
Problem	
6.12
,	list	all	of	the	hexadecimal	memory
addresses	that	will	hit	in	set	3.
6.4.5	
Issues	with	Writes
As	we	have	seen,	the	operation	of	a	cache	with	respect	to	reads	is
straightforward.	First,	look	for	a	copy	of	the	desired	word	
w
in	the	cache.
If	there	is	a	hit,	return	
w
immediately.	If	there	is	a	miss,	fetch	the	block
that	contains	
w
from	the	next	lower	level	of	the	memory	hierarchy,	store
the	block	in	some	cache	line	(possibly	evicting	a	valid	line),	and	then
return	
w.
The	situation	for	writes	is	a	little	more	complicated.	Suppose	we	write	a
word	
w
that	is	already	cached	(a	
write	hit).
After	the	cache	updates	its
copy	of	
w
,	what	does	it	do	about	updating	the	copy	of	
w
in	the	next	lower
level	of	the	hierarchy?	The	simplest	approach,	known	as	
write-through
,	is
to	immediately	write	w's	cache	block	to	the	next	lower	level.	While
simple,	write-through	has	the	disadvantage	of	causing	bus	traffic	with
every	write.	Another	approach,	known	as	
write-back
,	defers	the	update
as	long	as	possible	by	writing	the	updated	block	to	the	next	lower	level</p>
<p>only	when	it	is	evicted	from	the	cache	by	the	replacement	algorithm.
Because	of	locality,	write-back	can	significantly	reduce	the	amount	of	bus
traffic,	but	it	has	the	disadvantage	of	additional	complexity.	The	cache
must	maintain	an	additional	
dirty	bit
for	each	cache	line	that	indicates
whether	or	not	the	cache	block	has	been	modified.
Another	issue	is	how	to	deal	with	write	misses.	One	approach,	known	as
write-allocate
,	loads	the	corresponding	block	from	the	next	lower	level
into	the	cache	and	then	updates	the	cache	block.	Write-allocate	tries	to
exploit	spatial	locality	of	writes,	but	it	has	the	disadvantage	that	every
miss	results	in	a	block	transfer	from	the	next	lower	level	to	the	cache.
The	alternative,	known	as	
no-write-allocate
,	bypasses	the	cache	and
writes	the	word	directly	to	the	next	lower	level.	Write-through	caches	are
typically	no-write-allocate.	Write-back	caches	are	typically	write-allocate.
Optimizing	caches	for	writes	is	a	subtle	and	difficult	issue,	and	we	are
only	scratching	the	surface	here.	The	details	vary	from	system	to	system
and	are	often	proprietary	and	poorly	documented.	To	the	programmer
trying	to	write	reasonably	
cache-friendly	programs,	we	suggest	adopting
a	mental	model	that	assumes	write-back,	write-allocate	caches.	There
are	several	reasons	for	this	suggestion:	As	a	rule,	caches	at	lower	levels
of	the	memory	hierarchy	are	more	likely	to	use	write-back	instead	of
write-through	because	of	the	larger	transfer	times.	For	example,	virtual
memory	systems	(which	use	main	memory	as	a	cache	for	the	blocks
stored	on	disk)	use	write-back	exclusively.	But	as	logic	densities
increase,	the	increased	complexity	of	write-back	is	becoming	less	of	an
impediment	and	we	are	seeing	write-back	caches	at	all	levels	of	modern
systems.	So	this	assumption	matches	current	trends.	Another	reason	for
assuming	a	write-back,	write-allocate	approach	is	that	it	is	symmetric	to
the	way	reads	are	handled,	in	that	write-back	write-allocate	tries	to	exploit</p>
<p>locality.	Thus,	we	can	develop	our	programs	at	a	high	level	to	exhibit
good	spatial	and	temporal	locality	rather	than	trying	to	optimize	for	a
particular	memory	system.
6.4.6	
Anatomy	of	a	Real	Cache
Hierarchy
So	far,	we	have	assumed	that	caches	hold	only	program	data.	But,	in
fact,	caches	can	hold	instructions	as	well	as	data.	A	cache	that	holds
instructions	only	is	called	an	
i-cache.
A	cache	that	holds	program	data
only	is	called	a	
d-cache.
A	cache	that	holds	both	instructions	and	data	is
known	as	a	
unified	cache.
Modern	processors	include	separate	i-caches
and	d-caches.	There	are	a	number	of	reasons	for	this.	With	two	separate
caches,	the	processor	can	read	an	instruction	word	and	a	data	word	at
the	same	time.	I-caches	are	typically	read-only,	and	thus	simpler.	The	two
caches	are	often	optimized	to	different	access	patterns	and	can	have
different	block	sizes,	associativities,	and	capacities.	Also,	having
separate	caches	ensures	that	data	accesses	do	not	create	conflict
misses	with	instruction	accesses,	and	vice	versa,	at	the	cost	of	a
potential	increase	in	capacity	misses.
Figure	
6.38
shows	the	cache	hierarchy	for	the	Intel	Core	i7	processor.
Each	CPU	chip	has	four	cores.	Each	core	has	its	own	private	L1	i-cache,
L1	d-cache,	and	L2	unified	cache.	All	of	the	cores	share	an	on-chip	L3
unified	cache.	An	interesting	feature	of	this	hierarchy	is	that	all	of	the
SRAM	cache	memories	are	contained	in	the	CPU	chip.</p>
<p>Figure	
6.39
summarizes	the	basic	characteristics	of	the	Core	i7
caches.
6.4.7	
Performance	Impact	of	Cache
Parameters
Cache	performance	is	evaluated	with	a	number	of	metrics:
Miss	rate.	
The	fraction	of	memory	references	during	the	execution	of
a	program,	or	a	part	of	a	program,	that	miss.	It	is	computed	as	#
misses/
#	
references.
Hit	rate.	
The	fraction	of	memory	references	that	hit.	It	is	computed	as
1	−	
miss	rate.
Hit	time.	
The	time	to	deliver	a	word	in	the	cache	to	the	CPU,
including	the	time	for	set	selection,	line	identification,	and	word
selection.	Hit	time	is	on	the	order	of	several	clock	cycles	for	L1
caches.</p>
<p>Figure	
6.38	
Intel	Core	i7	cache	hierarchy.
Cache	type
Access	time
(cycles)
Cache	size
(
C
)
Assoc.
(
E
)
Block	size
(
B
)
Sets
(
S
)
L1	i-cache
4
32	KB
8
64	B
64
L1	d-cache
4
32	KB
8
64	B
64
L2	unified
cache
10
256	KB
8
64	B
512
L3	unified
cache
40−75
8	MB
16
64	B
8,192
Figure	
6.39	
Characteristics	of	the	Intel	Core	i7	cache	hierarchy.
Miss	penalty.	
Any	additional	time	required	because	of	a	miss.	The
penalty	for	Ll	misses	served	from	L2	is	on	the	order	of	10	cycles;	from</p>
<p>L3,50	cycles;	and	from	main	memory,	200	cycles.
Optimizing	the	cost	and	performance	trade-offs	of	cache	memories	is	a
subtle	exercise	that	requires	extensive	simulation	on	realistic	benchmark
codes	and	thus	is	beyond	our	scope.	However,	it	is	possible	to	identify
some	of	the	qualitative	trade-offs.
Impact	of	Cache	Size
On	the	one	hand,	a	larger	cache	will	tend	to	increase	the	hit	rate.	On	the
other	hand,	it	is	always	harder	to	make	large	memories	run	faster.	As	a
result,	larger	caches	tend	to	increase	the	hit	time.	This	explains	why	an
L1	cache	is	smaller	than	an	L2	cache,	and	an	L2	cache	is	smaller	than
an	L3	cache.
Impact	of	Block	Size
Large	blocks	are	a	mixed	blessing.	On	the	one	hand,	larger	blocks	can
help	increase	the	hit	rate	by	exploiting	any	spatial	locality	that	might	exist
in	a	program.	However,	for	a	given	cache	size,	larger	blocks	imply	a
smaller	number	of	cache	lines,	which	can	hurt	the	hit	rate	in	programs
with	more	temporal	locality	than	spatial	locality.	Larger	blocks	also	have	a
negative	impact	on	the	miss	penalty,	since	larger	blocks	cause	larger
transfer	times.	Modern	systems	such	as	the	Core	i7	compromise	with
cache	blocks	that	contain	64	bytes.
Impact	of	Associativity</p>
<p>The	issue	here	is	the	impact	of	the	choice	of	the	parameter	
E
,	the
number	of	cache	lines	per	set.	The	advantage	of	higher	associativity	(i.e.,
larger	values	of	
E
)	is	that	it	decreases	the	vulnerability	of	the	cache	to
thrashing	due	to	conflict	misses.	However,	higher	associativity	comes	at
a	significant	cost.	Higher	associativity	is	expensive	to	implement	and
hard	to	make	fast.	It	requires	more	tag	bits	per	line,	additional	LRU	state
bits	per	line,	and	additional	control	logic.	Higher	associativity	can
increase	hit	time,	because	of	the	increased	complexity,	and	it	can	also
increase	the	miss	penalty	because	of	the	increased	complexity	of
choosing	a	victim	line.
The	choice	of	associativity	ultimately	boils	down	to	a	trade-off	between
the	hit	time	and	the	miss	penalty.	Traditionally,	high-performance	systems
that	pushed	the	clock	rates	would	opt	for	smaller	associativity	for	L1
caches	(where	the	miss	penalty	is	only	a	few	cycles)	and	a	higher	degree
of	associativity	for	the	lower	levels,	where	the	miss	penalty	is	higher.	For
example,	in	Intel	Core	i7	systems,	the	L1	and	L2	caches	are	8-way
associative,	and	the	L3	cache	is	16-way.
Impact	of	Write	Strategy
Write-through	caches	are	simpler	to	implement	and	can	use	a	
write	buffer
that	works	independently	of	the	cache	to	update	memory.	Furthermore,
read	misses	are	less	expensive	because	they	do	not	trigger	a	memory
write.	On	the	other	hand,	write-back	caches	result	in	fewer	transfers,
which	allows	more	bandwidth	to	memory	for	I/O	devices	that	perform
DMA.	Further,	reducing	the	number	of	transfers	becomes	increasingly
important	as	we	move	down	the	hierarchy	and	the	transfer	times</p>
<p>increase.	In	general,	caches	further	down	the	hierarchy	are	more	likely	to
use	write-back	than	write-through.</p>
<p>6.5	
Writing	Cache-Friendly	Code
In	
Section	
6.2
,	we	introduced	the	idea	of	locality	and	talked	in
qualitative	terms	about	what	constitutes	good	locality.	Now	that	we
understand	how	cache	memories	work,	we	can	be	more	precise.
Programs	with	better	locality	will	tend	to	have	lower	miss	rates,	and
programs	with	lower	miss	rates	will	tend	to	run	faster	than	programs	with
higher	miss	rates.	Thus,	good	programmers	should	always	try	to
Aside	
Cache	lines,	sets,	and	blocks:
What's	the	difference?
It	is	easy	to	confuse	the	distinction	between	cache	lines,	sets,	and
blocks.	Let's	review	these	ideas	and	make	sure	they	are	clear:
A	
block
is	a	fixed-size	packet	of	information	that	moves	back
and	forth	between	a	cache	and	main	memory	(or	a	lower-level
cache).
A	
line
is	a	container	in	a	cache	that	stores	a	block,	as	well	as
other	information	such	as	the	valid	bit	and	the	tag	bits.
A	
set
is	a	collection	of	one	or	more	lines.	Sets	in	direct-mapped
caches	consist	of	a	single	line.	Sets	in	set	associative	and	fully
associative	caches	consist	of	multiple	lines.
In	direct-mapped	caches,	sets	and	lines	are	indeed	equivalent.
However,	in	associative	caches,	sets	and	lines	are	very	different</p>
<p>things	and	the	terms	cannot	be	used	interchangeably.
Since	a	line	always	stores	a	single	block,	the	terms	&quot;line&quot;	and
&quot;block&quot;	are	often	used	interchangeably.	For	example,	systems
professionals	usually	refer	to	the	&quot;line	size&quot;	of	a	cache,	when	what
they	really	mean	is	the	block	size.	This	usage	is	very	common	and
shouldn't	cause	any	confusion	as	long	as	you	understand	the
distinction	between	blocks	and	lines.
write	code	that	is	
cache	friendly
,	in	the	sense	that	it	has	good	locality.
Here	is	the	basic	approach	we	use	to	try	to	ensure	that	our	code	is	cache
friendly.
1
.	
Make	the	common	case	go	fast.	
Programs	often	spend	most	of
their	time	in	a	few	core	functions.	These	functions	often	spend
most	of	their	time	in	a	few	loops.	So	focus	on	the	inner	loops	of	the
core	functions	and	ignore	the	rest.
2
.	
Minimize	the	number	of	cache	misses	in	each	inner	loop.	
All
other	things	being	equal,	such	as	the	total	number	of	loads	and
stores,	loops	with	better	miss	rates	will	run	faster.
To	see	how	this	works	in	practice,	consider	the	
function	from
Section	
6.2
:</p>
<p>Is	this	function	cache	friendly?	First,	notice	that	there	is	good	temporal
locality	in	the	loop	body	with	respect	to	the	local	variables	
and	
.	In
fact,	because	these	are	local	variables,	any	reasonable	optimizing
compiler	will	cache	them	in	the	register	file,	the	highest	level	of	the
memory	hierarchy.	Now	consider	the	stride-1	references	to	vector	
.	In
general,	if	a	cache	has	a	block	size	of	
B
bytes,	then	a	
stride-
k
reference
pattern	(where	
k
is	expressed	in	words)	results	in	an	average	of	min	(1,
(
word	size
×	
k
)/
B
)	misses	per	loop	iteration.	This	is	minimized	for	
k
=	1,
so	the	stride-1	references	to	
are	indeed	cache	friendly.	For	example,
suppose	that	
is	block	aligned,	words	are	4	bytes,	cache	blocks	are	4
words,	and	the	cache	is	initially	empty	(a	cold	cache).	Then,	regardless	of
the	cache	organization,	the	references	to	
will	result	in	the	following
pattern	of	hits	and	misses:
i
=	0
i
=	1
i
=	2
i
=	3
i
=	4
i
=	5
i
=	6
i
=	7
Access	order,	[h]it	or	[m]iss
1	
[m]
2	[h]
3	[h]
4	[h]
5	
[m]
6	[h]
7	[h]
8	[h]
In	this	example,	the	reference	to	
misses	and	the	corresponding
block,	which	contains	
,	is	loaded	into	the	cache	from	memory.
Thus,	the	next	three	references	are	all	hits.	The	reference	to	
causes
another	miss	as	a	new	block	is	loaded	into	the	cache,	the	next	three
references	are	hits,	and	so	on.	In	general,	three	out	of	four	references	will
hit,	which	is	the	best	we	can	do	in	this	case	with	a	cold	cache.</p>
<p>To	summarize,	our	simple	
example	illustrates	two	important	points
about	writing	cache-friendly	code:
Repeated	references	to	local	variables	are	good	because	the
compiler	can	cache	them	in	the	register	file	(temporal	locality).
Stride-1	reference	patterns	are	good	because	caches	at	all	levels	of
the	memory	hierarchy	store	data	as	contiguous	blocks	(spatial
locality).
Spatial	locality	is	especially	important	in	programs	that	operate	on
multidimensional	arrays.	For	example,	consider	the	
function
from	
Section	
6.2
,	which	sums	the	elements	of	a	two-dimensional	array
in	row-major	order:
Since	C	stores	arrays	in	row-major	order,	the	inner	loop	of	this	function
has	the	same	desirable	stride-1	access	pattern	as	
.	For	example,
suppose	we	make	the	same	assumptions	about	the	cache	as	for	
.</p>
<p>Then	the	references	to	the	array	a	will	result	in	the	following	pattern	of
hits	and	misses:
j
=	0
j
=	1
j
=	2
j
=	3
j
=	4
j
=	5
j
=	6
j
=	7
i
=	0
1	
[m]
2	[h]
3	[h]
4	[h]
5	
[m]
6	[h]
7	[h]
8	[h]
i
=	1
9	
[m]
10	[h]
11	[h]
12	[h]
13	
[m]
14	[h]
15	[h]
16	[h]
i
=	2
17	
[m]
18	[h]
19	[h]
20	[h]
21	
[m]
22	[h]
23	[h]
24	[h]
i
=	3
25	
[m]
26	[h]
27	[h]
28	[h]
29	
[m]
30	[h]
31	[h]
32	[h]
But	consider	what	happens	if	we	make	the	seemingly	innocuous	change
of	permuting	the	loops:
In	this	case,	we	are	scanning	the	array	column	by	column	instead	of	row
by	row.	If	we	are	lucky	and	the	entire	array	fits	in	the	cache,	then	we	will</p>
<p>enjoy	the	same	miss	rate	of	1/4.	However,	if	the	array	is	larger	than	the
cache	(the	more	likely	case),	then	each	and	every	access	of	
will
miss!
j
=	0
j
=	1
j
=	2
j
=	3
j
=	4
j
=	5
j
=	6
j
=	7
i
=	0
1	
[m]
5	
[m]
9	
[m]
13	
[m]
17	
[m]
21	
[m]
25	
[m]
29	
[m]
i
=	1
2	
[m]
6	
[m]
10	
[m]
14	
[m]
18	
[m]
22	
[m]
26	
[m]
30	
[m]
i
=	2
3	
[m]
7	
[m]
11	
[m]
15	
[m]
19	
[m]
23	
[m]
27	
[m]
31	
[m]
i
=	3
4	
[m]
8	
[m]
12	
[m]
16	
[m]
20	
[m]
24	
[m]
28	
[m]
32	
[m]
Higher	miss	rates	can	have	a	significant	impact	on	running	time.	For
example,	on	our	desktop	machine,	
runs	25	times	faster
than	
for	large	array	sizes.	To	summarize,	programmers
should	be	aware	of	locality	in	their	programs	and	try	to	write	programs
that	exploit	it.
Practice	Problem	
6.17	
(solution
page	
665
)
Transposing	the	rows	and	columns	of	a	matrix	is	an	important
problem	in	signal	processing	and	scientific	computing	applications.
It	is	also	interesting	from	a	locality	point	of	view	because	its
reference	pattern	is	both	row-wise	and	column-wise.	For	example,
consider	the	following	transpose	routine:</p>
<p>Assume	this	code	runs	on	a	machine	with	the	following	properties:
=	4.
The	
array	starts	at	address	0	and	the	
array	starts	at
address	16	(decimal).
There	is	a	single	L1	data	cache	that	is	direct-mapped,	write-
through,	and	write-allocate,	with	a	block	size	of	8	bytes.
The	cache	has	a	total	size	of	16	data	bytes	and	the	cache	is
initially	empty.
Accesses	to	the	
and	
arrays	are	the	only	sources	of
read	and	write	misses,	respectively.
A
.	
For	each	
and	
,	indicate	whether	the	access	to
and	
is	a	hit	(h)	or	a	miss	(m).</p>
<p>For	example,	reading	
is	a	miss	and	writing	
is	also	a	miss.
Col.	0
Col.	1
Col.	0
Col.	1
Row	0
m</p>
<hr />
<p>Row0
m</p>
<hr />
<p>Row	1</p>
<hr />
<hr />
<p>Row	1</p>
<hr />
<hr />
<p>B
.	
Repeat	the	problem	for	a	cache	with	32	data	bytes.
Practice	Problem	
6.18	
(solution
page	
666
)
The	heart	of	the	recent	hit	game	
SimAquarium
is	a	tight	loop	that
calculates	the	average	position	of	256	algae.	You	are	evaluating
its	cache	performance	on	a	machine	with	a	1,024-byte	direct-
mapped	data	cache	with	16-byte	blocks	(
B
=	16).	You	are	given
the	following	definitions:</p>
<p>You	should	also	assume	the	following:
=	4.
begins	at	memory	address	0.
The	cache	is	initially	empty.
The	only	memory	accesses	are	to	the	entries	of	the	array
Variables	
,	and	
are	stored	in
registers.
Determine	the	cache	performance	for	the	following	code:
A
.	
What	is	the	total	number	of	reads?
B
.	
What	is	the	total	number	of	reads	that	miss	in	the	cache?</p>
<p>C
.	
What	is	the	miss	rate?
Practice	Problem	
6.19	
(solution
page	
666
)
Given	the	assumptions	of	
Practice	Problem	
6.18
,	determine
the	cache	performance	of	the	following	code:
A
.	
What	is	the	total	number	of	reads?
B
.	
What	is	the	total	number	of	reads	that	miss	in	the	cache?
C
.	
What	is	the	miss	rate?
D
.	
What	would	the	miss	rate	be	if	the	cache	were	twice	as	big?
Practice	Problem	
6.20	
(solution
page	
666
)</p>
<p>Given	the	assumptions	of	
Practice	Problem	
6.18
,	determine
the	cache	performance	of	the	following	code:
A
.	
What	is	the	total	number	of	reads?
B
.	
What	is	the	total	number	of	reads	that	miss	in	the	cache?
C
.	
What	is	the	miss	rate?
D
.	
What	would	the	miss	rate	be	if	the	cache	were	twice	as	big?</p>
<p>6.6	
Putting	It	Together:	The	Impact
of	Caches	on	Program	Performance
This	section	wraps	up	our	discussion	of	the	memory	hierarchy	by
studying	the	impact	that	caches	have	on	the	performance	of	programs
running	on	real	machines.
6.6.1	
The	Memory	Mountain
The	rate	that	a	program	reads	data	from	the	memory	system	is	called	the
read	throughput
,	or	sometimes	the	
read	bandwidth
.	If	a	program	reads	
n
bytes	over	a	period	of	
s
seconds,	then	the	read	throughput	over	that
period	is	
n/s
,	typically	expressed	in	units	of	megabytes	per	second
(MB/s).
If	we	were	to	write	a	program	that	issued	a	sequence	of	read	requests
from	a	tight	program	loop,	then	the	measured	read	throughput	would	give
us	some	insight	into	the	performance	of	the	memory	system	for	that
particular	sequence	of	reads.	
Figure	
6.40
shows	a	pair	of	functions
that	measure	the	read	throughput	for	a	particular	read	sequence.
The	
function	generates	the	read	sequence	by	scanning	the	first
elements	of	an	array	with	a	stride	of	
.	To	increase	the
available	parallelism	in	the	inner	loop,	it	uses	4	×	4	unrolling	(
Section</p>
<h2>5.9
).	The	
function	is	a	wrapper	that	calls	the	
function	and
returns	the	measured	read	throughput.	The	call	to	the	
function	in
line	37	warms	the	cache.	The	
function	in	line	38	calls	the	
function	with	arguments	
and	estimates	the	running	time	of	the	
function	in	CPU	cycles.	Notice	that	the	
argument	to	the	
function
is	in	units	of	bytes,	while	the	corresponding	
argument	to	the	
function	is	in	units	of	array	elements.	Also,	notice	that	line	39	computes
MB/s	as	10
bytes/s,	as	opposed	to	2
bytes/s.
The	
and	
arguments	to	the	
function	allow	us	to	control
the	degree	of	temporal	and	spatial	locality	in	the	resulting	read	sequence.
Smaller	values	of	
result	in	a	smaller	working	set	size,	and	thus
better	temporal	locality.	Smaller	values	of	
result	in	better	spatial
locality.	If	we	call	the	
function	repeatedly	with	different	values	of	
and	
,	then	we	can	recover	a	fascinating	two-dimensional	function
of	read	throughput	versus	temporal	and	spatial	locality.	This	function	is
called	a	
memory	mountain
[
112
].
Every	computer	has	a	unique	memory	mountain	that	characterizes	the
capabilities	of	its	memory	system.	For	example,	
Figure	
6.41
shows
the	memory	mountain	for	an	Intel	Core	i7	Haswell	system.	In	this
example,	the	
varies	from	16	KB	to	128	MB,	and	the	
varies
from	1	to	12	elements,	where	each	element	is	an	8-byte</h2>
<p>code/mem/mountain/mountain.c
6
20</p>
<hr />
<p>code/mem/mountain/mountain.c
Figure	
6.40	
Functions	that	measure	and	compute	read	throughput.
We	can	generate	a	memory	mountain	for	a	particular	computer	by	calling
the	
function	with	different	values	of	
(which	corresponds	to
temporal	locality)	and	
(which	corresponds	to	spatial	locality).</p>
<p>Figure	
6.41	
A	memory	mountain.
Shows	read	throughput	as	a	function	of	temporal	and	spatial	locality.
The	geography	of	the	Core	i7	mountain	reveals	a	rich	structure.
Perpendicular	to	the	
axis	are	four	
ridges
that	correspond	to	the
regions	of	temporal	locality	where	the	working	set	fits	entirely	in	the	L1
cache,	L2	cache,	L3	cache,	and	main	memory,	respectively.	Notice	that
there	is	more	than	an	order	of	magnitude	difference	between	the	highest
peak	of	the	L1	ridge,	where	the	CPU	reads	at	a	rate	of	over	14	GB/s,	and
the	lowest	point	of	the	main	memory	ridge,	where	the	CPU	reads	at	a
rate	of	900	MB/s.
On	each	of	the	L2,	L3,	and	main	memory	ridges,	there	is	a	slope	of
spatial	locality	that	falls	downhill	as	the	stride	increases	and	spatial
locality	decreases.	Notice	that	even	when	the	working	set	is	too	large	to</p>
<p>fit	in	any	of	the	caches,	the	highest	point	on	the	main	memory	ridge	is	a
factor	of	8	higher	than	its	lowest	point.	So	even	when	a	program	has	poor
temporal	locality,	spatial	locality	can	still	come	to	the	rescue	and	make	a
significant	difference.
There	is	a	particularly	interesting	flat	ridge	line	that	extends	perpendicular
to	the	stride	axis	for	a	stride	of	1,	where	the	read	throughput	is	a
relatively	flat	12	GB/s,	even	though	the	working	set	exceeds	the
capacities	of	L1	and	L2.	This	is	apparently	due	to	a	hardware	
prefetching
mechanism	in	the	Core	i7	memory	system	that	automatically	identifies
sequential	stride-1	reference	patterns	and	attempts	to	fetch	those	blocks
into	the	cache	before	they	are	accessed.	While	the
Figure	
6.42	
Ridges	of	temporal	locality	in	the	memory	mountain.
The	graph	shows	a	slice	through	
Figure	
6.41
with	
=	8.
details	of	the	particular	prefetching	algorithm	are	not	documented,	it	is
clear	from	the	memory	mountain	that	the	algorithm	works	best	for	small</p>
<p>strides—yet	another	reason	to	favor	sequential	stride-1	accesses	in	your
code.
If	we	take	a	slice	through	the	mountain,	holding	the	stride	constant	as	in
Figure	
6.42
,	we	can	see	the	impact	of	cache	size	and	temporal	locality
on	performance.	For	sizes	up	to	32	KB,	the	working	set	fits	entirely	in	the
L1	d-cache,	and	thus	reads	are	served	from	L1	at	throughput	of	about	12
GB/s.	For	sizes	up	to	256	KB,	the	working	set	fits	entirely	in	the	unified
L2	cache,	and	for	sizes	up	to	8	MB,	the	working	set	fits	entirely	in	the
unified	L3	cache.	Larger	working	set	sizes	are	served	primarily	from	main
memory.
The	dips	in	read	throughputs	at	the	leftmost	edges	of	the	L2	and	L3
cache	regions—where	the	working	set	sizes	of	256	KB	and	8	MB	are
equal	to	their	respective	cache	sizes—are	interesting.	It	is	not	entirely
clear	why	these	dips	occur.	The	only	way	to	be	sure	is	to	perform	a
detailed	cache	simulation,	but	it	is	likely	that	the	drops	are	caused	by
conflicts	with	other	code	and	data	lines.
Slicing	through	the	memory	mountain	in	the	opposite	direction,	holding
the	working	set	size	constant,	gives	us	some	insight	into	the	impact	of
spatial	locality	on	the	read	throughput.	For	example,	
Figure	
6.43
shows	the	slice	for	a	fixed	working	set	size	of	4	MB.	This	slice	cuts	along
the	L3	ridge	in	
Figure	
6.41
,	where	the	working	set	fits	entirely	in	the	L3
cache	but	is	too	large	for	the	L2	cache.
Notice	how	the	read	throughput	decreases	steadily	as	the	stride
increases	from	one	to	eight	words.	In	this	region	of	the	mountain,	a	read
miss	in	L2	causes	a	block	to	be	transferred	from	L3	to	L2.	This	is
followed	by	some	number	of	hits</p>
<p>Figure	
6.43	
A	slope	of	spatial	locality.
The	graph	shows	a	slice	through	
Figure	
6.41
with	
=	4	MB.
on	the	block	in	L2,	depending	on	the	stride.	As	the	stride	increases,	the
ratio	of	L2	misses	to	L2	hits	increases.	Since	misses	are	served	more
slowly	than	hits,	the	read	throughput	decreases.	Once	the	stride	reaches
eight	8-byte	words,	which	on	this	system	equals	the	block	size	of	64
bytes,	every	read	request	misses	in	L2	and	must	be	served	from	L3.
Thus,	the	read	throughput	for	strides	of	at	least	eight	is	a	constant	rate
determined	by	the	rate	that	cache	blocks	can	be	transferred	from	L3	into
L2.
To	summarize	our	discussion	of	the	memory	mountain,	the	performance
of	the	memory	system	is	not	characterized	by	a	single	number.	Instead,	it
is	a	mountain	of	temporal	and	spatial	locality	whose	elevations	can	vary
by	over	an	order	of	magnitude.	Wise	programmers	try	to	structure	their
programs	so	that	they	run	in	the	peaks	instead	of	the	valleys.	The	aim	is
to	exploit	temporal	locality	so	that	heavily	used	words	are	fetched	from</p>
<p>the	L1	cache,	and	to	exploit	spatial	locality	so	that	as	many	words	as
possible	are	accessed	from	a	single	L1	cache	line.
Practice	Problem	
6.21	
(solution	page	
666
)
Use	the	memory	mountain	in	
Figure	
6.41
to	estimate	the	time,
in	CPU	cycles,	to	read	an	8-byte	word	from	the	L1	d-cache.
6.6.2	
Rearranging	Loops	to	Increase
Spatial	Locality
Consider	the	problem	of	multiplying	a	pair	of	
n
×	
n
matrices:	
C
=	
AB
.	For
example,	if	
n
=	2,	then
where
A	matrix	multiply	function	is	usually	implemented	using	three	nested
loops,	which	are	identified	by	their	indices	
i,	j
,	and	
k.
If	we	permute	the
loops	and	make	some	other	minor	code	changes,	we	can	create	the	six
functionally	equivalent	versions	of	matrix	multiply	shown	in	
Figure
6.44
.	Each	version	is	uniquely	identified	by	the	ordering	of	its	loops.
[</p>
<p>c
11
c
12
c
21
c
22</p>
<h1 id="-9"><a class="header" href="#-9">]</a></h1>
<p>[</p>
<p>a
11
a
12
a
21
a
22</p>
<p>]
 
[</p>
<p>b
11
b
12
b
21
b
22</p>
<h1>]
c
11</h1>
<p>a
11
b
11</p>
<ul>
<li></li>
</ul>
<h1>a
12
b
21
c
12</h1>
<p>a
11
b
12</p>
<ul>
<li></li>
</ul>
<h1>a
12
b
22
c
21</h1>
<p>a
21
b
11</p>
<ul>
<li></li>
</ul>
<h1>a
22
b
21
c
22</h1>
<p>a
21
b
12</p>
<p>At	a	high	level,	the	six	versions	are	quite	similar.	If	addition	is	associative,
then	each	version	computes	an	identical	result.
Each	version	performs
O
(
n
)	total	operations	and	an	identical	number	of	adds	and	multiplies.
Each	of	the	
n
elements	of	
A
and	
B
is	read	
n
times.	Each	of	the	
n
elements	of	
C
is	computed	by	summing	
n
values.	However,	if	we	analyze
the	behavior	of	the	innermost	loop	iterations,	we	find	that	there	are
differences	in	the	number	of	accesses	and	the	locality.	For	the	purposes
of	this	analysis,	we	make	the	following	assumptions:</p>
<ol>
<li></li>
</ol>
<p>As	we	learned	in	
Chapter	
2
,	floating-point	addition	is	commutative,	but	in	general	not
associative.	In	practice,	if	the	matrices	do	not	mix	extremely	large	values	with	extremely	small
ones,	as	often	is	true	when	the	matrices	store	physical	properties,	then	the	assumption	of
associativity	is	reasonable.
Each	array	is	an	
n
×	
n
array	of	
,	with	
.
There	is	a	single	cache	with	a	32-byte	block	size	(
B
=	32).
The	array	size	
n
is	so	large	that	a	single	matrix	row	does	not	fit	in	the
L1	cache.
The	compiler	stores	local	variables	in	registers,	and	thus	references
to	local	variables	inside	loops	do	not	require	any	load	or	store
instructions.
Figure	
6.45
summarizes	the	results	of	our	inner-loop	analysis.	Notice
that	the	six	versions	pair	up	into	three	equivalence	classes,	which	we
denote	by	the	pair	of	matrices	that	are	accessed	in	the	inner	loop.	For
example,	versions	
ijk
and	
jik
are	members	of	class	
AB
because	they
reference	arrays	
A
and	
B
(but	not	
C
)	in	their	innermost	loop.	For	each
class,	we	have	counted	the	number	of	loads	(reads)	and	stores	(writes)	in
each	inner-loop	iteration,	the	number	of	references	to	
A,	B
,	and	
C
that
1
3
2
2</p>
<h2>will	miss	in	the	cache	in	each	loop	iteration,	and	the	total	number	of
cache	misses	per	iteration.
The	inner	loops	of	the	class	
AB
routines	(
Figure	
6.44(a)
and	
(b)
)
scan	a	row	of	array	
A
with	a	stride	of	1.	Since	each	cache	block	holds
four	8-byte	words,	the	miss	rate	for	
A
is	0.25	misses	per	iteration.	On	the
other	hand,	the	inner	loop	scans	a	column	of	
B
with	a	stride	of	
n.
Since	
n
is	large,	each	access	of	array	
B
results	in	a	miss,	for	a	total	of	1.25
misses	per	iteration.
The	inner	loops	in	the	class	
AC
routines	(
Figure	
6.44(c)
and	
(d)
)
have	some	problems.	Each	iteration	performs	two	loads	and	a	store	(as
opposed	to	the
(a)	Version	
i	j	k</h2>
<h2 id="codememmatmultmmc"><a class="header" href="#codememmatmultmmc">code/mem/matmult/mm.c</a></h2>
<p>code/mem/matmult/mm.c</p>
<h2>(b)	Version	
jik</h2>
<h2 id="codememmatmultmmc-1"><a class="header" href="#codememmatmultmmc-1">code/mem/matmult/mm.c</a></h2>
<h2>code/mem/matmult/mm.c
(c)	Version	
jki</h2>
<p>code/mem/matmult/mm.c</p>
<hr />
<h2>code/mem/matmult/mm.c
(d)	Version	
kji</h2>
<h2 id="codememmatmultmmc-2"><a class="header" href="#codememmatmultmmc-2">code/mem/matmult/mm.c</a></h2>
<h2>code/mem/matmult/mm.c
(e)	Version	
kij</h2>
<p>code/mem/matmult/mm.c</p>
<hr />
<h2>code/mem/matmult/mm.c
(f)	Version	
ikj</h2>
<h2 id="codememmatmultmmc-3"><a class="header" href="#codememmatmultmmc-3">code/mem/matmult/mm.c</a></h2>
<p>code/mem/matmult/mm.c
Figure	
6.44	
Six	versions	of	matrix	multiply.
Each	version	is	uniquely	identified	by	the	ordering	of	its	loops.
Matrix	multiply	version
(class)
Per	iteration
Loads
Stores
A
misses
B
misses
C
misses
Total
misses
ijk
&amp;	
jik
(
AB
)
2
0
0.25
1.00
0.00
1.25
jki
&amp;	
kji
(
AC
)
2
1
1.00
0.00
1.00
2.00
kij
&amp;	
ikj
(
BC
)
2
1
0.00
0.25
0.25
0.50</p>
<p>Figure	
6.45	
Analysis	of	matrix	multiply	inner	loops.
The	six	versions	partition	into	three	equivalence	classes,	denoted	by	the
pair	of	arrays	that	are	accessed	in	the	inner	loop.
Figure	
6.46	
Core	i7	matrix	multiply	performance.
class	
AB
routines,	which	perform	two	loads	and	no	stores).	Second,	the
inner	loop	scans	the	columns	of	
A
and	
C
with	a	stride	of	
n
.	The	result	is	a
miss	on	each	load,	for	a	total	of	two	misses	per	iteration.	Notice	that
interchanging	the	loops	has	decreased	the	amount	of	spatial	locality
compared	to	the	class	
AB
routines.
The	
BC
routines	(
Figure	
6.44(e)
and	
(f)
)	present	an	interesting
trade-off:	With	two	loads	and	a	store,	they	require	one	more	memory
operation	than	the	
AB
routines.	On	the	other	hand,	since	the	inner	loop
scans	both	
B
and	
C
row-wise	with	a	stride-1	access	pattern,	the	miss	rate
on	each	array	is	only	0.25	misses	per	iteration,	for	a	total	of	0.50	misses
per	iteration.</p>
<p>Figure	
6.46
summarizes	the	performance	of	different	versions	of
matrix	multiply	on	a	Core	i7	system.	The	graph	plots	the	measured
number	of	CPU	cycles	per	inner-loop	iteration	as	a	function	of	array	size
(
n
).
There	are	a	number	of	interesting	points	to	notice	about	this	graph:
For	large	values	of	
n
,	the	fastest	version	runs	almost	40	times	faster
than	the	slowest	version,	even	though	each	performs	the	same
number	of	floating-point	arithmetic	operations.
Pairs	of	versions	with	the	same	number	of	memory	references	and
misses	per	iteration	have	almost	identical	measured	performance.
The	two	versions	with	the	worst	memory	behavior,	in	terms	of	the
number	of	accesses	and	misses	per	iteration,	run	significantly	slower
than	the	other	four	versions,	which	have	fewer	misses	or	fewer
accesses,	or	both.
Miss	rate,	in	this	case,	is	a	better	predictor	of	performance	than	the
total	number	of	memory	accesses.	For	example,	the	class	
BC
routines,	with	0.5	misses	per	iteration,	perform	much	better	than	the
class	
AB
routines,	with	1.25	misses	per	iteration,	even	though	the
class	
BC
routines	perform	more
Web	Aside	MEM:BLOCKING	
Using
blocking	to	increase	temporal	locality
There	is	an	interesting	technique	called	
blocking
that	can
improve	the	temporal	locality	of	inner	loops.	The	general	idea
of	blocking	is	to	organize	the	data	structures	in	a	program	into
large	chunks	called	
blocks.
(In	this	context,	&quot;block&quot;	refers	to	an</p>
<p>application-level	chunk	of	data,	
not
to	a	cache	block.)	The
program	is	structured	so	that	it	loads	a	chunk	into	the	L1
cache,	does	all	the	reads	and	writes	that	it	needs	to	on	that
chunk,	then	discards	the	chunk,	loads	in	the	next	chunk,	and
so	on.
Unlike	the	simple	loop	transformations	for	improving	spatial
locality,	blocking	makes	the	code	harder	to	read	and
understand.	For	this	reason,	it	is	best	suited	for	optimizing
compilers	or	frequently	executed	library	routines.	Blocking
does	not	improve	the	performance	of	matrix	multiply	on	the
Core	i7,	because	of	its	sophisticated	prefetching	hardware.
Still,	the	technique	is	interesting	to	study	and	understand
because	it	is	a	general	concept	that	can	produce	big
performance	gains	on	systems	that	don't	prefetch.
memory	references	in	the	inner	loop	(two	loads	and	one	store)	than
the	class	
AB
routines	(two	loads).
For	large	values	of	
n
,	the	performance	of	the	fastest	pair	of	versions
(
kij
and	
ikj
)	is	constant.	Even	though	the	array	is	much	larger	than	any
of	the	SRAM	cache	memories,	the	prefetching	hardware	is	smart
enough	to	recognize	the	stride-1	access	pattern,	and	fast	enough	to
keep	up	with	memory	accesses	in	the	tight	inner	loop.	This	is	a
stunning	accomplishment	by	the	Intel	engineers	who	designed	this
memory	system,	providing	even	more	incentive	for	programmers	to
develop	programs	with	good	spatial	locality.
6.6.3	
Exploiting	Locality	in	Your</p>
<p>Programs
As	we	have	seen,	the	memory	system	is	organized	as	a	hierarchy	of
storage	devices,	with	smaller,	faster	devices	toward	the	top	and	larger,
slower	devices	toward	the	bottom.	Because	of	this	hierarchy,	the	effective
rate	that	a	program	can	access	memory	locations	is	not	characterized	by
a	single	number.	Rather,	it	is	a	wildly	varying	function	of	program	locality
(what	we	have	dubbed	the	memory	mountain)	that	can	vary	by	orders	of
magnitude.	Programs	with	good	locality	access	most	of	their	data	from
fast	cache	memories.	Programs	with	poor	locality	access	most	of	their
data	from	the	relatively	slow	DRAM	main	memory.
Programmers	who	understand	the	nature	of	the	memory	hierarchy	can
exploit	this	understanding	to	write	more	efficient	programs,	regardless	of
the	specific	memory	system	organization.	In	particular,	we	recommend
the	following	techniques:
Focus	your	attention	on	the	inner	loops,	where	the	bulk	of	the
computations	and	memory	accesses	occur.
Try	to	maximize	the	spatial	locality	in	your	programs	by	reading	data
objects	sequentially,	with	stride	1,	in	the	order	they	are	stored	in
memory.
Try	to	maximize	the	temporal	locality	in	your	programs	by	using	a
data	object	as	often	as	possible	once	it	has	been	read	from	memory.</p>
<p>6.7	
Summary
The	basic	storage	technologies	are	random	access	memories	(RAMs),
nonvolatile	memories	(ROMs),	and	disks.	RAM	comes	in	two	basic
forms.	Static	RAM	(SRAM)	is	faster	and	more	expensive	and	is	used	for
cache	memories.	Dynamic	RAM	(DRAM)	is	slower	and	less	expensive
and	is	used	for	the	main	memory	and	graphics	frame	buffers.	ROMs
retain	their	information	even	if	the	supply	voltage	is	turned	off.	They	are
used	to	store	firmware.	Rotating	disks	are	mechanical	nonvolatile	storage
devices	that	hold	enormous	amounts	of	data	at	a	low	cost	per	bit,	but
with	much	longer	access	times	than	DRAM.	Solid	state	disks	(SSDs)
based	on	nonvolatile	flash	memory	are	becoming	increasingly	attractive
alternatives	to	rotating	disks	for	some	applications.
In	general,	faster	storage	technologies	are	more	expensive	per	bit	and
have	smaller	capacities.	The	price	and	performance	properties	of	these
technologies	are	changing	at	dramatically	different	rates.	In	particular,
DRAM	and	disk	access	times	are	much	larger	than	CPU	cycle	times.
Systems	bridge	these	gaps	by	organizing	memory	as	a	hierarchy	of
storage	devices,	with	smaller,	faster	devices	at	the	top	and	larger,	slower
devices	at	the	bottom.	Because	well-written	programs	have	good	locality,
most	data	are	served	from	the	higher	levels,	and	the	effect	is	a	memory
system	that	runs	at	the	rate	of	the	higher	levels,	but	at	the	cost	and
capacity	of	the	lower	levels.
Programmers	can	dramatically	improve	the	running	times	of	their
programs	by	writing	programs	with	good	spatial	and	temporal	locality.</p>
<p>Exploiting	SRAM-based	cache	memories	is	especially	important.
Programs	that	fetch	data	primarily	from	cache	memories	can	run	much
faster	than	programs	that	fetch	data	primarily	from	memory.</p>
<p>Bibliographic	Notes
Memory	and	disk	technologies	change	rapidly.	In	our	experience,	the
best	sources	of	technical	information	are	the	Web	pages	maintained	by
the	manufacturers.	Companies	such	as	Micron,	Toshiba,	and	Samsung
provide	a	wealth	of	current	technical	information	on	memory	devices.	The
pages	for	Seagate	and	Western	Digital	provide	similarly	useful
information	about	disks.
Textbooks	on	circuit	and	logic	design	provide	detailed	information	about
memory	technology	[58,	89].	
IEEE	Spectrum
published	a	series	of	survey
articles	on	DRAM	[55].	The	International	Symposiums	on	Computer
Architecture	(ISCA)	and	High	Performance	Computer	Architecture
(HPCA)	are	common	forums	for	characterizations	of	DRAM	memory
performance	[
28
,	
29
,	
18
].
Wilkes	wrote	the	first	paper	on	cache	memories	[
117
].	Smith	wrote	a
classic	survey	[
104
].	Przybylski	wrote	an	authoritative	book	on	cache
design	[
86
].	Hennessy	and	Patterson	provide	a	comprehensive
discussion	of	cache	design	issues	[
46
].	Levinthal	wrote	a	comprehensive
performance	guide	for	the	Intel	Core	i7	[
70
].
Stricker	introduced	the	idea	of	the	memory	mountain	as	a	comprehensive
characterization	of	the	memory	system	in	[
112
]	and	suggested	the	term
&quot;memory	mountain&quot;	informally	in	later	presentations	of	the	work.
Compiler	researchers	
work	to	increase	locality	by	automatically
performing	the	kinds	of	manual	code	transformations	we	discussed	in</p>
<p>Section	
6.6
[
22
,	
32
,	
66
,	
72
,	
79
,	
87
,	
119
].	Carter	and	colleagues	have
proposed	a	cache-aware	memory	controller	[
17
].	Other	researchers	have
developed	
cache-oblivious
algorithms	that	are	designed	to	run	well
without	any	explicit	knowledge	of	the	structure	of	the	underlying	cache
memory	[
30
,	
38
,	
39
,	
9
].
There	is	a	large	body	of	literature	on	building	and	using	disk	storage.
Many	storage	researchers	look	for	ways	to	aggregate	individual	disks	into
larger,	more	robust,	and	more	secure	storage	pools	[
20
,	
40
,	
41
,	
83
,	
121
].
Others	look	for	ways	to	use	caches	and	locality	to	improve	the
performance	of	disk	accesses	[
12
,	
21
].	Systems	such	as	Exokernel
provide	increased	user-level	control	of	disk	and	memory	resources	[
57
].
Systems	such	as	the	Andrew	File	System	[
78
]	and	Coda	[
94
]	extend	the
memory	hierarchy	across	computer	networks	and	mobile	notebook
computers.	Schindler	and	Ganger	developed	an	interesting	tool	that
automatically	characterizes	the	geometry	and	performance	of	SCSI	disk
drives	[
95
].	Researchers	have	investigated	techniques	for	building	and
using	flash-based	SSDs	[
8
,	
81
].</p>
<p>Homework	Problems
6.22
Suppose	you	are	asked	to	design	a	rotating	disk	where	the	number	of
bits	per	track	is	constant.	You	know	that	the	number	of	bits	per	track	is
determined	by	the	circumference	of	the	innermost	track,	which	you	can
assume	is	also	the	circumference	of	the	hole.	Thus,	if	you	make	the	hole
in	the	center	of	the	disk	larger,	the	number	of	bits	per	track	increases,	but
the	total	number	of	tracks	decreases.	If	you	let	
r
denote	the	radius	of	the
platter,	and	
x
·	
r
the	radius	of	the	hole,	what	value	of	
x
maximizes	the
capacity	of	the	disk?
6.23
Estimate	the	average	time	(in	ms)	to	access	a	sector	on	the	following
disk:
Parameter
Value
Rotational	rate
15,000	RPM
T
4	ms
Average	number	of	sectors/track
800
avg	seek</p>
<p>6.24
Suppose	that	a	2	MB	file	consisting	of	512-byte	logical	blocks	is	stored
on	a	disk	drive	with	the	following	characteristics:
Parameter
Value
Rotational	rate
15,000	RPM
T
4	ms
Average	number	of	sectors/track
1,000
Surfaces
8
Sector	size
512	bytes
For	each	case	below,	suppose	that	a	program	reads	the	logical	blocks	of
the	file	sequentially,	one	after	the	other,	and	that	the	time	to	position	the
head	over	the	first	block	is	
T
+	
T
.
A
.	
Best	case:
Estimate	the	optimal	time	(in	ms)	required	to	read	the
file	over	all	possible	mappings	of	logical	blocks	to	disk	sectors.
B
.	
Random	case:
Estimate	the	time	(in	ms)	required	to	read	the	file	if
blocks	are	mapped	randomly	to	disk	sectors.
6.25
avg	seek
avg	seek
avg	rotation</p>
<p>The	following	table	gives	the	parameters	for	a	number	of	different
caches.	For	each	cache,	fill	in	the	missing	fields	in	the	table.	Recall	that
m
is	the	number	of	physical	address	bits,	
C
is	the	cache	size	(number	of
data	bytes),	
B
is	the	block	size	in	bytes,	
E
is	the	associativity,	
S
is	the
number	of	cache	sets,	
t
is	the	number	of	tag	bits,	
s
is	the	number	of	set
index	bits,	and	
b
is	the	number	of	block	offset	bits.
Cache
m
C
B
E
S
t
s
b</p>
<ol>
<li></li>
</ol>
<p>32
1,024
4
4</p>
<hr />
<hr />
<hr />
<hr />
<ol start="2">
<li></li>
</ol>
<p>32
1,024
4
256</p>
<hr />
<hr />
<hr />
<hr />
<ol start="3">
<li></li>
</ol>
<p>32
1,024
8
1</p>
<hr />
<hr />
<hr />
<hr />
<ol start="4">
<li></li>
</ol>
<p>32
1,024
8
128</p>
<hr />
<hr />
<hr />
<hr />
<ol start="5">
<li></li>
</ol>
<p>32
1,024
32
1</p>
<hr />
<hr />
<hr />
<hr />
<ol start="6">
<li></li>
</ol>
<p>32
1,024
32
4</p>
<hr />
<hr />
<hr />
<hr />
<p>6.26
The	following	table	gives	the	parameters	for	a	number	of	different
caches.	Your	task	is	to	fill	in	the	missing	fields	in	the	table.	Recall	that	
m
is	the	number	of	physical	address	bits,	
C
is	the	cache	size	(number	of
data	bytes),	
B
is	the	block	size	in	bytes,	
E
is	the	associativity,	
S
is	the
number	of	cache	sets,	
t
is	the	number	of	tag	bits,	
s
is	the	number	of	set
index	bits,	and	
b
is	the	number	of	block	offset	bits.</p>
<p>Cache
m
C
B
E
S
t
s
b</p>
<ol>
<li></li>
</ol>
<p>32</p>
<hr />
<p>8
1</p>
<hr />
<p>21
8
3
2.
32
2,048</p>
<hr />
<hr />
<p>128
23
7
2
3.
32
1,024
2
8
64</p>
<hr />
<hr />
<p>1
4.
32
1,024</p>
<hr />
<p>2
16
23
4</p>
<hr />
<p>6.27
This	problem	concerns	the	cache	in	
Practice	Problem	
6.12
.
A
.	
List	all	of	the	hex	memory	addresses	that	will	hit	in	set	1.
B
.	
List	all	of	the	hex	memory	addresses	that	will	hit	in	set	6.
6.28
This	problem	concerns	the	cache	in	
Practice	Problem	
6.12
.
A
.	
List	all	of	the	hex	memory	addresses	that	will	hit	in	set	2.
B
.	
List	all	of	the	hex	memory	addresses	that	will	hit	in	set	4.
C
.	
List	all	of	the	hex	memory	addresses	that	will	hit	in	set	5.
D
.	
List	all	of	the	hex	memory	addresses	that	will	hit	in	set	7.</p>
<p>6.29
Suppose	we	have	a	system	with	the	following	properties:
The	memory	is	byte	addressable.
Memory	accesses	are	to	1-byte	words	(not	to	4-byte	words).
Addresses	are	12	bits	wide.
The	cache	is	two-way	set	associative	(
E
=	2),	with	a	4-byte	block	size
(
B
=	4)	and	four	sets	(
S
=	4).
The	contents	of	the	cache	are	as	follows,	with	all	addresses,	tags,	and
values	given	in	hexadecimal	notation:
Set	index
Tag
Valid
Byte	0
Byte	1
Byte	2
Byte	3
0
00
1
40
41
42
43
83
1
FE
97
CC
D0
1
00
1
44
45
46
47
83
0
—
—
—
—
2
00
1
48
49
4A
4B
40
0
—
—
—
—
3
FF
1
9A
C0
03
FF
00
0
—
—
—
—</p>
<p>A
.	
The	following	diagram	shows	the	format	of	an	address	(1	bit	per
box).	Indicate	(by	labeling	the	diagram)	the	fields	that	would	be
used	to	determine	the	following:
CO.	The	cache	block	offset
CI.	The	cache	set	index
CT.	The	cache	tag
B
.	
For	each	of	the	following	memory	accesses,	indicate	if	it	will	be	a
cache	hit	or	miss	when	
carried	out	in	sequence
as	listed.	Also	give
the	value	of	a	read	if	it	can	be	inferred	from	the	information	in	the
cache.
Operation
Address
Hit?
Read	value	(or	unknown)
Read
0x834</p>
<hr />
<hr />
<p>Write
0x836</p>
<hr />
<hr />
<p>Read
0xFFD</p>
<hr />
<hr />
<p>6.30
Suppose	we	have	a	system	with	the	following	properties:
The	memory	is	byte	addressable.
Memory	accesses	are	to	1-byte	words	(not	to	4-byte	words).</p>
<p>Addresses	are	13	bits	wide.
The	cache	is	4-way	set	associative	(
E
=	4),	with	a	4-byte	block	size	(
B
=	4)	and	eight	sets	(
S
=	8).
Consider	the	following	cache	state.	All	addresses,	tags,	and	values	are
given	in	hexadecimal	format.	The	Index	column	contains	the	set	index	for
each	set	of	four	lines.	The	Tag	columns	contain	the	tag	value	for	each
line.	The	V	columns	contain	the	valid	bit	for	each	line.	The	Bytes	0−3
columns	contain	the	data	for	each	line,	numbered	left	to	right	starting	with
byte	0	on	the	left.
4-way	set	associative	cache
Index
Tag
V
Bytes
0−3
Tag
V
Bytes
0−3
Tag
V
Bytes
0−3
Tag
V
Bytes
0−3
0
F0
1
ED
32	0A
A2
8A
1
BF	80
1D
FC
14
1
EF	09
86	2A
BC
0
25	44
6F	1A
1
BC
0
03	3E
CD
38
A0
0
16	7B
ED
5A
BC
1
8E	4C
DF	18
E4
1
FB
B7	12
02
2
BC
1
54	9E
1E	FA
B6
1
DC
81	B2
14
00
0
B6	1F
7B	44
74
0
10	F5
B8	2E
3
BE
0
2F	7E
3D
A8
C0
1
27	95
A4	74
C4
0
07	11
6B	D8
BC
0
C7
B7
AF
C2</p>
<p>4
7E
1
32	21
1C
2C
8A
1
22	C2
DC
34
BC
1
BA
DD
37	D8
DC
0
E7	A2
39	BA
5
98
0
A9	76
2B
EE
54
0
BC	91
D5	92
98
1
80	BA
9B	F6
BC
1
48	16
81	0A
6
38
0
5D
4D	F7
DA
BC
1
69	C2
8C	74
8A
1
A8
CE
7F
DA
38
1
FA	93
EB	48
7
8A
1
04	2A
32	6A
9E
0
B1	86
56	0E
CC
1
96	30
47	F2
BC
1
F8	1D
42	30
A
.	
What	is	the	size	(
C
)	of	this	cache	in	bytes?
B
.	
The	box	that	follows	shows	the	format	of	an	address	(1	bit	per
box).	Indicate	(by	labeling	the	diagram)	the	fields	that	would	be
used	to	determine	the	following:
CO.	The	cache	block	offset
CI.	The	cache	set	index
CT.	The	cache	tag
6.31</p>
<p>Suppose	that	a	program	using	the	cache	in	
Problem	
6.30
references
the	1-byte	word	at	address	
.	Indicate	the	cache	entry	accessed
and	the	cache	byte	value	returned	
in	hex
.	Indicate	whether	a	cache	miss
occurs.	If	there	is	a	cache	miss,	enter	&quot;—&quot;	for	&quot;Cache	byte	returned.&quot;
Hint:
Pay	attention	to	those	valid	bits!
A
.	
Address	format	(1	bit	per	box):
B
.	
Memory	reference:
Parameter
Value
Block	offset	(CO)
0x_____
Index	(CI)
0x_____
Cache	tag	(CT)
0x_____
Cache	hit?	(Y/N)</p>
<hr />
<p>Cache	byte	returned
0x_____
6.32
Repeat	
Problem	
6.31
for	memory	address	
.
A
.	
Address	format	(1	bit	per	box):</p>
<p>B
.	
Memory	reference:
Parameter
Value
Cache	offset	(CO)
0x_____
Cache	index	(CI)
0x_____
Cache	tag	(CT)
0x_____
Cache	hit?	(Y/N)</p>
<hr />
<p>Cache	byte	returned
0x_____
6.33
For	the	cache	in	
Problem	
6.30
,	list	the	eight	memory	addresses	(in
hex)	that	will	hit	in	set	2.
6.34
Consider	the	following	matrix	transpose	routine:</p>
<p>Assume	this	code	runs	on	a	machine	with	the	following	properties:
=	4.
The	
array	starts	at	address	0	and	the	
array	starts	at	address
64	(decimal).
There	is	a	single	L1	data	cache	that	is	direct-mapped,	write-through,
write-allocate,	with	a	block	size	of	16	bytes.
The	cache	has	a	total	size	of	32	data	bytes,	and	the	cache	is	initially
empty.
Accesses	to	the	
and	
arrays	are	the	only	sources	of	read	and
write	misses,	respectively.
A
.	
For	each	row	and	
,	indicate	whether	the	access	to	
and	
is	a	hit	(h)	or	a	miss	(m).	For	example,
reading	
is	a	miss	and	writing	
is	also	a	miss.
Col.	0
Col.	1
Col.	2
Col.	3
Col.	0
Col.	1
Col.	2
Col.	3
Row
m</p>
<hr />
<hr />
<hr />
<p>Row
m</p>
<hr />
<hr />
<hr />
<p>0
0
Row
1</p>
<hr />
<hr />
<hr />
<hr />
<p>Row
1</p>
<hr />
<hr />
<hr />
<hr />
<p>Row
2</p>
<hr />
<hr />
<hr />
<hr />
<p>Row
2</p>
<hr />
<hr />
<hr />
<hr />
<p>Row
3</p>
<hr />
<hr />
<hr />
<hr />
<p>Row
3</p>
<hr />
<hr />
<hr />
<hr />
<p>6.35
Repeat	
Problem	
6.34
for	a	cache	with	a	total	size	of	128	data	bytes.
Col.	0
Col.	1
Col.	2
Col.	3
Col.	0
Col.	1
Col.	2
Col.	3
Row
0</p>
<hr />
<hr />
<hr />
<hr />
<p>Row
0</p>
<hr />
<hr />
<hr />
<hr />
<p>Row
1</p>
<hr />
<hr />
<hr />
<hr />
<p>Row
1</p>
<hr />
<hr />
<hr />
<hr />
<p>Row
2</p>
<hr />
<hr />
<hr />
<hr />
<p>Row
2</p>
<hr />
<hr />
<hr />
<hr />
<p>Row
3</p>
<hr />
<hr />
<hr />
<hr />
<p>Row
3</p>
<hr />
<hr />
<hr />
<hr />
<p>6.36
This	problem	tests	your	ability	to	predict	the	cache	behavior	of	C	code.
You	are	given	the	following	code	to	analyze:
Assume	we	execute	this	under	the	following	conditions:
=	4.
Array	
begins	at	memory	address	
and	is	stored	in	row-major
order.
In	each	case	below,	the	cache	is	initially	empty.
The	only	memory	accesses	are	to	the	entries	of	the	array	
.	All	other
variables	are	stored	in	registers.
Given	these	assumptions,	estimate	the	miss	rates	for	the	following	cases:</p>
<p>A
.	
Case	1:	Assume	the	cache	is	512	bytes,	direct-mapped,	with	16-
byte	cache	blocks.	What	is	the	miss	rate?
B
.	
Case	2:	What	is	the	miss	rate	if	we	double	the	cache	size	to	1,024
bytes?
C
.	
Case	3:	Now	assume	the	cache	is	512	bytes,	two-way	set
associative	using	an	LRU	replacement	policy,	with	16-byte	cache
blocks.	What	is	the	cache	miss	rate?
D
.	
For	case	3,	will	a	larger	cache	size	help	to	reduce	the	miss	rate?
Why	or	why	not?
E
.	
For	case	3,	will	a	larger	block	size	help	to	reduce	the	miss	rate?
Why	or	why	not?
6.37
This	is	another	problem	that	tests	your	ability	to	analyze	the	cache
behavior	of	C	code.	Assume	we	execute	the	three	summation	functions
in	
Figure	
6.47
under	the	following	conditions:
=	4.
The	machine	has	a	4	KB	direct-mapped	cache	with	a	16-byte	block
size.
Within	the	two	loops,	the	code	uses	memory	accesses	only	for	the
array	data.	The	loop	indices	and	the	value	
are	held	in	registers.
Array	a	is	stored	starting	at	memory	address	
.
Fill	in	the	table	for	the	approximate	cache	miss	rate	for	the	two	cases	
N
=
64	and	
N
=	60.</p>
<p>Function
N
=	64
N
=	60</p>
<hr />
<hr />
<hr />
<hr />
<hr />
<hr />
<p>Figure	
6.47	
Functions	referenced	in	
Problem	
6.37
.
6.38
3M	decides	to	make	Post-its	by	printing	yellow	squares	on	white	pieces
of	paper.	As	part	of	the	printing	process,	they	need	to	set	the	CMYK
(cyan,	magenta,	yellow,	black)	value	for	every	point	in	the	square.	3M
hires	you	to	determine	the	efficiency	of	the	following	algorithms	on	a
machine	with	a	2,048-byte	direct-mapped	data	cache	with	32-byte
blocks.	You	are	given	the	following	definitions:</p>
<p>Assume	the	following:
=	4.
begins	at	memory	address	0.
The	cache	is	initially	empty.
The	only	memory	accesses	are	to	the	entries	of	the	array	square.
Variables	
and	
are	stored	in	registers.
Determine	the	cache	performance	of	the	following	code:</p>
<p>A
.	
What	is	the	total	number	of	writes?
B
.	
What	is	the	total	number	of	writes	that	miss	in	the	cache?
C
.	
What	is	the	miss	rate?
6.39
Given	the	assumptions	in	
Problem	
6.38
,	determine	the	cache
performance	of	the	following	code:
A
.	
What	is	the	total	number	of	writes?
B
.	
What	is	the	total	number	of	writes	that	miss	in	the	cache?
C
.	
What	is	the	miss	rate?</p>
<p>6.40
Given	the	assumptions	in	
Problem	
6.38
,	determine	the	cache
performance	of	the	following	code:
A
.	
What	is	the	total	number	of	writes?
B
.	
What	is	the	total	number	of	writes	that	miss	in	the	cache?
C
.	
What	is	the	miss	rate?
6.41</p>
<p>You	are	writing	a	new	3D	game	that	you	hope	will	earn	you	fame	and
fortune.	You	are	currently	working	on	a	function	to	blank	the	screen	buffer
before	drawing	the	next	frame.	The	screen	you	are	working	with	is	a	640
×	480	array	of	pixels.	The	machine	you	are	working	on	has	a	64	KB
direct-mapped	cache	with	4-byte	lines.	The	C	structures	you	are	using
are	as	follows:
Assume	the	following:
=	1	and	
=	4.
begins	at	memory	address	0.
The	cache	is	initially	empty.
The	only	memory	accesses	are	to	the	entries	of	the	array	buffer.
Variables	
,	and	
are	stored	in	registers.</p>
<p>What	percentage	of	writes	in	the	following	code	will	miss	in	the	cache?
6.42
Given	the	assumptions	in	
Problem	
6.41
,	what	percentage	of	writes	in
the	following	code	will	miss	in	the	cache?
6.43</p>
<p>Given	the	assumptions	in	
Problem	
6.41
,	what	percentage	of	writes	in
the	following	code	will	miss	in	the	cache?
6.44
Download	the	
program	from	the	CS:APP	Web	site	and	run	it	on
your	favorite	PC/Linux	system.	Use	the	results	to	estimate	the	sizes	of
the	caches	on	your	system.
6.45
In	this	assignment,	you	will	apply	the	concepts	you	learned	in	
Chapters
5
and	
6
to	the	problem	of	optimizing	code	for	a	memory-intensive
application.	Consider	a	procedure	to	copy	and	transpose	the	elements	of
an	
N
×	
N
matrix	of	type	int.	That	is,	for	source	matrix	
S
and	destination
matrix	
D
,	we	want	to	copy	each	element	
s
to	
d
.	This	code	can	be
written	with	a	simple	loop,
i,j
j,i</p>
<p>where	the	arguments	to	the	procedure	are	pointers	to	the	destination
(
)	and	source	(
)	matrices,	as	well	as	the	matrix	size	
N
(
).	Your
job	is	to	devise	a	transpose	routine	that	runs	as	fast	as	possible.
6.46
This	assignment	is	an	intriguing	variation	of	
Problem	
6.45
.	Consider
the	problem	of	converting	a	directed	graph	
g
into	its	undirected
counterpart	
g′.
The	graph	
g′
has	an	edge	from	vertex	
u
to	vertex	
v
if	and
only	if	there	is	an	edge	from	
u
to	
v
or	from	
v
to	
u
in	the	original	graph	
g.
The	graph	
g
is	represented	by	its	
adjacency	matrix	G
as	follows.	If	
N
is
the	number	of	vertices	in	
g
,	then	
G
is	an	
N
×	
N
matrix	and	its	entries	are
all	either	0	or	1.	Suppose	the	vertices	of	
g
are	named	
v
,	
v
,	
v
,	...,	
v
.
Then	
G
[
i
][
j
]	is	1	if	there	is	an	edge	from	
v
to	
v
and	is	0	otherwise.
Observe	that	the	elements	on	the	diagonal	of	an	adjacency	matrix	are
always	1	and	that	the	adjacency	matrix	of	an	undirected	graph	is
symmetric.	This	code	can	be	written	with	a	simple	loop:
0
1
2
N
-1
i
j</p>
<p>Your	job	is	to	devise	a	conversion	routine	that	runs	as	fast	as	possible.
As	before,	you	will	need	to	apply	concepts	you	learned	in	
Chapters	
5
and	
6
to	come	up	with	a	good	solution.</p>
<p>Solutions	to	Practice	Problems
Solution	to	Problem	
6.1	
(page
584
)
The	idea	here	is	to	minimize	the	number	of	address	bits	by	minimizing
the	aspect	ratio	max(
r,	c
)/	min(
r,	c
).	In	other	words,	the	squarer	the	array,
the	fewer	the	address	bits.
Organization
r
c
b
b
max(
b
,	b
)
16	×	1
4
4
2
2
2
16	×	4
4
4
2
2
2
128	×	8
16
8
4
3
4
512	×	4
32
16
5
4
5
1,024	×	4
32
32
5
5
5
Solution	to	Problem	
6.2	
(page
592
)
r
c
r
c</p>
<h1>The	point	of	this	little	drill	is	to	make	sure	you	understand	the	relationship
between	cylinders	and	tracks.	Once	you	have	that	straight,	just	plug	and
chug:
Solution	to	Problem	
6.3	
(page
595
)
The	solution	to	this	problem	is	a	straightforward	application	of	the	formula
for	disk	access	time.	The	average	rotational	latency	(in	ms)	is
The	average	transfer	time	is
Putting	it	all	together,	the	total	estimated	access	time	is
Solution	to	Problem	
6.4	
(page
Disk
 
capacity</h1>
<h1>512
 
bytes
sector
×
400
 
sectors
track
×
10
,
000
 
tracks
surface
×
2
 
surfaces
T
avg
 
rotation</h1>
<h1>1
/
2
×
T
max
 
rotation</h1>
<h1>1
/
2
×
(
60
 
secs
/
15
,
000
 
RPM
)
×
1
,
000
 
ms/sec
T
avg
 
transfer</h1>
<h1>(
60
 
secs
/
15
,
000
 
RPM
)
×
1
/
500
 
sectors/track
×
1
,
000
 
ms/sec
≈
0.008
 
ms
T
access</h1>
<p>T
avg
 
seek</p>
<ul>
<li></li>
</ul>
<p>T
avg
 
rotation</p>
<ul>
<li></li>
</ul>
<h1>T
avg
 
transfer</h1>
<p>8
 
ms</p>
<ul>
<li></li>
</ul>
<p>2
 
ms</p>
<ul>
<li></li>
</ul>
<p>0.008
 
ms
≈
10</p>
<p>595
)
This	is	a	good	check	of	your	understanding	of	the	factors	that	affect	disk
performance.	First	we	need	to	determine	a	few	basic	properties	of	the	file
and	the	disk.	The	file	consists	of	2,000	512-byte	logical	blocks.	For	the
disk,	
T
=	5	ms,	
T
=	6	ms,	and	
T
=	3	ms.
A
.	
Best	case:	
In	the	optimal	case,	the	blocks	are	mapped	to
contiguous	sectors,	on	the	same	cylinder,	that	can	be	read	one
after	the	other	without	moving	the	head.	Once	the	head	is
positioned	over	the	first	sector	it	takes	two	full	rotations	(1,000
sectors	per	rotation)	of	the	disk	to	read	all	2,000	blocks.	So	the
total	time	to	read	the	file	is	
T
+	
T
+	2	×	
T
=	5	+
3	+	12	=	20	ms.
B
.	
Random	case:	
In	this	case,	where	blocks	are	mapped	randomly
to	sectors,	reading	each	of	the	2,000	blocks	requires	
T
+	
T
ms,	so	the	total	time	to	read	the	file	is	(
T
+	
T
)	×
2,000	=	16,000	ms	(16	seconds!).
You	can	see	now	why	it's	often	a	good	idea	to	defragment	your	disk
drive!
Solution	to	Problem	
6.5	
(page
601
)
avg	seek
max	rotation
avg	rotation
avg	seek
avg	rotation
max	rotation
avg	seek
avg
rotation
avg	seek
avg	rotation</p>
<p>This	is	a	simple	problem	that	will	give	you	some	interesting	insights	into
the	feasibility	of	SSDs.	Recall	that	for	disks,	1	PB	=	10
MB.	Then	the
following	straightforward	translation	of	units	yields	the	following	predicted
times	for	each	case:
A
.	
Worst-case	sequential	writes	(470	MB/s):
B
.	
Worst-case	random	writes	(303	MB/s):
C
.	
Average	case	(20	GB/day):
So	even	if	the	SSD	operates	continuously,	it	should	last	for	at	least	8
years,	which	is	longer	than	the	expected	lifetime	of	most	computers.
Solution	to	Problem	
6.6	
(page
604
)
In	the	10-year	period	between	2005	and	2015,	the	unit	price	of	rotating
disks	dropped	by	a	factor	of	166,	which	means	the	price	is	dropping	by
roughly	a	factor	of	2	every	18	months	or	so.	Assuming	this	trend
continues,	a	petabyte	of	storage,	which	costs	about	$30,000	in	2015,	will
drop	below	$500	after	about	seven	of	these	factor-of-2	reductions.	Since
these	are	occurring	every	18	months,	we	might	expect	a	petabyte	of
storage	to	be	available	for	$500	around	the	year	2025.
9
(
10
9
×
128
)
×
(
1
/
470
)
×
(
1
/
(
86
,
400
×
365
)
)
≈
8
 
years
(
10
9
×
128
)
×
(
1
/
303
)
×
(
1
/
(
86
,
400
×
365
)
)
≈
13
 
years
(
10
9
×
128
)
×
(
1
/
20
,
000
)
×
(
1
/
365
)
≈
140
 
years</p>
<p>Solution	to	Problem	
6.7	
(page
608
)
To	create	a	stride-1	reference	pattern,	the	loops	must	be	permuted	so
that	the	rightmost	indices	change	most	rapidly.
This	is	an	important	idea.	Make	sure	you	understand	why	this	particular
loop	permutation	results	in	a	stride-1	access	pattern.
Solution	to	Problem	
6.8	
(page</p>
<p>609
)
The	key	to	solving	this	problem	is	to	visualize	how	the	array	is	laid	out	in
memory	and	then	analyze	the	reference	patterns.	Function	
accesses	the	array	using	a	stride-1	reference	pattern	and	thus	clearly
has	the	best	spatial	locality.	Function	
scans	each	of	the	
N
structs
in	order,	which	is	good,	but	within	each	struct	it	hops	around	in	a	non-
stride-1	pattern	at	the	following	offsets	from	the	beginning	of	the	struct:	0,
12,	4,	16,	8,	20.	So	
has	worse	spatial	locality	than	
.
Function	
not	only	hops	around	within	each	struct,	but	also	hops
from	struct	to	struct.	So	
exhibits	worse	spatial	locality	than	
and	
.
Solution	to	Problem	
6.9	
(page
616
)
The	solution	is	a	straightforward	application	of	the	definitions	of	the
various	cache	parameters	in	
Figure	
6.26
.	Not	very	exciting,	but	you
need	to	understand	how	the	cache	organization	induces	these	partitions
in	the	address	bits	before	you	can	really	understand	how	caches	work.
Cache
m
C
B
E
S
t
s
b</p>
<ol>
<li></li>
</ol>
<p>32
1,024
4
1
256
22
8
2
2.
32
1,024
8
4
32
24
5
3</p>
<ol start="3">
<li></li>
</ol>
<p>32
1,024
32
32
1
27
0
5
Solution	to	Problem	
6.10	
(page
624
)
The	padding	eliminates	the	conflict	misses.	Thus,	three-fourths	of	the
references	are	hits.
Solution	to	Problem	
6.11	
(page
624
)
Sometimes,	understanding	why	something	is	a	bad	idea	helps	you
understand	why	the	alternative	is	a	good	idea.	Here,	the	bad	idea	we	are
looking	at	is	indexing	the	cache	with	the	high-order	bits	instead	of	the
middle	bits.
A
.	
With	high-order	bit	indexing,	each	contiguous	array	chunk	consists
of	2
blocks,	where	
t
is	the	number	of	tag	bits.	Thus,	the	first	2
contiguous	blocks	of	the	array	would	map	to	set	0,	the	next	2
blocks	would	map	to	set	1,	and	so	on.
B
.	
For	a	direct-mapped	cache	where	(
S,	E,	B,	m
)	=	(512,	1,	32,	32),
the	cache	capacity	is	512	32-byte	blocks	with	
t
=	18	tag	bits	in
each	cache	line.	Thus,	the	first	2
blocks	in	the	array	would	map
t
t
t
18
18</p>
<p>to	set	0,	the	next	2
blocks	to	set	1.	Since	our	array	consists	of
only	(4,096	×	4
)/
32	=	512	blocks,	all	of	the	blocks	in	the	array	map
to	set	0.	Thus,	the	cache	will	hold	at	most	1	array	block	at	any
point	in	time,	even	though	the	array	is	small	enough	to	fit	entirely
in	the	cache.	Clearly,	using	high-order	bit	indexing	makes	poor	use
of	the	cache.
Solution	to	Problem	
6.12	
(page
628
)
The	2	low-order	bits	are	the	block	offset	(CO),	followed	by	3	bits	of	set
index	(CI),	with	the	remaining	bits	serving	as	the	tag	(CT):
Solution	to	Problem	
6.13	
(page
628
)
Address:	
A
.	
Address	format	(1	bit	per	box):
18</p>
<p>B
.	
Memory	reference:
Parameter
Value
Cache	block	offset	(CO)
Cache	set	index	(CI)
Cache	tag	(CT)
Cache	hit?	(Y/N)
Y
Cache	byte	returned
Solution	to	Problem	
6.14	
(page
629
)
Address:	
A
.	
Address	format	(1	bit	per	box):
B
.	
Memory	reference:
Parameter
Value
Cache	block	offset	(CO)
Cache	set	index	(CI)</p>
<p>Cache	tag	(CT)
Cache	hit?	(Y/N)
N
Cache	byte	returned
—
Solution	to	Problem	
6.15	
(page
629
)
Address:	
A
.	
Address	format	(1	bit	per	box):
B
.	
Memory	reference:
Parameter
Value
Cache	block	offset
Cache	set	index
Cache	tag
Cache	hit?	(Y/N)
N
Cache	byte	returned
—</p>
<p>Solution	to	Problem	
6.16	
(page
630
)
This	problem	is	a	sort	of	inverse	version	of	Practice	Problems	6.12−6.15
that	requires	you	to	work	backward	from	the	contents	of	the	cache	to
derive	the	addresses	that	will	hit	in	a	particular	set.	In	this	case,	set	3
contains	one	valid	line	with	a	tag	of	
.	Since	there	is	only	one	valid
line	in	the	set,	four	addresses	will	hit.	These	addresses	have	the	binary
form	
.	Thus,	the	four	hex	addresses	that	hit	in	set	3	are
Solution	to	Problem	
6.17	
(page
636
)
A
.	
The	key	to	solving	this	problem	is	to	visualize	the	picture	in	
Figure
6.48
.	Notice	that	each	cache	line	holds	exactly	one	row	of	the
array,	that	the	cache	is	exactly	large	enough	to	hold	one	array,	and
that	for	all	
i
,	row	
i
of	
and	
maps	to	the	same	cache	line.
Because	the	cache	is	too	small	to	hold	both	arrays,	references	to
one	array	keep	evicting	useful	lines	from	the	other	array.	For
example,	the	write	to	
evicts	the	line	that	was	loaded</p>
<p>when	we	read	
.	So	when	we	next	read	
,	we
have	a	miss.
array
array
Col.	0
Col.	1
Col.	0
Col.	1
Row	0
m
m
Row	0
m
m
Row	1
m
m
Row	1
m
h
B
.	
When	the	cache	is	32	bytes,	it	is	large	enough	to	hold	both	arrays.
Thus,	the	only	misses	are	the	initial	cold	misses.
array
array
Col.	0
Col.	1
Col.	0
Col.	1
Row	0
m
h
Row	0
m
h
Row	1
m
h
Row	1
m
h
Figure	
6.48	
Figure	for	solution	to	
Problem	
6.17
.
Solution	to	Problem	
6.18	
(page
637
)</p>
<p>Each	16-byte	cache	line	holds	two	contiguous	
structures.
Each	loop	visits	these	structures	in	memory	order,	reading	one	integer
element	each	time.	So	the	pattern	for	each	loop	is	miss,	hit,	miss,	hit,	and
so	on.	Notice	that	for	this	problem	we	could	have	predicted	the	miss	rate
without	actually	enumerating	the	total	number	of	reads	and	misses.
A
.	
What	is	the	total	number	of	read	accesses?	512	reads.
B
.	
What	is	the	total	number	of	read	accesses	that	miss	in	the	cache?
256	misses.
C
.	
What	is	the	miss	rate?	256/512	=	50%.
Solution	to	Problem	
6.19	
(page
638
)
The	key	to	this	problem	is	noticing	that	the	cache	can	only	hold	1/2	of	the
array.	So	the	column-wise	scan	of	the	second	half	of	the	array	evicts	the
lines	that	were	loaded	during	the	scan	of	the	first	half.	For	example,
reading	the	first	element	of	
evicts	the	line	that	was	loaded
when	we	read	elements	from	
.	This	line	also	contained	
.	So	when	we	begin	scanning	the	next	column,	the	reference	to	the
first	element	of	
misses.
A
.	
What	is	the	total	number	of	read	accesses?	512	reads.
B
.	
What	is	the	total	number	of	read	accesses	that	miss	in	the	cache?
256	misses.
C
.	
What	is	the	miss	rate?	256/512	=	50%.</p>
<p>D
.	
What	would	the	miss	rate	be	if	the	cache	were	twice	as	big?	If	the
cache	were	twice	as	big,	it	could	hold	the	entire	grid	array.	The
only	misses	would	be	the	initial	cold	misses,	and	the	miss	rate
would	be	1/4	=	25%.
Solution	to	Problem	
6.20	
(page
638
)
This	loop	has	a	nice	stride-1	reference	pattern,	and	thus	the	only	misses
are	the	initial	cold	misses.
A
.	
What	is	the	total	number	of	read	accesses?	512	reads.
B
.	
What	is	the	total	number	of	read	accesses	that	miss	in	the	cache?
128	misses.
C
.	
What	is	the	miss	rate?	128/512	=	25%.
D
.	
What	would	the	miss	rate	be	if	the	cache	were	twice	as	big?
Increasing	the	cache	size	by	any	amount	would	not	change	the
miss	rate,	since	cold	misses	are	unavoidable.
Solution	to	Problem	
6.21	
(page
643
)
The	sustained	throughput	using	large	strides	from	L1	is	about	12,000
MB/s,	the	clock	frequency	is	2,100	MHz,	and	the	individual	read</p>
<p>accesses	are	in	units	of	8-byte	longs.	Thus,	from	this	graph	we	can
estimate	that	it	takes	roughly	2,100/12,000	×	8	=	1.4	≈	1.5	cycles	to
access	a	word	from	L1	on	this	machine,	which	is	roughly	2.5	times	faster
than	the	nominal	4-cycle	latency	from	L1.	This	is	due	to	the	parallelism	of
the	4	×	4	unrolled	loop,	which	allows	multiple	loads	to	be	in	flight	at	the
same	time.</p>
<p>Part	
II	
Running	Programs	on	a
System
Our	exploration	of	computer	systems	continues	with	a	closer	look	at	the
systems	software	that	builds	and	runs	application	programs.	The	linker
combines	different	parts	of	our	programs	into	a	single	file	that	can	be
loaded	into	memory	and	executed	by	the	processor.	Modern	operating
systems	cooperate	with	the	hardware	to	provide	each	program	with	the
illusion	that	it	has	exclusive	use	of	a	processor	and	the	main	memory,
when	in	reality	multiple	programs	are	running	on	the	system	at	any	point
in	time.
In	the	first	part	of	this	book,	you	developed	a	good	understanding	of	the
interaction	between	your	programs	and	the	hardware.	Part	II	of	the	book
will	broaden	your	view	of	systems	by	giving	you	a	solid	understanding	of
the	interactions	between	your	programs	and	the	operating	system.	You
will	learn	how	to	use	services	provided	by	the	operating	system	to	build
system-level	programs	such	as	Unix	shells	and	dynamic	memory
allocation	packages.</p>
<p>Chapter	
7	
Linking
7.1	
Compiler	Drivers	
671
7.2	
Static	Linking	
672
7.3	
Object	Files	
673
7.4	
Relocatable	Object	Files	
674
7.5	
Symbols	and	Symbol	Tables	
675
7.6	
Symbol	Resolution	
679
7.7	
Relocation	
689
7.8	
Executable	Object	Files	
695
7.9	
Loading	Executable	Object	Files	
697
7.10	
Dynamic	Linking	with	Shared	Libraries	
698
7.11	
Loading	and	Linking	Shared	Libraries	from	Applications	
701
7.12	
Position-Independent	Code	(PIC)	
704
7.13	
Library	Interpositioning	
707
7.14	
Tools	for	Manipulating	Object	Files	
713</p>
<p>7.15	
Summary</p>
<p>713
Bibliographic	Notes	
714
Homework	Problems	
714
Solutions	to	Practice	Problems	
717
Linking	is	the	process	of	collecting	and	combining
various	pieces	of	code	and	data	into	a	single	file
that	can	be	
loaded
(copied)	into	memory	and
executed.	Linking	can	be	performed	at	
compile	time
,
when	the	source	code	is	translated	into	machine
code;	at	
load	time
,	when	the	program	is	loaded	into
memory	and	executed	by	the	
loader
;	and	even	at
run	time
,	by	application	programs.	On	early
computer	systems,	linking	was	performed	manually.
On	modern	systems,	linking	is	performed
automatically	by	programs	called	
linkers
.
Linkers	play	a	crucial	role	in	software	development
because	they	enable	
separate	compilation
.	Instead
of	organizing	a	large	application	as	one	monolithic
source	file,	we	can	decompose	it	into	smaller,	more
manageable	modules	that	can	be	modified	and
compiled	separately.	When	we	change	one	of	these
modules,	we	simply	recompile	it	and	relink	the
application,	without	having	to	recompile	the	other
files.</p>
<p>Linking	is	usually	handled	quietly	by	the	linker	and	is
not	an	important	issue	for	students	who	are	building
small	programs	in	introductory	programming
classes.	So	why	bother	learning	about	linking?
Understanding	linkers	will	help	you	build
large	programs.	
Programmers	who	build	large
programs	often	encounter	linker	errors	caused
by	missing	modules,	missing	libraries,	or
incompatible	library	versions.	Unless	you
understand	how	a	linker	resolves	references,
what	a	library	is,	and	how	a	linker	uses	a	library
to	resolve	references,	these	kinds	of	errors	will
be	baffling	and	frustrating.
Understanding	linkers	will	help	you	avoid
dangerous	programming	errors.	
The
decisions	that	Linux	linkers	make	when	they
resolve	symbol	references	can	silently	affect	the
correctness	of	your	programs.	Programs	that
incorrectly	define	multiple	global	variables	can
pass	through	the	linker	without	any	warnings	in
the	default	case.	The	resulting	programs	can
exhibit	baffling	run-time	behavior	and	are
extremely	difficult	to	debug.	We	will	show	you
how	this	happens	and	how	to	avoid	it.
Understanding	linking	will	help	you
understand	how	language	scoping	rules	are
implemented.	
For	example,	what	is	the
difference	between	global	and	local	variables?</p>
<h2>What	does	it	really	mean	when	you	define	a
variable	or	function	with	the	static	attribute?
Understanding	linking	will	help	you
understand	other	important	systems
concepts.	
The	executable	object	files	produced
by	linkers	play	key	roles	in	important	systems
functions	such	as	loading	and	running	programs,
virtual	memory,	paging,	and	memory	mapping.
Understanding	linking	will	enable	you	to
exploit	shared	libraries.	
For	many	years,
linking	was	considered	to	be	fairly
straightforward	and	uninteresting.	However,	with
the	increased	importance	of	shared	libraries	and
dynamic	linking	in	modern	operating	systems,
linking	is	a	sophisticated	process	that	provides
the	knowledgeable	programmer	with	significant
power.	For	example,	many	software	products
use	shared	libraries	to	upgrade	shrink-wrapped
binaries	at	run	time.	Also,	many	Web	servers
rely	on	dynamic	linking	of	shared	libraries	to
serve	dynamic	content.
(a)</h2>
<p>code/link/main.c</p>
<hr />
<h2>code/link/main.c
(b)</h2>
<p>code/link/sum.c</p>
<hr />
<p>code/link/sum.c
Figure	
7.1	
Example	program	1.
The	example	program	consists	of	two	source	files,
and	
.	The	
initializes	an
array	of	
,	and	then	calls	the	
function	to	sum
the	array	elements.
This	chapter	provides	a	thorough	discussion	of	all
aspects	of	linking,	from	traditional	static	linking,	to
dynamic	linking	of	shared	libraries	at	load	time,	to
dynamic	linking	of	shared	libraries	at	run	time.	We
will	describe	the	basic	mechanisms	using	real
examples,	and	we	will	identify	situations	in	which
linking	issues	can	affect	the	performance	and
correctness	of	your	programs.	To	keep	things
concrete	and	understandable,	we	will	couch	our
discussion	in	the	context	of	an	x86-64	system
running	Linux	and	using	the	standard	ELF-64
(hereafter	referred	to	as	ELF)	object	file	format.
However,	it	is	important	to	realize	that	the	basic
concepts	of	linking	are	universal,	regardless	of	the
operating	system,	the	ISA,	or	the	object	file	format.
Details	may	vary,	but	the	concepts	are	the	same.</p>
<p>7.1	
Compiler	Drivers
Consider	the	C	program	in	
Figure	
7.1
.	It	will	serve	as	a	simple	running
example	throughout	this	chapter	that	will	allow	us	to	make	some
important	points	about	how	linkers	work.
Most	compilation	systems	provide	a	
compiler	driver
that	invokes	the
language	preprocessor,	compiler,	assembler,	and	linker,	as	needed	on
behalf	of	the	user.	For	example,	to	build	the	example	program	using	the
GNU	compilation	system,	we	might	invoke	the	
GCC</p>
<p>driver	by	typing	the
following	command	to	the	shell:
Figure	
7.2
summarizes	the	activities	of	the	driver	as	it	translates	the
example	program	from	an	ASCII	source	file	into	an	executable	object	file.
(If	you	want	to	see	these	steps	for	yourself,	run	
GCC</p>
<p>with	the	-
option.)
The	driver	first	runs	the	C	preprocessor	(
),
which	translates	the	C
source	file	
into	an	ASCII	intermediate	file	
:</p>
<ol>
<li></li>
</ol>
<p>In	some	versions	of	
GCC
,	the	preprocessor	is	integrated	into	the	compiler	driver.
1</p>
<p>Figure	
7.2	
Static	linking.
The	linker	combines	relocatable	object	files	to	form	an	executable	object
file	
.
Next,	the	driver	runs	the	C	compiler	(
),	which	translates	
into
an	ASCII	assembly-language	file	
:
Then,	the	driver	runs	the	assembler	(as),	which	translates	
into	a
binary	
relocatable	object	file</p>
<p>The	driver	goes	through	the	same	process	to	generate	
Finally,	it
runs	the	linker	program	
,	which	combines	
and	
,	along</p>
<p>with	the	necessary	system	object	files,	to	create	the	binary	
executable
object	file</p>
<p>:
To	run	the	executable	
,	we	type	its	name	on	the	Linux	shell's
command	line:
The	shell	invokes	a	function	in	the	operating	system	called	the	
loader
,
which	copies	the	code	and	data	in	the	executable	file	
into	memory,
and	then	transfers	control	to	the	beginning	of	the	program.</p>
<p>7.2	
Static	Linking
Static	linkers
such	as	the	Linux	
LD</p>
<p>program	take	as	input	a	collection	of
relocatable	object	files	and	command-line	arguments	and	generate	as
output	a	fully	linked	executable	object	file	that	can	be	loaded	and	run.
The	input	relocatable	object	files	consist	of	various	code	and	data
sections,	where	each	section	is	a	contiguous	sequence	of	bytes.
Instructions	are	in	one	section,	initialized	global	variables	are	in	another
section,	and	uninitialized	variables	are	in	yet	another	section.
To	build	the	executable,	the	linker	must	perform	two	main	tasks:
Step	</p>
<ol>
<li></li>
</ol>
<p>Symbol	resolution.	
Object	files	define	and	reference
symbols
,	where	each	symbol	corresponds	to	a	function,	a	global
variable,	or	a	
static	variable
(i.e.,	any	C	variable	declared	with	the
attribute).	The	purpose	of	symbol	resolution	is	to	associate
each	symbol	
reference
with	exactly	one	symbol	
definition
.
Step	
2.	
Relocation.	
Compilers	and	assemblers	generate	code
and	data	sections	that	start	at	address	0.	The	linker	
relocates
these	sections	by	associating	a	memory	location	with	each	symbol
definition,	and	then	modifying	all	of	the	references	to	those
symbols	so	that	they	point	to	this	memory	location.	The	linker
blindly	performs	these	relocations	using	detailed	instructions,
generated	by	the	assembler,	called	
relocation	entries
.
The	sections	that	follow	describe	these	tasks	in	more	detail.	As	you	read,
keep	in	mind	some	basic	facts	about	linkers:	Object	files	are	merely</p>
<p>collections	of	blocks	of	bytes.	Some	of	these	blocks	contain	program
code,	others	contain	program	data,	and	others	contain	data	structures
that	guide	the	linker	and	loader.	A	linker	concatenates	blocks	together,
decides	on	run-time	locations	for	the	concatenated	blocks,	and	modifies
various	locations	within	the	code	and	data	blocks.	Linkers	have	minimal
understanding	of	the	target	machine.	The	compilers	and	assemblers	that
generate	the	object	files	have	already	done	most	of	the	work.</p>
<p>7.3	
Object	Files
Object	files	come	in	three	forms:
Relocatable	object	file.	
Contains	binary	code	and	data	in	a	form	that
can	be	combined	with	other	relocatable	object	files	at	compile	time	to
create	an	executable	object	file.
Executable	object	file.	
Contains	binary	code	and	data	in	a	form	that
can	be	copied	directly	into	memory	and	executed.
Shared	object	file.	
A	special	type	of	relocatable	object	file	that	can
be	loaded	into	memory	and	linked	dynamically,	at	either	load	time	or
run	time.
Compilers	and	assemblers	generate	relocatable	object	files	(including
shared	object	files).	Linkers	generate	executable	object	files.	Technically,
an	
object	module
is	a	sequence	of	bytes,	and	an	
object	file
is	an	object
module	stored	on	disk	in	a	file.	However,	we	will	use	these	terms
interchangeably.
Object	files	are	organized	according	to	specific	
object	file	formats
,	which
vary	from	system	to	system.	The	first	Unix	systems	from	Bell	Labs	used
the	
format.	(To	this	day,	executables	are	still	referred	to	as	
files.)	Windows	uses	the	Portable	Executable	(PE)	format.	Mac	OS-X
uses	the	Mach-O	format.	Modern	x86-64	Linux	and	Unix	systems	use
Executable	and	Linkable	Format	(ELF
).	Although	our	discussion	will</p>
<p>focus	on	ELF,	the	basic	concepts	are	similar,	regardless	of	the	particular
format.
Figure	
7.3	
Typical	ELF	relocatable	object	file.</p>
<p>7.4	
Relocatable	Object	Files
Figure	
7.3
shows	the	format	of	a	typical	ELF	relocatable	object	file.
The	
ELF	header
begins	with	a	16-byte	sequence	that	describes	the	word
size	and	byte	ordering	of	the	system	that	generated	the	file.	The	rest	of
the	ELF	header	contains	information	that	allows	a	linker	to	parse	and
interpret	the	object	file.	This	includes	the	size	of	the	ELF	header,	the
object	file	type	(e.g.,	relocatable,	executable,	or	shared),	the	machine
type	(e.g.,	x86-64),	the	file	offset	of	the	section	header	table,	and	the	size
and	number	of	entries	in	the	section	header	table.	The	locations	and
sizes	of	the	various	sections	are	described	by	the	
section	header	table
,
which	contains	a	fixed-size	entry	for	each	section	in	the	object	file.
Sandwiched	between	the	ELF	header	and	the	section	header	table	are
the	sections	themselves.	A	typical	ELF	relocatable	object	file	contains	the
following	sections:
The	machine	code	of	the	compiled	program.
Read-only	data	such	as	the	format	strings	in	
statements,	and	jump	tables	for	switch	statements.</p>
<p>Initialized
global	and	static	C	variables.	Local	C	variables	are
maintained	at	run	time	on	the	stack	and	do	
not
appear	in	either	the
or	
sections.</p>
<p>Uninitialized
global	and	static	C	variables,	along	with	any	global
or	static	variables	that	are	initialized	to	zero.	This	section	occupies	no</p>
<p>actual	space	in	the	object	file;	it	is	merely	a	placeholder.	Object	file
formats	distinguish	between	initialized	and	uninitialized	variables	for
space	efficiency:	uninitialized	variables	do	not	have	to	occupy	any
actual	disk	space	in	the	object	file.	At	run	time,	these	variables	are
allocated	in	memory	with	an	initial	value	of	zero.
Aside	
Why	is	uninitialized	data	called
?
The	use	of	the	term	
to	denote	uninitialized	data	is
universal.	It	was	originally	an	acronym	for	the	&quot;block	started	by
symbol&quot;	directive	from	the	IBM	704	assembly	language	(circa
1957)	and	the	acronym	has	stuck.	A	simple	way	to	remember
the	difference	between	the	
and	
sections	is	to	think
of	&quot;bss&quot;	as	an	abbreviation	for	&quot;Better	Save	Space!&quot;
A	
symbol	table
with	information	about	functions	and	global
variables	that	are	defined	and	referenced	in	the	program.	Some
programmers	mistakenly	believe	that	a	program	must	be	compiled
with	the	-
option	to	get	symbol	table	information.	In	fact,	every
relocatable	object	file	has	a	symbol	table	in	
(unless	the
programmer	has	specifically	removed	it	with	the	
command).
However,	unlike	the	symbol	table	inside	a	compiler,	the	
symbol	table	does	not	contain	entries	for	local	variables.
A	list	of	locations	in	the	
section	that	will	need	to	be
modified	when	the	linker	combines	this	object	file	with	others.	In
general,	any	instruction	that	calls	an	external	function	or	references	a
global	variable	will	need	to	be	modified.	On	the	other	hand,</p>
<p>instructions	that	call	local	functions	do	not	need	to	be	modified.	Note
that	relocation	information	is	not	needed	in	executable	object	files,
and	is	usually	omitted	unless	the	user	explicitly	instructs	the	linker	to
include	it.
Relocation	information	for	any	global	variables	that	are
referenced	or	defined	by	the	module.	In	general,	any	initialized	global
variable	whose	initial	value	is	the	address	of	a	global	variable	or
externally	defined	function	will	need	to	be	modified.
A	debugging	symbol	table	with	entries	for	local	variables	and
typedefs	defined	in	the	program,	global	variables	defined	and
referenced	in	the	program,	and	the	original	C	source	file.	It	is	only
present	if	the	compiler	driver	is	invoked	with	the	-
option.
A	mapping	between	line	numbers	in	the	original	C	source
program	and	machine	code	instructions	in	the	
section.	It	is	only
present	if	the	compiler	driver	is	invoked	with	the	-
option.
A	string	table	for	the	symbol	tables	in	the	
and	
sections	and	for	the	section	names	in	the	section	headers.	A	string
table	is	a	sequence	of	null-terminated	character	strings.</p>
<p>7.5	
Symbols	and	Symbol	Tables
Each	relocatable	object	module,	
m
,	has	a	symbol	table	that	contains
information	about	the	symbols	that	are	defined	and	referenced	by	
m.
In
the	context	of	a	linker,	there	are	three	different	kinds	of	symbols:
Global	symbols
that	are	defined	by	module	
m
and	that	can	be
referenced	by	other	modules.	Global	linker	symbols	correspond	to
nonstatic
C	functions	and	global	variables.
Global	symbols	that	are	referenced	by	module	
m
but	defined	by	some
other	module.	Such	symbols	are	called	
externals
and	correspond	to
nonstatic	C	functions	and	global	variables	that	are	defined	in	other
modules.
Local	symbols
that	are	defined	and	referenced	exclusively	by	module
m
.These	correspond	to	static	C	functions	and	global	variables	that	are
defined	with	the	static	attribute.	These	symbols	are	visible	anywhere
within	module	
m
,	but	cannot	be	referenced	by	other	modules.
It	is	important	to	realize	that	local	linker	symbols	are	not	the	same	as
local	program	variables.	The	symbol	table	in	
does	not	contain
any	symbols	that	correspond	to	local	nonstatic	program	variables.	These
are	managed	at	run	time	on	the	stack	and	are	not	of	interest	to	the	linker.
Interestingly,	local	procedure	variables	that	are	defined	with	the	C	
attribute	are	not	managed	on	the	stack.	Instead,	the	compiler	allocates
space	in	
or	
for	each	definition	and	creates	a	local	linker</p>
<p>symbol	in	the	symbol	table	with	a	unique	name.	For	example,	suppose	a
pair	of	functions	in	the	same	module	define	a	static	local	variable	
:
In	this	case,	the	compiler	exports	a	pair	of	local	linker	symbols	with
different	names	to	the	assembler.	For	example,	it	might	use	
for	the
definition	in	function	
and	
for	the	definition	in	function	
.
Symbol	tables	are	built	by	assemblers,	using	symbols	exported	by	the
compiler	into	the	assembly-language	
file.	An	ELF	symbol	table	is
contained	in	the	
section.	It	contains	an	array	of	entries.	
Figure
7.4
shows	the	format	of	each	entry.
The	
is	a	byte	offset	into	the	string	table	that	points	to	the	null-
terminated	string	name	of	the	symbol.	The	
is	the	symbol's	address.</p>
<h2>For	relocatable	modules,	the	
is	an	offset	from	the	beginning	of	the
section	where	the	object	is	defined.	For	executable	object	files,	the	
is	an	absolute	run-time	address.	The	
is	the	size	(in	bytes)	of	the
object.	The	
is	usually	either	
or	
.	The	symbol	table	can
also	contain	entries	for	the	individual	sections
New	to	C?	
Hiding	variable	and	function
names	with	
C	programmers	use	the	
attribute	to	hide	variable	and
function	declarations	inside	modules,	much	as	you	would	use
public
and	
private
declarations	in	Java	and	C++.	In	C,	source	files
play	the	role	of	modules.	Any	global	variable	or	function	declared
with	the	
attribute	is	private	to	that	module.	Similarly,	any
global	variable	or	function	declared	without	the	
attribute	is
public	and	can	be	accessed	by	any	other	module.	It	is	good
programming	practice	to	protect	your	variables	and	functions	with
the	
attribute	wherever	possible.</h2>
<p>code/link/elfstructs.c</p>
<hr />
<p>code/link/elfstructs.c
Figure	
7.4	
ELF	symbol	table	entry.
The	type	and	binding	fields	are	4	bits	each.
and	for	the	path	name	of	the	original	source	file.	So	there	are	distinct
types	for	these	objects	as	well.	The	binding	field	indicates	whether	the
symbol	is	local	or	global.
Each	symbol	is	assigned	to	some	section	of	the	object	file,	denoted	by
the	section	field,	which	is	an	index	into	the	section	header	table.	There
are	three	special	pseudosections	that	don't	have	entries	in	the	section
header	table:	ABS	is	for	symbols	that	should	not	be	relocated.	UNDEF	is
for	undefined	symbols—that	is,	symbols	that	are	referenced	in	this	object
module	but	defined	elsewhere.	COMMON	is	for	uninitialized	data	objects
that	are	not	yet	allocated.	For	COMMON	symbols,	the	
field	gives
the	alignment	requirement,	and	
gives	the	minimum	size.	Note	that
these	pseudosections	exist	only	in	relocatable	object	files;	they	do	not
exist	in	executable	object	files.
The	distinction	between	COMMON	and	
is	subtle.	Modern	versions
of	
GCC</p>
<p>assign	symbols	in	relocatable	object	files	to	COMMON	and	
using	the	following	convention:</p>
<p>COMMON
Uninitialized	global	variables
Uninitialized	static	variables,	and	global	or	static	variables	that	are	initialized	to
zero
The	reason	for	this	seemingly	arbitrary	distinction	stems	from	the	way	the
linker	performs	symbol	resolution,	which	we	will	explain	in	
Section	
7.6
.
The	GNU	
program	is	a	handy	tool	for	viewing	the	contents	of
object	files.	For	example,	here	are	the	last	three	symbol	table	entries	for
the	relocatable	object	
,	from	the	example	program	in	
Figure
7.1
.	The	first	eight	entries,	which	are	not	shown,	are	local	symbols	that
the	linker	uses	internally.
In	this	example,	we	see	an	entry	for	the	definition	of	global	symbol	
,
a	24-byte	function	located	at	an	offset	(i.e.,	
)	of	zero	in	the	
section.	This	is	followed	by	the	definition	of	the	global	symbol	
,	an
8-byte	object	located	at	an	offset	of	zero	in	the	
section.	The	last
entry	comes	from	the	reference	to	the	external	symbol	
identifies	each	section	by	an	integer	index.	
denotes	the	
section,	and	
denotes	the	
section.</p>
<h2>Practice	Problem	
7.1	
(solution	page
717
)
This	problem	concerns	the	
and	
modules	from	
Figure
7.5
.	For	each	symbol	that	is	defined	or	referenced	in	
,
indicate	whether	or	not	it	will	have	a	symbol	table	entry	in	the
section	in	module	
.	If	so,	indicate	the	module	that
defines	the	symbol	(
),	the	symbol	type	(local,	global,	or
extern),	and	the	section	(
,	or	COMMON)	it	is
assigned	to	in	the	module.
(a)	m.c</h2>
<h2 id="codelinkmc"><a class="header" href="#codelinkmc">code/link/m.c</a></h2>
<h2>code/link/m.c
(b)	swap.c</h2>
<p>code/link/swap.c</p>
<hr />
<p>code/link/swap.c
Figure	
7.5	
Example	program	for	
Practice	Problem	
7.1
.
Symbol
entry?
Symbol
type
Module	where
defined
Section</p>
<hr />
<hr />
<hr />
<hr />
<hr />
<hr />
<hr />
<hr />
<hr />
<hr />
<hr />
<hr />
<hr />
<hr />
<hr />
<hr />
<hr />
<hr />
<hr />
<hr />
<p>7.6	
Symbol	Resolution
The	linker	resolves	symbol	references	by	associating	each	reference	with
exactly	one	symbol	definition	from	the	symbol	tables	of	its	input
relocatable	object	files.	Symbol	resolution	is	straightforward	for
references	to	local	symbols	that	are	defined	in	the	same	module	as	the
reference.	The	compiler	allows	only	one	definition	of	each	local	symbol
per	module.	The	compiler	also	ensures	that	static	local	variables,	which
get	local	linker	symbols,	have	unique	names.
Resolving	references	to	global	symbols,	however,	is	trickier.	When	the
compiler	encounters	a	symbol	(either	a	variable	or	function	name)	that	is
not	defined	in	the	current	module,	it	assumes	that	it	is	defined	in	some
other	module,	generates	a	linker	symbol	table	entry,	and	leaves	it	for	the
linker	to	handle.	If	the	linker	is	unable	to	find	a	definition	for	the
referenced	symbol	in	any	of	its	input	modules,	it	prints	an	(often	cryptic)
error	message	and	terminates.	For	example,	if	we	try	to	compile	and	link
the	following	source	file	on	a	Linux	machine,</p>
<p>then	the	compiler	runs	without	a	hitch,	but	the	linker	terminates	when	it
cannot	resolve	the	reference	to	
Symbol	resolution	for	global	symbols	is	also	tricky	because	multiple
object	modules	might	define	global	symbols	with	the	same	name.	In	this
case,	the	linker	must	either	flag	an	error	or	somehow	choose	one	of	the
definitions	and	discard	the	rest.	The	approach	adopted	by	Linux	systems
involves	cooperation	between	the	compiler,	assembler,	and	linker	and
can	introduce	some	baffling	bugs	to	the	unwary	programmer.
Aside	
Mangling	of	linker	symbols	in	C++
and	Java
Both	C++	and	Java	allow	overloaded	methods	that	have	the	same
name	in	the	source	code	but	different	parameter	lists.	So	how
does	the	linker	tell	the	difference	between	these	different
overloaded	functions?	Overloaded	functions	in	C++	and	Java
work	because	the	compiler	encodes	each	unique	method	and
parameter	list	combination	into	a	unique	name	for	the	linker.	This
encoding	process	is	called	
mangling
,	and	the	inverse	process	is
known	as	
demangling.</p>
<p>Happily,	C++	and	Java	use	compatible	mangling	schemes.	A
mangled	class	name	consists	of	the	integer	number	of	characters
in	the	name	followed	by	the	original	name.	For	example,	the	class
is	encoded	as	
.	A	method	is	encoded	as	the	original
method	name,	followed	by	__,	followed	by	the	mangled	class
name,	followed	by	single	letter	encodings	of	each	argument.	For
example,	
is	encoded	as	
.	Similar
schemes	are	used	to	mangle	global	variable	and	template	names.
7.6.1	
How	Linkers	Resolve
Duplicate	Symbol	Names
The	input	to	the	linker	is	a	collection	of	relocatable	object	modules.	Each
of	these	modules	defines	a	set	of	symbols,	some	of	which	are	local
(visible	only	to	the	module	that	defines	it),	and	some	of	which	are	global
(visible	to	other	modules).	What	happens	if	multiple	modules	define
global	symbols	with	the	same	name?	Here	is	the	approach	that	Linux
compilation	systems	use.
At	compile	time,	the	compiler	exports	each	global	symbol	to	the
assembler	as	either	
strong
or	
weak
,	and	the	assembler	encodes	this
information	implicitly	in	the	symbol	table	of	the	relocatable	object	file.
Functions	and	initialized	global	variables	get	strong	symbols.	Uninitialized
global	variables	get	weak	symbols.
Given	this	notion	of	strong	and	weak	symbols,	Linux	linkers	use	the
following	rules	for	dealing	with	duplicate	symbol	names:</p>
<p>Rule	1.	Multiple	strong	symbols	with	the	same	name	are	not	allowed.
Rule	2.	Given	a	strong	symbol	and	multiple	weak	symbols	with	the
same	name,	choose	the	strong	symbol.
Rule	3.	Given	multiple	weak	symbols	with	the	same	name,	choose
any	of	the	weak	symbols.
For	example,	suppose	we	attempt	to	compile	and	link	the	following	two	C
modules:
In	this	case,	the	linker	will	generate	an	error	message	because	the	strong
symbol	main	is	defined	multiple	times	(rule	1):</p>
<p>Similarly,	the	linker	will	generate	an	error	message	for	the	following
modules	because	the	strong	symbol	
is	defined	twice	(rule	1):
However,	if	
is	uninitialized	in	one	module,	then	the	linker	will	quietly
choose	the	strong	symbol	defined	in	the	other	(rule	2):</p>
<p>″
″
At	run	time,	function	
changes	the	value	of	
from	15213	to	15212,
which	might	come	as	an	unwelcome	surprise	to	the	author	of	function
!	Notice	that	the	linker	normally	gives	no	indication	that	it	has
detected	multiple	definitions	of	
:</p>
<p>The	same	thing	can	happen	if	there	are	two	weak	definitions	of	
(rule
3):
″
″</p>
<p>The	application	of	rules	2	and	3	can	introduce	some	insidious	run-time
bugs	that	are	incomprehensible	to	the	unwary	programmer,	especially	if
the	duplicate	symbol	definitions	have	different	types.	Consider	the
following	example,	in	which	
is	inadvertently	defined	as	an	
in	one
module	and	a	double	in	another:
″
″</p>
<p>On	an	x86-64/Linux	machine,	
are	8	bytes	and	
are	4	bytes.
On	our	system,	the	address	of	
is	
and	the	address	of	
is
.	Thus,	the	assignment	
=	
in	line	6	of	
will	overwrite
the	memory	locations	for	
and	
(lines	5	and	6	in	
)	with	the
double-precision	floating-point	representation	of	negative	zero!
This	is	a	subtle	and	nasty	bug,	especially	because	it	triggers	only	a
warning	from	the	linker,	and	because	it	typically	manifests	itself	much
later	in	the	execution	of	the	program,	far	away	from	where	the	error
occurred.	In	a	large	system	with	hundreds	of	modules,	a	bug	of	this	kind
is	extremely	hard	to	fix,	especially	because	many	programmers	are	not
aware	of	how	linkers	work,	and	because	they	often	ignore	compiler
warnings.	When	in	doubt,	invoke	the	linker	with	a	flag	such	as	the	
GCC</p>
<pre><code>flag,	which	triggers	an	error	if	it	encounters	multiply-defined
</code></pre>
<p>global	symbols.	Or	use	the	
option,	which	turns	all	warnings	into
errors.
In	
Section	
7.5
,	we	saw	how	the	compiler	assigns	symbols	to
COMMON	and	
using	a	seemingly	arbitrary	convention.	Actually,	this
convention	is	due	to	the	fact	that	in	some	cases	the	linker	allows	multiple
modules	to	define	global	symbols	with	the	same	name.	When	the
compiler	is	translating	some	module	and	encounters	a	weak	global
symbol,	say,	
,	it	does	not	know	if	other	modules	also	define	
,	and	if	so,
it	cannot	predict	which	of	the	multiple	instances	of	
the	linker	might
choose.	So	the	compiler	defers	the	decision	to	the	linker	by	assigning	
to	COMMON.	On	the	other	hand,	if	
is	initialized	to	zero,	then	it	is	a
strong	symbol	(and	thus	must	be	unique	by	rule	2),	so	the	compiler	can
confidently	assign	it	to	
.	Similarly,	static	symbols	are	unique	by
construction,	so	the	compiler	can	confidently	assign	them	to	either	
or	
.
Practice	Problem	
7.2	
(solution	page	
718
)
In	this	problem,	let	
denote	that	the	linker	will
associate	an	arbitrary	reference	to	symbol	
in	module	
to	the
definition	of	
in	module	
.	For	each	example	that	follows,	use
this	notation	to	indicate	how	the	linker	would	resolve	references	to
the	multiply-defined	symbol	in	each	module.	If	there	is	a	link-time
error	(rule	1),	write	&quot;
ERROR
&quot;.	If	the	linker	arbitrarily	chooses	one	of
the	definitions	(rule	3),	write	&quot;
UNKNOWN
&quot;.
A
.	</p>
<p>B
.	
C
.	
7.6.2	
Linking	with	Static	Libraries</p>
<p>So	far,	we	have	assumed	that	the	linker	reads	a	collection	of	relocatable
object	files	and	links	them	together	into	an	output	executable	file.	In
practice,	all	compilation	systems	provide	a	mechanism	for	packaging
related	object	modules	into	a	single	file	called	a	
static	library
,	which	can
then	be	supplied	as	input	to	the	linker.	When	it	builds	the	output
executable,	the	linker	copies	only	the	object	modules	in	the	library	that
are	referenced	by	the	application	program.
Why	do	systems	support	the	notion	of	libraries?	Consider	ISO	C99,
which	defines	an	extensive	collection	of	standard	I/O,	string	manipulation,
and	integer	math	functions	such	as	
,	and
.	They	are	available	
to	every	C	program	in	the	
library.	ISO
C99	also	defines	an	extensive	collection	of	floating-point	math	functions
such	as	
,	and	
in	the	
library.
Consider	the	different	approaches	that	compiler	developers	might	use	to
provide	these	functions	to	users	without	the	benefit	of	static	libraries.	One
approach	would	be	to	have	the	compiler	recognize	calls	to	the	standard
functions	and	to	generate	the	appropriate	code	directly.	Pascal,	which
provides	a	small	set	of	standard	functions,	takes	this	approach,	but	it	is
not	feasible	for	C,	because	of	the	large	number	of	standard	functions
defined	by	the	C	standard.	It	would	add	significant	complexity	to	the
compiler	and	would	require	a	new	compiler	version	each	time	a	function
was	added,	deleted,	or	modified.	To	application	programmers,	however,
this	approach	would	be	quite	convenient	because	the	standard	functions
would	always	be	available.
Another	approach	would	be	to	put	all	of	the	standard	C	functions	in	a
single	relocatable	object	module,	say,	
,	that	application</p>
<p>programmers	could	link	into	their	executables:
This	approach	has	the	advantage	that	it	would	decouple	the
implementation	of	the	standard	functions	from	the	implementation	of	the
compiler,	and	would	still	be	reasonably	convenient	for	programmers.
However,	a	big	disadvantage	is	that	every	executable	file	in	a	system
would	now	contain	a	complete	copy	of	the	collection	of	standard
functions,	which	would	be	extremely	wasteful	of	disk	space.	(On	our
system,	
is	about	5	MB	and	
is	about	2	MB.)	Worse,	each
running	program	would	now	contain	its	own	copy	of	these	functions	in
memory,	which	would	be	extremely	wasteful	of	memory.	Another	big
disadvantage	is	that	any	change	to	any	standard	function,	no	matter	how
small,	would	require	the	library	developer	to	recompile	the	entire	source
file,	a	time-consuming	operation	that	would	complicate	the	development
and	maintenance	of	the	standard	functions.
We	could	address	some	of	these	problems	by	creating	a	separate
relocatable	file	for	each	standard	function	and	storing	them	in	a	well-
known	directory.	However,	this	approach	would	require	application
programmers	to	explicitly	link	the	appropriate	object	modules	into	their
executables,	a	process	that	would	be	error	prone	and	time	consuming:</p>
<h2>The	notion	of	a	static	library	was	developed	to	resolve	the	disadvantages
of	these	various	approaches.	Related	functions	can	be	compiled	into
separate	object	modules	and	then	packaged	in	a	single	static	library	file.
Application	programs	can	then	use	any	of	the	functions	defined	in	the
library	by	specifying	a	single	filename	on	the	command	line.	For	example,
a	program	that	uses	functions	from	the	C	standard	library	and	the	math
library	could	be	compiled	and	linked	with	a	command	of	the	form
(a)</h2>
<p>code/link/addvec.c</p>
<hr />
<h2>code/link/addvec.c
(b)</h2>
<h2 id="codelinkmultvecc"><a class="header" href="#codelinkmultvecc">code/link/multvec.c</a></h2>
<p>code/link/multvec.c
Figure	
7.6	
Member	object	files	in	the	
library.
At	link	time,	the	linker	will	only	copy	the	object	modules	that	are
referenced	by	the	program,	which	reduces	the	size	of	the	executable	on
disk	and	in	memory.	On	the	other	hand,	the	application	programmer	only
needs	to	include	the	names	of	a	few	library	files.	(In	fact,	C	compiler</p>
<p>drivers	always	pass	
to	the	linker,	so	the	reference	to	
mentioned	previously	is	unnecessary.)
On	Linux	systems,	static	libraries	are	stored	on	disk	in	a	particular	file
format	known	as	an	
archive
.	An	archive	is	a	collection	of	concatenated
relocatable	object	files,	with	a	header	that	describes	the	size	and	location
of	each	member	object	file.	Archive	filenames	are	denoted	with	the	
To	make	our	discussion	of	libraries	concrete,	consider	the	pair	of	vector
routines	in	
Figure	
7.6
.	Each	routine,	defined	in	its	own	object	module,
performs	a	vector	operation	on	a	pair	of	input	vectors	and	stores	the
result	in	an	output	vector.	As	a	side	effect,	each	routine	records	the
number	of	times	it	has	been	called	by	incrementing	a	global	variable.
(This	will	be	useful	when	we	explain	the	idea	of	position-independent
code	in	
Section	
7.12
.)
To	create	a	static	library	of	these	functions,	we	would	use	the	
tool	as
follows:
To	use	the	library,	we	might	write	an	application	such	as	
in
Figure	
7.7
,	which	invokes	the	
library	routine.	The	include	(or
header)	file	
defines	the	function	prototypes	for	the	routines	in
,</p>
<h2>To	build	the	executable,	we	would	compile	and	link	the	input	files	
and	
:</h2>
<h2 id="codelinkmain2c"><a class="header" href="#codelinkmain2c">code/link/main2.c</a></h2>
<p>code/link/main2.c
Figure	
7.7	
Example	program	2.
This	program	invokes	a	function	in	the	
library.</p>
<p>Figure	
7.8	
Linking	with	static	libraries.
or	equivalently,
Figure	
7.8
summarizes	the	activity	of	the	linker.	The	
argument
tells	the	compiler	driver	that	the	linker	should	build	a	fully	linked
executable	object	file	that	can	be	loaded	into	memory	and	run	without
any	further	linking	at	load	time.	The	
argument	is	a	shorthand	for
,	and	the	
.	argument	tells	the	linker	to	look	for	
in	the	current	directory.
When	the	linker	runs,	it	determines	that	the	
symbol	defined	by
is	referenced	by	
,	so	it	copies	
into	the
executable.	
Since	the	program	doesn't	reference	any	symbols	defined	by
,	the	linker	does	
not
copy	this	module	into	the	executable.	The
linker	also	copies	the	
module	from	
,	along	with	a	number
of	other	modules	from	the	C	run-time	system.</p>
<p>7.6.3	
How	Linkers	Use	Static
Libraries	to	Resolve	References
While	static	libraries	are	useful,	they	are	also	a	source	of	confusion	to
programmers	because	of	the	way	the	Linux	linker	uses	them	to	resolve
external	references.	During	the	symbol	resolution	phase,	the	linker	scans
the	relocatable	object	files	and	archives	left	to	right	in	the	same
sequential	order	that	they	appear	on	the	compiler	driver's	command	line.
(The	driver	automatically	translates	any	
on	the	command	line
into	
.)	During	this	scan,	the	linker	maintains	a	set	
E
of
relocatable	object	files	that	will	be	merged	to	form	the	executable,	a	set	
U
of	unresolved	symbols	(i.e.,	symbols	referred	to	but	not	yet	defined),	and
a	set	
D
of	symbols	that	have	been	defined	in	previous	input	files.	Initially,
E
,	
U
,	and	
D
are	empty.
For	each	input	file	
f
on	the	command	line,	the	linker	determines	if	
f
is
an	object	file	or	an	archive.	If	
f
is	an	object	file,	the	linker	adds	
f
to	
E
,
updates	
U
and	
D
to	reflect	the	symbol	definitions	and	references	in	
f
,
and	proceeds	to	the	next	input	file.
If	
f
is	an	archive,	the	linker	attempts	to	match	the	unresolved	symbols
in	
U
against	the	symbols	defined	by	the	members	of	the	archive.	If
some	archive	member	
m
defines	a	symbol	that	resolves	a	reference
in	
U
,	then	
m
is	added	to	
E
,	and	the	linker	updates	
U
and	
D
to	reflect
the	symbol	definitions	and	references	in	
m
.	This	process	iterates	over
the	member	object	files	in	the	archive	until	a	fixed	point	is	reached
where	
U
and	
D
no	longer	change.	At	this	point,	any	member	object</p>
<p>files	not	contained	in	
E
are	simply	discarded	and	the	linker	proceeds
to	the	next	input	file.
If	
U
is	nonempty	when	the	linker	finishes	scanning	the	input	files	on
the	command	line,	it	prints	an	error	and	terminates.	Otherwise,	it
merges	and	relocates	the	object	files	in	
E
to	build	the	output
executable	file.
Unfortunately,	this	algorithm	can	result	in	some	baffling	link-time	errors
because	the	ordering	of	libraries	and	object	files	on	the	command	line	is
significant.	If	the	library	that	defines	a	symbol	appears	on	the	command
line	before	the	object	file	that	references	that	symbol,	then	the	reference
will	not	be	resolved	and	linking	will	fail.	For	example,	consider	the
following:
What	happened?	When	
is	processed,	
U
is	empty,	so	no
member	object	files	from	
are	added	to	
E
.	Thus,	the
reference	to	
is	never	resolved	and	the	linker	emits	an	error
message	and	terminates.
The	general	rule	for	libraries	is	to	place	them	at	the	end	of	the	command
line.	If	the	members	of	the	different	libraries	are	independent,	in	that	no
member	references	a	symbol	defined	by	another	member,	then	the
libraries	can	be	placed	at	the	end	of	the	command	line	in	any	order.	If,	on
the	other	hand,	the	libraries	are	not	independent,	then	they	must	be</p>
<p>ordered	so	that	for	each	symbol	
s
that	is	referenced	externally	by	a
member	of	an	archive,	at	least	one	definition	of	
s
follows	a	reference	to	
s
on	the	command	line.	For	example,	suppose	
calls	functions	in
and	
that	call	functions	in	
.	Then	
and	
must	precede	
on	the	command	line:
Libraries	can	be	repeated	on	the	command	line	if	necessary	to	satisfy	the
dependence	requirements.	For	example,	suppose	
calls	a	function
in	
that	calls	a	function	in	
that	calls	a	function	in	
.
Then	
must	be	repeated	on	the	command	line:
Alternatively,	we	could	combine	
and	
into	a	single	archive.
Practice	Problem	
7.3	
(solution	page	
718
)
Let	
and	
denote	object	modules	or	static	libraries	in	the	current
directory,	and	let	
denote	that	
a
depends	on	
b
,	in	the	sense
that	
b
defines	a	symbol	that	is	referenced	by	
a
.	For	each	of	the
following	scenarios,	show	the	minimal	command	line	(i.e.,	one	with
the	least	number	of	object	file	and	library	arguments)	that	will	allow
the	static	linker	to	resolve	all	symbol	references.</p>
<p>A
.	
B
.	
C
.	</p>
<p>7.7	
Relocation
Once	the	linker	has	completed	the	symbol	resolution	step,	it	has
associated	each	symbol	reference	in	the	code	with	exactly	one	symbol
definition	(i.e.,	a	symbol	table	entry	in	one	of	its	input	object	modules).	At
this	point,	the	linker	knows	the	exact	sizes	of	the	code	and	data	sections
in	its	input	object	modules.	It	is	now	ready	to	begin	the	relocation	step,
where	it	merges	the	input	modules	and	assigns	run-time	addresses	to
each	symbol.	Relocation	consists	of	two	steps:
1
.	
Relocating	sections	and	symbol	definitions.	
In	this	step,	the
linker	merges	all	sections	of	the	same	type	into	a	new	aggregate
section	of	the	same	type.	For	example,	the	
sections	from
the	input	modules	are	all	merged	into	one	section	that	will	become
the	
section	for	the	output	executable	object	
file.	The	linker
then	assigns	run-time	memory	addresses	to	the	new	aggregate
sections,	to	each	section	defined	by	the	input	modules,	and	to
each	symbol	defined	by	the	input	modules.	When	this	step	is
complete,	each	instruction	and	global	variable	in	the	program	has
a	unique	run-time	memory	address.
2
.	
Relocating	symbol	references	within	sections.	
In	this	step,	the
linker	modifies	every	symbol	reference	in	the	bodies	of	the	code
and	data	sections	so	that	they	point	to	the	correct	run-time
addresses.	To	perform	this	step,	the	linker	relies	on	data	structures
in	the	relocatable	object	modules	known	as	relocation	entries,
which	we	describe	next.</p>
<p>7.7.1	
Relocation	Entries
When	an	assembler	generates	an	object	module,	it	does	not	know	where
the	code	and	data	will	ultimately	be	stored	in	memory.	Nor	does	it	know
the	locations	of	any	externally	defined	functions	or	global	variables	that
are	referenced	by	the	module.	So	whenever	the	assembler	encounters	a
reference	to	an	object	whose	ultimate	location	is	unknown,	it	generates	a
relocation	entry
that	tells	the	linker	how	to	modify	the	reference	when	it
merges	the	object	file	into	an	executable.	Relocation	entries	for	code	are
placed	in	
.	Relocation	entries	for	data	are	placed	in	
Figure	
7.9
shows	the	format	of	an	ELF	relocation	entry.	The	
is
the	section	offset	of	the	reference	that	will	need	to	be	modified.	The
identifies	the	symbol	that	the	modified	reference	should	point	to.
The	
tells	the	linker	how	to	modify	the	new	reference.	The	
is
a	signed	constant	that	is	used	by	some	types	of	relocations	to	bias	the
value	of	the	modified	reference.
ELF	defines	32	different	relocation	types,	many	quite	arcane.	We	are
concerned	with	only	the	two	most	basic	relocation	types:
Relocate	a	reference	that	uses	a	32-bit	PC-relative
address.	Recall	from	
Section	
3.6.3
that	a	PC-relative	address	is	an
offset	from	the	current	run-time	value	of	the	program	counter	(PC).
When	the	CPU	executes	an	instruction	using	PC-relative	addressing,
it	forms	the	
effective	address
(e.g.,	the	target	of	the	call	instruction)	by
adding	the	32-bit	value</p>
<hr />
<h2 id="codelinkelfstructsc"><a class="header" href="#codelinkelfstructsc">code/link/elfstructs.c</a></h2>
<p>code/link/elfstructs.c
Figure	
7.9	
ELF	relocation	entry.
Each	entry	identifies	a	reference	that	must	be	relocated	and	specifies
how	to	compute	the	modified	reference.
encoded	in	the	instruction	to	the	current	run-time	value	of	the	PC,
which	is	always	the	address	of	the	next	instruction	in	memory.
.	Relocate	a	reference	that	uses	a	32-bit	absolute
address.	With	absolute	addressing,	the	CPU	directly	uses	the	32-bit
value	encoded	in	the	instruction	as	the	effective	address,	without
further	modifications.
These	two	relocation	types	support	the	x86-64	
small	code	model
,	which
assumes	that	the	total	size	of	the	code	and	data	in	the	executable	object
file	is	smaller	than	2	GB,	and	thus	can	be	accessed	at	run-time	using	32-
bit	PC-relative	addresses.	The	small	code	model	is	the	default	for	
.</p>
<p>Programs	larger	than	2	GB	can	be	compiled	using	the	
(
medium	code	model
)	and	
(
large	code	model
)	flags,	but
we	won't	discuss	those.
7.7.2	
Relocating	Symbol	References
Figure	
7.10
shows	the	pseudocode	for	the	linker's	relocation
algorithm.	Lines	1	and	2	iterate	over	each	section	
and	each	relocation
entry	
associated	with	each	section.	For	concreteness,	assume	that
each	section	
is	an	array	of	bytes	and	that	each	relocation	entry	
is	a
of	type	
,	as	defined	in	
Figure	
7.9
.	Also,	assume	that
when	the	algorithm	runs,	the	linker	has	already	chosen	runtime
addresses	for	each	section	(denoted	
)	and	each	symbol	(denoted
).	Line	3	computes	the	address	in	the	
array	of	the	4-byte
reference	that	needs	to	be	relocated.	If	this	reference	uses	PC-relative
addressing,	then	it	is	relocated	by	lines	5−9.	If	the	reference	uses
absolute	addressing,	then	it	is	relocated	by	lines	11−13.</p>
<h2>Figure	
7.10	
Relocation	algorithm.</h2>
<p>code/link/main-relo.d</p>
<hr />
<p>code/link/main-relo.d
Figure	
7.11	
Code	and	relocation	entries	from	
The	original	C	code	is	in	
Figure	
7.1
.
Let's	see	how	the	linker	uses	this	algorithm	to	relocate	the	references	in
our	example	program	in	
Figure	
7.1
.	
Figure	
7.11
shows	the
disassembled	code	from	
,	as	generated	by	the	GNU	
OBJDUMP</p>
<p>tool
(
).
The	main	function	references	two	global	symbols,	
and	
.	For
each	reference,	the	assembler	has	generated	a	relocation	entry,	which	is
displayed	on	the	following	line.
The	relocation	entries	tell	the	linker	that
the	reference	to	
should	be	relocated	using	a	32-bit	PC-relative
address,	and	the	reference	to	
should	be	relocated	using	a	32-bit
absolute	address.	The	next	two	sections	detail	how	the	linker	relocates
these	references.
2.	
Recall	that	relocation	entries	and	instructions	are	actually	stored	in	different	sections	of	the
object	file.	The	
tool	displays	them	together	for	convenience.
Relocating	PC-Relative	References
2</p>
<p>In	line	6	in	
Figure	
7.11
,	function	
calls	the	
function,	which	is
defined	in	module	
.	The	
instruction	begins	at	section	offset
and	consists	of	the	1-byte	opcode	
,	followed	by	a	placeholder
for	the	32-bit	PC-relative	reference	to	the	target	
.
The	corresponding	relocation	entry	
consists	of	four	fields:
These	fields	tell	the	linker	to	modify	the	32-bit	PC-relative	reference
starting	at	offset	
so	that	it	will	point	to	the	
routine	at	run	time.
Now,	suppose	that	the	linker	has	determined	that
and</p>
<p>Using	the	algorithm	in	
Figure	
7.10
,	the	linker	first	computes	the	run-
time	address	of	the	reference	(line	7):
It	then	updates	the	reference	so	that	it	will	point	to	the	sum	routine	at	run
time	(line	8):
In	the	resulting	executable	object	file,	the	call	instruction	has	the	following
relocated	form:
At	run	time,	the	call	instruction	will	be	located	at	address	
.	When
the	CPU	executes	the	call	instruction,	the	PC	has	a	value	of	
,
which	is	the	address	of	the	instruction	immediately	following	the	</p>
<p>instruction.	To	execute	the	call	instruction,	the	CPU	performs	the
following	steps:
1
.	
Push	PC	onto	stack
2
.	
PC	←	PC	+	
Thus,	the	next	instruction	to	execute	is	the	first	instruction	of	the	
routine,	which	of	course	is	what	we	want!
Relocating	Absolute	References
Relocating	absolute	references	is	straightforward.	For	example,	in	line	4
in	
Figure	
7.11
,	the	
instruction	copies	the	address	of	
(a	32-
bit	immediate	value)	into	register	
.	The	
instruction	begins	at
section	offset	
and	consists	of	the	1-byte	opcode	
,	followed	by	a
placeholder	for	the	32-bit	absolute	reference	to	
.
The	corresponding	relocation	entry	
consists	of	four	fields:
These	fields	tell	the	linker	to	modify	the	absolute	reference	starting	at
offset	
so	that	it	will	point	to	the	first	byte	of	
at	run	time.	Now,
suppose	that	the	linker	has	determined	that</p>
<p>(a)	Relocated	
section
(b)	Relocated	.data	section</p>
<p>Figure	
7.12	
Relocated	
and	
sections	for	the	executable
file	
The	original	C	code	is	in	
Figure	
7.1
.
The	linker	updates	the	reference	using	line	13	of	the	algorithm	in	
Figure
7.10
:
In	the	resulting	executable	object	file,	the	reference	has	the	following
relocated	form:
Putting	it	all	together,	
Figure	
7.12
shows	the	relocated	
and
sections	in	the	final	executable	object	file.	At	load	time,	the	loader
can	copy	the	bytes	from	these	sections	directly	into	memory	and	execute
the	instructions	without	any	further	modifications.</p>
<p>Practice	Problem	
7.4	
(solution	page	
718
)
This	problem	concerns	the	relocated	program	in	
Figure	
7.12(a)
.
A
.	
What	is	the	hex	address	of	the	relocated	reference	to	
in
line	5?
B
.	
What	is	the	hex	value	of	the	relocated	reference	to	
in
line	5?
Practice	Problem	
7.5	
(solution	page	
718
)
Consider	the	call	to	function	
in	object	file	
(
Figure	
7.5
).
with	the	following	relocation	entry:
Now	suppose	that	the	linker	relocates	
in	
to	address
and	swap	to	address	
.	Then	what	is	the	value	of
the	relocated	reference	to	
in	the	
instruction?</p>
<h2>7.8	
Executable	Object	Files
We	have	seen	how	the	linker	merges	multiple	object	files	into	a	single
executable	object	file.	Our	example	C	program,	which	began	life	as	a
collection	of	ASCII	text	files,	has	been	transformed	into	a	single	binary
file	that	contains	all	of	the	information	needed	to	load	the	program	into
memory	and	run	it.	
Figure	
7.13
summarizes	the	kinds	of	information	in
a	typical	ELF	executable	file.
Figure	
7.13	
Typical	ELF	executable	object	file.</h2>
<p>code/link/prog-exe.d</p>
<hr />
<p>code/link/prog-exe.d
Figure	
7.14	
Program	header	table	for	the	example	executable	
:	offset	in	object	file;	
:	memory	address;	
:	alignment
requirement;	
:	segment	size	in	object	file;	
:	segment	size	in
memory;	
:	run-time	permissions.
The	format	of	an	executable	object	file	is	similar	to	that	of	a	relocatable
object	file.	The	ELF	header	describes	the	overall	format	of	the	file.	It	also
includes	the	program's	
entry	point
,	which	is	the	address	of	the	first
instruction	to	execute	when	the	program	runs.	The	
,	and
sections	are	similar	to	those	in	a	relocatable	object	file,	except	that
these	sections	have	been	relocated	to	their	eventual	run-time	memory
addresses.	The	
section	defines	a	small	function,	called	
,	that
will	be	called	by	the	program's	initialization	code.	Since	the	executable	is
fully	linked
(relocated),	it	needs	no	
sections.
ELF	executables	are	designed	to	be	easy	to	load	into	memory,	with
contiguous	chunks	of	the	executable	file	mapped	to	contiguous	memory
segments.	This	mapping	is	described	by	the	
program	header	table
.</p>
<p>Figure	
7.14
shows	part	of	the	program	header	table	for	our	example
executable	
,	as	displayed	by	
.
From	the	program	header	table,	we	see	that	two	memory	segments	will
be	initialized	with	the	contents	of	the	executable	object	file.	Lines	1	and	2
tell	us	that	the	first	segment	(the	
code	segment
)	has	read/execute
permissions,	starts	at	memory	address	
,	has	a	total	size	in
memory	of	
bytes,	and	is	initialized	with	the	first	
bytes	of	the
executable	object	file,	which	includes	the	ELF	header,	the	program
header	table,	and	the	
,	and	
sections.
Lines	3	and	4	tell	us	that	the	second	segment	(the	
data	segment
)	has
read/write	permissions,	starts	at	memory	address	
,	has	a	total
memory	size	of	
bytes,	and	is	initialized	with	the	
bytes	in	the
.data	section	starting	at	offset	
in	the	object	file.	The	remaining	8
bytes	in	the	segment	correspond	to	
data	that	will	be	initialized	to
zero	at	run	time.
For	any	segment	
s
,	the	linker	must	choose	a	starting	address,	
,
such	that
where	
is	the	offset	of	the	segment's	first	section	in	the	object	file,	and
is	the	alignment	specified	in	the	program	header	(2
=	
).
For	example,	in	the	data	segment	in	
Figure	
7.14
,
21</p>
<p>and
This	alignment	requirement	is	an	optimization	that	enables	segments	in
the	object	file	to	be	transferred	efficiently	to	memory	when	the	program
executes.	The	reason	is	somewhat	subtle	and	is	due	to	the	way	that
virtual	memory	is	organized	as	large	contiguous	power-of-2	chunks	of
bytes.	You	will	learn	all	about	virtual	memory	in	
Chapter	
9
.</p>
<p>7.9	
Loading	Executable	Object	Files
To	run	an	executable	object	file	
,	we	can	type	its	name	to	the	Linux
shell's	command	line:
Since	
does	not	correspond	to	a	built-in	shell	command,	the	shell
assumes	that	
is	an	executable	object	file,	which	it	runs	for	us	by
invoking	some	memory-resident	operating	system	code	known	as	the
.	Any	Linux	program	can	invoke	the	loader	by	calling	the	
function,	which	we	will	describe	in	detail	in	
Section	
8.4.6
.	The	loader
copies	the	code	and	data	in	the	executable	object	file	from	disk	into
memory	and	then	runs	the	program	by	jumping	to	its	first	instruction,	or
entry	point
.	This	process	of	copying	the	program	into	memory	and	then
running	it	is	known	as	
loading
.
Every	running	Linux	program	has	a	run-time	memory	image	similar	to	the
one	in	
Figure	
7.15
.	On	Linux	x86-64	systems,	the	code	segment	starts
at	address	
,	followed	by	the	data	segment.	The	run-time	
heap
follows	the	data	segment	and	grows	upward	via	calls	to	the	
library.(We	will	describe	
and	the	heap	in	detail	in	
Section	
9.9
.)
This	is	followed	by	a	region	that	is	reserved	for	shared	modules.	The	user
stack	starts	below	the	largest	legal	user	address	(2
-	1)	and	grows
down,	toward	smaller	memory	addresses.	The	region	above	the	stack,
48
48</p>
<p>starting	at	address	2
,	is	reserved	for	the	code	and	data	in	the	
kernel
,
which	is	the	memory-resident	part	of	the	operating	system.
For	simplicity,	we've	drawn	the	heap,	data,	and	code	segments	as
abutting	each	other,	and	we've	placed	the	top	of	the	stack	at	the	largest
legal	user	address.	In	practice,	there	is	a	gap	between	the	code	and	data
segments	due	to	the	alignment	requirement	on	the	
segment
(
Section	
7.8
).	Also,	the	linker	uses	address-space	layout
randomization	(ASLR,	
Section	
3.10.4
)	when	it	assigns	runtime
addresses	to	the	stack,	shared	library,	and	heap	segments.	Even	though
the	locations	of	these	regions	change	each	time	the	program	is	run,	their
relative	positions	are	the	same.
When	the	loader	runs,	it	creates	a	memory	image	similar	to	the	one
shown	in	
Figure	
7.15
.	Guided	by	the	program	header	table,	it	copies
chunks	of	the
Figure	
7.15	
Linux	x86-64	run-time	memory	image.
48</p>
<p>Gaps	due	to	segment	alignment	requirements	and	address-space	layout
randomization	(ASLR)	are	not	shown.	Not	to	scale.
executable	object	file	into	the	code	and	data	segments.	Next,	the	loader
jumps	to	the	program's	entry	point,	which	is	always	the	address	of	the
function.	This	function	is	defined	in	the	system	object	file	
and	is	the	same	for	all	C	programs.	The	
function	calls	the	
system
startup	function
,	
,	which	is	defined	in	
.	It
initializes	the	execution	environment,	calls	the	user-level	
function,
handles	its	return	value,	and	if	necessary	returns	control	to	the	kernel.</p>
<p>7.10	
Dynamic	Linking	with	Shared
Libraries
The	static	libraries	that	we	studied	in	
Section	
7.6.2
address	many	of
the	issues	associated	with	making	large	collections	of	related	functions
available	to	application	programs.	However,	static	libraries	still	have
some	significant	disadvantages.	Static	libraries,	like	all	software,	need	to
be	maintained	and	updated	periodically.	If	application	programmers	want
to	use	the	most	recent	version	of	a	library,	they	must	somehow	become
aware	that	the	library	has	changed	and	then	explicitly	relink	their
programs	against	the	updated	library.
Another	issue	is	that	almost	every	C	program	uses	standard	I/O	functions
such	as	
and	
.	At	run	time,	the	code	for	these	functions	is
duplicated	in	the	text	segment	of	each	running	process.	On	a	typical
system	that	is	running	hundreds	of	processes,	this	can	be	a	significant
waste	of	scarce	memory	system	resources.	(An	interesting	property	of
memory	is	that	it	is	
always
a	scarce	resource,	regardless
Aside	
How	do	loaders	really	work?
Our	description	of	loading	is	conceptually	correct	but	intentionally
not	entirely	accurate.	To	understand	how	loading	really	works,	you
must	understand	the	concepts	of	
processes
,	
virtual	memory
,	and
memory	mapping
,	which	we	haven't	discussed	yet.	As	we</p>
<p>encounter	these	concepts	later	in	
Chapters	
8
and	
9
,	we	will
revisit	loading	and	gradually	reveal	the	mystery	to	you.
For	the	impatient	reader,	here	is	a	preview	of	how	loading	really
works:	Each	program	in	a	Linux	system	runs	in	the	context	of	a
process	with	its	own	virtual	address	space.	When	the	shell	runs	a
program,	the	parent	shell	process	forks	a	child	process	that	is	a
duplicate	of	the	parent.	The	child	process	invokes	the	loader	via
the	
system	call.	The	loader	deletes	the	child's	existing
virtual	memory	segments	and	creates	a	new	set	of	code,	data,
heap,	and	stack	segments.	The	new	stack	and	heap	segments
are	initialized	to	zero.	The	new	code	and	data	segments	are
initialized	to	the	contents	of	the	executable	file	by	mapping	pages
in	the	virtual	address	space	to	page-size	chunks	of	the	executable
file.	Finally,	the	loader	jumps	to	the	
address,	which
eventually	calls	the	application's	
routine.	Aside	from	some
header	information,	there	is	no	copying	of	data	from	disk	to
memory	during	loading.	The	copying	is	deferred	until	the	CPU
references	a	mapped	virtual	page,	at	which	point	the	operating
system	automatically	transfers	the	page	from	disk	to	memory
using	its	paging	mechanism.
of	how	much	there	is	in	a	system.	Disk	space	and	kitchen	trash	cans
share	this	same	property.)
Shared	libraries
are	modern	innovations	that	address	the	disadvantages
of	static	libraries.	A	shared	library	is	an	object	module	that,	at	either	run
time	or	load	time,	can	be	loaded	at	an	arbitrary	memory	address	and
linked	with	a	program	in	memory.	This	process	is	known	as	
dynamic
linking
and	is	performed	by	a	program	called	a	
dynamic	linker
.	Shared
libraries	are	also	referred	to	as	
shared	objects
,	and	on	Linux	systems</p>
<p>they	are	indicated	by	the	
suffix.	Microsoft	operating	systems	make
heavy	use	of	shared	libraries,	which	they	refer	to	as	DLLs	(dynamic	link
libraries).
Shared	libraries	are	&quot;shared&quot;	in	two	different	ways.	First,	in	any	given	file
system,	there	is	exactly	one	
file	for	a	particular	library.	The	code	and
data	in	this	
file	are	shared	by	all	of	the	executable	object	files	that
reference	the	library,	as	opposed	to	the	contents	of	static	libraries,	which
are	copied	and	embedded	in	the	executables	that	reference	them.
Second,	a	single	copy	of	the	
section	of	a	shared	library	in	memory
can	be	shared	by	different	running	processes.	We	will	explore	this	in
more	detail	when	we	study	virtual	memory	in	
Chapter	
9
.
Figure	
7.16
summarizes	the	dynamic	linking	process	for	the	example
program	in	
Figure	
7.7
.	To	build	a	shared	library	
of	our
example	vector	routines	in	
Figure	
7.6
,	we	invoke	the	compiler	driver
with	some	special	directives	to	the	compiler	and	linker:
The	
flag	directs	the	compiler	to	generate	
position-independent
code
(more	on	this	in	the	next	section).	The	
flag	directs	the	linker
to	create	a	shared</p>
<p>Figure	
7.16	
Dynamic	linking	with	shared	libraries.
object	file.	Once	we	have	created	the	library,	we	would	then	link	it	into	our
example	program	in	
Figure	
7.7
:
This	creates	an	executable	object	file	
in	a	form	that	can	be	linked
with	
at	run	time.	The	basic	idea	is	to	do	some	of	the	linking
statically	when	the	executable	file	is	created,	and	then	complete	the
linking	process	dynamically	when	the	program	is	loaded.	It	is	important	to
realize	that	none	of	the	code	or	data	sections	from	
are
actually	copied	into	the	executable	
at	this	point.	Instead,	the	linker
copies	some	relocation	and	symbol	table	information	that	will	allow
references	to	code	and	data	in	
to	be	resolved	at	load	time.
When	the	loader	loads	and	runs	the	executable	
,	it	loads	the
partially	linked	executable	
,	using	the	techniques	discussed	in</p>
<p>Section	
7.9
.	Next,	it	notices	that	
contains	a	
section,
which	contains	the	path	name	of	the	dynamic	linker,	which	is	itself	a
shared	object	(e.g.,	
on	Linux	systems).	Instead	of	passing
control	to	the	application,	as	it	would	normally	do,	the	loader	loads	and
runs	the	dynamic	linker.	The	dynamic	linker	then	finishes	the	linking	task
by	performing	the	following	relocations:
Relocating	the	text	and	data	of	
into	some	memory	segment
Relocating	the	text	and	data	of	
into	another	memory
segment
Relocating	any	references	in	
to	symbols	defined	by	
and	
Finally,	the	dynamic	linker	passes	control	to	the	application.	From	this
point	on,	the	locations	of	the	shared	libraries	are	fixed	and	do	not	change
during	execution	of	the	program.</p>
<p>7.11	
Loading	and	Linking	Shared
Libraries	from	Applications
Up	to	this	point,	we	have	discussed	the	scenario	in	which	the	dynamic
linker	loads	and	links	shared	libraries	when	an	application	is	loaded,	just
before	it	executes.	However,	it	is	also	possible	for	an	application	to
request	the	dynamic	linker	to	load	and	link	arbitrary	shared	libraries	while
the	application	is	running,	without	having	to	link	in	the	applications
against	those	libraries	at	compile	time.
Dynamic	linking	is	a	powerful	and	useful	technique.	Here	are	some
examples	in	the	real	world:
Distributing	software.	
Developers	of	Microsoft	Windows	applications
frequently	use	shared	libraries	to	distribute	software	updates.	They
generate	a	new	copy	of	a	shared	library,	which	users	can	then
download	and	use	as	a	replacement	for	the	current	version.	The	next
time	they	run	their	application,	it	will	automatically	link	and	load	the
new	shared	library.
Building	high-performance	Web	servers.	
Many	Web	servers
generate	
dynamic	content
,	such	as	personalized	Web	pages,	account
balances,	and	banner	ads.	Early	Web	servers	generated	dynamic
content	by	using	
and	
to	create	a	child	process	and	run	a
&quot;CGI	program&quot;	in	the	context	of	the	child.	However,	modern	high-
performance	Web	servers	can	generate	dynamic	content	using	a
more	efficient	and	sophisticated	approach	based	on	dynamic	linking.</p>
<p>The	idea	is	to	package	each	function	that	generates	dynamic	content
in	a	shared	library.	When	a	request	arrives	from	a	Web	browser,	the
server	dynamically	loads	and	links	the	appropriate	function	and	then
calls	it	directly,	as	opposed	to	using	
and	
to	run	the
function	in	the	context	of	a	child	process.	The	function	remains
cached	in	the	server's	address	space,	so	subsequent	requests	can	be
handled	at	the	cost	of	a	simple	function	call.	This	can	have	a
significant	impact	on	the	throughput	of	a	busy	site.	Further,	existing
functions	can	be	updated	and	new	functions	can	be	added	at	run
time,	without	stopping	the	server.
Linux	systems	provide	a	simple	interface	to	the	dynamic	linker	that	allows
application	programs	to	load	and	link	shared	libraries	at	run	time.
The	
function	loads	and	links	the	shared	library	
.	The
external	symbols	in	
are	resolved	using	libraries	previously
opened	with	the	
flag.	If	the	current	executable	was	compiled
with	the	
flag,	then	its	global	symbols	are	also	available	for
symbol	resolution.	The	
argument	must	include	either	
,
which	tells	the	linker	to	resolve	references	to	external	symbols
immediately,	or	the	
flag,	which	instructs	the	linker	to	defer</p>
<p>symbol	resolution	until	code	from	the	library	is	executed.	Either	of	these
values	can	be	
OR
ed	with	the	
flag.
The	
function	takes	a	handle	to	a	previously	opened	shared	library
and	a	
name	and	returns	the	address	of	the	symbol,	if	it	exists,	or
NULL	otherwise.
The	
function	unloads	the	shared	library	if	no	other	shared
libraries	are	still	using	it.</p>
<p>The	
function	returns	a	string	describing	the	most	recent	error
that	occurred	as	a	result	of	calling	
,	or	
,	or	NULL	if
no	error	occurred.
Figure	
7.17
shows	how	we	would	use	this	interface	to	dynamically	link
our	
shared	library	at	run	time	and	then	invoke	its	
routine.	To	compile	the	program,	we	would	invoke	
GCC</p>
<h2>in	the	following
way:</h2>
<p>code/link/dll.c</p>
<hr />
<p>code/link/dll.c
Figure	
7.17	
Example	program	3.
Dynamically	loads	and	links	the	shared	library	
at	run	time.
Aside	
Shared	libraries	and	the	Java
Native	Interface
Java	defines	a	standard	calling	convention	called	
Java	Native
Interface	(JNI)
that	allows	&quot;native&quot;	C	and	C++	functions	to	be
called	from	Java	programs.	The	basic	idea	of	JNI	is	to	compile	the
native	C	function,	say,	
,	into	a	shared	library,	say,	
.
When	a	running	Java	program	attempts	to	invoke	function	
,
the	Java	interpreter	uses	the	
interface	(or	something	like	it)
to	dynamically	link	and	load	
and	then	call	
.</p>
<p>7.12	
Position-Independent	Code
(PIC)
A	key	purpose	of	shared	libraries	is	to	allow	multiple	running	processes	to
share	the	same	library	code	in	memory	and	thus	save	precious	memory
resources.	So	how	can	multiple	processes	share	a	single	copy	of	a
program?	One	approach	would	be	to	assign	a	priori	a	dedicated	chunk	of
the	address	space	to	each	shared	library,	and	then	require	the	loader	to
always	load	the	shared	library	at	that	address.	While	straightforward,	this
approach	creates	some	serious	problems.	It	would	be	an	inefficient	use
of	the	address	space	because	portions	of	the	space	would	be	allocated
even	if	a	process	didn't	use	the	library.	It	would	also	be	difficult	to
manage.	We	would	have	to	ensure	that	none	of	the	chunks	overlapped.
Each	time	a	library	was	modified,	we	would	have	to	make	sure	that	it	still
fit	in	its	assigned	chunk.	If	not,	then	we	would	have	to	find	a	new	chunk.
And	if	we	created	a	new	library,	we	would	have	to	find	room	for	it.	Over
time,	given	the	hundreds	of	libraries	and	versions	of	libraries	in	a	system,
it	would	be	difficult	to	keep	the	address	space	from	fragmenting	into	lots
of	small	unused	but	unusable	holes.	Even	worse,	the	assignment	of
libraries	to	memory	would	be	different	for	each	system,	thus	creating
even	more	management	headaches.
To	avoid	these	problems,	modern	systems	compile	the	code	segments	of
shared	modules	so	that	they	can	be	loaded	anywhere	in	memory	without
having	to	be	modified	by	the	linker.	With	this	approach,	a	single	copy	of	a
shared	module's	code	segment	can	be	shared	by	an	unlimited	number	of</p>
<p>processes.	(Of	course,	each	process	will	still	get	its	own	copy	of	the
read/write	data	segment.)
Code	that	can	be	loaded	without	needing	any	relocations	is	known	as
position-independent	code	(PIC).
Users	direct	GNU	compilation	systems
to	generate	PIC	code	with	the	
option	to	
GCC
.	
Shared	libraries	must
always	be	compiled	with	this	option.
On	x86-64	systems,	references	to	symbols	in	the	same	executable	object
module	require	no	special	treatment	to	be	PIC.	These	references	can	be
compiled	using	PC-relative	addressing	and	relocated	by	the	static	linker
when	it	builds	the	object	file.	However,	references	to	external	procedures
and	global	variables	that	are	defined	by	shared	modules	require	some
special	techniques,	which	we	describe	next.
PIC	Data	References
Compilers	generate	PIC	references	to	global	variables	by	exploiting	the
following	interesting	fact:	no	matter	where	we	load	an	object	module
(including	shared
Figure	
7.18	
Using	the	GOT	to	reference	a	global	variable.</p>
<p>The	
routine	in	
references	
indirectly	through
the	GOT	for	
.
object	modules)	in	memory,	the	data	segment	is	always	the	same
distance	from	the	code	segment.	Thus,	the	
distance
between	any
instruction	in	the	code	segment	and	any	variable	in	the	data	segment	is	a
run-time	constant,	independent	of	the	absolute	memory	locations	of	the
code	and	data	segments.
Compilers	that	want	to	generate	PIC	references	to	global	variables
exploit	this	fact	by	creating	a	table	called	the	
global	offset	table	(GOT)
at
the	beginning	of	the	data	segment.	The	GOT	contains	an	8-byte	entry	for
each	global	data	object	(procedure	or	global	variable)	that	is	referenced
by	the	object	module.	The	compiler	also	generates	a	relocation	record	for
each	entry	in	the	GOT.	At	load	time,	the	dynamic	linker	relocates	each
GOT	entry	so	that	it	contains	the	absolute	address	of	the	object.	Each
object	module	that	references	global	objects	has	its	own	GOT.
Figure	
7.18
shows	the	GOT	from	our	example	
shared
module.	The	
routine	loads	the	address	of	the	global	variable
indirectly	via	GOT[3]	and	then	increments	
in	memory.	The
key	idea	here	is	that	the	offset	in	the	PC-relative	reference	to	
is	a
run-time	constant.
Since	
is	defined	by	the	
module,	the	compiler	could
have	exploited	the	constant	distance	between	the	code	and	data
segments	by	generating	a	direct	PC-relative	reference	to	
and
adding	a	relocation	for	the	linker	to	resolve	when	it	builds	the	shared
module.	However,	if	
were	defined	by	another	shared	module,	then</p>
<p>the	indirect	access	through	the	GOT	would	be	necessary.	In	this	case,
the	compiler	has	chosen	to	use	the	most	general	solution,	the	GOT,	for
all	references.
PIC	Function	Calls
Suppose	that	a	program	calls	a	function	that	is	defined	by	a	shared
library.	The	compiler	has	no	way	of	predicting	the	run-time	address	of	the
function,	since	the	shared	module	that	defines	it	could	be	loaded
anywhere	at	run	time.	The	normal	approach	would	be	to	generate	a
relocation	record	for	the	reference,	which	
the	dynamic	linker	could	then
resolve	when	the	program	was	loaded.	However,	this	approach	would	not
be	PIC,	since	it	would	require	the	linker	to	modify	the	code	segment	of
the	calling	module.	GNU	compilation	systems	solve	this	problem	using	an
interesting	technique,	called	
lazy	binding
,	that	defers	the	binding	of	each
procedure	address	until	the	
first	time
the	procedure	is	called.
The	motivation	for	lazy	binding	is	that	a	typical	application	program	will
call	only	a	handful	of	the	hundreds	or	thousands	of	functions	exported	by
a	shared	library	such	as	
.	By	deferring	the	resolution	of	a
function's	address	until	it	is	actually	called,	the	dynamic	linker	can	avoid
hundreds	or	thousands	of	unnecessary	relocations	at	load	time.	There	is
a	nontrivial	run-time	overhead	the	first	time	the	function	is	called,	but
each	call	thereafter	costs	only	a	single	instruction	and	a	memory
reference	for	the	indirection.
Lazy	binding	is	implemented	with	a	compact	yet	somewhat	complex
interaction	between	two	data	structures:	the	GOT	and	the	
procedure
linkage	table	(PLT)
.	If	an	object	module	calls	any	functions	that	are</p>
<p>defined	in	shared	libraries,	then	it	has	its	own	GOT	and	PLT.	The	GOT	is
part	of	the	data	segment.	The	PLT	is	part	of	the	code	segment.
Figure	
7.19
shows	how	the	PLT	and	GOT	work	together	to	resolve	the
address	of	a	function	at	run	time.	First,	let's	examine	the	contents	of	each
of	these	tables.
Procedure	linkage	table	(PLT).	
The	PLT	is	an	array	of	16-byte	code
entries.	
is	a	special	entry	that	jumps	into	the	dynamic	linker.
Each	shared	library	function	called	by	the	executable	has	its	own	PLT
entry.	Each	of
Figure	
7.19	
Using	the	PLT	and	GOT	to	call	external	functions.
The	dynamic	linker	resolves	the	address	of	
the	first	time	it	is
called.</p>
<p>these	entries	is	responsible	for	invoking	a	specific	function.	
(not	shown	here)	invokes	the	system	startup	function
(
),	which	initializes	the	execution	environment,	calls
the	main	function,	and	handles	its	return	value.	Entries	starting	at
invoke	functions	called	by	the	user	code.	In	our	example,
invokes	
and	
(not	shown)	invokes	
.
Global	offset	table	(GOT).	
As	we	have	seen,	the	GOT	is	an	array	of
8-byte	address	entries.	When	used	in	conjunction	with	the	PLT,
and	
contain	information	that	the	dynamic	linker	uses
when	it	resolves	function	addresses.	
is	the	entry	point	for	the
dynamic	linker	in	the	
module.	Each	of	the	remaining
entries	corresponds	to	a	called	function	whose	address	needs	to	be
resolved	at	run	time.	Each	has	a	matching	PLT	entry.	For	example,
and	
correspond	to	
.	Initially,	each	GOT	entry
points	to	the	second	instruction	in	the	corresponding	PLT	entry.
Figure	
7.19(a)
shows	how	the	GOT	and	PLT	work	together	to	lazily
resolve	the	run-time	address	of	function	
the	first	time	it	is	called:
Step	</p>
<ol>
<li></li>
</ol>
<p>Instead	of	directly	calling	
,	the	program	calls	into
,	which	is	the	PLT	entry	for	
.
Step	
2.	
The	first	PLT	instruction	does	an	indirect	jump	through
.	Since	each	GOT	entry	initially	points	to	the	second
instruction	in	its	corresponding	PLT	entry,	the	indirect	jump	simply
transfers	control	back	to	the	next	instruction	in	
.
Step	
3.	
After	pushing	an	ID	for	
onto	the	stack,	
jumps	to	
.</p>
<p>Step	
4.	
pushes	an	argument	for	the	dynamic	linker
indirectly	through	
and	then	jumps	into	the	dynamic	linker
indirectly	through	
.	The	dynamic	linker	uses	the	two	stack
entries	to	determine	the	runtime	location	of	
,	overwrites
with	this	address,	and	passes	control	to	
.
Figure	
7.19(b)
shows	the	control	flow	for	any	subsequent	invocations
of	
:
Step	</p>
<ol>
<li></li>
</ol>
<p>Control	passes	to	
as	before.
Step	
2.	
However,	this	time	the	indirect	jump	through	
transfers	control	directly	to	
.</p>
<p>7.13	
Library	Interpositioning
Linux	linkers	support	a	powerful	technique,	called	
library	interpositioning
,
that	allows	you	to	intercept	calls	to	shared	library	functions	and	execute
your	own	code	instead.	Using	interpositioning,	you	could	trace	the
number	of	times	a	particular	
library	function	is	called,	validate	and	trace
its	input	and	output	values,	or	even	replace	it	with	a	completely	different
implementation.
Here's	the	basic	idea:	Given	some	
target	function
to	be	interposed	on,
you	create	a	
wrapper	function
whose	prototype	is	identical	to	the	target
function.	Using	some	particular	interpositioning	mechanism,	you	then
trick	the	system	into	calling	the	wrapper	function	instead	of	the	target
function.	The	wrapper	function	typically	executes	its	own	logic,	then	calls
the	target	function	and	passes	its	return	value	back	to	the	caller.
Interpositioning	can	occur	at	compile	time,	link	time,	or	run	time	as	the
program	is	being	loaded	and	executed.	To	explore	these	different
mechanisms,	we	will	use	the	example	program	in	
Figure	
7.20(a)
as	a
running	example.	It	calls	the	
and	
functions	from	the	C
standard	library	(
).	The	call	to	
allocates	a	block	of	32
bytes	from	the	heap	and	returns	a	pointer	to	the	block.	The	call	to	
gives	the	block	back	to	the	heap,	for	use	by	subsequent	calls	to	
.
Our	goal	is	to	use	interpositioning	to	trace	the	calls	to	
and	
as
the	program	runs.</p>
<p>7.13.1	
Compile-Time
Interpositioning
Figure	
7.20
shows	how	to	use	the	C	preprocessor	to	interpose	at
compile	time.	Each	wrapper	function	in	
(
Figure	
7.20(c)
)
calls	the	target	function,	prints	a	trace,	and	returns.	The	local	
header	file	(
Figure	
7.20(b)
)	instructs	the	preprocessor	to	replace	each
call	to	a	target	function	with	a	call	to	its	wrapper.	Here	is	how	to	compile
and	link	the	program:
The	interpositioning	happens	because	of	the	
.	argument,	which	tells
the	C	preprocessor	to	look	for	
in	the	current	directory	before
looking	in	the	usual	system	directories.	Notice	that	the	wrappers	in
are	compiled	with	the	standard	
header	file.
Running	the	program	gives	the	following	trace:</p>
<h2>7.13.2	
Link-Time	Interpositioning
The	Linux	static	linker	supports	link-time	interpositioning	with	the	
flag.	This	flag	tells	the	linker	to	resolve	references	to	symbol	
as
(two	underscores	for	the	prefix),	and	to	resolve	references	to
symbol	
(two	underscores	for	the	prefix)	as	
.	
Figure	
7.21
shows	the	wrappers	for	our	example	program.
Here	is	how	to	compile	the	source	files	into	relocatable	object	files:
(a)	Example	program	int.c</h2>
<p>code/link/interpose/int.c</p>
<hr />
<h2>code/link/interpose/int.c
(b)	Local	malloc.h	file</h2>
<h2 id="codelinkinterposemalloch"><a class="header" href="#codelinkinterposemalloch">code/link/interpose/malloc.h</a></h2>
<h2>code/link/interpose/malloc.h
(c)	Wrapper	functions	in	mymalloc.c</h2>
<p>code/link/interpose/mymalloc.c</p>
<hr />
<h2>code/link/interpose/mymalloc.c
Figure	
7.20	
Compile-time	interpositioning	with	the	C	preprocessor.</h2>
<p>code/link/interpose/mymalloc.c</p>
<hr />
<p>code/link/interpose/mymalloc.c
Figure	
7.21	
Link-time	interpositioning	with	the	
flag.
And	here	is	how	to	link	the	object	files	into	an	executable:</p>
<p>The	
,	option	flag	passes	option	to	the	linker.	Each	comma	in	option	is
replaced	with	a	space.	So	
passes	
to
the	linker,	and	similarly	for	
.
Running	the	program	gives	the	following	trace:
7.13.3	
Run-Time	Interpositioning
Compile-time	interpositioning	requires	access	to	a	program's	source	files.
Link-time	interpositioning	requires	access	to	its	relocatable	object	files.
However,	there	is	a	mechanism	for	interpositioning	at	run	time	that
requires	access	only	to	the	executable	object	file.	This	fascinating
mechanism	is	based	on	the	dynamic	linker's	
environment
variable.
If	the	
environment	variable	is	set	to	a	list	of	shared	library
pathnames	(separated	by	spaces	or	colons),	then	when	you	load	and
execute	a	program,	the	dynamic	linker	(
)	will	search	the
libraries	first,	before	any	other	shared	libraries,	when	it
resolves	undefined	references.	With	this	mechanism,	you	can	interpose
on	any	function	in	any	shared	library,	including	
,	when	you	load
and	execute	any	executable.</p>
<p>Figure	
7.22
shows	the	wrappers	for	
and	
.	In	each
wrapper,	the	call	to	
returns	the	pointer	to	the	target	
function.
The	wrapper	then	calls	the	target	function,	prints	a	trace,	and	returns.
Here	is	how	to	build	the	shared	library	that	contains	the	wrapper
functions:
Here	is	how	to	compile	the	main	program:
Here	is	how	to	run	the	program	from	the	bash	shell:
3.	
If	you	don't	know	what	shell	you	are	running,	type	
at	the	command	line.
And	here	is	how	to	run	it	from	the	
or	
shells:
3</p>
<h2>Notice	that	you	can	use	
to	interpose	on	the	library	calls	of	
any
executable	program!</h2>
<p>code/link/interpose/mymalloc.c</p>
<hr />
<p>code/link/interpose/mymalloc.c
Figure	
7.22	
Run-time	interpositioning	with	
.</p>
<p>7.14	
Tools	for	Manipulating	Object
Files
There	are	a	number	of	tools	available	on	Linux	systems	to	help	you
understand	and	manipulate	object	files.	In	particular,	the	GNU	
binutils
package	is	especially	helpful	and	runs	on	every	Linux	platform.
AR
.	Creates	static	libraries,	and	inserts,	deletes,	lists,	and	extracts
members.
STRINGS
.	Lists	all	of	the	printable	strings	contained	in	an	object	file.
STRIP
.	Deletes	symbol	table	information	from	an	object	file.
NM
.	Lists	the	symbols	defined	in	the	symbol	table	of	an	object	file.
SIZE
.	Lists	the	names	and	sizes	of	the	sections	in	an	object	file.
READELF
.	Displays	the	complete	structure	of	an	object	file,	including	all
of	the	information	encoded	in	the	ELF	header.	Subsumes	the
functionality	of	
SIZE</p>
<p>and	
NM
.
OBJDUMP
.	The	mother	of	all	binary	tools.	Can	display	all	of	the
information	in	an	object	file.	Its	most	useful	function	is	disassembling
the	binary	instructions	in	the	
section.
Linux	systems	also	provide	the	
LDD</p>
<p>program	for	manipulating	shared
libraries:</p>
<p>LDD
:	Lists	the	shared	libraries	that	an	executable	needs	at	run	time.</p>
<p>7.15	
Summary
Linking	can	be	performed	at	compile	time	by	static	linkers	and	at	load
time	and	run	time	by	dynamic	linkers.	Linkers	manipulate	binary	files
called	object	files,	which	come	in	three	different	forms:	relocatable,
executable,	and	shared.	Relocatable	object	files	are	combined	by	static
linkers	into	an	executable	object	file	that	can	be	loaded	into	memory	and
executed.	Shared	object	files	(shared	libraries)	are	linked	and	loaded	by
dynamic	linkers	at	run	time,	either	implicitly	when	the	calling	program	is
loaded	and	begins	executing,	or	on	demand,	when	the	program	calls
functions	from	the	
library.
The	two	main	tasks	of	linkers	are	symbol	resolution,	where	each	global
symbol	in	an	object	file	is	bound	to	a	unique	definition,	and	relocation,
where	the	ultimate	memory	address	for	each	symbol	is	determined	and
where	references	to	those	objects	are	modified.
Static	linkers	are	invoked	by	compiler	drivers	such	as	
GCC
.	They	combine
multiple	relocatable	object	files	into	a	single	executable	object	file.
Multiple	object	files	can	define	the	same	symbol,	and	the	rules	that
linkers	use	for	silently	resolving	these	multiple	definitions	can	introduce
subtle	bugs	in	user	programs.
Multiple	object	files	can	be	concatenated	in	a	single	static	library.	Linkers
use	libraries	to	resolve	symbol	references	in	other	object	modules.	The
left-to-right	sequential	scan	that	many	linkers	use	to	resolve	symbol
references	is	another	source	of	confusing	link-time	errors.</p>
<p>Loaders	map	the	contents	of	executable	files	into	memory	and	run	the
program.	Linkers	can	also	produce	partially	linked	executable	object	files
with	unresolved	references	to	the	routines	and	data	defined	in	a	shared
library.	At	load	time,	the	loader	maps	the	partially	linked	executable	into
memory	and	then	calls	a	dynamic	linker,	which	completes	the	linking	task
by	loading	the	shared	library	and	relocating	the	references	in	the
program.
Shared	libraries	that	are	compiled	as	position-independent	code	can	be
loaded	anywhere	and	shared	at	run	time	by	multiple	processes.
Applications	can	also	use	the	dynamic	linker	at	run	time	in	order	to	load,
link,	and	access	the	functions	and	data	in	shared	libraries.</p>
<p>Bibliographic	Notes
Linking	is	poorly	documented	in	the	computer	systems	literature.	Since	it
lies	at	the	intersection	of	compilers,	computer	architecture,	and	operating
systems,	linking	requires	an	understanding	of	code	generation,	machine-
language	programming,	program	instantiation,	and	virtual	memory.	It
does	not	fit	neatly	into	any	of	the	usual	computer	systems	specialties	and
thus	is	not	well	covered	by	the	classic	texts	in	these	areas.	However,
Levine's	monograph	provides	a	good	general	reference	on	the	subject
[
69
].	The	original	IA	32	specifications	for	ELF	and	DWARF	(a
specification	for	the	contents	of	the	
and	
sections)	are
described	in	[
54
].	The	x86-64	extensions	to	the	ELF	file	format	are
described	in	[
36
].	The	x86-64	application	binary	interface	(ABI)	describes
the	conventions	for	compiling,	linking,	and	running	x86-64	programs,
including	the	rules	for	relocation	and	position-independent	code	[
77
].</p>
<p>Homework	Problems
7.6	
♦
This	problem	concerns	the	
module	from	
Figure	
7.5
and	the
following	version	of	the	
function	that	counts	the	number	of	times	it
has	been	called:</p>
<p>For	each	symbol	that	is	defined	and	referenced	in	
,	indicate	if	it	will
have	a	symbol	table	entry	in	the	
section	in	module	
.	If	so,
indicate	the	module	that	defines	the	symbol	(
),	the	symbol
type(local,	global,	or	extern),	and	the	section	(
,	or	
)	it
occupies	in	that	module.
Symbol
entry?
Symbol
type
Module	where
defined
Section</p>
<hr />
<hr />
<hr />
<hr />
<hr />
<hr />
<hr />
<hr />
<hr />
<hr />
<hr />
<hr />
<hr />
<hr />
<hr />
<hr />
<hr />
<hr />
<hr />
<hr />
<hr />
<hr />
<hr />
<hr />
<hr />
<hr />
<hr />
<hr />
<p>7.7	
♦
Without	changing	any	variable	names,	modify	
on	page	683	so
that	
prints	the	correct	values	of	
and	
(i.e.,	the	hex
representations	of	integers	15213	and	15212).
7.8	
♦
In	this	problem,	let	
denote	that	the	linker	will
associate	an	arbitrary	reference	to	symbol	
in	module	
to	the	definition
of	
in	module	
.	For	each	example	below,	use	this	notation	to	indicate
how	the	linker	would	resolve	references	to	the	multiply-defined	symbol	in
each	module.	If	there	is	a	link-time	error	(rule	1),	write	&quot;
&quot;.	If	the	linker
arbitrarily	chooses	one	of	the	definitions	(rule	3),	write	&quot;
&quot;.
A
.	</p>
<p>B
.	
C
.	
7.9	
♦
Consider	the	following	program,	which	consists	of	two	object	modules:</p>
<p>When	this	program	is	compiled	and	executed	on	an	x86-64	Linux	system,
it	prints	the	string	
and	terminates	normally,	even	though	function
never	initializes	variable	
.	Can	you	explain	this?
7.10	
♦♦</p>
<p>Let	
and	
denote	object	modules	or	static	libraries	in	the	current
directory,	and	let	
→
denote	that	
depends	on	
,	in	the	sense	that	
defines	a	symbol	that	is	
referenced	by	
.	For	each	of	the	following
scenarios,	show	the	minimal	command	line	(i.e.,	one	with	the	least
number	of	object	file	and	library	arguments)	that	will	allow	the	static	linker
to	resolve	all	symbol	references:
A
.	
B
.	
C
.	
7.11	
♦♦
The	program	header	in	
Figure	
7.14
indicates	that	the	data	segment
occupies	
bytes	in	memory.	However,	only	the	first	
bytes	of
these	come	from	the	sections	of	the	executable	file.	What	causes	this
discrepancy?
7.12	
♦♦</p>
<p>Consider	the	call	to	function	swap	in	object	file	
(
Problem	
7.6
).
with	the	following	relocation	entry:
A
.	
Suppose	that	the	linker	relocates	
in	
to	address
and	
to	address	
.	Then	what	is	the	value	of
the	relocated	reference	to	
in	the	
instruction?
B
.	
Suppose	that	the	linker	relocates	
in	
to	address
and	
to	address	
.	Then	what	is	the	value	of
the	relocated	reference	to	
in	the	
instruction?
7.13	
♦♦
Performing	the	following	tasks	will	help	you	become	more	familiar	with
the	various	tools	for	manipulating	object	files.</p>
<p>A
.	
How	many	object	files	are	contained	in	the	versions	of	
and
on	your	system?
B
.	
Does	
produce	different	executable	code	than	
?
C
.	
What	shared	libraries	does	the	
GCC</p>
<p>driver	on	your	system	use?</p>
<p>Solutions	to	Practice	Problems
Solution	to	Problem	
7.1	
(page
678
)
The	purpose	of	this	problem	is	to	help	you	understand	the	relationship
between	linker	symbols	and	C	variables	and	functions.	Notice	that	the	C
local	variable	
does	
not
have	a	symbol	table	entry.
Symbol
entry?
Symbol	type
Module	where	defined
Section
Yes
extern
Yes
global
Yes
global
COMMON
Yes
global
No
—
—
—
Solution	to	Problem	
7.2	
(page
684
)</p>
<p>This	is	a	simple	drill	that	checks	your	understanding	of	the	rules	that	a
Unix	linker	uses	when	it	resolves	global	symbols	that	are	defined	in	more
than	one	module.	Understanding	these	rules	can	help	you	avoid	some
nasty	programming	bugs.
A
.	
The	linker	chooses	the	strong	symbol	defined	in	module	1	over	the
weak	symbol	defined	in	module	2	(rule	2):
a
.	
b
.	
B
.	
This	is	an	
,	because	each	module	defines	a	strong	symbol
(rule	1).
C
.	
The	linker	chooses	the	strong	symbol	defined	in	module	2	over	the
weak	symbol	defined	in	module	1	(rule	2):
a
.	
b
.	
Solution	to	Problem	
7.3	
(page
689
)
Placing	static	libraries	in	the	wrong	order	on	the	command	line	is	a
common	source	of	linker	errors	that	confuses	many	programmers.
However,	once	you	understand	how	linkers	use	static	libraries	to	resolve
references,	it's	pretty	straightforward.	This	little	drill	checks	your
understanding	of	this	idea:</p>
<p>A
.	
B
.	
C
.	
Solution	to	Problem	
7.4	
(page
694
)
This	problem	concerns	the	disassembly	listing	in	
Figure	
7.12(a)
.	Our
purpose	here	is	to	give	you	some	practice	reading	disassembly	listings
and	to	check	your	understanding	of	PC-relative	addressing.
A
.	
The	hex	address	of	the	relocated	reference	in	line	5	is	
.
B
.	
The	hex	value	of	the	relocated	reference	in	line	5	is	
.
Remember	that	the	disassembly	listing	shows	the	value	of	the
reference	in	little-endian	byte	order.
Solution	to	Problem	
7.5	
(page
695
)
This	problem	tests	your	understanding	of	how	the	linker	relocates	PC-
relative	references.	You	were	given	that</p>
<p>and
Using	the	algorithm	in	
Figure	
7.10
,	the	linker	first	computes	the	run-
time	address	of	the	reference:
It	then	updates	the	reference:
Thus,	in	the	resulting	executable	object	file,	the	PC-relative	reference	to
has	a	value	of	
:</p>
<p>Chapter	
8	
Exceptional	Control	Flow
8.1	
Exceptions	723
8.2	
Processes	732
8.3	
System	Call	Error	Handling	737
8.4	
Process	Control	738
8.5	
Signals	756
8.6	
Nonlocal	Jumps	781
8.7	
Tools	for	Manipulating	Processes	786
8.8	
Summary
787
Bibliographic	Notes	787
Homework	Problems	788
Solutions	to	Practice	Problems	795
From	the	time	you	first	apply	power	to	a	processor
until	the	time	you	shut	it	off,	the	program	counter
assumes	a	sequence	of	values</p>
<p>where	each	
a
is	the	address	of	some	corresponding
instruction	
I
.	Each	transition	from	
a
to	
a
is	called
a	
control	transfer
.	A	sequence	of	such	control
transfers	is	called	the	
flow	of	control
,	or	
control	flow
,
of	the	processor.
The	simplest	kind	of	control	flow	is	a	“smooth”
sequence	where	each	
I
and	
I
are	adjacent	in
memory.	Typically,	abrupt	changes	to	this	smooth
flow,	where	
I
is	not	adjacent	to	
I
,	are	caused	by
familiar	program	instructions	such	as	jumps,	calls,
and	returns.	Such	instructions	are	necessary
mechanisms	that	allow	programs	to	react	to
changes	in	internal	program	state	represented	by
program	variables.
But	systems	must	also	be	able	to	react	to	changes
in	system	state	that	are	not	captured	by	internal
program	variables	and	are	not	necessarily	related	to
the	execution	of	the	program.	For	example,	a
hardware	timer	goes	off	at	regular	intervals	and
must	be	dealt	with.	Packets	arrive	at	the	network
adapter	and	must	be	stored	in	memory.	Programs
request	data	from	a	disk	and	then	sleep	until	they
are	notified	that	the	data	are	ready.	Parent
a
0
,
a
1
,
...
,
a
n
−
1
k
k
k
k
+1
k
k
+1
k
+1
k</p>
<p>processes	that	create	child	processes	must	be
notified	when	their	children	terminate.
Modern	systems	react	to	these	situations	by	making
abrupt	changes	in	the	control	flow.	In	general,	we
refer	to	these	abrupt	changes	as	
exceptional	control
flow	(ECF
).	ECF	occurs	at	all	levels	of	a	computer
system.	For	example,	at	the	hardware	level,	events
detected	by	the	hardware	trigger	abrupt	control
transfers	to	exception	handlers.	At	the	operating
systems	level,	the	kernel	transfers	control	from	one
user	process	to	another	via	context	switches.	At	the
application	level,	a	process	can	send	a	
signal
to
another	process	that	abruptly	transfers	control	to	a
signal	handler	in	the	recipient.	An	individual
program	can	react	to	errors	by	sidestepping	the
usual	stack	discipline	and	making	nonlocal	jumps	to
arbitrary	locations	in	other	functions.
As	programmers,	there	are	a	number	of	reasons
why	it	is	important	for	you	to	understand	ECF:
Understanding	ECF	will	help	you	understand
important	systems	concepts.	
ECF	is	the	basic
mechanism	that	operating	systems	use	to
implement	I/O,	processes,	and	virtual	memory.
Before	you	can	really	understand	these
important	ideas,	you	need	to	understand	ECF.</p>
<p>Understanding	ECF	will	help	you	understand
how	applications	interact	with	the	operating
system.	
Applications	request	services	from	the
operating	system	by	using	a	form	of	ECF	known
as	a	
trap
or	
system	call
.	For	example,	writing
data	to	a	disk,	reading	data	from	a	network,
creating	a	new	process,	and	terminating	the
current	process	are	all	accomplished	by
application	programs	invoking	system	calls.
Understanding	the	basic	system	call	mechanism
will	help	you	understand	how	these	services	are
provided	to	applications.
Understanding	ECF	will	help	you	write
interesting	new	application	programs.	
The
operating	system	provides	application	programs
with	powerful	ECF	
mechanisms	for	creating	new
processes,	waiting	for	processes	to	terminate,
notifying	other	processes	of	exceptional	events
in	the	system,	and	detecting	and	responding	to
these	events.	If	you	understand	these	ECF
mechanisms,	then	you	can	use	them	to	write
interesting	programs	such	as	Unix	shells	and
Web	servers.
Understanding	ECF	will	help	you	understand
concurrency.	
ECF	is	a	basic	mechanism	for
implementing	concurrency	in	computer	systems.
The	following	are	all	examples	of	concurrency	in
action:	an	exception	handler	that	interrupts	the
execution	of	an	application	program;	processes</p>
<p>and	threads	whose	execution	overlap	in	time;
and	a	signal	handler	that	interrupts	the	execution
of	an	application	program.	Understanding	ECF	is
a	first	step	to	understanding	concurrency.	We	will
return	to	study	it	in	more	detail	in	
Chapter	
12
.
Understanding	ECF	will	help	you	understand
how	software	exceptions	work.	
Languages
such	as	C++	and	Java	provide	software
exception	mechanisms	via	
,	and	
statements.	Software	exceptions	allow	the
program	to	make	
nonlocal
jumps	(i.e.,	jumps	that
violate	the	usual	call/return	stack	discipline)	in
response	to	error	conditions.	Nonlocal	jumps	are
a	form	of	application-level	ECF	and	are	provided
in	C	via	the	
and	
functions.
Understanding	these	low-level	functions	will	help
you	understand	how	higher-level	software
exceptions	can	be	implemented.
Up	to	this	point	in	your	study	of	systems,	you	have
learned	how	applications	interact	with	the	hardware.
This	chapter	is	pivotal	in	the	sense	that	you	will
begin	to	learn	how	your	applications	interact	with
the	operating	system.	Interestingly,	these
interactions	all	revolve	around	ECF.	We	describe	the
various	forms	of	ECF	that	exist	at	all	levels	of	a
computer	system.	We	start	with	exceptions,	which
lie	at	the	intersection	of	the	hardware	and	the
operating	system.	We	also	discuss	system	calls,</p>
<p>which	are	exceptions	that	provide	applications	with
entry	points	into	the	operating	system.	We	then
move	up	a	level	of	abstraction	and	describe
processes	and	signals,	which	lie	at	the	intersection
of	applications	and	the	operating	system.	Finally,	we
discuss	nonlocal	jumps,	which	are	an	application-
level	form	of	ECF.</p>
<p>8.1	
Exceptions
Exceptions	are	a	form	of	exceptional	control	flow	that	are	implemented
partly	by	the	hardware	and	partly	by	the	operating	system.	Because	they
are	partly	implemented	in	hardware,	the	details	vary	from	system	to
system.	However,	the	basic	ideas	are	the	same	for	every	system.	Our
aim	in	this	section	is	to	give	you	a	general	understanding	of	exceptions
and	exception	handling	and	to	help	demystify	what	is	often	a	confusing
aspect	of	modern	computer	systems.
An	
exception
is	an	abrupt	change	in	the	control	flow	in	response	to	some
change	in	the	processor's	state.	
Figure	
8.1
shows	the	basic	idea.
In	the	figure,	the	processor	is	executing	some	current	instruction	
I
when	a	significant	change	in	the	processor's	
state
occurs.	The	state	is
encoded	in	various	bits	and	signals	inside	the	processor.	The	change	in
state	is	known	as	an	
event
.
Aside	
Hardware	versus	software
exceptions
C++	and	Java	programmers	will	have	noticed	that	the	term
“exception”	is	also	used	to	describe	the	application-level	ECF
mechanism	provided	by	C++	and	Java	in	the	form	of	
,	and	
statements.	If	we	wanted	to	be	perfectly	clear,	we
might	distinguish	between	“hardware”	and	“software”	exceptions,
curr</p>
<p>but	this	is	usually	unnecessary	because	the	meaning	is	clear	from
the	context.
Figure	
8.1	
Anatomy	of	an	exception.
A	change	in	the	processor's	state	(an	event)	triggers	an	abrupt	control
transfer	(an	exception)	from	the	application	program	to	an	exception
handler.	After	it	finishes	processing,	the	handler	either	returns	control	to
the	interrupted	program	or	aborts.
The	event	might	be	directly	related	to	the	execution	of	the	current
instruction.	For	example,	a	virtual	memory	page	fault	occurs,	an
arithmetic	overflow	occurs,	or	an	instruction	attempts	a	divide	by	zero.	On
the	other	hand,	the	event	might	be	unrelated	to	the	execution	of	the
current	instruction.	For	example,	a	system	timer	goes	off	or	an	I/O
request	completes.
In	any	case,	when	the	processor	detects	that	the	event	has	occurred,	it
makes	an	indirect	procedure	call	(the	exception),	through	a	jump	table
called	an	
exception	table
,	to	an	operating	system	subroutine	(the
exception	handler
)	that	is	specifically	designed	to	process	this	particular
kind	of	event.	When	the	exception	handler	finishes	processing,	one	of</p>
<p>three	things	happens,	depending	on	the	type	of	event	that	caused	the
exception:
1
.	
The	handler	returns	control	to	the	current	instruction	
I
,	the
instruction	that	was	executing	when	the	event	occurred.
2
.	
The	handler	returns	control	to	
I
,	the	instruction	that	would	have
executed	next	had	the	exception	not	occurred.
3
.	
The	handler	aborts	the	interrupted	program.
Section	
8.1.2
says	more	about	these	possibilities.
8.1.1	
Exception	Handling
Exceptions	can	be	difficult	to	understand	because	handling	them	involves
close	cooperation	between	hardware	and	software.	It	is	easy	to	get
confused	about
Figure	
8.2	
Exception	table.
curr
next</p>
<p>The	exception	table	is	a	jump	table	where	entry	
k
contains	the	address	of
the	handler	code	for	exception	
k
.
Figure	
8.3	
Generating	the	address	of	an	exception	handler.
The	exception	number	is	an	index	into	the	exception	table.
which	component	performs	which	task.	Let's	look	at	the	division	of	labor
between	hardware	and	software	in	more	detail.
Each	type	of	possible	exception	in	a	system	is	assigned	a	unique
nonnegative	integer	
exception	number
.	Some	of	these	numbers	are
assigned	by	the	designers	of	the	processor.	Other	numbers	are	assigned
by	the	designers	of	the	operating	system	
kernel
(the	memory-resident
part	of	the	operating	system).	Examples	of	the	former	include	divide	by
zero,	page	faults,	memory	access	violations,	breakpoints,	and	arithmetic
overflows.	Examples	of	the	latter	include	system	calls	and	signals	from
external	I/O	devices.
At	system	boot	time	(when	the	computer	is	reset	or	powered	on),	the
operating	system	allocates	and	initializes	a	jump	table	called	an
exception	table
,	so	that	entry	
k
contains	the	address	of	the	handler	for
exception	
k
.	
Figure	
8.2
shows	the	format	of	an	exception	table.
At	run	time	(when	the	system	is	executing	some	program),	the	processor
detects	that	an	event	has	occurred	and	determines	the	corresponding</p>
<p>exception	number	
k
.	The	processor	then	triggers	the	exception	by
making	an	indirect	procedure	call,	through	entry	
k
of	the	exception	table,
to	the	corresponding	handler.	
Figure	
8.3
shows	how	the	processor
uses	the	exception	table	to	form	the	address	of	the	appropriate	exception
handler.	The	exception	number	is	an	index	into	the	exception	table,
whose	starting	address	is	contained	in	a	special	CPU	register	called	the
exception	table	base	register
.
An	exception	is	akin	to	a	procedure	call,	but	with	some	important
differences:
As	with	a	procedure	call,	the	processor	pushes	a	return	address	on
the	stack	before	branching	to	the	handler.	However,	depending	on	the
class	of	exception,	the	return	address	is	either	the	current	instruction
(the	instruction	that	
was	executing	when	the	event	occurred)	or	the
next	instruction	(the	instruction	that	would	have	executed	after	the
current	instruction	had	the	event	not	occurred).
The	processor	also	pushes	some	additional	processor	state	onto	the
stack	that	will	be	necessary	to	restart	the	interrupted	program	when
the	handler	returns.	For	example,	an	x86-64	system	pushes	the
EFLAGS	register	containing	the	current	condition	codes,	among	other
things,	onto	the	stack.
When	control	is	being	transferred	from	a	user	program	to	the	kernel,
all	of	these	items	are	pushed	onto	the	kernel's	stack	rather	than	onto
the	user's	stack.
Exception	handlers	run	in	
kernel	mode
(
Section	
8.2.4
),	which
means	they	have	complete	access	to	all	system	resources.
Once	the	hardware	triggers	the	exception,	the	rest	of	the	work	is	done	in
software	by	the	exception	handler.	After	the	handler	has	processed	the</p>
<p>event,	it	optionally	returns	to	the	interrupted	program	by	executing	a
special	“return	from	interrupt”	instruction,	which	pops	the	appropriate
state	back	into	the	processor's	control	and	data	registers,	restores	the
state	to	
user	mode
(
Section	
8.2.4
)	if	the	exception	interrupted	a	user
program,	and	then	returns	control	to	the	interrupted	program.
8.1.2	
Classes	of	Exceptions
Exceptions	can	be	divided	into	four	classes:	
interrupts
,	
traps
,	
faults
,	and
aborts
.	The	table	in	
Figure	
8.4
summarizes	the	attributes	of	these
classes.
Interrupts
Interrupts
occur	
asynchronously
as	a	result	of	signals	from	I/O	devices
that	are	external	to	the	processor.	Hardware	interrupts	are	asynchronous
in	the	sense	that	they	are	not	caused	by	the	execution	of	any	particular
instruction.	Exception	handlers	for	hardware	interrupts	are	often	called
interrupt	handlers
.
Figure	
8.5	
summarizes	the	processing	for	an	interrupt.	I/O	devices
such	as	network	adapters,	disk	controllers,	and	timer	chips	trigger
interrupts	by	signaling	a	pin	on	the	processor	chip	and	placing	onto	the
system	bus	the	exception	number	that	identifies	the	device	that	caused
the	interrupt.
Class
Cause
Async/sync
Return	behavior</p>
<p>Interrupt
Signal	from	I/O	device
Async
Always	returns	to	next	instruction
Trap
Intentional	exception
Sync
Always	returns	to	next	instruction
Fault
Potentially	recoverable	error
Sync
Might	return	to	current	instruction
Abort
Nonrecoverable	error
Sync
Never	returns
Figure	
8.4	
Classes	of	exceptions.
Asynchronous	exceptions	occur	as	a	result	of	events	in	I/O	devices	that
are	external	to	the	processor.	Synchronous	exceptions	occur	as	a	direct
result	of	executing	an	instruction.
Figure	
8.5	
Interrupt	handling.
The	interrupt	handler	returns	control	to	the	next	instruction	in	the
application	program's	control	flow.</p>
<p>Figure	
8.6	
Trap	handling.
The	trap	handler	returns	control	to	the	next	instruction	in	the	application
program's	control	flow.
After	the	current	instruction	finishes	executing,	the	processor	notices	that
the	interrupt	pin	has	gone	high,	reads	the	exception	number	from	the
system	bus,	and	then	calls	the	appropriate	interrupt	handler.	When	the
handler	returns,	it	returns	control	to	the	next	instruction	(i.e.,	the
instruction	that	would	have	followed	the	current	instruction	in	the	control
flow	had	the	interrupt	not	occurred).	The	effect	is	that	the	program
continues	executing	as	though	the	interrupt	had	never	happened.
The	remaining	classes	of	exceptions	(traps,	faults,	and	aborts)	occur
synchronously
as	a	result	of	executing	the	current	instruction.	We	refer	to
this	instruction	as	the	
faulting	instruction
.
Traps	and	System	Calls
Traps
are	
intentional
exceptions	that	occur	as	a	result	of	executing	an
instruction.	Like	interrupt	handlers,	trap	handlers	return	control	to	the	next
instruction.	The	most	important	use	of	traps	is	to	provide	a	procedure-like
interface	between	user	programs	and	the	kernel,	known	as	a	
system	call
.
User	programs	often	need	to	request	services	from	the	kernel	such	as
reading	a	file	(
),	creating	a	new	process	(
),	loading	a	new
program	(
),	and	terminating	the	current	process	(
).	To	allow
controlled	access	to	such	kernel	services,	processors	provide	a	special</p>
<p>n
instruction	that	user	programs	can	execute	when	they	want	to
request	service	
n
.	Executing	the	
instruction	causes	a	trap	to	an</p>
<p>exception	handler	that	decodes	the	argument	and	calls	the	appropriate
kernel	routine.	
Figure	
8.6
summarizes	the	processing	for	a	system
call.
From	a	programmer's	perspective,	a	system	call	is	identical	to	a	regular
function	call.	However,	their	implementations	are	quite	different.	Regular
functions
Figure	
8.7	
Fault	handling.
Depending	on	whether	the	fault	can	be	repaired	or	not,	the	fault	handler
either	re-executes	the	faulting	instruction	or	aborts.
Figure	
8.8	
Abort	handling.
The	abort	handler	passes	control	to	a	kernel	
routine	that
terminates	the	application	program.</p>
<p>run	in	
user	mode
,	which	restricts	the	types	of	instructions	they	can
execute,	and	they	access	the	same	stack	as	the	calling	function.	A
system	call	runs	in	
kernel	mode
,	which	allows	it	to	execute	privileged
instructions	and	access	a	stack	defined	in	the	kernel.	
Section	
8.2.4
discusses	user	and	kernel	modes	in	more	detail.
Faults
Faults	result	from	error	conditions	that	a	handler	might	be	able	to	correct.
When	a	fault	occurs,	the	processor	transfers	control	to	the	fault	handler.	If
the	handler	is	able	to	correct	the	error	condition,	it	returns	control	to	the
faulting	instruction,	thereby	re-executing	it.	Otherwise,	the	handler	returns
to	an	
routine	in	the	kernel	that	terminates	the	application	program
that	caused	the	fault.	
Figure	
8.7	
summarizes	the	processing	for	a
fault.
A	classic	example	of	a	fault	is	the	page	fault	exception,	which	occurs
when	an	instruction	references	a	virtual	address	whose	corresponding
page	is	not	resident	in	memory	and	must	therefore	be	retrieved	from	disk.
As	we	will	see	in	
Chapter	
9
,	a	page	is	a	contiguous	block	(typically	4
KB)	of	virtual	memory.	The	page	fault	handler	loads	the	appropriate	page
from	disk	and	then	returns	control	to	the	instruction	that	caused	the	fault.
When	the	instruction	executes	again,	the	appropriate	page	is	now
resident	in	memory	and	the	instruction	is	able	to	run	to	completion
without	faulting.
Aborts</p>
<p>Aborts	result	from	unrecoverable	fatal	errors,	typically	hardware	errors
such	as	parity	errors	that	occur	when	DRAM	or	SRAM	bits	are	corrupted.
Abort	handlers	never	return	control	to	the	application	program.	As	shown
in	
Figure	
8.8
,	the	handler	returns	control	to	an	
routine	that
terminates	the	application	program.
Exception	number
Description
Exception	class
0
Divide	error
Fault
13
General	protection	fault
Fault
14
Page	fault
Fault
18
Machine	check
Abort
32-255
OS-defined	exceptions
Interrupt	or	trap
Figure	
8.9	
Examples	of	exceptions	in	x86-64	systems.
8.1.3	
Exceptions	in	Linux/x86-64
Systems
To	help	make	things	more	concrete,	let's	look	at	some	of	the	exceptions
defined	for	x86-64	systems.	There	are	up	to	256	different	exception	types
[
50
].	Numbers	in	the	range	from	0	to	31	correspond	to	exceptions	that
are	defined	by	the	Intel	architects	and	thus	are	identical	for	any	x86-64
system.	Numbers	in	the	range	from	32	to	255	correspond	to	interrupts</p>
<p>and	traps	that	are	defined	by	the	operating	system.	
Figure	
8.9	
shows
a	few	examples.
Linux/x86-64	Faults	and	Aborts
Divide	error.	
A	divide	error	(exception	0)	occurs	when	an	application
attempts	to	divide	by	zero	or	when	the	result	of	a	divide	instruction	is
too	big	for	the	destination	operand.	Unix	does	not	attempt	to	recover
from	divide	errors,	opting	instead	to	abort	the	program.	Linux	shells
typically	report	divide	errors	as	“Floating	exceptions.”
General	protection	fault.	
The	infamous	general	protection	fault
(exception	13)	occurs	for	many	reasons,	usually	because	a	program
references	an	undefined	area	of	virtual	memory	or	because	the
program	attempts	to	write	to	a	read-only	text	segment.	Linux	does	not
attempt	to	recover	from	this	fault.	Linux	shells	typically	report	general
protection	faults	as	“Segmentation	faults.”
Page	fault.	
A	page	fault	(exception	14)	is	an	example	of	an	exception
where	the	faulting	instruction	is	restarted.	The	handler	maps	the
appropriate	page	of	virtual	memory	on	disk	into	a	page	of	physical
memory	and	then	restarts	the	faulting	instruction.	We	will	see	how
page	faults	work	in	detail	in	
Chapter	
9
.
Machine	check.	
A	machine	check	(exception	18)	occurs	as	a	result
of	a	fatal	hardware	error	that	is	detected	during	the	execution	of	the
faulting	instruction.	Machine	check	handlers	never	return	control	to
the	application	program.
Linux/x86-64	System	Calls</p>
<p>Linux	provides	hundreds	of	system	calls	that	application	programs	use
when	they	want	to	request	services	from	the	kernel,	such	as	reading	a
file,	writing	a	file,	and
Number
Name
Description
Number
Name
Description
0
Read	file
33
Suspend	process	until
signal	arrives
1
Write	file
37
Schedule	delivery	of
alarm	signal
2
Open	file
39
Get	process	ID
3
Close	file
57
Create	process
4
Get	info	about	file
59
Execute	a	program
9
Map	memory	page
to	file
60
Terminate	process
12
Reset	the	top	of
the	heap
61
Wait	for	a	process	to
terminate
32
Copy	file	descriptor
62
Send	signal	to	a
process
Figure	
8.10	
Examples	of	popular	system	calls	in	Linux	x86-64
systems.
creating	a	new	process.	
Figure	
8.10	
lists	some	popular	Linux	system
calls.	Each	system	call	has	a	unique	integer	number	that	corresponds	to</p>
<p>an	offset	in	a	jump	table	in	the	kernel.	(Notice	that	this	jump	table	is	not
the	same	as	the	exception	table.)
C	programs	can	invoke	any	system	call	directly	by	using	the	
function.	However,	this	is	rarely	necessary	in	practice.	The	C	standard
library	provides	a	set	of	convenient	wrapper	functions	for	most	system
calls.	The	wrapper	functions	package	up	the	arguments,	trap	to	the
kernel	with	the	appropriate	system	call	instruction,	and	then	pass	the
return	status	of	the	system	call	back	to	the	calling	program.	Throughout
this	text,	we	will	refer	to	system	calls	and	their	associated	wrapper
functions	interchangeably	as	
system-level	functions
.
System	calls	are	provided	on	x86-64	systems	via	a	trapping	instruction
called	
.	It	is	quite	interesting	to	study	how	programs	can	use	this
instruction	to	invoke	Linux	system	calls	directly.	All	arguments	to	Linux
system	calls	are	passed	through	general-purpose	registers	rather	than
the	stack.	By	convention,	register	
contains	the	syscall	number,	with
up	to	six	arguments	in	
,	and	
.	The	first
argument	is	in	
,	the	second	in	
,	and	so	on.	On	return	from	the
system	call,	registers	
and	
are	destroyed,	and	
contains
the	return	value.	A	negative	return	value	between	-4,095	and	-1	indicates
an	error	corresponding	to	negative	
.
For	example,	consider	the	following	version	of	the	familiar	
program,	written	using	the	
system-level	function	(
Section	
10.4
)
instead	of	
:</p>
<h2>The	first	argument	to	
sends	the	output	to	
.	The	second
argument	is	the	sequence	of	bytes	to	write,	and	the	third	argument	gives
the	number	of	bytes	to	write.
Aside	
A	note	on	terminology
The	terminology	for	the	various	classes	of	exceptions	varies	from
system	to	system.	Processor	ISA	specifications	often	distinguish
between	asynchronous	“interrupts”	and	synchronous	“exceptions”
yet	provide	no	umbrella	term	to	refer	to	these	very	similar
concepts.	To	avoid	having	to	constantly	refer	to	“exceptions	and
interrupts”	and	“exceptions	or	interrupts,”	we	use	the	word
“exception”	as	the	general	term	and	distinguish	between
asynchronous	exceptions	(interrupts)	and	synchronous	exceptions
(traps,	faults,	and	aborts)	only	when	it	is	appropriate.	As	we	have
noted,	the	basic	ideas	are	the	same	for	every	system,	but	you
should	be	aware	that	some	manufacturers'	manuals	use	the	word
“exception”	to	refer	only	to	those	changes	in	control	flow	caused
by	synchronous	events.</h2>
<hr />
<p>code/ecf/hello-asm64.sa</p>
<hr />
<hr />
<p>code/ecf/hello-asm64.sa
Figure	
8.11	
Implementing	the	
program	directly	with	Linux
system	calls.</p>
<p>Figure	
8.11
shows	an	assembly-language	version	of	
that	uses
the	
instruction	to	invoke	the	
and	
system	calls	directly.
Lines	9-13	invoke	the	
function.	First,	line	9	stores	the	number	of
the	
system	call	in	
,	and	lines	10-12	set	up	the	argument	list.
Then,	line	13	uses	the	
instruction	to	invoke	the	system	call.
Similarly,	lines	14-16	invoke	the	
system	call.</p>
<p>8.2	
Processes
Exceptions	are	the	basic	building	blocks	that	allow	the	operating	system
kernel	to	provide	the	notion	of	a	
process
,	one	of	the	most	profound	and
successful	ideas	in	computer	science.
When	we	run	a	program	on	a	modern	system,	we	are	presented	with	the
illusion	that	our	program	is	the	only	one	currently	running	in	the	system.
Our	program	appears	to	have	exclusive	use	of	both	the	processor	and
the	memory.	The	processor	appears	to	execute	the	instructions	in	our
program,	one	after	the	other,	without	interruption.	Finally,	the	code	and
data	of	our	program	appear	to	be	the	only	objects	in	the	system's
memory.	These	illusions	are	provided	to	us	by	the	notion	of	a	process.
The	classic	definition	of	a	process	is	
an	instance	of	a	program	in
execution
.	Each	program	in	the	system	runs	in	the	
context
of	some
process.	The	context	consists	of	the	state	that	the	program	needs	to	run
correctly.	This	state	includes	the	program's	code	and	data	stored	in
memory,	its	stack,	the	contents	of	its	general	purpose	registers,	its
program	counter,	environment	variables,	and	the	set	of	open	file
descriptors.
Each	time	a	user	runs	a	program	by	typing	the	name	of	an	executable
object	file	to	the	shell,	the	shell	creates	a	new	process	and	then	runs	the
executable	object	file	in	the	context	of	this	new	process.	Application
programs	can	also	create	new	processes	and	run	either	their	own	code
or	other	applications	in	the	context	of	the	new	process.</p>
<p>A	detailed	discussion	of	how	operating	systems	implement	processes	is
beyond	our	scope.	Instead,	we	will	focus	on	the	key	abstractions	that	a
process	provides	to	the	application:
An	independent	logical	control	flow	that	provides	the	illusion	that	our
program	has	exclusive	use	of	the	processor.
A	private	address	space	that	provides	the	illusion	that	our	program
has	exclusive	use	of	the	memory	system.
Let's	look	more	closely	at	these	abstractions.
8.2.1	
Logical	Control	Flow
A	process	provides	each	program	with	the	illusion	that	it	has	exclusive
use	of	the	processor,	even	though	many	other	programs	are	typically
running	concurrently	on	the	system.	If	we	were	to	use	a	debugger	to
single-step	the	execution	of	our	program,	we	would	observe	a	series	of
program	counter	(PC)	values	that	corresponded	exclusively	to
instructions	contained	in	our	program's	executable	object	file	or	in	shared
objects	linked	into	our	program	dynamically	at	run	time.	This	sequence	of
PC	values	is	known	as	a	
logical	control	flow
,	or	simply	
logical	flow
.
Consider	a	system	that	runs	three	processes,	as	shown	in	
Figure
8.12
.	The	single	physical	control	flow	of	the	processor	is	partitioned
into	three	logical	flows,	one	for	each	process.	Each	vertical	line
represents	a	portion	of	the	logical	flow	for</p>
<p>Figure	
8.12	
Logical	control	flows.
Processes	provide	each	program	with	the	illusion	that	it	has	exclusive
use	of	the	processor.	Each	vertical	bar	represents	a	portion	of	the	logical
control	flow	for	a	process.
a	process.	In	the	example,	the	execution	of	the	three	logical	flows	is
interleaved.	Process	A	runs	for	a	while,	followed	by	B,	which	runs	to
completion.	Process	C	then	runs	for	a	while,	followed	by	A,	which	runs	to
completion.	Finally,	C	is	able	to	run	to	completion.
The	key	point	in	
Figure	
8.12
is	that	processes	take	turns	using	the
processor.	Each	process	executes	a	portion	of	its	flow	and	then	is
preempted
(temporarily	suspended)	while	other	processes	take	their
turns.	To	a	program	running	in	the	context	of	one	of	these	processes,	it
appears	to	have	exclusive	use	of	the	processor.	The	only	evidence	to	the
contrary	is	that	if	we	were	to	precisely	measure	the	elapsed	time	of	each
instruction,	we	would	notice	that	the	CPU	appears	to	periodically	stall
between	the	execution	of	some	of	the	instructions	in	our	program.
However,	each	time	the	processor	stalls,	it	subsequently	resumes
execution	of	our	program	without	any	change	to	the	contents	of	the
program's	memory	locations	or	registers.</p>
<p>8.2.2	
Concurrent	Flows
Logical	flows	take	many	different	forms	in	computer	systems.	Exception
handlers,	processes,	signal	handlers,	threads,	and	Java	processes	are
all	examples	of	logical	flows.
A	logical	flow	whose	execution	overlaps	in	time	with	another	flow	is	called
a	
concurrent	flow
,	and	the	two	flows	are	said	to	
run	concurrently
.	More
precisely,	flows	X	and	Y	are	concurrent	with	respect	to	each	other	if	and
only	if	X	begins	after	Y	begins	and	before	Y	finishes,	or	Y	begins	after	X
begins	and	before	X	finishes.	For	example,	in	
Figure	
8.12
,	processes
A	and	B	run	concurrently,	as	do	A	and	C.	On	the	other	hand,	B	and	C	do
not	run	concurrently,	because	the	last	instruction	of	B	executes	before
the	first	instruction	of	C.
The	general	phenomenon	of	multiple	flows	executing	concurrently	is
known	as	
concurrency
.	The	notion	of	a	process	taking	turns	with	other
processes	is	also	known	as	
multitasking
.	Each	time	period	that	a	process
executes	a	portion	of	its	flow	is	called	a	
time	slice
.	Thus,	multitasking	is
also	referred	to	as	
time	slicing
.	For	example,	in	
Figure	
8.12
,	the	flow
for	process	A	consists	of	two	time	slices.
Notice	that	the	idea	of	concurrent	flows	is	independent	of	the	number	of
processor	cores	or	computers	that	the	flows	are	running	on.	If	two	flows
overlap	in	time,	then	they	are	concurrent,	even	if	they	are	running	on	the
same	processor.	However,	we	will	sometimes	find	it	useful	to	identify	a
proper	subset	of	concurrent	
flows	known	as	
parallel	flows
.	If	two	flows
are	running	concurrently	on	different	processor	cores	or	computers,	then</p>
<p>we	say	that	they	are	
parallel	flows
,	that	they	are	
running	in	parallel
,	and
have	
parallel	execution
.
Practice	Problem	
8.1	
(solution	page	
795
)
Consider	three	processes	with	the	following	starting	and	ending	times:
Process
Start	time
End	time
A
0
2
B
1
4
C
3
5
For	each	pair	of	processes,	indicate	whether	they	run	concurrently	(Y)	or
not	(N):
Process	pair
Concurrent?
AB</p>
<p>AC</p>
<p>BC</p>
<p>8.2.3	
Private	Address	Space
A	process	provides	each	program	with	the	illusion	that	it	has	exclusive
use	of	the	system's	address	space.	On	a	machine	with	
n
-bit	addresses,
n
n</p>
<p>the	
address	space
is	the	set	of	2
possible	addresses,	0,	1,	...	,	2
-	1.	A
process	provides	each	program	with	its	own	
private	address	space
.	This
space	is	private	in	the	sense	that	a	byte	of	memory	associated	with	a
particular	address	in	the	space	cannot	in	general	be	read	or	written	by
any	other	process.
Although	the	contents	of	the	memory	associated	with	each	private
address	space	is	different	in	general,	each	such	space	has	the	same
general	organization.	For	example,	
Figure	
8.13
shows	the
organization	of	the	address	space	for	an	x86-64	Linux	process.
The	bottom	portion	of	the	address	space	is	reserved	for	the	user
program,	with	the	usual	code,	data,	heap,	and	stack	segments.	The	code
segment	always	begins	at	address	
.	The	top	portion	of	the
address	space	is	reserved	for	the	kernel	(the	memory-resident	part	of	the
operating	system).	This	part	of	the	address	space	contains	the	code,
data,	and	stack	that	the	kernel	uses	when	it	executes	instructions	on
behalf	of	the	process	(e.g.,	when	the	application	program	executes	a
system	call).
8.2.4	
User	and	Kernel	Modes
In	order	for	the	operating	system	kernel	to	provide	an	airtight	process
abstraction,	the	processor	must	provide	a	mechanism	that	restricts	the
instructions	that	an
n
n</p>
<p>Figure	
8.13	
Process	address	space.
application	can	execute,	as	well	as	the	portions	of	the	address	space	that
it	can	access.
Processors	typically	provide	this	capability	with	a	
mode	bit
in	some
control	register	that	characterizes	the	privileges	that	the	process	currently
enjoys.	When	the	mode	bit	is	set,	the	process	is	running	in	
kernel	mode
(sometimes	called	
supervisor	mode
).	A	process	running	in	kernel	mode
can	execute	any	instruction	in	the	instruction	set	and	access	any	memory
location	in	the	system.</p>
<p>When	the	mode	bit	is	not	set,	the	process	is	running	in	
user	mode
.	A
process	in	user	mode	is	not	allowed	to	execute	
privileged	instructions
that	do	things	such	as	halt	the	processor,	change	the	mode	bit,	or	initiate
an	I/O	operation.	Nor	is	it	allowed	to	directly	reference	code	or	data	in	the
kernel	area	of	the	address	space.	Any	such	attempt	results	in	a	fatal
protection	fault.	User	programs	must	instead	access	kernel	code	and
data	indirectly	via	the	system	call	interface.
A	process	running	application	code	is	initially	in	user	mode.	The	only	way
for	the	process	to	change	from	user	mode	to	kernel	mode	is	via	an
exception	such	as	an	interrupt,	a	fault,	or	a	trapping	system	call.	When
the	exception	occurs,	and	control	passes	to	the	exception	handler,	the
processor	changes	the	mode	from	user	mode	to	kernel	mode.	The
handler	runs	in	kernel	mode.	When	it	returns	to	the	application	code,	the
processor	changes	the	mode	from	kernel	mode	back	to	user	mode.
Linux	provides	a	clever	mechanism,	called	the	
filesystem,	that
allows	user	mode	processes	to	access	the	contents	of	kernel	data
structures.	The	
filesystem	exports	the	contents	of	many	kernel
data	structures	as	a	hierarchy	of	text	
files	that	can	be	read	by	user
programs.	For	example,	you	can	use	the	
filesystem	to	find	out
general	system	attributes	such	as	CPU	type	(
),	or	the
memory	segments	used	by	a	particular	process	(
).
The	2.6	version	of	the	Linux	kernel	introduced	a	
filesystem,	which
exports	additional	low-level	information	about	system	buses	and	devices.
8.2.5	
Context	Switches</p>
<p>The	operating	system	kernel	implements	multitasking	using	a	higher-level
form	of	exceptional	control	flow	known	as	a	
context	switch
.	The	context
switch	mechanism	is	built	on	top	of	the	lower-level	exception	mechanism
that	we	discussed	in	
Section	
8.1
.
The	kernel	maintains	a	
context
for	each	process.	The	context	is	the	state
that	the	kernel	needs	to	restart	a	preempted	process.	It	consists	of	the
values	of	objects	such	as	the	general-purpose	registers,	the	floating-point
registers,	the	program	counter,	user's	stack,	status	registers,	kernel's
stack,	and	various	kernel	data	structures	such	as	a	
page	table
that
characterizes	the	address	space,	a	
process	table
that	contains
information	about	the	current	process,	and	a	
file	table
that	contains
information	about	the	files	that	the	process	has	opened.
At	certain	points	during	the	execution	of	a	process,	the	kernel	can	decide
to	preempt	the	current	process	and	restart	a	previously	preempted
process.	This	decision	is	known	as	
scheduling
and	is	handled	by	code	in
the	kernel,	called	the	
scheduler
.	When	the	kernel	selects	a	new	process
to	run,	we	say	that	the	kernel	has	
scheduled
that	process.	After	the
kernel	has	scheduled	a	new	process	to	run,	it	preempts	the	current
process	and	transfers	control	to	the	new	process	using	a	mechanism
called	a	
context	switch
that	(1)	saves	the	context	of	the	current	process,
(2)	restores	the	saved	context	of	some	previously	preempted	process,
and	(3)	passes	control	to	this	newly	restored	process.
A	context	switch	can	occur	while	the	kernel	is	executing	a	system	call	on
behalf	of	the	user.	If	the	system	call	blocks	because	it	is	waiting	for	some
event	to	occur,	then	the	kernel	can	put	the	current	process	to	sleep	and
switch	to	another	process.	For	example,	if	a	
system	call	requires	a</p>
<p>disk	access,	the	kernel	can	opt	to	perform	a	context	switch	and	run
another	process	instead	of	waiting	for	the	data	to	arrive	from	the	disk.
Another	example	is	the	
system	call,	which	is	an	explicit	request	to
put	the	calling	process	to	sleep.	In	general,	even	if	a	system	call	does	not
block,	the	kernel	can	decide	to	perform	a	context	switch	rather	than
return	control	to	the	calling	process.
A	context	switch	can	also	occur	as	a	result	of	an	interrupt.	For	example,
all	systems	have	some	mechanism	for	generating	periodic	timer
interrupts,	typically	every	1	ms	or	10	ms.	Each	time	a	timer	interrupt
occurs,	the	kernel	can	decide	that	the	current	process	has	run	long
enough	and	switch	to	a	new	process.
Figure	
8.13	
shows	an	example	of	context	switching	between	a	pair	of
processes	A	and	B.	In	this	example,	initially	process	A	is	running	in	user
mode	until	it	traps	to	the	kernel	by	executing	a	
system	call.	The	trap
handler	in	the	kernel	requests	a	DMA	transfer	from	the	disk	controller	and
arranges	for	the	disk	to	interrupt	the
Figure	
8.14	
Anatomy	of	a	process	context	switch.</p>
<p>processor	after	the	disk	controller	has	finished	transferring	the	data	from
disk	to	memory.
The	disk	will	take	a	relatively	long	time	to	fetch	the	data	(on	the	order	of
tens	of	milliseconds),	so	instead	of	waiting	and	doing	nothing	in	the
interim,	the	kernel	performs	a	context	switch	from	process	A	to	B.	Note
that,	before	the	switch,	the	kernel	is	executing	instructions	in	user	mode
on	behalf	of	process	A	(i.e.,	there	is	no	separate	kernel	process).	During
the	first	part	of	the	switch,	the	kernel	is	executing	instructions	in	kernel
mode	on	behalf	of	process	A.	Then	at	some	point	it	begins	executing
instructions	(still	in	kernel	mode)	on	behalf	of	process	B.	And	after	the
switch,	the	kernel	is	executing	instructions	in	user	mode	on	behalf	of
process	B.
Process	B	then	runs	for	a	while	in	user	mode	until	the	disk	sends	an
interrupt	to	signal	that	data	have	been	transferred	from	disk	to	memory.
The	kernel	decides	that	process	B	has	run	long	enough	and	performs	a
context	switch	from	process	B	to	A,	returning	control	in	process	A	to	the
instruction	immediately	following	the	
system	call.	Process	A
continues	to	run	until	the	next	exception	occurs,	and	so	on.</p>
<p>8.3	
System	Call	Error	Handling
When	Unix	system-level	functions	encounter	an	error,	they	typically
return	-1	and	set	the	global	integer	variable	
to	indicate	what	went
wrong.	Programmers	should	
always
check	for	errors,	but	unfortunately,
many	skip	error	checking	because	it	bloats	the	code	and	makes	it	harder
to	read.	For	example,	here	is	how	we	might	check	for	errors	when	we	call
the	Linux	
function:
The	
function	returns	a	text	string	that	describes	the	error
associated	with	a	particular	value	of	
We	can	simplify	this	code
somewhat	by	defining	the	following	
error-reporting	function:</p>
<p>Given	this	function,	our	call	to	
reduces	from	four	lines	to	two	lines:
We	can	simplify	our	code	even	further	by	using	
error-handling	wrappers
,
as	pioneered	by	Stevens	in	[
110
].	For	a	given	base	function	
,	we
define	a	wrapper	function	
with	identical	arguments	but	with	the	first
letter	of	the	name	capitalized.	The	wrapper	calls	the	base	function,
checks	for	errors,	and	terminates	if	there	are	any	problems.	For	example,
here	is	the	error-handling	wrapper	for	the	
function:</p>
<p>Given	this	wrapper,	our	call	to	
shrinks	to	a	single	compact	line:
We	will	use	error-handling	wrappers	throughout	the	remainder	of	this
book.	They	allow	us	to	keep	our	code	examples	concise	without	giving
you	the	mistaken	impression	that	it	is	permissible	to	ignore	error
checking.	Note	that	when	we	discuss	system-level	functions	in	the	text,
we	will	always	refer	to	them	by	their	lowercase	base	names,	rather	than
by	their	uppercase	wrapper	names.
See	Appendix	A	for	a	discussion	of	Unix	error	handling	and	the	error-
handling	wrappers	used	throughout	this	book.	The	wrappers	are	defined
in	a	file	called	
,	and	their	prototypes	are	defined	in	a	header	file
called	
These	are	available	online	from	the	CS:APP	Web	site.</p>
<p>8.4	
Process	Control
Unix	provides	a	number	of	system	calls	for	manipulating	processes	from
C	programs.	This	section	describes	the	important	functions	and	gives
examples	of	how	they	are	used.
8.4.1	
Obtaining	Process	IDs
Each	process	has	a	unique	positive	(nonzero)	
process	ID	(PID)
.	The
function	returns	the	PID	of	the	calling	process.	The	
function	returns	the	PID	of	its	
parent
(i.e.,	the	process	that	created	the
calling	process).
The	
and	
routines	return	an	integer	value	of	type	
,
which	on	Linux	systems	is	defined	in	
as	an	int.</p>
<p>8.4.2	
Creating	and	Terminating
Processes
From	a	programmer's	perspective,	we	can	think	of	a	process	as	being	in
one	of	three	states:
Running.	
The	process	is	either	executing	on	the	CPU	or	waiting	to	be
executed	and	will	eventually	be	scheduled	by	the	kernel.
Stopped.	
The	execution	of	the	process	is	
suspended
and	will	not	be
scheduled.	A	process	stops	as	a	result	of	receiving	a	SIGSTOP,
SIGTSTP,	SIGTTIN,	or	SIGTTOU	signal,	and	it	remains	stopped	until
it	receives	a	SIGCONT	signal,	at	which	point	it	becomes	running
again.	(A	
signal
is	a	form	of	software	interrupt	that	we	will	describe	in
detail	in	
Section	
8.5
.)
Terminated.	
The	process	is	stopped	permanently.	A	process
becomes	terminated	for	one	of	three	reasons:	(1)	receiving	a	signal
whose	default	action	is	to	terminate	the	process,	(2)	returning	from
the	main	routine,	or	(3)	calling	the	
function.</p>
<p>The	
function	terminates	the	process	with	an	
exit	status
of	
.
(The	other	way	to	set	the	exit	status	is	to	return	an	integer	value	from	the
main	routine.)
A	
parent	process
creates	a	new	running	
child	process
by	calling	the	
function.
The	newly	created	child	process	is	almost,	but	not	quite,	identical	to	the
parent.	The	child	gets	an	identical	(but	separate)	copy	of	the	parent's
user-level	virtual	address	space,	including	the	code	and	data	segments,
heap,	shared	libraries,	and	user	stack.	The	child	also	gets	identical
copies	of	any	of	the	parent's	open	file	descriptors,	which	means	the	child
can	read	and	write	any	files	that	were	open	in	the	parent	when	it	called
.	The	most	significant	difference	between	the	parent	and	the	newly
created	child	is	that	they	have	different	PIDs.
The	
function	is	interesting	(and	often	confusing)	because	it	is	called
once
but	it	returns	
twice:
once	in	the	calling	process	(the	parent),	and
once	in	the	newly	created	child	process.	In	the	parent,	
returns	the
PID	of	the	child.	In	the	child,	
returns	a	value	of	0.	Since	the	PID	of</p>
<p>the	child	is	always	nonzero,	the	return	value	provides	an	unambiguous
way	to	tell	whether	the	program	is	executing	in	the	parent	or	the	child.
Figure	
8.15	
shows	a	simple	example	of	a	parent	process	that	uses
to	create	a	child	process.	When	the	
call	returns	in	line	6,	
has	a	value	of	1	in	both	the	parent	and	child.	The	child	increments	and
prints	its	copy	of	
in	line	8.	Similarly,	the	parent	decrements	and	prints
its	copy	of	
in	line	13.
When	we	run	the	program	on	our	Unix	system,	we	get	the	following
result:
There	are	some	subtle	aspects	to	this	simple	example.
Call	once,	return	twice.	
The	
function	is	called	once	by	the
parent,	but	it	returns	twice:	once	to	the	parent	and	once	to	the	newly
created	child.	This	is	fairly	straightforward	for	programs	that	create	a
single	child.	But	programs	with	multiple	instances	of	
can	be
confusing	and	need	to	be	reasoned	about	carefully.
Concurrent	execution.	
The	parent	and	the	child	are	separate
processes	that	run	concurrently.	The	instructions	in	their	logical</p>
<h2>control	flows	can	be	interleaved	by	the	kernel	in	an	arbitrary	way.
When	we	run	the	program	on	our	system,	the	parent	process
completes	its	
statement	first,	followed	by	the	child.	However,
on	another	system	the	reverse	might	be	true.	In	general,	as
programmers	we	can	never	make	assumptions	about	the	interleaving
of	the	instructions	in	different	processes.</h2>
<hr />
<p>code/ecf/fork.c</p>
<hr />
<hr />
<p>code/ecf/fork.c
Figure	
8.15	
Using	
to	create	a	new	process.
Duplicate	but	separate	address	spaces.	
If	we	could	halt	both	the
parent	and	the	child	immediately	after	the	
function	returned	in
each	process,	we	would	see	that	the	address	space	of	each	process
is	identical.	Each	process	has	the	same	user	stack,	the	same	local
variable	values,	the	same	heap,	the	same	global	variable	values,	and
the	same	code.	Thus,	in	our	example	program,	local	variable	
has	a
value	of	1	in	both	the	parent	and	the	child	when	the	
function
returns	in	line	6.	However,	since	the	parent	and	the	child	are	separate
processes,	they	each	have	their	own	private	address	spaces.	Any
subsequent	changes	that	a	parent	or	child	makes	to	
are	private	and
are	not	reflected	in	the	memory	of	the	other	process.	This	is	why	the
variable	
has	different	values	in	the	parent	and	child	when	they	call
their	respective	
statements.
Shared	files.	
When	we	run	the	example	program,	we	notice	that	both
parent	and	child	print	their	output	on	the	screen.	The	reason	is	that
the	child	inherits	all	of	the	parent's	open	files.	When	the	parent	calls
,	the	
file	is	open	and	directed	to	the	screen.	The	child
inherits	this	file,	and	thus	its	output	is	also	directed	to	the	screen.
When	you	are	first	learning	about	the	
function,	it	is	often	helpful	to
sketch	the	
process	graph
,	which	is	a	simple	kind	of	precedence	graph
that	captures	the	partial	ordering	of	program	statements.	Each	vertex	
a
corresponds	to	the	execution	of	a	program	statement.	A	directed	edge	
a
→	
b
denotes	that	statement	
a
“happens	before”	statement	
b
.	Edges	can</p>
<p>be	labeled	with	information	such	as	the	current	value	of	a	variable.
Vertices	corresponding	to	
statements	can	be	labeled	with	the
output	of	the	
.	Each	graph	begins	with	a	vertex	that
Figure	
8.16	
Process	graph	for	the	example	program	in	
Figure
8.15
.
Figure	
8.17	
Process	graph	for	a	nested	
.
corresponds	to	the	parent	process	calling	main.	This	vertex	has	no
inedges	and	exactly	one	outedge.	The	sequence	of	vertices	for	each
process	ends	with	a	vertex	corresponding	to	a	call	to	
.	This	vertex
has	one	inedge	and	no	outedges.
For	example,	
Figure	
8.16	
shows	the	process	graph	for	the	example
program	in	
Figure	
8.15
.	Initially,	the	parent	sets	variable	
to	1.	The</p>
<h2>parent	calls	
,	which	creates	a	child	process	that	runs	concurrently
with	the	parent	in	its	own	private	address	space.
For	a	program	running	on	a	single	processor,	any	
topological	sort
of	the
vertices	in	the	corresponding	process	graph	represents	a	feasible	total
ordering	of	the	statements	in	the	program.	Here's	a	simple	way	to
understand	the	idea	of	a	topological	sort:	Given	some	permutation	of	the
vertices	in	the	process	graph,	draw	the	sequence	of	vertices	in	a	line
from	left	to	right,	and	then	draw	each	of	the	directed	edges.	The
permutation	is	a	topological	sort	if	and	only	if	each	edge	in	the	drawing
goes	from	left	to	right.	Thus,	in	our	example	program	in	
Figure	
8.15
,
the	
statements	in	the	parent	and	child	can	occur	in	either	order
because	each	of	the	orderings	corresponds	to	some	topological	sort	of
the	graph	vertices.
The	process	graph	can	be	especially	helpful	in	understanding	programs
with	nested	
calls.	For	example,	
Figure	
8.17
shows	a	program
with	two	calls	to	
in	the	source	code.	The	corresponding	process
graph	helps	us	see	that	this	program	runs	four	processes,	each	of	which
makes	a	call	to	
and	which	can	execute	in	any	order.
Practice	Problem	
8.2	
(solution	page	
795
)
Consider	the	following	program:</h2>
<hr />
<p>code/ecf/forkprob0.c</p>
<hr />
<hr />
<p>code/ecf/forkprob0.c
A
.	
What	is	the	output	of	the	child	process?
B
.	
What	is	the	output	of	the	parent	process?
8.4.3	
Reaping	Child	Processes
When	a	process	terminates	for	any	reason,	the	kernel	does	not	remove	it
from	the	system	immediately.	Instead,	the	process	is	kept	around	in	a
terminated	state	until	it	is	
reaped
by	its	parent.	When	the	parent	reaps
the	terminated	child,	the	kernel	passes	the	child's	exit	status	to	the	parent
and	then	discards	the	terminated	process,	at	which	point	it	ceases	to
exist.	A	terminated	process	that	has	not	yet	been	reaped	is	called	a
zombie
.</p>
<p>When	a	parent	process	terminates,	the	kernel	arranges	for	the	
process	to	become	the	adopted	parent	of	any	orphaned	children.	The
process,	which	has	a	PID	of	1,	is	created	by	the	kernel	during
system	start-up,	never	terminates,	and	is	the	ancestor	of	every	process.
If	a	parent	process	terminates	without	reaping	its	zombie	children,	then
the	kernel	arranges	for	the	
process	to	reap	them.	However,	long-
running	programs	such	as	shells	or	servers	should	always	reap	their
zombie	children.	Even	though	zombies	are	not	running,	they	still
consume	system	memory	resources.
A	process	waits	for	its	children	to	terminate	or	stop	by	calling	the	
function.
Aside	
Why	are	terminated	children
called	zombies?
In	folklore,	a	zombie	is	a	living	corpse,	an	entity	that	is	half	alive
and	half	dead.	A	zombie	process	is	similar	in	the	sense	that</p>
<p>although	it	has	already	terminated,	the	kernel	maintains	some	of
its	state	until	it	can	be	reaped	by	the	parent.
The	
function	is	complicated.	By	default	(when	
),
suspends	execution	of	the	calling	process	until	a	child	process	in
its	
wait	set
terminates.	If	a	process	in	the	wait	set	has	already	terminated
at	the	time	of	the	call,	then	
returns	immediately.	In	either	case,
returns	the	PID	of	the	terminated	child	that	caused	
to
return.	At	this	point,	the	terminated	child	has	been	reaped	and	the	kernel
removes	all	traces	of	it	from	the	system.
Determining	the	Members	of	the	Wait	Set
The	members	of	the	wait	set	are	determined	by	the	
argument:
If	
,	then	the	wait	set	is	the	singleton	child	process	whose
process	ID	is	equal	to	
.
If	
,	then	the	wait	set	consists	of	all	of	the	parent's	child
processes.
The	
function	also	supports	other	kinds	of	wait	sets,	involving
Unix	process	groups,	which	we	will	not	discuss.
Modifying	the	Default	Behavior
The	default	behavior	can	be	modified	by	setting	
to	various
combinations	of	the	WNOHANG,	WUNTRACED,	and	WCONTINUED
constants:</p>
<p>WNOHANG.	
Return	immediately	(with	a	return	value	of	0)	if	none	of
the	child	processes	in	the	wait	set	has	terminated	yet.	The	default
behavior	suspends	the	calling	process	until	a	child	terminates;	this
option	is	useful	in	those	cases	where	you	want	to	continue	doing
useful	work	while	waiting	for	a	child	to	terminate.
WUNTRACED.	
Suspend	execution	of	the	calling	process	until	a
process	in	the	wait	set	becomes	either	terminated	or	stopped.	Return
the	PID	of	the	terminated	or	stopped	child	that	caused	the	return.	The
default	behavior	returns	only	for	terminated	children;	this	option	is
useful	when	you	want	to	check	for	both	terminated	
and
stopped
children.
WCONTINUED.	
Suspend	execution	of	the	calling	process	until	a
running	process	in	the	wait	set	is	terminated	or	until	a	stopped
process	in	the	wait	set	has	been	resumed	by	the	receipt	of	a
SIGCONT	signal.	(Signals	are	explained	in	
Section	
8.5
.)
You	can	combine	options	by	
OR
ing	them	together.	For	example:
WNOHANG	|	WUNTRACED:	Return	immediately,	with	a	return	value
of	0,	if	none	of	the	children	in	the	wait	set	has	stopped	or	terminated,
or	with	a	return	value	equal	to	the	PID	of	one	of	the	stopped	or
terminated	children.
Checking	the	Exit	Status	of	a	Reaped	Child
If	the	
argument	is	non-NULL,	then	
encodes	status
information	about	the	child	that	caused	the	return	in	status,	which	is	the</p>
<p>value	pointed	to	by	
.	The	
include	file	defines	several
macros	for	interpreting	the	
argument:
WIFEXITED(
).	
Returns	true	if	the	child	terminated	normally,	via
a	call	to	
or	a	return.
WEXITSTATUS(
).	
Returns	the	exit	status	of	a	normally
terminated	child.	This	status	is	only	defined	if	WIFEXITED()	returned
true.
WIFSIGNALED(
).	
Returns	true	if	the	child	process	terminated
because	of	a	signal	that	was	not	caught.
WTERMSIG(
).	
Returns	the	number	of	the	signal	that	caused
the	child	process	to	terminate.	This	status	is	only	defined	if
WIFSIGNALED()	returned	true.
WIFSTOPPED(
).	
Returns	true	if	the	child	that	caused	the
return	is	currently	stopped.
WSTOPSIG(
).	
Returns	the	number	of	the	signal	that	caused
the	child	to	stop.	This	status	is	only	defined	if	WIFSTOPPED()
returned	true.
WIFCONTINUED(
).	
Returns	true	if	the	child	process	was
restarted	by	receipt	of	a	SIGCONT	signal.
Error	Conditions
If	the	calling	process	has	no	children,	then	
returns	-1	and	sets
to	ECHILD.	If	the	
function	was	interrupted	by	a	signal,</p>
<h2>then	it	returns	-1	and	sets	
to	EINTR.
Practice	Problem	
8.3	
(solution	page	
797
)
List	all	of	the	possible	output	sequences	for	the	following	program:</h2>
<hr />
<h2 id="codeecfwaitprob0c"><a class="header" href="#codeecfwaitprob0c">code/ecf/waitprob0.c</a></h2>
<hr />
<p>code/ecf/waitprob0.c</p>
<p>The	
Function
The	
function	is	a	simpler	version	of	
.
Calling	
is	equivalent	to	calling	
Examples	of	Using	
Because	the	
function	is	somewhat	complicated,	it	is	helpful	to
look	at	a	few	examples.	
Figure	
8.18	
shows	a	program	that	uses
to	wait,	in	no	particular	order,	for	all	of	its	
N
children	to	terminate.
In	line	11,	the	parent	creates	each	of	the	
N
children,	and	in	line	12,	each
child	exits	with	a	unique	exit	status.
Aside	
Constants	associated	with	Unix
functions</p>
<h2>Constants	such	as	WNOHANG	and	WUNTRACED	are	defined	by
system	header	files.	For	example,	WNOHANG	and	WUNTRACED
are	defined	(indirectly)	by	the	
header	file:
In	order	to	use	these	constants,	you	must	include	the	
header	file	in	your	code:
The	
page	for	each	Unix	function	lists	the	header	files	to
include	whenever	you	use	that	function	in	your	code.	Also,	in
order	to	check	return	codes	such	as	ECHILD	and	EINTR,	you
must	include	
To	simplify	our	code	examples,	we	include
a	single	header	file	called	
that	includes	the	header	files
for	all	of	the	functions	used	in	the	book.	The	
header	file	is
available	online	from	the	CS:APP	Web	site.</h2>
<hr />
<p>code/ecf/waitpid1.c</p>
<hr />
<hr />
<h2>code/ecf/waitpid1.c
Figure	
8.18	
Using	the	
function	to	reap	zombie	children	in	no
particular	order.</h2>
<p>code/ecf/waitpid1.c
Before	moving	on,	make	sure	you	understand	why	line	12	is	executed	by
each	of	the	children,	but	not	the	parent.
In	line	15,	the	parent	waits	for	all	of	its	children	to	terminate	by	using
as	the	test	condition	of	a	while	loop.	Because	the	first	argument
is	-1,	the	call	to	
blocks	until	an	arbitrary	child	has	terminated.	As
each	child	terminates,	the	call	to	
returns	with	the	nonzero	PID	of
that	child.	Line	16	checks	the	exit	status	of	the	child.	If	the	child
terminated	normally—in	this	case,	by	calling	the	
function—then	the
parent	extracts	the	exit	status	and	prints	it	on	
.
When	all	of	the	children	have	been	reaped,	the	next	call	to	
returns	-1	and	sets	
to	ECHILD.	Line	24	checks	that	the	
function	terminated	normally,	and	prints	an	error	message	otherwise.</p>
<p>When	we	run	the	program	on	our	Linux	system,	it	produces	the	following
output:
Notice	that	the	program	reaps	its	children	in	no	particular	order.	The
order	that	they	were	reaped	is	a	property	of	this	specific	computer
system.	On	another	system,	or	even	another	execution	on	the	same
system,	the	two	children	might	have	been	reaped	in	the	opposite	order.
This	is	an	example	of	the	
nondeterministic
behavior	that	can	make
reasoning	about	concurrency	so	difficult.	Either	of	the	two	possible
outcomes	is	equally	correct,	and	as	a	programmer	you	may	
never
assume	that	one	outcome	will	always	occur,	no	matter	how	unlikely	the
other	outcome	appears	to	be.	The	only	correct	assumption	is	that	each
possible	outcome	is	equally	likely.
Figure	
8.19	
shows	a	simple	change	that	eliminates	this
nondeterminism	in	the	output	order	by	reaping	the	children	in	the	same
order	that	they	were	created	by	the	parent.	In	line	11,	the	parent	stores
the	PIDs	of	its	children	in	order	and	then	waits	for	each	child	in	this	same
order	by	calling	
with	the	appropriate	PID	in	the	first	argument.
Practice	Problem	
8.4	
(solution	page	
797
)</p>
<h2 id="consider-the-following-program"><a class="header" href="#consider-the-following-program">Consider	the	following	program:</a></h2>
<hr />
<h2 id="codeecfwaitprob1c"><a class="header" href="#codeecfwaitprob1c">code/ecf/waitprob1.c</a></h2>
<hr />
<p>code/ecf/waitprob1.c</p>
<h2>A
.	
How	many	output	lines	does	this	program	generate?
B
.	
What	is	one	possible	ordering	of	these	output	lines?</h2>
<hr />
<p>code/ecf/waitpid2.c</p>
<hr />
<hr />
<p>code/ecf/waitpid2.c
Figure	
8.19	
Using	
to	reap	zombie	children	in	the	order	they
were	created.
8.4.4	
Putting	Processes	to	Sleep
The	sleep	function	suspends	a	process	for	a	specified	period	of	time.</p>
<pre><code>returns	zero	if	the	requested	amount	of	time	has	elapsed,	and	the
</code></pre>
<p>number	of	seconds	still	left	to	sleep	otherwise.	The	latter	case	is	possible
if	the	
function	
returns	prematurely	because	it	was	interrupted	by	a
signal.
We	will	discuss	signals	in	detail	in	
Section	
8.5
.
Another	function	that	we	will	find	useful	is	the	
function,	which	puts
the	calling	function	to	sleep	until	a	signal	is	received	by	the	process.
Practice	Problem	
8.5	
(solution	page	
797
)
Write	a	wrapper	function	for	
,	called	
,	with	the	following
interface:</p>
<p>The	
function	behaves	exactly	as	the	
function,	except	that	it
prints	a	message	describing	how	long	the	process	actually	slept:
8.4.5	
Loading	and	Running
Programs
The	
function	loads	and	runs	a	new	program	in	the	context	of	the
current	process.
The	
function	loads	and	runs	the	executable	object	file	
with	the	argument	list	
and	the	environment	variable	list	
returns	to	the	calling	program	only	if	there	is	an	error,	such	as	not	being
able	to	find	
.	So	unlike	
,	which	is	called	once	but	returns
twice,	
is	called	once	and	never	returns.</p>
<p>The	argument	list	is	represented	by	the	data	structure	shown	in	
Figure
8.20
.	The	
variable	points	to	a	null-terminated	array	of	pointers,
each	of	which	points	to	an	argument	string.	By	convention,	
is
the	name	of	the	executable	object	file.	The	list	of	environment	variables	is
represented	by	a	similar	data	structure,	shown	in	
Figure	
8.21
.	The
variable	points	to	a	null-terminated	array	of	pointers	to	environment
variable	strings,	each	of	which	is	a	name-value	pair	of	the	form
name=value.
Figure	
8.20	
Organization	of	an	argument	list.
Figure	
8.21	
Organization	of	an	environment	variable	list.
After	
loads	
,	it	calls	the	start-up	code	described	in
Section	
7.9
.	The	start-up	code	sets	up	the	stack	and	passes	control	to
the	main	routine	of	the	new	program,	which	has	a	prototype	of	the	form</p>
<p>or	equivalently,
When	
begins	executing,	the	user	stack	has	the	organization	shown
in	
Figure	
8.22
.	Let's	work	our	way	from	the	bottom	of	the	stack	(the
highest	address)	to	the	top	(the	lowest	address).	First	are	the	argument
and	environment	strings.	These	are	followed	further	up	the	stack	by	a
null-terminated	array	of	pointers,	each	of	which	points	to	an	environment
variable	string	on	the	stack.	The	global	variable	
points	to	the	first
of	these	pointers,	
.	The	environment	array	is	followed	by	the
null-terminated	
array,	with	each	element	pointing	to	an	argument
string	on	the	stack.	At	the	top	of	the	stack	is	the	stack	frame	for	the
system	start-up	function,	
(
Section	
7.9
).
There	are	three	arguments	to	function	main,	each	stored	in	a	register
according	to	the	x86-64	stack	discipline:	(1)	
,	which	gives	the
number	of	non-null	pointers	in	the	
array;	(2)	
,	which	points
to	the	first	entry	in	the	
array;	and	(3)	
,	which	points	to	the
first	entry	in	the	
array.
Linux	provides	several	functions	for	manipulating	the	environment	array:</p>
<p>Figure	
8.22	
Typical	organization	of	the	user	stack	when	a	new
program	starts.
The	
function	searches	the	environment	array	for	a	string
If	found,	it	returns	a	pointer	to	
value;
otherwise,	it	returns
.</p>
<p>If	the	environment	array	contains	a	string	of	the	form	
,	then
deletes	it	and	
replaces	
oldvalue
with	
,	but	only
if	
is	nonzero.	If	name	does	not	exist,	then	
adds
to	the	array.
Practice	Problem	</p>
<div style="break-before: page; page-break-before: always;"></div><p>8.6	
(solution	page	
797
)
Write	a	program	called	
that	prints	its	command-line
arguments	and	environment	variables.	For	example:
⋮</p>
<p>8.4.6	
Using	
and	
to
Run	Programs
Programs	such	as	Unix	shells	and	Web	servers	make	heavy	use	of	the
and	
functions.	A	
shell
is	an	interactive	application-level
program	that	runs	other	programs	on	behalf	of	the	user.	The	original	shell
was	the	
program,	which	was	followed	by	variants	such	as	
,	and	
.	A	shell	performs	a	sequence	of	
read/evaluate
steps	and
then	terminates.	The	read	step	reads	a	command	line	from	the	user.	The
evaluate	step	parses	the	command	line	and	runs	programs	on	behalf	of
the	user.
Figure	
8.23	
shows	the	main	routine	of	a	simple	shell.	The	shell	prints
a	command-line	prompt,	waits	for	the	user	to	type	a	command	line	on
,	and	then	evaluates	the	command	line.
Figure	
8.24	
shows	the	code	that	evaluates	the	command	line.	Its	first
task	is	to	call	the	
function	(
Figure	
8.25
),	which	parses	the
space-separated	command-line	arguments	and	builds	the	
vector
that	will	eventually	be	passed	to	
.	The	first	argument	is	assumed	to</p>
<h2>be	either	the	name	of	a	built-in	shell	command	that	is	interpreted
immediately,	or	an	executable	object	file	that	will	be	loaded	and	run	in	the
context	of	a	new	child	process.
If	the	last	argument	is	an	‘&amp;’	character,	then	
returns	1,
indicating	that	the	program	should	be	executed	in	the	
background
(the
shell	does	not	wait	for	it	to	complete).	Otherwise,	it	returns	0,	indicating
that	the	program	should	be	run	in	the	
foreground
(the	shell	waits	for	it	to
complete).
Aside	
Programs	versus	processes
This	is	a	good	place	to	pause	and	make	sure	you	understand	the
distinction	between	a	program	and	a	process.	A	program	is	a
collection	of	code	and	data;	programs	can	exist	as	object	files	on
disk	or	as	segments	in	an	address	space.	A	process	is	a	specific
instance	of	a	program	in	execution;	a	program	always	runs	in	the
context	of	some	process.	Understanding	this	distinction	is
important	if	you	want	to	understand	the	
and	
functions.	The	
function	runs	the	same	program	in	a	new	child
process	that	is	a	duplicate	of	the	parent.	The	
function
loads	and	runs	a	new	program	in	the	context	of	the	current
process.	While	it	overwrites	the	address	space	of	the	current
process,	it	does	
not
create	a	new	process.	The	new	program	still
has	the	same	PID,	and	it	inherits	all	of	the	file	descriptors	that
were	open	at	the	time	of	the	call	to	the	
function.</h2>
<hr />
<p>code/ecf/shellex.c</p>
<hr />
<hr />
<p>code/ecf/shellex.c</p>
<h2>Figure	
8.23	
The	main	routine	for	a	simple	shell	program.
After	parsing	the	command	line,	the	
function	calls	the
function,	which	checks	whether	the	first	command-line
argument	is	a	built-in	shell	command.	If	so,	it	interprets	the	command
immediately	and	returns	1.	Otherwise,	it	returns	0.	Our	simple	shell	has
just	one	built-in	command,	the	
command,	which	terminates	the
shell.	Real	shells	have	numerous	commands,	such	as	
,	and	
.
If	
returns	0,	then	the	shell	creates	a	child	process	and
executes	the	requested	program	inside	the	child.	If	the	user	has	asked
for	the	program	to	run	in	the	background,	then	the	shell	returns	to	the	top
of	the	loop	and	waits	for	the	next	command	line.	Otherwise	the	shell	uses
the	
function	to	wait	for	the	job	to	terminate.	When	the	job
terminates,	the	shell	goes	on	to	the	next	iteration.
Notice	that	this	simple	shell	is	flawed	because	it	does	not	reap	any	of	its
background	children.	Correcting	this	flaw	requires	the	use	of	signals,
which	we	describe	in	the	next	section.</h2>
<hr />
<p>code/ecf/shellex.c</p>
<hr />
<hr />
<h2>code/ecf/shellex.c
Figure	
8.24	
evaluates	the	shell	command	line.</h2>
<hr />
<p>code/ecf/shellex.c</p>
<hr />
<hr />
<p>code/ecf/shellex.c
Figure	
8.25	
parses	a	line	of	input	for	the	shell.</p>
<p>8.5	
Signals
To	this	point	in	our	study	of	exceptional	control	flow,	we	have	seen	how
hardware	and	software	cooperate	to	provide	the	fundamental	low-level
exception	mechanism.	We	have	also	seen	how	the	operating	system
uses	exceptions	to	support	a	form	of	exceptional	control	flow	known	as
the	process	context	switch.	In	this	section,	we	will	study	a	higher-level
software	form	of	exceptional	control	flow,	known	as	a	Linux	signal,	that
allows	processes	and	the	kernel	to	interrupt	other	processes.
Number
Name
Default	action
Corresponding	event
1
SIGHUP
Terminate
Terminal	line	hangup
2
SIGINT
Terminate
Interrupt	from	keyboard
3
SIGQUIT
Terminate
Quit	from	keyboard
4
SIGILL
Terminate
Illegal	instruction
5
SIGTRAP
Terminate	and	dump
core
Trace	trap
6
SIGABRT
Terminate	and	dump
core
Abort	signal	from	abort	function
7
SIGBUS
Terminate
Bus	error
8
SIGFPE
Terminate	and	dump
core
Floating-point	exception
a
a
a
b</p>
<p>9
SIGKILL
Terminate
Kill	program
10
SIGUSR1
Terminate
User-defined	signal	1
11
SIGSEGV
Terminate	and	dump
core
Invalid	memory	reference	(seg	fault)
12
SIGUSR2
Terminate
User-defined	signal	2
13
SIGPIPE
Terminate
Wrote	to	a	pipe	with	no	reader
14
SIGALRM
Terminate
Timer	signal	from	alarm	function
15
SIGTERM
Terminate
Software	termination	signal
16
SIGSTKFLT
Terminate
Stack	fault	on	coprocessor
17
SIGCHLD
Ignore
A	child	process	has	stopped	or
terminated
18
SIGCONT
Ignore
Continue	process	if	stopped
19
SIGSTOP
Stop	until	next
SIGCONT
Stop	signal	not	from	terminal
20
SIGTSTP
Stop	until	next
SIGCONT
Stop	signal	from	terminal
21
SIGTTIN
Stop	until	next
SIGCONT
Background	process	read	from
terminal
22
SIGTTOU
Stop	until	next
SIGCONT
Background	process	wrote	to	terminal
23
SIGURG
Ignore
Urgent	condition	on	socket
24
SIGXCPU
Terminate
CPU	time	limit	exceeded
b
a
b</p>
<p>25
SIGXFSZ
Terminate
File	size	limit	exceeded
26
SIGVTALRM
Terminate
Virtual	timer	expired
27
SIGPROF
Terminate
Profiling	timer	expired
28
SIGWINCH
Ignore
Window	size	changed
29
SIGIO
Terminate
I/O	now	possible	on	a	descriptor
30
SIGPWR
Terminate
Power	failure
Figure	
8.26	
Linux	signals.
Notes:	
(a)	Years	ago,	main	memory	was	implemented	with	a	technology
known	as	
core	memory.
“Dumping	core”	is	a	historical	term	that	means
writing	an	image	of	the	code	and	data	memory	segments	to	disk,	(b)	This
signal	can	be	neither	caught	nor	ignored.
(
Source:</p>
<p>.	Data	from	the	Linux	Foundation.)
A	
signal
is	a	small	message	that	notifies	a	process	that	an	event	of	some
type	has	occurred	in	the	system.	
Figure	
8.26	
shows	the	30	different
types	of	signals	that	are	supported	on	Linux	systems.
Each	signal	type	corresponds	to	some	kind	of	system	event.	Low-level
hardware	exceptions	are	processed	by	the	kernel's	exception	handlers
and	would	not	normally	be	visible	to	user	processes.	Signals	provide	a
mechanism	for	exposing	
the	occurrence	of	such	exceptions	to	user
processes.	For	example,	if	a	process	attempts	to	divide	by	zero,	then	the
kernel	sends	it	a	SIGFPE	signal	(number	8).	If	a	process	executes	an
illegal	instruction,	the	kernel	sends	it	a	SIGILL	signal	(number	4).	If	a
process	makes	an	illegal	memory	reference,	the	kernel	sends	it	a</p>
<p>SIGSEGV	signal	(number	11).	Other	signals	correspond	to	higher-level
software	events	in	the	kernel	or	in	other	user	processes.	For	example,	if
you	type	Ctrl+C	(i.e.,	press	the	Ctrl	key	and	the	‘c’	key	at	the	same	time)
while	a	process	is	running	in	the	foreground,	then	the	kernel	sends	a
SIGINT	(number	2)	to	each	process	in	the	foreground	process	group.	A
process	can	forcibly	terminate	another	process	by	sending	it	a	SIGKILL
signal	(number	9).	When	a	child	process	terminates	or	stops,	the	kernel
sends	a	SIGCHLD	signal	(number	17)	to	the	parent.
8.5.1	
Signal	Terminology
The	transfer	of	a	signal	to	a	destination	process	occurs	in	two	distinct
steps:
Sending	a	signal.	
The	kernel	
sends	(delivers)
a	signal	to	a
destination	process	by	updating	some	state	in	the	context	of	the
destination	process.	The	signal	is	delivered	for	one	of	two	reasons:
(1)	The	kernel	has	detected	a	system	event	such	as	a	divide-by-zero
error	or	the	termination	of	a	child	process.	(2)	A	process	has	invoked
the	
function	(discussed	in	the	next	section)	to	explicitly	request
the	kernel	to	send	a	signal	to	the	destination	process.	A	process	can
send	a	signal	to	itself.
Receiving	a	signal.	
A	destination	process	
receives
a	signal	when	it	is
forced	by	the	kernel	to	react	in	some	way	to	the	delivery	of	the	signal.
The	process	can	either	ignore	the	signal,	terminate,	or	
catch
the
signal	by	executing	a	user-level	function	called	a	
signal	handler.
Figure	
8.27
shows	the	basic	idea	of	a	handler	catching	a	signal.</p>
<p>A	signal	that	has	been	sent	but	not	yet	received	is	called	
spending	signal.
At	any	point	in	time,	there	can	be	at	most	one	pending	signal	of	a
particular	type.	If	a	process	has	a	pending	signal	of	type	
k
,	then	any
subsequent	signals	of	type	
k
sent	to	that	process	are	
not
queued;	they
are	simply	discarded.	A	process	can	selectively	
block
the	receipt	of
certain	signals.	When	a	signal	is	blocked,	it	can	be
Figure	
8.27	
Signal	handling.
Receipt	of	a	signal	triggers	a	control	transfer	to	a	signal	handler.	After	it
finishes	processing,	the	handler	returns	control	to	the	interrupted
program.
delivered,	but	the	resulting	pending	signal	will	not	be	received	until	the
process	unblocks	the	signal.
A	pending	signal	is	received	at	most	once.	For	each	process,	the	kernel
maintains	the	set	of	pending	signals	in	the	
bit	vector,	and	the	set
of	blocked	signals	in	the	
bit	vector.
The	kernel	sets	bit	
k
in
whenever	a	signal	of	type	
k
is	delivered	and	clears	bit	
k
in
whenever	a	signal	of	type	
k
is	received.</p>
<ol>
<li></li>
</ol>
<p>Also	known	as	the	
signal	mask.
1</p>
<p>8.5.2	
Sending	Signals
Unix	systems	provide	a	number	of	mechanisms	for	sending	signals	to
processes.	All	of	the	mechanisms	rely	on	the	notion	of	a	
process	group.
Process	Groups
Every	process	belongs	to	exactly	one	
process	group
,	which	is	identified
by	a	positive	
integer	process	group	ID.
The	
function	returns	the
process	group	ID	of	the	current	process.
By	default,	a	child	process	belongs	to	the	same	process	group	as	its
parent.	A	process	can	change	the	process	group	of	itself	or	another
process	by	using	the	
function:</p>
<p>The	
function	changes	the	process	group	of	process	
to	
.
If	
is	zero,	the	PID	of	the	current	process	is	used.	If	
is	zero,	the
PID	of	the	process	specified	by	
is	used	for	the	process	group	ID.	For
example,	if	process	15213	is	the	calling	process,	then
creates	a	new	process	group	whose	process	group	ID	is	15213,	and
adds	process	15213	to	this	new	group.
Sending	Signals	with	the	
Program
The	
program	sends	an	arbitrary	signal	to	another	process.	For
example,	the	command
sends	signal	9	(SIGKILL)	to	process	15213.	A	negative	PID	causes	the
signal	to	be	sent	to	every	process	in	process	group	PID.	For	example,
the	command</p>
<p>sends	a	SIGKILL	signal	to	every	process	in	process	group	15213.	Note
that	we	use	the	complete	path	
here	because	some	Unix	shells
have	their	own	built-in	
command.
Sending	Signals	from	the	Keyboard
Unix	shells	use	the	abstraction	of	a	
job
to	represent	the	processes	that
are	created	as	a	result	of	evaluating	a	single	command	line.	At	any	point
in	time,	there	is	at	most	one	foreground	job	and	zero	or	more	background
jobs.	For	example,	typing
creates	a	foreground	job	consisting	of	two	processes	connected	by	a
Unix	pipe:	one	running	the	
program,	the	other	running	the	
program.	The	shell	creates	a	separate	process	group	for	each	job.
Typically,	the	process	group	ID	is	taken	from	one	of	the	parent	processes
in	the	job.	For	example,	
Figure	
8.28
shows	a	shell	with	one
foreground	job	and	two	background	jobs.	The	parent	process	in	the
foreground	job	has	a	PID	of	20	and	a	process	group	ID	of	20.	The	parent
process	has	created	two	children,	each	of	which	are	also	members	of
process	group	20.</p>
<p>Figure	
8.28	
Foreground	and	background	process	groups.
Typing	Ctrl+C	at	the	keyboard	causes	the	kernel	to	send	a	SIGINT	signal
to	every	process	in	the	foreground	process	group.	In	the	default	case,	the
result	is	to	terminate	the	foreground	job.	Similarly,	typing	Ctrl+Z	causes
the	kernel	to	send	a	SIGTSTP	signal	to	every	process	in	the	foreground
process	group.	In	the	default	case,	the	result	is	to	stop	(suspend)	the
foreground	job.
Sending	Signals	with	the	
Function
Processes	send	signals	to	other	processes	(including	themselves)	by
calling	the	
function.</p>
<h2>If	
is	greater	than	zero,	then	the	
function	sends	signal	number
to	process	
.	If	
is	equal	to	zero,	then	
sends	signal	
to
every	process	in	the	process	group	of	the	calling	process,	including	the
calling	process	itself.	If	
is	less	than	zero,	then	
sends	signal	
to	every	process	in	process	group	
(the	absolute	value	of	
).
Figure	
8.29
shows	an	example	of	a	parent	that	uses	the	
function
to	send	a	SIGKILL	signal	to	its	child.</h2>
<hr />
<p>code/ecf/kill.c</p>
<hr />
<hr />
<p>code/ecf/kill.c
Figure	
8.29	
Using	the	
function	to	send	a	signal	to	a	child.
Sending	Signals	with	the	
Function
A	process	can	send	SIGALRM	signals	to	itself	by	calling	the	
function.</p>
<p>The	
function	arranges	for	the	kernel	to	send	a	SIGALRM	signal	to
the	calling	process	in	
seconds.	If	
is	0,	then	no	new	alarm	is
scheduled.	In	any	event,	the	call	to	
cancels	any	pending	alarms
and	returns	the	number	of	seconds	remaining	until	any	pending	alarm
was	due	to	be	delivered	(had	not	this	call	to	
canceled	it),	or	0	if
there	were	no	pending	alarms.
8.5.3	
Receiving	Signals
When	the	kernel	switches	a	process	
p
from	kernel	mode	to	user	mode
(e.g.,	returning	from	a	system	call	or	completing	a	context	switch),	it
checks	the	set	of	unblocked	pending	signals	(
)	for	
p.
If
this	set	is	empty	(the	usual	case),	then	the	kernel	passes	control	to	the
next	instruction	(
I
)	in	the	logical	control	flow	of	
p.
However,	if	the	set	is
nonempty,	then	the	kernel	chooses	some	signal	
k
in	the	set	(typically	the
smallest	
k
)	and	forces	
p
to	
receive
signal	
k.
The	receipt	of	the	signal
triggers	some	
action
by	the	process.	Once	the	process	completes	the
action,	then	control	passes	back	to	the	next	instruction	(
I
)	in	the	logical
control	flow	of	
p.
Each	signal	type	has	a	predefined	
default	action
,	which
is	one	of	the	following:
The	process	terminates.
The	process	terminates	and	dumps	core.
The	process	stops	(suspends)	until	restarted	by	a	SIGCONT	signal.
The	process	ignores	the	signal.
next
next</p>
<p>Figure	
8.26	
shows	the	default	actions	associated	with	each	type	of
signal.	For	example,	the	default	action	for	the	receipt	of	a	SIGKILL	is	to
terminate	the	receiving	process.	On	the	other	hand,	the	default	action	for
the	receipt	of	a	SIGCHLD	is	to	ignore	the	signal.	A	process	can	modify
the	default	action	associated	with	a	signal	by	using	the	
function.
The	only	exceptions	are	SIGSTOP	and	SIGKILL,	whose	default	actions
cannot	be	changed.
The	
function	can	change	the	action	associated	with	a	signal
in	one	of	three	ways:
If	
is	SIG_IGN,	then	signals	of	type	
are	ignored.
If	
is	SIG_DFL,	then	the	action	for	signals	of	type	
reverts	to	the	default	action.
Otherwise,	
is	the	address	of	a	user-defined	function,	called	a
signal	handler
,	that	will	be	called	whenever	the	process	receives	a
signal	of	type	
.	Changing	the	default	action	by	passing	the
address	of	a	handler	to	the	
function	is	known	as	
installing	the</p>
<h2>handler.
The	invocation	of	the	handler	is	called	
catching	the	signal.
The	execution	of	the	handler	is	referred	to	as	
handling	the	signal.
When	a	process	catches	a	signal	of	type	
k
,	the	handler	installed	for
signal	
k
is	invoked	with	a	single	integer	argument	set	to	
k.
This	argument
allows	the	same	handler	function	to	catch	different	types	of	signals.
When	the	handler	executes	its	
statement,	control	(usually)	passes
back	to	the	instruction	in	the	control	flow	where	the	process	was
interrupted	by	the	receipt	of	the	signal.	We	say	“usually”	because	in	some
systems,	interrupted	system	calls	return	immediately	with	an	error.
Figure	
8.30
shows	a	program	that	catches	the	SIGINT	signal	that	is
sent	whenever	the	user	types	Ctrl+C	at	the	keyboard.	The	default	action
for	SIGINT</h2>
<hr />
<p>code/ecf/sigint.c</p>
<hr />
<hr />
<p>code/ecf/sigint.c
Figure	
8.30	
A	program	that	uses	a	signal	handler	to	catch	a	SIGINT
signal.
Figure	
8.31	
Handlers	can	be	interrupted	by	other	handlers.
is	to	immediately	terminate	the	process.	In	this	example,	we	modify	the
default	behavior	to	catch	the	signal,	print	a	message,	and	then	terminate</p>
<p>the	process.
Signal	handlers	can	be	interrupted	by	other	handlers,	as	shown	in	
Figure
8.31
.	In	this	example,	the	main	program	catches	signal	
s
,	which
interrupts	the	main	program	and	transfers	control	to	handler	
S.
While	
S
is
running,	the	program	catches	signal	
t
≠	
s
,	which	interrupts	
S
and
transfers	control	to	handler	
T.
When	
T
returns,	
S
resumes	where	it	was
interrupted.	Eventually,	
S
returns,	transferring	control	back	to	the	main
program,	which	resumes	where	it	left	off.
Practice	Problem	
8.7	
(solution	page	
798
)
Write	a	program	called	
that	takes	a	single	command-line
argument,	calls	the	
function	from	
Problem	
8.5
with	this
argument,	and	then	terminates.	Write	your	program	so	that	the
user	can	interrupt	the	
function	by	typing	Ctrl+C	at	the
keyboard.	For	example:
8.5.4	
Blocking	and	Unblocking</p>
<p>Signals
Linux	provides	implicit	and	explicit	mechanisms	for	blocking	signals:
Implicit	blocking	mechanism.	
By	default,	the	kernel	blocks	any
pending	signals	of	the	type	currently	being	processed	by	a	handler.
For	example,	in	
Figure	
8.31
,	suppose	the	program	has	caught	signal
s
and	is	currently	running	handler	
S.
If	another	signal	
s
is	sent	to	the
process,	then	
s
will	become	pending	but	will	not	be	received	until	after
handler	
S
returns.
Explicit	blocking	mechanism.	
Applications	can	explicitly	block	and
unblock	selected	signals	using	the	
function	and	its
helpers.</p>
<p>The	
function	changes	the	set	of	currently	
signals
(the	
bit	vector	described	in	
Section	
8.5.1
).	The	specific
behavior	depends	on	the	value	of	
:
SIG_BLOCK.	Add	the	signals	in	
to	blocked	(
).
SIG_UNBLOCK.	Remove	the	signals	in	
from	
.
SIG_SETMASK.	
If	
is	non-NULL,	the	previous	value	of	the	
bit	vector	is
stored	in	
.
Signal	sets	such	as	
are	manipulated	using	the	following	functions:
The	
initializes	
to	the	empty	set.	The	
function
adds	every	signal	to	
.	The	
function	adds	
to	
deletes	
from	
,	and	
returns	1	if	
is
a	member	of	
,	and	0	if	not.
For	example,	
Figure	
8.32
shows	how	you	would	use	
to
temporarily	block	the	receipt	of	SIGINT	signals.</p>
<p>⋮
Figure	
8.32	
Temporarily	blocking	a	signal	from	being	received.
8.5.5	
Writing	Signal	Handlers
Signal	handling	is	one	of	the	thornier	aspects	of	Linux	system-level
programming.	Handlers	have	several	attributes	that	make	them	difficult	to
reason	about:	(1)	Handlers	run	concurrently	with	the	main	program	and
share	the	same	global	variables,	and	thus	can	interfere	with	the	main
program	and	with	other	handlers.	(2)	The	rules	for	how	and	when	signals
are	received	is	often	counterintuitive.	(3)	Different	systems	can	have
different	signal-handling	semantics.
In	this	section,	we	address	these	issues	and	give	you	some	basic
guidelines	for	writing	safe,	correct,	and	portable	signal	handlers.
Safe	Signal	Handling</p>
<p>Signal	handlers	are	tricky	because	they	can	run	concurrently	with	the
main	program	and	with	each	other,	as	we	saw	in	
Figure	
8.31
.	If	a	handler
and	the	main	program	access	the	same	global	data	structure
concurrently,	then	the	results	can	be	unpredictable	and	often	fatal.
We	will	explore	concurrent	programming	in	detail	in	
Chapter	
12
.	Our
aim	here	is	to	give	you	some	conservative	guidelines	for	writing	handlers
that	are	safe	to	run	concurrently.	If	you	ignore	these	guidelines,	you	run
the	risk	of	introducing	subtle	concurrency	errors.	With	such	errors,	your
program	works	correctly	most	of	the	time.	However,	when	it	fails,	it	fails	in
unpredictable	and	unrepeatable	ways	that	are	horrendously	difficult	to
debug.	Forewarned	is	forearmed!
G0.	Keep	handlers	as	simple	as	possible.	
The	best	way	to	avoid
trouble	is	to	keep	your	handlers	as	small	and	simple	as	possible.	For
example,	the	handler	might	simply	set	a	global	flag	and	return
immediately;	all	processing	associated	with	the	receipt	of	the	signal	is
performed	by	the	main	program,	which	periodically	checks	(and
resets)	the	flag.
G1.	Call	only	async-signal-safe	functions	in	your	handlers.	
A
function	that	is	
async-signal-safe
,	or	simply	
safe
,	has	the	property	that
it	can	be	safely	called	from	a	signal	handler,	either	because	it	is
reentrant
(e.g.,	accesses	only	local	variables;	see	
Section	
12.7.2
),
or	because	it	cannot	be	interrupted	by	a	signal	handler.	
Figure	
8.33
lists	the	system-level	functions	that	Linux	guarantees	to	be	safe.
Notice	that	many	popular	functions,	such	as	
,
and	
,	are	
not
on	this	list.</p>
<p>The	only	safe	way	to	generate	output	from	a	signal	handler	is	to	use
the	
function	(see	
Section	
10.1
).	In	particular,	calling	
or	
is	unsafe.	To	work	around	this	unfortunate	restriction,	we
have	developed	some	safe	functions,	called	the	S
IO
(Safe	I/O)
package,	that	you	can	use	to	print	simple	messages	from	signal
handlers.</p>
<p>Figure	
8.33	
Async-signal-safe	functions.
(
Source:	
signal.	Data	from	the	Linux	Foundation.)
The	
and	
functions	emit	a	long	and	a	string,
respectively,	to	standard	output.	The	
function	prints	an	error
message	and	terminates.
Figure	
8.34
shows	the	implementation	of	the	S
IO
package,	which
uses	two	private	reentrant	functions	from	
.	The	
function	in	line	3	returns	the	length	of	string	
.	The	
function
in	line	10,	which	is	based	on	the	
function	from	[
61
],	converts	
to	its	base	
string	representation	in	
.	The	
function	in	line	17
is	an	async-signal-safe	variant	of	
.
Figure	
8.35	
shows	a	safe	version	of	the	SIGINT	handler	from
Figure	
8.30
.</p>
<p>G2.	Save	and	restore</p>
<h2>.	
Many	of	the	Linux	async-signal-safe
functions	set	
when	they	return	with	an	error.	Calling	such
functions	inside	a	handler	might	interfere	with	other	parts	of	the
program	that	rely	on	
.</h2>
<hr />
<p>code/src/csapp.c</p>
<hr />
<hr />
<p>code/src/csapp.c
Figure	
8.34	
The	
(Safe	I/O)	package	for	signal	handlers.
Figure	
8.35	
A	safe	version	of	the	SICINT	handler	from	
Figure
8.30
.
The	workaround	is	to	save	
to	a	local	variable	on	entry	to	the
handler	and	restore	it	before	the	handler	returns.	Note	that	this	is	only
necessary	if	the	handler	returns.	It	is	not	necessary	if	the	handler
terminates	the	process	by	calling	
.
G3.	Protect	accesses	to	shared	global	data	structures	by
blocking	all	signals.	
If	a	handler	shares	a	global	data	structure	with
the	main	program	or	with	other	handlers,	then	your	handlers	and	main
program	should	temporarily	block	all	signals	while	accessing	(reading</p>
<p>or	writing)	that	data	structure.	The	reason	for	this	rule	is	that
accessing	a	data	structure	
d
from	the	main	program	typically	requires
a	sequence	of	instructions.	If	this	instruction	sequence	is	interrupted
by	a	handler	that	accesses	
d
,	then	the	handler	might	find	
d
in	an
inconsistent	state,	with	unpredictable	results.	Temporarily	blocking
signals	while	you	access	
d
guarantees	that	a	handler	will	not	interrupt
the	instruction	sequence.
G4.	Declare	global	variables	with	
.	
Consider	a	handler	and
routine	that	share	a	global	variable	
g.
The	handler	updates	
g
,
and	
periodically	reads	
g.
To	an	optimizing	compiler,	it	would
appear	that	the	value	of	
g
never	changes	in	
,	and	thus	it	would
be	safe	to	use	a	copy	of	
g
that	is	cached	in	a	register	to	satisfy	every
reference	to	
g.
In	this	case,	the	
function	would	never	see	the
updated	values	from	the	handler.
You	can	tell	the	compiler	not	to	cache	a	variable	by	declaring	it	with
the	
type	qualifier.	For	example:
The	
qualifier	forces	the	compiler	to	read	the	value	of	
from
memory	each	time	it	is	referenced	in	the	code.	In	general,	as	with	any
shared	data	structure,	each	access	to	a	global	variable	should	be
protected	by	temporarily	blocking	signals.
G5.	Declare	flags	with	
.	
In	one	common	handler	design,
the	handler	records	the	receipt	of	the	signal	by	writing	to	a	global	
flag.
The	main	program	periodically	reads	the	flag,	responds	to	the	signal,</p>
<p>and	
clears	the	flag.	For	flags	that	are	shared	in	this	way,	C	provides
an	integer	data	type,	
,	for	which	reads	and	writes	are
guaranteed	to	be	
atomic
(uninterruptible)	because	they	can	be
implemented	with	a	single	instruction:
Since	they	can't	be	interrupted,	you	can	safely	read	from	and	write	to
variables	without	temporarily	blocking	signals.	Note	that
the	guarantee	of	atomicity	only	applies	to	individual	reads	and	writes.
It	does	not	apply	to	updates	such	as	
or	
,
which	might	require	multiple	instructions.
Keep	in	mind	that	the	guidelines	we	have	presented	are	conservative,	in
the	sense	that	they	are	not	always	strictly	necessary.	For	example,	if	you
know	that	a	handler	can	never	modify	
,	then	you	don't	need	to	save
and	restore	
.	Or	if	you	can	prove	that	no	instance	of	
can
ever	be	interrupted	by	a	handler,	then	it	is	safe	to	call	
from	the
handler.	The	same	holds	for	accesses	to	shared	global	data	structures.
However,	it	is	very	difficult	to	prove	such	assertions	in	general.	So	we
recommend	that	you	take	the	conservative	approach	and	follow	the
guidelines	by	keeping	your	handlers	as	simple	as	possible,	calling	safe
functions,	saving	and	restoring	
,	protecting	accesses	to	shared	data
structures,	and	using	
and	
.
Correct	Signal	Handling</p>
<h2>One	of	the	nonintuitive	aspects	of	signals	is	that	pending	signals	are	not
queued.	Because	the	
bit	vector	contains	exactly	one	bit	for	each
type	of	signal,	there	can	be	at	most	one	pending	signal	of	any	particular
type.	Thus,	if	two	signals	of	type	
k
are	sent	to	a	destination	process	while
signal	
k
is	blocked	because	the	destination	process	is	currently	executing
a	handler	for	signal	
k
,	then	the	second	signal	is	simply	discarded;	it	is	not
queued.	The	key	idea	is	that	the	existence	of	a	pending	signal	merely
indicates	that	
at	least
one	signal	has	arrived.
To	see	how	this	affects	correctness,	let's	look	at	a	simple	application	that
is	similar	in	nature	to	real	programs	such	as	shells	and	Web	servers.	The
basic	structure	is	that	a	parent	process	creates	some	children	that	run
independently	for	a	while	and	then	terminate.	The	parent	must	reap	the
children	to	avoid	leaving	zombies	in	the	system.	But	we	also	want	the
parent	to	be	free	to	do	other	work	while	the	children	are	running.	So	we
decide	to	reap	the	children	with	a	SIGCHLD	handler,	instead	of	explicitly
waiting	for	the	children	to	terminate.	(Recall	that	the	kernel	sends	a
SIGCHLD	signal	to	the	parent	whenever	one	of	its	children	terminates	or
stops.)
Figure	
8.36	
shows	our	first	attempt.	The	parent	installs	a	SIGCHLD
handler	and	then	creates	three	children.	In	the	meantime,	the	parent
waits	for	a	line	of	input	from	the	terminal	and	then	processes	it.	This
processing	is	modeled	by	an	infinite	loop.	When	each	child	terminates,
the	kernel	notifies	the	parent	by	sending	it	a	SIGCHLD	signal.	The	parent
catches	the	SIGCHLD,	reaps	one	child,</h2>
<hr />
<p>code/ecf/signal1.	c</p>
<hr />
<hr />
<p>code/ecf/signal1.	c
Figure	
8.36	
.	This	program	is	flawed	because	it	assumes	that
signals	are	queued.
does	some	additional	cleanup	work	(modeled	by	the	
statement),
and	then	returns.
The	
program	in	
Figure	
8.36	
seems	fairly	straightforward.
When	we	run	it	on	our	Linux	system,	however,	we	get	the	following
output:</p>
<p>From	the	output,	we	note	that	although	three	SIGCHLD	signals	were	sent
to	the	parent,	only	two	of	these	signals	were	received,	and	thus	the
parent	only	reaped	two	children.	If	we	suspend	the	parent	process,	we
see	that,	indeed,	child	process	14075	was	never	reaped	and	remains	a
zombie	(indicated	by	the	string	
in	the	output	of	the	
command):
⋮</p>
<h2>What	went	wrong?	The	problem	is	that	our	code	failed	to	account	for	the
fact	that	signals	are	not	queued.	Here's	what	happened:	The	first	signal
is	received	and	caught	by	the	parent.	While	the	handler	is	still	processing
the	first	signal,	the	second	signal	is	delivered	and	added	to	the	set	of
pending	signals.	However,	since	SIGCHLD	signals	are	blocked	by	the
SIGCHLD	handler,	the	second	signal	is	not	received.	Shortly	thereafter,
while	the	handler	is	still	processing	the	first	signal,	the	third	signal	arrives.
Since	there	is	already	a	pending	SIGCHLD,	this	third	SIGCHLD	signal	is
discarded.	Sometime	later,	after	the	handler	has	returned,	the	kernel
notices	that	there	is	a	pending	SIGCHLD	signal	and	forces	the	parent	to
receive	the	signal.	The	parent	catches	the	signal	and	executes	the
handler	a	second	time.	After	the	handler	finishes	processing	the	second
signal,	there	are	no	more	pending	SIGCHLD	signals,	and	there	never	will
be,	because	all	knowledge	of	the	third	SIGCHLD	has	been	lost.	
The
crucial	lesson	is	that	signals	cannot	be	used	to	count	the	occurrence	of
events	in	other	processes.
To	fix	the	problem,	we	must	recall	that	the	existence	of	a	pending	signal
only	implies	that	at	least	one	signal	has	been	delivered	since	the	last	time
the	process	received	a	signal	of	that	type.	So	we	must	modify	the
SIGCHLD	handler	to	reap</h2>
<hr />
<p>code/ecf/signal2.c</p>
<hr />
<hr />
<p>code/ecf/signal2.c
Figure	
8.37	
signal2.	An	improved	version	of	
Figure	
8.36	
that
correctly	accounts	for	the	fact	that	signals	are	not	queued.
as	many	zombie	children	as	possible	each	time	it	is	invoked.	
Figure	
8.37
shows	the	modified	SIGCHLD	handler.
When	we	run	
on	our	Linux	system,	it	now	correctly	reaps	all	of
the	zombie	children:</p>
<h2>Practice	Problem	
8.8	
(solution	page	
799
)
What	is	the	output	of	the	following	program?</h2>
<hr />
<p>code/ecf/signalprob0.c</p>
<hr />
<hr />
<h2>code/ecf/signalprob0.c
Portable	Signal	Handling
Another	ugly	aspect	of	Unix	signal	handling	is	that	different	systems	have
different	signal-handling	semantics.	For	example:
The	semantics	of	the	
function	varies.	
Some	older	Unix
systems	restore	the	action	for	signal	
k
to	its	default	after	signal	
k
has
been	caught	by	a	handler.	On	these	systems,	the	handler	must
explicitly	reinstall	itself,	by	calling	
,	each	time	it	runs.
System	calls	can	be	interrupted.	
System	calls	such	as	
,
and	
that	can	potentially	block	the	process	for	a	long	period	of
time	are	called	
slow	system	calls.
On	some	older	versions	of	Unix,
slow	system	calls	that	are	interrupted	when	a	handler	catches	a	signal
do	not	resume	when	the	signal	handler	returns	but	instead	return
immediately	to	the	user	with	an	error	condition	and	
set	to
EINTR.	On	these	systems,	programmers	must	include	code	that
manually	restarts	interrupted	system	calls.</h2>
<hr />
<p>code/src/csapp.c</p>
<hr />
<hr />
<p>code/src/csapp.c
Figure	
8.38	
A	wrapper	for	
that	provides	portable
signal	handling	on	Posix-compliant	systems.
To	deal	with	these	issues,	the	Posix	standard	defines	the	
function,	which	allows	users	to	clearly	specify	the	signal-handling
semantics	they	want	when	they	install	a	handler.</p>
<p>The	
function	is	unwieldy	because	it	requires	the	user	to	set	the
entries	of	a	complicated	structure.	A	cleaner	approach,	originally
proposed	by	W.	Richard	Stevens	[
110
],	is	to	define	a	wrapper	function,
called	
,	that	calls	
for	us.	
Figure	
8.38	
shows	the
definition	of	
,	which	is	invoked	in	the	same	way	as	the	
function.
The	
wrapper	installs	a	signal	handler	with	the	following	signal-
handling	semantics:
Only	signals	of	the	type	currently	being	processed	by	the	handler	are
blocked.
As	with	all	signal	implementations,	signals	are	not	queued.
Interrupted	system	calls	are	automatically	restarted	whenever
possible.
Once	the	signal	handler	is	installed,	it	remains	installed	until	
is
called	with	a	
argument	of	either	SIG_IGN	or	SIG_DFL.
We	will	use	the	
wrapper	in	all	of	our	code.
8.5.6	
Synchronizing	Flows	to	Avoid
Nasty	Concurrency	Bugs</p>
<p>The	problem	of	how	to	program	concurrent	flows	that	read	and	write	the
same	storage	locations	has	challenged	generations	of	computer
scientists.	In	general,	the	number	of	potential	interleavings	of	the	flows	is
exponential	in	the	number	of	instructions.	Some	of	those	interleavings	will
produce	correct	answers,	and	others	will	not.	The	fundamental	problem	is
to	somehow	
synchronize
the	concurrent	flows	so	as	to	allow	the	largest
set	of	feasible	interleavings	such	that	each	of	the	feasible	interleavings
produces	a	correct	answer.
Concurrent	programming	is	a	deep	and	important	problem	that	we	will
discuss	in	more	detail	in	
Chapter	
12
.	However,	we	can	use	what
you've	learned	about	exceptional	control	flow	in	this	chapter	to	give	you	a
sense	of	the	interesting	intellectual	challenges	associated	with
concurrency.	For	example,	consider	the	program	in	
Figure	
8.39
,	which
captures	the	structure	of	a	typical	Unix	shell.	The	parent	keeps	track	of	its
current	children	using	entries	in	a	global	job	list,	with	one	entry	per	job.
The	
and	
functions	add	and	remove	entries	from	the	job
list.
After	the	parent	creates	a	new	child	process,	it	adds	the	child	to	the	job
list.	When	the	parent	reaps	a	terminated	(zombie)	child	in	the	SIGCHLD
signal	handler,	it	deletes	the	child	from	the	job	list.
At	first	glance,	this	code	appears	to	be	correct.	Unfortunately,	the
following	sequence	of	events	is	possible:
1
.	
The	parent	executes	the	
function	and	the	kernel	schedules
the	newly	created	child	to	run	instead	of	the	parent.</p>
<h2>2
.	
Before	the	parent	is	able	to	run	again,	the	child	terminates	and
becomes	a	zombie,	causing	the	kernel	to	deliver	a	SIGCHLD
signal	to	the	parent.
3
.	
Later,	when	the	parent	becomes	runnable	again	but	before	it	is
executed,	the	kernel	notices	the	pending	SIGCHLD	and	causes	it
to	be	received	by	running	the	signal	handler	in	the	parent.
4
.	
The	signal	handler	reaps	the	terminated	child	and	calls	
,
which	does	nothing	because	the	parent	has	not	added	the	child	to
the	list	yet.
5
.	
After	the	handler	completes,	the	kernel	then	runs	the	parent,	which
returns	from	
and	incorrectly	adds	the	(nonexistent)	child	to
the	job	list	by	calling	
Thus,	for	some	interleavings	of	the	parent's	main	routine	and	signal-
handling	flows,	it	is	possible	for	
to	be	called	before	
.
This	results	in	an	incorrect	entry	on	the	job	list,	for	a	job	that	no	longer
exists	and	that	will	never	be	removed.	On	the	other	hand,	there	are	also
interleavings	where	events	occur	in	the	correct	order.	For	example,	if	the
kernel	happens	to	schedule	the	parent	to	run	when	the	
call	returns
instead	of	the	child,	then	the	parent	will	correctly	add	the	child	to	the	job
list	before	the	child	terminates	and	the	signal	handler	removes	the	job
from	the	list.
This	is	an	example	of	a	classic	synchronization	error	known	as	a	
race.
In
this	case,	the	race	is	between	the	call	to	
in	the	main	routine	and
the	call	to</h2>
<hr />
<p>code/ecf/procmask1.c</p>
<hr />
<hr />
<p>code/ecf/procmask1.c
Figure	
8.39	
A	shell	program	with	a	subtle	synchronization	error.
If	the	child	terminates	before	the	parent	is	able	to	run,	then	
and
will	be	called	in	the	wrong	order.
in	the	handler.	If	
wins	the	race,	then	the	answer	is
correct.	If	not,	the	answer	is	incorrect.	Such	errors	are	enormously
difficult	to	debug	because	it	is	often	impossible	to	test	every	interleaving.
You	might	run	the	code	a	billion	times	without	a	problem,	but	then	the
next	test	results	in	an	interleaving	that	triggers	the	race.</p>
<p>Figure	
8.40	
shows	one	way	to	eliminate	the	race	in	
Figure	
8.39
.	By
blocking	SIGCHLD	signals	before	the	call	to	
and	then	unblocking
them	only	after	we	have	called	
,	we	guarantee	that	the	child	will	be
reaped	
after
it	is	added	to	the	job	list.	Notice	that	children	inherit	the
set	of	their	parents,	so	we	must	be	careful	to	unblock	the
SIGCHLD	signal	in	the	child	before	calling	
.
8.5.7	
Explicitly	Waiting	for	Signals
Sometimes	a	main	program	needs	to	explicitly	wait	for	a	certain	signal
handler	to	run.	For	example,	when	a	Linux	shell	creates	a	foreground	job,
it	must	wait	for	the	job	to	terminate	and	be	reaped	by	the	SIGCHLD
handler	before	accepting	the	next	user	command.
Figure	
8.41	
shows	the	basic	idea.	The	parent	installs	handlers	for
SIGINT	and	SIGCHLD	and	then	enters	an	infinite	loop.	It	blocks
SIGCHLD	to	avoid	the	race	between	parent	and	child	that	we	discussed
in	
Section	
8.5.6
.	After	creating	the	child,	it	resets	
to	zero,
unblocks	SIGCHLD,	and	then	waits	in	a	spin	loop	for	
to	become
nonzero.	After	the	child	terminates,	the	handler	reaps	it	and	assigns	its
nonzero	PID	to	the	global	
variable.	This	terminates	the	spin	loop,
and	the	parent	continues	with	additional	work	before	starting	the	next
iteration.
While	this	code	is	correct,	the	spin	loop	is	wasteful	of	processor
resources.	We	might	be	tempted	to	fix	this	by	inserting	a	
in	the
body	of	the	spin	loop:</p>
<p>Notice	that	we	still	need	a	loop	because	
might	be	interrupted	by
the	receipt	of	one	or	more	SIGINT	signals.	However,	this	code	has	a
serious	race	condition:	if	the	SIGCHLD	is	received	after	the	
test
but	before	the	pause,	the	
will	sleep	forever.
Another	option	is	to	replace	the	
with	
:
While	correct,	this	code	is	too	slow.	If	the	signal	is	received	after	the
and	before	the	
,	the	program	must	wait	a	(relatively)	long
time	before	it	can	check	the	loop	termination	condition	again.	Using	a
higher-resolution	sleep	function	such	as	
isn't	acceptable,
either,	because	there	is	no	good	rule	for	determining	the	sleep	interval.
Make	it	too	small	and	the	loop	is	too	wasteful.	Make	it	too	high	and	the
program	is	too	slow.</p>
<hr />
<hr />
<p>code/ecf/procmask2.c</p>
<hr />
<hr />
<p>code/ecf/procmask2.c</p>
<h2>Figure	
8.40	
Using	
to	synchronize	processes.
In	this	example,	the	parent	ensures	that	
executes	before	the
corresponding	
.</h2>
<hr />
<p>code/ecf/waitforsignal.c</p>
<hr />
<hr />
<p>code/ecf/waitforsignal.c</p>
<p>Figure	
8.41	
Waiting	for	a	signal	with	a	spin	loop.
This	code	is	correct,	but	the	spin	loop	is	wasteful.
The	proper	solution	is	to	use	sigsuspend.
The	
function	temporarily	replaces	the	current	blocked	set	with
mask	and	then	suspends	the	process	until	the	receipt	of	a	signal	whose
action	is	either	to	run	a	handler	or	to	terminate	the	process.	If	the	action
is	to	terminate,	then	the	process	terminates	without	returning	from
.	If	the	action	is	to	run	a	handler,	then	
returns	after
the	handler	returns,	restoring	the	blocked	set	to	its	state	when	
was	called.
The	
function	is	equivalent	to	an	
atomic
(uninterruptible)
version	of	the	following:</p>
<p>The	atomic	property	guarantees	that	the	calls	to	
(line	1)	and
(line	2)	occur	together,	without	being	interrupted.	This	eliminates
the	potential	race	where	a	signal	is	received	after	the	call	to	
and	before	the	call	to	pause.
Figure	
8.42	
shows	how	we	would	use	
to	replace	the	spin
loop	in	
Figure	
8.41
.	Before	each	call	to	
,	SIGCHLD	is	blocked.
The	
temporarily	unblocks	SIGCHLD,	and	then	sleeps	until	the
parent	catches	a	signal.	Before	returning,	it	restores	the	original	blocked
set,	which	blocks	SIGCHLD	again.	If	the	parent	caught	a	SIGINT,	then
the	loop	test	succeeds	and	the	next	iteration	calls	
again.	If	the
parent	caught	a	SIGCHLD,	then	the	loop	test	fails	and	we	exit	the	loop.
At	this	point,	SIGCHLD	is	blocked,	and	so	we	can	optionally	unblock
SIGCHLD.	This	might	be	useful	in	a	real	shell	with	background	jobs	that
need	to	be	reaped.
The	
version	is	less	wasteful	than	the	original	spin	loop,	avoids
the	race	introduced	by	
,	and	is	more	efficient	than	
.</p>
<h2>8.6	
Nonlocal	Jumps
C	provides	a	form	of	user-level	exceptional	control	flow,	called	a	
nonlocal
jump
,	that	transfers	control	directly	from	one	function	to	another	currently
executing	function	without	having	to	go	through	the	normal	call-and-
return	sequence.	Nonlocal	jumps	are	provided	by	the	
and	
functions.</h2>
<hr />
<p>code/ecf/sigsuspend.c</p>
<hr />
<hr />
<p>code/ecf/sigsuspend.c
Figure	
8.42	
Waiting	for	a	signal	with	
.
The	
function	saves	the	current	
calling	environment
in	the	
buffer,	for	later	use	by	
,	and	returns	0.	The	calling	environment
includes	the	program	counter,	stack	pointer,	and	general-purpose
registers.	For	subtle	reasons	beyond	our	scope,	the	value	that	
returns	should	not	be	assigned	to	a	variable:
However,	it	can	be	safely	used	as	a	test	in	a	
or	conditional
statement	[
62
].</p>
<p>The	
function	restores	the	calling	environment	from	the	
buffer
and	then	triggers	a	return	from	the	most	recent	
call	that	initialized
.	The	
then	returns	with	the	nonzero	return	value	
.
The	interactions	between	
and	
can	be	confusing	at	first
glance.	The	
function	is	called	once	but	returns	
multiple	times:
once	when	the	
is	first	called	and	the	calling	environment	is	stored
in	the	
buffer,	and	once	for	each	corresponding	
call.	On	the
other	hand,	the	
function	is	called	once	but	never	returns.
An	important	application	of	nonlocal	jumps	is	to	permit	an	immediate
return	from	a	deeply	nested	function	call,	usually	as	a	result	of	detecting
some	error	condition.	If	an	error	condition	is	detected	deep	in	a	nested
function	call,	we	can	use	a	nonlocal	jump	to	return	directly	to	a	common
localized	error	handler	instead	of	laboriously	unwinding	the	call	stack.
Figure	
8.43	
shows	an	example	of	how	this	might	work.	The	
routine	first	calls	
to	save	the	current	calling	environment,	and	then
calls	function	
,	which	in	turn	calls	function	
.	If	
or	
encounter	an	error,	they	return	immediately	from	the	
via	a	</p>
<h2>call.	The	nonzero	return	value	of	the	
indicates	the	error	type,
which	can	then	be	decoded	and	handled	in	one	place	in	the	code.
The	feature	of	
that	allows	it	to	skip	up	through	all	intermediate
calls	can	have	unintended	consequences.	For	example,	if	some	data
structures	were	allocated	in	the	intermediate	function	calls	with	the
intention	to	deallocate	them	at	the	end	of	the	function,	the	deallocation
code	gets	skipped,	thus	creating	a	memory	leak.</h2>
<hr />
<p>code/ecf/setjmp.c</p>
<hr />
<hr />
<h2>code/ecf/setjmp.c
Figure	
8.43	
Nonlocal	jump	example.
This	example	shows	the	framework	for	using	nonlocal	jumps	to	recover
from	error	conditions	in	deeply	nested	functions	without	having	to	unwind
the	entire	stack.</h2>
<hr />
<p>code/ecf/restart.c</p>
<hr />
<hr />
<p>code/ecf/restart.c
Figure	
8.44	
A	program	that	uses	nonlocal	jumps	to	restart	itself
when	the	user	types	Ctrl+C.
Another	important	application	of	nonlocal	jumps	is	to	branch	out	of	a
signal	handler	to	a	specific	code	location,	rather	than	returning	to	the
instruction	that	was	interrupted	by	the	arrival	of	the	signal.	
Figure	
8.44
shows	a	simple	program	that	illustrates	this	basic	technique.	The
program	uses	signals	and	nonlocal	jumps	to	do	a	soft	restart	whenever
the	user	types	Ctrl+C	at	the	keyboard.	The	
and	
functions	are	versions	of	
and	
that	can	be	used	by	signal
handlers.
The	initial	call	to	the	
function	saves	the	calling	environment
and	signal	context	(including	the	pending	and	blocked	signal	vectors)
when	the	program	first	starts.	The	main	routine	then	enters	an	infinite
processing	loop.	When	the	user	types	Ctrl+C,	the	kernel	sends	a	SIGINT
signal	to	the	process,	which	catches	it.	Instead	of	returning	from	the</p>
<p>signal	handler,	which	would	pass	control	back	to	the	interrupted
processing	loop,	the	handler	performs	a	nonlocal	jump	back	to	the
beginning	of	the	
program.	When	we	run	the	program	on	our	system,
we	get	the	following	output:
Aside	
Software	exceptions	in	C++	and
Java
The	exception	mechanisms	provided	by	C++	and	Java	are	higher-
level,	more	structured	versions	of	the	C	
and	
functions.	You	can	think	of	a	
clause	inside	a	
statement
as	being	akin	to	a	
function.	Similarly,	a	
statement	is
similar	to	a	
function.</p>
<p>There	a	couple	of	interesting	things	about	this	program.	First,	To	avoid	a
race,	we	must	install	the	handler	
after
we	call	
.	If	not,	we	would
run	the	risk	of	the	handler	running	before	the	initial	call	to	
sets
up	the	calling	environment	for	
.	Second,	you	might	have
noticed	that	the	
and	
functions	are	not	on	the	list	of
async-signal-safe	functions	in	
Figure	
8.33
.	The	reason	is	that	in	general
can	jump	into	arbitrary	code,	so	we	must	be	careful	to	call
only	safe	functions	in	any	code	reachable	from	a	
.	In	our
example,	we	call	the	safe	
and	
functions.	The	unsafe	
function	is	unreachable.</p>
<p>8.7	
Tools	for	Manipulating
Processes
Linux	systems	provide	a	number	of	useful	tools	for	monitoring	and
manipulating	processes:
STRACE
.	Prints	a	trace	of	each	system	call	invoked	by	a	running
program	and	its	children.	It	is	a	fascinating	tool	for	the	curious
student.	Compile	your	program	with	
to	get	a	cleaner	trace
without	a	lot	of	output	related	to	shared	libraries.
PS
.	Lists	processes	(including	zombies)	currently	in	the	system.
TOP
.	Prints	information	about	the	resource	usage	of	current	processes.
PMAP
.	Displays	the	memory	map	of	a	process.
.	A	virtual	filesystem	that	exports	the	contents	of	numerous
kernel	data	structures	in	an	ASCII	text	form	that	can	be	read	by	user
programs.	For	example,	type	
to	see	the	current
load	average	on	your	Linux	system.</p>
<p>8.8	
Summary
Exceptional	control	flow	(ECF)	occurs	at	all	levels	of	a	computer	system
and	is	a	basic	mechanism	for	providing	concurrency	in	a	computer
system.
At	the	hardware	level,	exceptions	are	abrupt	changes	in	the	control	flow
that	are	triggered	by	events	in	the	processor.	The	control	flow	passes	to	a
software	handler,	which	does	some	processing	and	then	returns	control
to	the	interrupted	control	flow.
There	are	four	different	types	of	exceptions:	interrupts,	faults,	aborts,	and
traps.	Interrupts	occur	asynchronously	(with	respect	to	any	instructions)
when	an	external	I/O	device	such	as	a	timer	chip	or	a	disk	controller	sets
the	interrupt	pin	on	the	processor	chip.	Control	returns	to	the	instruction
following	the	faulting	instruction.	Faults	and	aborts	occur	synchronously
as	the	result	of	the	execution	of	an	instruction.	Fault	handlers	restart	the
faulting	instruction,	while	abort	handlers	never	return	control	to	the
interrupted	flow.	Finally,	traps	are	like	function	calls	that	are	used	to
implement	the	system	calls	that	provide	applications	with	controlled	entry
points	into	the	operating	system	code.
At	the	operating	system	level,	the	kernel	uses	ECF	to	provide	the
fundamental	notion	of	a	process.	A	process	provides	applications	with
two	important	abstractions:	(1)	logical	control	flows	that	give	each
program	the	illusion	that	it	has	exclusive	use	of	the	processor,	and	(2)</p>
<p>private	address	spaces	that	provide	the	illusion	that	each	program	has
exclusive	use	of	the	main	memory.
At	the	interface	between	the	operating	system	and	applications,
applications	can	create	child	processes,	wait	for	their	child	processes	to
stop	or	terminate,	run	new	programs,	and	catch	signals	from	other
processes.	The	semantics	of	signal	handling	is	subtle	and	can	vary	from
system	to	system.	However,	mechanisms	exist	on	Posix-compliant
systems	that	allow	programs	to	clearly	specify	the	expected	signal-
handling	semantics.
Finally,	at	the	application	level,	C	programs	can	use	nonlocal	jumps	to
bypass	the	normal	call/return	stack	discipline	and	branch	directly	from
one	function	to	another.</p>
<p>Bibliographic	Notes
Kerrisk	is	the	essential	reference	for	all	aspects	of	programming	in	the
Linux	environment	[
62
].	The	Intel	ISA	specification	contains	a	detailed
discussion	of	exceptions	and	interrupts	on	Intel	processors	[
50
].
Operating	systems	texts	[
102
,	
106
,	
113
]	contain	additional	information	on
exceptions,	processes,	and	signals.	The	classic	work	by	W.	Richard
Stevens	[
111
]	is	a	valuable	and	highly	readable	description	of	how	to
work	with	processes	and	signals	from	application	programs.	Bovet	and
Cesati	[
11
]	give	a	wonderfully	clear	description	of	the	Linux	kernel,
including	details	of	the	process	and	signal	implementations.</p>
<p>Homework	Problems
8.9	
♦
Consider	four	processes	with	the	following	starting	and	ending	times:
Process
Start	time
End	time
A
5
7
B
2
4
C
3
6
D
1
8
For	each	pair	of	processes,	indicate	whether	they	run	concurrently	(Y)	or
not	(N):
Process	pair
Concurrent?
AB</p>
<p>AC</p>
<p>AD</p>
<p>BC</p>
<p>BD</p>
<p>CD</p>
<h2>8.10	
♦
In	this	chapter,	we	have	introduced	some	functions	with	unusual
call	and	return	behaviors:	
,	and	
.
Match	each	function	with	one	of	the	following	behaviors:
A
.	
Called	once,	returns	twice
B
.	
Called	once,	never	returns
C
.	
Called	once,	returns	one	or	more	times
8.11	
♦
How	many	“hello”	output	lines	does	this	program	print?</h2>
<hr />
<p>code/ecf/forkprob1.c</p>
<hr />
<hr />
<h2>code/ecf/forkprob1.c
8.12	
♦
How	many	“hello”	output	lines	does	this	program	print?</h2>
<hr />
<p>code/ecf/forkprob4.c</p>
<hr />
<hr />
<h2>code/ecf/forkprob4.c
8.13	
♦
What	is	one	possible	output	of	the	following	program?</h2>
<hr />
<p>code/ecf/forkprob3.c</p>
<hr />
<hr />
<h2>code/ecf/forkprob3.c
8.14	
♦
How	many	“hello”	output	lines	does	this	program	print?</h2>
<hr />
<p>code/ecf/forkprob5.c</p>
<hr />
<hr />
<h2>code/ecf/forkprob5.c
8.15	
♦
How	many	“hello”	lines	does	this	program	print?</h2>
<hr />
<p>code/ecf/forkprob6.c</p>
<hr />
<hr />
<h2>code/ecf/forkprob6.c
8.16	
♦
What	is	the	output	of	the	following	program?</h2>
<hr />
<p>code/ecf/forkprob7.c</p>
<hr />
<hr />
<h2>code/ecf/forkprob7.c
8.17	
♦
Enumerate	all	of	the	possible	outputs	of	the	program	in	Practice
Problem	
8.4
.
8.18	
♦♦
Consider	the	following	program:</h2>
<hr />
<p>code/ecf/forkprob2.c</p>
<hr />
<hr />
<p>code/ecf/forkprob2.c
Determine	which	of	the	following	outputs	are	possible.	
Note:	
The
function	takes	a	pointer	to	a	function	and	adds	it	to	a	list	of
functions	(initially	empty)	that	will	be	called	when	the	
function
is	called.
A
.	
112002
B
.	
211020
C
.	
102120
D
.	
122001
E
.	
100212</p>
<h2>8.19	
♦♦
How	many	lines	of	output	does	the	following	function	print?	Give
your	answer	as	a	function	of	
n.
Assume	
n
≥	1.</h2>
<hr />
<h2 id="codeecfforkprob8c"><a class="header" href="#codeecfforkprob8c">code/ecf/forkprob8.c</a></h2>
<hr />
<p>code/ecf/forkprob8.c
8.20	
♦♦
Use	
to	write	a	program	called	
whose	behavior	is
identical	to	the	
program.	Your	program	should	accept	the</p>
<h2>same	command-line	arguments,	interpret	the	identical
environment	variables,	and	produce	the	identical	output.
The	
program	gets	the	width	of	the	screen	from	the	COLUMNS
environment	variable.	If	COLUMNS	is	unset,	then	
assumes
that	the	screen	is	80	columns	wide.	Thus,	you	can	check	your
handling	of	the	environment	variables	by	setting	the	COLUMNS
environment	to	something	less	than	80:
⋮
⋮
8.21	
♦♦
What	are	the	possible	output	sequences	from	the	following
program?</h2>
<hr />
<p>code/ecf/waitprob3.c</p>
<hr />
<hr />
<p>code/ecf/waitprob3.c
8.22	
♦♦♦
Write	your	own	version	of	the	Unix	
function
The	
function	executes	
by	invoking	
,	and	then	returns	after	
has	completed.	If	
exits	normally	(by	calling	the	
function	or	executing	a	
statement),	then	
returns	the	
exit	status.	For
example,	if	
terminates	by	calling	
,	then	</p>
<p>returns	the	value	8.	Otherwise,	if	
terminates	abnormally,
then	
returns	the	status	returned	by	the	shell.
8.23	
♦♦
One	of	your	colleagues	is	thinking	of	using	signals	to	allow	a
parent	process	to	count	events	that	occur	in	a	child	process.	The
idea	is	to	notify	the	parent	each	time	an	event	occurs	by	sending	it
a	signal	and	letting	the	parent's	signal	handler	increment	a	global
variable,	which	the	parent	can	then	inspect	after	the	child
has	terminated.	However,	when	he	runs	the	test	program	in
Figure	
8.45	
on	his	system,	he	discovers	that	when	the	parent
calls	
always	has	a	value	of	2,	even	though	the
child	has	sent	five	signals	to	the	parent.	Perplexed,	he	comes	to
you	for	help.	Can	you	explain	the	bug?
8.24	
♦♦♦
Modify	the	program	in	
Figure	
8.18	
so	that	the	following	two
conditions	are	met:
1
.	
Each	child	terminates	abnormally	after	attempting	to	write
to	a	location	in	the	read-only	text	segment.
2
.	
The	parent	prints	output	that	is	identical	(except	for	the
PIDs)	to	the	following:</p>
<h2>Hint:	
Read	the	
page	for	
(3).</h2>
<hr />
<p>code/ecf/counterprob.c</p>
<hr />
<hr />
<p>code/ecf/counterprob.c
Figure	
8.45	
Counter	program	referenced	in	
Problem	
8.23
.
8.25	
♦♦♦
Write	a	version	of	the	
function,	called	
,	that	times	out
after	5	seconds.	The	
function	accepts	the	same	inputs	as
.	If	the	user	doesn't	type	an	input	line	within	5	seconds,
returns	NULL.	Otherwise,	it	returns	a	pointer	to	the	input
line.</p>
<p>8.26	
♦♦♦♦
Using	the	example	in	
Figure	
8.23	
as	a	starting	point,	write	a
shell	program	that	supports	job	control.	Your	shell	should	have	the
following	features:
The	command	line	typed	by	the	user	consists	of	a	
and
zero	or	more	arguments,	all	separated	by	one	or	more	spaces.
If	
is	a	built-in	command,	the	
shell	handles	it	immediately
and	waits	for	the	next	command	line.	Otherwise,	the	shell
assumes	that	
is	an	executable	file,	which	it	loads	and
runs	in	the	context	of	an	initial	child	process	(job).	The	process
group	ID	for	the	job	is	identical	to	the	PID	of	the	child.
Each	job	is	identified	by	either	a	process	ID	(PID)	or	a	job	ID
(JID),	which	is	a	small	arbitrary	positive	integer	assigned	by	the
shell.	JIDs	are	denoted	on	the	command	line	by	the	prefix	‘%’.
For	example,	‘%5’	denotes	JID	5,	and	‘5’	denotes	PID	5.
If	the	command	line	ends	with	an	ampersand,	then	the	shell
runs	the	job	in	the	background.	Otherwise,	the	shell	runs	the
job	in	the	foreground.
Typing	Ctrl+C	(Ctrl+Z)	causes	the	kernel	to	send	a	SIGINT
(SIGTSTP)	signal	to	your	shell,	which	then	forwards	it	to	every
process	in	the	foreground	process	group.
2.	
Note	that	this	is	a	simplification	of	the	way	that	real	shells	work.	With	real	shells,
the	kernel	responds	to	Ctrl+C	(Ctrl+Z)	by	sending	SIGINT	(SIGTSTP)	directly	to
each	process	in	the	terminal	foreground	process	group.	The	shell	manages	the
membership	of	this	group	using	the	
function,	and	manages	the
attributes	of	the	terminal	using	the	
function,	both	of	which	are	outside
the	scope	of	this	book.	See	[
62
]	for	details.
2</p>
<p>The	
built-in	command	lists	all	background	jobs.
The	</p>
<p>job
built-in	command	restarts	
job
by	sending	it	a
SIGCONT	signal	and	then	runs	it	in	the	background.	The	
job
argument	can	be	either	a	PID	or	a	JID.
The	</p>
<p>job
built-in	command	restarts	
job
by	sending	it	a
SIGCONT	signal	and	then	runs	it	in	the	foreground.
The	shell	reaps	all	of	its	zombie	children.	If	any	job	terminates
because	it	receives	a	signal	that	was	not	caught,	then	the	shell
prints	a	message	to	the	terminal	with	the	job's	PID	and	a
description	of	the	offending	signal.
Figure	
8.46	
shows	an	example	shell	session.</p>
<p>Solutions	to	Practice	Problems
Solution	to	Problem	
8.1	
(page
734
)
Processes	A	and	B	are	concurrent	with	respect	to	each	other,	as	are	B
and	C,	because	their	respective	executions	overlap—that	is,	one	process
starts	before	the	other	finishes.	Processes	A	and	C	are	not	concurrent
because	their	executions	do	not	overlap;	A	finishes	before	C	begins.
Solution	to	Problem	
8.2	
(page
743
)
In	our	example	program	in	
Figure	
8.15
,	the	parent	and	child	execute
disjoint	sets	of	instructions.	However,	in	this	program,	the	parent	and
child	execute	nondisjoint	sets	of	instructions,	which	is	possible	because
the	parent	and	child	have	identical	code	segments.	This	can	be	a	difficult
conceptual	hurdle,	so	be	sure	you	understand	the	solution	to	this
problem.	
Figure	
8.47
shows	the	process	graph.</p>
<p>Figure	
8.46	
Sample	shell	session	for	
Problem	
8.26
.
Figure	
8.47	
Process	graph	for	Practice	
Problem	
8.2
.
A
.	
The	key	idea	here	is	that	the	child	executes	both	
statements.	After	the	
returns,	it	executes	the	
in	line	6.
Then	it	falls	out	of	the	
statement	and	executes	the	
in
line	7.	Here	is	the	output	produced	by	the	child:
B
.	
The	parent	executes	only	the	
in	line	7:</p>
<p>Figure	
8.48	
Process	graph	for	Practice	
Problem	
8.3
.
Figure	
8.49	
Process	graph	for	Practice	
Problem	
8.4
.
Solution	to	Problem	
8.3	
(page
745
)
We	know	that	the	sequences	
acbc,	abcc
,	and	
bacc
are	possible	because
they	correspond	to	topological	sorts	of	the	process	graph	(
Figure	
8.48
).
However,	sequences	such	as	
bcac
and	
cbca
do	not	correspond	to	any
topological	sort	and	thus	are	not	feasible.
Solution	to	Problem	
8.4	
(page
748
)
A
.	
We	can	determine	the	number	of	lines	of	output	by	simply
counting	the	number	of	
vertices	in	the	process	graph
(
Figure	
8.49
).	In	this	case,	there	are	six	such	vertices,	and	thus
the	program	will	print	six	lines	of	output.</p>
<h2>B
.	
Any	output	sequence	corresponding	to	a	topological	sort	of	the
graph	is	possible.	For	example:	
is
possible.
Solution	to	Problem	
8.5	
(page
750
)</h2>
<hr />
<h2 id="codeecfsnoozec"><a class="header" href="#codeecfsnoozec">code/ecf/snooze.c</a></h2>
<hr />
<p>code/ecf/snooze.c
Solution	to	Problem	
8.6	
(page</p>
<h2>752
)</h2>
<hr />
<h2 id="codeecfmyechoc"><a class="header" href="#codeecfmyechoc">code/ecf/myecho.c</a></h2>
<hr />
<p>code/ecf/myecho.c</p>
<h2>Solution	to	Problem	
8.7	
(page
764
)
The	
function	returns	prematurely	whenever	the	sleeping	process
receives	a	signal	that	is	not	ignored.	But	since	the	default	action	upon
receipt	of	a	SIGINT	is	to	terminate	the	process	(
Figure	
8.26
),	we	must
install	a	SIGINT	handler	to	allow	the	
function	to	return.	The	handler
simply	catches	the	SIGNAL	and	returns	control	to	the	
function,
which	returns	immediately.</h2>
<hr />
<p>code/ecf/snooze.c</p>
<hr />
<hr />
<p>code/ecf/snooze.c
Solution	to	Problem	
8.8	
(page
773
)</p>
<p>This	program	prints	the	string	213,	which	is	the	shorthand	name	of	the
CS:APP	course	at	Carnegie	Mellon.	The	parent	starts	by	printing	‘2’,	then
s	the	child,	which	spins	in	an	infinite	loop.	The	parent	then	sends	a
signal	to	the	child	and	waits	for	it	to	terminate.	The	child	catches	the
signal	(interrupting	the	infinite	loop),	decrements	the	counter	(from	an
initial	value	of	2),	prints	‘1’,	and	then	terminates.	After	the	parent	reaps
the	child,	it	increments	the	counter	(from	an	initial	value	of	2),	prints	‘3’,
and	terminates.</p>
<p>Chapter	
9	
Virtual	Memory
9.1	
Physical	and	Virtual	Addressing	
803
9.2	
Address	Spaces	
804
9.3	
VM	as	a	Tool	for	Caching	
805
9.4	
VM	as	a	Tool	for	Memory	Management	
811
9.5	
VM	as	a	Tool	for	Memory	Protection	
812
9.6	
Address	Translation	
813
9.7	
Case	Study:	The	Intel	Core	i7/Linux	Memory	System	
825
9.8	
Memory	Mapping	
833
9.9	
Dynamic	Memory	Allocation	
839
9.10	
Garbage	Collection	
865
9.11	
Common	Memory-Related	Bugs	in	C	Programs	
870
9.12	
Summary</p>
<p>875
Bibliographic	Notes</p>
<p>876
Homework	Problems	
876</p>
<p>Solutions	to	Practice	Problems	
880
Processes	in	a	system	share	the	CPU	and	main
memory	with	other	processes.	However,	sharing	the
main	memory	poses	some	special	challenges.	As
demand	on	the	CPU	increases,	processes	slow
down	in	some	reasonably	smooth	way.	But	if	too
many	processes	need	too	much	memory,	then	some
of	them	will	simply	not	be	able	to	run.	When	a
program	is	out	of	space,	it	is	out	of	luck.	Memory	is
also	vulnerable	to	corruption.	If	some	process
inadvertently	writes	to	the	memory	used	by	another
process,	that	process	might	fail	in	some	bewildering
fashion	totally	unrelated	to	the	program	logic.
In	order	to	manage	memory	more	efficiently	and
with	fewer	errors,	modern	systems	provide	an
abstraction	of	main	memory	known	as	
virtual
memory	(VM).
Virtual	memory	is	an	elegant
interaction	of	hardware	exceptions,	hardware
address	translation,	main	memory,	disk	files,	and
kernel	software	that	provides	each	process	with	a
large,	uniform,	and	private	address	space.	With	one
clean	mechanism,	virtual	memory	provides	three
important	capabilities:	(1)	It	uses	main	memory
efficiently	by	treating	it	as	a	cache	for	an	address
space	stored	on	disk,	keeping	only	the	active	areas
in	main	memory	and	transferring	data	back	and
forth	between	disk	and	memory	as	needed.	(2)	It</p>
<p>simplifies	memory	management	by	providing	each
process	with	a	uniform	address	space.	(3)	It
protects	the	address	space	of	each	process	from
corruption	by	other	processes.
Virtual	memory	is	one	of	the	great	ideas	in	computer
systems.	A	major	reason	for	its	success	is	that	it
works	silently	and	automatically,	without	any
intervention	from	the	application	programmer.	Since
virtual	memory	works	so	well	behind	the	scenes,
why	would	a	programmer	need	to	understand	it?
There	are	several	reasons.
Virtual	memory	is	central.	
Virtual	memory
pervades	all	levels	of	computer	systems,	playing
key	roles	in	the	design	of	hardware	exceptions,
assemblers,	linkers,	loaders,	shared	objects,
files,	and	processes.	Understanding	virtual
memory	will	help	you	better	understand	how
systems	work	in	general.
Virtual	memory	is	powerful.	
Virtual	memory
gives	applications	powerful	capabilities	to	create
and	destroy	chunks	of	memory,	map	chunks	of
memory	to	portions	of	disk	files,	and	share
memory	with	other	processes.	For	example,	did
you	know	that	you	can	read	or	modify	the
contents	of	a	disk	file	by	reading	and	writing
memory	locations?	Or	that	you	can	load	the
contents	of	a	file	into	memory	without	doing	any</p>
<p>explicit	copying?	Understanding	virtual	memory
will	help	you	harness	its	powerful	capabilities	in
your	applications.
Virtual	memory	is	dangerous.	
Applications
interact	with	virtual	memory	every	time	they
reference	a	variable,	dereference	a	pointer,	or
make	a	call	to	a	dynamic	allocation	package
such	as	
.	If	virtual	memory	is	used
improperly,	applications	can	suffer	from
perplexing	and	insidious	memory-related	bugs.
For	example,	a	program	with	a	bad	pointer	can
crash	immediately	with	a	&quot;segmentation	fault&quot;	or
a	&quot;protection	fault,&quot;	run	silently	for	hours	before
crashing,	or	scariest	of	all,	run	to	completion	with
incorrect	results.	Understanding	virtual	memory,
and	the	allocation	packages	such	as	
that
manage	it,	can	help	you	avoid	these	errors.
This	chapter	looks	at	virtual	memory	from	two
angles.	The	first	half	of	the	chapter	describes	how
virtual	memory	works.	The	second	half	describes
how	virtual	memory	is	used	and	managed	by
applications.	There	is	no	avoiding	the	fact	that	VM	is
complicated,	and	the	discussion	reflects	this	in
places.	The	good	news	is	that	if	you	work	through
the	details,	you	will	be	able	to	simulate	the	virtual
memory	mechanism	of	a	small	system	by	hand,	and
the	virtual	memory	idea	will	be	forever	demystified.</p>
<p>The	second	half	builds	on	this	understanding,
showing	you	how	to	use	and	manage	virtual
memory	in	your	programs.	You	will	learn	how	to
manage	virtual	memory	via	explicit	memory
mapping	and	calls	to	dynamic	storage	allocators
such	as	the	
package.	You	will	also	learn
about	a	host	of	common	memory-related	errors	in	C
programs	and	how	to	avoid	them.</p>
<p>9.1	
Physical	and	Virtual	Addressing
The	main	memory	of	a	computer	system	is	organized	as	an	array	of	
M
contiguous	byte-size	cells.	Each	byte	has	a	unique	
physical	address
(PA).
The	first	byte	has	an	address	of	0,	the	next	byte	an	address	of	1,
the	next	byte	an	address	of	2,	and	so	on.	Given	this	simple	organization,
the	most	natural	way	for	a	CPU	to	access	memory	would	be	to	use
physical	addresses.	We	call	this	approach	
physical	addressing.</p>
<p>Figure
9.1
shows	an	example	of	physical	addressing	in	the	context	of	a	load
instruction	that	reads	the	4-byte	word	starting	at	physical	address	4.
When	the	CPU	executes	the	load	instruction,	it	generates	an	effective
physical	address	and	passes	it	to	main	memory	over	the	memory	bus.
The	main	memory	fetches	the	4-byte	word	starting	at	physical	address	4
and	returns	it	to	the	CPU,	which	stores	it	in	a	register.
Early	PCs	used	physical	addressing,	and	systems	such	as	digital	signal
processors,	embedded	microcontrollers,	and	Cray	supercomputers
continue	to	do	so.	However,	modern	processors	use	a	form	of	addressing
known	as	
virtual	addressing
,	as	shown	in	
Figure	
9.2
.</p>
<p>Figure	
9.1	
A	system	that	uses	physical	addressing.
Figure	
9.2	
A	system	that	uses	virtual	addressing.
With	virtual	addressing,	the	CPU	accesses	main	memory	by	generating	a
virtual	address	(VA)
,	which	is	converted	to	the	appropriate	physical
address	before	being	sent	to	main	memory.	The	task	of	converting	a
virtual	address	to	a	physical	one	is	known	as	
address	translation.
Like
exception	handling,	address	translation	requires	close	cooperation</p>
<p>between	the	CPU	hardware	and	the	operating	system.	Dedicated
hardware	on	the	CPU	chip	called	the	
memory	management	unit	(MMU)
translates	virtual	addresses	on	the	fly,	using	a	lookup	table	stored	in	main
memory	whose	contents	are	managed	by	the	operating	system.</p>
<p>9.2	
Address	Spaces
An	
address	space
is	an	ordered	set	of	nonnegative	integer	addresses
If	the	integers	in	the	address	space	are	consecutive,	then	we	say	that	it	is
a	
linear	address	space.
To	simplify	our	discussion,	we	will	always
assume	linear	address	spaces.	In	a	system	with	virtual	memory,	the	CPU
generates	virtual	addresses	from	an	address	space	of	
N
=	2
addresses
called	the	
virtual	address	space:
The	size	of	an	address	space	is	characterized	by	the	number	of	bits	that
are	needed	to	represent	the	largest	address.	For	example,	a	virtual
address	space	with	
N
=	2
addresses	is	called	an	
n
-bit	address	space.
Modern	systems	typically	support	either	32-bit	or	64-bit	virtual	address
spaces.
A	system	also	has	
a	physical	address	space
that	corresponds	to	the	
M
bytes	of	physical	memory	in	the	system:
M
is	not	required	to	be	a	power	of	2,	but	to	simplify	the	discussion,	we	will
assume	that	
M
=	2
.
{</p>
<p>0
,
 
1
,
 
2
,
 
…</p>
<p>}
n
{</p>
<p>0
,
 
1
,
 
2
,
 
…
,
 
N
−
1</p>
<p>}
n
{</p>
<p>0
,
 
1
,
 
2
,
 
…
,
 
M
−
1</p>
<p>}
m</p>
<p>The	concept	of	an	address	space	is	important	because	it	makes	a	clean
distinction	between	data	objects	(bytes)	and	their	attributes	(addresses).
Once	we	recognize	this	distinction,	then	we	can	generalize	and	allow
each	data	object	to	have	multiple	independent	addresses,	each	chosen
from	a	different	address	space.	This	is	the	basic	idea	of	virtual	memory.
Each	byte	of	main	memory	has	a	virtual	address	chosen	from	the	virtual
address	space,	and	a	physical	address	chosen	from	the	physical	address
space.
Practice	Problem	
9.1	
(solution	page
880
)
Complete	the	following	table,	filling	in	the	missing	entries	and
replacing	each	question	mark	with	the	appropriate	integer.	Use	the
following	units:	K	=	2
(kilo),	M	=	2
(mega),	G	=	2
(giga),	T	=
2
(tera),	P	=	2
(peta),	or	E	=	2
(exa).
Number	of	virtual
address	bits	
(n)
Number	of	virtual
addresses	
(N)
Largest	possible	virtual
address
8</p>
<hr />
<hr />
<hr />
<p>2
=	64	K</p>
<hr />
<hr />
<hr />
<p>2
--	1	=?	G	--	1</p>
<hr />
<p>2
=	256	T</p>
<hr />
<p>64</p>
<hr />
<hr />
<p>10
20
30
40
50
60
?
32
?</p>
<p>9.3	
VM	as	a	Tool	for	Caching
Conceptually,	a	virtual	memory	is	organized	as	an	array	of	
N
contiguous
byte-size	cells	stored	on	disk.	Each	byte	has	a	unique	virtual	address	that
serves	as	an	index	into	the	array.	The	contents	of	the	array	on	disk	are
cached	in	main	memory.	As	with	any	other	cache	in	the	memory
hierarchy,	the	data	on	disk	(the	lower	level)	is	partitioned	into	blocks	that
serve	as	the	transfer	units	between	the	disk	and	the	main	memory	(the
upper	level).	VM	systems	handle	this	by	partitioning	the	virtual	memory
into	fixed-size	blocks	called	
virtual	pages	(VPs).
Each	virtual	page	is	
P
=
2
bytes	in	size.	Similarly,	physical	memory	is	partitioned	into	
physical
pages	(PPs)
,	also	
P
bytes	in	size.	(Physical	pages	are	also	referred	to	as
page	frames.)
At	any	point	in	time,	the	set	of	virtual	pages	is	partitioned	into	three
disjoint	subsets:
Unallocated.	
Pages	that	have	not	yet	been	allocated	(or	created)	by
the	VM	system.	Unallocated	blocks	do	not	have	any	data	associated
with	them,	and	thus	do	not	occupy	any	space	on	disk.
Cached.	
Allocated	pages	that	are	currently	cached	in	physical
memory.
Uncached.	
Allocated	pages	that	are	not	cached	in	physical	memory.
The	example	in	
Figure	
9.3
shows	a	small	virtual	memory	with	eight
virtual	pages.	Virtual	pages	0	and	3	have	not	been	allocated	yet,	and
P</p>
<p>thus	do	not	yet	exist
Figure	
9.3	
How	a	VM	system	uses	main	memory	as	a	cache.
on	disk.	Virtual	pages	1,4,	and	6	are	cached	in	physical	memory.	Pages
2,5,	and	7	are	allocated	but	are	not	currently	cached	in	physical	memory.
9.3.1	
DRAM	Cache	Organization
To	help	us	keep	the	different	caches	in	the	memory	hierarchy	straight,	we
will	use	the	term	
SRAM	cache
to	denote	the	L1,	L2,	and	L3	cache
memories	between	the	CPU	and	main	memory,	and	the	term	
DRAM
cache
to	denote	the	VM	system's	cache	that	caches	virtual	pages	in	main
memory.
The	position	of	the	DRAM	cache	in	the	memory	hierarchy	has	a	big
impact	on	the	way	that	it	is	organized.	Recall	that	a	DRAM	is	at	least	10
times	slower	than	an	SRAM	and	that	disk	is	about	100,000	times	slower
than	a	DRAM.	Thus,	misses	in	DRAM	caches	are	very	expensive
compared	to	misses	in	SRAM	caches	because	DRAM	cache	misses	are</p>
<p>served	from	disk,	while	SRAM	cache	misses	are	usually	served	from
DRAM-based	main	memory.	Further,	the	cost	of	reading	the	first	byte
from	a	disk	sector	is	about	100,000	times	slower	than	reading	successive
bytes	in	the	sector.	The	bottom	line	is	that	the	organization	of	the	DRAM
cache	is	driven	entirely	by	the	enormous	cost	of	misses.
Because	of	the	large	miss	penalty	and	the	expense	of	accessing	the	first
byte,	virtual	pages	tend	to	be	large—typically	4	KB	to	2	MB.	Due	to	the
large	miss	penalty,	DRAM	caches	are	fully	associative;	that	is,	any	virtual
page	can	be	placed	in	any	physical	page.	The	replacement	policy	on
misses	also	assumes	greater	importance,	because	the	penalty
associated	with	replacing	the	wrong	virtual	page	is	so	high.	Thus,
operating	systems	use	much	more	sophisticated	replacement	algorithms
for	DRAM	caches	than	the	hardware	does	for	SRAM	caches.	(These
replacement	algorithms	are	beyond	our	scope	here.)	Finally,	because	of
the	large	access	time	of	disk,	DRAM	caches	always	use	write-back
instead	of	write-through.
9.3.2	
Page	Tables
As	with	any	cache,	the	VM	system	must	have	some	way	to	determine	if	a
virtual	page	is	cached	somewhere	in	DRAM.	If	so,	the	system	must
determine	which	physical	page	it	is	cached	in.	If	there	is	a	miss,	the
system	must	determine</p>
<p>Figure	
9.4	
Page	table.
where	the	virtual	page	is	stored	on	disk,	select	a	victim	page	in	physical
memory,	and	copy	the	virtual	page	from	disk	to	DRAM,	replacing	the
victim	page.
These	capabilities	are	provided	by	a	combination	of	operating	system
software,	address	translation	hardware	in	the	MMU	(memory
management	unit),	and	a	data	structure	stored	in	physical	memory
known	as	a	
page	table
that	maps	virtual	pages	to	physical	pages.	The
address	translation	hardware	reads	the	page	table	each	time	it	converts
a	virtual	address	to	a	physical	address.	The	operating	system	is
responsible	for	maintaining	the	contents	of	the	page	table	and
transferring	pages	back	and	forth	between	disk	and	DRAM.
Figure	
9.4
shows	the	basic	organization	of	a	page	table.	A	page	table
is	an	array	of	
page	table	entries	(PTEs).
Each	page	in	the	virtual	address</p>
<p>space	has	a	PTE	at	a	fixed	offset	in	the	page	table.	For	our	purposes,	we
will	assume	that	each	PTE	consists	of	a	
valid	bit
and	an	
n
-bit	address
field.	The	valid	bit	indicates	whether	the	virtual	page	is	currently	cached
in	DRAM.	If	the	valid	bit	is	set,	the	address	field	indicates	the	start	of	the
corresponding	physical	page	in	DRAM	where	the	virtual	page	is	cached.
If	the	valid	bit	is	not	set,	then	a	null	address	indicates	that	the	virtual	page
has	not	yet	been	allocated.	Otherwise,	the	address	points	to	the	start	of
the	virtual	page	on	disk.
The	example	in	
Figure	
9.4
shows	a	page	table	for	a	system	with	eight
virtual	pages	and	four	physical	pages.	Four	virtual	pages	(VP	1,	VP	2,	VP
4,	and	VP	7)	are	currently	cached	in	DRAM.	Two	pages	(VP	0	and	VP	5)
have	not	yet	been	allocated,	and	the	rest	(VP	3	and	VP	6)	have	been
allocated	but	are	not	currently	cached.	An	important	point	to	notice	about
Figure	
9.4
is	that	because	the	DRAM	cache	is	fully	associative,	any
physical	page	can	contain	any	virtual	page.
Practice	Problem	
9.2	
(solution	page	
881
)
Determine	the	number	of	page	table	entries	(PTEs)	that	are
needed	for	the	following	combinations	of	virtual	address	size	(
n
)
and	page	size	(
P
):
n
P
=	2
Number	of	PTEs
16
4K</p>
<hr />
<p>16
8K</p>
<hr />
<p>32
4K</p>
<hr />
<p>32
8K</p>
<hr />
<p>p</p>
<p>9.3.3	
Page	Hits
Consider	what	happens	when	the	CPU	reads	a	word	of	virtual	memory
contained	in	VP	2,	which	is	cached	in	DRAM	(
Figure	
9.5
).	Using	a
technique	we	will	describe	in	detail	in	
Section	
9.6
,	the	address
translation	hardware	uses	the	virtual	address	as	an	index	to	locate	PTE	2
and	read	it	from	memory.	Since	the	valid	bit	is	set,	the	address	translation
hardware	knows	that	VP	2	is	cached	in	memory.	So	it	uses	the	physical
memory	address	in	the	PTE	(which	points	to	the	start	of	the	cached	page
in	PP	1)	to	construct	the	physical	address	of	the	word.
9.3.4	
Page	Faults
In	virtual	memory	parlance,	a	DRAM	cache	miss	is	known	as	a	
page
fault.</p>
<p>Figure	
9.6
shows	the	state	of	our	example	page	table	before	the
fault.	The	CPU	has	referenced	a	word	in	VP	3,	which	is	not	cached	in
DRAM.	The	address	translation	hardware	reads	PTE	3	from	memory,
infers	from	the	valid	bit	that	VP	3	is	not	cached,	and	triggers	a	page	fault
exception.	The	page	fault	exception	invokes	a	page	fault	exception
handler	in	the	kernel,	which	selects	a	victim	page—in	this	case,	VP	4
stored	in	PP	3.	If	VP	4	has	been	modified,	then	the	kernel	copies	it	back
to	disk.	In	either	case,	the	kernel	modifies	the	page	table	entry	for	VP	4	to
reflect	the	fact	that	VP	4	is	no	longer	cached	in	main	memory.</p>
<p>Figure	
9.5	
VM	page	hit.
The	reference	to	a	word	in	VP	2	is	a	hit.
Figure	
9.6	
VM	page	fault	(before).
The	reference	to	a	word	in	VP	3	is	a	miss	and	triggers	a	page	fault.</p>
<p>Figure	
9.7	
VM	page	fault	(after).
The	page	fault	handler	selects	VP	4	as	the	victim	and	replaces	it	with	a
copy	of	VP	3	from	disk.	After	the	page	fault	handler	restarts	the	faulting
instruction,	it	will	read	the	word	from	memory	normally,	without
generating	an	exception.
Next,	the	kernel	copies	VP	3	from	disk	to	PP	3	in	memory,	updates	PTE
3,	and	then	returns.	When	the	handler	returns,	it	restarts	the	faulting
instruction,	which	resends	the	faulting	virtual	address	to	the	address
translation	hardware.	But	now,	VP	3	is	cached	in	main	memory,	and	the
page	hit	is	handled	normally	by	the	address	translation	hardware.	
Figure
9.7
shows	the	state	of	our	example	page	table	after	the	page	fault.
Virtual	memory	was	invented	in	the	early	1960s,	long	before	the	widening
CPU-memory	gap	spawned	SRAM	caches.	As	a	result,	virtual	memory
systems	use	a	different	terminology	from	SRAM	caches,	even	though
many	of	the	ideas	are	similar.	In	virtual	memory	parlance,	blocks	are</p>
<p>known	as	pages.	The	activity	of	transferring	a	page	between	disk	and
memory	is	known	as	
swapping
or	
paging.
Pages	are	
swapped	in	(paged
in)
from	disk	to	DRAM,	and	
swapped	out	(paged	out)
from	DRAM	to	disk.
The	strategy	of	waiting	until	the	last	moment	to	swap
Figure	
9.8	
Allocating	a	new	virtual	page.
The	kernel	allocates	VP	5	on	disk	and	points	PTE	5	to	this	new	location.
in	a	page,	when	a	miss	occurs,	is	known	as	
demand	paging.
Other
approaches,	such	as	trying	to	predict	misses	and	swap	pages	in	before
they	are	actually	referenced,	are	possible.	However,	all	modern	systems
use	demand	paging.
9.3.5	
Allocating	Pages</p>
<p>Figure	
9.8
shows	the	effect	on	our	example	page	table	when	the
operating	system	allocates	a	new	page	of	virtual	memory—for	example,
as	a	result	of	calling	
.	In	the	example,	VP	5	is	allocated	by	creating
room	on	disk	and	updating	PTE	5	to	point	to	the	newly	created	page	on
disk.
9.3.6	
Locality	to	the	Rescue	Again
When	many	of	us	learn	about	the	idea	of	virtual	memory,	our	first
impression	is	often	that	it	must	be	terribly	inefficient.	Given	the	large	miss
penalties,	we	worry	that	paging	will	destroy	program	performance.	In
practice,	virtual	memory	works	well,	mainly	because	of	our	old	friend
locality.
Although	the	total	number	of	distinct	pages	that	programs	reference
during	an	entire	run	might	exceed	the	total	size	of	physical	memory,	the
principle	of	locality	promises	that	at	any	point	in	time	they	will	tend	to
work	on	a	smaller	set	of	
active	pages
known	as	the	
working	set
or
resident	set.
After	an	initial	overhead	where	the	working	set	is	paged	into
memory,	subsequent	references	to	the	working	set	result	in	hits,	with	no
additional	disk	traffic.
As	long	as	our	programs	have	good	temporal	locality,	virtual	memory
systems	work	quite	well.	But	of	course,	not	all	programs	exhibit	good
temporal	locality.	If	the	working	set	size	exceeds	the	size	of	physical
memory,	then	the	program	can	produce	an	unfortunate	situation	known
as	
thrashing
,	where	pages	are	swapped	in	and	out	continuously.
Although	virtual	memory	is	usually	efficient,	if	a	program's	performance</p>
<p>slows	to	a	crawl,	the	wise	programmer	will	consider	the	possibility	that	it
is	thrashing.
Aside	
Counting	page	faults
You	can	monitor	the	number	of	page	faults	(and	lots	of	other
information)	with	the	Linux	
function.
Figure	
9.9	
How	VM	provides	processes	with	separate	address
spaces.
The	operating	system	maintains	a	separate	page	table	for	each	process
in	the	system.</p>
<p>9.4	
VM	as	a	Tool	for	Memory
Management
In	the	last	section,	we	saw	how	virtual	memory	provides	a	mechanism	for
using	the	DRAM	to	cache	pages	from	a	typically	larger	virtual	address
space.	Interestingly,	some	early	systems	such	as	the	DEC	PDP-11/70
supported	a	virtual	address	space	that	was	
smaller
than	the	available
physical	memory.	Yet	virtual	memory	was	still	a	useful	mechanism
because	it	greatly	simplified	memory	management	and	provided	a	natural
way	to	protect	memory.
Thus	far,	we	have	assumed	a	single	page	table	that	maps	a	single	virtual
address	space	to	the	physical	address	space.	In	fact,	operating	systems
provide	a	separate	page	table,	and	thus	a	separate	virtual	address
space,	for	each	process.	
Figure	
9.9
shows	the	basic	idea.	In	the
example,	the	page	table	for	process	
i
maps	VP	1	to	PP	2	and	VP	2	to	PP
7.	Similarly,	the	page	table	for	process	
j
maps	VP	1	to	PP	7	and	VP	2	to
PP	10.	Notice	that	multiple	virtual	pages	can	be	mapped	to	the	same
shared	physical	page.
The	combination	of	demand	paging	and	separate	virtual	address	spaces
has	a	profound	impact	on	the	way	that	memory	is	used	and	managed	in
a	system.	In	particular,	VM	simplifies	linking	and	loading,	the	sharing	of
code	and	data,	and	allocating	memory	to	applications.</p>
<p>Simplifying	linking.	
A	separate	address	space	allows	each	process
to	use	the	same	basic	format	for	its	memory	image,	regardless	of
where	the	code	and	data	actually	reside	in	physical	memory.	For
example,	as	we	saw	in	
Figure	
8.13
,	every	process	on	a	given
Linux	system	has	a	similar	memory	format.	For	64-bit	address
spaces,	the	code	segment	
always
starts	at	virtual	address	
.
The	data	segment	follows	the	code	segment	after	a	suitable
alignment	gap.	The	stack	occupies	the	highest	portion	of	the	user
process	address	space	and	
grows	downward.	Such	uniformity	greatly
simplifies	the	design	and	implementation	of	linkers,	allowing	them	to
produce	fully	linked	executables	that	are	independent	of	the	ultimate
location	of	the	code	and	data	in	physical	memory.
Simplifying	loading.	
Virtual	memory	also	makes	it	easy	to	load
executable	and	shared	object	files	into	memory.	To	load	the	
and	
sections	of	an	object	file	into	a	newly	created	process,	the
Linux	loader	allocates	virtual	pages	for	the	code	and	data	segments,
marks	them	as	invalid	(i.e.,	not	cached),	and	points	their	page	table
entries	to	the	appropriate	locations	in	the	object	file.	The	interesting
point	is	that	the	loader	never	actually	copies	any	data	from	disk	into
memory.	The	data	are	paged	in	automatically	and	on	demand	by	the
virtual	memory	system	the	first	time	each	page	is	referenced,	either
by	the	CPU	when	it	fetches	an	instruction	or	by	an	executing
instruction	when	it	references	a	memory	location.
This	notion	of	mapping	a	set	of	contiguous	virtual	pages	to	an
arbitrary	location	in	an	arbitrary	file	is	known	as	
memory	mapping.
Linux	provides	a	system	call	called	
that	allows	application
programs	to	do	their	own	memory	mapping.	We	will	describe
application-level	memory	mapping	in	more	detail	in	
Section	
9.8
.</p>
<p>Simplifying	sharing.	
Separate	address	spaces	provide	the	operating
system	with	a	consistent	mechanism	for	managing	sharing	between
user	processes	and	the	operating	system	itself.	In	general,	each
process	has	its	own	private	code,	data,	heap,	and	stack	areas	that
are	not	shared	with	any	other	process.	In	this	case,	the	operating
system	creates	page	tables	that	map	the	corresponding	virtual	pages
to	disjoint	physical	pages.
However,	in	some	instances	it	is	desirable	for	processes	to	share
code	and	data.	For	example,	every	process	must	call	the	same
operating	system	kernel	code,	and	every	C	program	makes	calls	to
routines	in	the	standard	C	library	such	as	
.	Rather	than
including	separate	copies	of	the	kernel	and	standard	C	library	in	each
process,	the	operating	system	can	arrange	for	multiple	processes	to
share	a	single	copy	of	this	code	by	mapping	the	appropriate	virtual
pages	in	different	processes	to	the	same	physical	pages,	as	we	saw
in	
Figure	
9.9
.
Simplifying	memory	allocation.	
Virtual	memory	provides	a	simple
mechanism	for	allocating	additional	memory	to	user	processes.	When
a	program	running	in	a	user	process	requests	additional	heap	space
(e.g.,	as	a	result	of	calling	
),	the	operating	system	allocates	an
appropriate	number,	say,	
k
,	of	contiguous	virtual	memory	pages,	and
maps	them	to	
k
arbitrary	physical	pages	located	anywhere	in	physical
memory.	Because	of	the	way	page	tables	work,	there	is	no	need	for
the	operating	system	to	locate	
k
contiguous	pages	of	physical
memory.	The	pages	can	be	scattered	randomly	in	physical	memory.</p>
<p>9.5	
VM	as	a	Tool	for	Memory
Protection
Any	modern	computer	system	must	provide	the	means	for	the	operating
system	to	control	access	to	the	memory	system.	A	user	process	should
not	be	allowed
Figure	
9.10	
Using	VM	to	provide	page-level	memory	protection.
to	modify	its	read-only	code	section.	Nor	should	it	be	allowed	to	read	or
modify	any	of	the	code	and	data	structures	in	the	kernel.	It	should	not	be
allowed	to	read	or	write	the	private	memory	of	other	processes,	and	it
should	not	be	allowed	to	modify	any	virtual	pages	that	are	shared	with
other	processes,	unless	all	parties	explicitly	allow	it	(via	calls	to	explicit
interprocess	communication	system	calls).</p>
<p>As	we	have	seen,	providing	separate	virtual	address	spaces	makes	it
easy	to	isolate	the	private	memories	of	different	processes.	But	the
address	translation	mechanism	can	be	extended	in	a	natural	way	to
provide	even	finer	access	control.	Since	the	address	translation	hardware
reads	a	PTE	each	time	the	CPU	generates	an	address,	it	is
straightforward	to	control	access	to	the	contents	of	a	virtual	page	by
adding	some	additional	permission	bits	to	the	PTE.	
Figure	
9.10
shows
the	general	idea.
In	this	example,	we	have	added	three	permission	bits	to	each	PTE.	The
SUP	bit	indicates	whether	processes	must	be	running	in	kernel
(supervisor)	mode	to	access	the	page.	Processes	running	in	kernel	mode
can	access	any	page,	but	processes	running	in	user	mode	are	only
allowed	to	access	pages	for	which	SUP	is	0.	The	READ	and	WRITE	bits
control	read	and	write	access	to	the	page.	For	example,	if	process	
i
is
running	in	user	mode,	then	it	has	permission	to	read	VP	0	and	to	read	or
write	VP	1.	However,	it	is	not	allowed	to	access	VP	2.
If	an	instruction	violates	these	permissions,	then	the	CPU	triggers	a
general	protection	fault	that	transfers	control	to	an	exception	handler	in
the	kernel,	which	sends	a	SIGSEGV	signal	to	the	offending	process.
Linux	shells	typically	report	this	exception	as	a	&quot;segmentation	fault.&quot;</p>
<p>9.6	
Address	Translation
This	section	covers	the	basics	of	address	translation.	Our	aim	is	to	give
you	an	appreciation	of	the	hardware's	role	in	supporting	virtual	memory,
with	enough	detail	so	that	you	can	work	through	some	concrete
examples	by	hand.	However,	keep	in	mind	that	we	are	omitting	a	number
of	details,	especially	related	to	timing,
Symbol
Description
Basic	parameters
N
=	2
Number	of	addresses	in	virtual	address	space
M
=	2
Number	of	addresses	in	physical	address	space
P
=	2
Page	size	(bytes)
Components	of	a	virtual	address	(VA)
VPO
Virtual	page	offset	(bytes)
VPN
Virtual	page	number
TLBI
TLB	index
TLBT
TLB	tag
Components	of	a	physical	address	(PA)
PPO
Physical	page	offset	(bytes)
n
m
p</p>
<h1>PPN
Physical	page	number
CO
Byte	offset	within	cache	block
CI
Cache	index
CT
Cache	tag
Figure	
9.11	
Summary	of	address	translation	symbols.
that	are	important	to	hardware	designers	but	are	beyond	our	scope.	For
your	reference,	
Figure	
9.11
summarizes	the	symbols	that	we	will	be
using	throughout	this	section.
Formally,	address	translation	is	a	mapping	between	the	elements	of	an
N
-element	virtual	address	space	(VAS)	and	an	M-element	physical
address	space	(PAS),
where
Figure	
9.12
shows	how	the	MMU	uses	the	page	table	to	perform	this
mapping.	A	control	register	in	the	CPU,	the	
page	table	base	register
(PTBR)
points	to	the	current	page	table.	The	
n
-bit	virtual	address	has	two
components:	a	
p
-bit	
virtual	page	offset	(VPO)
and	an	
(n	--	p)
-bit	
virtual
page	number	(VPN).
The	MMU	uses	the	VPN	to	select	the	appropriate
PTE.	For	example,	VPN	0	selects	PTE	0,	VPN	1	selects	PTE	1,	and	so
MAP
:
 
V
A
S
→
P
A
S
∪
ϕ
MAP
(
A
)</h1>
<p>{
A
'
if	data	at	virtual	addr
.	
A
are	present	at	physical	addr
.	
A
'
in	PAS
ϕ
if	data	at	virtual	addr</p>
<p>on.	The	corresponding	physical	address	is	the	concatenation	of	the
physical	page	number	(PPN)
from	the	page	table	entry	and	the	VPO	from
the	virtual	address.	Notice	that	since	the	physical	and	virtual	pages	are
both	
P
bytes,	the	
physical	page	offset	(PPO)
is	identical	to	the	VPO.
Figure	
9.12	
Address	translation	with	a	page	table.
Figure	
9.13(a)
shows	the	steps	that	the	CPU	hardware	performs	when
there	is	a	page	hit.
Step	</p>
<ol>
<li></li>
</ol>
<p>The	processor	generates	a	virtual	address	and	sends	it	to
the	MMU.
Step	
2.	
The	MMU	generates	the	PTE	address	and	requests	it	from
the	cache/main	memory.
Step	
3.	
The	cache/main	memory	returns	the	PTE	to	the	MMU.</p>
<p>Step	
4.	
The	MMU	constructs	the	physical	address	and	sends	it	to
the	cache/main	memory.
Step	
5.	
The	cache/main	memory	returns	the	requested	data	word
to	the	processor.
Unlike	a	page	hit,	which	is	handled	entirely	by	hardware,	handling	a	page
fault	requires	cooperation	between	hardware	and	the	operating	system
kernel	(
Figure	
9.13(b)
).
Steps	
1	to	3.	
The	same	as	steps	1	to	3	in	
Figure	
9.13(a)
.
Step	
4.	
The	valid	bit	in	the	PTE	is	zero,	so	the	MMU	triggers	an
exception,	which	transfers	control	in	the	CPU	to	a	page	fault
exception	handler	in	the	operating	system	kernel.
Step	
5.	
The	fault	handler	identifies	a	victim	page	in	physical
memory,	and	if	that	page	has	been	modified,	pages	it	out	to	disk.
Step	
6.	
The	fault	handler	pages	in	the	new	page	and	updates	the
PTE	in	memory.</p>
<p>Figure	
9.13	
Operational	view	of	page	hits	and	page	faults.
VA:	virtual	address.	PTEA:	page	table	entry	address.	PTE:	page
table	entry.	PA:	physical	address.
Step	
7.	
The	fault	handler	returns	to	the	original	process,	causing
the	faulting	instruction	to	be	restarted.	The	CPU	resends	the
offending	virtual	address	to	the	MMU.	Because	the	virtual	page	is
now	cached	in	physical	memory,	there	is	a	hit,	and	after	the	MMU</p>
<p>performs	the	steps	in	
Figure	
9.13(a)
,	the	main	memory	returns
the	requested	word	to	the	processor.
Practice	Problem	
9.3	
(solution	page
881
)
Given	a	32-bit	virtual	address	space	and	a	24-bit	physical	address,
determine	the	number	of	bits	in	the	VPN,	VPO,	PPN,	and	PPO	for
the	following	page	sizes	
P:
P
Number	of
VPN	bits
VPO	bits
PPN	bits
PPO	bits
1	KB</p>
<hr />
<hr />
<hr />
<hr />
<p>2	KB</p>
<hr />
<hr />
<hr />
<hr />
<p>4	KB</p>
<hr />
<hr />
<hr />
<hr />
<p>8	KB</p>
<hr />
<hr />
<hr />
<hr />
<p>Figure	
9.14	
Integrating	VM	with	a	physically	addressed	cache.
VA:	virtual	address.	PTEA:	page	table	entry	address.	PTE:	page	table
entry.	PA:	physical	address.
9.6.1	
Integrating	Caches	and	VM
In	any	system	that	uses	both	virtual	memory	and	SRAM	caches,	there	is
the	issue	of	whether	to	use	virtual	or	physical	addresses	to	access	the
SRAM	cache.	Although	a	detailed	discussion	of	the	trade-offs	is	beyond
our	scope	here,	most	systems	opt	for	physical	addressing.	With	physical
addressing,	it	is	straightforward	for	multiple	processes	to	have	blocks	in
the	cache	at	the	same	time	and	to	share	blocks	from	the	same	virtual
pages.	Further,	the	cache	does	not	have	to	deal	with	protection	issues,
because	access	rights	are	checked	as	part	of	the	address	translation
process.
Figure	
9.14
shows	how	a	physically	addressed	cache	might	be
integrated	with	virtual	memory.	The	main	idea	is	that	the	address</p>
<p>translation	occurs	before	the	cache	lookup.	Notice	that	page	table	entries
can	be	cached,	just	like	any	other	data	words.
9.6.2	
Speeding	Up	Address
Translation	with	a	TLB
As	we	have	seen,	every	time	the	CPU	generates	a	virtual	address,	the
MMU	must	refer	to	a	PTE	in	order	to	translate	the	virtual	address	into	a
physical	address.	In	the	worst	case,	this	requires	an	additional	fetch	from
memory,	at	a	cost	of	tens	to	hundreds	of	cycles.	If	the	PTE	happens	to	be
cached	in	L1,	then	the	cost	goes	down	to	a	handful	of	cycles.	However,
many	systems	try	to	eliminate	even	this	cost	by	including	a	small	cache
of	PTEs	in	the	MMU	called	a	
translation	lookaside	buffer	(TLB).
A	TLB	is	a	small,	virtually	addressed	cache	where	each	line	holds	a	block
consisting	of	a	single	PTE.	A	TLB	usually	has	a	high	degree	of
associativity.	As	shown	in	
Figure	
9.15
,	the	index	and	tag	fields	that	are
used	for	set	selection	and	line	matching	are	extracted	from	the	virtual
page	number	in	the	virtual	address.	If	the	TLB	has	
T
=	2
sets,	then	the
TLB	index	(TLBI)
consists	of	the	
t
least	significant	bits	of	the	VPN,	and
the	
TLB	tag	(TLBT)
consists	of	the	remaining	bits	in	the	VPN.
Figure	
9.15	
Components	of	a	virtual	address	that	are	used	to	access
the	TLB.
t</p>
<p>Figure	
9.16	
Operational	view	of	a	TLB	hit	and	miss.</p>
<p>Figure	
9.16(a)
shows	the	steps	involved	when	there	is	a	TLB	hit	(the
usual	case).	The	key	point	here	is	that	all	of	the	address	translation	steps
are	performed	inside	the	on-chip	MMU	and	thus	are	fast.
Step	1.	
The	CPU	generates	a	virtual	address.
Steps	2	and	3.	
The	MMU	fetches	the	appropriate	PTE	from	the
TLB.
Step	4.	
The	MMU	translates	the	virtual	address	to	a	physical
address	and	sends	it	to	the	cache/main	memory.
Step	5.	
The	cache/main	memory	returns	the	requested	data	word
to	the	CPU.
When	there	is	a	TLB	miss,	then	the	MMU	must	fetch	the	PTE	from	the	L1
cache,	as	shown	in	
Figure	
9.16(b)
.	The	newly	fetched	PTE	is	stored	in
the	TLB,	possibly	overwriting	an	existing	entry.
9.6.3	
Multi-Level	Page	Tables
Thus	far,	we	have	assumed	that	the	system	uses	a	single	page	table	to
do	address	translation.	But	if	we	had	a	32-bit	address	space,	4	KB
pages,	and	a	4-byte	PTE,	then	we	would	need	a	4	MB	page	table
resident	in	memory	at	all	times,	even	if	the	application	referenced	only	a
small	chunk	of	the	virtual	address	space.	The	problem	is	compounded	for
systems	with	64-bit	address	spaces.
The	common	approach	for	compacting	the	page	table	is	to	use	a
hierarchy	of	page	tables	instead.	The	idea	is	easiest	to	understand	with	a
concrete	example.	Consider	a	32-bit	virtual	address	space	partitioned</p>
<p>into	4	KB	pages,	with	page	table	entries	that	are	4	bytes	each.	Suppose
also	that	at	this	point	in	time	the	virtual	address	space	has	the	following
form:	The	first	2	K	pages	of	memory	are	allocated	for	code	and	data,	the
next	6	K	pages	are	unallocated,	the	next	1,023	pages	are	also
unallocated,	and	the	next	page	is	allocated	for	the	user	stack.	
Figure
9.17
shows	how	we	might	construct	a	two-level	page	table	hierarchy
for	this	virtual	address	space.
Each	PTE	in	the	level	1	table	is	responsible	for	mapping	a	4	MB	chunk	of
the	virtual	address	space,	where	each	chunk	consists	of	1,024
contiguous	pages.	For	example,	PTE	0	maps	the	first	chunk,	PTE	1	the
next	chunk,	and	so	on.	Given	that	the	address	space	is	4	GB,	1,024
PTEs	are	sufficient	to	cover	the	entire	space.
If	every	page	in	chunk	
i
is	unallocated,	then	level	1	PTE	
i
is	null.	For
example,	in	
Figure	
9.17
,	chunks	2--7	are	unallocated.	However,	if	at
least	one	page	in	chunk	
i
is	allocated,	then	level	1	PTE	
i
points	to	the
base	of	a	level	2	page	table.	For	example,	in	
Figure	
9.17
,	all	or
portions	of	chunks	0,1,	and	8	are	allocated,	so	their	level	1	PTEs	point	to
level	2	page	tables.
Each	PTE	in	a	level	2	page	table	is	responsible	for	mapping	a	4-KB	page
of	virtual	memory,	just	as	before	when	we	looked	at	single-level	page
tables.	Notice	that	with	4-byte	PTEs,	each	level	1	and	level	2	page	table
is	4	kilobytes,	which	conveniently	is	the	same	size	as	a	page.
This	scheme	reduces	memory	requirements	in	two	ways.	First,	if	a	PTE
in	the	level	1	table	is	null,	then	the	corresponding	level	2	page	table	does
not	even	have	to	exist.	This	represents	a	significant	potential	savings,
since	most	of	the	4	GB	virtual	address	space	for	a	typical	program	is</p>
<p>unallocated.	Second,	only	the	level	1	table	needs	to	be	in	main	memory
at	all	times.	The	level	2	page	tables	can	be	created	and	paged	in	and	out
by	the	VM	system	as	they	are	needed,	which	reduces	pressure	on	main
memory.	Only	the	most	heavily	used	level	2	page	tables	need	to	be
cached	in	main	memory.
Figure	
9.17	
A	two-level	page	table	hierarchy.
Notice	that	addresses	increase	from	top	to	bottom.</p>
<p>Figure	
9.18	
Address	translation	with	a	
k
-level	page	table.
Figure	
9.18
summarizes	address	translation	with	a	
k
-level	page	table
hierarchy.	The	virtual	address	is	partitioned	into	
k
VPNs	and	a	VPO.
Each	VPN	
i
,	1	≤	
i
≤	
k
,	is	an	index	into	a	page	table	at	level	
i.
Each	PTE	in
a	level	
j
table,	1	≤	
j
≤	
k
−	1,	points	to	the	base	of	some	page	table	at	level
j
+	1.	Each	PTE	in	a	level	
k
table	contains	either	the	PPN	of	some
physical	page	or	the	address	of	a	disk	block.	To	construct	the	physical
address,	the	MMU	must	access	
k
PTEs	before	it	can	
determine	the	PPN.
As	with	a	single-level	hierarchy,	the	PPO	is	identical	to	the	VPO.
Accessing	
k
PTEs	may	seem	expensive	and	impractical	at	first	glance.
However,	the	TLB	comes	to	the	rescue	here	by	caching	PTEs	from	the
page	tables	at	the	different	levels.	In	practice,	address	translation	with
multi-level	page	tables	is	not	significantly	slower	than	with	single-level
page	tables.</p>
<p>9.6.4	
Putting	It	Together:	End-to-End
Address	Translation
In	this	section,	we	put	it	all	together	with	a	concrete	example	of	end-to-
end	address	translation	on	a	small	system	with	a	TLB	and	L1	d-cache.	To
keep	things	manageable,	we	make	the	following	assumptions:
The	memory	is	byte	addressable.
Memory	accesses	are	to	
1-byte	words
(not	4-byte	words).
Virtual	addresses	are	14	bits	wide	(
n
=	14).
Physical	addresses	are	12	bits	wide	(
m
=	12).
The	page	size	is	64	bytes	(
P
=	64).
The	TLB	is	4-way	set	associative	with	16	total	entries.
The	L1	d-cache	is	physically	addressed	and	direct	mapped,	with	a	4-
byte	line	size	and	16	total	sets.
Figure	
9.19
shows	the	formats	of	the	virtual	and	physical	addresses.
Since	each	page	is	2
=	64	bytes,	the	low-order	6	bits	of	the	virtual	and
physical	addresses	serve	as	the	VPO	and	PPO,	respectively.	The	high-
order	8	bits	of	the	virtual	address	serve	as	the	VPN.	The	high-order	6	bits
of	the	physical	address	serve	as	the	PPN.
Figure	
9.20
shows	a	snapshot	of	our	little	memory	system,	including
the	TLB	(
Figure	
9.20
(a)
),	a	portion	of	the	page	table	(
Figure
9.20(b)
),	and	the	L1	cache	(
Figure	
9.20(c)
).	Above	the	figures	of
the	TLB	and	cache,	we	have	also	shown	how	the	bits	of	the	virtual	and
6</p>
<p>physical	addresses	are	partitioned	by	the	hardware	as	it	accesses	these
devices.
Figure	
9.19	
Addressing	for	small	memory	system.
Assume	14-bit	virtual	addresses	(
n
=	14),	12-bit	physical	addresses	(
m
=
12),	and	64-byte	pages	(
P
=	64).</p>
<p>Figure	
9.20	
TLB,	page	table,	and	cache	for	small	memory	system.
All	values	in	the	TLB,	page	table,	and	cache	are	in	hexadecimal	notation.
TLB.	
The	TLB	is	virtually	addressed	using	the	bits	of	the	VPN.	Since
the	TLB	has	four	sets,	the	2	low-order	bits	of	the	VPN	serve	as	the	set
index	(TLBI).	The	remaining	6	high-order	bits	serve	as	the	tag	(TLBT)
that	distinguishes	the	different	VPNs	that	might	map	to	the	same	TLB
set.
Page	table.	
The	page	table	is	a	single-level	design	with	a	total	of	2
=
256	page	table	entries	(PTEs).	However,	we	are	only	interested	in	the
first	16	of	these.	For	convenience,	we	have	labeled	each	PTE	with	the
VPN	that	indexes	it;	but	keep	in	mind	that	these	VPNs	are	not	part	of
the	page	table	and	not	stored	in	memory.	Also,	notice	that	the	PPN	of
each	invalid	PTE	is	denoted	with	a	dash	to	reinforce	the	idea	that
whatever	bit	values	might	happen	to	be	stored	there	are	not
meaningful.
Cache.	
The	direct-mapped	cache	is	addressed	by	the	fields	in	the
physical	address.	Since	each	block	is	4	bytes,	the	low-order	2	bits	of
the	physical	address	serve	as	the	block	offset	(CO).	Since	there	are
16	sets,	the	next	4	bits	serve	as	the	set	index	(CI).	The	remaining	6
bits	serve	as	the	tag	(CT).
Given	this	initial	setup,	let's	see	what	happens	when	the	CPU	executes	a
load	instruction	that	reads	the	byte	at	address	
.	(Recall	that	our
hypothetical	CPU	reads	1-byte	words	rather	than	4-byte	words.)	To	begin
this	kind	of	manual	simulation,	we	find	it	helpful	to	write	down	the	bits	in
the	virtual	address,	identify	the	various	fields	we	will	need,	and	determine
8</p>
<p>their	hex	values.	The	hardware	performs	a	similar	task	when	it	decodes
the	address.
To	begin,	the	MMU	extracts	the	VPN	(
)	from	the	virtual	address	and
checks	with	the	TLB	to	see	if	it	has	cached	a	copy	of	PTE	
from
some	previous	memory	reference.	The	TLB	extracts	the	TLB	index
(
)	and	the	TLB	tag	(
)	from	the	VPN,	hits	on	a	valid	match	in	the
second	entry	of	set	
,	and	returns	the	cached	PPN	(
)	to	the	MMU.
If	the	TLB	had	missed,	then	the	MMU	would	need	to	fetch	the	PTE	from
main	memory.	However,	in	this	case,	we	got	lucky	and	had	a	TLB	hit.	The
MMU	now	has	everything	it	needs	to	form	the	physical	address.	It	does
this	by	concatenating	the	PPN	(
)	from	the	PTE	with	the	VPO	(
)
from	the	virtual	address,	which	forms	the	physical	address	(
).
Next,	the	MMU	sends	the	physical	address	to	the	cache,	which	extracts
the	cache	offset	CO	(
),	the	cache	set	index	CI	(
),	and	the	cache
tag	CT	(
)	from	the	physical	address.</p>
<p>Since	the	tag	in	set	
matches	CT,	the	cache	detects	a	hit,	reads	out
the	data	byte	(
)	at	offset	CO,	and	returns	it	to	the	MMU,	which	then
passes	it	back	to	the	CPU.
Other	paths	through	the	translation	process	are	also	possible.	For
example,	if	the	TLB	misses,	then	the	MMU	must	fetch	the	PPN	from	a
PTE	in	the	page	table.	If	the	resulting	PTE	is	invalid,	then	there	is	a	page
fault	and	the	kernel	must	page	in	the	appropriate	page	and	rerun	the	load
instruction.	Another	possibility	is	that	the	PTE	is	valid,	but	the	necessary
memory	block	misses	in	the	cache.
Practice	Problem	
9.4	
(solution	page	
881
)
Show	how	the	example	memory	system	in	
Section	
9.6.4
translates	a	virtual	address	into	a	physical	address	and	accesses
the	cache.	For	the	given	virtual	address,	indicate	the	TLB	entry
accessed,	physical	address,	and	cache	byte	value	returned.
Indicate	whether	the	TLB	misses,	whether	a	page	fault	occurs,
and	whether	a	cache	miss	occurs.	If	there	is	a	cache	miss,	enter
&quot;—&quot;	for	&quot;Cache	byte	returned.&quot;	If	there	is	a	page	fault,	enter	&quot;—&quot;
for	&quot;PPN&quot;	and	leave	parts	C	and	D	blank.
Virtual	address:	</p>
<p>A
.	
Virtual	address	format
B
.	
Address	translation
Parameter
Value
VPN</p>
<hr />
<p>TLB	index</p>
<hr />
<p>TLB	tag</p>
<hr />
<p>TLB	hit?	(Y/N)</p>
<hr />
<p>Page	fault?	(Y/N)</p>
<hr />
<p>PPN</p>
<hr />
<p>C
.	
Physical	address	format
D
.	
Physical	memory	reference
Parameter
Value
Byte	offset</p>
<hr />
<p>Cache	index</p>
<hr />
<p>Cache	tag</p>
<hr />
<p>Cache	hit?	(Y/N)</p>
<hr />
<p>Cache	byte	returned</p>
<hr />
<p>9.7	
Case	Study:	The	Intel	Core
i7/Linux	Memory	System
We	conclude	our	discussion	of	virtual	memory	mechanisms	with	a	case
study	of	a	real	system:	an	Intel	Core	i7	running	Linux.	Although	the
underlying	Haswell	microarchitecture	allows	for	full	64-bit	virtual	and
physical	address	spaces,	the	current	Core	i7	implementations	(and	those
for	the	foreseeable	future)	support	a	48-bit	(256	TB)	virtual	address
space	and	a	52-bit	(4	PB)	physical	address	space,	along	with	a
compatibility	mode	that	supports	32-bit	(4	GB)	virtual	and	physical
address	spaces.
Figure	
9.21
gives	the	highlights	of	the	Core	i7	memory	system.	The
processor	package
(chip)	includes	four	cores,	a	large	L3	cache	shared	by
all	of	the	cores,	and</p>
<p>Figure	
9.21	
The	Core	i7	memory	system.</p>
<p>Figure	
9.22	
Summary	of	Core	i7	address	translation.
For	simplicity,	the	i-caches,	i-TLB,	and	L2	unified	TLB	are	not	shown.
a	DDR3	memory	controller.	Each	core	contains	a	hierarchy	of	TLBs,	a
hierarchy	of	data	and	instruction	caches,	and	a	set	of	fast	point-to-point
links,	based	on	the	QuickPath	technology,	for	communicating	directly	with
the	other	cores	and	the	external	I/O	bridge.	The	TLBs	are	virtually
addressed,	and	4-way	set	associative.	The	L1,	L2,	and	L3	caches	are
physically	addressed,	with	a	block	size	of	64	bytes.	L1	and	L2	are	8-way
set	associative,	and	L3	is	16-way	set	associative.	The	page	size	can	be</p>
<p>configured	at	start-up	time	as	either	4	KB	or	4	MB.	Linux	uses	4	KB
pages.
9.7.1	
Core	i7	Address	Translation
Figure	
9.22
summarizes	the	entire	Core	i7	address	translation
process,	from	the	time	the	CPU	generates	a	virtual	address	until	a	data
word	arrives	from	memory.	The	Core	i7	uses	a	four-level	page	table
hierarchy.	Each	process	has	its	own	private	page	table	hierarchy.	When	a
Linux	process	is	running,	the	page	tables	associated	with	allocated
pages	are	all	memory-resident,	although	the	Core	i7	architecture	allows
these	page	tables	to	be	swapped	in	and	out.	The	
CR3
control	register
contains	the	physical	address	of	the	beginning	of	the	level	1	(L1)	page
table.	The	value	of	CR3	is	part	of	each	process	context,	and	is	restored
during	each	context	switch.
Field
Description
P
Child	page	table	present	in	physical	memory	(1)	or	not	(0).
R/W
Read-only	or	read-write	access	permission	for	all	reachable	pages.
U/S
User	or	supervisor	(kernel)	mode	access	permission	for	all	reachable	pages.
WT
Write-through	or	write-back	cache	policy	for	the	child	page	table.</p>
<p>CD
Caching	disabled	or	enabled	for	the	child	page	table.
A
Reference	bit	(set	by	MMU	on	reads	and	writes,	cleared	by	software).
PS
Page	size	either	4	KB	or	4	MB	(defined	for	level	1	PTEs	only).
Base	addr
40	most	significant	bits	of	physical	base	address	of	child	page	table.
XD
Disable	or	enable	instruction	fetches	from	all	pages	reachable	from	this	PTE.
Figure	
9.23	
Format	of	level	1,	level	2,	and	level	3	page	table	entries.
Each	entry	references	a	4	KB	child	page	table.
Figure	
9.23
shows	the	format	of	an	entry	in	a	level	1,	level	2,	or	level	3
page	table.	When	
P
=	1	(which	is	always	the	case	with	Linux),	the
address	field	contains	a	40-bit	physical	page	number	(PPN)	that	points	to
the	beginning	of	the	appropriate	page	table.	Notice	that	this	imposes	a	4
KB	alignment	requirement	on	page	tables.
Figure	
9.24
shows	the	format	of	an	entry	in	a	level	4	page	table.	When
P
=	1,	the	address	field	contains	a	40-bit	PPN	that	points	to	the	base	of
some	page	in	physical	memory.	Again,	this	imposes	a	4	KB	alignment
requirement	on	physical	pages.
The	PTE	has	three	permission	bits	that	control	access	to	the	page.	The
R/W
bit	determines	whether	the	contents	of	a	page	are	read/write	or
read-only.	The	
U/S
bit,	which	determines	whether	the	page	can	be
accessed	in	user	mode,	protects	code	and	data	in	the	operating	system
kernel	from	user	programs.	The	
XD
(execute	disable)	bit,	which	was
introduced	in	64-bit	systems,	can	be	used	to	disable	instruction	fetches
from	individual	memory	pages.	This	is	an	important	new	feature	that</p>
<p>allows	the	operating	system	kernel	to	reduce	the	risk	of	buffer	overflow
attacks	by	restricting	execution	to	the	read-only	code	segment.
As	the	MMU	translates	each	virtual	address,	it	also	updates	two	other
bits	that	can	be	used	by	the	kernel's	page	fault	handler.	The	MMU	sets
the	
A
bit,	which	is	known	as	a	
reference	bit
,	each	time	a	page	is
accessed.	The	kernel	can	use	the	reference	bit	to	implement	its	page
replacement	algorithm.	The	MMU	sets	the	
D
bit,	or	
dirty	bit
,	each	time	the
page	is	written	to.	A	page	that	has	been	modified	is	sometimes	called	a
dirty	page.
The	dirty	bit	tells	the	kernel	whether	or	not	it	must
Field
Description
P
Child	page	present	in	physical	memory	(1)	or	not	(0).
R/W
Read-only	or	read/write	access	permission	for	child	page.
U/S
User	or	supervisor	mode	(kernel	mode)	access	permission	for	child	page.
WT
Write-through	or	write-back	cache	policy	for	the	child	page.
CD
Cache	disabled	or	enabled.
A
Reference	bit	(set	by	MMU	on	reads	and	writes,	cleared	by	software).
D
Dirty	bit	(set	by	MMU	on	writes,	cleared	by	software).
G
Global	page	(don't	evict	from	TLB	on	task	switch).</p>
<p>Base	addr
40	most	significant	bits	of	physical	base	address	of	child	page.
XD
Disable	or	enable	instruction	fetches	from	the	child	page.
Figure	
9.24	
Format	of	level	4	page	table	entries.
Each	entry	references	a	4	KB	child	page.
write	back	a	victim	page	before	it	copies	in	a	replacement	page.	The
kernel	can	call	a	special	kernel-mode	instruction	to	clear	the	reference	or
dirty	bits.
Figure	
9.25
shows	how	the	Core	i7	MMU	uses	the	four	levels	of	page
tables	to	translate	a	virtual	address	to	a	physical	address.	The	36-bit
VPN	is	partitioned	into	four	9-bit	chunks,	each	of	which	is	used	as	an
offset	into	a	page	table.	The	CR3	register	contains	the	physical	address
of	the	L1	page	table.	VPN	1	provides	an	offset	to	an	L1	PTE,	which
contains	the	base	address	of	the	L2	page	table.	VPN	2	provides	an	offset
to	an	L2	PTE,	and	so	on.
9.7.2	
Linux	Virtual	Memory	System
A	virtual	memory	system	requires	close	cooperation	between	the
hardware	and	the	kernel.	Details	vary	from	version	to	version,	and	a
complete	description	is	beyond	our	scope.	Nonetheless,	our	aim	in	this
section	is	to	describe	enough	of	the	Linux	virtual	memory	system	to	give
you	a	sense	of	how	a	real	operating	system	organizes	virtual	memory
and	how	it	handles	page	faults.</p>
<p>Linux	maintains	a	separate	virtual	address	space	for	each	process	of	the
form	shown	in	
Figure	
9.26
.	We	have	seen	this	picture	a	number	of
times	already,	with	its	familiar	code,	data,	heap,	shared	library,	and	stack
segments.	Now	that	we	understand	address	translation,	we	can	fill	in
some	more	details	about	the	kernel	virtual	memory	that	lies	above	the
user	stack.
The	kernel	virtual	memory	contains	the	code	and	data	structures	in	the
kernel.	Some	regions	of	the	kernel	virtual	memory	are	mapped	to
physical	pages	that
Figure	
9.25	
Core	i7	page	table	translation.</p>
<p>PT:	page	table;	PTE:	page	table	entry;	VPN:	virtual	page	number;	VPO:
virtual	page	offset;	PPN:	physical	page	number;	PPO:	physical	page
offset.	The	Linux	names	for	the	four	levels	of	page	tables	are	also	shown.
Figure	
9.26	
The	virtual	memory	of	a	Linux	process.
Aside	
Optimizing	address	translation
In	our	discussion	of	address	translation,	we	have	described	a
sequential	two-step	process	where	the	MMU	(1)	translates	the
virtual	address	to	a	physical	address	and	then	(2)	passes	the
physical	address	to	the	L1	cache.	However,	real	hardware</p>
<p>implementations	use	a	neat	trick	that	allows	these	steps	to	be
partially	overlapped,	thus	speeding	up	accesses	to	the	L1	cache.
For	example,	a	virtual	address	on	a	Core	i7	with	4	KB	pages	has
12	bits	of	VPO,	and	these	bits	are	identical	to	the	12	bits	of	PPO
in	the	corresponding	physical	address.	Since	the	8-way	set
associative	physically	addressed	L1	caches	have	64	sets	and	64-
byte	cache	blocks,	each	physical	address	has	6	(log
64)	cache
offset	bits	and	6	(log
64)	index	bits.	These	12	bits	fit	exactly	in	the
12-bit	VPO	of	a	virtual	address,	which	is	no	accident!	When	the
CPU	needs	a	virtual	address	translated,	it	sends	the	VPN	to	the
MMU	and	the	VPO	to	the	L1	cache.	While	the	MMU	is	requesting
a	page	table	entry	from	the	TLB,	the	L1	cache	is	busy	using	the
VPO	bits	to	find	the	appropriate	set	and	read	out	the	eight	tags
and	corresponding	data	words	in	that	set.	When	the	MMU	gets	the
PPN	back	from	the	TLB,	the	cache	is	ready	to	try	to	match	the
PPN	to	one	of	these	eight	tags.
are	shared	by	all	processes.	For	example,	each	process	shares	the
kernel's	code	and	global	data	structures.	Interestingly,	Linux	also	maps	a
set	of	contiguous	virtual	pages	(equal	in	size	to	the	total	amount	of
DRAM	in	the	system)	to	the	corresponding	set	of	contiguous	physical
pages.	This	provides	the	kernel	with	a	convenient	way	to	access	any
specific	location	in	physical	memory—for	example,	when	it	needs	to
access	page	tables	or	to	perform	memory-mapped	I/O	operations	on
devices	that	are	mapped	to	particular	physical	memory	locations.
Other	regions	of	kernel	virtual	memory	contain	data	that	differ	for	each
process.	Examples	include	page	tables,	the	stack	that	the	kernel	uses
when	it	is	executing	code	in	the	context	of	the	process,	and	various	data
2
2</p>
<p>structures	that	keep	track	of	the	current	organization	of	the	virtual
address	space.
Linux	Virtual	Memory	Areas
Linux	organizes	the	virtual	memory	as	a	collection	of	
areas
(also	called
segments).
An	area	is	a	contiguous	chunk	of	existing	(allocated)	virtual
memory	whose	pages	are	related	in	some	way.	For	example,	the	code
segment,	data	segment,	heap,	shared	library	segment,	and	user	stack
are	all	distinct	areas.	Each	existing	virtual	page	is	contained	in	some
area,	and	any	virtual	page	that	is	not	part	of	some	area	does	not	exist
and	cannot	be	referenced	by	the	process.	The	notion	of	an	area	is
important	because	it	allows	the	virtual	address	space	to	have	gaps.	The
kernel	does	not	keep	track	of	virtual	pages	that	do	not	exist,	and	such
pages	do	not	consume	any	additional	resources	in	memory,	on	disk,	or	in
the	kernel	itself.
Figure	
9.27
highlights	the	kernel	data	structures	that	keep	track	of	the
virtual	memory	areas	in	a	process.	The	kernel	maintains	a	distinct	task
structure	(
in	the	source	code)	for	each	process	in	the
system.	The	elements	of	the	task	structure	either	contain	or	point	to	all	of
the	information	that	the	kernel	needs	to</p>
<p>Figure	
9.27	
How	Linux	organizes	virtual	memory.
run	the	process	(e.g.,	the	PID,	pointer	to	the	user	stack,	name	of	the
executable	object	file,	and	program	counter).
One	of	the	entries	in	the	task	structure	points	to	an	
that
characterizes	the	current	state	of	the	virtual	memory.	The	two	fields	of
interest	to	us	are	
,	which	points	to	the	base	of	the	level	1	table	(the
page	global	directory),	and	
,	which	points	to	a	list	of	
(area	structs),	each	of	which	characterizes	an	area	of	the	current	virtual
address	space.	When	the	kernel	runs	this	process,	it	stores	
in	the
CR3	control	register.</p>
<p>For	our	purposes,	the	area	struct	for	a	particular	area	contains	the
following	fields:
.	
Points	to	the	beginning	of	the	area.
.	
Points	to	the	end	of	the	area.
.	
Describes	the	read/write	permissions	for	all	of	the	pages
contained	in	the	area.
.	
Describes	(among	other	things)	whether	the	pages	in	the
area	are	shared	with	other	processes	or	private	to	this	process.
.	
Points	to	the	next	area	struct	in	the	list.
Figure	
9.28	
Linux	page	fault	handling.</p>
<p>Linux	Page	Fault	Exception	Handling
Suppose	the	MMU	triggers	a	page	fault	while	trying	to	translate	some
virtual	address	
A.
The	exception	results	in	a	transfer	of	control	to	the
kernel's	page	fault	handler,	which	then	performs	the	following	steps:
1
.	
Is	virtual	address	
A
legal?	In	other	words,	does	
A
lie	within	an	area
defined	by	some	area	struct?	To	answer	this	question,	the	fault
handler	searches	the	list	of	area	structs,	comparing	
A
with	the
and	
in	each	area	struct.	If	the	instruction	is	not
legal,	then	the	fault	handler	triggers	a	segmentation	fault,	which
terminates	the	process.	This	situation	is	labeled	&quot;1&quot;	in	
Figure
9.28
.
Because	a	process	can	create	an	arbitrary	number	of	new	virtual
memory	areas	(using	the	
function	described	in	the	next
section),	a	sequential	search	of	the	list	of	area	structs	might	be
very	costly.	So	in	practice,	Linux	superimposes	a	tree	on	the	list,
using	some	fields	that	we	have	not	shown,	and	performs	the
search	on	this	tree.
2
.	
Is	the	attempted	memory	access	legal?	In	other	words,	does	the
process	have	permission	to	read,	write,	or	execute	the	pages	in
this	area?	For	example,	was	the	page	fault	the	result	of	a	store
instruction	trying	to	write	to	a	read-only	page	in	the	code	segment?
Is	the	page	fault	the	result	of	a	process	running	in	user	mode	that
is	attempting	to	read	a	word	from	kernel	virtual	memory?	If	the
attempted	access	is	not	legal,	then	the	fault	handler	triggers	a
protection	exception,	which	terminates	the	process.	This	situation
is	labeled	&quot;2&quot;	in	
Figure	
9.28
.</p>
<p>3
.	
At	this	point,	the	kernel	knows	that	the	page	fault	resulted	from	a
legal	operation	on	a	legal	virtual	address.	It	handles	the	fault	by
selecting	a	victim	page,	swapping	out	the	victim	page	if	it	is	dirty,
swapping	in	the	new	page,	
and	updating	the	page	table.	When	the
page	fault	handler	returns,	the	CPU	restarts	the	faulting
instruction,	which	sends	
A
to	the	MMU	again.	This	time,	the	MMU
translates	
A
normally,	without	generating	a	page	fault.</p>
<p>9.8	
Memory	Mapping
Linux	initializes	the	contents	of	a	virtual	memory	area	by	associating	it
with	an	
object
on	disk,	a	process	known	as	
memory	mapping.
Areas	can
be	mapped	to	one	of	two	types	of	objects:
1
.	
Regular	file	in	the	Linux	file	system:	
An	area	can	be	mapped	to
a	contiguous	section	of	a	regular	disk	file,	such	as	an	executable
object	file.	The	file	section	is	divided	into	page-size	pieces,	with
each	piece	containing	the	initial	contents	of	a	virtual	page.
Because	of	demand	paging,	none	of	these	virtual	pages	is	actually
swapped	into	physical	memory	until	the	CPU	first	
touches
the
page	(i.e.,	issues	a	virtual	address	that	falls	within	that	page's
region	of	the	address	space).	If	the	area	is	larger	than	the	file
section,	then	the	area	is	padded	with	zeros.
2
.	
Anonymous	file:	
An	area	can	also	be	mapped	to	an	anonymous
file,	created	by	the	kernel,	that	contains	all	binary	zeros.	The	first
time	the	CPU	touches	a	virtual	page	in	such	an	area,	the	kernel
finds	an	appropriate	victim	page	in	physical	memory,	swaps	out
the	victim	page	if	it	is	dirty,	overwrites	the	victim	page	with	binary
zeros,	and	updates	the	page	table	to	mark	the	page	as	resident.
Notice	that	no	data	are	actually	transferred	between	disk	and
memory.	For	this	reason,	pages	in	areas	that	are	mapped	to
anonymous	files	are	sometimes	called	
demand-zero	pages.
In	either	case,	once	a	virtual	page	is	initialized,	it	is	swapped	back	and
forth	between	a	special	
swap	file
maintained	by	the	kernel.	The	swap	file</p>
<p>is	also	known	as	the	
swap	space
or	the	
swap	area.
An	important	point	to
realize	is	that	at	any	point	in	time,	the	swap	space	bounds	the	total
amount	of	virtual	pages	that	can	be	allocated	by	the	currently	running
processes.
9.8.1	
Shared	Objects	Revisited
The	idea	of	memory	mapping	resulted	from	a	clever	insight	that	if	the
virtual	memory	system	could	be	integrated	into	the	conventional	file
system,	then	it	could	provide	a	simple	and	efficient	way	to	load	programs
and	data	into	memory.
As	we	have	seen,	the	process	abstraction	promises	to	provide	each
process	with	its	own	private	virtual	address	space	that	is	protected	from
errant	writes	or	reads	by	other	processes.	However,	many	processes
have	identical	read-only	code	areas.	For	example,	each	process	that
runs	the	Linux	shell	program	bash	has	the	same	code	area.	Further,
many	programs	need	to	access	identical	copies	of	read-only	run-time
library	code.	For	example,	every	C	program	requires	functions	from	the
standard	C	library	such	as	
.	It	would	be	extremely	wasteful	for
each	process	to	keep	duplicate	copies	of	these	commonly	used	codes	in
physical	
memory.	Fortunately,	memory	mapping	provides	us	with	a	clean
mechanism	for	controlling	how	objects	are	shared	by	multiple	processes.
An	object	can	be	mapped	into	an	area	of	virtual	memory	as	either	a
shared	object
or	
a	private	object.
If	a	process	maps	a	shared	object	into
an	area	of	its	virtual	address	space,	then	any	writes	that	the	process
makes	to	that	area	are	visible	to	any	other	processes	that	have	also</p>
<p>mapped	the	shared	object	into	their	virtual	memory.	Further,	the	changes
are	also	reflected	in	the	original	object	on	disk.
Changes	made	to	an	area	mapped	to	a	private	object,	on	the	other	hand,
are	not	visible	to	other	processes,	and	any	writes	that	the	process	makes
to	the	area	are	
not
reflected	back	to	the	object	on	disk.	A	virtual	memory
area	into	which	a	shared	object	is	mapped	is	often	called	a	
shared	area.
Similarly	for	
a	private	area.
Suppose	that	process	1	maps	a	shared	object	into	an	area	of	its	virtual
memory,	as	shown	in	
Figure	
9.29(a)
.	Now	suppose	that	process	2
maps	the	same	shared	object</p>
<p>Figure	
9.29	
A	shared	object.
(a)	After	process	1	maps	the	shared	object,	(b)	After	process	2	maps	the
same	shared	object.	(Note	that	the	physical	pages	are	not	necessarily
contiguous.)</p>
<p>Figure	
9.30	
A	private	copy-on-write	object.
(a)	After	both	processes	have	mapped	the	private	copy-on-write	object,
(b)	After	process	2	writes	to	a	page	in	the	private	area.</p>
<p>into	its	address	space	(not	necessarily	at	the	same	virtual	address	as
process	1),	as	shown	in	
Figure	
9.29(b)
.
Since	each	object	has	a	unique	filename,	the	kernel	can	quickly
determine	that	process	1	has	already	mapped	this	object	and	can	point
the	page	table	entries	in	process	2	to	the	appropriate	physical	pages.
The	key	point	is	that	only	a	single	copy	of	the	shared	object	needs	to	be
stored	in	physical	memory,	even	though	the	object	is	mapped	into
multiple	shared	areas.	For	convenience,	we	have	shown	the	physical
pages	as	being	contiguous,	but	of	course	this	is	not	true	in	general.
Private	objects	are	mapped	into	virtual	memory	using	a	clever	technique
known	as	
copy-on-write.
A	private	object	begins	life	in	exactly	the	same
way	as	a	shared	object,	with	only	one	copy	of	the	private	object	stored	in
physical	memory.	For	example,	
Figure	
9.30(a)
shows	a	case	where
two	processes	have	mapped	a	private	object	into	different	areas	of	their
virtual	memories	but	share	the	same	
physical	copy	of	the	object.	For
each	process	that	maps	the	private	object,	the	page	table	entries	for	the
corresponding	private	area	are	flagged	as	read-only,	and	the	area	struct
is	flagged	
as	private	copy-on-write.
So	long	as	neither	process	attempts
to	write	to	its	respective	private	area,	they	continue	to	share	a	single	copy
of	the	object	in	physical	memory.	However,	as	soon	as	a	process
attempts	to	write	to	some	page	in	the	private	area,	the	write	triggers	a
protection	fault.
When	the	fault	handler	notices	that	the	protection	exception	was	caused
by	the	process	trying	to	write	to	a	page	in	a	private	copy-on-write	area,	it
creates	a	new	copy	of	the	page	in	physical	memory,	updates	the	page
table	entry	to	point	to	the	new	copy,	and	then	restores	write	permissions
to	the	page,	as	shown	in	
Figure	
9.30(b)
.	When	the	fault	handler</p>
<p>returns,	the	CPU	re-executes	the	write,	which	now	proceeds	normally	on
the	newly	created	page.
By	deferring	the	copying	of	the	pages	in	private	objects	until	the	last
possible	moment,	copy-on-write	makes	the	most	efficient	use	of	scarce
physical	memory.
9.8.2	
The	
Function	Revisited
Now	that	we	understand	virtual	memory	and	memory	mapping,	we	can
get	a	clear	idea	of	how	the	
function	creates	a	new	process	with	its
own	independent	virtual	address	space.
When	the	
function	is	called	by	the	
current	process
,	the	kernel
creates	various	data	structures	for	the	
new	process
and	assigns	it	a
unique	PID.	To	create	the	virtual	memory	for	the	new	process,	it	creates
exact	copies	of	the	current	process's	
,	area	structs,	and	page
tables.	It	flags	each	page	in	both	processes	as	read-only,	and	flags	each
area	struct	in	both	processes	as	private	copy-on-write.
When	the	
returns	in	the	new	process,	the	new	process	now	has	an
exact	copy	of	the	virtual	memory	as	it	existed	when	the	fork	was	called.
When	either	of	the	processes	performs	any	subsequent	writes,	the	copy-
on-write	mechanism	creates	new	pages,	thus	preserving	the	abstraction
of	a	private	address	space	for	each	process.</p>
<p>9.8.3	
The	execve	Function	Revisited
Virtual	memory	and	memory	mapping	also	play	key	roles	in	the	process
of	loading	programs	into	memory.	Now	that	we	understand	these
concepts,	we	can	understand	how	the	
function	really	loads	and
executes	programs.	Suppose	that	the	program	running	in	the	current
process	makes	the	following	call:
As	you	learned	in	
Chapter	
8
,	the	
function	loads	and	runs	the
program	contained	in	the	executable	object	file	
within	the	current
process,	effectively	replacing	the	current	program	with	the	
program.	Loading	and	running	
requires	the	following	steps:</p>
<p>Figure	
9.31	
How	the	loader	maps	the	areas	of	the	user	address
space.
1
.	
Delete	existing	user	areas.	
Delete	the	existing	area	structs	in	the
user	portion	of	the	current	process's	virtual	address.
2
.	
Map	private	areas.	
Create	new	area	structs	for	the	code,	data,
bss,	and	stack	areas	of	the	new	program.	All	of	these	new	areas
are	private	copy-on-write.	The	code	and	data	areas	are	mapped	to
the	
and	
sections	of	the	
file.	The	bss	area	is
demand-zero,	mapped	to	an	anonymous	file	whose	size	is
contained	in	
.	The	stack	and	heap	area	are	also	demand-
zero,	initially	of	zero	length.	
Figure	
9.31
summarizes	the
different	mappings	of	the	private	areas.</p>
<p>3
.	
Map	shared	areas.	
If	the	
program	was	linked	with	shared
objects,	such	as	the	standard	C	library	
,	then	these	objects
are	dynamically	linked	into	the	program,	and	then	mapped	into	the
shared	region	of	the	user's	virtual	address	space.
4
.	
Set	the	program	counter	(PC).	
The	last	thing	that	
does	is
to	set	the	program	counter	in	the	current	process's	context	to	point
to	the	entry	point	in	the	code	area.
The	next	time	this	process	is	scheduled,	it	will	begin	execution	from	the
entry	point.	Linux	will	swap	in	code	and	data	pages	as	needed.
9.8.4	
User-Level	Memory	Mapping
with	the	
Function
Linux	processes	can	use	the	
function	to	create	new	areas	of	virtual
memory	and	to	map	objects	into	these	areas.</p>
<p>Figure	
9.32	
Visual	interpretation	of	
arguments.
The	
function	asks	the	kernel	to	create	a	new	virtual	memory	area,
preferably	one	that	starts	at	address	
,	and	to	map	a	contiguous
chunk	of	the	object	specified	by	file	descriptor	
to	the	new	area.	The
contiguous	object	chunk	has	a	size	of	length	bytes	and	starts	at	an
of	offset	bytes	from	the	beginning	of	the	file.	The	
address	is
merely	a	hint,	and	is	usually	specified	as	NULL.	For	our	purposes,	we	will</p>
<p>always	assume	a	NULL	start	address.	
Figure	
9.32
depicts	the
meaning	of	these	arguments.
The	
argument	contains	bits	that	describe	the	access	permissions	of
the	newly	mapped	virtual	memory	area	(i.e.,	the	
bits	in	the
corresponding	area	struct).
PROT_EXEC.	
Pages	in	the	area	consist	of	instructions	that	may	be
executed	by	the	CPU.
PROT_READ.	
Pages	in	the	area	may	be	read.
PROT_WRITE.	
Pages	in	the	area	may	be	written.
PROT_NONE.	
Pages	in	the	area	cannot	be	accessed.
The	
argument	consists	of	bits	that	describe	the	type	of	the	mapped
object.	If	the	MAP_ANON	flag	bit	is	set,	then	the	backing	store	is	an
anonymous	object	and	the	corresponding	virtual	pages	are	demand-zero.
MAP_PRIVATE	indicates	a	private	copy-on-write	object,	and
MAP_SHARED	indicates	a	shared	object.	For	example,
asks	the	kernel	to	create	a	new	read-only,	private,	demand-zero	area	of
virtual	memory	containing	size	bytes.	If	the	call	is	successful,	then	
contains	the	address	of	the	new	area.</p>
<p>The	
function	deletes	regions	of	virtual	memory:
The	
function	deletes	the	area	starting	at	virtual	address	
and
consisting	of	the	next	
bytes.	Subsequent	references	to	the
deleted	region	result	in	segmentation	faults.
Practice	Problem	
9.5	
(solution	page	
882
)
Write	a	C	program	
that	uses	
to	copy	an	arbitrary-
size	disk	file	to	
.	The	name	of	the	input	file	should	be
passed	as	a	command-line	argument.</p>
<p>9.9	
Dynamic	Memory	Allocation
While	it	is	certainly	possible	to	use	the	low-level	
and	
functions	to	create	and	delete	areas	of	virtual	memory,	C	programmers
typically	find	it	more	convenient	and	more	portable	to	use	a	
dynamic
memory	allocator
when	they	need	to	acquire	additional	virtual	memory	at
run	time.
A	dynamic	memory	allocator	maintains	an	area	of	a	process's	virtual
memory	known	as	the	
heap
(
Figure	
9.33
).	Details	vary	from	system	to
system,	but	without	loss	of	generality,	we	will	assume	that	the	heap	is	an
area	of	demand-zero	memory	that	begins	immediately	after	the
uninitialized	data	area	and	grows	upward	(toward	higher	addresses).	For
each	process,	the	kernel	maintains	a	variable	
(pronounced	&quot;break&quot;)
that	points	to	the	top	of	the	heap.
An	allocator	maintains	the	heap	as	a	collection	of	various-size	
blocks.
Each	block	is	a	contiguous	chunk	of	virtual	memory	that	is	either
allocated
or	
free.
An	allocated	block	has	been	explicitly	reserved	for	use
by	the	application.	A	free	block	is	available	to	be	allocated.	A	free	block
remains	free	until	it	is	explicitly	allocated	by	the	application.	An	allocated
block	remains	allocated	until	it	is	freed,	either	explicitly	by	the	application
or	implicitly	by	the	memory	allocator	itself.
Allocators	come	in	two	basic	styles.	Both	styles	require	the	application	to
explicitly	allocate	blocks.	They	differ	about	which	entity	is	responsible	for
freeing	allocated	blocks.</p>
<p>Explicit	allocators	
require	the	application	to	explicitly	free	any
allocated	blocks.	For	example,	the	C	standard	library	provides	an
explicit	allocator	called	the	
package.	C	programs	allocate	a
block	by	calling	the	
Figure	
9.33	
The	heap.
function,	and	free	a	block	by	calling	the	free	function.	The	
and
calls	in	
are	comparable.
Implicit	allocators,	
on	the	other	hand,	require	the	allocator	to	detect
when	an	allocated	block	is	no	longer	being	used	by	the	program	and
then	free	the	block.	Implicit	allocators	are	also	known	as	
garbage
collectors
,	and	the	process	of	automatically	freeing	unused	allocated
blocks	is	known	as	
garbage	collection.
For	example,	higher-level</p>
<p>languages	such	as	Lisp,	ML,	and	Java	rely	on	garbage	collection	to
free	allocated	blocks.
The	remainder	of	this	section	discusses	the	design	and	implementation
of	explicit	allocators.	We	will	discuss	implicit	allocators	in	
Section	
9.10
.
For	concrete	-ness,	our	discussion	focuses	on	allocators	that	manage
heap	memory.	However,	you	should	be	aware	that	memory	allocation	is	a
general	idea	that	arises	in	a	variety	of	contexts.	For	example,
applications	that	do	intensive	manipulation	of	graphs	will	often	use	the
standard	allocator	to	acquire	a	large	block	of	virtual	memory	and	then
use	an	application-specific	allocator	to	manage	the	memory	within	that
block	as	the	nodes	of	the	graph	are	created	and	destroyed.
9.9.1	
The	
and	
Functions
The	C	standard	library	provides	an	explicit	allocator	known	as	the	
package.	Programs	allocate	blocks	from	the	heap	by	calling	the	
function.</p>
<p>Aside	
How	big	is	a	word?
Recall	from	our	discussion	of	machine	code	in	
Chapter	
3
that
Intel	refers	to	4-byte	objects	as	
double	words.
However,
throughout	this	section,	we	will	assume	that	
words
are	4-byte
objects	and	that	
double	words
are	8-byte	objects,	which	is
consistent	with	conventional	terminology.
The	
function	returns	a	pointer	to	a	block	of	memory	of	at	least
size	bytes	that	is	suitably	aligned	for	any	kind	of	data	object	that	might	be
contained	in	the	block.	In	practice,	the	alignment	depends	on	whether	the
code	is	compiled	to	run	in	32-bit	mode	(
)	or	64-bit	mode	(the
default).	In	32-bit	mode,	
returns	a	block	whose	address	is	always
a	multiple	of	8.	In	64-bit	mode,	the	address	is	always	a	multiple	of	16.
If	
encounters	a	problem	(e.g.,	the	program	requests	a	block	of
memory	that	is	larger	than	the	available	virtual	memory),	then	it	returns
NULL	and	sets	
.	
does	not	initialize	the	memory	it	returns.
Applications	that	want	initialized	dynamic	memory	can	use	
,	a	thin
wrapper	around	the	
function	that	initializes	the	allocated	memory
to	
.	Applications	that	want	to	change	the	size	of	a	previously
allocated	block	can	use	the	
function.
Dynamic	memory	allocators	such	as	
can	allocate	or	deallocate
heap	memory	explicitly	by	using	the	
and	
functions,	or	they
can	use	the	
function:</p>
<p>The	
function	grows	or	shrinks	the	heap	by	adding	
to	the
kernel's	
pointer.	If	successful,	it	returns	the	old	value	of	
,
otherwise	it	returns	–1	and	sets	
to	ENOMEM.	If	
is	zero,	then
returns	the	current	value	of	
.	Calling	
with	a	negative	
is	legal	but	tricky	because	the	return	value	(the	old	value	of	
)	points	to
bytes	past	the	new	top	of	the	heap.
Programs	free	allocated	heap	blocks	by	calling	the	free	function.
The	
argument	must	point	to	the	beginning	of	an	allocated	block	that
was	obtained	from	
,	or	
.	If	not,	then	the	behavior	of
is	undefined.	Even	worse,	since	it	returns	nothing,	
gives	no</p>
<p>indication	to	the	application	that	something	is	wrong.	As	we	shall	see	in
Section	
9.11
,	this	can	produce	some	baffling	run-time	errors.
Figure	
9.34	
Allocating	and	freeing	blocks	with	</p>
<p>and</p>
<p>.
Each	square	corresponds	to	a	word.	Each	heavy	rectangle	corresponds
to	a	block.	Allocated	blocks	are	shaded.	Padded	regions	of	allocated
blocks	are	shaded	with	a	darker	blue.	Free	blocks	are	unshaded.	Heap
addresses	increase	from	left	to	right.
Figure	
9.34
shows	how	an	implementation	of	
and	
might
manage	a	(very)	small	heap	of	16	words	for	a	C	program.	Each	box
represents	a	4-byte	word.	The	heavy-lined	rectangles	correspond	to</p>
<p>allocated	blocks	(shaded)	and	free	blocks	(unshaded).	Initially,	the	heap
consists	of	a	single	16-word	double-word-aligned	free	block.</p>
<ol>
<li></li>
</ol>
<p>Throughout	this	section,	we	will	assume	that	the	allocator	returns	blocks	aligned	to	8-byte
double-word	boundaries.
Figure	
9.34(a)
.	
The	program	asks	for	a	four-word	block.	
responds	by	carving	out	a	four-word	block	from	the	front	of	the	free
block	and	returning	a	pointer	to	the	first	word	of	the	block.
Figure	
9.34(b)
.	
The	program	requests	a	five-word	block.	
responds	by	allocating	a	six-word	block	from	the	front	of	the	free
block.	In	this	example,	
pads	the	block	with	an	extra	word	in
order	to	keep	the	free	block	aligned	on	a	double-word	boundary.
Figure	
9.34(c)
.	
The	program	requests	a	six-word	block	and	
responds	by	carving	out	a	six-word	block	from	the	free	block.
Figure	
9.34(d)
.	
The	program	frees	the	six-word	block	that	was
allocated	in	
Figure	
9.34(b)
.	Notice	that	after	the	call	to	
returns,	the	pointer	</p>
<p>still	points	to	the	freed	block.	It	is	the
responsibility	of	the	application	not	to	use	
again	until	it	is
reinitialized	by	a	new	call	to	
.
Figure	
9.34(e)
.	
The	program	requests	a	two-word	block.	In	this
case,	
allocates	a	portion	of	the	block	that	was	freed	in	the
previous	step	and	returns	a	pointer	to	this	new	block.
9.9.2	
Why	Dynamic	Memory
1</p>
<p>Allocation?
The	most	important	reason	that	programs	use	dynamic	memory
allocation	is	that	often	they	do	not	know	the	sizes	of	certain	data
structures	until	the	program	actually	runs.	For	example,	suppose	we	are
asked	to	write	a	C	program	that	reads	a	list	of	
n
ASCII	integers,	one
integer	per	line,	from	
into	a	C	array.	The	input	consists	of	the
integer	
n
,	followed	by	the	
n
integers	to	be	read	and	stored	into	the	array.
The	simplest	approach	is	to	define	the	array	statically	with	some	hard-
coded	maximum	array	size:</p>
<p>Allocating	arrays	with	hard-coded	sizes	like	this	is	often	a	bad	idea.	The
value	of	MAXN	is	arbitrary	and	has	no	relation	to	the	actual	amount	of
available	virtual	memory	on	the	machine.	Further,	if	the	user	of	this
program	wanted	to	read	a	file	that	was	larger	than	MAXN,	the	only
recourse	would	be	to	recompile	the	program	with	a	larger	value	of	MAXN.
While	not	a	problem	for	this	simple	example,	the	presence	of	hard-coded
array	bounds	can	become	a	maintenance	nightmare	for	large	software
products	with	millions	of	lines	of	code	and	numerous	users.
A	better	approach	is	to	allocate	the	array	dynamically,	at	run	time,	after
the	value	of	
n
becomes	known.	With	this	approach,	the	maximum	size	of
the	array	is	limited	only	by	the	amount	of	available	virtual	memory.</p>
<p>Dynamic	memory	allocation	is	a	useful	and	important	programming
technique.	However,	in	order	to	use	allocators	correctly	and	efficiently,
programmers	need	to	have	an	understanding	of	how	they	work.	We	will
discuss	some	of	the	gruesome	errors	that	can	result	from	the	improper
use	of	allocators	in	
Section	
9.11
.
9.9.3	
Allocator	Requirements	and
Goals
Explicit	allocators	must	operate	within	some	rather	stringent	constraints:
Handling	arbitrary	request	sequences.	
An	application	can	make	an
arbitrary	sequence	of	allocate	and	free	requests,	subject	to	the
constraint	that	each	free	request	must	correspond	to	a	currently
allocated	block	obtained	from	a	previous	allocate	request.	Thus,	the
allocator	cannot	make	any	assumptions	about	the	ordering	of	allocate
and	free	requests.	For	example,	the	allocator	cannot	assume	that	all
allocate	requests	are	accompanied	by	a	matching	free	request,	or	that
matching	allocate	and	free	requests	are	nested.
Making	immediate	responses	to	requests.	
The	allocator	must
respond	immediately	to	allocate	requests.	Thus,	the	allocator	is	not
allowed	to	reorder	or	buffer	requests	in	order	to	improve	performance.
Using	only	the	heap.	
In	order	for	the	allocator	to	be	scalable,	any
nonscalar	data	structures	used	by	the	allocator	must	be	stored	in	the
heap	itself.</p>
<p>Aligning	blocks	(alignment	requirement).	
The	allocator	must	align
blocks	in	such	a	way	that	they	can	hold	any	type	of	data	object.
Not	modifying	allocated	blocks.	
Allocators	can	only	manipulate	or
change	free	blocks.	In	particular,	they	are	not	allowed	to	modify	or
move	blocks	once	they	are	allocated.	Thus,	techniques	such	as
compaction	of	allocated	blocks	are	not	permitted.
Working	within	these	constraints,	the	author	of	an	allocator	attempts	to
meet	the	often	conflicting	performance	goals	of	maximizing	throughput
and	memory	utilization.
Goal	1:	Maximizing	throughput.
Given	some	sequence	of	
n
allocate
and	free	requests
we	would	like	to	maximize	an	allocator's	
throughput
,	which	is	defined	as
the	number	of	requests	that	it	completes	per	unit	time.	For	example,	if	an
allocator	completes	500	allocate	requests	and	500	free	requests	in	1
second,	then	its	throughput	is	1,000	operations	per	second.	In	general,
we	can	maximize	throughput	by	minimizing	the	average	time	to	satisfy
allocate	and	free	requests.	As	we'll	see,	it	is	not	too	difficult	to	develop
allocators	with	reasonably	good	performance	where	the	worst-case
running	time	of	an	allocate	request	is	linear	in	the	number	of	free	blocks
and	the	running	time	of	a	free	request	is	constant.
Goal	2:	Maximizing	memory	utilization.
Naive	programmers	often
incorrectly	assume	that	virtual	memory	is	an	unlimited	resource.	In	fact,
the	total	amount	of	virtual	memory	allocated	by	all	of	the	processes	in	a
R
0
,
 
R
1
,
…
,
 
R
k
,
…
,
 
R
n
−
1</p>
<h1>system	is	limited	by	the	amount	of	swap	space	on	disk.	Good
programmers	know	that	virtual	memory	is	a	finite	resource	that	must	be
used	efficiently.	This	is	especially	true	for	a	dynamic	memory	allocator
that	might	be	asked	to	allocate	and	free	large	blocks	of	memory.
There	are	a	number	of	ways	to	characterize	how	efficiently	an	allocator
uses	the	heap.	In	our	experience,	the	most	useful	metric	is	
peak
utilization.
As	before,	we	are	given	some	sequence	of	
n
allocate	and	free
requests
If	an	application	requests	a	block	of	
p
bytes,	then	the	resulting	allocated
block	has	a	
payload
of	
p
bytes.	After	request	
R
has	completed,	let	the
aggregate	payload
,	denoted	
P
,	be	the	sum	of	the	pay	loads	of	the
currently	allocated	blocks,	and	let	
H
denote	the	current	(monotonically
nondecreasing)	size	of	the	heap.
Then	the	peak	utilization	over	the	first	
k
+	1	requests,	denoted	by	
U
,	is
given	by
The	objective	of	the	allocator,	then,	is	to	maximize	the	peak	utilization	
U
over	the	entire	sequence.	As	we	will	see,	there	is	a	tension	between
maximizing	throughput	and	utilization.	In	particular,	it	is	easy	to	write	an
allocator	that	maximizes	throughput	at	the	expense	of	heap	utilization.
One	of	the	interesting	challenges	in	any	allocator	design	is	finding	an
appropriate	balance	between	the	two	goals.
R
0
,
 
R
1
,
…
,
 
R
k
,
…
,
 
R
n
−
1
k
k
k
k
U
k</h1>
<p>max
i
≤
k
 
P
i
H
k
n
–
1</p>
<p>Aside	
Relaxing	the	monotonicity
assumption
We	could	relax	the	monotonically	nondecreasing	assumption	in
our	definition	of	
U
and	allow	the	heap	to	grow	up	and	down	by
letting	
H
be	the	high-water	mark	over	the	first	
k
+	1	requests.
9.9.4	
Fragmentation
The	primary	cause	of	poor	heap	utilization	is	a	phenomenon	known	as
fragmentation
,	which	occurs	when	otherwise	unused	memory	is	not
available	to	satisfy	allocate	requests.	There	are	two	forms	of
fragmentation:	
internal	fragmentation
and	
external	fragmentation.
Internal	fragmentation
occurs	when	an	allocated	block	is	larger	than	the
pay-load.	This	might	happen	for	a	number	of	reasons.	For	example,	the
implementation	of	an	allocator	might	impose	a	minimum	size	on	allocated
blocks	that	is	greater	than	some	requested	payload.	Or,	as	we	saw	in
Figure	
9.34(b)
,	the	allocator	might	increase	the	block	size	in	order	to
satisfy	alignment	constraints.
Internal	fragmentation	is	straightforward	to	quantify.	It	is	simply	the	sum
of	the	differences	between	the	sizes	of	the	allocated	blocks	and	their
payloads.	Thus,	at	any	point	in	time,	the	amount	of	internal	fragmentation
depends	only	on	the	pattern	of	previous	requests	and	the	allocator
implementation.
k
k</p>
<p>External	fragmentation
occurs	when	there	
is
enough	aggregate	free
memory	to	satisfy	an	allocate	request,	but	no	single	free	block	is	large
enough	to	handle	the	request.	For	example,	if	the	request	in	
Figure
9.34(e)
were	for	eight	words	rather	than	two	words,	then	the	request
could	not	be	satisfied	without	requesting	additional	virtual	memory	from
the	kernel,	even	though	there	are	eight	free	words	remaining	in	the	heap.
The	problem	arises	because	these	eight	words	are	spread	over	two	free
blocks.
External	fragmentation	is	much	more	difficult	to	quantify	than	internal
fragmentation	because	it	depends	not	only	on	the	pattern	of	previous
requests	and	the	allocator	implementation	but	also	on	the	pattern	of
future
requests.	For	example,	suppose	that	after	
k
requests	all	of	the	free
blocks	are	exactly	four	words	in	size.	Does	this	heap	suffer	from	external
fragmentation?	The	answer	depends	on	the	pattern	of	future	requests.	If
all	of	the	future	allocate	requests	are	for	blocks	that	are	smaller	than	or
equal	to	four	words,	then	there	is	no	external	fragmentation.	On	the	other
hand,	if	one	or	more	requests	ask	for	blocks	larger	than	four	words,	then
the	heap	does	suffer	from	external	fragmentation.
Since	external	fragmentation	is	difficult	to	quantify	and	impossible	to
predict,	allocators	typically	employ	heuristics	that	attempt	to	maintain
small	numbers	of	larger	free	blocks	rather	than	large	numbers	of	smaller
free	blocks.
9.9.5	
Implementation	Issues</p>
<p>The	simplest	imaginable	allocator	would	organize	the	heap	as	a	large
array	of	bytes	and	a	pointer	
that	initially	points	to	the	first	byte	of	the
array.	To	allocate	
bytes,	
would	save	the	current	value	of	
on	the	stack,	increment	
by	
,	and	return	the	old	value	of	
to	the
caller.	
would	simply	return	to	the	caller	without	doing	anything.
This	naive	allocator	is	an	extreme	point	in	the	design	space.	Since	each
and	
execute	only	a	handful	of	instructions,	throughput	would
be	extremely	good.	However,	since	the	allocator	never	reuses	any
blocks,	memory	utilization	would	be	extremely	bad.	A	practical	allocator
that	strikes	a	better	balance	between	throughput	and	utilization	must
consider	the	following	issues:
Free	block	organization.	
How	do	we	keep	track	of	free	blocks?
Placement.	
How	do	we	choose	an	appropriate	free	block	in	which	to
place	a	newly	allocated	block?
Splitting.	
After	we	place	a	newly	allocated	block	in	some	free	block,
what	do	we	do	with	the	remainder	of	the	free	block?
Coalescing.	
What	do	we	do	with	a	block	that	has	just	been	freed?
The	rest	of	this	section	looks	at	these	issues	in	more	detail.	Since	the
basic	techniques	of	placement,	splitting,	and	coalescing	cut	across	many
different	free	block	organizations,	we	will	introduce	them	in	the	context	of
a	simple	free	block	organization	known	as	an	implicit	free	list.
9.9.6	
Implicit	Free	Lists</p>
<p>Any	practical	allocator	needs	some	data	structure	that	allows	it	to
distinguish	block	boundaries	and	to	distinguish	between	allocated	and
free	blocks.	Most	allocators	embed	this	information	in	the	blocks
themselves.	One	simple	approach	is	shown	in	
Figure	
9.35
.
In	this	case,	a	block	consists	of	a	one-word	
header
,	the	payload,	and
possibly	some	additional	
padding.
The	header	encodes	the	block	size
(including	the	header	and	any	padding)	as	well	as	whether	the	block	is
allocated	or	free.	If	we	impose	a	double-word	alignment	constraint,	then
the	block	size	is	always	a	multiple	of	8	and	the	3	low-order	bits	of	the
block	size	are	always	zero.	Thus,	we	need	to	store	only	the	29	high-order
bits	of	the	block	size,	freeing	the	remaining	3	bits	to	encode	other
information.	In	this	case,	we	are	using	the	least	significant	of	these	bits
Figure	
9.35	
Format	of	a	simple	heap	block.
Figure	
9.36	
Organizing	the	heap	with	an	implicit	free	list.</p>
<p>Allocated	blocks	are	shaded.	Free	blocks	are	unshaded.	Headers	are
labeled	with	(size	(bytes)/allocated	bit).
(the	
allocated	bit)
to	indicate	whether	the	block	is	allocated	or	free.	For
example,	suppose	we	have	an	allocated	block	with	a	block	size	of	24
(
)	bytes.	Then	its	header	would	be
Similarly,	a	free	block	with	a	block	size	of	40	(
)	bytes	would	have	a
header	of
The	header	is	followed	by	the	payload	that	the	application	requested
when	it	called	
.	The	payload	is	followed	by	a	chunk	of	unused
padding	that	can	be	any	size.	There	are	a	number	of	reasons	for	the
padding.	For	example,	the	padding	might	be	part	of	an	allocator's
strategy	for	combating	external	fragmentation.	Or	it	might	be	needed	to
satisfy	the	alignment	requirement.
Given	the	block	format	in	
Figure	
9.35
,	we	can	organize	the	heap	as	a
sequence	of	contiguous	allocated	and	free	blocks,	as	shown	in	
Figure
9.36
.</p>
<p>We	call	this	organization	an	
implicit	free	list
because	the	free	blocks	are
linked	implicitly	by	the	size	fields	in	the	headers.	The	allocator	can
indirectly	traverse	the	entire	set	of	free	blocks	by	traversing	
all
of	the
blocks	in	the	heap.	Notice	that	we	need	some	kind	of	specially	marked
end	block—in	this	example,	a	terminating	header	with	the	allocated	bit
set	and	a	size	of	zero.	(As	we	will	see	in	
Section	
9.9.12
,	setting	the
allocated	bit	simplifies	the	coalescing	of	free	blocks.)
The	advantage	of	an	implicit	free	list	is	simplicity.	A	significant
disadvantage	is	that	the	cost	of	any	operation	that	requires	a	search	of
the	free	list,	such	as	placing	allocated	blocks,	will	be	linear	in	the	
total
number	of	allocated	and	free	blocks	in	the	heap.
It	is	important	to	realize	that	the	system's	alignment	requirement	and	the
allocator's	choice	of	block	format	impose	a	
minimum	block	size
on	the
allocator.	No	allocated	or	free	block	may	be	smaller	than	this	minimum.
For	example,	if	we	assume	a	double-word	alignment	requirement,	then
the	size	of	each	block	must	be	a	multiple	of	two	words	(8	bytes).	Thus,
the	block	format	in	
Figure	
9.35
induces	a	minimum	block	size	of	two
words:	one	word	for	the	header	and	another	to	maintain	the	alignment
requirement.	Even	if	the	application	were	to	request	a	single	byte,	the
allocator	would	still	create	a	two-word	block.
Practice	Problem	
9.6	
(solution	page	
883
)
Determine	the	block	sizes	and	header	values	that	would	result
from	the	following	sequence	of	
requests.	Assumptions:	(1)
The	allocator	maintains	double-word	alignment	and	uses	an</p>
<p>implicit	free	list	with	the	block	format	from	
Figure	
9.35
.	(2)	Block
sizes	are	rounded	up	to	the	nearest	multiple	of	8	bytes.
Request
Block	size	(decimal	bytes)
Block	header	(hex)</p>
<hr />
<hr />
<hr />
<hr />
<hr />
<hr />
<hr />
<hr />
<p>9.9.7	
Placing	Allocated	Blocks
When	an	application	requests	a	block	of	
k
bytes,	the	allocator	searches
the	free	list	for	a	free	block	that	is	large	enough	to	hold	the	requested
block.	The	manner	in	which	the	allocator	performs	this	search	is
determined	by	the	
placement	policy.
Some	common	policies	are	first	fit,
next	fit,	and	best	fit.
First	fit
searches	the	free	list	from	the	beginning	and	chooses	the	first
free	block	that	fits.	
Next	fit
is	similar	to	first	fit,	but	instead	of	starting	each
search	at	the	beginning	of	the	list,	it	starts	each	search	where	the
previous	search	left	off.	
Best	fit
examines	every	free	block	and	chooses
the	free	block	with	the	smallest	size	that	fits.
An	advantage	of	first	fit	is	that	it	tends	to	retain	large	free	blocks	at	the
end	of	the	list.	A	disadvantage	is	that	it	tends	to	leave	&quot;splinters&quot;	of	small
free	blocks	toward	the	beginning	of	the	list,	which	will	increase	the	search</p>
<p>time	for	larger	blocks.	Next	fit	was	first	proposed	by	Donald	Knuth	as	an
alternative	to	first	fit,	motivated	by	the	idea	that	if	we	found	a	fit	in	some
free	block	the	last	time,	there	is	a	good	chance	that	we	will	find	a	fit	the
next	time	in	the	remainder	of	the	block.	Next	fit	can	run	significantly	faster
than	first	fit,	especially	if	the	front	of	the	list	becomes	littered	with	many
small	splinters.	However,	some	studies	suggest	that	next	fit	suffers	from
worse	memory	utilization	than	first	fit.	Studies	have	found	that	best	fit
generally	enjoys	better	memory	utilization	than	either	first	fit	or	next	fit.
However,	the	disadvantage	of	using	best	fit	with	simple	free	list
organizations	such	as	the	implicit	free	list	is	that	it	requires	an	exhaustive
search	of	the	heap.	Later,	we	will	look	at	more	sophisticated	segregated
free	list	organizations	that	approximate	a	best-fit	policy	without	an
exhaustive	search	of	the	heap.
9.9.8	
Splitting	Free	Blocks
Once	the	allocator	has	located	a	free	block	that	fits,	it	must	make	another
policy	decision	about	how	much	of	the	free	block	to	allocate.	One	option
is	to	use	the	entire	free	block.	Although	simple	and	fast,	the	main
disadvantage	is	that	it
Figure	
9.37	
Splitting	a	free	block	to	satisfy	a	three-word	allocation
request.</p>
<p>Allocated	blocks	are	shaded.	Free	blocks	are	unshaded.	Headers	are
labeled	with	(size	(bytes)/allocated	bit).
introduces	internal	fragmentation.	If	the	placement	policy	tends	to
produce	good	fits,	then	some	additional	internal	fragmentation	might	be
acceptable.
However,	if	the	fit	is	not	good,	then	the	allocator	will	usually	opt	to	
split
the	free	block	into	two	parts.	The	first	part	becomes	the	allocated	block,
and	the	remainder	becomes	a	new	free	block.	
Figure	
9.37
shows	how
the	allocator	might	split	the	eight-word	free	block	in	
Figure	
9.36
to
satisfy	an	application's	request	for	three	words	of	heap	memory.
9.9.9	
Getting	Additional	Heap
Memory
What	happens	if	the	allocator	is	unable	to	find	a	fit	for	the	requested
block?	One	option	is	to	try	to	create	some	larger	free	blocks	by	merging
(coalescing)	free	blocks	that	are	physically	adjacent	in	memory	(next
section).	However,	if	this	does	not	yield	a	sufficiently	large	block,	or	if	the
free	blocks	are	already	maximally	coalesced,	then	the	allocator	asks	the
kernel	for	additional	heap	memory	by	calling	the	
function.	The
allocator	transforms	the	additional	memory	into	one	large	free	block,
inserts	the	block	into	the	free	list,	and	then	places	the	requested	block	in
this	new	free	block.</p>
<p>9.9.10	
Coalescing	Free	Blocks
When	the	allocator	frees	an	allocated	block,	there	might	be	other	free
blocks	that	are	adjacent	to	the	newly	freed	block.	Such	adjacent	free
blocks	can	cause	a	phenomenon	known	as,	
false	fragmentation
,	where
there	is	a	lot	of	available	free	memory	chopped	up	into	small,	unusable
free	blocks.	For	example,	
Figure	
9.38
shows	the	result	of	freeing	the
block	that	was	allocated	in	
Figure	
9.37
.	The	result	is	two	adjacent	free
blocks	with	payloads	of	three	words	each.	As	a	result,	a	subsequent
request	for	a	payload	of	four	words	would	fail,	even	though	the	aggregate
size	of	the	two	free	blocks	is	large	enough	to	satisfy	the	request.
To	combat	false	fragmentation,	any	practical	allocator	must	merge
adjacent	free	blocks	in	a	process	known	as	
coalescing.
This	raises	an
important	policy	decision	about	when	to	perform	coalescing.	The
allocator	can	opt	for	
immediate	coalescing
by	merging	any	adjacent
blocks	each	time	a	block	is	freed.	Or	it	can	opt	for	
deferred	coalescing
by
waiting	to	coalesce	free	blocks	at	some	later	time.	For	example,	the
allocator	might	defer	coalescing	until	some	allocation	request	fails,	and
then	scan	the	entire	heap,	coalescing	all	free	blocks.
Figure	
9.38	
An	example	of	false	fragmentation.
Allocated	blocks	are	shaded.	Free	blocks	are	unshaded.	Headers	are
labeled	with	(size	(bytes)/allocated	bit).</p>
<p>Immediate	coalescing	is	straightforward	and	can	be	performed	in
constant	time,	but	with	some	request	patterns	it	can	introduce	a	form	of
thrashing	where	a	block	is	repeatedly	coalesced	and	then	split	soon
thereafter.	For	example,	in	
Figure	
9.38
,	a	repeated	pattern	of
allocating	and	freeing	a	three-word	block	would	introduce	a	lot	of
unnecessary	splitting	and	coalescing.	In	our	discussion	of	allocators,	we
will	assume	immediate	coalescing,	but	you	should	be	aware	that	fast
allocators	often	opt	for	some	form	of	deferred	coalescing.
9.9.11	
Coalescing	with	Boundary
Tags
How	does	an	allocator	implement	coalescing?	Let	us	refer	to	the	block
we	want	to	free	as	the	
current	block.
Then	coalescing	the	next	free	block
(in	memory)	is	straightforward	and	efficient.	The	header	of	the	current
block	points	to	the	header	of	the	next	block,	which	can	be	checked	to
determine	if	the	next	block	is	free.	If	so,	its	size	is	simply	added	to	the
size	of	the	current	header	and	the	blocks	are	coalesced	in	constant	time.
But	how	would	we	coalesce	the	previous	block?	Given	an	implicit	free	list
of	blocks	with	headers,	the	only	option	would	be	to	search	the	entire	list,
remembering	the	location	of	the	previous	block,	until	we	reached	the
current	block.	With	an	implicit	free	list,	this	means	that	each	call	to	free
would	require	time	linear	in	the	size	of	the	heap.	Even	with	more
sophisticated	free	list	organizations,	the	search	time	would	not	be
constant.</p>
<p>Knuth	developed	a	clever	and	general	technique,	known	as	
boundary
tags
,	that	allows	for	constant-time	coalescing	of	the	previous	block.	The
idea,	which	is	shown	in	
Figure	
9.39
,	is	to	add	
&amp;	footer
(the	boundary
tag)	at	the	end	of	each	block,	where	the	footer	is	a	replica	of	the	header.
If	each	block	includes	such	a	footer,	then	the	allocator	can	determine	the
starting	location	and	status	of	the	previous	block	by	inspecting	its	footer,
which	is	always	one	word	away	from	the	start	of	the	current	block.
Consider	all	the	cases	that	can	exist	when	the	allocator	frees	the	current
block:
1
.	
The	previous	and	next	blocks	are	both	allocated.
2
.	
The	previous	block	is	allocated	and	the	next	block	is	free.
3
.	
The	previous	block	is	free	and	the	next	block	is	allocated.
4
.	
The	previous	and	next	blocks	are	both	free.
Figure	
9.39	
Format	of	heap	block	that	uses	a	boundary	tag.
Figure	
9.40
shows	how	we	would	coalesce	each	of	the	four	cases.</p>
<p>In	case	1,	both	adjacent	blocks	are	allocated	and	thus	no	coalescing	is
possible.	So	the	status	of	the	current	block	is	simply	changed	from
allocated	to	free.	In	case	2,	the	current	block	is	merged	with	the	next
block.	The	header	of	the	current	block	and	the	footer	of	the	next	block	are
updated	with	the	combined	sizes	of	the	current	and	next	blocks.	In	case
3,	the	previous	block	is	merged	with	the	current	block.	The	header	of	the
previous	block	and	the	footer	of	the	current	block	are	updated	with	the
combined	sizes	of	the	two	blocks.	In	case	4,	all	three	blocks	are	merged
to	form	a	single	free	block,	with	the	header	of	the	previous	block	and	the
footer	of	the	next	block	updated	with	the	combined	sizes	of	the	three
blocks.	In	each	case,	the	coalescing	is	performed	in	constant	time.
The	idea	of	boundary	tags	is	a	simple	and	elegant	one	that	generalizes	to
many	different	types	of	allocators	and	free	list	organizations.	However,
there	is	a	potential	disadvantage.	Requiring	each	block	to	contain	both	a
header	and	a	footer	can	introduce	significant	memory	overhead	if	an
application	manipulates	many	small	blocks.	For	example,	if	a	graph
application	dynamically	creates	and	destroys	graph	nodes	by	making
repeated	calls	to	
and	
,	and	each	graph	node	requires	only	a
couple	of	words	of	memory,	then	the	header	and	the	footer	will	consume
half	of	each	allocated	block.
Fortunately,	there	is	a	clever	optimization	of	boundary	tags	that
eliminates	the	need	for	a	footer	in	allocated	blocks.	Recall	that	when	we
attempt	to	coalesce	the	current	block	with	the	previous	and	next	blocks	in
memory,	the	size	field	in	the	footer	of	the	previous	block	is	only	needed	if
the	previous	block	is	
free.
If	we	were	to	store	the	allocated/free	bit	of	the
previous	block	in	one	of	the	excess	low-order	bits	of	the	current	block,
then	allocated	blocks	would	not	need	footers,	and	we	could	use	that	extra</p>
<p>space	for	payload.	Note,	however,	that	free	blocks	would	still	need
footers.
Practice	Problem	
9.7	
(solution	page	
883
)
Determine	the	minimum	block	size	for	each	of	the	following
combinations	of	alignment	requirements	and	block	formats.
Assumptions:	Implicit	free	list,	zero-size	payloads	are	not	allowed,
and	headers	and	footers	are	stored	in	4-byte	words.</p>
<p>Figure	
9.40	
Coalescing	with	boundary	tags.
Case	1	:	prev	and	next	allocated.	Case	2:	prev	allocated,	next
free.	Case	3:	prev	free,	next	allocated.	Case	4:	next	and	prev	free.
Alignment
Allocated	block
Free	block
Minimum	block	size
(bytes)
Single
word
Header	and	footer
Header	and
footer</p>
<hr />
<p>Single
word
Header,	but	no
footer
Header	and
footer</p>
<hr />
<p>Double
word
Header	and	footer
Header	and
footer</p>
<hr />
<p>Double
word
Header,	but	no
footer
Header	and
footer</p>
<hr />
<p>9.9.12	
Putting	It	Together:
Implementing	a	Simple	Allocator
Building	an	allocator	is	a	challenging	task.	The	design	space	is	large,
with	numerous	alternatives	for	block	format	and	free	list	format,	as	well
as	placement,	splitting,	and	coalescing	policies.	Another	challenge	is	that
you	are	often	forced	to	program	outside	the	safe,	familiar	confines	of	the
type	system,	relying	on	the	error-prone	pointer	casting	and	pointer
arithmetic	that	is	typical	of	low-level	systems	programming.</p>
<p>While	allocators	do	not	require	enormous	amounts	of	code,	they	are
subtle	and	unforgiving.	Students	familiar	with	higher-level	languages
such	as	C++	or	Java	often	hit	a	conceptual	wall	when	they	first	encounter
this	style	of	programming.	To	help	you	clear	this	hurdle,	we	will	work
through	the	implementation	of	a	simple	allocator	based	on	an	implicit	free
list	with	immediate	boundary-tag	coalescing.	The	maximum	block	size	is
2
=	4	GB.	The	code	is	64-bit	clean,	running	without	modification	in	32-
bit	(
)	or	64-bit	(
)	processes.
General	Allocator	Design
Our	allocator	uses	a	model	of	the	memory	system	provided	by	the
package	shown	in	
Figure	
9.41
.	The	purpose	of	the	model	is
to	allow	us	to	run	our	allocator	without	interfering	with	the	existing
system-level	
package.
The	
function	models	the	virtual	memory	available	to	the	heap
as	a	large	double-word	aligned	array	of	bytes.	The	bytes	between
and	
represent	allocated	virtual	memory.	The	bytes
following	
represent	unallocated	virtual	memory.	The	allocator
requests	additional	heap	memory	by	calling	the	
function,	which
has	the	same	interface	as	the	system's	
function,	as	well	as	the
same	semantics,	except	that	it	rejects	requests	to	shrink	the	heap.
The	allocator	itself	is	contained	in	a	source	file	(
)	that	users	can
compile	and	link	into	their	applications.	The	allocator	exports	three
functions	to	application	programs:
32</p>
<p>The	
function	initializes	the	allocator,	returning	0	if	successful	and
–1	otherwise.	The	
and	
functions	have	the	same
interfaces	and	semantics	as	their	system	counterparts.	The	allocator
uses	the	block	format</p>
<hr />
<hr />
<p>Figure	
9.41	
:	Memory	system	model.
shown	in	
Figure	
9.39
.	The	minimum	block	size	is	16	bytes.	The	free
list	is	organized	as	an	implicit	free	list,	with	the	invariant	form	shown	in
Figure	
9.42
.
The	first	word	is	an	unused	padding	word	aligned	to	a	double-word
boundary.	The	padding	is	followed	by	a	special	
prologue	block
,	which	is
an	8-byte	allocated	block	consisting	of	only	a	header	and	a	footer.	The
prologue	block	is	created	during	initialization	and	is	never	freed.
Following	the	prologue	block	are	zero	or	more	regular	blocks	that	are
created	by	calls	to	
or	
.	The	heap	always	ends	with	a	special
epilogue	block
,	which	is	a	zero-size	allocated	block
Figure	
9.42	
Invariant	form	of	the	implicit	free	list.
that	consists	of	only	a	header.	The	prologue	and	epilogue	blocks	are
tricks	that	eliminate	the	edge	conditions	during	coalescing.	The	allocator
uses	a	single	private	(
)	global	variable	(
)	that	always
points	to	the	prologue	block.	(As	a	minor	optimization,	we	could	make	it
point	to	the	next	block	instead	of	the	prologue	block.)</p>
<p>Basic	Constants	and	Macros	for
Manipulating	the	Free	List
Figure	
9.43
shows	some	basic	constants	and	macros	that	we	will	use
throughout	the	allocator	code.	Lines	2–4	define	some	basic	size
constants:	the	sizes	of	words	(WSIZE)	and	double	words	(DSIZE),	and
the	size	of	the	initial	free	block	and	the	default	size	for	expanding	the
heap	(CHUNKSIZE).
Manipulating	the	headers	and	footers	in	the	free	list	can	be	troublesome
because	it	demands	extensive	use	of	casting	and	pointer	arithmetic.
Thus,	we	find	it	helpful	to	define	a	small	set	of	macros	for	accessing	and
traversing	the	free	list	(lines	9–25).	The	PACK	macro	(line	9)	combines	a
size	and	an	allocate	bit	and	returns	a	value	that	can	be	stored	in	a
header	or	footer.
The	GET	macro	(line	12)	reads	and	returns	the	word	referenced	by
argument	
.	The	casting	here	is	crucial.	The	argument	
is	typically	a
(
)	pointer,	which	cannot	be	dereferenced	directly.	Similarly,	the
PUT	macro	(line	13)	stores	
in	the	word	pointed	at	by	argument	
.
The	GET_SIZE	and	GET_ALLOC	macros	(lines	16–17)	return	the	size
and	allocated	bit,	respectively,	from	a	header	or	footer	at	address	
.	The
remaining	macros	operate	on	
block	pointers
(denoted	
)	that	point	to
the	first	payload	byte.	Given	a	block	pointer	
,	the	HDRP	and	FTRP
macros	(lines	20–21)	return	pointers	to	the	block	header	and	footer,
respectively.	The	NEXT_BLKP	and	PREV_BLKP	macros	(lines	24–25)
return	the	block	pointers	of	the	next	and	previous	blocks,	respectively.</p>
<p>The	macros	can	be	composed	in	various	ways	to	manipulate	the	free	list.
For	example,	given	a	pointer	
to	the	current	block,	we	could	use	the
following	line	of	code	to	determine	the	size	of	the	next	block	in	memory:</p>
<hr />
<hr />
<p>Figure	
9.43	
Basic	constants	and	macros	for	manipulating	the	free
list.
Creating	the	Initial	Free	List
Before	calling	
or	
,	the	application	must	initialize	the
heap	by	calling	the	
function	(
Figure	
9.44
).</p>
<p>The	
function	gets	four	words	from	the	memory	system	and
initializes	them	to	create	the	empty	free	list	(lines	4–10).	It	then	calls	the
function	(
Figure	
9.45
),	which	extends	the	heap	by
CHUNKSIZE	bytes	and	creates	the	initial	free	block.	At	this	point,	the
allocator	is	initialized	and	ready	to	accept	allocate	and	free	requests	from
the	application.
The	
function	is	invoked	in	two	different	circumstances:	(1)
when	the	heap	is	initialized	and	(2)	when	
is	unable	to	find	a
suitable	fit.	To	maintain	alignment,	
rounds	up	the	requested
size	to	the	nearest</p>
<hr />
<p>code/vm/malloc/mm.c</p>
<hr />
<p>Figure	
9.44	
creates	a	heap	with	an	initial	free	block.</p>
<hr />
<hr />
<p>Figure	
9.45	
extends	the	heap	with	a	new	free	block.
multiple	of	2	words	(8	bytes)	and	then	requests	the	additional	heap	space
from	the	memory	system	(lines	7–9).
The	remainder	of	the	
function	(lines	12–17)	is	somewhat
subtle.	The	heap	begins	on	a	double-word	aligned	boundary,	and	every
call	to	
returns	a	block	whose	size	is	an	integral	number	of
double	words.	Thus,	every	call	to	
returns	a	double-word	aligned
chunk	of	memory	immediately	following	the	header	of	the	epilogue	block.
This	header	becomes	the	header	of	the	new	free	block	(line	12),	and	the
last	word	of	the	chunk	becomes	the	new	epilogue	block	header	(line	14).
Finally,	in	the	likely	case	that	the	previous	heap	was	terminated	by	a	free</p>
<p>block,	we	call	the	coalesce	function	to	merge	the	two	free	blocks	and
return	the	block	pointer	of	the	merged	blocks	(line	17).
Freeing	and	Coalescing	Blocks
An	application	frees	a	previously	allocated	block	by	calling	the	
function	(
Figure	
9.46
),	which	frees	the	requested	block	(
)	and	then
merges	adjacent	free	blocks	using	the	boundary-tags	coalescing
technique	described	in	
Section	
9.9.11
.
The	code	in	the	coalesce	helper	function	is	a	straightforward
implementation	of	the	four	cases	outlined	in	
Figure	
9.40
.	There	is	one
somewhat	subtle	aspect.	The	free	list	format	we	have	chosen—with	its
prologue	and	epilogue	blocks	that	are	always	marked	as	allocated—
allows	us	to	ignore	the	potentially	troublesome	edge	conditions	where	the
requested	block	
is	at	the	beginning	or	end	of	the	heap.	Without	these
special	blocks,	the	code	would	be	messier,	more	error	prone,	and	slower
because	we	would	have	to	check	for	these	rare	edge	conditions	on	each
and	every	free	request.
Allocating	Blocks
An	application	requests	a	block	of	size	bytes	of	memory	by	calling	the
function	(
Figure	
9.47
).	After	checking	for	spurious	requests,
the	allocator	must	adjust	the	requested	block	size	to	allow	room	for	the
header	and	the	footer,	and	to	satisfy	the	double-word	alignment
requirement.	Lines	12–13	enforce	the	minimum	block	size	of	16	bytes:	8
bytes	to	satisfy	the	alignment	requirement	and	8	more	bytes	for	the
overhead	of	the	header	and	footer.	For	requests	over	8	bytes	(line	15),</p>
<p>the	general	rule	is	to	add	in	the	overhead	bytes	and	then	round	up	to	the
nearest	multiple	of	8.
Once	the	allocator	has	adjusted	the	requested	size,	it	searches	the	free
list	for	a	suitable	free	block	(line	18).	If	there	is	a	fit,	then	the	allocator
places	the	requested	block	and	optionally	splits	the	excess	(line	19)	and
then	returns	the	address	of	the	newly	allocated	block.
If	the	allocator	cannot	find	a	fit,	it	extends	the	heap	with	a	new	free	block
(lines	24–26),	places	the	requested	block	in	the	new	free	block,	optionally
splitting	the	block	(line	27),	and	then	returns	a	pointer	to	the	newly
allocated	block.</p>
<hr />
<p>code/vm/malloc/mm.c</p>
<hr />
<p>Figure	
9.46	
frees	a	block	and	uses	boundary-tag	coalescing
to	merge	it	with	any	adjacent	free	blocks	in	constant	time.</p>
<hr />
<hr />
<p>Figure	
9.47	
allocates	a	block	from	the	free	list.
Practice	Problem	
9.8	
(solution	page	
884
)
Implement	a	
function	for	the	simple	allocator	described	in
Section	
9.9.12
.</p>
<p>Your	solution	should	perform	a	first-fit	search	of	the	implicit	free
list.
Practice	Problem	
9.9	
(solution	page	
884
)
Implement	a	place	function	for	the	example	allocator.
Your	solution	should	place	the	requested	block	at	the	beginning	of
the	free	block,	splitting	only	if	the	size	of	the	remainder	would
equal	or	exceed	the	minimum	block	size.
9.9.13	
Explicit	Free	Lists
The	implicit	free	list	provides	us	with	a	simple	way	to	introduce	some
basic	allocator	concepts.	However,	because	block	allocation	time	is	linear
in	the	total	number	of	heap	blocks,	the	implicit	free	list	is	not	appropriate
for	a	general-purpose	allocator	(although	it	might	be	fine	for	a	special-
purpose	allocator	where	the	number	of	heap	blocks	is	known	beforehand
to	be	small).
A	better	approach	is	to	organize	the	free	blocks	into	some	form	of	explicit
data	structure.	Since	by	definition	the	body	of	a	free	block	is	not	needed
by	the	program,	the	pointers	that	implement	the	data	structure	can	be
stored	within	the	bodies	of	the	free	blocks.	For	example,	the	heap	can	be</p>
<p>organized	as	a	doubly	linked	free	list	by	including	a	
(predecessor)
and	
(successor)	pointer	in	each	free	block,	as	shown	in	
Figure
9.48
.
Using	a	doubly	linked	list	instead	of	an	implicit	free	list	reduces	the	first-fit
allocation	time	from	linear	in	the	total	number	of	blocks	to	linear	in	the
number	of	
free
blocks.	However,	the	time	to	free	a	block	can	be	either
linear	or	constant,	depending	on	the	policy	we	choose	for	ordering	the
blocks	in	the	free	list.
Figure	
9.48	
Format	of	heap	blocks	that	use	doubly	linked	free	lists.
One	approach	is	to	maintain	the	list	in	
last-in	first-out
(LIFO)	order	by
inserting	newly	freed	blocks	at	the	beginning	of	the	list.	With	a	LIFO
ordering	and	a	first-fit	placement	policy,	the	allocator	inspects	the	most
recently	used	blocks	first.	In	this	case,	freeing	a	block	can	be	performed</p>
<p>in	constant	time.	If	boundary	tags	are	used,	then	coalescing	can	also	be
performed	in	constant	time.
Another	approach	is	to	maintain	the	list	in	
address	order
,	where	the
address	of	each	block	in	the	list	is	less	than	the	address	of	its	successor.
In	this	case,	freeing	a	block	requires	a	linear-time	search	to	locate	the
appropriate	predecessor.	The	trade-off	is	that	address-ordered	first	fit
enjoys	better	memory	utilization	than	LIFO-ordered	first	fit,	approaching
the	utilization	of	best	fit.
A	disadvantage	of	explicit	lists	in	general	is	that	free	blocks	must	be	large
enough	to	contain	all	of	the	necessary	pointers,	as	well	as	the	header
and	possibly	a	footer.	This	results	in	a	larger	minimum	block	size	and
increases	the	potential	for	internal	fragmentation.
9.9.14	
Segregated	Free	Lists
As	we	have	seen,	an	allocator	that	uses	a	single	linked	list	of	free	blocks
requires	time	linear	in	the	number	of	free	blocks	to	allocate	a	block.	A
popular	approach	for	reducing	the	allocation	time,	known	generally	as
segregated	storage
,	is	to	maintain	multiple	free	lists,	where	each	list
holds	blocks	that	are	roughly	the	same	size.	The	general	idea	is	to
partition	the	set	of	all	possible	block	sizes	into	equivalence	classes	called
size	classes.
There	are	many	ways	to	define	the	size	classes.	For
example,	we	might	partition	the	block	sizes	by	powers	of	2:
{</p>
<p>1</p>
<p>}
,
 
{</p>
<p>2</p>
<p>}
,
 
{</p>
<p>3
,
4</p>
<p>}
,
 
{</p>
<p>5
−
8</p>
<p>}
,
⋯
,
 
{</p>
<p>1
,
025
−
2
,
048</p>
<p>}
,
 
{</p>
<p>2
,
049
−
4
,
096</p>
<p>}
,
 
{</p>
<p>4
,
097
−
∞</p>
<p>}</p>
<p>Or	we	might	assign	small	blocks	to	their	own	size	classes	and	partition
large	blocks	by	powers	of	2:
The	allocator	maintains	an	array	of	free	lists,	with	one	free	list	per	size
class,	ordered	by	increasing	size.	When	the	allocator	needs	a	block	of
size	
n
,	it	searches	the	appropriate	free	list.	If	it	cannot	find	a	block	that
fits,	it	searches	the	next	list,	and	so	on.
The	dynamic	storage	allocation	literature	describes	dozens	of	variants	of
segregated	storage	that	differ	in	how	they	define	size	classes,	when	they
perform	coalescing,	when	they	request	additional	heap	memory	from	the
operating	system,	whether	they	allow	splitting,	and	so	forth.	To	give	you	a
sense	of	what	is	possible,	we	will	describe	two	of	the	basic	approaches:
simple	segregated	storage
and	
segregated	fits.
Simple	Segregated	Storage
With	simple	segregated	storage,	the	free	list	for	each	size	class	contains
same-size	blocks,	each	the	size	of	the	largest	element	of	the	size	class.
For	example,	if	some	size	class	is	defined	as	{17–32},	then	the	free	list
for	that	class	consists	entirely	of	blocks	of	size	32.
To	allocate	a	block	of	some	given	size,	we	check	the	appropriate	free	list.
If	the	list	is	not	empty,	we	simply	allocate	the	first	block	in	its	entirety.
Free	blocks	are	never	split	to	satisfy	allocation	requests.	If	the	list	is
empty,	the	allocator	requests	a	fixed-size	chunk	of	additional	memory
{</p>
<p>1</p>
<p>}
,
 
{</p>
<p>2</p>
<p>}
,
 
{</p>
<p>3</p>
<p>}
,
⋯
,
 
{</p>
<p>1
,
023</p>
<p>}
,
 
{</p>
<p>1
,
024</p>
<p>}
 
{</p>
<p>1
,
025
−
2
,
048</p>
<p>}
,
 
{</p>
<p>2
,
049
−
4
,
096</p>
<p>}
,
 
 
{
4
,
097
−
∞</p>
<p>}</p>
<p>from	the	operating	system	(typically	a	multiple	of	the	page	size),	divides
the	chunk	into	equal-size	blocks,	and	links	the	blocks	together	to	form	the
new	free	list.	To	free	a	block,	the	allocator	simply	inserts	the	block	at	the
front	of	the	appropriate	free	list.
There	are	a	number	of	advantages	to	this	simple	scheme.	Allocating	and
freeing	blocks	are	both	fast	constant-time	operations.	Further,	the
combination	of	the	same-size	blocks	in	each	chunk,	no	splitting,	and	no
coalescing	means	that	there	is	very	little	per-block	memory	overhead.
Since	each	chunk	has	only	same-size	blocks,	the	size	of	an	allocated
block	can	be	inferred	from	its	address.	Since	there	is	no	coalescing,
allocated	blocks	do	not	need	an	allocated/free	flag	in	the	header.	Thus,
allocated	blocks	require	no	headers,	and	since	there	is	no	coalescing,
they	do	not	require	any	footers	either.	Since	allocate	and	free	operations
insert	and	delete	blocks	at	the	beginning	of	the	free	list,	the	list	need	only
be	singly	linked	instead	of	doubly	linked.	The	bottom	line	is	that	the	only
required	field	in	any	block	is	a	one-word	
pointer	in	each	free	block,
and	thus	the	minimum	block	size	is	only	one	word.
A	significant	disadvantage	is	that	simple	segregated	storage	is
susceptible	to	internal	and	external	fragmentation.	Internal	fragmentation
is	possible	because	free	blocks	are	never	split.	Worse,	certain	reference
patterns	can	cause	extreme	external	fragmentation	because	free	blocks
are	never	coalesced	(
Practice	Problem	
9.10
).
Practice	Problem	
9.10	
(solution	page	
885
)
Describe	a	reference	pattern	that	results	in	severe	external
fragmentation	in	an	allocator	based	on	simple	segregated	storage.</p>
<p>Segregated	Fits
With	this	approach,	the	allocator	maintains	an	array	of	free	lists.	Each
free	list	is	associated	with	a	size	class	and	is	organized	as	some	kind	of
explicit	or	implicit	list.	Each	list	contains	potentially	different-size	blocks
whose	sizes	are	members	of	the	size	class.	There	are	many	variants	of
segregated	fits	allocators.	Here	we	describe	a	simple	version.
To	allocate	a	block,	we	determine	the	size	class	of	the	request	and	do	a
first-fit	search	of	the	appropriate	free	list	for	a	block	that	fits.	If	we	find
one,	then	we	(optionally)	split	it	and	insert	the	fragment	in	the	appropriate
free	list.	If	we	cannot	find	a	block	that	fits,	then	we	search	the	free	list	for
the	next	larger	size	class.	We	
repeat	until	we	find	a	block	that	fits.	If	none
of	the	free	lists	yields	a	block	that	fits,	then	we	request	additional	heap
memory	from	the	operating	system,	allocate	the	block	out	of	this	new
heap	memory,	and	place	the	remainder	in	the	appropriate	size	class.	To
free	a	block,	we	coalesce	and	place	the	result	on	the	appropriate	free	list.
The	segregated	fits	approach	is	a	popular	choice	with	production-quality
allocators	such	as	the	GNU	
package	provided	in	the	C	standard
library	because	it	is	both	fast	and	memory	efficient.	Search	times	are
reduced	because	searches	are	limited	to	particular	parts	of	the	heap
instead	of	the	entire	heap.	Memory	utilization	can	improve	because	of	the
interesting	fact	that	a	simple	first-fit	search	of	a	segregated	free	list
approximates	a	best-fit	search	of	the	entire	heap.
Buddy	Systems</p>
<p>A	
buddy	system
is	a	special	case	of	segregated	fits	where	each	size
class	is	a	power	of	2.	The	basic	idea	is	that,	given	a	heap	of	2
words,
we	maintain	a	separate	free	list	for	each	block	size	
2
,	where	0	≤	
k
≤	
m
.
Requested	block	sizes	are	rounded	up	to	the	nearest	power	of	2.
Originally,	there	is	one	free	block	of	size	2
words.
To	allocate	a	block	of	size	2
,	we	find	the	first	available	block	of	size	2
,
such	that	
k
≤	
j
≤	
m
.	If	
j
=	
k
,	then	we	are	done.	Otherwise,	we	recursively
split	the	block	in	half	until	
j
=	
k.
As	we	perform	this	splitting,	each
remaining	half	(known	as	a	
buddy
)	is	placed	on	the	appropriate	free	list.
To	free	a	block	of	size	2
,	we	continue	coalescing	with	the	free	buddies.
When	we	encounter	an	allocated	buddy,	we	stop	the	coalescing.
A	key	fact	about	buddy	systems	is	that,	given	the	address	and	size	of	a
block,	it	is	easy	to	compute	the	address	of	its	buddy.	For	example,	a
block	of	size	32	bytes	with	address
has	its	buddy	at	address
In	other	words,	the	addresses	of	a	block	and	its	buddy	differ	in	exactly
one	bit	position.
The	major	advantage	of	a	buddy	system	allocator	is	its	fast	searching
and	coalescing.	The	major	disadvantage	is	that	the	power-of-2
requirement	on	the	block	size	can	cause	significant	internal
m
k
m
k
j
k
x
x
x
 
…
x
00000
x
x
x
 
…
x
10000</p>
<p>fragmentation.	For	this	reason,	buddy	system	allocators	are	not
appropriate	for	general-purpose	workloads.	However,	for	certain
application-specific	workloads,	where	the	block	sizes	are	known	in
advance	to	be	powers	of	2,	buddy	system	allocators	have	a	certain
appeal.</p>
<p>9.10	
Garbage	Collection
With	an	explicit	allocator	such	as	the	C	
package,	an	application
allocates	and	frees	heap	blocks	by	making	calls	to	
and	
.	It	is
the	application's	responsibility	to	free	any	allocated	blocks	that	it	no
longer	needs.
Failing	to	free	allocated	blocks	is	a	common	programming	error.	For
example,	consider	the	following	C	function	that	allocates	a	block	of
temporary	storage	as	part	of	its	processing:
Since	
is	no	longer	needed	by	the	program,	it	should	have	been	freed
before	garbage	returned.	Unfortunately,	the	programmer	has	forgotten	to
free	the	block.	It	remains	allocated	for	the	lifetime	of	the	program,
needlessly	occupying	heap	space	that	could	be	used	to	satisfy
subsequent	allocation	requests.</p>
<p>A	
garbage	collector
is	a	dynamic	storage	allocator	that	automatically
frees	allocated	blocks	that	are	no	longer	needed	by	the	program.	Such
blocks	are	known	as	
garbage
(hence	the	term	&quot;garbage	collector&quot;).	The
process	of	automatically	reclaiming	heap	storage	is	known	as	
garbage
collection.
In	a	system	that	supports	garbage	collection,	applications
explicitly	allocate	heap	blocks	but	never	explicitly	free	them.	In	the
context	of	a	C	program,	the	application	calls	
but	never	calls	
.
Instead,	the	garbage	collector	periodically	identifies	the	garbage	blocks
and	makes	the	appropriate	calls	to	
to	place	those	blocks	back	on
the	free	list.
Garbage	collection	dates	back	to	Lisp	systems	developed	by	John
McCarthy	at	MIT	in	the	early	1960s.	It	is	an	important	part	of	modern
language	systems	such	as	Java,	ML,	Perl,	and	Mathematica,	and	it
remains	an	active	and	important	area	of	research.	The	literature
describes	an	amazing	number	of	approaches	for	garbage	collection.	We
will	limit	our	discussion	to	McCarthy's	original	
Mark&amp;Sweep
algorithm,
which	is	interesting	because	it	can	be	built	on	top	of	an	existing	
package	to	provide	garbage	collection	for	C	and	C++	programs.
9.10.1	
Garbage	Collector	Basics
A	garbage	collector	views	memory	as	a	directed	
reachability	graph
of	the
form	shown	in	
Figure	
9.49
.	The	nodes	of	the	graph	are	partitioned	into
a	set	of	
root	nodes
and	a	set	of	
heap	nodes.
Each	heap	node
corresponds	to	an	allocated	block	in	the	heap.	A	directed	edge	
p
→	
q
means	that	some	location	in	block	
p
points	to	some	location	in	block	
q.
Root	nodes	correspond	to	locations	not	in	the	heap	that	contain	pointers</p>
<p>into	the	heap.	These	locations	can	be	registers,	variables	on	the	stack,	or
global	variables	in	the	read/write	data	area	of	virtual	memory.
We	say	that	a	node	
p
is	
reachable
if	there	exists	a	directed	path	from	any
root	node	to	
p.
At	any	point	in	time,	the	unreachable	nodes	correspond	to
garbage	that	can	never	be	used	again	by	the	application.	The	role	of	a
garbage	collector	is	to	maintain	some	representation	of	the	reachability
graph	and	periodically	reclaim	the	unreachable	nodes	by	freeing	them
and	returning	them	to	the	free	list.
Figure	
9.49	
A	garbage	collector's	view	of	memory	as	a	directed
graph.
Figure	
9.50	
Integrating	a	conservative	garbage	collector	and	a	C
package.
Garbage	collectors	for	languages	like	ML	and	Java,	which	exert	tight
control	over	how	applications	create	and	use	pointers,	can	maintain	an</p>
<p>exact	representation	of	the	reachability	graph	and	thus	can	reclaim	all
garbage.	However,	collectors	for	languages	like	C	and	C++	cannot	in
general	maintain	exact	representations	of	the	reachability	graph.	Such
collectors	are	known	as	
conservative	garbage	collectors.
They	are
conservative	in	the	sense	that	each	reachable	block	is	correctly	identified
as	reachable,	while	some	unreachable	nodes	might	be	incorrectly
identified	as	reachable.
Collectors	can	provide	their	service	on	demand,	or	they	can	run	as
separate	threads	in	parallel	with	the	application,	continuously	updating
the	reachability	graph	and	reclaiming	garbage.	For	example,	consider
how	we	might	incorporate	a	conservative	collector	for	C	programs	into	an
existing	
package,	as	shown	in	
Figure	
9.50
.
The	application	calls	
in	the	usual	manner	whenever	it	needs	heap
space.	If	
is	unable	to	find	a	free	block	that	fits,	then	it	calls	the
garbage	collector	in	hopes	of	reclaiming	some	garbage	to	the	free	list.
The	collector	identifies	the	garbage	blocks	and	returns	them	to	the	heap
by	calling	the	
function.	The	key	idea	is	that	the	collector	calls	free
instead	of	the	application.	When	the	call	to	the	collector	returns,	
tries	again	to	find	a	free	block	that	fits.	If	that	fails,	then	it	can	ask	the
operating	system	for	additional	memory.	Eventually,	
returns	a
pointer	to	the	requested	block	(if	successful)	or	the	NULL	pointer	(if
unsuccessful).
9.10.2	
Mark&amp;Sweep	Garbage</p>
<p>Collectors
A	Mark&amp;Sweep	garbage	collector	consists	of	a	
mark	phase
,	which	marks
all	reachable	and	allocated	descendants	of	the	root	nodes,	followed	by	a
sweep	phase
,	which	frees	each	unmarked	allocated	block.	Typically,	one
of	the	spare	low-order	bits	in	the	block	header	is	used	to	indicate	whether
a	block	is	marked	or	not.
(a)	
function
(b)	
function</p>
<p>Figure	
9.51	
Pseudocode	for	the	
and	
functions.
Our	description	of	Mark&amp;Sweep	will	assume	the	following	functions,
where	
is	defined	as	
:</p>
<p>If	
points	to	some	word	in	an	allocated	block,	it
returns	a	pointer	
to	the	beginning	of	that	block.	Returns	NULL
otherwise.
.	
Returns	true	if	block	
is	already	marked.
.	
Returns	true	if	block	
is	allocated.
.	
Marks	block	
.
.	
Returns	the	length	in	words	(excluding	the
header)	of	block	
.
.	
Changes	the	status	of	block	
from	marked
to	unmarked.</p>
<p>.	
Returns	the	successor	of	block	
b
in	the	heap.
The	mark	phase	calls	the	mark	function	shown	in	
Figure	
9.51(a)
once
for	each	root	node.	The	
function	returns	immediately	if	
does	not
point	to	an	allocated	and	unmarked	heap	block.	Otherwise,	it	marks	the
block	and	calls	itself	recursively	on	each	word	in	block.	Each	call	to	the
function	marks	any	unmarked	and	reachable	descendants	of	some
root	node.	At	the	end	of	the	mark	phase,	any	allocated	block	that	is	not
marked	is	guaranteed	to	be	unreachable	and,	hence,	garbage	that	can
be	reclaimed	in	the	sweep	phase.
The	sweep	phase	is	a	single	call	to	the	sweep	function	shown	in	
Figure
9.51(b)
.	The	
function	iterates	over	each	block	in	the	heap,
freeing	any	unmarked	allocated	blocks	(i.e.,	garbage)	that	it	encounters.
Figure	
9.52
shows	a	graphical	interpretation	of	Mark&amp;Sweep	for	a
small	heap.	Block	boundaries	are	indicated	by	heavy	lines.	Each	square
corresponds	to	a	word	of	memory.	Each	block	has	a	one-word	header,
which	is	either	marked	or	unmarked.</p>
<p>Figure	
9.52	
Mark&amp;Sweep	example.
Note	that	the	arrows	in	this	example	denote	memory	references,	not	free
list	pointers.
Figure	
9.53	
Left	and	right	pointers	in	a	balanced	tree	of	allocated
blocks.
Initially,	the	heap	in	
Figure	
9.52
consists	of	six	allocated	blocks,	each
of	which	is	unmarked.	Block	3	contains	a	pointer	to	block	1.	Block	4
contains	pointers	to	blocks	3	and	6.	The	root	points	to	block	4.	After	the
mark	phase,	blocks	1,3,4,	and	6	are	marked	because	they	are	reachable
from	the	root.	Blocks	2	and	5	are	unmarked	because	they	are
unreachable.	After	the	sweep	phase,	the	two	unreachable	blocks	are
reclaimed	to	the	free	list.</p>
<p>9.10.3	
Conservative	Mark&amp;Sweep
for	C	Programs
Mark&amp;Sweep	is	an	appropriate	approach	for	garbage	collecting	C
programs	because	it	works	in	place	without	moving	any	blocks.	However,
the	C	language	poses	some	interesting	challenges	for	the
implementation	of	the	
function.
First,	C	does	not	tag	memory	locations	with	any	type	information.	Thus,
there	is	no	obvious	way	for	
to	determine	if	its	input	parameter	
is
a	pointer	or	not.	Second,	even	if	we	were	to	know	that	
was	a	pointer,
there	would	be	no	obvious	way	for	
to	determine	whether	
points
to	some	location	in	the	payload	of	an	allocated	block.
One	solution	to	the	latter	problem	is	to	maintain	the	set	of	allocated
blocks	as	a	balanced	binary	tree	that	maintains	the	invariant	that	all
blocks	in	the	left	subtree	are	located	at	smaller	addresses	and	all	blocks
in	the	right	subtree	are	located	in	larger	addresses.	As	shown	in	
Figure
9.53
,	this	requires	two	additional	fields	(
and	
)	in	the	header
of	each	allocated	block.	Each	field	points	to	the	header	of	some	allocated
block.	The	
function	uses	the	tree	to	perform	a	binary
search	of	the	allocated	blocks.	At	each	step,	it	relies	on	the	size	field	in
the	block	header	to	determine	if	
falls	within	the	extent	of	the	block.
The	balanced	tree	approach	is	correct	in	the	sense	that	it	is	guaranteed
to	mark	all	of	the	nodes	that	are	reachable	from	the	roots.	This	is	a
necessary	guarantee,	as	application	users	would	certainly	not	appreciate</p>
<p>having	their	allocated	blocks	prematurely	returned	to	the	free	list.
However,	it	is	conservative	in	the	sense	that	it	may	incorrectly	mark
blocks	that	are	actually	unreachable,	and	thus	it	may	fail	to	free	some
garbage.	While	this	does	not	affect	the	correctness	of	application
programs,	it	can	result	in	unnecessary	external	fragmentation.
The	fundamental	reason	that	Mark&amp;Sweep	collectors	for	C	programs
must	be	conservative	is	that	the	C	language	does	not	tag	memory
locations	with	type	information.	Thus,	scalars	like	
or	
can
masquerade	as	pointers.	For	example,	suppose	that	some	reachable
allocated	block	contains	an	
in	its	payload	whose	value	happens	to
correspond	to	an	address	in	the	payload	of	some	other	allocated	block	
b.
There	is	no	way	for	the	collector	to	infer	that	the	data	is	really	an	
and
not	a	pointer.	Therefore,	the	allocator	must	conservatively	mark	block	
b
as	reachable,	when	in	fact	it	might	not	be.</p>
<p>9.11	
Common	Memory-Related
Bugs	in	C	Programs
Managing	and	using	virtual	memory	can	be	a	difficult	and	error-prone
task	for	C	programmers.	Memory-related	bugs	are	among	the	most
frightening	because	they	often	manifest	themselves	at	a	distance,	in	both
time	and	space,	from	the	source	of	the	bug.	Write	the	wrong	data	to	the
wrong	location,	and	your	program	can	run	for	hours	before	it	finally	fails
in	some	distant	part	of	the	program.	We	conclude	our	discussion	of
virtual	memory	with	a	look	at	of	some	of	the	common	memory-related
bugs.
9.11.1	
Dereferencing	Bad	Pointers
As	we	learned	in	
Section	
9.7.2
,	there	are	large	holes	in	the	virtual
address	space	of	a	process	that	are	not	mapped	to	any	meaningful	data.
If	we	attempt	to	dereference	a	pointer	into	one	of	these	holes,	the
operating	system	will	terminate	our	program	with	a	segmentation
exception.	Also,	some	areas	of	virtual	memory	are	read-only.	Attempting
to	write	to	one	of	these	areas	terminates	the	program	with	a	protection
exception.
A	common	example	of	dereferencing	a	bad	pointer	is	the	classic	
bug.	Suppose	we	want	to	use	
to	read	an	integer	from	
into	a</p>
<p>variable.	The	correct	way	to	do	this	is	to	pass	
a	format	string	and
the	
address
of	the	variable:
However,	it	is	easy	for	new	C	programmers	(and	experienced	ones	too!)
to	pass	the	
contents
of	
instead	of	its	address:
In	this	case,	
will	interpret	the	contents	of	
as	an	address	and
attempt	to	write	a	word	to	that	location.	In	the	best	case,	the	program
terminates	immediately	with	an	exception.	In	the	worst	case,	the	contents
of	
correspond	to	some	valid	read/write	area	of	virtual	memory,	and
we	overwrite	memory,	usually	with	disastrous	and	baffling	consequences
much	later.
9.11.2	
Reading	Uninitialized
Memory
While	bss	memory	locations	(such	as	uninitialized	global	C	variables)	are
always	initialized	to	zeros	by	the	loader,	this	is	not	true	for	heap	memory.</p>
<p>A	common	error	is	to	assume	that	heap	memory	is	initialized	to	zero:
In	this	example,	the	programmer	has	incorrectly	assumed	that	vector	
has	been	initialized	to	zero.	A	correct	implementation	would	explicitly
zero	
or	use	
.
9.11.3	
Allowing	Stack	Buffer
Overflows
As	we	saw	in	
Section	
3.10.3
,	a	program	has	a	
buffer	overflow	bug
if	it
writes	to	a	target	buffer	on	the	stack	without	examining	the	size	of	the</p>
<p>input	string.	For	example,	the	following	function	has	a	buffer	overflow	bug
because	the	
function	copies	an	arbitrary-length	string	to	the	buffer.
To	fix	this,	we	would	need	to	use	the	
function,	which	limits	the	size
of	the	input	string.
9.11.4	
Assuming	That	Pointers	and
the	Objects	They	Point	to	Are	the
Same	Size
One	common	mistake	is	to	assume	that	pointers	to	objects	are	the	same
size	as	the	objects	they	point	to:</p>
<p>The	intent	here	is	to	create	an	array	of	
n
pointers,	each	of	which	points	to
an	array	of	
m</p>
<p>.	However,	because	the	programmer	has	written
instead	of	
in	line	5,	the	code	actually
creates	an	array	of	
.
This	code	will	run	fine	on	machines	where	
and	pointers	to	
are
the	same	size.	But	if	we	run	this	code	on	a	machine	like	the	Core	i7,
where	a	pointer	is	larger	than	an	
,	then	the	loop	in	lines	7–8	will	write
past	the	end	of	the	A	array.	Since	one	of	these	words	will	likely	be	the
boundary-tag	footer	of	the	allocated	block,	we	may	not	discover	the	error
until	we	free	the	block	much	later	in	the	program,	at	which	point	the
coalescing	code	in	the	allocator	will	fail	dramatically	and	for	no	apparent
reason.	This	is	an	insidious	example	of	the	kind	of	&quot;action	at	a	distance&quot;
that	is	so	typical	of	memory-related	programming	bugs.
9.11.5	
Making	Off-by-One	Errors</p>
<p>Off-by-one	errors	are	another	common	source	of	overwriting	bugs:
This	is	another	version	of	the	program	in	the	previous	section.	Here	we
have	created	an	
n
-element	array	of	pointers	in	line	5	but	then	tried	to
initialize	
n
+	1	of	its	elements	in	lines	7	and	8,	in	the	process	overwriting
some	memory	that	follows	the	A	array.
9.11.6	
Referencing	a	Pointer	Instead
of	the	Object	It	Points	To
If	we	are	not	careful	about	the	precedence	and	associativity	of	C
operators,	then	we	incorrectly	manipulate	a	pointer	instead	of	the	object	it
points	to.	For	example,	consider	the	following	function,	whose	purpose	is</p>
<p>to	remove	the	first	item	in	a	binary	heap	of	
items	and	then
reheapify	the	remaining	
-	1	items:
In	line	6,	the	intent	is	to	decrement	the	integer	value	pointed	to	by	the
size	pointer.	However,	because	the	unary	--	and	*	operators	have	the
same	precedence	and	associate	from	right	to	left,	the	code	in	line	6
actually	decrements	the	pointer	itself	instead	of	the	integer	value	that	it
points	to.	If	we	are	lucky,	the	program	will	crash	immediately.	But	more
likely	we	will	be	left	scratching	our	heads	when	the	program	produces	an
incorrect	answer	much	later	in	its	execution.	The	moral	here	is	to	use
parentheses	whenever	in	doubt	about	precedence	and	associativity.	For
example,	in	line	6,	we	should	have	clearly	stated	our	intent	by	using	the
expression	(
)--.
9.11.7	
Misunderstanding	Pointer</p>
<p>Arithmetic
Another	common	mistake	is	to	forget	that	arithmetic	operations	on
pointers	are	performed	in	units	that	are	the	size	of	the	objects	they	point
to,	which	are	not	necessarily	bytes.	For	example,	the	intent	of	the
following	function	is	to	scan	an	array	of	
and	return	a	pointer	to	the
first	occurrence	of	
:
However,	because	line	4	increments	the	pointer	by	4	(the	number	of
bytes	in	an	integer)	each	time	through	the	loop,	the	function	incorrectly
scans	every	fourth	integer	in	the	array.
9.11.8	
Referencing	Nonexistent
Variables</p>
<p>Naive	C	programmers	who	do	not	understand	the	stack	discipline	will
sometimes	reference	local	variables	that	are	no	longer	valid,	as	in	the
following	example:
This	function	returns	a	pointer	(say,	p)	to	a	local	variable	on	the	stack	and
then	pops	its	stack	frame.	Although	
still	points	to	a	valid	memory
address,	it	no	longer	points	to	a	valid	variable.	When	other	functions	are
called	later	in	the	program,	the	memory	will	be	reused	for	their	stack
frames.	Later,	if	the	program	assigns	some	value	to	
,	then	it	might
actually	be	modifying	an	entry	in	another	function's	stack	frame,	with
potentially	disastrous	and	baffling	consequences.
9.11.9	
Referencing	Data	in	Free
Heap	Blocks
A	similar	error	is	to	reference	data	in	heap	blocks	that	have	already	been
freed.	Consider	the	following	example,	which	allocates	an	integer	array	</p>
<p>in	line	6,	prematurely	frees	block	
in	line	10,	and	then	later	references	it
in	line	14:
⋮
Depending	on	the	pattern	of	
and	
e	calls	that	occur	between
lines	6	and	10,	when	the	program	references	
in	line	14,	the	array	
might	be	part	of	some	other	allocated	heap	block	and	may	have	been
overwritten.	As	with	many	
memory-related	bugs,	the	error	will	only
become	evident	later	in	the	program	when	we	notice	that	the	values	in	
are	corrupted.</p>
<p>9.11.10	
Introducing	Memory	Leaks
Memory	leaks	are	slow,	silent	killers	that	occur	when	programmers
inadvertently	create	garbage	in	the	heap	by	forgetting	to	free	allocated
blocks.	For	example,	the	following	function	allocates	a	heap	block	
and
then	returns	without	freeing	it:
If	
is	called	frequently,	then	the	heap	will	gradually	fill	up	with
garbage,	in	the	worst	case	consuming	the	entire	virtual	address	space.
Memory	leaks	are	particularly	serious	for	programs	such	as	daemons
and	servers,	which	by	definition	never	terminate.</p>
<p>9.12	
Summary
Virtual	memory	is	an	abstraction	of	main	memory.	Processors	that
support	virtual	memory	reference	main	memory	using	a	form	of
indirection	known	as	virtual	addressing.	The	processor	generates	a
virtual	address,	which	is	translated	into	a	physical	address	before	being
sent	to	the	main	memory.	The	translation	of	addresses	from	a	virtual
address	space	to	a	physical	address	space	requires	close	cooperation
between	hardware	and	software.	Dedicated	hardware	translates	virtual
addresses	using	page	tables	whose	contents	are	supplied	by	the
operating	system.
Virtual	memory	provides	three	important	capabilities.	First,	it
automatically	caches	recently	used	contents	of	the	virtual	address	space
stored	on	disk	in	main	memory.	The	block	in	a	virtual	memory	cache	is
known	as	a	page.	A	reference	to	a	page	on	disk	triggers	a	page	fault	that
transfers	control	to	a	fault	handler	in	the	operating	system.	The	fault
handler	copies	the	page	from	disk	to	the	main	memory	cache,	writing
back	the	evicted	page	if	necessary.	Second,	virtual	memory	simplifies
memory	management,	which	in	turn	simplifies	linking,	sharing	data
between	processes,	the	allocation	of	memory	for	processes,	and
program	loading.	Finally,	virtual	memory	simplifies	memory	protection	by
incorporating	protection	bits	into	every	page	table	entry.
The	process	of	address	translation	must	be	integrated	with	the	operation
of	any	hardware	caches	in	the	system.	Most	page	table	entries	are
located	in	the	L1	cache,	but	the	cost	of	accessing	page	table	entries	from</p>
<p>L1	is	usually	eliminated	by	an	on-chip	cache	of	page	table	entries	called
a	TLB.
Modern	systems	initialize	chunks	of	virtual	memory	by	associating	them
with	chunks	of	files	on	disk,	a	process	known	as	memory	mapping.
Memory	mapping	provides	an	efficient	mechanism	for	sharing	data,
creating	new	processes,	and	loading	programs.	Applications	can
manually	create	and	delete	areas	of	the	virtual	address	space	using	the
function.	However,	most	programs	rely	on	a	dynamic	memory
allocator	such	as	
,	which	manages	memory	in	an	area	of	the
virtual	address	space	called	the	heap.	Dynamic	memory	allocators	are
application-level	programs	with	a	system-level	feel,	directly	manipulating
memory	without	much	help	from	the	type	system.	Allocators	come	in	two
flavors.	Explicit	allocators	require	applications	to	explicitly	free	their
memory	blocks.	Implicit	allocators	(garbage	collectors)	free	any	unused
and	unreachable	blocks	automatically.
Managing	and	using	memory	is	a	difficult	and	error-prone	task	for	C
programmers.	Examples	of	common	errors	include	dereferencing	bad
pointers,	reading	uninitialized	memory,	allowing	stack	buffer	overflows,
assuming	that	pointers	and	the	objects	they	point	to	are	the	same	size,
referencing	a	pointer	instead	of	the	object	it	points	to,	misunderstanding
pointer	arithmetic,	referencing	nonexistent	variables,	and	introducing
memory	leaks.</p>
<p>Bibliographic	Notes
Kilburn	and	his	colleagues	published	the	first	description	of	virtual
memory	[
63
].	Architecture	texts	contain	additional	details	about	the
hardware's	role	in	virtual	memory	[
46
].	Operating	systems	texts	contain
additional	information	about	the	operating	system's	role	[
102
,
106
,
113
].
Bovet	and	Cesati	[
11
]	give	a	detailed	description	of	the	Linux	virtual
memory	system.	Intel	Corporation	provides	detailed	documentation	on
32-bit	and	64-bit	address	translation	on	IA	processors	[
52
].
Knuth	wrote	the	classic	work	on	storage	allocation	in	1968	[
64
].	Since
that	time,	there	has	been	a	tremendous	amount	of	work	in	the	area.
Wilson,	Johnstone,	Neely,	and	Boles	have	written	a	beautiful	survey	and
performance	evaluation	of	explicit	allocators	[
118
].	The	general
comments	in	this	book	about	the	throughput	and	utilization	of	different
allocator	strategies	are	paraphrased	from	their	survey.	Jones	and	Lins
provide	a	comprehensive	survey	of	garbage	collection	[
56
].	Kernighan
and	Ritchie	[
61
]	show	the	complete	code	for	a	simple	allocator	based	on
an	explicit	free	list	with	a	block	size	and	successor	pointer	in	each	free
block.	The	code	is	interesting	in	that	it	uses	unions	to	eliminate	a	lot	of
the	complicated	pointer	arithmetic,	but	at	the	expense	of	a	linear-time
(rather	than	constant-time)	free	operation.	Doug	Lea	developed	a	widely
used	open-source	malloc	package	called	
[
67
].</p>
<p>Homework	Problems
9.11
In	the	following	series	of	problems,	you	are	to	show	how	the
example	memory	system	in	
Section	
9.6.4
translates	a	virtual
address	into	a	physical	address	and	accesses	the	cache.	For	the
given	virtual	address,	indicate	the	TLB	entry	accessed,	
the
physical	address,	and	the	cache	byte	value	returned.	Indicate
whether	the	TLB	misses,	whether	a	page	fault	occurs,	and
whether	a	cache	miss	occurs.	If	there	is	a	cache	miss,	enter	&quot;—&quot;
for	&quot;Cache	byte	returned.&quot;	If	there	is	a	page	fault,	enter	&quot;—&quot;	for
&quot;PPN&quot;	and	leave	parts	C	and	D	blank.
Virtual	address:	
A
.	
Virtual	address	format
B
.	
Address	translation
Parameter
Value
VPN</p>
<hr />
<p>TLB	index</p>
<hr />
<p>TLB	tag</p>
<hr />
<p>TLB	hit?	(Y/N)</p>
<hr />
<p>Page	fault?	(Y/N)</p>
<hr />
<p>PPN</p>
<hr />
<p>C
.	
Physical	address	format
D
.	
Physical	memory	reference
Parameter
Value
Byte	offset</p>
<hr />
<p>Cache	index</p>
<hr />
<p>Cache	tag</p>
<hr />
<p>Cache	hit?	(Y/N)</p>
<hr />
<p>Cache	byte	returned</p>
<hr />
<p>9.12
Repeat	
Problem	
9.11
for	the	following	address.
Virtual	address:	
A
.	
Virtual	address	format</p>
<p>B
.	
Address	translation
Parameter
Value
VPN</p>
<hr />
<p>TLB	index</p>
<hr />
<p>TLB	tag</p>
<hr />
<p>TLB	hit?	(Y/N)</p>
<hr />
<p>Page	fault?	(Y/N)</p>
<hr />
<p>PPN</p>
<hr />
<p>C
.	
Physical	address	format
D
.	
Physical	memory	reference
Parameter
Value
Byte	offset</p>
<hr />
<p>Cache	index</p>
<hr />
<p>Cache	tag</p>
<hr />
<p>Cache	hit?	(Y/N)</p>
<hr />
<p>Cache	byte	returned</p>
<hr />
<p>9.13
Repeat	
Problem	
9.11
for	the	following	address.
Virtual	address:	
A
.	
Address	translation
Parameter
Value
VPN</p>
<hr />
<p>TLB	index</p>
<hr />
<p>TLB	tag</p>
<hr />
<p>TLB	hit?	(Y/N)</p>
<hr />
<p>Page	fault?	(Y/N)</p>
<hr />
<p>PPN</p>
<hr />
<p>B
.	
Physical	address	format
C
.	
Physical	memory	reference
Parameter
Value
Byte	offset</p>
<hr />
<p>Cache	index</p>
<hr />
<p>Cache	tag</p>
<hr />
<p>Cache	hit?	(Y/N)</p>
<hr />
<p>Cache	byte	returned</p>
<hr />
<p>9.14
Given	an	input	file	
that	consists	of	the	string	
,	write	a	C	program	that	uses	
to	change	the	contents
of	
.
9.15
Determine	the	block	sizes	and	header	values	that	would	result
from	the	following	sequence	of	
requests.	Assumptions:	(1)
The	allocator	maintains	double-word	alignment	and	uses	an
implicit	free	list	with	the	block	format	from	
Figure	
9.35
.	(2)	Block
sizes	are	rounded	up	to	the	nearest	multiple	of	8	bytes.
Request
Block	size	(decimal	bytes)
Block	header	(hex)</p>
<hr />
<hr />
<hr />
<hr />
<hr />
<hr />
<hr />
<hr />
<p>9.16
Determine	the	minimum	block	size	for	each	of	the	following
combinations	of	alignment	requirements	and	block	formats.
Assumptions:	Explicit	free	list,	4-byte	
and	
pointers	in
each	free	block,	zero-size	payloads	are	not	allowed,	and	headers
and	footers	are	stored	in	4-byte	words.
Alignment
Allocated	block
Free	block
Minimum	block	size
(bytes)
Single
word
Header	and	footer
Header	and
footer</p>
<hr />
<p>Single
word
Header,	but	no
footer
Header	and
footer</p>
<hr />
<p>Double
word
Header	and	footer
Header	and
footer</p>
<hr />
<p>Double
word
Header,	but	no
footer
Header	and
footer</p>
<hr />
<p>9.17</p>
<p>Develop	a	version	of	the	allocator	in	
Section	
9.9.12
that
performs	a	next-fit	search	instead	of	a	first-fit	search.
9.18
The	allocator	in	
Section	
9.9.12
requires	both	a	header	and	a
footer	for	each	block	in	order	to	perform	constant-time	coalescing.
Modify	the	allocator	so	that	free	blocks	require	a	header	and	a
footer,	but	allocated	blocks	require	only	a	header.
9.19
You	are	given	three	groups	of	statements	relating	to	memory
management	and	garbage	collection	below.	In	each	group,	only
one	statement	is	true.	Your	task	is	to	indicate	which	statement	is
true.
1
.	
a
.	
In	a	buddy	system,	up	to	50%	of	the	space	can	be
wasted	due	to	internal	fragmentation.
b
.	
The	first-fit	memory	allocation	algorithm	is	slower
than	the	best-fit	algorithm	(on	average).
c
.	
Deallocation	using	boundary	tags	is	fast	only	when
the	list	of	free	blocks	is	ordered	according	to
increasing	memory	addresses.
d
.	
The	buddy	system	suffers	from	internal
fragmentation,	but	not	from	external	fragmentation.</p>
<p>2
.	
a
.	
Using	the	first-fit	algorithm	on	a	free	list	that	is
ordered	according	to	decreasing	block	sizes	results
in	low	performance	for	allocations,	but	avoids
external	fragmentation.
b
.	
For	the	best-fit	method,	the	list	of	free	blocks	should
be	ordered	according	to	increasing	memory
addresses.
c
.	
The	best-fit	method	chooses	the	largest	free	block
into	which	the	requested	segment	fits.
d
.	
Using	the	first-fit	algorithm	on	a	free	list	that	is
ordered	according	to	increasing	block	sizes	is
equivalent	to	using	the	best-fit	algorithm.
3
.	
Mark&amp;Sweep	garbage	collectors	are	called	conservative	if
a
.	
They	coalesce	freed	memory	only	when	a	memory
request	cannot	be	satisfied.
b
.	
They	treat	everything	that	looks	like	a	pointer	as	a
pointer.
c
.	
They	perform	garbage	collection	only	when	they	run
out	of	memory.
d
.	
They	do	not	free	memory	blocks	forming	a	cyclic	list.
9.20
Write	your	own	version	of	
and	
,	and	compare	its
running	time	and	space	utilization	to	the	version	of	
provided	in	the	standard	C	library.</p>
<p>Solutions	to	Practice	Problems
Solution	to	Problem	
9.1	
(page
805
)
This	problem	gives	you	some	appreciation	for	the	sizes	of	different
address	spaces.	At	one	point	in	time,	a	32-bit	address	space	seemed
impossibly	large.	But	now	there	are	database	and	scientific
applications	that	need	more,	and	you	can	expect	this	trend	to
continue.	At	some	point	in	your	lifetime,	expect	to	find	yourself
complaining	about	the	cramped	64-bit	address	space	on	your
personal	computer!
Number	of	address
bits	
(n)
Number	of	virtual
addresses	
(N)
Largest	possible	virtual
address
8
2
=	256
2
-	1	=	255
16
2
=	64	K
2
–	1	=	64	K	–	1
32
2
=	4	G
2
–	1	=	4	G	–	1
48
2
=	256	T
2
–	1	=	256	T	–	1
64
2
=	16,384	P
2
–	1	=	16,384P	–	1
s
8
16
16
32
32
48
48
64
64</p>
<p>Solution	to	Problem	
9.2	
(page
807
)
Since	each	virtual	page	is	
P
=	2
bytes,	there	are	a	total	of	2
/2
=	2
possible	pages	in	the	system,	each	of	which	needs	a	page	table
entry	(PTE).
n
P
=	2
Number	of	PTEs
16
4	K
16
16
8	K
8
32
4	K
1	M
32
8	K
512	K
Solution	to	Problem	
9.3	
(page
816
)
You	need	to	understand	this	kind	of	problem	well	in	order	to	fully
grasp	address	translation.	Here	is	how	to	solve	the	first	subproblem:
We	are	given	
n
=	32	virtual	address	bits	and	
m
=	24	physical	address
bits.	A	page	size	of	
P
=	1	KB	means	we	need	log
(1	K)	=	10	bits	for
both	the	VPO	and	PPO.	(Recall	that	the	VPO	and	PPO	are	identical.)
The	remaining	address	bits	are	the	VPN	and	PPN,	respectively.
P
n
p
n–
p
p
2</p>
<p>Number	of
p
VPN	bits
VPO	bits
PPN	bits
PPO	bits
1	KB
22
10
14
10
2	KB
21
11
13
11
4	KB
20
12
12
12
8	KB
19
13
11
13
Solution	to	Problem	
9.4	
(page
824
)
Doing	a	few	of	these	manual	simulations	is	a	great	way	to	firm	up
your	understanding	of	address	translation.	You	might	find	it	helpful	to
write	out	all	the	bits	in	the	addresses	and	then	draw	boxes	around	the
different	bit	fields,	such	as	VPN,	TLBI,	and	so	on.	In	this	particular
problem,	there	are	no	misses	of	any	kind:	the	TLB	has	a	copy	of	the
PTE	and	the	cache	has	a	copy	of	the	requested	data	words.	See
Problems	
9.11
,	
9.12
,	and	
9.13
for	some	different
combinations	of	hits	and	misses.
A
.	
B
.	
Parameter
Value</p>
<p>VPN
TLB	index
TLB	tag
TLB	hit?	(Y/N)
Y
Page	fault?	(Y/N)
N
PPN
C
.	
D
.	
Parameter
Value
Byte	offset
Cache	index
Cache	tag
Cache	hit?	(Y/N)
Y
Cache	byte	returned
Solution	to	Problem	
9.5	
(page
839
)</p>
<p>Solving	this	problem	will	give	you	a	good	feel	for	the	idea	of	memory
mapping.	Try	it	yourself.	We	haven't	discussed	the	
,	or
functions,	so	you'll	need	to	read	their	man	pages	to	see	how
they	work.</p>
<hr />
<hr />
<p>Solution	to	Problem	
9.6	
(page
849
)
This	problem	touches	on	some	core	ideas	such	as	alignment
requirements,	minimum	block	sizes,	and	header	encodings.	The
general	approach	for	determining	the	block	size	is	to	round	the	sum	of
the	requested	payload	and	the	header	size	to	the	nearest	multiple	of
the	alignment	requirement	(in	this	case,	8	bytes).	For	example,	the
block	size	for	the	
request	is	4	+	1	=	5	rounded	up	to	8.	The</p>
<p>block	size	for	the	
request	is	13	+	4	=	17	rounded	up	to
24.
Request
Block	size	(decimal	bytes)
Block	header	(hex)
8
16
16
24
Solution	to	Problem	
9.7	
(page
852
)
The	minimum	block	size	can	have	a	significant	effect	on	internal
fragmentation.	Thus,	it	is	good	to	understand	the	minimum	block	sizes
associated	with	different	allocator	designs	and	alignment
requirements.	The	tricky	part	is	to	realize	that	the	same	block	can	be
allocated	or	free	at	different	points	in	time.	Thus,	the	minimum	block
size	is	the	maximum	of	the	minimum	allocated	block	size	and	the
minimum	free	block	size.	For	example,	in	the	last	subproblem,	the
minimum	allocated	block	size	is	a	4-byte	header	and	a	1-byte	payload
rounded	up	to	8	bytes.	The	minimum	free	block	size	is	a	4-byte
header	and	4-byte	footer,	which	is	already	a	multiple	of	8	and	doesn't
need	to	be	rounded.	So	the	minimum	block	size	for	this	allocator	is	8
bytes.</p>
<p>Alignment
Allocated	block
Free	block
Minimum	block	size
(bytes)
Single	word
Header	and	footer
Header	and
footer
12
Single	word
Header,	but	no
footer
Header	and
footer
8
Double
word
Header	and	footer
Header	and
footer
16
Double
word
Header,	but	no
footer
Header	and
footer
8
Solution	to	Problem	
9.8	
(page
861
)
There	is	nothing	very	tricky	here.	But	the	solution	requires	you	to
understand	how	the	rest	of	our	simple	implicit-list	allocator	works	and
how	to	manipulate	and	traverse	blocks.</p>
<hr />
<hr />
<p>Solution	to	Problem	
9.9	
(page
861
)
This	is	another	warm-up	exercise	to	help	you	become	familiar	with
allocators.	Notice	that	for	this	allocator	the	minimum	block	size	is	16
bytes.	If	the	remainder	of	the	block	after	splitting	would	be	greater
than	or	equal	to	the	minimum	block	size,	then	we	go	ahead	and	split
the	block	(lines	6–10).	The	only	tricky	part	here	is	to	realize	that	you
need	to	place	the	new	allocated	block	(lines	6	and	7)	before	moving	to
the	next	block	(line	8).</p>
<hr />
<hr />
<p>Solution	to	Problem	
9.10	
(page
864
)
Here	is	one	pattern	that	will	cause	external	fragmentation:	The
application	makes	numerous	allocation	and	free	requests	to	the	first
size	class,	followed	by	numerous	allocation	and	free	requests	to	the</p>
<p>second	size	class,	followed	by	numerous	allocation	and	free	requests
to	the	third	size	class,	and	so	on.	For	each	size	class,	the	allocator
creates	a	lot	of	memory	that	is	never	reclaimed	because	the	allocator
doesn't	coalesce,	and	because	the	application	never	requests	blocks
from	that	size	class	again.</p>
<p>Part	
III	
Interaction	and
Communication	between	Programs
To	this	point	in	our	study	of	computer	systems,	we	have	assumed	that
programs	run	in	isolation,	with	minimal	input	and	output.	However,	in	the
real	world,	application	programs	use	services	provided	by	the	operating
system	to	communicate	with	I/O	devices	and	with	other	programs.
This	part	of	the	book	will	give	you	an	understanding	of	the	basic	I/O
services	provided	by	Unix	operating	systems	and	how	to	use	these
services	to	build	applications	such	as	Web	clients	and	servers	that
communicate	with	each	other	over	the	Internet.	You	will	learn	techniques
for	writing	concurrent	programs,	such	as	Web	servers	that	can	service
multiple	clients	at	the	same	time.	Writing	concurrent	application	programs
can	also	allow	them	to	execute	faster	on	modern	multi-core	processors.
When	you	finish	this	part,	you	will	be	well	on	your	way	to	becoming	a
power	programmer	with	a	mature	understanding	of	computer	systems
and	their	impact]	on	your	programs.</p>
<p>Chapter	
10	
System-Level	I/O
10.1	
Unix	I/O	
890
10.2	
Files	
891
10.3	
Opening	and	Closing	Files	
893
10.4	
Reading	and	Writing	Files	
895
10.5	
Robust	Reading	and	Writing	with	the	Rio	Package	
897
10.6	
Reading	File	Metadata	
903
10.7	
Reading	Directory	Contents	
905
10.8	
Sharing	Files	
906
10.9	
I/O	Redirection	
909
10.10	
Standard	I/O</p>
<p>911
10.11	
Putting	It	Together:	Which	I/O	Functions	Should	I	Use?	
911
10.12	
Summary</p>
<p>913
Bibliographic	Notes	
914
Homework	Problems	
914</p>
<p>Solutions	to	Practice	Problems	
915
I
nput/output	(I/O)
is	the	process	of	copying	data
between	main	memory	and	external	devices	such
as	disk	drives,	terminals,	and	networks.	An	input
operation	copies	data	from	an	I/O	device	to	main
memory,	and	an	output	operation	copies	data	from
memory	to	a	device.
All	language	run-time	systems	provide	higher-level
facilities	for	performing	I/O.	For	example,	ANSIC
provides	the	
standard	I/O
library,	with	functions	such
as	
and	
that	perform	buffered	I/O.	The
C++	language	provides	similar	functionality	with	its
overloaded	&lt;&lt;	(&quot;put	to&quot;)	and	&gt;&gt;	(&quot;get	from&quot;)
operators.	On	Linux	systems,	these	higher-level	I/O
functions	are	implemented	using	system-level	
Unix
I/O
functions	provided	by	the	kernel.	Most	of	the
time,	the	higher-level	I/O	functions	work	quite	well
and	there	is	no	need	to	use	Unix	I/O	directly.	So	why
bother	learning	about	Unix	I/O?
Understanding	Unix	I/O	will	help	you
understand	other	systems	concepts.	
I/O	is
integral	to	the	operation	of	a	system,	and
because	of	this,	we	often	encounter	circular
dependencies	between	I/O	and	other	systems
ideas.	For	example,	I/O	plays	a	key	role	in</p>
<p>process	creation	and	execution.	Conversely,
process	creation	plays	a	key	role	in	how	files	are
shared	by	different	processes.	Thus,	to	really
understand	I/O,	you	need	to	understand
processes,	and	vice	versa.	We	have	already
touched	on	aspects	of	I/O	in	our	discussions	of
the	memory	hierarchy,	linking	and	loading,
processes,	and	virtual	memory.	Now	that	you
have	a	better	understanding	of	these	ideas,	we
can	close	the	circle	and	delve	into	I/O	in	more
detail.
Sometimes	you	have	no	choice	but	to	use
Unix	I/O.	
There	are	some	important	cases
where	using	higher-level	I/O	functions	is	either
impossible	or	inappropriate.	For	example,	the
standard	I/O	library	provides	no	way	to	access
file	metadata	such	as	file	size	or	file	creation
time.	Further,	there	are	problems	with	the
standard	I/O	library	that	make	it	risky	to	use	for
network	programming.
This	chapter	introduces	you	to	the	general	concepts
of	Unix	I/O	and	standard	I/O	and	shows	you	how	to
use	them	reliably	from	your	C	programs.	Besides
serving	as	a	general	introduction,	this	chapter	lays	a
firm	foundation	for	our	subsequent	study	of	network
programming	and	concurrency.</p>
<h2>10.1	
Unix	I/O
A	Linux	
file
is	a	sequence	of	
m
bytes:
All	I/O	devices,	such	as	networks,	disks,	and	terminals,	are	modeled	as
files,	and	all	input	and	output	is	performed	by	reading	and	writing	the
appropriate	files.	This	elegant	mapping	of	devices	to	files	allows	the
Linux	kernel	to	export	a	simple,	low-level	application	interface,	known	as
Unix	I/O
,	that	enables	all	input	and	output	to	be	performed	in	a	uniform
and	consistent	way:
Opening	files.	
An	application	announces	its	intention	to	access	an
I/O	device	by	asking	the	kernel	to	
open
the	corresponding	file.	The
kernel	returns	a	small	nonnegative	integer,	called	a	
descriptor
,	that
identifies	the	file	in	all	subsequent	operations	on	the	file.	The	kernel
keeps	track	of	all	information	about	the	open	file.	The	application	only
keeps	track	of	the	descriptor.
Each	process	created	by	a	Linux	shell	begins	life	with	three	open
files:	
standard	input
(descriptor	0),	
standard	output
(descriptor	1),	and
standard	error
(descriptor	2).	The	header	file	
defines
constants	
,	and	
,	which	can
be	used	instead	of	the	explicit	descriptor	values.
Changing	the	current	file	position.	
The	kernel	maintains	a	
file
position	k
,	initially	0,	for	each	open	file.	The	file	position	is	a	byte
B
0
,
 
B
1
,
 
…
,
 
B
k
,
 
…
,
 
B
m</h2>
<p>1</p>
<p>offset	from	the	beginning	of	a	file.	An	application	can	set	the	current
file	position	
k
explicitly	by	performing	a	
seek
operation.
Reading	and	writing	files.	
A	
read
operation	copies	
n
&gt;	0	bytes	from
a	file	to	memory,	starting	at	the	current	file	position	
k
and	then
incrementing	
k
by	
n
.	Given	a	file	with	a	size	of	
m
bytes,	performing	a
read	operation	when	
k
≥	
m
triggers	a	condition	known	as	
end-of-file
(EOF)
,	which	can	be	detected	by	the	application.	There	is	no	explicit
&quot;EOF	character&quot;	at	the	end	of	a	file.
Similarly,	a	
write
operation	copies	
n
&gt;	0	bytes	from	memory	to	a	file,
starting	at	the	current	file	position	
k
and	then	updating	
k
.
Closing	files.	
When	an	application	has	finished	accessing	a	file,	it
informs	the	kernel	by	asking	it	to	
close
the	file.	The	kernel	responds
by	freeing	the	data	structures	it	created	when	the	file	was	opened	and
restoring	the	descriptor	to	a	pool	of	available	descriptors.	When	a
process	terminates	for	any	reason,	the	kernel	closes	all	open	files	and
frees	their	memory	resources.</p>
<p>10.2	
Files
Each	Linux	file	has	a	
type
that	indicates	its	role	in	the	system:
A	
regular	file
contains	arbitrary	data.	Application	programs	often
distinguish	between	
text	files
,	which	are	regular	files	that	contain	only
ASCII	or	Unicode	characters,	and	
binary	files
,	which	are	everything
else.	To	the	kernel	there	is	no	difference	between	text	and	binary	files.
A	Linux	text	file	consists	of	a	sequence	of	
text	lines
,	where	each	line
is	a	sequence	of	characters	terminated	by	a	
newline
character	(<code>\n'). The	newline	character	is	the	same	as	the	ASCII	line	feed	character (LF)	and	has	a	numeric	value	of	 . A	 directory is	a	file	consisting	of	an	array	of	 links ,	where	each	link maps	a	 filename to	a	file,	which	may	be	another	directory.	Each directory	contains	at Aside	 End	of	line	(EOL)	indicators One	of	the	clumsy	aspects	of	working	with	text	files	is	that different	systems	use	different	characters	to	mark	the	end	of	a line.	Linux	and	Mac	OS	X	use	</code>\n'	(
),	which	is	the	ASCII	line
feed	(LF)	character.	However,	MS	Windows	and	Internet
protocols	such	as	HTTP	use	the	sequence	`\r\n'	(
),
which	is	the	ASCII	carriage	return	(CR)	character	followed	by	a
line	feed	(LF).	If	you	create	a	file	
in	Windows	and	then
view	it	in	a	Linux	text	editor,	you'll	see	an	annoying	
⁁
at	the
end	of	each	line,	which	is	how	Linux	tools	display	the	CR</p>
<p>character.	You	can	remove	these	unwanted	CR	characters
from	
in	place	by	running	the	following	command:
least	two	entries:	.	(dot)	is	a	link	to	the	directory	itself,	and	
(dot-dot)
is	a	link	to	the	
parent	directory
in	the	directory	hierarchy	(see	below).
You	can	create	a	directory	with	the	
command,	view	its	contents
with	
,	and	delete	it	with	
.
A	
socket
is	a	file	that	is	used	to	communicate	with	another	process
across	a	network	(
Section	
11.4
).
Other	file	types	include	
named	pipes
,	
symbolic	links
,	and	
character
and
block	devices
,	which	are	beyond	our	scope.
The	Linux	kernel	organizes	all	files	in	a	single	
directory	hierarchy
anchored	by	the	
root	directory
named	/	(slash).	Each	file	in	the	system	is
a	direct	or	indirect	descendant	of	the	root	directory.	
Figure	
10.1
shows
a	portion	of	the	directory	hierarchy	on	our	Linux	system.
As	part	of	its	context,	each	process	has	a	
current	working	directory
that
identifies	its	current	location	in	the	directory	hierarchy.	You	can	change
the	shell's	current	working	directory	with	the	
command.</p>
<p>Figure	
10.1	
Portion	of	the	Linux	directory	hierarchy.
A	trailing	slash	denotes	a	directory.
Locations	in	the	directory	hierarchy	are	specified	by	
pathnames
.	A
pathname	is	a	string	consisting	of	an	optional	slash	followed	by	a
sequence	of	filenames	separated	by	slashes.	Pathnames	have	two
forms:
An	
absolute	pathname
starts	with	a	slash	and	denotes	a	path	from	the
root	node.	For	example,	in	
Figure	
10.1
,	the	absolute	pathname	for
is	
.
A	
relative	pathname
starts	with	a	filename	and	denotes	a	path	from
the	current	working	directory.	For	example,	in	
Figure	
10.1
,	if
is	the	current	working	directory,	then	the	relative
pathname	for	
is	
On	the	other	hand,	if
is	the	current	working	directory,	then	the	relative
pathname	is	</p>
<p>10.3	
Opening	and	Closing	Files
A	process	opens	an	existing	file	or	creates	a	new	file	by	calling	the	open
function.
The	
function	converts	a	
to	a	file	descriptor	and	returns	the
descriptor	number.	The	descriptor	returned	is	always	the	smallest
descriptor	that	is	not	currently	open	in	the	process.	The	
argument
indicates	how	the	process	intends	to	access	the	file:
O_RDONLY.	Reading	only
O_WRONLY.	Writing	only
O_RDWR.	Reading	and	writing
For	example,	here	is	how	to	open	an	existing	file	for	reading:</p>
<p>The	
argument	can	also	be	
OR
ed	with	one	or	more	bit	masks	that
provide	additional	instructions	for	writing:
O_CREAT.	If	the	file	doesn't	exist,	then	create	a	
truncated
(empty)
version	of	it.
O_TRUNC.	If	the	file	already	exists,	then	truncate	it.
O_APPEND.	Before	each	write	operation,	set	the	file	position	to	the
end	of	the	file.
Mask
Description
S_IRUSR
User	(owner)	can	read	this	file
S_IWUSR
User	(owner)	can	write	this	file
S_IXUSR
User	(owner)	can	execute	this	file
S_IRGRP
Members	of	the	owner's	group	can	read	this	file
S_IWGRP
Members	of	the	owner's	group	can	write	this	file
S_IXGRP
Members	of	the	owner's	group	can	execute	this	file
S_IROTH
Others	(anyone)	can	read	this	file
S_IWOTH
Others	(anyone)	can	write	this	file
S_IXOTH
Others	(anyone)	can	execute	this	file</p>
<p>Figure	
10.2	
Access	permission	bits.
Defined	in	
For	example,	here	is	how	you	might	open	an	existing	file	with	the	intent	of
appending	some	data:
The	
argument	specifies	the	access	permission	bits	of	new	files.	The
symbolic	names	for	these	bits	are	shown	in	
Figure	
10.2
.
As	part	of	its	context,	each	process	has	a	
that	is	set	by	calling	the
function.	When	a	process	creates	a	new	file	by	calling	the	
function	with	some	
argument,	then	the	access	permission	bits	of	the
file	are	set	to	
.	For	example,	suppose	we	are	given	the
following	default	values	for	
and	
:
Then	the	following	code	fragment	creates	a	new	file	in	which	the	owner	of
the	file	has	read	and	write	permissions,	and	all	other	users	have	read
permissions:</p>
<p>Finally,	a	process	closes	an	open	file	by	calling	the	
function.
Closing	a	descriptor	that	is	already	closed	is	an	error.
Practice	Problem	
10.1	
(solution
page	
915
)
What	is	the	output	of	the	following	program?</p>
<p>10.4	
Reading	and	Writing	Files
Applications	perform	input	and	output	by	calling	the	
and	
functions,	respectively.
The	
function	copies	at	most	
bytes	from	the	current	file	position	of
descriptor	
to	memory	location	
.	A	return	value	of	−1	indicates	an
error,	and	a	return	value	of	0	indicates	EOF.	Otherwise,	the	return	value
indicates	the	number	of	bytes	that	were	actually	transferred.
The	
function	copies	at	most	
bytes	from	memory	location	
to
the	current	file	position	of	descriptor	
.	
Figure	
10.3
shows	a	program
that	uses	
and	
calls	to	copy	the	standard	input	to	the	standard
output,	1	byte	at	a	time.</p>
<p>Applications	can	explicitly	modify	the	current	file	position	by	calling	the
function,	which	is	beyond	our	scope.
In	some	situations,	
and	
transfer	fewer	bytes	than	the
application	requests.	Such	
short	counts
do	
not
indicate	an	error.	They
occur	for	a	number	of	reasons:
Aside	
What's	the	difference	between
and	
You	might	have	noticed	that	the	
function	has	a	
input
argument	and	an	
return	value.	So	what's	the	difference
between	these	two	types?	On	x86-64	systems,	a	
is	defined
as	an	
,	and	an	
(
signed	size
)	is	defined	as	a
.	The	read	function	returns	a	signed	size	rather	than	an
unsigned	size	because	it	must	return	a	−1	on	error.	Interestingly,
the	possibility	of	returning	a	single	−1	reduces	the	maximum	size
of	a	
by	a	factor	of	2.</p>
<p>Figure	
10.3	
Using	read	and	write	to	copy	standard	input	to	standard
output	1	byte	at	a	time.
Encountering	EOF	on	reads.	
Suppose	that	we	are	ready	to	read
from	a	file	that	contains	only	20	more	bytes	from	the	current	file
position	and	that	we	are	reading	the	file	in	50-byte	chunks.	Then	the
next	read	will	return	a	short	count	of	20,	and	the	
after	that	will
signal	EOF	by	returning	a	short	count	of	0.
Reading	text	lines	from	a	terminal.	
If	the	open	file	is	associated	with
a	terminal	(i.e.,	a	keyboard	and	display),	then	each	
function	will
transfer	one	text	line	at	a	time,	returning	a	short	count	equal	to	the
size	of	the	text	line.
Reading	and	writing	network	sockets.	
If	the	open	file	corresponds
to	a	network	socket	(
Section	
11.4
),	then	internal	buffering
constraints	and	long	network	delays	can	cause	
and	
to
return	short	counts.	Short	counts	can	also	occur	when	you	call	
and	
on	a	Linux	
pipe
,	an	interprocess	communication
mechanism	that	is	beyond	our	scope.
In	practice,	you	will	never	encounter	short	counts	when	you	read	from
disk	files	except	on	EOF,	and	you	will	never	encounter	short	counts	when
you	write	to	disk	files.	However,	if	you	want	to	build	robust	(reliable)
network	applications	
such	as	Web	servers,	then	you	must	deal	with	short</p>
<p>counts	by	repeatedly	calling	
and	
until	all	requested	bytes
have	been	transferred.</p>
<p>10.5	
Robust	Reading	and	Writing
with	the	R
IO
Package
In	this	section,	we	will	develop	an	I/O	package,	called	the	R
IO</p>
<p>(Robust
I/O)	package,	that	handles	these	short	counts	for	you	automatically.	The
R
IO</p>
<p>package	provides	convenient,	robust,	and	efficient	I/O	in	applications
such	as	network	programs	that	are	subject	to	short	counts.	R
IO</p>
<p>provides
two	different	kinds	of	functions:
Unbuffered	input	and	output	functions.	
These	functions	transfer
data	directly	between	memory	and	a	file,	with	no	application-level
buffering.	They	are	especially	useful	for	reading	and	writing	binary
data	to	and	from	networks.
Buffered	input	functions.	
These	functions	allow	you	to	efficiently
read	text	lines	and	binary	data	from	a	file	whose	contents	are	cached
in	an	application-level	buffer,	similar	to	the	one	provided	for	standard
I/O	functions	such	as	
.	Unlike	the	buffered	I/O	routines
presented	in	[110],	the	buffered	R
IO</p>
<p>input	functions	are	thread-safe
(
Section	
12.7.1
)	and	can	be	interleaved	arbitrarily	on	the	same
descriptor.	For	example,	you	can	read	some	text	lines	from	a
descriptor,	then	some	binary	data,	and	then	some	more	text	lines.
We	are	presenting	the	R
IO</p>
<p>routines	for	two	reasons.	First,	we	will	be
using	them	in	the	network	applications	we	develop	in	the	next	two
chapters.	Second,	by	studying	the	code	for	these	routines,	you	will	gain	a
deeper	understanding	of	Unix	I/O	in	general.</p>
<p>10.5.1	
Unbuffered	Input	and
Output	Functions
Applications	can	transfer	data	directly	between	memory	and	a	file	by
calling	the	
and	
functions.
The	
function	transfers	up	to	
bytes	from	the	current	file
position	of	descriptor	
to	memory	location	
.	Similarly,	the
function	transfers	
bytes	from	location	
to	descriptor
.	The	
function	can	only	return	a	short	count	if	it	encounters
EOF.	The	
function	never	returns	a	short	count.	Calls	to
and	
can	be	interleaved	arbitrarily	on	the	same
descriptor.
Figure	
10.4
shows	the	code	for	
and	
.	Notice	that
each	function	manually	restarts	the	
or	
function	if	it	is
interrupted	by	the	return	from	an	application	signal	handler.	To	be	as</p>
<p>portable	as	possible,	we	allow	for	interrupted	system	calls	and	restart
them	when	necessary.
10.5.2	
Buffered	Input	Functions
Suppose	we	wanted	to	write	a	program	that	counts	the	number	of	lines	in
a	text	file.	How	might	we	do	this?	One	approach	is	to	use	the	
function	to	transfer	1	byte	at	a	time	from	the	file	to	the	user's	memory,
checking	each	byte	for	the	newline	character.	The	disadvantage	of	this
approach	is	that	it	is	inefficient,	requiring	a	trap	to	the	kernel	to	read	each
byte	in	the	file.
A	better	approach	is	to	call	a	wrapper	function	(
)	that	copies
the	text	line	from	an	internal	
read	buffer
,	automatically	making	a	read	call
to	refill	the	buffer	whenever	it	becomes	empty.	For	files	that	contain	both
text	lines	and	binary	data	(such	as	the	HTTP	responses	described	in
Section	
11.5.3
),	we	also	provide	a	buffered	version	of	
,
called	
b,	that	transfers	raw	bytes	from	the	same	read	buffer	as
.</p>
<p>The	
function	is	called	once	per	open	descriptor.	It
associates	the	descriptor	
with	a	read	buffer	of	type	
at	address
.
The	
function	reads	the	next	text	line	from	file	
(including
the	terminating	newline	character),	copies	it	to	memory	location	
,
and	terminates	the	text	line	with	the	NULL	(zero)	character.	The
function	reads	at	most	
bytes,	leaving	room	for
the	terminating	NULL	character.	Text	lines	that	exceed	
bytes
are	truncated	and	terminated	with	a	NULL	character.
The	
b	function	reads	up	to	
bytes	from	file	
to	memory
location	
.	Calls	to	
and	
can	be
interleaved	arbitrarily	on	the	same	descriptor.	However,	calls	to	these
buffered	functions	should	not	be	interleaved	with	calls	to	the	unbuffered
function.
You	will	encounter	numerous	examples	of	the	R
IO</p>
<p>functions	in	the
remainder	of	this	text.	
Figure	
10.5
shows	how	to	use	the	R
IO</p>
<p>functions
to	copy	a	text	file	from	standard	input	to	standard	output,	one	line	at	a
time.
Figure	
10.6
shows	the	format	of	a	read	buffer,	along	with	the	code	for
the	
function	that	initializes	it.	The	
function</p>
<p>sets	up	an	empty	read	buffer	and	associates	an	open	file	descriptor	with
that	buffer.</p>
<p>Figure	
10.4	
The	
and	
functions.</p>
<p>Figure	
10.5	
Copying	a	text	file	from	standard	input	to	standard
output.</p>
<p>Figure	
10.6	
A	read	buffer	of	type	
and	the	
function	that	initializes	it.
The	heart	of	the	R
IO</p>
<p>read	routines	is	the	
function	shown	in
Figure	
10.7
.	The	
function	is	a	buffered	version	of	the	Linux
function.	When	
is	called	with	a	request	to	read	
bytes,
there	are	
unread	bytes	in	the	read	buffer.	If	the	buffer	is
empty,	then	it	is	replenished	with	a	call	to	
.	Receiving	a	short	count
from	this	invocation	of	
is	not	an	error;	it	simply	has	the	effect	of
partially	filling	the	read	buffer.	Once	the	buffer	is</p>
<p>Figure	
10.7	
The	internal	
function.
nonempty,	
copies	the	minimum	of	
and	
bytes
from	the	read	buffer	to	the	user	buffer	and	returns	the	number	of	bytes</p>
<p>copied.
To	an	application	program,	the	
function	has	the	same	semantics
as	the	Linux	read	function.	On	error,	it	returns	−1	and	sets	
appropriately.	On	EOF,	it	returns	0.	It	returns	a	short	count	if	the	number
of	requested	bytes	exceeds	the	number	of	unread	bytes	in	the	read
buffer.	The	similarity	of	the	two	functions	makes	it	easy	to	build	different
kinds	of	buffered	read	functions	by	substituting	
for	
.	For
example,	the	
function	in	
Figure	
10.8
has	the	same
structure	as	
,	with	
substituted	for	read.	Similarly,	the
routine	in	
Figure	
10.8
calls	
at	most	
times.	Each	call	returns	1	byte	from	the	read	buffer,	which	is	then
checked	for	being	the	terminating	newline.</p>
<p>Figure	
10.8	
The	
and	
functions.
Aside	
Origins	of	the	Rio	package
The	R
IO</p>
<p>functions	are	inspired	by	the	
,	and	
functions	described	by	W.	Richard	Stevens	in	his	classic	network
programming	text	[110].	The	
and	
functions
are	identical	to	the	Stevens	
and	
functions.	However,
the	Stevens	
function	has	some	limitations	that	are
corrected	in	R
IO
.	First,	because	
is	buffered	and	
is
not,	these	two	functions	cannot	be	used	together	on	the	same
descriptor.	Second,	because	it	uses	a	
buffer,	the	Stevens
function	is	not	thread-safe,	which	required	Stevens	to
introduce	a	different	thread-safe	version	called	
.	We
have	corrected	both	of	these	flaws	with	the	
and
functions,	which	are	mutually	compatible	and	thread-
safe.</p>
<p>10.6	
Reading	File	Metadata
An	application	can	retrieve	information	about	a	file	(sometimes	called	the
file's	
metadata
)	by	calling	the	
and	
functions.
The	
function	takes	as	input	a	filename	and	fills	in	the	members	of	a
structure	shown	in	
Figure	
10.9
.	The	
function	is	similar,	but
it	takes	a	file	descriptor	instead	of	a	filename.	We	will	need	the	
and	
members	of	the	
structure	when	we	discuss	Web
servers	in	
Section	
11.5
.	The	other	members	are	beyond	our	scope.
The	
member	contains	the	file	size	in	bytes.	The	
member
encodes	both	the	file	permission	bits	(
Figure	
10.2
)	and	the	file	type
(
Section	
10.2
).	Linux	defines	macro	predicates	in	
for
determining	the	file	type	from	the	
member:
S_ISREG(m).	Is	this	a	regular	file?</p>
<p>S_ISDIR(m).	Is	this	a	directory	file?
S_ISSOCK(m).	Is	this	a	network	socket?
Figure	
10.10
shows	how	we	might	use	these	macros	and	the	
function	to	read	and	interpret	a	file's	
bits.</p>
<p>Figure	
10.9	
The	
structure.</p>
<p>Figure	
10.10	
Querying	and	manipulating	a	file's	
bits.</p>
<p>10.7	
Reading	Directory	Contents
Applications	can	read	the	contents	of	a	directory	with	the	
family
of	functions.
The	
function	takes	a	pathname	and	returns	a	pointer	to	a
directory	stream
.	A	stream	is	an	abstraction	for	an	ordered	list	of	items,	in
this	case	a	list	of	directory	entries.
Each	call	to	
returns	a	pointer	to	the	next	directory	entry	in	the
stream	
,	or	NULL	if	there	are	no	more	entries.	Each	directory	entry	is</p>
<p>a	structure	of	the	form
Although	some	versions	of	Linux	include	other	structure	members,	these
are	the	only	two	that	are	standard	across	all	systems.	The	
member	is	the	filename,	and	
is	the	file	location.
On	error,	
returns	NULL	and	sets	
.	Unfortunately,	the	only
way	to	distinguish	an	error	from	the	end-of-stream	condition	is	to	check	if
has	been	modified	since	the	call	to	
.
The	
function	closes	the	stream	and	frees	up	any	of	its
resources.	
Figure	
10.11
shows	how	we	might	use	
to	read	the
contents	of	a	directory.</p>
<p>Figure	
10.11	
Reading	the	contents	of	a	directory.</p>
<p>10.8	
Sharing	Files
Linux	files	can	be	shared	in	a	number	of	different	ways.	Unless	you	have
a	clear	picture	of	how	the	kernel	represents	open	files,	the	idea	of	file
sharing	can	be	quite	confusing.	The	kernel	represents	open	files	using
three	related	data	structures:
Descriptor	table.	
Each	process	has	its	own	separate	
descriptor	table
whose	entries	are	indexed	by	the	process's	open	file	descriptors.
Each	open	descriptor	entry	points	to	an	entry	in	the	
file	table.
File	table.	
The	set	of	open	files	is	represented	by	a	file	table	that	is
shared	by	all	processes.	Each	file	table	entry	consists	of	(for	our
purposes)	the	current	file	position,	a	
reference	count
of	the	number	of
descriptor	entries	that	currently	point	to	it,	and	a	pointer	to	an	entry	in
the	
v-node	table
.	Closing	a	descriptor	decrements	the	reference	count
in	the	associated	file	table	entry.	The	kernel	will	not	delete	the	file
table	entry	until	its	reference	count	is	zero.
v-node	table.	
Like	the	file	table,	the	v-node	table	is	shared	by	all
processes.	Each	entry	contains	most	of	the	information	in	the	
structure,	including	the	
and	
members.</p>
<p>Figure	
10.12	
Typical	kernel	data	structures	for	open	files.
In	this	example,	two	descriptors	reference	distinct	files.	There	is	no
sharing.
Figure	
10.13	
File	sharing.
This	example	shows	two	descriptors	sharing	the	same	disk	file	through
two	open	file	table	entries.</p>
<p>Figure	
10.12
shows	an	example	where	descriptors	1	and	4	reference
two	different	files	through	distinct	open	file	table	entries.	This	is	the
typical	situation,	where	files	are	not	shared	and	where	each	descriptor
corresponds	to	a	distinct	file.
Multiple	descriptors	can	also	reference	the	same	file	through	different	file
table	entries,	as	shown	in	
Figure	
10.13
.	This	might	happen,	for
example,	if	you	were	to	call	the	
function	twice	with	the	same
filename.	The	key	idea	is	that	each	descriptor	has	its	own	distinct	file
position,	so	different	reads	on	different	descriptors	can	fetch	data	from
different	locations	in	the	file.
We	can	also	understand	how	parent	and	child	processes	share	files.
Suppose	that	before	a	call	to	
,	the	parent	process	has	the	open	files
shown	in	
Figure	
10.12
.	Then	
Figure	
10.14
shows	the	situation	after
the	call	to	
.
The	child	gets	its	own	duplicate	copy	of	the	parent's	descriptor	table.
Parent	and	child	share	the	same	set	of	open	file	tables	and	thus	share
the	same	file	position.	An	important	consequence	is	that	the	parent	and
child	must	both	close	their	descriptors	before	the	kernel	will	delete	the
corresponding	file	table	entry.</p>
<p>Figure	
10.14	
How	a	child	process	inherits	the	parent's	open	files.
The	initial	situation	is	in	
Figure	
10.12
.
Practice	Problem	
10.2	
(solution
page	
915
)
Suppose	the	disk	file	
consists	of	the	six	ASCII
characters	
.	Then	what	is	the	output	of	the	following
program?</p>
<p>Practice	Problem	
10.3	
(solution
page	
915
)
As	before,	suppose	the	disk	file	
consists	of	the	six	ASCII
characters	
.	Then	what	is	the	output	of	the	following	program?</p>
<p>10.9	
I/O	Redirection
Linux	shells	provide	
I/O	redirection
operators	that	allow	users	to
associate	standard	input	and	output	with	disk	files.	For	example,	typing
causes	the	shell	to	load	and	execute	the	
program,	with	standard
output	redirected	to	disk	file	
.	As	we	will	see	in	
Section	
11.5
,	a
Web	server	performs	a	similar	kind	of	redirection	when	it	runs	a	CGI
program	on	behalf	of	the	client.	So	how	does	I/O	redirection	work?	One
way	is	to	use	the	
function.
The	
function	copies	descriptor	table	entry	
to	descriptor	table
entry	
,	overwriting	the	previous	contents	of	descriptor	table	entry
.	If	
was	already	open,	then	
closes	
before	it
copies	
.</p>
<p>Suppose	that	before	calling	
,	we	have	the	situation	in	
Figure
10.12
,	where	descriptor	1	(standard	output)	corresponds	to	file	A	(say,
a	terminal)	and	descriptor	4	corresponds	to	file	B	(say,	a	disk	file).	The
reference	counts	for	A	and	B	are	both	equal	to	1.	
Figure	
10.15
shows
the	situation	after	calling	
.	Both	descriptors	now	point	to	file	B;
file	A	has	been	closed	and	its	file	table	and	v-node	table	entries	deleted;
and	the	reference	count	for	file	B	has	been	incremented.	From	this	point
on,	any	data	written	to	standard	output	are	redirected	to	file	B.
Practice	Problem	
10.4	
(solution
page	
915
)
How	would	you	use	
to	redirect	standard	input	to	descriptor
5?
Aside	
Right	and	left	hoinkies
To	avoid	confusion	with	other	bracket-type	operators	such	as	<code>]' and	</code>[',	we	have	always	referred	to	the	shell's	<code>&gt;'	operator	as	a &quot;right	hoinky&quot;	and	the	</code>&lt;'	operator	as	a	&quot;left	hoinky.&quot;</p>
<p>Figure	
10.15	
Kernel	data	structures	after	redirecting	standard	output
by	calling	
.
The	initial	situation	is	shown	in	
Figure	
10.12
.
Practice	Problem	
10.5	
(solution
page	
916
)
Assuming	that	the	disk	file	
consists	of	the	six	ASCII
characters	
,	what	is	the	output	of	the	following	program?</p>
<p>10.10	
Standard	I/O
The	C	language	defines	a	set	of	higher-level	input	and	output	functions,
called	the	
standard	I/O	library
,	that	provides	programmers	with	a	higher-
level	alternative	to	Unix	I/O.	The	library	(
)	provides	functions	for
opening	and	closing	files	(
and	
),	reading	and	writing	bytes
(
and	
),	reading	and	writing	strings	(
and	
),	and
sophisticated	formatted	I/O	(
and	
).
The	standard	I/O	library	models	an	open	file	as	a	
stream
.	To	the
programmer,	a	stream	is	a	pointer	to	a	structure	of	type	
.	Every	ANSI
C	program	begins	with	three	open	streams,	
,	and	
,
which	correspond	to	standard	input,	standard	output,	and	standard	error,
respectively:
A	stream	of	type	FILE	is	an	abstraction	for	a	file	descriptor	and	a	
stream
buffer
.	The	purpose	of	the	stream	buffer	is	the	same	as	the	R
IO</p>
<p>read
buffer:	to	minimize	the	number	of	expensive	Linux	I/O	system	calls.	For
example,	suppose	we	have	a	program	that	makes	repeated	calls	to	the</p>
<p>standard	I/O	
function,	where	each	invocation	returns	the	next
character	from	a	file.	When	
is	called	the	first	time,	the	library	fills	the
stream	buffer	with	a	single	call	to	the	
function	and	then	returns	the
first	byte	in	the	buffer	to	the	application.	As	long	as	there	are	unread
bytes	in	the	buffer,	subsequent	calls	to	
can	be	served	directly	from
the	stream	buffer.</p>
<p>10.11	
Putting	It	Together:	Which	I/O
Functions	Should	I	Use?
Figure	
10.16
summarizes	the	various	I/O	packages	that	we	have
discussed	in	this	chapter.
Figure	
10.16	
Relationship	between	Unix	I/O,	standard	I/O,	and	R
IO
.
The	Unix	I/O	model	is	implemented	in	the	operating	system	kernel.	It	is
available	to	applications	through	functions	such	as	
,	and	
.	The	higher-level	R
IO</p>
<p>and	standard	I/O	functions
are	implemented	&quot;on	top	of&quot;	(using)	the	Unix	I/O	functions.	The	R
IO
functions	are	robust	wrappers	for	
and	
that	were	developed
specifically	for	this	textbook.	They	automatically	deal	with	short	counts
and	provide	an	efficient	buffered	approach	for	reading	text	lines.	The
standard	I/O	functions	provide	a	more	complete	buffered	alternative	to
the	Unix	I/O	functions,	including	formatted	I/O	routines	such	as	
and	
.</p>
<p>So	which	of	these	functions	should	you	use	in	your	programs?	Here	are
some	basic	guidelines:
G1:	
Use	the	standard	I/O	functions	whenever	possible.
The
standard	I/O	functions	are	the	method	of	choice	for	I/O	on	disk	and
terminal	devices.	Most	C	programmers	use	standard	I/O	exclusively
throughout	their	careers,	never	bothering	with	the	lower-level	Unix	I/O
functions	(except	possibly	
,	which	has	no	counterpart	in	the
standard	I/O	library).	Whenever	possible,	we	recommend	that	you	do
likewise.
G2:	
Don't	use	
or	
to	read	binary	files.
Functions	like	
and	
are	designed	specifically	for
reading	text	files.	A	common	error	that	students	make	is	to	use	these
functions	to	read	binary	data,	causing	their	programs	to	fail	in	strange
and	unpredictable	ways.	For	example,	binary	files	might	be	littered
with	many	
bytes	that	have	nothing	to	do	with	terminating	text
lines.
G3:	
Use	the	R
IO</p>
<p>functions	for	I/O	on	network	sockets.
Unfortunately,	standard	I/O	poses	some	nasty	problems	when	we
attempt	to	use	it	for	input	and	output	on	networks.	As	we	will	see	in
Section	
11.4
,	the	Linux	abstraction	for	a	network	is	a	type	of	file
called	a	
socket
.	Like	any	Linux	file,	sockets	are	referenced	by	file
descriptors,	known	in	this	case	as	
socket	descriptors
.	Application
processes	communicate	with	processes	running	on	other	computers
by	reading	and	writing	socket	descriptors.
Standard	I/O	streams	are	
full	duplex
in	the	sense	that	programs	can
perform	input	and	output	on	the	same	stream.	However,	there	are	poorly</p>
<p>documented	restrictions	on	streams	that	interact	badly	with	restrictions
on	sockets:
Restriction	1:	
Input	functions	following	output	functions.
An	input
function	cannot	follow	an	output	function	without	an	intervening	call	to
,	or	rewind.	The	
function	empties	the
buffer	associated	with	a	stream.	The	latter	three	functions	use	the
Unix	I/O	
function	to	reset	the	current	file	position.
Restriction	2:	Output	functions	following	input	functions.
An
output	function	cannot	follow	an	input	function	without	an	intervening
call	to	
,	or	
,	unless	the	input	function	encounters
an	end-of-file.
These	restrictions	pose	a	problem	for	network	applications	because	it	is
illegal	to	use	the	
function	on	a	socket.	The	first	restriction	on
stream	I/O	can	be	worked	around	by	adopting	a	discipline	of	flushing	the
buffer	before	every	input	operation.	However,	the	only	way	to	work
around	the	second	restriction	is	to	open	two	streams	on	the	same	open
socket	descriptor,	one	for	reading	and	one	for	writing:
But	this	approach	has	problems	as	well,	because	it	requires	the
application	to	call	
on	both	streams	in	order	to	free	the	memory</p>
<p>resources	associated	with	each	stream	and	avoid	a	memory	leak:
Each	of	these	operations	attempts	to	close	the	same	underlying	socket
descriptor,	so	the	second	
operation	will	fail.	This	is	not	a	problem
for	sequential	programs,	but	closing	an	already	closed	descriptor	in	a
threaded	program	is	a	recipe	for	disaster	(see	
Section	
12.7.4
).
Thus,	we	recommend	that	you	not	use	the	standard	I/O	functions	for
input	and	output	on	network	sockets.	Use	the	robust	R
IO</p>
<p>functions
instead.	If	you	need	formatted	output,	use	the	
function	to	format
a	string	in	memory,	and	then	send	it	to	the	socket	using	
.	If
you	need	formatted	input,	use	
to	read	an	entire	text	line,
and	then	use	
to	extract	different	fields	from	the	text	line.</p>
<p>10.12	
Summary
Linux	provides	a	small	number	of	system-level	functions,	based	on	the
Unix	I/O	model,	that	allow	applications	to	open,	close,	read,	and	write
files,	to	fetch	file	metadata,	and	to	perform	I/O	redirection.	Linux	read	and
write	operations	are	subject	to	short	counts	that	applications	must
anticipate	and	handle	correctly.	Instead	of	calling	the	Unix	I/O	functions
directly,	applications	should	use	the	R
IO</p>
<p>package,	which	deals	with	short
counts	automatically	by	repeatedly	performing	read	and	write	operations
until	all	of	the	requested	data	have	been	transferred.
The	Linux	kernel	uses	three	related	data	structures	to	represent	open
files.	Entries	in	a	descriptor	table	point	to	entries	in	the	open	file	table,
which	point	to	entries	in	the	v-node	table.	Each	process	has	its	own
distinct	descriptor	table,	while	all	processes	share	the	same	open	file	and
v-node	tables.	Understanding	the	general	organization	of	these
structures	clarifies	our	understanding	of	both	file	sharing	and	I/O
redirection.
The	standard	I/O	library	is	implemented	on	top	of	Unix	I/O	and	provides	a
powerful	set	of	higher-level	I/O	routines.	For	most	applications,	standard
I/O	is	the	
simpler,	preferred	alternative	to	Unix	I/O.	However,	because	of
some	mutually	incompatible	restrictions	on	standard	I/O	and	network
files,	Unix	I/O,	rather	than	standard	I/O,	should	be	used	for	network
applications.</p>
<p>Bibliographic	Notes
Kerrisk	gives	a	comprehensive	treatment	of	Unix	I/O	and	the	Linux	file
system	
[62]
.	Stevens	wrote	the	original	standard	reference	text	for	Unix
I/O	[111].	Kernighan	and	Ritchie	give	a	clear	and	complete	discussion	of
the	standard	I/O	functions	
[61]
.</p>
<p>Homework	Problems
10.6
What	is	the	output	of	the	following	program?
10.7</p>
<p>Modify	the	
program	in	
Figure	
10.5
so	that	it	uses	the	R
IO
functions	to	copy	standard	input	to	standard	output,	MAXBUF	bytes	at	a
time.
10.8
Write	a	version	of	the	
program	in	
Figure	
10.10
,	called
,	that	takes	a	descriptor	number	on	the	command	line	rather
than	a	filename.
10.9
Consider	the	following	invocation	of	the	
program	from
Problem	
10.8
:
You	might	expect	that	this	invocation	of	
would	fetch	and
display	metadata	for	file	
.	However,	when	we	run	it	on	our
system,	it	fails	with	a	&quot;bad	file	descriptor.&quot;	Given	this	behavior,	fill	in	the
pseudocode	that	the	shell	must	be	executing	between	the	
and
calls:</p>
<p>10.10
Modify	the	
program	in	
Figure	
10.5
so	that	it	takes	an	optional
command-line	argument	
.	If	
is	given,	then	copy	
to
standard	output;	otherwise,	copy	standard	input	to	standard	output	as
before.	The	twist	is	that	your	solution	must	use	the	original	copy	loop
(lines	9−11)	for	both	cases.	You	are	only	allowed	to	insert	code,	and	you
are	not	allowed	to	change	any	of	the	existing	code.</p>
<p>Solutions	to	Practice	Problems
Solution	to	Problem	
10.1	
(page
895
)
Unix	processes	begin	life	with	open	descriptors	assigned	to	
(descriptor	0),	
(descriptor	1),	and	
(descriptor	2).	The	open
function	always	returns	the	lowest	unopened	descriptor,	so	the	first	call	to
open	returns	descriptor	3.	The	call	to	the	close	function	frees	up
descriptor	3.	The	final	call	to	open	returns	descriptor	3,	and	thus	the
output	of	the	program	is	
.
Solution	to	Problem	
10.2	
(page
908
)
The	descriptors	
and	
each	have	their	own	open	file	table	entry,	so
each	descriptor	has	its	own	file	position	for	
.	Thus,	the	read
from	
reads	the	first	byte	of	
,	and	the	output	is</p>
<p>and	not
as	you	might	have	thought	initially.
Solution	to	Problem	
10.3	
(page
908
)
Recall	that	the	child	inherits	the	parent's	descriptor	table	and	that	all
processes	shared	the	same	open	file	table.	Thus,	the	descriptor	
in
both	the	parent	and	child	points	to	the	same	open	file	table	entry.	When
the	child	reads	the	first	byte	of	the	file,	the	file	position	increases	by	1.
Thus,	the	parent	reads	the	second	byte,	and	the	output	is
Solution	to	Problem	
10.4	
(page</p>
<p>909
)
To	redirect	standard	input	(descriptor	0)	to	descriptor	5,	we	would	call
,	or	equivalently,	
.
Solution	to	Problem	
10.5	
(page
910
)
At	first	glance,	you	might	think	the	output	would	be
but	because	we	are	redirecting	
to	
,	the	output	is	really</p>
<p>Chapter	
11	
Network	Programming
11.1	
The	Client-Server	Programming	Model	
918
11.2	
Networks	
919
11.3	
The	Global	IP	Internet	
924
11.4	
The	Sockets	Interface	
932
11.5	
Web	Servers	
948
11.6	
Putting	It	Together:	The	Tiny	Web	Server	
956
11.7	
Summary</p>
<p>964
Bibliographic	Notes	
965
Homework	Problems	
965
Solutions	to	Practice	Problems	
966
Network	applications	are	everywhere.	Any	time	you
browse	the	Web,	send	an	email	message,	or	play
an	online	game,	you	are	using	a	network
application.	Interestingly,	all	network	applications
are	based	on	the	same	basic	programming	model,</p>
<p>have	similar	overall	logical	structures,	and	rely	on
the	same	programming	interface.
Network	applications	rely	on	many	of	the	concepts
that	you	have	already	learned	in	our	study	of
systems.	For	example,	processes,	signals,	byte
ordering,	memory	mapping,	and	dynamic	storage
allocation	all	play	important	roles.	There	are	new
concepts	to	master	as	well.	You	will	need	to
understand	the	basic	client-server	programming
model	and	how	to	write	client-server	programs	that
use	the	services	provided	by	the	Internet.	At	the
end,	we	will	tie	all	of	these	ideas	together	by
developing	a	tiny	but	functional	Web	server	that	can
serve	both	static	and	dynamic	content	with	text	and
graphics	to	real	Web	browsers.</p>
<p>11.1	
The	Client-Server
Programming	Model
Every	network	application	is	based	on	the	
client-server	model
.	With	this
model,	an	application	consists	of	a	
server
process	and	one	or	more	
client
processes.	A	server	manages	some	
resource
,	and	it	provides	some
service
for	its	clients	by	manipulating	that	resource.	For	example,	a	Web
server	manages	a	set	of	disk	files	that	it	retrieves	and	executes	on	behalf
of	clients.	An	FTP	server	manages	a	set	of	disk	files	that	it	stores	and
retrieves	for	clients.	Similarly,	an	email	server	manages	a	spool	file	that	it
reads	and	updates	for	clients.
The	fundamental	operation	in	the	client-server	model	is	the	
transaction
(
Figure	
11.1
).	A	client-server	transaction	consists	of	four	steps:
1
.	
When	a	client	needs	service,	it	initiates	a	transaction	by	sending	a
request
to	the	server.	For	example,	when	a	Web	browser	needs	a
file,	it	sends	a	request	to	a	Web	server.
2
.	
The	server	receives	the	request,	interprets	it,	and	manipulates	its
resources	in	the	appropriate	way.	For	example,	when	a	Web
server	receives	a	request	from	a	browser,	it	reads	a	disk	file.
3
.	
The	server	sends	a	
response
to	the	client	and	then	waits	for	the
next	request.	For	example,	a	Web	server	sends	the	file	back	to	a
client.</p>
<p>Figure	
11.1	
A	client-server	transaction.
Aside	
Client-server	transactions
versus	database	transactions
Client-server	transactions	are	
not
database	transactions
and	do	not	share	any	of	their	properties,	such	as	atomicity.
In	our	context,	a	transaction	is	simply	a	sequence	of	steps
carried	out	by	a	client	and	a	server.
4
.	
The	client	receives	the	response	and	manipulates	it.	For	example,
after	a	Web	browser	receives	a	page	from	the	server,	it	displays	it
on	the	screen.
It	is	important	to	realize	that	clients	and	servers	are	processes	and	not
machines,	or	
hosts
as	they	are	often	called	in	this	context.	A	single	host
can	run	many	different	clients	and	servers	concurrently,	and	a	client	and
server	transaction	can	be	on	the	same	or	different	hosts.	The	client-
server	model	is	the	same,	regardless	of	the	mapping	of	clients	and
servers	to	hosts.</p>
<p>11.2	
Networks
Clients	and	servers	often	run	on	separate	hosts	and	communicate	using
the	hardware	and	software	resources	of	a	
computer	network
.	Networks
are	sophisticated	systems,	and	we	can	only	hope	to	scratch	the	surface
here.	Our	aim	is	to	give	you	a	workable	mental	model	from	a
programmer's	perspective.
To	a	host,	a	network	is	just	another	I/O	device	that	serves	as	a	source
and	sink	for	data,	as	shown	in	
Figure	
11.2
.
Figure	
11.2	
Hardware	organization	of	a	network	host.</p>
<p>Figure	
11.3	
Ethernet	segment.
An	adapter	plugged	into	an	expansion	slot	on	the	I/O	bus	provides	the
physical	interface	to	the	network.	Data	received	from	the	network	are
copied	from	the	adapter	across	the	I/O	and	memory	buses	into	memory,
typically	by	a	DMA	transfer.	Similarly,	data	can	also	be	copied	from
memory	to	the	network.
Physically,	a	network	is	a	hierarchical	system	that	is	organized	by
geographical	proximity.	At	the	lowest	level	is	a	LAN	(local	area	network)
that	spans	a	building	or	a	campus.	The	most	popular	LAN	technology	by
far	is	
Ethernet
,	which	was	developed	in	the	mid-1970s	at	Xerox	PARC.
Ethernet	has	proven	to	be	remarkably	resilient,	evolving	from	3	Mb/s	to
10	Gb/s.
An	
Ethernet	segment
consists	of	some	wires	(usually	twisted	pairs	of
wires)	and	a	small	box	called	a	
hub
,	as	shown	in	
Figure	
11.3
.	Ethernet
segments	typically	span	small	areas,	such	as	a	room	or	a	floor	in	a
building.	Each	wire	has	the	same	maximum	bit	bandwidth,	typically	100
Mb/s	or	1	Gb/s.	One	end	is	attached	to	an	adapter	on	a	host,	and	the
other	end	is	attached	to	a	
port
on	the	hub.	A	hub	slavishly	copies	every
bit	that	it	receives	on	each	port	to	every	other	port.	Thus,	every	host	sees
every	bit.</p>
<p>Each	Ethernet	adapter	has	a	globally	unique	48-bit	address	that	is	stored
in	a	nonvolatile	memory	on	the	adapter.	A	host	can	send	a	chunk	of	bits
called	a	
frame
to	any	other	host	on	the	segment.	Each	frame	includes
some	fixed	number	of	
header
bits	that	identify	the	source	and	destination
of	the	frame	and	the	frame	length,	followed	by	a	
payload
of	data	bits.
Every	host	adapter	sees	the	frame,	but	only	the	destination	host	actually
reads	it.
Multiple	Ethernet	segments	can	be	connected	into	larger	LANs,	called
bridged	Ethernets
,	using	a	set	of	wires	and	small	boxes	called	
bridges
,
as	shown	in	
Figure	
11.4
.	Bridged	Ethernets	can	span	entire	buildings
or	campuses.	In	a	bridged	Ethernet,	some	wires	connect	bridges	to
bridges,	and	others	connect	bridges	to	hubs.	The	bandwidths	of	the	wires
can	be	different.	In	our	example,	the	bridge-bridge	wire	has	a	1	Gb/s
bandwidth,	while	the	four	hub-bridge	wires	have	bandwidths	of	100	Mb/s.
Bridges	make	better	use	of	the	available	wire	bandwidth	than	hubs.
Using	a	clever	distributed	algorithm,	they	automatically	learn	over	time
which	hosts	are	reachable	from	which	ports	and	then	selectively	copy
frames	from	one	port	to	another	only	when	it	is	necessary.	For	example,
if	host	A	sends	a	frame	to	host	B,	which	is	on	the	segment,	then	bridge	X
will	throw	away	the	frame	when	it	arrives	at	its	input	port,	thus	saving
bandwidth	on	the	other	segments.	However,	if	host	A	sends	a	frame	to
host	C	on	a	different	segment,	then	bridge	X	will	copy	the	frame	only	to
the	port	connected	to	bridge	Y,	which	will	copy	the	frame	only	to	the	port
connected	to	host	C's	segment.
Aside	
Internet	versus	internet</p>
<p>We	will	always	use	lowercase	
internet
to	denote	the	general
concept,	and	uppercase	
Internet
to	denote	a	specific
implementation—namely,	the	global	IP	Internet.
Figure	
11.4	
Bridged	Ethernet	segments.
Figure	
11.5	
Conceptual	view	of	a	LAN.
To	simplify	our	pictures	of	LANs,	we	will	draw	the	hubs	and	bridges	and
the	wires	that	connect	them	as	a	single	horizontal	line,	as	shown	in
Figure	
11.5
.</p>
<p>At	a	higher	level	in	the	hierarchy,	multiple	incompatible	LANs	can	be
connected	by	specialized	computers	called	
routers
to	form	an	
internet
(interconnected	network).	Each	router	has	an	adapter	(port)	for	each
network	that	it	is	connected	to.	Routers	can	also	connect	high-speed
point-to-point	phone	connections,	which	are	examples	of	networks	known
as	WANs	(wide	area	networks),	so	called	because	they	span	larger
geographical	areas	than	LANs.	In	general,	routers	can	be	used	to	build
internets	from	arbitrary	collections	of	LANs	and	WANs.	For	example,
Figure	
11.6
shows	an	example	internet	with	a	pair	of	LANs	and	WANs
connected	by	three	routers.
Figure	
11.6	
A	small	internet.
Two	LANs	and	two	WANs	are	connected	by	three	routers.
The	crucial	property	of	an	internet	is	that	it	can	consist	of	different	LANs
and	WANs	with	radically	different	and	incompatible	technologies.	Each
host	is	physically	connected	to	every	other	host,	but	how	is	it	possible	for
some	
source	host
to	send	data	bits	to	another	
destination	host
across	all
of	these	incompatible	networks?
The	solution	is	a	layer	of	
protocol	software
running	on	each	host	and
router	that	smoothes	out	the	differences	between	the	different	networks.
This	software	implements	a	
protocol
that	governs	how	hosts	and	routers</p>
<p>cooperate	in	order	to	transfer	data.	The	protocol	must	provide	two	basic
capabilities:
Naming	scheme.	
Different	LAN	technologies	have	different	and
incompatible	ways	of	assigning	addresses	to	hosts.	The	internet
protocol	smoothes	these	differences	by	defining	a	uniform	format	for
host	addresses.	Each	host	is	then	assigned	at	least	one	of	these
internet	addresses
that	uniquely	identifies	it.
Delivery	mechanism.	
Different	networking	technologies	have
different	and	incompatible	ways	of	encoding	bits	on	wires	and	of
packaging	these	bits	into	frames.	The	internet	protocol	smoothes
these	differences	by	defining	a	uniform	way	to	bundle	up	data	bits	into
discrete	chunks	called	
packets
.	A	packet	consists	of	a	
header
,	which
contains	the	packet	size	and	addresses	of	the	source	and	destination
hosts,	and	a	
payload
,	which	contains	data	bits	sent	from	the	source
host.
Figure	
11.7
shows	an	example	of	how	hosts	and	routers	use	the
internet	protocol	to	transfer	data	across	incompatible	LANs.	The	example
internet	consists	of	two	LANs	connected	by	a	router.	A	client	running	on
host	A,	which	is	attached	to	LAN1,	sends	a	sequence	of	data	bytes	to	a
server	running	on	host	B,	which	is	attached	to	LAN2.	There	are	eight
basic	steps:
1
.	
The	client	on	host	A	invokes	a	system	call	that	copies	the	data
from	the	client's	virtual	address	space	into	a	kernel	buffer.
2
.	
The	protocol	software	on	host	A	creates	a	LAN1	frame	by
appending	an	internet	header	and	a	LAN1	frame	header	to	the
data.	The	internet	header	is	addressed	to	internet	host	B.	The</p>
<p>LAN1	frame	header	is	addressed	to	the	router.	It	then	passes	the
frame	to	the	adapter.	Notice	that	the	payload	of	the	LAN1	frame	is
an	internet	packet,	whose	payload	is	the	actual	user	data.	This
kind	of	
encapsulation
is	one	of	the	fundamental	insights	of
internetworking.
Figure	
11.7	
How	data	travel	from	one	host	to	another	on	an
internet.
PH:	internet	packet	header;	FH1:	frame	header	for	LAN1;	FH2:
frame	header	for	LAN2.
3
.	
The	LAN1	adapter	copies	the	frame	to	the	network.
4
.	
When	the	frame	reaches	the	router,	the	router's	LAN1	adapter
reads	it	from	the	wire	and	passes	it	to	the	protocol	software.
5
.	
The	router	fetches	the	destination	internet	address	from	the
internet	packet	header	and	uses	this	as	an	index	into	a	routing</p>
<p>table	to	determine	where	to	forward	the	packet,	which	in	this	case
is	LAN2.	The	router	then	strips	off	the	old	LAN1	frame	header,
prepends	a	new	LAN2	frame	header	addressed	to	host	B,	and
passes	the	resulting	frame	to	the	adapter.
6
.	
The	router's	LAN2	adapter	copies	the	frame	to	the	network.
7
.	
When	the	frame	reaches	host	B,	its	adapter	reads	the	frame	from
the	wire	and	passes	it	to	the	protocol	software.
8
.	
Finally,	the	protocol	software	on	host	B	strips	off	the	packet	header
and	frame	header.	The	protocol	software	will	eventually	copy	the
resulting	data	into	the	server's	virtual	address	space	when	the
server	invokes	a	system	call	that	reads	the	data.
Of	course,	we	are	glossing	over	many	difficult	issues	here.	What	if
different	networks	have	different	maximum	frame	sizes?	How	do	routers
know	where	to	forward	frames?	How	are	routers	informed	when	the
network	topology	changes?	What	if	a	packet	gets	lost?	Nonetheless,	our
example	captures	the	essence	of	the	internet	idea,	and	encapsulation	is
the	key.</p>
<p>Figure	
11.8	
Hardware	and	software	organization	of	an	Internet
application.</p>
<p>11.3	
The	Global	IP	Internet
The	global	IP	Internet	is	the	most	famous	and	successful	implementation
of	an	internet.	It	has	existed	in	one	form	or	another	since	1969.	While	the
internal	architecture	of	the	Internet	is	complex	and	constantly	changing,
the	organization	of	client-server	applications	has	remained	remarkably
stable	since	the	early	1980s.	
Figure	
11.8
shows	the	basic	hardware
and	software	organization	of	an	Internet	client-server	application.
Each	Internet	host	runs	software	that	implements	the	
TCP/IP
protocol
(
Transmission	Control	Protocol/Internet	Protocol
),	which	is	supported	by
almost	every	modern	computer	system.	Internet	clients	and	servers
communicate	using	a	mix	of	
sockets	interface
functions	and	Unix	I/O
functions.	(We	will	describe	the	sockets	interface	in	
Section	
11.4
)	The
sockets	functions	are	typically	implemented	as	system	calls	that	trap	into
the	kernel	and	call	various	kernel-mode	functions	in	TCP/IP.
TCP/IP	is	actually	a	family	of	protocols,	each	of	which	contributes
different	capabilities.	For	example,	IP	provides	the	basic	naming	scheme
and	a	delivery	mechanism	that	can	send	packets,	known	as	
datagrams
,
from	one	Internet	host	to	any	other	host.	The	IP	mechanism	is	unreliable
in	the	sense	that	it	makes	no	effort	to	recover	if	datagrams	are	lost	or
duplicated	in	the	network.	UDP	(Unreliable	Datagram	Protocol)	extends
IP	slightly,	so	that	datagrams	can	be	transferred	from	process	to	process,
rather	than	host	to	host.	TCP	is	a	complex	protocol	that	builds	on	IP	to
provide	reliable	full	duplex	(bidirectional)	
connections
between
processes.	To	simplify	our	discussion,	we	will	treat	TCP/IP	as	a	single</p>
<p>monolithic	protocol.	We	will	not	discuss	its	inner	workings,	and	we	will
only	discuss	some	of	the	basic	capabilities	that	TCP	and	IP	provide	to
application	programs.	We	will	not	discuss	UDP.
From	a	programmer's	perspective,	we	can	think	of	the	Internet	as	a
worldwide	collection	of	hosts	with	the	following	properties:
The	set	of	hosts	is	mapped	to	a	set	of	32-bit	
IP	addresses.
Aside	
IPv4	and	IPv6
The	original	Internet	protocol,	with	its	32-bit	addresses,	is
known	as	Internet	Protocol	Version	4	(IPv4).	In	1996,	the
Internet	Engineering	Task	Force	(IETF)	proposed	a	new
version	of	IP,	called	Internet	Protocol	Version	6	(IPv6),	that
uses	128-bit	addresses	and	that	was	intended	as	the
successor	to	IPv4.	However,	as	of	2015,	almost	20	years	later,
the	vast	majority	of	Internet	traffic	is	still	carried	by	IPv4
networks.	For	example,	only	4	percent	of	users	access	Google
services	using	IPv6	
[42]
.
Because	of	its	low	adoption	rate,	we	will	not	discuss	IPv6	in
any	detail	in	this	book	and	will	focus	exclusively	on	the
concepts	behind	IPv4.	When	we	talk	about	the	Internet,	what
we	mean	is	the	Internet	based	on	IPv4.	Nonetheless,	the
techniques	for	writing	clients	and	servers	that	we	will	teach	you
later	in	this	chapter	are	based	on	modern	interfaces	that	are
independent	of	any	particular	protocol.
The	set	of	IP	addresses	is	mapped	to	a	set	of	identifiers	called
Internet	domain	names.</p>
<p>A	process	on	one	Internet	host	can	communicate	with	a	process	on
any	other	Internet	host	over	a	
connection
.
The	following	sections	discuss	these	fundamental	Internet	ideas	in	more
detail.
11.3.1	
IP	Addresses
An	IP	address	is	an	unsigned	32-bit	integer.	Network	programs	store	IP
addresses	in	the	
IP	address	structure
shown	in	
Figure	
11.9
.
Storing	a	scalar	address	in	a	structure	is	an	unfortunate	artifact	from	the
early	implementations	of	the	sockets	interface.	It	would	make	more	sense
to	define	a	scalar	type	for	IP	addresses,	but	it	is	too	late	to	change	now
because	of	the	enormous	installed	base	of	applications.
Because	Internet	hosts	can	have	different	host	byte	orders,	TCP/IP
defines	a	uniform	
network	byte	order
(big-endian	byte	order)	for	any
integer	data	item,	such	as	an	IP	address,	that	is	carried	across	the
network	in	a	packet	header.	Addresses	in	IP	address	structures	are
always	stored	in	(big-endian)	network	byte	order,	even	if	the	host	byte
order	is	little-endian.	Unix	provides	the	following	functions	for	converting
between	network	and	host	byte	order.</p>
<p>Figure	
11.9	
IP	address	structure.
The	
function	converts	an	unsigned	32-bit	integer	from	host	byte
order	to	network	byte	order.	The	
function	converts	an	unsigned	32-
bit	integer	from	network	byte	order	to	host	byte	order.	The	
and
functions	perform	corresponding	conversions	for	unsigned	16-bit
integers.	Note	that	there	are	no	equivalent	functions	for	manipulating	64-
bit	values.
IP	addresses	are	typically	presented	to	humans	in	a	form	known	as
dotted-decimal	notation
,	where	each	byte	is	represented	by	its	decimal
value	and	separated	from	the	other	bytes	by	a	period.	For	example,</p>
<pre><code>is	the	dotted-decimal	representation	of	the	address
</code></pre>
<p>.	On	Linux	systems,	you	can	use	the	
HOSTNAME</p>
<p>command	to
determine	the	dotted-decimal	address	of	your	own	host:
Application	programs	can	convert	back	and	forth	between	IP	addresses
and	dotted-decimal	strings	using	the	functions	
and	
In	these	function	names,	the	
stands	for	
network
and	the	
stands
for	
presentation
.	They	can	manipulate	either	32-bit	IPv4	addresses
(
),	as	shown	here,	or	128-bit	IPv6	addresses	(
),	which
we	do	not	cover.</p>
<p>The	
function	converts	a	dotted-decimal	string	(
)	to	a	binary
IP	address	in	network	byte	order	(
).If	
does	not	point	to	a	valid
dotted-decimal	string,	then	it	returns	0.	Any	other	error	returns	−1	and
sets	
.	Similarly,	the	
function	converts	a	binary	IP	address
in	network	byte	order	(
)	to	the	corresponding	dotted-decimal
representation	and	copies	at	most	
bytes	of	the	resulting	null-
terminated	string	to	
.
Practice	Problem	
11.1	
(solution	page	
966
)
Complete	the	following	table:
Hex	address
Dotted-decimal	address</p>
<hr />
<hr />
<hr />
<hr />
<hr />
<hr />
<p>Practice	Problem	
11.2	
(solution	page	
967
)
Write	a	program	
that	converts	its	hex	argument	to	a
dotted-decimal	string	and	prints	the	result.	For	example,</p>
<p>Practice	Problem	
11.3	
(solution	page	
967
)
Write	a	program	
that	converts	its	dotted-decimal
argument	to	a	hex	number	and	prints	the	result.	For	example,
11.3.2	
Internet	Domain	Names
Internet	clients	and	servers	use	IP	addresses	when	they	communicate
with	each	other.	However,	large	integers	are	difficult	for	people	to
remember,	so	the	Internet	also	defines	a	separate	set	of	more	human-
friendly	
domain	names
,	as	well	as	a	mechanism	that	maps	the	set	of
domain	names	to	the	set	of	IP	addresses.	A	domain	name	is	a	sequence
of	words	(letters,	numbers,	and	dashes)	separated	by	periods,	such	as
.
The	set	of	domain	names	forms	a	hierarchy,	and	each	domain	name
encodes	its	position	in	the	hierarchy.	An	example	is	the	easiest	way	to</p>
<p>understand	this.	
Figure	
11.10
shows	a	portion	of	the	domain	name
hierarchy.
The	hierarchy	is	represented	as	a	tree.	The	nodes	of	the	tree	represent
domain	names	that	are	formed	by	the	path	back	to	the	root.	Subtrees	are
referred	to	as	
sub-domains
.	The	first	level	in	the	hierarchy	is	an	unnamed
root	node.	The	next	level	is	a	collection	of	
first-level	domain	names
that
are	defined	by	a	nonprofit	organization	called	ICANN	(Internet
Corporation	for	Assigned	Names	and	Numbers).	Common	first-level
domains	include	
,	and	
.
Figure	
11.10	
Subset	of	the	Internet	domain	name	hierarchy.
At	the	next	level	are	
second-level
domain	names	such	as	
,	which
are	assigned	on	a	first-come	first-serve	basis	by	various	authorized
agents	of	ICANN.	Once	an	organization	has	received	a	second-level</p>
<p>domain	name,	then	it	is	free	to	create	any	other	new	domain	name	within
its	subdomain,	such	as	
The	Internet	defines	a	mapping	between	the	set	of	domain	names	and
the	set	of	IP	addresses.	Until	1988,	this	mapping	was	maintained
manually	in	a	single	text	file	called	
Since	then,	the	mapping
has	been	maintained	in	a	distributed	worldwide	database	known	as	
DNS
(Domain	Name	System)
.	Conceptually,	the	DNS	database	consists	of
millions	of	
host	entries
,	each	of	which	defines	the	mapping	between	a	set
of	domain	names	and	a	set	of	IP	addresses.	In	a	mathematical	sense,
think	of	each	host	entry	as	an	equivalence	class	of	domain	names	and	IP
addresses.	We	can	explore	some	of	the	properties	of	the	DNS	mappings
with	the	Linux	
NSLOOKUP</p>
<p>program,	which	displays	the	IP	addresses
associated	with	a	domain	name.</p>
<ol>
<li></li>
</ol>
<p>We've	reformatted	the	output	of	
NSLOOKUP</p>
<p>to	improve	readability.
Each	Internet	host	has	the	locally	defined	domain	name	
,	which
always	maps	to	the	
loopback	address</p>
<p>:
The	
name	provides	a	convenient	and	portable	way	to
reference	clients	and	servers	that	are	running	on	the	same	machine,
which	can	be	especially	useful	
for	debugging.	We	can	use	
HOSTNAME</p>
<p>to
determine	the	real	domain	name	of	our	local	host:
1</p>
<p>In	the	simplest	case,	there	is	a	one-to-one	mapping	between	a	domain
name	and	an	IP	address:
However,	in	some	cases,	multiple	domain	names	are	mapped	to	the
same	IP	address:
In	the	most	general	case,	multiple	domain	names	are	mapped	to	the
same	set	of	multiple	IP	addresses:</p>
<p>Finally,	we	notice	that	some	valid	domain	names	are	not	mapped	to	any
IP	address:
11.3.3	
Internet	Connections
Internet	clients	and	servers	communicate	by	sending	and	receiving
streams	of	bytes	over	
connections
.	A	connection	is	
point-to-point
in	the</p>
<p>sense	that	it	connects	a	pair	of	processes.	It	is	
full	duplex
in	the	sense
that	data	can	flow	in	both	directions
Aside	
How	many	Internet	hosts	are
there?
Twice	a	year	since	1987,	the	Internet	Systems	Consortium
conducts	the	
Internet	Domain	Survey.
The	survey,	which
estimates	the	number	of	Internet	hosts	by	counting	the	number	of
IP	addresses	that	have	been	assigned	a	domain	name,	reveals	an
amazing	trend.	Since	1987,	when	there	were	about	20,000
Internet	hosts,	the	number	of	hosts	has	been	increasing
exponentially.	By	2015,	there	were	over	1,000,000,000	Internet
hosts!
at	the	same	time.	And	it	is	
reliable
in	the	sense	that—barring	some
catastrophic	failure	such	as	a	cable	cut	by	the	proverbial	careless
backhoe	operator—the	stream	of	bytes	sent	by	the	source	process	is
eventually	received	by	the	destination	process	in	the	same	order	it	was
sent.
A	
socket
is	an	end	point	of	a	connection.	Each	socket	has	a
corresponding	
socket	address
that	consists	of	an	Internet	address	and	a
16-bit	integer	
port
and	is	denoted	by	the	notation	
2.	
These	software	ports	have	no	relation	to	the	hardware	ports	in	network	switches	and	routers.
The	port	in	the	client's	socket	address	is	assigned	automatically	by	the
kernel	when	the	client	makes	a	connection	request	and	is	known	as	an
ephemeral	port.
However,	the	port	in	the	server's	socket	address	is
2</p>
<p>typically	some	
well-known	port
that	is	permanently	associated	with	the
service.	For	example,	Web	servers	typically	use	port	80,	and	email
servers	use	port	25.	Associated	with	each	service	with	a	well-known	port
is	a	corresponding	
well-known	service	name.
For	example,	the	well-
known	name	for	the	Web	service	is	
,	and	the	well-known	name	for
email	is	
.	The	mapping	between	well-known	names	and	well-known
ports	is	contained	in	a	file	called	
A	connection	is	uniquely	identified	by	the	socket	addresses	of	its	two	end
points.	This	pair	of	socket	addresses	is	known	as	a	
socket	pair
and	is
denoted	by	the	tuple
where	
cliaddr
is	the	client's	IP	address,	
cliport
is	the	client's	port,
servaddr
is	the	server's	IP	address,	and	
servport
is	the	server's	port.	For
example,	
Figure	
11.11
shows	a	connection	between	a	Web	client	and
a	Web	server.
In	this	example,	the	Web	client's	socket	address	is
where	port	
is	an	ephemeral	port	assigned	by	the	kernel.	The	Web
server's	socket	address	is</p>
<p>Aside	
Origins	of	the	Internet
The	Internet	is	one	of	the	most	successful	examples	of
government,	university,	and	industry	partnership.	Many	factors
contributed	to	its	success,	but	we	think	two	are	particularly
important:	a	sustained	30-year	investment	by	the	United	States
government	and	a	commitment	by	passionate	researchers	to	what
Dave	Clarke	at	MIT	has	dubbed	&quot;rough	consensus	and	working
code.&quot;
The	seeds	of	the	Internet	were	sown	in	1957,	when,	at	the	height
of	the	Cold	War,	the	Soviet	Union	shocked	the	world	by	launching
Sputnik,	the	first	artificial	earth	satellite.	In	response,	the	United
States	government	created	the	Advanced	Research	Projects
Agency	(ARPA),	whose	charter	was	to	reestablish	the	US	lead	in
science	and	technology.	In	1967,	Lawrence	Roberts	at	ARPA
published	plans	for	a	new	network	called	the	ARPANET.	The	first
ARPANET	nodes	were	up	and	running	by	1969.	By	1971,	there
were	13	ARPANET	nodes,	and	email	had	emerged	as	the	first
important	network	application.
In	1972,	Robert	Kahn	outlined	the	general	principles	of
internetworking:	a	collection	of	interconnected	networks,	with
communication	between	the	networks	handled	independently	on	a
&quot;best-effort	basis&quot;	by	black	boxes	called	&quot;routers.&quot;	In	1974,	Kahn
and	Vinton	Cerf	published	the	first	details	of	TCP/IP,	which	by
1982	had	become	the	standard	internetworking	protocol	for</p>
<p>ARPANET.	On	January	1,	1983,	every	node	on	the	ARPANET
switched	to	TCP/IP,	marking	the	birth	of	the	global	IP	Internet.
In	1985,	Paul	Mockapetris	invented	DNS,	and	there	were	over
1,000	Internet	hosts.	The	next	year,	the	National	Science
Foundation	(NSF)	built	the	NSFNET	backbone	connecting	13
sites	with	56	Kb/s	phone	lines.	It	was	upgraded	to	1.5	Mb/s	T1
links	in	1988	and	45	Mb/s	T3	links	in	1991.	By	1988,	there	were
more	than	50,000	hosts.	In	1989,	the	original	ARPANET	was
officially	retired.	In	1995,	when	there	were	almost	10,000,000
Internet	hosts,	NSF	retired	NSFNET	and	replaced	it	with	the
modern	Internet	architecture	based	on	private	commercial
backbones	connected	by	public	network	access	points.
Figure	
11.11	
Anatomy	of	an	Internet	connection.
where	port	
is	the	well-known	port	associated	with	Web	services.
Given	these	client	and	server	socket	addresses,	the	connection	between
the	client	and	server	is	uniquely	identified	by	the	socket	pair
Aside	
Origins	of	the	sockets	interface</p>
<p>The	original	sockets	interface	was	developed	by	researchers	at
University	of	California,	Berkeley,	in	the	early	1980s.	For	this
reason,	it	is	often	referred	to	as	
Berkeley	sockets
.	The	Berkeley
researchers	developed	the	sockets	interface	to	work	with	any
underlying	protocol.	The	first	implementation	was	for	TCP/IP,
which	they	included	in	the	Unix	4.2BSD	kernel	and	distributed	to
numerous	universities	and	labs.	This	was	an	important	event	in
Internet	history.	Almost	overnight,	thousands	of	people	had
access	to	TCP/IP	and	its	source	codes.	It	generated	tremendous
excitement	and	sparked	a	flurry	of	new	research	in	networking
and	internetworking.</p>
<p>11.4	
The	Sockets	Interface
The	
sockets	interface
is	a	set	of	functions	that	are	used	in	conjunction
with	the	Unix	I/O	functions	to	build	network	applications.	It	has	been
implemented	on	most	modern	systems,	including	all	Unix	variants	as	well
as	Windows	and	Macintosh	systems.	
Figure	
11.12
gives	an	overview
of	the	sockets	interface	in	the	context	of	a	typical	client-server
transaction.	You	should	use	this	picture	as	a	road	map	when	we	discuss
the	individual	functions.</p>
<p>Figure	
11.12	
Overview	of	network	applications	based	on	the	sockets
interface.
Aside	
What	does	the	
suffix	mean?
The	
suffix	is	short	for	
internet
,	not	
input
.
Figure	
11.13	
Socket	address	structures.
11.4.1	
Socket	Address	Structures</p>
<p>From	the	perspective	of	the	Linux	kernel,	a	socket	is	an	end	point	for
communication.	From	the	perspective	of	a	Linux	program,	a	socket	is	an
open	file	with	a	corresponding	descriptor.
Internet	socket	addresses	are	stored	in	16-byte	structures	having	the
type	
,	shown	in	
Figure	
11.13
.	For	Internet	applications,	the
field	is	AF_INET,	the	
field	is	a	16-bit	port	number,
and	the	
field	contains	a	32-bit	IP	address.	The	IP	address	and
port	number	are	always	stored	in	network	(big-endian)	byte	order.
The	
,	and	
functions	require	a	pointer	to	a	protocol-
specific	socket	address	structure.	The	problem	faced	by	the	designers	of
the	sockets	interface	was	how	to	define	these	functions	to	accept	any
kind	of	socket	address	structure.	Today,	we	would	use	the	generic	
*
pointer,	which	did	not	exist	in	C	at	that	time.	Their	solution	was	to	define
sockets	functions	to	expect	a	pointer	to	a	generic	
structure
(
Figure	
11.13
)	and	then	require	applications	to	cast	any	pointers	to
protocol-specific	structures	to	this	generic	structure.	To	simplify	our	code
examples,	we	follow	Stevens's	lead	and	define	the	following	type:
We	then	use	this	type	whenever	we	need	to	cast	a	
structure
to	a	generic	
structure.
11.4.2	
The	
Function</p>
<p>Clients	and	servers	use	the	
function	to	create	a	
socket	descriptor
.
If	we	wanted	the	socket	to	be	the	end	point	for	a	connection,	then	we
could	call	socket	with	the	following	hardcoded	arguments:
where	AF_INET	indicates	that	we	are	using	32-bit	IP	addresses	and
SOCK_STREAM	indicates	that	the	socket	will	be	an	end	point	for	a
connection.	However,	the	best	practice	is	to	use	the	
function
(
Section	
11.4.7
)	to	generate	these	parameters	automatically,	so	that
the	code	is	protocol-independent.	We	will	show	you	how	to	use
with	the	
function	in	
Section	
11.4.8
.
The	
descriptor	returned	by	
is	only	partially	opened	and
cannot	yet	be	used	for	reading	and	writing.	How	we	finish	opening	the
socket	depends	on	whether	we	are	a	client	or	a	server.	The	next	section
describes	how	we	finish	opening	the	socket	if	we	are	a	client.</p>
<p>11.4.3	
The	
Function
A	client	establishes	a	connection	with	a	server	by	calling	the	
function.
The	
function	attempts	to	establish	an	Internet	connection	with
the	server	at	socket	address	
,	where	
is	
.
The	
function	blocks	until	either	the	connection	is	successfully
established	or	an	error	occurs.	If	successful,	the	
descriptor	is
now	ready	for	reading	and	writing,	and	the	resulting	connection	is
characterized	by	the	socket	pair
where	
is	the	client's	IP	address	and	
is	the	ephemeral	port	that
uniquely	identifies	the	client	process	on	the	client	host.	As	with	
,
the	best	practice	is	to	use	
to	supply	the	arguments	to
(see	
Section	
11.4.8
).</p>
<p>11.4.4	
The	
Function
The	remaining	sockets	functions—
,	and	
—are	used
by	servers	to	establish	connections	with	clients.
The	
function	asks	the	kernel	to	associate	the	server's	socket
address	in	
with	the	socket	descriptor	
.	The	
argument
is	
.	As	with	
and	
,	the	best	practice	is
to	use	
to	supply	the	arguments	to	
(see	
Section
11.4.8
).
11.4.5	
The	
Function
Clients	are	active	entities	that	initiate	connection	requests.	Servers	are
passive	entities	that	wait	for	connection	requests	from	clients.	By	default,
the	kernel	assumes	that	a	descriptor	created	by	the	
function
corresponds	to	an	
active	socket
that	will	live	on	the	client	end	of	a
connection.	A	server	calls	the	
function	to	tell	the	kernel	that	the
descriptor	will	be	used	by	a	server	instead	of	a	client.</p>
<p>The	
function	converts	
from	an	active	socket	to	a	
listening
socket
that	can	accept	connection	requests	from	clients.	The	
argument	is	a	hint	about	the	number	of	outstanding	connection	requests
that	the	kernel	should	queue	up	before	it	starts	to	refuse	requests.	The
exact	meaning	of	the	
argument	requires	an	understanding	of
TCP/IP	that	is	beyond	our	scope.	We	will	typically	set	it	to	a	large	value,
such	as	1,024.
Figure	
11.14	
The	roles	of	the	listening	and	connected	descriptors.</p>
<p>11.4.6	
The	
Function
Servers	wait	for	connection	requests	from	clients	by	calling	the	
function.
The	
function	waits	for	a	connection	request	from	a	client	to	arrive
on	the	listening	descriptor	
,	then	fills	in	the	client's	socket
address	in	
,	and	returns	a	
connected	descriptor
that	can	be	used	to
communicate	with	the	client	using	Unix	I/O	functions.
The	distinction	between	a	listening	descriptor	and	a	connected	descriptor
confuses	many	students.	The	listening	descriptor	serves	as	an	end	point
for	client	connection	requests.	It	is	typically	created	once	and	exists	for
the	lifetime	of	the	server.	The	connected	descriptor	is	the	end	point	of	the
connection	that	is	established	between	the	client	and	the	server.	It	is
created	each	time	the	server	accepts	a	connection	request	and	exists
only	as	long	as	it	takes	the	server	to	service	a	client.
Figure	
11.14
outlines	the	roles	of	the	listening	and	connected
descriptors.	In	step	1,	the	server	calls	
,	which	waits	for	a
connection	request	to	arrive	on	the	listening	descriptor,	which	for</p>
<p>concreteness	we	will	assume	is	descriptor	3.	Recall	that	descriptors	0−2
are	reserved	for	the	standard	files.
In	step	2,	the	client	calls	the	
function,	which	sends	a	connection
request	to	
.	In	step	3,	the	
function	opens	a	new
connected	descriptor	
(which	we	will	assume	is	descriptor	4),
establishes	the	connection	between	
and	
,	and	then
returns	
to	the	application.	The
Aside	
Why	the	distinction	between
listening	and	connected	descriptors?
You	might	wonder	why	the	sockets	interface	makes	a	distinction
between	listening	and	connected	descriptors.	At	first	glance,	it
appears	to	be	an	unnecessary	complication.	However,
distinguishing	between	the	two	turns	out	to	be	quite	useful,
because	it	allows	us	to	build	concurrent	servers	that	can	process
many	client	connections	simultaneously.	For	example,	each	time	a
connection	request	arrives	on	the	listening	descriptor,	we	might
fork	a	new	process	that	communicates	with	the	client	over	its
connected	descriptor.	You'll	learn	more	about	concurrent	servers
in	
Chapter	
12
.
client	also	returns	from	the	
,	and	from	this	point,	the	client	and
server	can	pass	data	back	and	forth	by	reading	and	writing	
and
,	respectively.</p>
<p>11.4.7	
Host	and	Service	Conversion
Linux	provides	some	powerful	functions,	called	
and
,	for	converting	back	and	forth	between	binary	socket
address	structures	and	the	string	representations	of	hostnames,	host
addresses,	service	names,	and	port	numbers.	When	used	in	conjunction
with	the	sockets	interface,	they	allow	us	to	write	network	programs	that
are	independent	of	any	particular	version	of	the	IP	protocol.
The	
Function
The	
function	converts	string	representations	of	hostnames,
host	addresses,	service	names,	and	port	numbers	into	socket	address
structures.	It	is	the	modern	replacement	for	the	obsolete	
and	
functions.	Unlike	these	functions,	it	is	reentrant	(see
Section	
12.7.2
)	and	works	with	any	protocol.</p>
<p>Figure	
11.15	
Data	structure	returned	by	
.
Given	
and	
(the	two	components	of	a	socket	address),
returns	a	
that	points	to	a	linked	list	of	
structures,	each	of	which	points	to	a	socket	address	structure	that
corresponds	to	
and	
(
Figure	
11.15
).
After	a	client	calls	
,	it	walks	this	list,	trying	each	socket
address	in	turn	until	the	calls	to	
and	
succeed	and	the</p>
<p>connection	is	established.	Similarly,	a	server	tries	each	socket	address
on	the	list	until	the	calls	to	
and	
succeed	and	the	descriptor	is
bound	to	a	valid	socket	address.	To	avoid	memory	leaks,	the	application
must	eventually	free	the	list	by	calling	
.	If	
returns	a	nonzero	error	code,	the	application	can	call	
to
convert	the	code	to	a	message	string.
The	
argument	to	
can	be	either	a	domain	name	or	a
numeric	address	(e.g.,	a	dotted-decimal	IP	address).	The	
argument	can	be	either	a	service	name	
or	a	decimal	port
number.	If	we	are	not	interested	in	converting	the	hostname	to	an
address,	we	can	set	
to	NULL.	The	same	holds	for	
.
However,	at	least	one	of	them	must	be	specified.
The	optional	
argument	is	an	
structure	(
Figure	
11.16
)
that	provides	finer	control	over	the	list	of	socket	addresses	that
returns.	When	passed	as	a	hints	argument,	only	the
,	and	
fields	can	be	set.	The
other	fields	must	be	set	to	zero	(or	NULL).	In	practice,	we	use	
to
zero	the	entire	structure	and	then	set	a	few	selected	fields:
By	default,	
can	return	both	IPv4	and	IPv6	socket
addresses.	Setting	
to	AF_INET	restricts	the	list	to	IPv4
addresses.	Setting	it	to	AF_INET6	restricts	the	list	to	IPv6	addresses.</p>
<p>Figure	
11.16	
The	
structure	used	by	
.
By	default,	for	each	unique	address	associated	with	host,	the
function	can	return	up	to	three	
structures,	each
with	a	different	
field:	one	for	connections,	one	for
datagrams	(not	covered),	and	one	for	raw	sockets	(not	covered).
Setting	
to	SOCK_STREAM	restricts	the	list	to	at	most
one	
structure	for	each	unique	address,	one	whose	socket
address	can	be	used	as	the	end	point	of	a	connection.	This	is	the
desired	behavior	for	all	of	our	example	programs.
The	
field	is	a	bit	mask	that	further	modifies	the	default
behavior.	You	create	it	by	
OR
ing	combinations	of	various	values.	Here
are	some	that	we	find	useful:</p>
<p>AI_ADDRCONFIG.	This	flag	is	recommended	if	you	are	using
connections	
[34]
.	It	asks	
to	return	IPv4	addresses
only	if	the	local	host	is	configured	for	IPv4.	Similarly	for	IPv6.
AI_CANONNAME.	By	default,	the	
field	is	NULL.	If
this	flag	is	set,	it	instructs	
to	point	the	
field	in	the	first	
structure	in	the	list	to	the	canonical
(official)	name	of	
(see	
Figure	
11.15
).
AI_NUMERICSERV.	By	default,	the	
argument	can	be	a
service	name	or	a	port	number.	This	flag	forces	the	
argument	to	be	a	port	number.
AI_PASSIVE.	By	default,	
returns	socket	addresses
that	can	be	used	by	clients	as	active	sockets	in	calls	to	
.
This	flag	instructs	it	to	return	socket	addresses	that	can	be	used
by	servers	as	listening	sockets.	In	this	case,	the	
argument
should	be	NULL.	The	address	field	in	the	resulting	socket	address
structure(s)	will	be	the	
wildcard	address
,	which	tells	the	kernel	that
this	server	will	accept	requests	to	any	of	the	IP	addresses	for	this
host.	This	is	the	desired	behavior	for	all	of	our	example	servers.
When	
creates	an	
structure	in	the	output	list,	it	fills
in	each	field	except	for	
.	The	
field	points	to	a	socket
address	structure,	the	
field	gives	the	size	of	this	socket
address	structure,	and	the	
field	points	to	the	next	
structure	in	the	list.	The	other	fields	describe	various	attributes	of	the
socket	address.
One	of	the	elegant	aspects	of	
is	that	the	fields	in	an	</p>
<p>structure	are	opaque,	in	the	sense	that	they	can	be	passed	directly	to	the
functions	in	the	sockets	interface	without	any	further	manipulation	by	the
application	code.	For	example,	
,	and	
can	be	passed	directly	to	socket.	Similarly,	
and	
can
be	passed	directly	to	
and	
.	This	powerful	property	allows	us
to	write	clients	and	servers	that	are	independent	of	any	particular	version
of	the	IP	protocol.
The	
Function
The	
function	is	the	inverse	of	
.	It	converts	a
socket	address	structure	to	the	corresponding	host	and	service	name
strings.	It	is	the	modern	replacement	for	the	obsolete	
and
functions,	and	unlike	those	functions,	it	is	reentrant	and
protocol-independent.</p>
<p>The	
argument	points	to	a	socket	address	structure	of	size	
bytes,	
to	a	buffer	of	size	
bytes,	and	
to	a	buffer	of</p>
<div style="break-before: page; page-break-before: always;"></div><p>size	
bytes.	The	
function	converts	the	socket	address
structure	
to	the	corresponding	
and	
name	strings	and
copies	them	to	the	
and	
buffers.	If	
returns	a
nonzero	error	code,	the	application	can	convert	it	to	a	string	by	calling
.
If	we	don't	want	the	hostname,	we	can	
to	NULL	and	
to
zero.	The	same	holds	for	the	service	fields.	However,	one	or	the	other
must	be	set.
The	
argument	is	a	bit	mask	that	modifies	the	default	behavior.	You
create	it	by	
OR
ing	combinations	of	various	values.	Here	are	a	couple	of
useful	ones:
NI_NUMERICHOST.	By	default,	
tries	to	return	a	domain
name	in	
.	Setting	this	flag	will	cause	it	to	return	a	numeric
address	string	instead.
NI_NUMERICSERV.	By	default,	
will	look	in	
and	if	possible,	return	a	service	name	instead	of	a	port	number.
Setting	this	flag	forces	it	to	skip	the	lookup	and	simply	return	the	port
number.</p>
<p>Figure	
11.17	
H
OSTINFO
displays	the	mapping	of	a	domain	name	to	its
associated	IP	addresses.
Figure	
11.17
shows	a	simple	program,	called	
HOSTINFO
,	that	uses
and	
to	display	the	mapping	of	a	domain	name	to
its	associated	IP	addresses.	It	is	similar	to	the	
NSLOOKUP</p>
<p>program	from
Section	
11.3.2
.
First,	we	initialize	the	
structure	so	that	
returns	the
addresses	we	want.	In	this	case,	we	are	looking	for	32-bit	IP	addresses
(line	16)	
that	can	be	used	as	end	points	of	connections	(line	17).	Since
we	are	only	asking	
to	convert	domain	names,	we	call	it	with
a	NULL	
argument.
After	the	call	to	
,	we	walk	the	list	of	
structures,	using
to	convert	each	socket	address	to	a	dotted-decimal	address
string.	After	walking	the	list,	we	are	careful	to	free	it	by	calling
(although	for	this	simple	program	it	is	not	strictly
necessary).</p>
<p>When	we	run	
HOSTINFO
,	we	see	that	
maps	to	four	IP
addresses,	which	is	what	we	saw	using	
NSLOOKUP</p>
<p>in	
Section	
11.3.2
.
Practice	Problem	
11.4	
(solution	page	
968
)
The	
and	
functions	subsume	the
functionality	of	
and	
,	respectively,	and	they
provide	a	higher-level	of	abstraction	that	is	independent	of	any
particular	address	format.	To	convince	yourself	how	handy	this	is,
write	a	version	of	
HOSTINFO</p>
<p>(
Figure	
11.17
)	that	uses	
instead	of	
to	convert	each	socket	address	to	a	dotted-
decimal	address	string.
11.4.8	
Helper	Functions	for	the
Sockets	Interface
The	
function	and	the	sockets	interface	can	seem	somewhat
daunting	when	you	first	learn	about	them.	We	find	it	convenient	to	wrap</p>
<p>them	with	higher-level	helper	functions,	called	
and
,	that	clients	and	servers	can	use	when	they	want	to
communicate	with	each	other.
The	
Function
A	client	establishes	a	connection	with	a	server	by	calling	
.
The	
function	establishes	a	connection	with	a	server
running	on	host	
and	listening	for	connection	requests	on	port
number	port.	It	returns	an	open	socket	descriptor	that	is	ready	for	input
and	output	using	the	Unix	I/O	functions.	
Figure	
11.18
shows	the	code
for	
We	call	
,	which	returns	a	list	of	
structures,	each	of
which	points	to	a	socket	address	structure	that	is	suitable	for	establishing
a	connection</p>
<p>Figure	
11.18	
:	Helper	function	that	establishes	a
connection	with	a	server.
It	is	reentrant	and	protocol-independent.
with	a	server	running	on	
and	listening	on	
.	We	then	walk
the	list,	trying	each	list	entry	in	turn,	until	the	calls	to	
and	
succeed.	If	the	
fails,	we	are	careful	to	close	the	socket	descriptor
before	trying	the	next	entry.	If	the	
succeeds,	we	free	the	list
memory	and	return	the	socket	descriptor	to	the	client,	which	can
immediately	begin	using	Unix	I/O	to	communicate	with	the	server.
Notice	how	there	is	no	dependence	on	any	particular	version	of	IP
anywhere	in	the	code.	The	arguments	to	
and	
are
generated	for	us	automatically	by	
,	which	allows	our	code	to
be	clean	and	portable.
The	
Function
A	server	creates	a	listening	descriptor	that	is	ready	to	receive	connection
requests	by	calling	the	
function.</p>
<p>The	
function	returns	a	listening	descriptor	that	is	ready	to
receive	connection	requests	on	port	
.	
Figure	
11.19
shows	the
code	for	
.
The	style	is	similar	to	
.	We	call	
and	then	walk
the	resulting	list	until	the	calls	to	
and	
succeed.	Note	that	in
line	20	we	use	the	
function	(not	described	here)	to	configure
the	server	so	that	it	can	be	terminated,	be	restarted,	and	begin	accepting
connection	requests	immediately.	By	default,	a	restarted	server	will	deny
connection	requests	from	clients	for	approximately	30	seconds,	which
seriously	hinders	debugging.
Since	we	have	called	
with	the	AI_PASSIVE	flag	and	a	NULL
argument,	the	address	field	in	each	socket	address	structure	is	set
to	the	wildcard	address,	which	tells	the	kernel	that	this	server	will	accept
requests	to	any	of	the	IP	addresses	for	this	host.
Finally,	we	call	the	
function	to	convert	
to	a	listening
descriptor	and	return	it	to	the	caller.	If	the	
fails,	we	are	careful	to
avoid	a	memory	leak	by	closing	the	descriptor	before	returning.</p>
<p>11.4.9	
Example	Echo	Client	and
Server
The	best	way	to	learn	the	sockets	interface	is	to	study	example	code.
Figure	
11.20
shows	the	code	for	an	echo	client.	After	establishing	a
connection	with	the	server,	the	client	enters	a	loop	that	repeatedly	reads
a	text	line	from	standard	input,	sends	the	text	line	to	the	server,	reads	the
echo	line	from	the	server,	and	prints	the	result	to	standard	output.	The
loop	terminates	when	
encounters	EOF	on	standard	input,	either
because	the	user	typed	Ctrl+D	at	the	keyboard	or	because	it	has
exhausted	the	text	lines	in	a	redirected	input	file.
After	the	loop	terminates,	the	client	closes	the	descriptor.	This	results	in
an	EOF	notification	being	sent	to	the	server,	which	it	detects	when	it
receives	a	return	code	of	zero	from	its	
function.	After
closing	its	descriptor,	the	client	terminates.	Since	the	client's	kernel
automatically	closes	all	open	descriptors	when	a	process	terminates,	the
in	line	24	is	not	necessary.	However,	it	is	good	programming
practice	to	explicitly	close	any	descriptors	that	you	have	opened.
Figure	
11.21
shows	the	main	routine	for	the	echo	server.	After
opening	the	listening	descriptor,	it	enters	an	infinite	loop.	Each	iteration
waits	for	a	connection	request	from	a	client,	prints	the	domain	name	and
port	of	the	connected	client,	and	then	calls	the	
function	that	services
the	client.	After	the	echo	routine	returns,</p>
<p>Figure	
11.19	
:	Helper	function	that	opens	and	returns	a
listening	descriptor.
It	is	reentrant	and	protocol-independent.</p>
<p>Figure	
11.20	
Echo	client	main	routine.
the	main	routine	closes	the	connected	descriptor.	Once	the	client	and
server	have	closed	their	respective	descriptors,	the	connection	is
terminated.
The	
variable	in	line	9	is	a	socket	address	structure	that	is
passed	to	
.	Before	
returns,	it	fills	in	
with	the
socket	address	of	the	client	on	the	other	end	of	the	connection.	Notice
how	we	declare	
as	type	struct	
rather	than
struct	
.	By	definition,	the	
structure	is	large
enough	to	hold	any	type	of	socket	address,	which	keeps	the	code
protocol-independent.
Notice	that	our	simple	echo	server	can	only	handle	one	client	at	a	time.	A
server	of	this	type	that	iterates	through	clients,	one	at	a	time,	is	called	an
iterative	server
.	In	
Chapter	
12
,	we	will	learn	how	to	build	more
sophisticated	
concurrent	servers
that	can	handle	multiple	clients
simultaneously.
Finally,	
Figure	
11.22
shows	the	code	for	the	
routine,	which
repeatedly	reads	and	writes	lines	of	text	until	the	
function
encounters	EOF	in	line	10.</p>
<p>Figure	
11.21	
Iterative	echo	server	main	routine.
Figure	
11.22	
function	that	reads	and	echoes	text	lines.
Aside	
What	does	EOF	on	a	connection
mean?
The	idea	of	EOF	is	often	confusing	to	students,	especially	in	the
context	of	Internet	connections.	First,	we	need	to	understand	that</p>
<p>there	is	no	such	thing	as	an	EOF	character.	Rather,	EOF	is	a
condition	that	is	detected	by	the	kernel.	An	application	finds	out
about	the	EOF	condition	when	it	receives	a	zero	return	code	from
the	read	function.	For	disk	files,	EOF	occurs	when	the	current	file
position	exceeds	the	file	length.	For	Internet	connections,	EOF
occurs	when	a	process	closes	its	end	of	the	connection.	The
process	at	the	other	end	of	the	connection	detects	the	EOF	when
it	attempts	to	read	past	the	last	byte	in	the	stream.</p>
<p>11.5	
Web	Servers
So	far	we	have	discussed	network	programming	in	the	context	of	a
simple	echo	server.	In	this	section,	we	will	show	you	how	to	use	the	basic
ideas	of	network	programming	to	build	your	own	small,	but	quite
functional,	Web	server.
11.5.1	
Web	Basics
Web	clients	and	servers	interact	using	a	text-based	application-level
protocol	known	as	
HTTP	(hypertext	transfer	protocol).
HTTP	is	a	simple
protocol.	A	Web	client	(known	as	a	
browser)
opens	an	Internet
connection	to	a	server	and	requests	some	
content.
The	server	responds
with	the	requested	content	and	then	closes	the	connection.	The	browser
reads	the	content	and	displays	it	on	the	screen.
What	distinguishes	Web	services	from	conventional	file	retrieval	services
such	as	FTP?	The	main	difference	is	that	Web	content	can	be	written	in	a
language	known	as	
HTML	(hypertext	markup	language).
An	HTML
program	(page)	contains	instructions	(tags)	that	tell	the	browser	how	to
display	various	text	and	graphical	objects	in	the	page.	For	example,	the
code</p>
<p>tells	the	browser	to	print	the	text	between	the	
and	
tags	in	boldface	type.
However,	the	real	power	of	HTML	is	that	a	page	can	contain	pointers
(hyperlinks)	to	content	stored	on	any	Internet	host.	For	example,	an
HTML	line	of	the	form
tells	the	browser	to	highlight	the	text	object	
and	to	create
a	hyperlink	to	an	HTML	file	called	
that	is	stored	on	the	CMU
Web	server.	If	the	user	clicks	on	the	highlighted	text	object,	the	browser
requests	the	corresponding	HTML	file	from	the	CMU	server	and	displays
it.
Aside	
Origins	of	the	World	Wide	Web
The	World	Wide	Web	was	invented	by	Tim	Berners-Lee,	a
software	engineer	working	at	CERN,	a	Swiss	physics	lab.	In	1989,
Berners-Lee	wrote	an	internal	memo	proposing	a	distributed
hypertext	system	that	would	connect	a	&quot;web	of	notes	with	links.&quot;
The	intent	of	the	proposed	system	was	to	help	CERN	scientists
share	and	manage	information.	Over	the	next	two	years,	after
Berners-Lee	implemented	the	first	Web	server	and	Web	browser,
the	Web	developed	a	small	following	within	CERN	and	a	few	other
sites.	A	pivotal	event	occurred	in	1993,	when	Marc	Andreesen
(who	later	founded	Netscape	and	Andreessen	Horowitz)	and	his
colleagues	at	NCSA	released	a	graphical	browser	called	
MOSAIC</p>
<p>for
all	three	major	platforms:	Linux,	Windows,	and	Macintosh.	After
the	release	of	
MOSAIC
,	interest	in	the	Web	exploded,	with	the</p>
<p>number	of	Web	sites	increasing	at	an	exponential	rate.	By	2015,
there	were	over	975,000,000	sites	worldwide.
(
Source:
Netcraft	Web	Survey)
MIME	type
Description
HTML	page
Unformatted	text
Postscript	document
Binary	image	encoded	in	GIF	format
Binary	image	encoded	in	PNG	format
Binary	image	encoded	in	JPEG	format
Figure	
11.23	
Example	MIME	types.
11.5.2	
Web	Content
To	Web	clients	and	servers,	
content
is	a	sequence	of	bytes	with	an
associated	
MIME	(multipurpose	internet	mail	extensions)
type.	
Figure
11.23
shows	some	common	MIME	types.
Web	servers	provide	content	to	clients	in	two	different	ways:
Fetch	a	disk	file	and	return	its	contents	to	the	client.	The	disk	file	is
known	as	
static	content
and	the	process	of	returning	the	file	to	the
client	is	known	as	
serving	static	content
.</p>
<p>Run	an	executable	file	and	return	its	output	to	the	client.	The	output
produced	by	the	executable	at	run	time	is	known	as	
dynamic	content
,
and	the	process	of	running	the	program	and	returning	its	output	to	the
client	is	known	as	
serving	dynamic	content
.
Every	piece	of	content	returned	by	a	Web	server	is	associated	with	some
file	that	it	manages.	Each	of	these	files	has	a	unique	name	known	as	a
URL	(universal	resource	locator)
.	For	example,	the	URL
identifies	an	HTML	file	called	
on	Internet	host	
that	is	managed	by	a	Web	server	listening	on	port	80.	The	port	number	is
optional	and	defaults	to	the	well-known	HTTP	port	80.	URLs	for
executable	files	can	include	program	arguments	after	the	filename.	A	<code>?' character	separates	the	filename	from	the	arguments,	and	each argument	is	separated	by	an	</code>&amp;'	character.	For	example,	the	URL
identifies	an	executable	called	
that	will	be	called	with	two
argument	strings:	15000	and	213.	Clients	and	servers	use	different	parts
of	the	URL	during	a	transaction.	For	instance,	a	client	uses	the	prefix
http:/
/
www.google.com:80</p>
<p>to	determine	what	kind	of	server	to	contact,	where	the	server	is,	and	what
port	it	is	listening	on.	The	server	uses	the	suffix
to	find	the	file	on	its	filesystem	and	to	determine	whether	the	request	is
for	static	or	dynamic	content.
There	are	several	points	to	understand	about	how	servers	interpret	the
suffix	of	a	URL:
There	are	no	standard	rules	for	determining	whether	a	URL	refers	to
static	or	dynamic	content.	Each	server	has	its	own	rules	for	the	files	it
manages.	A	classic	(old-fashioned)	approach	is	to	identify	a	set	of
directories,	such	as	
,	where	all	executables	must	reside.
The	initial	<code>/'	in	the	suffix	does	 not denote	the	Linux	root	directory. Rather,	it	denotes	the	home	directory	for	whatever	kind	of	content	is being	requested.	For	example,	a	server	might	be	configured	so	that all	static	content	is	stored	in	directory	 and	all	dynamic content	is	stored	in	directory	 . The	minimal	URL	suffix	is	the	</code>/'	character,	which	all	servers	expand
to	some	default	home	page	such	as	
.	This	explains	why	it
is	possible	to	fetch	the	home	page	of	a	site	by	simply	typing	a	domain
name	to	the	browser.	The	browser	appends	the	missing	<code>/'	to	the	URL and	passes	it	to	the	server,	which	expands	the	</code>/'	to	some	default
filename.</p>
<p>11.5.3	
HTTP	Transactions
Since	HTTP	is	based	on	text	lines	transmitted	over	Internet	connections,
we	can	use	the	Linux	
TELNET</p>
<p>program	to	conduct	transactions	with	any
Web	server	on	the	Internet.	The	
TELNET</p>
<p>program	has	been	largely
supplanted	by	
SSH</p>
<p>as	a	remote	login	tool,	but	it	is	very	handy	for
debugging	servers	that	talk	to	clients	with	text	lines	over	connections.	For
example,	
Figure	
11.24
uses	
TELNET</p>
<p>to	request	the	home	page	from	the
AOL	Web	server.
⁁</p>
<p>Figure	
11.24	
Example	of	an	HTTP	transaction	that	serves	static
content.
In	line	1,	we	run	
TELNET</p>
<p>from	a	Linux	shell	and	ask	it	to	open	a	connection
to	the	AOL	Web	server.	
TELNET</p>
<p>prints	three	lines	of	output	to	the	terminal,
opens	the	connection,	and	then	waits	for	us	to	enter	text	(line	5).	Each
time	we	enter	a	text	line	and	hit	the	
key,	
TELNET</p>
<p>reads	the	line,
appends	carriage	return	and	line	feed	characters	(
in	C	notation),
and	sends	the	line	to	the	server.	This	is	consistent	with	the	HTTP
standard,	which	requires	every	text	line	to	be	terminated	by	a	carriage
return	and	line	feed	pair.	To	initiate	the	transaction,	we	enter	an	HTTP</p>
<p>request	(lines	5−7).	The	server	replies	with	an	HTTP	response	(lines
8−17)	and	then	closes	the	connection	(line	18).
HTTP	Requests
An	
HTTP	request
consists	of	a	
request	line
(line	5),	followed	by	zero	or
more	
request	headers
(line	6),	followed	by	an	empty	text	line	that
terminates	the	list	of	headers	(line	7).	A	request	line	has	the	form
method	URI	version
HTTP	supports	a	number	of	different	
methods
,	including	GET,	POST,
OPTIONS,	HEAD,	PUT,	DELETE,	and	TRACE.	We	will	only	discuss	the
workhorse	GET	method,	which	accounts	for	a	majority	of	HTTP	requests.
The	GET	method	instructs	the	server	to	generate	and	return	the	content
identified	by	the	
URI</p>
<p>(uniform	resource	identifier)
.	The	URI	is	the	suffix	of
the	corresponding	URL	that	includes	the	filename	and	optional
arguments.
3.	
Actually,	this	is	only	true	when	a	browser	requests	content.	If	a	proxy	server	requests	content,
then	the	URI	must	be	the	complete	URL.
The	
version
field	in	the	request	line	indicates	the	HTTP	version	to	which
the	request	conforms.	The	most	recent	HTTP	version	is	HTTP/1.1	
[37]
.
HTTP/1.0	is	an	earlier,	much	simpler	version	from	1996	
[6]
.	HTTP/1.1
defines	additional	headers	that	provide	support	for	advanced	features
such	as	caching	and	security,	as	well	as	a	mechanism	that	allows	a	client
and	server	to	perform	multiple	transactions	over	the	same	
persistent
connection
.	In	practice,	the	two	versions	are	compatible	because
HTTP/1.0	clients	and	servers	simply	ignore	unknown	HTTP/1.1	headers.
3</p>
<p>To	summarize,	the	request	line	in	line	5	asks	the	server	to	fetch	and
return	the	HTML	file	
.	It	also	informs	the	server	that	the
remainder	of	the	request	will	be	in	HTTP/1.1	format.
Request	headers	provide	additional	information	to	the	server,	such	as	the
brand	name	of	the	browser	or	the	MIME	types	that	the	browser
understands.	Request	headers	have	the	form
header-name
:	
header-data
For	our	purposes,	the	only	header	to	be	concerned	with	is	the	
header	(line	6),	which	is	required	in	HTTP/1.1	requests,	but	not	in
HTTP/1.0	requests.	The	
header	is	used	by	
proxy	caches
,	which
sometimes	serve	as	intermediaries	between	a	browser	and	the	
origin
server
that	manages	the	requested	file.	Multiple	proxies	can	exist
between	a	client	and	an	origin	server	in	a	so-called	proxy	chain.	The	data
in	the	
header,	which	identifies	the	domain	name	of	the	origin	server,
allow	a	proxy	in	the	middle	of	a	proxy	chain	to	determine	if	it	might	have	a
locally	cached	copy	of	the	requested	content.
Continuing	with	our	example	in	
Figure	
11.24
,	the	empty	text	line	in	line
7	(generated	by	hitting	
on	our	keyboard)	terminates	the	headers
and	instructs	the	server	to	send	the	requested	HTML	file.
HTTP	Responses
HTTP	responses	are	similar	to	HTTP	requests.	An	
HTTP	response
consists	of	a	
response	line
(line	8),	followed	by	zero	or	more	
response
headers
(lines	9−13),	followed	by	an	empty	line	that	terminates	the</p>
<p>headers	(line	14),	followed	by	the	
response	body
(lines	15−17).	A
response	line	has	the	form
version	status-code	status-message
The	
version
field	describes	the	HTTP	version	that	the	response	conforms
to.	The	
status-code
is	a	three-digit	positive	integer	that	indicates	the
disposition	of	the	request.	The	
status-message
gives	the	English
equivalent	of	the	error	code.	
Figure	
11.25
lists	some	common	status
codes	and	their	corresponding	messages.
Aside	
Passing	arguments	in	HTTP
POST	requests
Arguments	for	HTTP	POST	requests	are	passed	in	the	request
body	rather	than	in	the	URI.
Status
code
Status	message
Description
200
OK
Request	was	handled	without	error.
301
Moved	permanently
Content	has	moved	to	the	hostname	in	the	Location
header.
400
Bad	request
Request	could	not	be	understood	by	the	server.
403
Forbidden
Server	lacks	permission	to	access	the	requested
file.
404
Not	found
Server	could	not	find	the	requested	file.</p>
<p>501
Not	implemented
Server	does	not	support	the	request	method.
505
HTTP	version	not
supported
Server	does	not	support	version	in	request.
Figure	
11.25	
Some	HTTP	status	codes.
The	response	headers	in	lines	9−13	provide	additional	information	about
the	response.	For	our	purposes,	the	two	most	important	headers	are
(line	12),	which	tells	the	client	the	MIME	type	of	the	content
in	the	response	body,	and	
(line	13),	which	indicates	its
size	in	bytes.
The	empty	text	line	in	line	14	that	terminates	the	response	headers	is
followed	by	the	response	body,	which	contains	the	requested	content.
11.5.4	
Serving	Dynamic	Content
If	we	stop	to	think	for	a	moment	how	a	server	might	provide	dynamic
content	to	a	client,	certain	questions	arise.	For	example,	how	does	the
client	pass	any	program	arguments	to	the	server?	How	does	the	server
pass	these	arguments	to	the	child	process	that	it	creates?	How	does	the
server	pass	other	information	to	the	child	that	it	might	need	to	generate
the	content?	Where	does	the	child	send	its	output?	These	questions	are
addressed	by	a	de	facto	standard	called	
CGI	(common	gateway
interface)
.
How	Does	the	Client	Pass	Program</p>
<p>Arguments	to	the	Server?
Arguments	for	GET	requests	are	passed	in	the	URI.	As	we	have	seen,	a
<code>?'	character	separates	the	filename	from	the	arguments,	and	each argument	is	separated	by	an	</code>&amp;'	character.	Spaces	are	not	allowed	in
arguments	and	must	be	represented	with	the	%20	string.	Similar
encodings	exist	for	other	special	characters.
How	Does	the	Server	Pass	Arguments	to
the	Child?
After	a	server	receives	a	request	such	as
Environment	variable
Description
QUERY_STRING
Program	arguments
SERVER_PORT
Port	that	the	parent	is	listening	on
REQUEST_METHOD
GET	or	POST
REMOTE_HOST
Domain	name	of	client
REMOTE_ADDR
Dotted-decimal	IP	address	of	client
CONTENT_TYPE
POST	only:	MIME	type	of	the	request	body
CONTENT_LENGTH
POST	only:	Size	in	bytes	of	the	request	body</p>
<p>Figure	
11.26	
Examples	of	CGI	environment	variables.
it	calls	
to	create	a	child	process	and	calls	
to	run	the	
program	in	the	context	of	the	child.	Programs	like	the	
program	are	often	referred	to	as	
CGI	programs
because	they	obey	the
rules	of	the	CGI	standard.	Before	the	call	to	
,	the	child	process
sets	the	CGI	environment	variable	QUERY_STRING	to	
,	which
the	
program	can	reference	at	run	time	using	the	Linux	
function.
How	Does	the	Server	Pass	Other
Information	to	the	Child?
CGI	defines	a	number	of	other	environment	variables	that	a	CGI	program
can	expect	to	be	set	when	it	runs.	
Figure	
11.26
shows	a	subset.
Where	Does	the	Child	Send	Its	Output?
A	CGI	program	sends	its	dynamic	content	to	the	standard	output.	Before
the	child	process	loads	and	runs	the	CGI	program,	it	uses	the	Linux	
function	to	redirect	standard	output	to	the	connected	descriptor	that	is
associated	with	the	client.	Thus,	anything	that	the	CGI	program	writes	to
standard	output	goes	directly	to	the	client.
Notice	that	since	the	parent	does	not	know	the	type	or	size	of	the	content
that	the	child	generates,	the	child	is	responsible	for	generating	the</p>
<pre><code>and	
response	headers,	as	well	as	the	empty
</code></pre>
<p>line	that	terminates	the	headers.
Figure	
11.27
shows	a	simple	CGI	program	that	sums	its	two
arguments	and	returns	an	HTML	file	with	the	result	to	the	client.	
Figure
11.28
shows	an	HTTP	transaction	that	serves	dynamic	content	from
the	
program.
Practice	Problem	
11.5	
(solution	page	
969
)
In	
Section	
10.11
,	we	warned	you	about	the	dangers	of	using
the	C	standard	I/O	functions	in	network	applications.	Yet	the	CGI
program	in	
Figure	
11.27
is	able	to	use	standard	I/O	without	any
problems.	Why?
Aside	
Passing	arguments	in	HTTP
POST	requests	to	CGI	programs
For	POST	requests,	the	child	would	also	need	to	redirect	standard
input	to	the	connected	descriptor.	The	CGI	program	would	then
read	the	arguments	in	the	request	body	from	standard	input.</p>
<p>Figure	
11.27	
CGI	program	that	sums	two	integers.
⁁</p>
<p>Figure	
11.28	
An	HTTP	transaction	that	serves	dynamic	HTML
content.</p>
<p>11.6	
Putting	It	Together:	The	T
INY
Web	Server
We	conclude	our	discussion	of	network	programming	by	developing	a
small	but	functioning	Web	server	called	T
INY
.	T
INY</p>
<p>is	an	interesting
program.	It	combines	many	of	the	ideas	that	we	have	learned	about,
such	as	process	control,	Unix	I/O,	the	sockets	interface,	and	HTTP,	in
only	250	lines	of	code.	While	it	lacks	the	functionality,	robustness,	and
security	of	a	real	server,	it	is	powerful	enough	to	serve	both	static	and
dynamic	content	to	real	Web	browsers.	We	encourage	you	to	study	it	and
implement	it	yourself.	It	is	quite	exciting	(even	for	the	authors!)	to	point	a
real	browser	at	your	own	server	and	watch	it	display	a	complicated	Web
page	with	text	and	graphics.
The	T
INY</p>
<pre><code>Routine
</code></pre>
<p>Figure	
11.29
shows	T
INY
'
S</p>
<p>main	routine.	T
INY</p>
<p>is	an	iterative	server	that
listens	for	connection	requests	on	the	port	that	is	passed	in	the	command
line.	After	opening	a	listening	socket	by	calling	the	
function,	T
INY</p>
<p>executes	the	typical	infinite	server	loop,	repeatedly
accepting	a	connection	request	(line	32),	performing	a	transaction	(line
36),	and	closing	its	end	of	the	connection	(line	37).
The	
Function</p>
<p>The	
function	in	
Figure	
11.30
handles	one	HTTP	transaction.
First,	we	read	and	parse	the	request	line	(lines	11−14).	Notice	that	we	are
using	the	
function	from	Figure	
Figure	
10.8
to	read	the
request	line.
T
INY</p>
<p>supports	only	the	GET	method.	If	the	client	requests	another	method
(such	as	POST),	we	send	it	an	error	message	and	return	to	the	main
routine</p>
<p>Figure	
11.29	
The	T
INY</p>
<p>Web	server.</p>
<p>Figure	
11.30	
handles	one	HTTP	transaction.
(lines	15−19),	which	then	closes	the	connection	and	awaits	the	next
connection	request.	Otherwise,	we	read	and	(as	we	shall	see)	ignore	any
request	headers	(line	20).
Next,	we	parse	the	URI	into	a	filename	and	a	possibly	empty	CGI
argument	string,	and	we	set	a	flag	that	indicates	whether	the	request	is
for	static	or	dynamic	content	(line	23).	If	the	file	does	not	exist	on	disk,	we
immediately	send	an	error	message	to	the	client	and	return.</p>
<p>Finally,	if	the	request	is	for	static	content,	we	verify	that	the	file	is	a
regular	file	and	that	we	have	read	permission	(line	31).	If	so,	we	serve	the
static	content	(line	36)	to	the	client.	Similarly,	if	the	request	is	for	dynamic
content,	we	verify	that	the	file	is	executable	(line	39),	and,	if	so,	we	go
ahead	and	serve	the	dynamic	content	(line	44).
The	
Function
T
INY</p>
<p>lacks	many	of	the	error-handling	features	of	a	real	server.	However,
it	does	check	for	some	obvious	errors	and	reports	them	to	the	client.	The
function	in	
Figure	
11.31
sends	an	HTTP	response	to	the
client	with	the	appropriate</p>
<p>Figure	
11.31	
sends	an	error	message	to	the	client.</p>
<p>Figure	
11.32	
reads	and	ignores	request
headers.
status	code	and	status	message	in	the	response	line,	along	with	an
HTML	file	in	the	response	body	that	explains	the	error	to	the	browser's
user.
Recall	that	an	HTML	response	should	indicate	the	size	and	type	of	the
content	in	the	body.	Thus,	we	have	opted	to	build	the	HTML	content	as	a
single	string	so	that	we	can	easily	determine	its	size.	Also,	notice	that	we
are	using	the	robust	
function	from	
Figure	
10.4
for	all
output.
The	
Function
T
INY</p>
<p>does	not	use	any	of	the	information	in	the	request	headers.	It	simply
reads	and	ignores	them	by	calling	the	
function	in
Figure	
11.32
.	Notice	that	the	empty	text	line	that	terminates	the
request	headers	consists	of	a	carriage	return	and	line	feed	pair,	which	we
check	for	in	line	6.
The	
Function
T
INY</p>
<p>assumes	that	the	home	directory	for	static	content	is	its	current
directory	and	that	the	home	directory	for	executables	is	
.	Any
URI	that	contains	the	string	
is	assumed	to	denote	a	request	for
dynamic	content.	The	default	filename	is	
.</p>
<p>The	
function	in	
Figure	
11.33
implements	these	policies.	It
parses	the	URI	into	a	filename	and	an	optional	CGI	argument	string.	If
the	request	is	for	static	content	(line	5),	we	clear	the	CGI	argument	string
(line	6)	and	then	convert	the	URI	into	a	relative	Linux	pathname	such	as
(lines	7−8).	If	the	URI	ends	with	a	`/'	character	(line	9),	then
we	append	the	default	filename	(line	10).	On	the	other	hand,	if	the
request	is	for	dynamic	content	(line	13),	we	extract	any	CGI	arguments
(lines	14−20)	and	convert	the	remaining	portion	of	the	URI	to	a	relative
Linux	filename	(lines	21−22).</p>
<p>Figure	
11.33	
T
INY</p>
<p>parse_uri	parses	an	HTTP	URI.
The	
Function
T
INY</p>
<p>serves	five	common	types	of	static	content:	HTML	files,	unformatted
text	files,	and	images	encoded	in	GIF,	PNG,	and	JPEG	formats.
The	
function	in	
Figure	
11.34
sends	an	HTTP	response
whose	body	contains	the	contents	of	a	local	file.	First,	we	determine	the
file	type	by	inspecting	the	suffix	in	the	filename	(line	7)	and	then	send	the
response	line	and	response	headers	to	the	client	(lines	8−13).	Notice	that
a	blank	line	terminates	the	headers.
Next,	we	send	the	response	body	by	copying	the	contents	of	the
requested	file	to	the	connected	descriptor	
.	The	code	here	is
somewhat	subtle	and	needs	to	be	studied	carefully.	Line	18	opens
for	reading	and	gets	its	descriptor.	In	line	19,	the	Linux	
function	maps	the	requested	file	to	a	virtual	memory	area.	Recall	from
our	discussion	of	
in	
Section	
9.8
that	the	call	to	
maps	the</p>
<p>Figure	
11.34	
T
INY</p>
<p>serve_static	serves	static	content	to	a	client.
first	
bytes	of	file	
to	a	private	read-only	area	of	virtual
memory	that	starts	at	address	
.
Once	we	have	mapped	the	file	to	memory,	we	no	longer	need	its
descriptor,	so	we	close	the	file	(line	20).	Failing	to	do	this	would	introduce
a	potentially	fatal	memory	leak.	Line	21	performs	the	actual	transfer	of
the	file	to	the	client.	The	
function	copies	the	
bytes
starting	at	location	
(which	of	course	is	mapped	to	the	requested	file)
to	the	client's	connected	descriptor.	Finally,	line	22	frees	the	mapped
virtual	memory	area.	This	is	important	to	avoid	a	potentially	fatal	memory
leak.</p>
<p>The	
Function
T
INY</p>
<p>serves	any	type	of	dynamic	content	by	forking	a	child	process	and
then	running	a	CGI	program	in	the	context	of	the	child.
The	
function	in	
Figure	
11.35
begins	by	sending	a
response	line	indicating	success	to	the	client,	along	with	an	informational
header.	The	CGI	program	is	responsible	for	sending	the	rest	of
the	response.	Notice	that	this	is	not	as	robust	as	we	might	wish,	since	it
doesn't	allow	for	the	possibility	that	the	CGI	program	might	encounter
some	error.
After	sending	the	first	part	of	the	response,	we	fork	a	new	child	process
(line	11).	The	child	initializes	the	QUERY_STRING	environment	variable
with	the	CGI	arguments	from	the	request	URI	(line	13).	Notice	that	a	real
server	would</p>
<p>Figure	
11.35	
T
INY</p>
<p>serve_dynamic	serves	dynamic	content	to	a	client.
Aside	
Dealing	with	prematurely	closed
connections
Although	the	basic	functions	of	a	Web	server	are	quite	simple,	we
don't	want	to	give	you	the	false	impression	that	writing	a	real	Web
server	is	easy.	Building	a	robust	Web	server	that	runs	for
extended	periods	without	crashing	is	a	difficult	task	that	requires	a
deeper	understanding	of	Linux	systems	programming	than	we've
learned	here.	For	example,	if	a	server	writes	to	a	connection	that
has	already	been	closed	by	the	client	(say,	because	you	clicked
the	&quot;Stop&quot;	button	on	your	browser),	then	the	first	such	write
returns	normally,	but	the	second	write	causes	the	delivery	of	a
SIGPIPE	signal	whose	default	behavior	is	to	terminate	the
process.	If	the	SIGPIPE	signal	is	caught	or	ignored,	then	the</p>
<p>second	write	operation	returns	−1	with	
set	to	EPIPE.	The
and	
functions	report	the	EPIPE	error	as	a	&quot;Broken
pipe,&quot;	a	nonintuitive	message	that	has	confused	generations	of
students.	The	bottom	line	is	that	a	robust	server	must	catch	these
SIGPIPE	signals	and	check	
function	calls	for	EPIPE	errors.
set	the	other	CGI	environment	variables	here	as	well.	For	brevity,	we
have	omitted	this	step.
Next,	the	child	redirects	the	child's	standard	output	to	the	connected	file
descriptor	(line	14)	and	then	loads	and	runs	the	CGI	program	(line	15).
Since	the	CGI	program	runs	in	the	context	of	the	child,	it	has	access	to
the	same	open	files	and	environment	variables	that	existed	before	the
call	to	the	
function.	Thus,	everything	that	the	CGI	program	writes
to	standard	output	goes	directly	to	the	client	process,	without	any
intervention	from	the	parent	process.	Meanwhile,	the	parent	blocks	in	a
call	to	
,	waiting	to	reap	the	child	when	it	terminates	(line	17).</p>
<p>11.7	
Summary
Every	network	application	is	based	on	the	client-server	model.	With	this
model,	an	application	consists	of	a	server	and	one	or	more	clients.	The
server	manages	resources,	providing	a	service	for	its	clients	by
manipulating	the	resources	in	some	way.	The	basic	operation	in	the
client-server	model	is	a	client-server	transaction,	which	consists	of	a
request	from	a	client,	followed	by	a	response	from	the	server.
Clients	and	servers	communicate	over	a	global	network	known	as	the
Internet.	From	a	programmer's	point	of	view,	we	can	think	of	the	Internet
as	a	worldwide	collection	of	hosts	with	the	following	properties:	(1)	Each
Internet	host	has	a	unique	32-bit	name	called	its	IP	address.	(2)	The	set
of	IP	addresses	is	mapped	to	a	set	of	Internet	domain	names.	(3)
Processes	on	different	Internet	hosts	can	communicate	with	each	other
over	connections.
Clients	and	servers	establish	connections	by	using	the	sockets	interface.
A	socket	is	an	end	point	of	a	connection	that	is	presented	to	applications
in	the	form	of	a	file	descriptor.	The	sockets	interface	provides	functions
for	opening	and	closing	socket	descriptors.	Clients	and	servers
communicate	with	each	other	by	reading	and	writing	these	descriptors.
Web	servers	and	their	clients	(such	as	browsers)	communicate	with	each
other	using	the	HTTP	protocol.	A	browser	requests	either	static	or
dynamic	content	from	the	server.	A	request	for	static	content	is	served	by
fetching	a	file	from	the	server's	disk	and	returning	it	to	the	client.	A</p>
<p>request	for	dynamic	content	is	served	by	running	a	program	in	the
context	of	a	child	process	on	the	server	and	returning	its	output	to	the
client.	The	CGI	standard	provides	a	set	of	rules	that	govern	how	the
client	passes	program	arguments	to	the	server,	how	the	server	passes
these	arguments	and	other	information	to	the	child	process,	and	how	the
child	sends	its	output	back	to	the	client.	A	simple	but	functioning	Web
server	that	serves	both	static	and	dynamic	content	can	be	implemented
in	a	few	hundred	lines	of	C	code.</p>
<p>Bibliographic	Notes
The	official	source	of	information	for	the	Internet	is	contained	in	a	set	of
freely	available	numbered	documents	known	as	
RFCs	(requests	for
comments)
.	A	searchable	index	of	RFCs	is	available	on	the	Web	at
RFCs	are	typically	written	for	developers	of	Internet	infrastructure,	and
thus	they	are	usually	too	detailed	for	the	casual	reader.	However,	for
authoritative	information,	there	is	no	better	source.	The	HTTP/1.1
protocol	is	documented	in	RFC	2616.	The	authoritative	list	of	MIME	types
is	maintained	at
http:/
/
www.iana.org/
assignments/
media-types
Kerrisk	is	the	bible	for	all	aspects	of	Linux	programming	and	provides	a
detailed	discussion	of	modern	network	programming	
[62]
.	There	are	a
number	of	good	general	texts	on	computer	networking	
[65
,	
84
,	
114]
.	The
great	technical	writer	W.	Richard	Stevens	developed	a	series	of	classic
texts	on	such	topics	as	advanced	Unix	programming	
[111]
,	the	Internet
protocols	
[109
,	
120
,	
107]
,	and	Unix	network	programming	
[108
,	
110]
.
Serious	students	of	Unix	systems	programming	will	want	to	study	all	of
them.	Tragically,	Stevens	died	on	September	1,	1999.	His	contributions
are	greatly	missed.</p>
<p>Homework	Problems
11.6
Modify	T
INY</p>
<p>so	that	it	echoes	every	request	line	and	request
header.
Use	your	favorite	browser	to	make	a	request	to	T
INY</p>
<p>for	static
content.	Capture	the	output	from	T
INY</p>
<p>in	a	file.
Inspect	the	output	from	T
INY</p>
<p>to	determine	the	version	of	HTTP	your
browser	uses.
Consult	the	HTTP/1.1	standard	in	RFC	2616	to	determine	the
meaning	of	each	header	in	the	HTTP	request	from	your	browser.
You	can	obtain	RFC	2616	from	
www.rfc-editor.org/
rfc.html
.
11.7
Extend	T
INY</p>
<p>so	that	it	serves	MPG	video	files.	Check	your	work	using	a
real	browser.
11.8</p>
<p>Modify	T
INY</p>
<p>so	that	it	reaps	CGI	children	inside	a	SIGCHLD	handler
instead	of	explicitly	waiting	for	them	to	terminate.
11.9
Modify	T
INY</p>
<p>so	that	when	it	serves	static	content,	it	copies	the	requested
file	to	the	connected	descriptor	using	
,	and	
,
instead	of	
and	
.
11.10
A
.	
Write	an	HTML	form	for	the	CGI	
function	in	
Figure	
11.27
.
Your	form	should	include	two	text	boxes	that	users	fill	in	with	the
two	numbers	to	be	added	together.	Your	form	should	request
content	using	the	GET	method.
B
.	
Check	your	work	by	using	a	real	browser	to	request	the	form	from
T
INY
,	submit	the	filled-in	form	to	T
INY
,	and	then	display	the	dynamic
content	generated	by	
.
11.11
Extend	T
INY</p>
<p>to	support	the	HTTP	HEAD	method.	Check	your	work	using
TELNET</p>
<p>as	a	Web	client.</p>
<p>11.12
Extend	T
INY</p>
<p>so	that	it	serves	dynamic	content	requested	by	the	HTTP
POST	method.	Check	your	work	using	your	favorite	Web	browser.
11.13
Modify	T
INY</p>
<p>so	that	it	deals	cleanly	(without	terminating)	with	the	SIGPIPE
signals	and	EPIPE	errors	that	occur	when	the	write	function	attempts	to
write	to	a	prematurely	closed	connection.</p>
<p>Solutions	to	Practice	Problems
Solution	to	Problem	
11.1	
(page
927
)
Hex	address
Dotted-decimal	address
Solution	to	Problem	
11.2	
(page
927
)</p>
<p>Solution	to	Problem	
11.3	
(page</p>
<p>927
)</p>
<p>Solution	to	Problem	
11.4	
(page
942
)
Here's	a	solution.	Notice	how	much	more	difficult	it	is	to	use	
,
which	requires	messy	casting	and	deep	structure	references.	The
function	is	much	simpler	because	it	does	all	of	that	work	for
us.</p>
<p>Solution	to	Problem	
11.5	
(page</p>
<p>954
)
The	reason	that	standard	I/O	works	in	CGI	programs	is	that	the	CGI
program	running	in	the	child	process	does	not	need	to	explicitly	close	any
of	its	input	or	output	streams.	When	the	child	terminates,	the	kernel
closes	all	descriptors	automatically.</p>
<p>Chapter	
12	
Concurrent
Programming
12.1	
Concurrent	Programming	with	Processes	
973
12.2	
Concurrent	Programming	with	I/O	Multiplexing	
977
12.3	
Concurrent	Programming	with	Threads	
985
12.4	
Shared	Variables	in	Threaded	Programs	
992
12.5	
Synchronizing	Threads	with	Semaphores	
995
12.6	
Using	Threads	for	Parallelism	
1013
12.7	
Other	Concurrency	Issues	
1020
12.8	
Summary</p>
<p>1030
Bibliographic	Notes	
1030
Homework	Problems	
1031
Solutions	to	Practice	Problems	
1036
As	we	learned	in	
Chapter	
8
,	logical	control	flows
are	
concurrent
if	they	overlap	in	time.	This	general</p>
<p>phenomenon,	known	as	
concurrency
,	shows	up	at
many	different	levels	of	a	computer	system.
Hardware	exception	handlers,	processes,	and	Linux
signal	handlers	are	all	familiar	examples.
Thus	far,	we	have	treated	concurrency	mainly	as	a
mechanism	that	the	operating	system	kernel	uses	to
run	multiple	application	programs.	But	concurrency
is	not	just	limited	to	the	kernel.	It	can	play	an
important	role	in	application	programs	as	well.	For
example,	we	have	seen	how	Linux	signal	handlers
allow	applications	to	respond	to	asynchronous
events	such	as	the	user	typing	Ctrl+C	or	the
program	accessing	an	undefined	area	of	virtual
memory.	Application-level	concurrency	is	useful	in
other	ways	as	well:
Accessing	slow	I/O	devices.	
When	an
application	is	waiting	for	data	to	arrive	from	a
slow	I/O	device	such	as	a	disk,	the	kernel	keeps
the	CPU	busy	by	running	other	processes.
Individual	applications	can	exploit	concurrency	in
a	similar	way	by	overlapping	useful	work	with	I/O
requests.
Interacting	with	humans.	
People	who	interact
with	computers	demand	the	ability	to	perform
multiple	tasks	at	the	same	time.	For	example,
they	might	want	to	resize	a	window	while	they
are	printing	a	document.	Modern	windowing</p>
<p>systems	use	concurrency	to	provide	this
capability.	Each	time	the	user	requests	some
action	(say,	by	clicking	the	mouse),	a	separate
concurrent	logical	flow	is	created	to	perform	the
action.
Reducing	latency	by	deferring	work.
Sometimes,	applications	can	use	concurrency	to
reduce	the	latency	of	certain	operations	by
deferring	other	operations	and	performing	them
concurrently.	For	example,	a	dynamic	storage
allocator	might	reduce	the	latency	of	individual
free	operations	by	deferring	coalescing	to	a
concurrent	&quot;coalescing&quot;	flow	that	runs	at	a	lower
priority,	soaking	up	spare	CPU	cycles	as	they
become	available.
Servicing	multiple	network	clients.	
The
iterative	network	servers	that	we	studied	in
Chapter	
11
are	unrealistic	because	they	can
only	service	one	client	at	a	time.	Thus,	a	single
slow	client	can	deny	service	to	every	other	client.
For	a	real	server	that	might	be	expected	to
service	hundreds	or	thousands	of	clients	per
second,	it	is	not	acceptable	to	allow	one	slow
client	to	deny	service	to	the	others.	A	better
approach	is	to	build	a	
concurrent	server
that
creates	a	separate	logical	flow	for	each	client.
This	allows	the	server	to	service	multiple	clients
concurrently	and	precludes	slow	clients	from
monopolizing	the	server.</p>
<p>Computing	in	parallel	on	multi-core
machines.	
Many	modern	systems	are	equipped
with	multi-core	processors	that	contain	multiple
CPUs.	Applications	that	are	partitioned	into
concurrent	flows	often	run	faster	on	multi-core
machines	than	on	uniprocessor	machines
because	the	flows	execute	in	parallel	rather	than
being	interleaved.
Applications	that	use	application-level	concurrency
are	known	as	
concurrent	programs
.	Modern
operating	systems	provide	three	basic	approaches
for	building	concurrent	programs:
Processes.	
With	this	approach,	each	logical
control	flow	is	a	process	that	is	scheduled	and
maintained	by	the	kernel.	Since	processes	have
separate	virtual	address	spaces,	flows	that	want
to	communicate	with	each	other	must	use	some
kind	of	explicit	
interprocess	communication	(IPC)
mechanism.
I/O	multiplexing.	
his	is	a	form	of	concurrent
programming	where	applications	explicitly
schedule	their	own	logical	flows	in	the	context	of
a	single	process.	Logical	flows	are	modeled	as
state	machines	that	the	main	program	explicitly
transitions	from	state	to	state	as	a	result	of	data
arriving	on	file	descriptors.	Since	the	program	is</p>
<p>a	single	process,	all	flows	share	the	same
address	space.
Threads.	
Threads	are	logical	flows	that	run	in
the	context	of	a	single	process	and	are
scheduled	by	the	kernel.	You	can	think	of
threads	as	a	hybrid	of	the	other	two	approaches,
scheduled	by	the	kernel	like	process	flows	and
sharing	the	same	virtual	address	space	like	I/O
multiplexing	flows.
This	chapter	investigates	these	three	different
concurrent	programming	techniques.	To	keep	our
discussion	concrete,	we	will	work	with	the	same
motivating	application	throughout—a	concurrent
version	of	the	iterative	echo	server	from	
Section
11.4.9
.</p>
<p>12.1	
Concurrent	Programming	with
Processes
The	simplest	way	to	build	a	concurrent	program	is	with	processes,	using
familiar	functions	such	as	
,	and	
.	For	example,	a
natural	approach	for	building	a	concurrent	server	is	to	accept	client
connection	requests	in	the	parent	and	then	create	a	new	child	process	to
service	each	new	client.
To	see	how	this	might	work,	suppose	we	have	two	clients	and	a	server
that	is	listening	for	connection	requests	on	a	listening	descriptor	(say,	3).
Now	suppose	that	the	server	accepts	a	connection	request	from	client	1
and	returns	a	connected	descriptor	(say,	4),	as	shown	in	
Figure	
12.1
.
After	accepting	the	connection	request,	the	server	forks	a	child,	which
gets	a	complete	copy	of	the	server's	descriptor	table.	The	child	closes	its
copy	of	listening	descriptor	3,	and	the	parent	closes	its	copy	of	connected
descriptor	4,	since	they	are	no	longer	needed.	This	gives	us	the	situation
shown	in	
Figure	
12.2
,	where	the	child	process	is	busy	servicing	the
client.
Since	the	connected	descriptors	in	the	parent	and	child	each	point	to	the
same	file	table	entry,	it	is	crucial	for	the	parent	to	close	its	copy	of	the
connected</p>
<p>Figure	
12.1	
Step	1:	Server	accepts	connection	request	from	client.
Figure	
12.2	
Step	2:	Server	forks	a	child	process	to	service	the	client.
Figure	
12.3	
Step	3:	Server	accepts	another	connection	request.</p>
<p>descriptor.	Otherwise,	the	file	table	entry	for	connected	descriptor	4	will
never	be	released,	and	the	resulting	memory	leak	will	eventually
consume	the	available	memory	and	crash	the	system.
Now	suppose	that	after	the	parent	creates	the	child	for	client	1,	it	accepts
a	new	connection	request	from	client	2	and	returns	a	new	connected
descriptor	(say,	5),	as	shown	in	
Figure	
12.3
.	The	parent	then	forks
another	child,	which	begins	servicing	its	client	using	connected	descriptor
5,	as	shown	in	
Figure	
12.4
.	At	this	point,	the	parent	is	waiting	for	the
next	connection	request	and	the	two	children	are	servicing	their
respective	clients	concurrently.
12.1.1	
A	Concurrent	Server	Based
on	Processes
Figure	
12.5
shows	the	code	for	a	concurrent	echo	server	based	on
processes.	The	echo	function	called	in	line	29	comes	from	
Figure
11.22
.	There	are	several	important	points	to	make	about	this	server:
First,	servers	typically	run	for	long	periods	of	time,	so	we	must	include
a	SIGCHLD	handler	that	reaps	zombie	children	(lines	4−9).	Since
SIGCHLD	signals	are	blocked	while	the	SIGCHLD	handler	is
executing,	and	since	Linux	signals	are	not	queued,	the	SIGCHLD
handler	must	be	prepared	to	reap	multiple	zombie	children.
Second,	the	parent	and	the	child	must	close	their	respective	copies	of
(lines	33	and	30,	respectively).	As	we	have	mentioned,	this	is
especially	important</p>
<p>Figure	
12.4	
Step	4:	Server	forks	another	child	to	service	the	new
client.
for	the	parent,	which	must	close	its	copy	of	the	connected	descriptor
to	avoid	a	memory	leak.
Finally,	because	of	the	reference	count	in	the	socket's	file	table	entry,
the	connection	to	the	client	will	not	be	terminated	until	both	the
parent's	and	child's	copies	of	
are	closed.
12.1.2	
Pros	and	Cons	of	Processes
Processes	have	a	clean	model	for	sharing	state	information	between
parents	and	children:	file	tables	are	shared	and	user	address	spaces	are
not.	Having	separate	address	spaces	for	processes	is	both	an	advantage
and	a	disadvantage.	It	is	impossible	for	one	process	to	accidentally
overwrite	the	virtual	memory	of	another	process,	which	eliminates	a	lot	of
confusing	failures—an	obvious	advantage.</p>
<h2>On	the	other	hand,	separate	address	spaces	make	it	more	difficult	for
processes	to	share	state	information.	To	share	information,	they	must	use
explicit	IPC	(interprocess	communications)	mechanisms.	(See	the	Aside
on	
page	977
.)	Another	disadvantage	of	process-based	designs	is	that
they	tend	to	be	slower	because	the	overhead	for	process	control	and	IPC
is	high.
Practice	Problem	
12.1	
(solution	page	
1036
)
After	the	parent	closes	the	connected	descriptor	in	line	33	of	the
concurrent	server	in	
Figure	
12.5
,	the	child	is	still	able	to
communicate	with	the	client	using	its	copy	of	the	descriptor.	Why?
Practice	Problem	
12.2	
(solution	page	
1036
)
If	we	were	to	delete	line	30	of	
Figure	
12.5
,	which	closes	the
connected	descriptor,	the	code	would	still	be	correct,	in	the	sense
that	there	would	be	no	memory	leak.	Why?</h2>
<p>code/conc/echoserverp.c</p>
<hr />
<p>code/conc/echoserverp.c
Figure	
12.5	
Concurrent	echo	server	based	on	processes.
The	parent	forks	a	child	to	handle	each	new	connection	request.
Aside	
Unix	IPC
You	have	already	encountered	several	examples	of	IPC	in	this
text.	The	
function	and	signals	from	
Chapter	
8
are
primitive	IPC	mechanisms	that	allow	processes	to	send	tiny
messages	to	process	running	on	the	same	host.	The	sockets
interface	from	
Chapter	
11
is	an	important	form	of	IPC	that
allows	processes	on	different	hosts	to	exchange	arbitrary	byte
streams.	However,	the	term	
Unix	IPC
is	typically	reserved	for	a
hodgepodge	of	techniques	that	allow	processes	to	communicate
with	other	processes	that	are	running	on	the	same	host.	Examples
include	pipes,	FIFOs,	System	V	shared	memory,	and	System	V
semaphores.	These	mechanisms	are	beyond	our	scope.	The	book
by	Kerrisk	[
62
]	is	an	excellent	reference.</p>
<p>12.2	
Concurrent	Programming	with
I/O	Multiplexing
Suppose	you	are	asked	to	write	an	echo	server	that	can	also	respond	to
interactive	commands	that	the	user	types	to	standard	input.	In	this	case,
the	server	must	respond	to	two	independent	I/O	events:	(1)	a	network
client	making	a	connection	request,	and	(2)	a	user	typing	a	command	line
at	the	keyboard.	Which	event	do	we	wait	for	first?	Neither	option	is	ideal.
If	we	are	waiting	for	a	connection	request	in	accept,	then	we	cannot
respond	to	input	commands.	Similarly,	if	we	are	waiting	for	an	input
command	in	read,	then	we	cannot	respond	to	any	connection	requests.
One	solution	to	this	dilemma	is	a	technique	called	
I/O	multiplexing
.	The
basic	idea	is	to	use	the	
function	to	ask	the	kernel	to	suspend	the
process,	returning	control	to	the	application	only	after	one	or	more	I/O
events	have	occurred,	as	in	the	following	examples:
Return	when	any	descriptor	in	the	set	{0,	4}	is	ready	for	reading.
Return	when	any	descriptor	in	the	set	{1,	2,	7}	is	ready	for	writing.
Time	out	if	152.13	seconds	have	elapsed	waiting	for	an	I/O	event	to
occur.
is	a	complicated	function	with	many	different	usage	scenarios.	We
will	only	discuss	the	first	scenario:	waiting	for	a	set	of	descriptors	to	be
ready	for	reading.	See	[
62
,	
110
]	for	a	complete	discussion.</p>
<h2>The	
function	manipulates	sets	of	type	
,	which	are	known	as
descriptor	sets
.	Logically,	we	think	of	a	descriptor	set	as	a	bit	vector
(introduced	in	
Section	
2.1
)	of	size	
n
:
Each	bit	
b
corresponds	to	descriptor	
k
.	Descriptor	
k
is	a	member	of	the
descriptor	set	if	and	only	if	
b
=	1.	You	are	only	allowed	to	do	three	things
with	descriptor	sets:	(1)	allocate	them,	(2)	assign	one	variable	of	this	type
to	another,	and	(3)	modify	and	inspect	them	using	the	FD_ZERO,
FD_SET,	FD_CLR,	and	FD_ISSET	macros.
For	our	purposes,	the	select	function	takes	two	inputs:	a	descriptor	set
(
)	called	the	
read	set
,	and	the	cardinality	(n)	of	the	read	set	(actually
b
n</h2>
<p>1
,
 
…
,
 
b
1
,
 
b
0
k
k</p>
<p>the	maximum	cardinality	of	any	descriptor	set).	The	
function
blocks	until	at	least	one	descriptor	in	the	read	set	is	ready	for	reading.	A
descriptor	
k
is	
ready	for	reading
if	and	only	if	a	request	to	read	1	byte
from	that	descriptor	would	not	block.	As	a	side	effect,	
modifies	the
pointed	to	by	argument	
to	indicate	a	subset	of	the	read	set
called	the	
ready	set
,	consisting	of	the	descriptors	in	the	read	set	that	are
ready	for	reading.	The	value	returned	by	the	function	indicates	the
cardinality	of	the	ready	set.	Note	that	because	of	the	side	effect,	we	must
update	the	read	set	every	time	
is	called.
The	best	way	to	understand	
is	to	study	a	concrete	example.
Figure	
12.6
shows	how	we	might	use	
to	implement	an	iterative
echo	server	that	also	accepts	user	commands	on	the	standard	input.	We
begin	by	using	the	
function	from	
Figure	
11.19
to	open	a
listening	descriptor	(line	16),	and	then	using	FD_ZERO	to	create	an
empty	read	set	(line	18):
Next,	in	lines	19	and	20,	we	define	the	read	set	to	consist	of	descriptor	0
(standard	input)	and	descriptor	3	(the	listening	descriptor),	respectively:</p>
<h2>At	this	point,	we	begin	the	typical	server	loop.	But	instead	of	waiting	for	a
connection	request	by	calling	the	
function,	we	call	the	
function,	which	blocks	until	either	the	listening	descriptor	or	standard
input	is	ready	for	reading	(line	24).	For	example,	here	is	the	value	of
that	
would	return	if	the	user	hit	the	enter	key,	thus
causing	the	standard	input	descriptor	to</h2>
<p>code/conc/select.c</p>
<hr />
<p>code/conc/select.c
Figure	
12.6	
An	iterative	echo	server	that	uses	I/O	multiplexing.
The	server	uses	
to	wait	for	connection	requests	on	a	listening
descriptor	and	commands	on	standard	input.
become	ready	for	reading:
Once	
returns,	we	use	the	FD_ISSET	macro	to	determine	which
descriptors	are	ready	for	reading.	If	standard	input	is	ready	(line	25),	we
call	the	
function,	which	reads,	parses,	and	responds	to	the
command	before	returning	to	the	main	routine.	If	the	listening	descriptor
is	ready	(line	27),	we	call	
to	get	a	connected	descriptor	and	then
call	the	echo	function	from	
Figure	
11.22
,	which	echoes	each	line	from
the	client	until	the	client	closes	its	end	of	the	connection.
While	this	program	is	a	good	example	of	using	
,	it	still	leaves
something	to	be	desired.	The	problem	is	that	once	it	connects	to	a	client,
it	continues	echoing	input	lines	until	the	client	closes	its	end	of	the
connection.	Thus,	if	you	type	a	command	to	standard	input,	you	will	not
get	a	response	until	the	server	is	finished	with	the	client.	A	better
approach	would	be	to	multiplex	at	a	finer	granularity,	echoing	(at	most)
one	text	line	each	time	through	the	server	loop.</p>
<p>Practice	Problem	
12.3	
(solution
page	
1036
)
In	Linux	systems,	typing	Ctrl+D	indicates	EOF	on	standard	input.
What	happens	if	you	type	Ctrl+D	to	the	program	in	
Figure	
12.6
while	it	is	blocked	in	the	call	to	
?
12.2.1	
A	Concurrent	Event-Driven
Server	Based	on	I/O	Multiplexing
I/O	multiplexing	can	be	used	as	the	basis	for	concurrent	
event-driven
programs,	where	flows	make	progress	as	a	result	of	certain	events.	The
general	idea	is	to	model	logical	flows	as	state	machines.	Informally,	a
state	machine
is	a	collection	of	
states
,	
input	events
,	and	
transitions
that
map	states	and	input	events	to	states.	Each	transition	maps	an	(input
state,	input	event)	pair	to	an	output	state.	A	
self-loop
is	a	transition
between	the	same	input	and	output	state.	State	machines	are	typically
drawn	as	directed	graphs,	where	nodes	represent	states,	directed	arcs
represent	transitions,	and	arc	labels	represent	input	events.	A	state
machine	begins	execution	in	some	initial	state.	Each	input	event	triggers
a	transition	from	the	current	state	to	the	next	state.
For	each	new	client	
k
,	a	concurrent	server	based	on	I/O	multiplexing
creates	a	new	state	machine	
s
and	associates	it	with	connected
descriptor	
d
.	As	shown	in	
Figure	
12.7
,	each	state	machine	
s
has	one
k
k
k</p>
<p>state	(&quot;waiting	for	descriptor	
d
to	be	ready	for	reading&quot;),	one	input	event
(&quot;descriptor	
d
is	ready	for	reading&quot;),	and	one	transition	(&quot;read	a	text	line
from	descriptor	
d
&quot;).
Figure	
12.7	
State	machine	for	a	logical	flow	in	a	concurrent	event-
driven	echo	server.
The	server	uses	the	I/O	multiplexing,	courtesy	of	the	
function,	to
detect	the	occurrence	of	input	events.	As	each	connected	descriptor
becomes	ready	for	reading,	the	server	executes	the	transition	for	the
corresponding	state	machine—in	this	case,	reading	and	echoing	a	text
line	from	the	descriptor.
Figure	
12.8
shows	the	complete	example	code	for	a	concurrent	event-
driven	server	based	on	I/O	multiplexing.	The	set	of	active	clients	is
maintained	in	a	pool	structure	(lines	3−11).	After	initializing	the	pool	by
calling	
(line	27),	the	server	enters	an	infinite	loop.	During	each
iteration	of	this	loop,	the	server	calls	the	
function	to	detect	two
different	kinds	of	input	events:	(1)	a	connection	request	arriving	from	a
new	client,	and	(2)	a	connected	descriptor	for	an	existing	client	being
ready	for	reading.	When	a	connection	request	arrives	(line	35),	the	server
k
k
k</p>
<p>opens	the	connection	(line	37)	and	calls	the	
function	to	add
the	client	to	the	pool	(line	38).	Finally,	the	server	calls	the	
function	to	echo	a	single	text	line	from	each	ready	connected	descriptor
(line	42).
The	
function	(
Figure	
12.9
)	initializes	the	client	pool.	The
array	represents	a	set	of	connected	descriptors,	with	the	integer
−1	denoting	an	available	slot.	Initially,	the	set	of	connected	descriptors	is
empty	(lines	5−7),	and	the	listening	descriptor	is	the	only	descriptor	in	the
read	set	(lines	10−12).
The	
function	(
Figure	
12.10
)	adds	a	new	client	to	the	pool
of	active	clients.	After	finding	an	empty	slot	in	the	
array,	the
server	adds	the	connected	descriptor	to	the	array	and	initializes	a
corresponding	R
IO</p>
<p>read	buffer	so	that	we	can	call	
on	the
descriptor	(lines	8−9).	We	then	add	the	connected	descriptor	to	the
read	set	(line	12),	and	we	update	some	global	properties	of	the
pool.	The	
variable	(lines	15−16)	keeps	track	of	the	largest	file
descriptor	for	
.	The	maxi	variable	(lines	17−18)	keeps	track	of	the
largest	index	into	the	
array	so	that	the	
function
does	not	have	to	search	the	entire	array.
The	
function	in	
Figure	
12.11
echoes	a	text	line	from
each	ready	connected	descriptor.	If	we	are	successful	in	reading	a	text
line	from	the	descriptor,	then	we	echo	that	line	back	to	the	client	(lines
15−18).	Notice	that	in	line	15,	we	are	maintaining	a	cumulative	count	of
total	bytes	received	from	all	clients.	If	we	detect	EOF	because	the	client
has	closed	its	end	of	the	connection,	then	we	close	our	end	of	the</p>
<h2>connection	(line	23)	and	remove	the	descriptor	from	the	pool	(lines
24−25).</h2>
<p>code/conc/echoservers.c</p>
<hr />
<h2>code/conc/echoservers.c
Figure	
12.8	
Concurrent	echo	server	based	on	I/O	multiplexing.
Each	server	iteration	echoes	a	text	line	from	each	ready	descriptor.</h2>
<p>code/conc/echoservers.c</p>
<hr />
<h2>code/conc/echoservers.c
Figure	
12.9	
initializes	the	pool	of	active	clients.</h2>
<p>code/conc/echoservers.c</p>
<hr />
<h2>code/conc/echoservers.c
Figure	
12.10	
adds	a	new	client	connection	to	the	pool.</h2>
<p>code/conc/echoservers.c</p>
<hr />
<p>code/conc/echoservers.c
Figure	
12.11	
services	ready	client	connections.</p>
<p>In	terms	of	the	finite	state	model	in	
Figure	
12.7
,	the	
function
detects	input	events,	and	the	
function	creates	a	new	logical
flow	(state	machine).	The	
function	performs	state
transitions	by	echoing	input	lines,	and	it	also	deletes	the	state	machine
when	the	client	has	finished	sending	text	lines.
Practice	Problem	
12.4	
(solution	page	
1036
)
In	the	server	in	
Figure	
12.8
,	we	are	careful	to	reinitialize	the
variable	immediately	before	every	call	to	
.
Why?
Aside	
Event-driven	Web	servers
Despite	the	disadvantages	outlined	in	
Section	
12.2.2
,	modern
high-performance	servers	such	as	Node.js,	nginx,	and	Tornado
use	event-driven	programming	based	on	I/O	multiplexing,	mainly
because	of	the	significant	performance	advantage	compared	to
processes	and	threads.
12.2.2	
Pros	and	Cons	of	I/O
Multiplexing
The	server	in	
Figure	
12.8
provides	a	nice	example	of	the	advantages
and	disadvantages	of	event-driven	programming	based	on	I/O
multiplexing.	One	advantage	is	that	event-driven	designs	give</p>
<p>programmers	more	control	over	the	behavior	of	their	programs	than
process-based	designs.	For	example,	we	can	imagine	writing	an	event-
driven	concurrent	server	that	gives	preferred	service	to	some	clients,
which	would	be	difficult	for	a	concurrent	server	based	on	processes.
Another	advantage	is	that	an	event-driven	server	based	on	I/O
multiplexing	runs	in	the	context	of	a	single	process,	and	thus	every
logical	flow	has	access	to	the	entire	address	space	of	the	process.	This
makes	it	easy	to	share	data	between	flows.	A	related	advantage	of
running	as	a	single	process	is	that	you	can	debug	your	concurrent	server
as	you	would	any	sequential	program,	using	a	familiar	debugging	tool
such	as	
GDB
.	Finally,	event-driven	designs	are	often	significantly	more
efficient	than	process-based	designs	because	they	do	not	require	a
process	context	switch	to	schedule	a	new	flow.
A	significant	disadvantage	of	event-driven	designs	is	coding	complexity.
Our	event-driven	concurrent	echo	server	requires	three	times	more	code
than	the	process-based	server.	Unfortunately,	the	complexity	increases
as	the	granularity	of	the	concurrency	decreases.	By	
granularity
,	we	mean
the	number	of	instructions	that	each	logical	flow	executes	per	time	slice.
For	instance,	in	our	example	concurrent	server,	the	granularity	of
concurrency	is	the	number	of	instructions	required	to	read	an	entire	text
line.	As	long	as	some	logical	flow	is	busy	reading	a	text	line,	no	other
logical	flow	can	make	progress.	This	is	fine	for	our	example,	but	it	makes
our	event-driven	server	vulnerable	to	a	malicious	client	that	sends	only	a
partial	text	line	and	then	halts.	Modifying	an	event-driven	server	to	handle
partial	text	lines	is	a	nontrivial	task,	but	it	is	handled	cleanly	and
automatically	by	a	process-based	design.	Another	significant
disadvantage	of	event-based	designs	is	that	they	cannot	fully	utilize
multi-core	processors.</p>
<p>12.3	
Concurrent	Programming	with
Threads
To	this	point,	we	have	looked	at	two	approaches	for	creating	concurrent
logical	flows.	With	the	first	approach,	we	use	a	separate	process	for	each
flow.	The	kernel	schedules	each	process	automatically,	and	each	process
has	its	own	private	address	space,	which	makes	it	difficult	for	flows	to
share	data.	With	the	second	approach,	we	create	our	own	logical	flows
and	use	I/O	multiplexing	to	explicitly	schedule	the	flows.	Because	there	is
only	one	process,	flows	share	the	entire	address	space.	
This	section
introduces	a	third	approach—based	on	threads—that	is	a	hybrid	of	these
two.
A	
thread
is	a	logical	flow	that	runs	in	the	context	of	a	process.	Thus	far	in
this	book,	our	programs	have	consisted	of	a	single	thread	per	process.
But	modern	systems	also	allow	us	to	write	programs	that	have	multiple
threads	running	concurrently	in	a	single	process.	The	threads	are
scheduled	automatically	by	the	kernel.	Each	thread	has	its	own	
thread
context
,	including	a	unique	integer	
thread	ID	(TID)
,	stack,	stack	pointer,
program	counter,	general-purpose	registers,	and	condition	codes.	All
threads	running	in	a	process	share	the	entire	virtual	address	space	of
that	process.
Logical	flows	based	on	threads	combine	qualities	of	flows	based	on
processes	and	I/O	multiplexing.	Like	processes,	threads	are	scheduled
automatically	by	the	kernel	and	are	known	to	the	kernel	by	an	integer	ID.</p>
<p>Like	flows	based	on	I/O	multiplexing,	multiple	threads	run	in	the	context
of	a	single	process,	and	thus	they	share	the	entire	contents	of	the
process	virtual	address	space,	including	its	code,	data,	heap,	shared
libraries,	and	open	files.
12.3.1	
Thread	Execution	Model
The	execution	model	for	multiple	threads	is	similar	in	some	ways	to	the
execution	model	for	multiple	processes.	Consider	the	example	in	
Figure
12.12
.	Each	process	begins	life	as	a	single	thread	called	the	
main
thread
.	At	some	point,	the	main	thread	creates	a	
peer	thread
,	and	from
this	point	in	time	the	two	threads	run	concurrently.	Eventually,	control
passes	to	the	peer	thread	via	a	context	switch,	either	because	the	main
thread	executes	a	slow	system	call	such	as	
or	
or	because	it
is	interrupted	by	the	system's	interval	timer.	The	peer	thread	executes	for
a	while	before	control	passes	back	to	the	main	thread,	and	so	on.
Thread	execution	differs	from	processes	in	some	important	ways.
Because	a	thread	context	is	much	smaller	than	a	process	context,	a
thread	context	switch	is	faster	than	a	process	context	switch.	Another
difference	is	that	threads,	unlike	processes,	are	not	organized	in	a	rigid
parent-child	hierarchy.	The	threads	associated</p>
<p>Figure	
12.12	
Concurrent	thread	execution.
with	a	process	form	a	
pool
of	peers,	independent	of	which	threads	were
created	by	which	other	threads.	The	main	thread	is	distinguished	from
other	threads	only	in	the	sense	that	it	is	always	the	first	thread	to	run	in
the	process.	The	main	impact	of	this	notion	of	a	pool	of	peers	is	that	a
thread	can	kill	any	of	its	peers	or	wait	for	any	of	its	peers	to	terminate.
Further,	each	peer	can	read	and	write	the	same	shared	data.
12.3.2	
Posix	Threads
Posix	threads	(Pthreads)	is	a	standard	interface	for	manipulating	threads
from	C	programs.	It	was	adopted	in	1995	and	is	available	on	all	Linux
systems.	Pthreads	defines	about	60	functions	that	allow	programs	to
create,	kill,	and	reap	threads,	to	share	data	safely	with	peer	threads,	and
to	notify	peers	about	changes	in	the	system	state.</p>
<h2>Figure	
12.13
shows	a	simple	Pthreads	program.	The	main	thread
creates	a	peer	thread	and	then	waits	for	it	to	terminate.	The	peer	thread
prints	
and	terminates.	When	the	main	thread	detects	that
the	peer	thread	has	terminated,	it	terminates	the	process	by	calling	
.
This	is	the	first	threaded	program	we	have	seen,	so	let	us	dissect	it
carefully.	The	code	and	local	data	for	a	thread	are	encapsulated	in	a
thread	routine
.	As	shown	by	the	prototype	in	line	2,	each	thread	routine
takes	as	input	a	single	generic	pointer	and	returns	a	generic	pointer.	If
you	want	to	pass	multiple	arguments	to	a	thread	routine,	then	you	should
put	the	arguments	into	a	structure	and	pass	a	pointer	to	the	structure.
Similarly,	if	you</h2>
<p>code/conc/hello.c</p>
<hr />
<p>code/conc/hello.c
Figure	
12.13	
:	The	Pthreads	&quot;Hello,	world!&quot;	program.
want	the	thread	routine	to	return	multiple	arguments,	you	can	return	a
pointer	to	a	structure.
Line	4	marks	the	beginning	of	the	code	for	the	main	thread.	The	main
thread	declares	a	single	local	variable	
,	which	will	be	used	to	store
the	thread	ID	of	the	peer	thread	(line	6).	The	main	thread	creates	a	new
peer	thread	by	calling	the	
function	(line	7).	When	the	call
to	
returns,	the	main	thread	and	the	newly	created	peer
thread	are	running	concurrently,	and	
contains	the	ID	of	the	new
thread.	The	main	thread	waits	for	the	peer	thread	to	terminate	with	the
call	to	
in	line	8.	Finally,	the	main	thread	calls	
(line	9),
which	terminates	all	threads	(in	this	case,	just	the	main	thread)	currently
running	in	the	process.
Lines	12−16	define	the	thread	routine	for	the	peer	thread.	It	simply	prints
a	string	and	then	terminates	the	peer	thread	by	executing	the	
statement	in	line	15.
12.3.3	
Creating	Threads</p>
<p>Threads	create	other	threads	by	calling	the	
function.
The	
function	creates	a	new	thread	and	runs	the	
thread
routine</p>
<pre><code>in	the	context	of	the	new	thread	and	with	an	input	argument	of
</code></pre>
<p>.	The	
argument	can	be	used	to	change	the	default	attributes	of
the	newly	created	thread.	Changing	these	attributes	is	beyond	our	scope,
and	in	our	examples,	we	will	always	call	
with	a	NULL
argument.
When	
returns,	argument	
contains	the	ID	of	the	newly
created	thread.	The	new	thread	can	determine	its	own	thread	ID	by
calling	the	
function.</p>
<p>12.3.4	
Terminating	Threads
A	thread	terminates	in	one	of	the	following	ways:
The	thread	terminates	
implicitly
when	its	top-level	thread	routine
returns.
The	thread	terminates	
explicitly
by	calling	the	
function.	If
the	main	thread	calls	
,	it	waits	for	all	other	peer	threads
to	terminate	and	then	terminates	the	main	thread	and	the	entire
process	with	a	return	value	of	
.
Some	peer	thread	calls	the	Linux	
function,	which	terminates	the
process	and	all	threads	associated	with	the	process.
Another	peer	thread	terminates	the	current	thread	by	calling	the
function	with	the	ID	of	the	current	thread.</p>
<p>12.3.5	
Reaping	Terminated	Threads
Threads	wait	for	other	threads	to	terminate	by	calling	the	
function.
The	
function	blocks	until	thread	
terminates,	assigns	the
generic	(
)	pointer	returned	by	the	thread	routine	to	the	location
pointed	to	by	
,	and	then	
reaps
any	memory	resources	held
by	the	terminated	thread.
Notice	that,	unlike	the	Linux	
function,	the	
function	can
only	wait	for	a	specific	thread	to	terminate.	There	is	no	way	to	instruct
to	wait	for	an	
arbitrary
thread	to	terminate.	This	can
complicate	our	code	by	forcing	us	to	use	other,	less	intuitive	mechanisms
to	detect	process	termination.	Indeed,	Stevens	argues	convincingly	that
this	is	a	bug	in	the	specification	[
110
].
12.3.6	
Detaching	Threads</p>
<p>At	any	point	in	time,	a	thread	is	
joinable
or	
detached
.	A	joinable	thread
can	be	reaped	and	killed	by	other	threads.	Its	memory	resources	(such
as	the	stack)	are	not	freed	until	it	is	reaped	by	another	thread.	In	contrast,
a	detached	thread	cannot	
be	reaped	or	killed	by	other	threads.	Its
memory	resources	are	freed	automatically	by	the	system	when	it
terminates.
By	default,	threads	are	created	joinable.	In	order	to	avoid	memory	leaks,
each	joinable	thread	should	be	either	explicitly	reaped	by	another	thread
or	detached	by	a	call	to	the	
function.
The	
function	detaches	the	joinable	thread	
.	Threads
can	detach	themselves	by	calling	
with	an	argument	of
.
Although	some	of	our	examples	will	use	joinable	threads,	there	are	good
reasons	to	use	detached	threads	in	real	programs.	For	example,	a	high-
performance	Web	server	might	create	a	new	peer	thread	each	time	it
receives	a	connection	request	from	a	Web	browser.	Since	each
connection	is	handled	independently	by	a	separate	thread,	it	is
unnecessary—and	indeed	undesirable—for	the	server	to	explicitly	wait
for	each	peer	thread	to	terminate.	In	this	case,	each	peer	thread	should</p>
<p>detach	itself	before	it	begins	processing	the	request	so	that	its	memory
resources	can	be	reclaimed	after	it	terminates.
12.3.7	
Initializing	Threads
The	
function	allows	you	to	initialize	the	state	associated
with	a	thread	routine.
The	
variable	is	a	global	or	static	variable	that	is	always
initialized	to	PTHREAD_ONCE_INIT.	The	first	time	you	call	
with	an	argument	of	
,	it	invokes	
,	which	is	a
function	with	no	input	arguments	that	returns	nothing.	Subsequent	calls
to	
with	the	same	
variable	do	nothing.	The
function	is	useful	whenever	you	need	to	dynamically
initialize	global	variables	that	are	shared	by	multiple	threads.	We	will	look
at	an	example	in	
Section	
12.5.5
.</p>
<h2>12.3.8	
A	Concurrent	Server	Based
on	Threads
Figure	
12.14
shows	the	code	for	a	concurrent	echo	server	based	on
threads.	The	overall	structure	is	similar	to	the	process-based	design.	The
main	thread	repeatedly	waits	for	a	connection	request	and	then	creates	a
peer	thread	to	handle	the	request.	While	the	code	looks	simple,	there	are
a	couple	of	general	and	somewhat	subtle	issues	we	need	to	look	at	more
closely.	The	first	issue	is	how	to	pass</h2>
<p>code/conc/echoservert.c</p>
<hr />
<p>code/conc/echoservert.c
Figure	
12.14	
Concurrent	echo	server	based	on	threads.</p>
<p>the	connected	descriptor	to	the	peer	thread	when	we	call	
.
The	obvious	approach	is	to	pass	a	pointer	to	the	descriptor,	as	in	the
following:
Then	we	have	the	peer	thread	dereference	the	pointer	and	assign	it	to	a
local	variable,	as	follows:
⋮
This	would	be	wrong,	however,	because	it	introduces	a	
race
between	the
assignment	statement	in	the	peer	thread	and	the	
statement	in	the
main	thread.	If	the	assignment	statement	completes	before	the	next
,	then	the	local	
variable	in	the	peer	thread	gets	the	correct
descriptor	value.	However,	if	the	assignment	completes	
after
the	
,
then	the	local	
variable	in	the	peer	thread	gets	the	descriptor
number	of	the	
next
connection.	The	unhappy	result	is	that	two	threads
are	now	performing	input	and	output	on	the	same	descriptor.	In	order	to
avoid	the	potentially	deadly	race,	we	must	assign	each	connected</p>
<p>descriptor	returned	by	
to	its	own	dynamically	allocated	memory
block,	as	shown	in	lines	21−22.	We	will	return	to	the	issue	of	races	in
Section	
12.7.4
.
Another	issue	is	avoiding	memory	leaks	in	the	thread	routine.	Since	we
are	not	explicitly	reaping	threads,	we	must	detach	each	thread	so	that	its
memory	resources	will	be	reclaimed	when	it	terminates	(line	31).	Further,
we	must	be	careful	to	free	the	memory	block	that	was	allocated	by	the
main	thread	(line	32).
Practice	Problem	
12.5	
(solution	page	
1036
)
In	the	process-based	server	in	
Figure	
12.5
,	we	were	careful	to
close	the	connected	descriptor	in	two	places:	the	parent	process
and	the	child	process.	However,	in	the	threads-based	server	in
Figure	
12.14
,	we	only	closed	the	connected	descriptor	in	one
place:	the	peer	thread.	Why?</p>
<h2>12.4	
Shared	Variables	in	Threaded
Programs
From	a	programmer's	perspective,	one	of	the	attractive	aspects	of
threads	is	the	ease	with	which	multiple	threads	can	share	the	same
program	variables.	However,	this	sharing	can	be	tricky.	In	order	to	write
correctly	threaded	programs,	we	must	have	a	clear	understanding	of
what	we	mean	by	sharing	and	how	it	works.
There	are	some	basic	questions	to	work	through	in	order	to	understand
whether	a	variable	in	a	C	program	is	shared	or	not:	(1)	What	is	the
underlying	memory	model	for	threads?	(2)	Given	this	model,	how	are
instances	of	the	variable	mapped	to	memory?	(3)	Finally,	how	many
threads	reference	each	of	these</h2>
<p>code/conc/sharing.c</p>
<hr />
<p>code/conc/sharing.c
Figure	
12.15	
Example	program	that	illustrates	different	aspects	of
sharing.</p>
<p>instances?	The	variable	is	
shared
if	and	only	if	multiple	threads	reference
some	instance	of	the	variable.
To	keep	our	discussion	of	sharing	concrete,	we	will	use	the	program	in
Figure	
12.15
as	a	running	example.	Although	somewhat	contrived,	it
is	nonetheless	useful	to	study	because	it	illustrates	a	number	of	subtle
points	about	sharing.	The	example	program	consists	of	a	main	thread
that	creates	two	peer	threads.	The	main	thread	passes	a	unique	ID	to
each	peer	thread,	which	uses	the	ID	to	print	a	personalized	message
along	with	a	count	of	the	total	number	of	times	that	the	thread	routine	has
been	invoked.
12.4.1	
Threads	Memory	Model
A	pool	of	concurrent	threads	runs	in	the	context	of	a	process.	Each
thread	has	its	own	separate	
thread	context
,	which	includes	a	thread	ID,
stack,	stack	pointer,	
program	counter,	condition	codes,	and	general-
purpose	register	values.	Each	thread	shares	the	rest	of	the	process
context	with	the	other	threads.	This	includes	the	entire	user	virtual
address	space,	which	consists	of	read-only	text	(code),	read/write	data,
the	heap,	and	any	shared	library	code	and	data	areas.	The	threads	also
share	the	same	set	of	open	files.
In	an	operational	sense,	it	is	impossible	for	one	thread	to	read	or	write
the	register	values	of	another	thread.	On	the	other	hand,	any	thread	can
access	any	location	in	the	shared	virtual	memory.	If	some	thread	modifies
a	memory	location,	then	every	other	thread	will	eventually	see	the</p>
<p>change	if	it	reads	that	location.	Thus,	registers	are	never	shared,
whereas	virtual	memory	is	always	shared.
The	memory	model	for	the	separate	thread	stacks	is	not	as	clean.	These
stacks	are	contained	in	the	stack	area	of	the	virtual	address	space	and
are	
usually
accessed	independently	by	their	respective	threads.	We	say
usually
rather	than	
always
,	because	different	thread	stacks	are	not
protected	from	other	threads.	So	if	a	thread	somehow	manages	to
acquire	a	pointer	to	another	thread's	stack,	then	it	can	read	and	write	any
part	of	that	stack.	Our	example	program	shows	this	in	line	26,	where	the
peer	threads	reference	the	contents	of	the	main	thread's	stack	indirectly
through	the	global	
variable.
12.4.2	
Mapping	Variables	to
Memory
Variables	in	threaded	C	programs	are	mapped	to	virtual	memory
according	to	their	storage	classes:
Global	variables.	
A	
global	variable
is	any	variable	declared	outside	of
a	function.	At	run	time,	the	read/write	area	of	virtual	memory	contains
exactly	one	instance	of	each	global	variable	that	can	be	referenced	by
any	thread.	For	example,	the	global	
variable	declared	in	line	5
has	one	run-time	instance	in	the	read/write	area	of	virtual	memory.
When	there	is	only	one	instance	of	a	variable,	we	will	denote	the
instance	by	simply	using	the	variable	name—in	this	case,	
.</p>
<p>Local	automatic	variables.	
A	
local	automatic	variable
is	one	that	is
declared	inside	a	function	without	the	
attribute.	At	run	time,
each	thread's	stack	contains	its	own	instances	of	any	local	automatic
variables.	This	is	true	even	if	multiple	threads	execute	the	same
thread	routine.	For	example,	there	is	one	instance	of	the	local	variable
,	and	it	resides	on	the	stack	of	the	main	thread.	We	will	denote
this	instance	as	
As	another	example,	there	are	two	instances
of	the	local	variable	myid,	one	instance	on	the	stack	of	peer	thread	0
and	the	other	on	the	stack	of	peer	thread	1.	We	will	denote	these
instances	as	
and	
,	respectively.
Local	static	variables.	
A	
local	
variable
is	one	that	is	declared
inside	a	function	with	the	
attribute.	As	with	global	variables,
the	read/write	area	of	virtual	memory	contains	exactly	one	instance	of
each	local	static	
variable	declared	in	a	program.	For	example,	even
though	each	peer	thread	in	our	example	program	declares	
in	line
25,	at	run	time	there	is	only	one	instance	of	
residing	in	the
read/write	area	of	virtual	memory.	Each	peer	thread	reads	and	writes
this	instance.
12.4.3	
Shared	Variables
We	say	that	a	variable	
v
is	
shared
if	and	only	if	one	of	its	instances	is
referenced	by	more	than	one	thread.	For	example,	variable	
in	our
example	program	is	shared	because	it	has	only	one	run-time	instance
and	this	instance	is	referenced	by	both	peer	threads.	On	the	other	hand,
is	not	shared,	because	each	of	its	two	instances	is	referenced	by</p>
<p>exactly	one	thread.	However,	it	is	important	to	realize	that	local	automatic
variables	such	as	
can	also	be	shared.
Practice	Problem	
12.6	
(solution	page	
1036
)
Using	the	analysis	from	
Section	
12.4
,	fill	each	entry	in	the
following	table	with	&quot;Yes&quot;	or	&quot;No&quot;	for	the	example	program	in
Figure	
12.15
.	In	the	first	column,	the	notation	
v.t
denotes	an
instance	of	variable	
v
residing	on	the	local	stack	for	thread	
t
,
where	
t
is	either	
(main	thread),	
(peer	thread	0),	or	
(peer
thread	1).
Variable	instance
Referenced	by
main	thread?
peer	thread	0?
peer	thread	1?</p>
<hr />
<hr />
<hr />
<hr />
<hr />
<hr />
<hr />
<hr />
<hr />
<hr />
<hr />
<hr />
<hr />
<hr />
<hr />
<hr />
<hr />
<hr />
<p>Given	the	analysis	in	part	A,	which	of	the	variables	
,	and	
are	shared?</p>
<h2>12.5	
Synchronizing	Threads	with
Semaphores
Shared	variables	can	be	convenient,	but	they	introduce	the	possibility	of
nasty	
synchronization	errors
.	Consider	the	
program	in	
Figure
12.16
,	which	creates	two	threads,	each	of	which	increments	a	global
shared	counter	variable	called	
.
Since	each	thread	increments	the	counter	niters	times,	we	expect	its	final
value	to	be	
.	This	seems	quite	simple	and	straightforward.
However,	when	we	run	
on	our	Linux	system,	we	not	only	get
wrong	answers,	we	get	different	answers	each	time!</h2>
<p>code/conc/badcnt.c</p>
<hr />
<p>code/conc/badcnt.c
Figure	
12.16	
:	An	improperly	synchronized	counter
program.
So	what	went	wrong?	To	understand	the	problem	clearly,	we	need	to
study	the	assembly	code	for	the	counter	loop	(lines	40−41),	as	shown	in
Figure	
12.17
.	We	will	find	it	helpful	to	partition	the	loop	code	for	thread
i
into	five	parts:
H
:	The	block	of	instructions	at	the	head	of	the	loop
L
:	The	instruction	that	loads	the	shared	variable	
into	the
accumulator	register	%rdx
,	where	%rdx
denotes	the	value	of	register
%rdx	in	thread	
i
i
i
i
i</p>
<p>U
:	The	instruction	that	updates	(increments)	%rdx
S
:	The	instruction	that	stores	the	updated	value	of	
back	to	the
shared	variable	
T
:	The	block	of	instructions	at	the	tail	of	the	loop
Notice	that	the	head	and	tail	manipulate	only	local	stack	variables,	while
L
,	
U
,	and	
S
manipulate	the	contents	of	the	shared	counter	variable.
When	the	two	peer	threads	in	
run	concurrently	on	a
uniprocessor,	the	machine	instructions	are	completed	one	after	the	other
in	some	order.	Thus,	each	concurrent	execution	defines	some	total
ordering	(or	interleaving)	of	the	instructions	in	the	two	threads.
Unfortunately,	some	of	these	orderings	will	produce	correct	results,	but
others	will	not.
Figure	
12.17	
Assembly	code	for	the	counter	loop	(lines	40−41)	in
(a)	Correct	ordering
i
i
i
i
i
i
i</p>
<p>Step
Thread
Instr.
1
1
H
—
—
0
2
1
L
0
—
0
3
1
U
1
—
0
4
1
S
1
—
1
5
2
H
—
—
1
6
2
L
—
1
1
7
2
U
—
2
1
8
2
S
—
2
2
9
2
T
—
2
2
10
1
T
1
—
2
(b)	Incorrect	ordering
Step
Thread
Instr.
1
1
H
—
—
0
2
1
L
0
—
0
3
1
U
1
—
0
4
2
H
—
—
0
5
2
L
—
0
0
1
1
1
1
2
2
2
2
2
1
1
1
1
2
2</p>
<p>6
1
S
1
—
1
7
1
T
1
—
1
8
2
U
—
1
1
9
2
S
—
1
1
10
2
T
—
1
1
Figure	
12.18	
Instruction	orderings	for	the	first	loop	iteration	in
Here	is	the	crucial	point:	
In	general,	there	is	no	way	for	you	to	predict
whether	the	operating	system	will	choose	a	correct	ordering	for	your
threads.
For	example,	
Figure	
12.18(a)
shows	the	step-by-step
operation	of	a	correct	instruction	ordering.	After	each	thread	has	updated
the	shared	variable	cnt,	its	value	in	memory	is	2,	which	is	the	expected
result.
Ontheother	hand,	the	ordering	in	
Figure	
12.18(b)
produces	an
incorrect	value	for	
.	The	problem	occurs	because	thread	2	loads	
in	step	5,	after	thread	1	loads	
in	step	2	but	before	thread	1	stores	its
updated	value	in	step	6.	Thus,	each	thread	ends	up	storing	an	updated
counter	value	of	1.	We	can	clarify	these	notions	of	correct	and	incorrect
instruction	orderings	with	the	help	of	a	device	known	as	a	
progress
graph
,	which	we	introduce	in	the	next	section.
Practice	Problem	
12.7	
(solution
1
1
2
2
2</p>
<p>page	
1037
)
Complete	the	table	for	the	following	instruction	ordering	of
:
Step
Thread
Instr.
1
1
H
—
—
0
2
1
L</p>
<hr />
<hr />
<hr />
<p>3
2
H</p>
<hr />
<hr />
<hr />
<p>4
2
L</p>
<hr />
<hr />
<hr />
<p>5
2
U</p>
<hr />
<hr />
<hr />
<p>6
2
S</p>
<hr />
<hr />
<hr />
<p>7
1
U</p>
<hr />
<hr />
<hr />
<p>Step
Thread
Instr.
8
1
S</p>
<hr />
<hr />
<hr />
<p>9
1
T</p>
<hr />
<hr />
<hr />
<p>10
2
T</p>
<hr />
<hr />
<hr />
<p>Does	this	ordering	result	in	a	correct	value	for	
?
12.5.1	
Progress	Graphs
1
1
2
2
2
2
1
1
1
2</p>
<p>A	
progress	graph
models	the	execution	of	
n
concurrent	threads	as	a
trajectory	through	an	
n
-dimensional	Cartesian	space.	Each	axis	
k
corresponds	to	the	progress	of	thread	
k
.	Each	point	(
I
,	
I
,	.	.	.	,	
I
)
represents	the	state	where	thread	
k
(
k
=	1,	.	.	.	,	
n
)	has	completed
instruction	
I
.	The	origin	of	the	graph	corresponds	to	the	
initial	state
where	none	of	the	threads	has	yet	completed	an	instruction.
Figure	
12.19
shows	the	two-dimensional	progress	graph	for	the	first
loop	iteration	of	the	
program.	The	horizontal	axis	corresponds
to	thread	1,	the	vertical	axis	to	thread	2.	Point	(
L
,	
S
)	corresponds	to	the
state	where	thread	1	has	completed	
L
and	thread	2	has	completed	
S
.
A	progress	graph	models	instruction	execution	as	a	
transition
from	one
state	to	another.	A	transition	is	represented	as	a	directed	edge	from	one
point	to	an	adjacent	point.	Legal	transitions	move	to	the	right	(an
instruction	in	thread	1	completes)	or	up	(an	instruction	in	thread	2
completes).	Two	instructions	cannot	complete	at	the	same	time—
diagonal	transitions	are	not	allowed.	Programs	never	run	backward	so
transitions	that	move	down	or	to	the	left	are	not	legal	either.
1
2
n
k
1
2
1
2</p>
<p>Figure	
12.19	
Progress	graph	for	the	first	loop	iteration	of	
.
Figure	
12.20	
An	example	trajectory.</p>
<p>The	execution	history	of	a	program	is	modeled	as	a	
trajectory
through	the
state	space.	
Figure	
12.20
shows	the	trajectory	that	corresponds	to	the
following	instruction	ordering:
For	thread	
i
,	the	instructions	(
L
,	U
,	S
)	that	manipulate	the	contents	of	the
shared	variable	
constitute	a	
critical	section
(with	respect	to	shared
variable	
)	that	should	not	be	interleaved	with	the	critical	section	of	the
other	thread.	In	other	words,	we	want	to	ensure	that	each	thread	has
mutually	exclusive	access
to	the	shared	variable	while	it	is	executing	the
instructions	in	its	critical	section.	The	phenomenon	in	general	is	known	as
mutual	exclusion
.
On	the	progress	graph,	the	intersection	of	the	two	critical	sections	defines
a	region	of	the	state	space	known	as	an	
unsafe	region
.	
Figure	
12.21
shows	the	unsafe	region	for	the	variable	
.	Notice	that	the	unsafe
region	abuts,	but	does	not	include,	the	states	along	its	perimeter.	For
example,	states	(
H
,	H
)	and	(
S
,	U
)	abut	the	unsafe	region,	but	they	are
not	part	of	it.	A	trajectory	that	skirts	the	unsafe	region	is	known	as	a	
safe
trajectory
.	Conversely,	a	trajectory	that	touches	any	part	of	the	unsafe
region	is	an	
unsafe	trajectory
.	
Figure	
12.21
shows	examples	of	safe
and	unsafe	trajectories	through	the	state	space	of	our	example	
program.	The	upper	trajectory	skirts	the	unsafe	region	along	its	left	and
top	sides,	and	thus	is	safe.	The	lower	trajectory	crosses	the	unsafe
region,	and	thus	is	unsafe.
Any	safe	trajectory	will	correctly	update	the	shared	counter.	In	order	to
guarantee	correct	execution	of	our	example	threaded	program—and
H
1
,
 
L
1
,
 
U
1
,
 
H
2
,
 
L
2
,
 
S
1
,
 
T
1
,
 
U
2
,
 
S
2
,
 
T
2
i
i
i
1
2
1
2</p>
<p>indeed	any	concurrent	program	that	shares	global	data	structures—we
must	somehow	
synchronize
the	threads	so	that	they	always	have	a	safe
trajectory.	A	classic	approach	is	based	on	the	idea	of	a	semaphore,	which
we	introduce	next.
Figure	
12.21	
Safe	and	unsafe	trajectories.
The	intersection	of	the	critical	regions	forms	an	unsafe	region.
Trajectories	that	skirt	the	unsafe	region	correctly	update	the	counter
variable.
Practice	Problem	
12.8	
(solution	page	
1038
)
Using	the	progress	graph	in	
Figure	
12.21
,	classify	the	following
trajectories	as	either	
safe
or	
unsafe
.
A
.	
H
,	
L
,	
U
,	
S
,	
H
,	
L
,	
U
,	
S
,	
T
,	
T
1
1
1
1
2
2
2
2
2
1</p>
<p>B
.	
H
,	
L
,	
H
,	
L
,	
U
,	
S
,	
T
,	
U
,	
S
,	
T
C
.	
H
,	
H
,	
L
,	
U
,	
S
,	
L
,	
U
,	
S
,	
T
,	
T
12.5.2	
Semaphores
Edsger	Dijkstra,	a	pioneer	of	concurrent	programming,	proposed	a
classic	solution	to	the	problem	of	synchronizing	different	execution
threads	based	on	a	special	type	of	variable	called	a	
semaphore
.	A
semaphore,	
s
,	is	a	global	variable	with	a	nonnegative	integer	value	that
can	only	be	manipulated	by	two	special	operations,	called	
P
and	
V
:
P	(s:
If	
s
is	nonzero,	then	
P
decrements	
s
and	returns	immediately.	If
s
is	zero,	then	suspend	the	thread	until	
s
becomes	nonzero	and	the
thread	is	restarted	by	a	
V
operation.	After	restarting,	the	
P
operation
decrements	
s
and	returns	control	to	the	caller.
V	(s):
The	
V
operation	increments	
s
by	1.	If	there	are	any	threads
blocked	at	a	
P
operation	waiting	for	
s
to	become	nonzero,	then	the	
V
operation	restarts	exactly	one	of	these	threads,	which	then	completes
its	
P
operation	by	decrementing	
s
.
Aside	
Origin	of	the	names	
P
and	
V
Edsger	Dijkstra	(1930−2002)	was	originally	from	the	Netherlands.
The	names	
P
and	
V
come	from	the	Dutch	words	
proberen
(to	test)
and	
verhogen
(to	increment).
The	test	and	decrement	operations	in	
P
occur	indivisibly,	in	the	sense
that	once	the	semaphore	
s
becomes	nonzero,	the	decrement	of	
s
occurs
2
2
1
1
1
1
1
2
2
2
1
2
2
2
2
1
1
1
1
2</p>
<p>without	interruption.	The	increment	operation	in	
V
also	occurs	indivisibly,
in	that	it	loads,	increments,	and	stores	the	semaphore	without
interruption.	Notice	that	the	definition	of	
V
does	
not
define	the	order	in
which	waiting	threads	are	restarted.	The	only	requirement	is	that	the	
V
must	restart	exactly	one	waiting	thread.	
Thus,	when	several	threads	are
waiting	at	a	semaphore,	you	cannot	predict	which	one	will	be	restarted	as
a	result	of	the	V.
The	definitions	of	
P
and	
V
ensure	that	a	running	program	can	never	enter
a	state	where	a	properly	initialized	semaphore	has	a	negative	value.	This
property,	known	as	the	
semaphore	invariant
,	provides	a	powerful	tool	for
controlling	the	trajectories	of	concurrent	programs,	as	we	shall	see	in	the
next	section.
The	Posix	standard	defines	a	variety	of	functions	for	manipulating
semaphores.
The	
function	initializes	semaphore	
to	
.	Each
semaphore	must	be	initialized	before	it	can	be	used.	For	our	purposes,
the	middle	argument	is	always	0.	Programs	perform	
P
and	
V
operations</p>
<p>by	calling	the	
and	
functions,	respectively.	For
conciseness,	we	prefer	to	use	the	following	equivalent	
P
and	
V
wrapper
functions	instead:
12.5.3	
Using	Semaphores	for
Mutual	Exclusion
Semaphores	provide	a	convenient	way	to	ensure	mutually	exclusive
access	to	shared	variables.	The	basic	idea	is	to	associate	a	semaphore
s
,	initially	1,	with</p>
<p>Figure	
12.22	
Using	semaphores	for	mutual	exclusion.
The	infeasible	states	where	
s
&lt;	0	define	a	
forbidden	region
that
surrounds	the	unsafe	region	and	prevents	any	feasible	trajectory	from
touching	the	unsafe	region.
each	shared	variable	(or	related	set	of	shared	variables)	and	then
surround	the	corresponding	critical	section	with	
P	(s)
and	
V	(s)
operations.
A	semaphore	that	is	used	in	this	way	to	protect	shared	variables	is	called
a	
binary	semaphore
because	its	value	is	always	0	or	1.	Binary
semaphores	whose	purpose	is	to	provide	mutual	exclusion	are	often
called	
mutexes
.	Performing	a	
P
operation	on	a	mutex	is	called	
locking
the</p>
<p>mutex.	Similarly,	performing	the	
V
operation	is	called	
unlocking
the
mutex.	A	thread	that	has	locked	but	not	yet	unlocked	a	mutex	is	said	to
be	
holding
the	mutex.	A	semaphore	that	is	used	as	a	counter	for	a	set	of
available	resources	is	called	a	
counting	semaphore
.
The	progress	graph	in	
Figure	
12.22
shows	how	we	would	use	binary
semaphores	to	properly	synchronize	our	example	counter	program.
Each	state	is	labeled	with	the	value	of	semaphore	
s
in	that	state.	The
crucial	idea	is	that	this	combination	of	
P
and	
V
operations	creates	a
collection	of	states,	called	a	
forbidden	region
,	where	
s
&lt;	0.	Because	of
the	semaphore	invariant,	no	feasible	trajectory	can	include	one	of	the
states	in	the	forbidden	region.	And	since	the	forbidden	region	completely
encloses	the	unsafe	region,	no	feasible	trajectory	can	touch	any	part	of
the	unsafe	region.	Thus,	every	feasible	trajectory	is	safe,	and	regardless
of	the	ordering	of	the	instructions	at	run	time,	the	program	correctly
increments	the	counter.
Aside	
Limitations	of	progress	graphs
Progress	graphs	give	us	a	nice	way	to	visualize	concurrent
program	execution	on	uniprocessors	and	to	understand	why	we
need	synchronization.	However,	they	do	have	limitations,
particularly	with	respect	to	concurrent	execution	on
multiprocessors,	where	a	set	of	CPU/cache	pairs	share	the	same
main	memory.	Multiprocessors	behave	in	ways	that	cannot	be
explained	by	progress	graphs.	In	particular,	a	multiprocessor
memory	system	can	be	in	a	state	that	does	not	correspond	to	any
trajectory	in	a	progress	graph.	Regardless,	the	message	remains</p>
<p>the	same:	always	synchronize	accesses	to	your	shared	variables,
regardless	if	you're	running	on	a	uniprocessor	or	a	multiprocessor.
In	an	operational	sense,	the	forbidden	region	created	by	the	
P
and	
V
operations	makes	it	impossible	for	multiple	threads	to	be	executing
instructions	in	the	enclosed	critical	region	at	any	point	in	time.	In	other
words,	the	semaphore	operations	ensure	mutually	exclusive	access	to
the	critical	region.
Putting	it	all	together,	to	properly	synchronize	the	example	counter
program	in	
Figure	
12.16
using	semaphores,	we	first	declare	a
semaphore	called	mutex:
and	then	we	initialize	it	to	unity	in	the	main	routine:
Finally,	we	protect	the	update	of	the	shared	
variable	in	the	thread
routine	by	surrounding	it	with	
P
and	
V
operations:</p>
<p>When	we	run	the	properly	synchronized	program,	it	now	produces	the
correct	answer	each	time.
12.5.4	
Using	Semaphores	to
Schedule	Shared	Resources
Another	important	use	of	semaphores,	besides	providing	mutual
exclusion,	is	to	schedule	accesses	to	shared	resources.	In	this	scenario,
a	thread	uses	a	semaphore
Figure	
12.23	
Producer-consumer	problem.</p>
<p>The	producer	generates	items	and	inserts	them	into	a	bounded	buffer.
The	consumer	removes	items	from	the	buffer	and	then	consumes	them.
operation	to	notify	another	thread	that	some	condition	in	the	program
state	has	become	true.	Two	classical	and	useful	examples	are	the
producer-consumer
and	
readers-writers
problems.
Producer-Consumer	Problem
The	
producer-consumer
problem	is	shown	in	
Figure	
12.23
.	A	producer
and	consumer	thread	share	a	
bounded	buffer
with	
n	slots
.	The	producer
thread	repeatedly	produces	new	
items
and	inserts	them	in	the	buffer.	The
consumer	thread	repeatedly	removes	items	from	the	buffer	and	then
consumes	(uses)	them.	Variants	with	multiple	producers	and	consumers
are	also	possible.
Since	inserting	and	removing	items	involves	updating	shared	variables,
we	must	guarantee	mutually	exclusive	access	to	the	buffer.	But
guaranteeing	mutual	exclusion	is	not	sufficient.	We	also	need	to	schedule
accesses	to	the	buffer.	If	the	buffer	is	full	(there	are	no	empty	slots),	then
the	producer	must	wait	until	a	slot	becomes	available.	Similarly,	if	the
buffer	is	empty	(there	are	no	available	items),	then	the	consumer	must
wait	until	an	item	becomes	available.
Producer-consumer	interactions	occur	frequently	in	real	systems.	For
example,	in	a	multimedia	system,	the	producer	might	encode	video
frames	while	the	consumer	decodes	and	renders	them	on	the	screen.
The	purpose	of	the	buffer	is	to	reduce	jitter	in	the	video	stream	caused	by
data-dependent	differences	in	the	encoding	and	decoding	times	for</p>
<h2>individual	frames.	The	buffer	provides	a	reservoir	of	slots	to	the	producer
and	a	reservoir	of	encoded	frames	to	the	consumer.	Another	common
example	is	the	design	of	graphical	user	interfaces.	The	producer	detects
mouse	and	keyboard	events	and	inserts	them	in	the	buffer.	The
consumer	removes	the	events	from	the	buffer	in	some	priority-based
manner	and	paints	the	screen.
In	this	section,	we	will	develop	a	simple	package,	called	S
BUF
,	for	building
producer-consumer	programs.	In	the	next	section,	we	look	at	how	to	use
it	to	build	an	interesting	concurrent	server	based	on	prethreading.	S
BUF
manipulates	bounded	buffers	of	type	
(
Figure	
12.24
).	Items	are
stored	in	a	dynamically	allocated	integer	array	(
)	with	n	items.	The
and	
indices	keep	track	of	the	first	and	last	items	in	the	array.
Three	semaphores	synchronize	access	to	the	buffer.	The	
semaphore	provides	mutually	exclusive	buffer	access.	Semaphores
and	
are	counting	semaphores	that	count	the	number	of
empty	slots	and	available	items,	respectively.</h2>
<p>code/conc/sbuf.h</p>
<hr />
<p>code/conc/sbuf.h
Figure	
12.24	
:	Bounded	buffer	used	by	the	
package.
Figure	
12.25
shows	the	implementation	of	the	S
BUF</p>
<p>package.	The
function	allocates	heap	memory	for	the	buffer,	sets	
and
to	indicate	an	empty	buffer,	and	assigns	initial	values	to	the	three
semaphores.	This	function	is	called	once,	before	calls	to	any	of	the	other
three	functions.	The	
function	frees	the	buffer	storage	when
the	application	is	through	using	it.	The	
function	waits	for	an
available	slot,	locks	the	mutex,	adds	the	item,	unlocks	the	mutex,	and
then	announces	the	availability	of	a	new	item.	The	
function	is
symmetric.	After	waiting	for	an	available	buffer	item,	it	locks	the	mutex,
removes	the	item	from	the	front	of	the	buffer,	unlocks	the	mutex,	and	then
signals	the	availability	of	a	new	slot.
Practice	Problem	
12.9	
(solution	page	
1038
)
Let	
p
denote	the	number	of	producers,	
c
the	number	of
consumers,	and	
n
the	buffer	size	in	units	of	items.	For	each	of	the</p>
<h2>following	scenarios,	indicate	whether	the	mutex	semaphore	in
and	
is	necessary	or	not.
A
.	
p
=	1,	
c
=	1,	
n
&gt;	1
B
.	
p
=	1,	
c
=	1,	
n
=	1
C
.	
p
&gt;	1,	
c
&gt;	1,	
n
=	1
Readers-Writers	Problem
The	
readers-writers	problem
is	a	generalization	of	the	mutual	exclusion
problem.	A	collection	of	concurrent	threads	is	accessing	a	shared	object
such	as	a	data	structure	in	main	memory	or	a	database	on	disk.	Some
threads	only	read	the	object,	while	others	modify	it.	Threads	that	modify
the	object	are	called	
writers
.	Threads	that	only	read	it	are	called	
readers
.
Writers	must	have	exclusive	access	to	the	object,	but	readers	may	share
the	object	with	an	unlimited	number	of	other	readers.	In	general,	there
are	an	unbounded	number	of	concurrent	readers	and	writers.</h2>
<p>code/conc/sbuf.c</p>
<hr />
<p>code/conc/sbuf.c
Figure	
12.25	
:	A	package	for	synchronizing	concurrent	access	to
bounded	buffers.
Readers-writers	interactions	occur	frequently	in	real	systems.	For
example,	in	an	online	airline	reservation	system,	an	unlimited	number	of
customers	are	al-lowed	to	concurrently	inspect	the	seat	assignments,	but
a	customer	who	is	booking	a	seat	must	have	exclusive	access	to	the</p>
<p>database.	As	another	example,	in	a	multithreaded	caching	Web	proxy,	an
unlimited	number	of	threads	can	fetch	existing	pages	from	the	shared
page	cache,	but	any	thread	that	writes	a	new	page	to	the	cache	must
have	exclusive	access.
The	readers-writers	problem	has	several	variations,	each	based	on	the
priorities	of	readers	and	writers.	The	
first	readers-writers	problem
,	which
favors	readers,	requires	that	no	reader	be	kept	waiting	unless	a	writer
has	already	been	granted	permission	to	use	the	object.	In	other	words,
no	reader	should	wait	simply	because	a	writer	is	waiting.	The	
second
readers-writers	problem
,	which	favors	writers,	requires	that	once	a	writer
is	ready	to	write,	it	performs	its	write	as	soon	as	possible.	Unlike	the	first
problem,	a	reader	that	arrives	after	a	writer	must	wait,	even	if	the	writer	is
also	waiting.
Figure	
12.26
shows	a	solution	to	the	first	readers-writers	problem.
Like	the	solutions	to	many	synchronization	problems,	it	is	subtle	and
deceptively	simple.	The	
semaphore	controls	access	to	the	critical
sections	that	access	the	shared	object.	The	mutex	semaphore	protects
access	to	the	shared	
variable,	which	counts	the	number	of
readers	currently	in	the	critical	section.	A	writer	locks	thew	mutex	each
time	it	enters	the	critical	section	and	unlocks	it	each	time	it	leaves.	This
guarantees	that	there	is	at	most	one	writer	in	the	critical	section	at	any
point	in	time.	On	the	other	hand,	only	the	first	reader	to	enter	the	critical
section	locks	
,	and	only	the	last	reader	to	leave	the	critical	section
unlocks	it.	The	
mutex	is	ignored	by	readers	who	enter	and	leave	while
other	readers	are	present.	This	means	that	as	long	as	a	single	reader
holds	the	
mutex,	an	unbounded	number	of	readers	can	enter	the
critical	section	unimpeded.</p>
<p>A	correct	solution	to	either	of	the	readers-writers	problems	can	result	in
starvation
,	where	a	thread	blocks	indefinitely	and	fails	to	make	progress.
For	example,	in	the	solution	in	
Figure	
12.26
,	a	writer	could	wait
indefinitely	while	a	stream	of	readers	arrived.
Practice	Problem	
12.10	
(solution	page	
1038
)
The	solution	to	the	first	readers-writers	problem	in	
Figure	
12.26
gives	priority	to	readers,	but	this	priority	is	weak	in	the	sense	that	a
writer	leaving	its	critical	section	might	restart	a	waiting	writer
instead	of	a	waiting	reader.	Describe	a	scenario	where	this	weak
priority	would	allow	a	collection	of	writers	to	starve	a	reader.
12.5.5	
Putting	It	Together:	A
Concurrent	Server	Based	on
Prethreading
We	have	seen	how	semaphores	can	be	used	to	access	shared	variables
and	to	schedule	accesses	to	shared	resources.	To	help	you	understand
these	ideas	more	clearly,	let	us	apply	them	to	a	concurrent	server	based
on	a	technique	called	
prethreading
.</p>
<p>Figure	
12.26	
Solution	to	the	first	readers-writers	problem.</p>
<p>Favors	readers	over	writers.
In	the	concurrent	server	in	
Figure	
12.14
,	we	created	a	new	thread	for
each	new	client.	A	disadvantage	of	this	approach	is	that	we	incur	the
nontrivial	cost	of	creating	a	new	thread	for	each	new	client.	A	server
based	on	prethreading	tries	to	reduce	this	overhead	by	using	the
producer-consumer	model	shown	in	
Figure	
12.27
.	The	server	consists
of	a	main	thread	and	a	set	of	worker	threads.	The	main	thread	repeatedly
accepts	connection	requests	from	clients	and	places
Aside	
Other	synchronization
mechanisms
We	have	shown	you	how	to	synchronize	threads	using
semaphores,	mainly	because	they	are	simple,	classical,	and	have
a	clean	semantic	model.	But	you	should	know	that	other
synchronization	techniques	exist	as	well.	For	example,	Java
threads	are	synchronized	with	a	mechanism	called	a	
Java	monitor
[
48
],	which	provides	a	higher-level	abstraction	of	the	mutual
exclusion	and	scheduling	capabilities	of	semaphores;	in	fact,
monitors	can	be	implemented	with	semaphores.	As	another
example,	the	Pthreads	interface	defines	a	set	of	synchronization
operations	on	
mutex
and	
condition
variables.	Pthreads	mutexes
are	used	for	mutual	exclusion.	Condition	variables	are	used	for
scheduling	accesses	to	shared	resources,	such	as	the	bounded
buffer	in	a	producer-consumer	program.</p>
<p>Figure	
12.27	
Organization	of	a	prethreaded	concurrent	server.
A	set	of	existing	threads	repeatedly	remove	and	process	connected
descriptors	from	a	bounded	buffer.
the	resulting	connected	descriptors	in	a	bounded	buffer.	Each	worker
thread	repeatedly	removes	a	descriptor	from	the	buffer,	services	the
client,	and	then	waits	for	the	next	descriptor.
Figure	
12.28
shows	how	we	would	use	the	S
BUF</p>
<p>package	to
implement	a	prethreaded	concurrent	echo	server.	After	initializing	buffer
(line	24),	the	main	thread	creates	the	set	of	worker	threads	(lines
25−26).	Then	it	enters	the	infinite	server	loop,	accepting	connection
requests	and	inserting	the	resulting	connected	descriptors	in	
.	Each
worker	thread	has	a	very	simple	behavior.	It	waits	until	it	is	able	to
remove	a	connected	descriptor	from	the	buffer	(line	39)	and	then	calls	the
function	to	echo	client	input.
The	
function	in	
Figure	
12.29
is	a	version	of	the	
function
from	
Figure	
11.22
that	records	the	cumulative	number	of	bytes
received	from	all	clients	in	a	global	variable	called	
.	This	is</p>
<p>interesting	code	to	study	because	it	shows	you	a	general	technique	for
initializing	packages	that	are	called	from	thread	routines.	In	our	case,	we
need	to	initialize	the	
counter	and	the	
semaphore.	One
approach,	which	we	used	for	the	S
BUF</p>
<p>and	R
IO</p>
<h2>packages,	is	to	require	the
main	thread	to	explicitly	call	an	initialization	function.	Another	approach,
shown	here,	uses	the	
function	(line	19)	to	call</h2>
<p>code/conc/echoservert-pre.c</p>
<hr />
<h2>code/conc/echoservert-pre.c
Figure	
12.28	
A	prethreaded	concurrent	echo	server.
The	server	uses	a	producer-consumer	model	with	one	producer	and
multiple	consumers.</h2>
<p>code/conc/echo-cnt.c</p>
<hr />
<p>code/conc/echo-cnt.c
Figure	
12.29	
:	A	version	of	
that	counts	all	bytes
received	from	clients.
the	initialization	function	the	first	time	some	thread	calls	the	
function.	The	advantage	of	this	approach	is	that	it	makes	the	package
easier	to	use.	The	disadvantage	is	that	every	call	to	
makes	a
call	to	
,	which	most	times	does	nothing	useful.
Once	the	package	is	initialized,	the	
function	initializes	the	R
IO
buffered	I/O	package	(line	20)	and	then	echoes	each	text	line	that	is
received	from	the	client.	Notice	that	the	accesses	to	the	shared	
variable	in	lines	23−25	are	protected	by	
P
and	
V
operations.</p>
<p>Aside	
Event-driven	programs	based	on
threads
I/O	multiplexing	is	not	the	only	way	to	write	an	event-driven
program.	For	example,	you	might	have	noticed	that	the	concurrent
prethreaded	server	that	we	just	developed	is	really	an	event-
driven	server	with	simple	state	machines	for	the	main	and	worker
threads.	The	main	thread	has	two	states	(&quot;waiting	for	connection
request&quot;	and	&quot;waiting	for	available	buffer	slot&quot;),	two	I/O	events
(&quot;connection	request	arrives&quot;	and	&quot;buffer	slot	becomes	available&quot;),
and	two	transitions	(&quot;accept	connection	request&quot;	and	&quot;insert	buffer
item&quot;).	Similarly,	each	worker	thread	has	one	state	(&quot;waiting	for
available	buffer	item&quot;),	one	I/O	event	(&quot;buffer	item	becomes
available&quot;),	and	one	transition	(&quot;remove	buffer	item&quot;).
Figure	
12.30	
Relationships	between	the	sets	of	sequential,
concurrent,	and	parallel	programs.</p>
<p>12.6	
Using	Threads	for	Parallelism
Thus	far	in	our	study	of	concurrency,	we	have	assumed	concurrent
threads	exe-cuting	on	uniprocessor	systems.	However,	most	modern
machines	have	multi-core	processors.	Concurrent	programs	often	run
faster	on	such	machines	because	the	operating	system	kernel	schedules
the	concurrent	threads	in	parallel	on	multiple	cores,	rather	than
sequentially	on	a	single	core.	Exploiting	such	parallelism	is	critically
important	in	applications	such	as	busy	Web	servers,	database	servers,
and	large	scientific	codes,	and	it	is	becoming	increasingly	useful	in
mainstream	applications	such	as	Web	browsers,	spreadsheets,	and
document	processors.
Figure	
12.30
shows	the	set	relationships	between	sequential,
concurrent,	and	parallel	programs.	The	set	of	all	programs	can	be
partitioned	into	the	disjoint	sets	of	sequential	and	concurrent	programs.	A
sequential	program	is	written	as	a	single	logical	flow.	A	concurrent
program	is	written	as	multiple	concurrent	flows.	A	parallel	program	is	a
concurrent	program	running	on	multiple	processors.	Thus,	the	set	of
parallel	programs	is	a	proper	subset	of	the	set	of	concurrent	programs.
A	detailed	treatment	of	parallel	programs	is	beyond	our	scope,	but
studying	a	few	simple	example	programs	will	help	you	understand	some
important	aspects	of	parallel	programming.	For	example,	consider	how
we	might	sum	the	sequence	of	integers	0,	.	.	.	,	
n
−	1	in	parallel.	Of
course,	there	is	a	closed-form	solution	for	this	particular	problem,	but</p>
<p>nonetheless	it	is	a	concise	and	easy-to-understand	exemplar	that	will
allow	us	to	make	some	interesting	points	about	parallel	programs.
The	most	straightforward	approach	for	assigning	work	to	different	threads
is	to	partition	the	sequence	into	
t
disjoint	regions	and	then	assign	each	of
t
different	
threads	to	work	on	its	own	region.	For	simplicity,	assume	that	
n
is	a	multiple	of	
t
,	such	that	each	region	has	
n/t
elements.	Let's	look	at
some	of	the	different	ways	that	multiple	threads	might	work	on	their
assigned	regions	in	parallel.
The	simplest	and	most	straightforward	option	is	to	have	the	threads	sum
into	a	shared	global	variable	that	is	protected	by	a	mutex.	
Figure
12.31
shows	how	we	might	implement	this.	In	lines	28−33,	the	main
thread	creates	the	peer	threads	and	then	waits	for	them	to	terminate.
Notice	that	the	main	thread	passes	a	small	integer	to	each	peer	thread
that	serves	as	a	unique	thread	ID.	Each	peer	thread	will	use	its	thread	ID
to	determine	which	portion	of	the	sequence	it	should	work	on.	This	idea
of	passing	a	small	unique	thread	ID	to	the	peer	threads	is	a	general
technique	that	is	used	in	many	parallel	applications.	After	the	peer
threads	have	terminated,	the	global	variable	
contains	the	final	sum.
The	main	thread	then	uses	the	closed-form	solution	to	verify	the	result
(lines	36−37).
Figure	
12.32
shows	the	function	that	each	peer	thread	executes.	In
line	4,	the	thread	extracts	the	thread	ID	from	the	thread	argument	and
then	uses	this	ID	to	determine	the	region	of	the	sequence	it	should	work
on	(lines	5−6).	In	lines	9−13,	the	thread	iterates	over	its	portion	of	the
sequence,	updating	the	shared	global	variable	
on	each	iteration.</p>
<p>Notice	that	we	are	careful	to	protect	each	update	with	
P
and	
V
mutex
operations.
When	we	run	
on	a	system	with	four	cores	on	a	sequence	of
size	
n
=	2
and	measure	its	running	time	(in	seconds)	as	a	function	of
the	number	of	threads,	we	get	a	nasty	surprise:
Number	of	threads
Version
1
2
4
8
16
68
432
719
552
599
Not	only	is	the	program	extremely	slow	when	it	runs	sequentially	as	a
single	thread,	it	is	nearly	an	order	of	magnitude	slower	when	it	runs	in
parallel	as	multiple	threads.	And	the	performance	gets	worse	as	we	add
more	cores.	The	reason	for	this	poor	performance	is	that	the
synchronization	operations	(
P
and	
V
)	are	very	expensive	relative	to	the
cost	of	a	single	memory	update.	This	highlights	an	important	lesson
about	parallel	programming:	
Synchronization	overhead	is	expensive	and
should	be	avoided	if	possible.	If	it	cannot	be	avoided,	the	overhead
should	be	amortized	by	as	much	useful	computation	as	possible.
One	way	to	avoid	synchronization	in	our	example	program	is	to	have
each	peer	thread	compute	its	partial	sum	in	a	private	variable	that	is	not
shared	with	any	other	thread,	as	shown	in	
Figure	
12.33
.	The	main
thread	(not	shown)	defines	a	global	array	called	
,	and	each	peer
thread	
i
accumulates	its	partial	sum	in	
.	Since	we	are	careful	to
give	each	peer	thread	a	unique	memory	location	to	update,	it	is	not
necessary	to	protect	these	updates	with	mutexes.	The	only	necessary
31</p>
<h2>synchronization	is	that	the	main	thread	must	wait	for	all	of	the	children	to
finish.	After	the	peer	threads	have	terminated,	the	main	thread	sums	up
the	elements	of	the	
vector	to	arrive	at	the	final	result.</h2>
<p>code/conc/psum-mutex.c</p>
<hr />
<h2>code/conc/psum-mutex.c
Figure	
12.31	
Main	routine	for	
.
Uses	multiple	threads	to	sum	the	elements	of	a	sequence	into	a	shared
global	variable	protected	by	a	mutex.</h2>
<p>code/conc/psum-mutex.c</p>
<hr />
<h2>code/conc/psum-mutex.c
Figure	
12.32	
Thread	routine	for	
.
Each	peer	thread	sums	into	a	shared	global	variable	protected	by	a
mutex.</h2>
<p>code/conc/psum-array.c</p>
<hr />
<p>code/conc/psum-array.c
Figure	
12.33	
Thread	routine	for	
.
Each	peer	thread	accumulates	its	partial	sum	in	a	private	array	element
that	is	not	shared	with	any	other	peer	thread.
When	we	run	
on	our	four-core	system,	we	see	that	it	runs
orders	of	magnitude	faster	than	
:
Number	of	threads</p>
<h2>Version
1
2
4
8
16
68.00
432.00
719.00
552.00
599.00
7.26
3.64
1.91
1.85
1.84
In	
Chapter	
5
,	we	learned	how	to	use	local	variables	to	eliminate
unnecessary	memory	references.	
Figure	
12.34
shows	how	we	can
apply	this	principle	by	having	each	peer	thread	accumulate	its	partial	sum
into	a	local	variable	rather	than	a	global	variable.	When	we	run	
on	our	four-core	machine,	we	get	another	order-of-magnitude
decrease	in	running	time:
Number	of	threads
Version
1
2
4
8
16
68.00
432.00
719.00
552.00
599.00
7.26
3.64
1.91
1.85
1.84
1.06
0.54
0.28
0.29
0.30</h2>
<p>code/conc/psum-local.c</p>
<hr />
<p>code/conc/psum-local.c
Figure	
12.34	
Thread	routine	for	
.
Each	peer	thread	accumulates	its	partial	sum	in	a	local	variable.</p>
<p>Figure	
12.35	
Performance	of	
(
Figure	
12.34
).
Summing	a	sequence	of	2
elements	using	four	processor	cores.
An	important	lesson	to	take	away	from	this	exercise	is	that	writing	parallel
programs	is	tricky.	Seemingly	small	changes	to	the	code	have	a
significant	impact	on	performance.
Characterizing	the	Performance	of	Parallel
Programs
Figure	
12.35
plots	the	total	elapsed	running	time	of	the	
program	in	
Figure	
12.34
as	a	function	of	the	number	of	threads.	In
each	case,	the	program	runs	on	a	system	with	four	processor	cores	and
sums	a	sequence	of	
n
=	2
elements.	We	see	that	running	time
decreases	as	we	increase	the	number	of	threads,	up	to	four	threads,	at
which	point	it	levels	off	and	even	starts	to	increase	a	little.
In	the	ideal	case,	we	would	expect	the	running	time	to	decrease	linearly
with	the	number	of	cores.	That	is,	we	would	expect	running	time	to	drop
by	half	each	time	we	double	the	number	of	threads.	This	is	indeed	the
case	until	we	reach	the	point	(
t
&gt;	4)	where	each	of	the	four	cores	is	busy
running	at	least	one	thread.	Running	time	actually	increases	a	bit	as	we
increase	the	number	of	threads	because	of	the	overhead	of	context
switching	multiple	threads	on	the	same	core.	For	this	reason,	parallel
programs	are	often	written	so	that	each	core	runs	exactly	one	thread.
Although	absolute	running	time	is	the	ultimate	measure	of	any	program's
performance,	there	are	some	useful	relative	measures	that	can	provide
31
31</p>
<h1>insight	into	how	well	a	parallel	program	is	exploiting	potential	parallelism.
The	
speedup
of	a	parallel	program	is	typically	defined	as
where	
p
is	the	number	of	processor	cores	and	
T
is	the	running	time	on	
k
cores.	This	formulation	is	sometimes	referred	to	as	
strong	scaling.
When
T
is	the	execution
Threads	(
t
)
1
2
4
8
16
Cores	(
p
)
1
2
4
4
4
Running	time	(
T
)
1.06
0.54
0.28
0.29
0.30
Speedup	(
S
)
1
1.9
3.8
3.7
3.5
Efficiency	(
E
)
100%
98%
95%
91%
88%
Figure	
12.36	
Speedup	and	parallel	efficiency	for	the	execution	times
in	
Figure	
12.35
.
time	of	a	sequential	version	of	the	program,	then	
S
is	called	the	
absolute
speedup
.	When	
T
is	the	execution	time	of	the	parallel	version	of	the
program	running	on	one	core,	then	
S
is	called	the	
relative	speedup
.
Absolute	speedup	is	a	truer	measure	of	the	benefits	of	parallelism	than
relative	speedup.	Parallel	programs	often	suffer	from	synchronization
overheads,	even	when	they	run	on	one	processor,	and	these	overheads
can	artificially	inflate	the	relative	speedup	numbers	because	they
increase	the	size	of	the	numerator.	On	the	other	hand,	absolute	speedup
S
p</h1>
<p>T
1
T
p
k
1
p
p
p
1
p</p>
<h1>is	more	difficult	to	measure	than	relative	speedup	because	measuring
absolute	speedup	requires	two	different	versions	of	the	program.	For
complex	parallel	codes,	creating	a	separate	sequential	version	might	not
be	feasible,	either	because	the	code	is	too	complex	or	because	the
source	code	is	not	available.
A	related	measure,	known	as	
efficiency
,	is	defined	as
and	is	typically	reported	as	a	percentage	in	the	range	(0,	100].	Efficiency
is	a	measure	of	the	overhead	due	to	parallelization.	Programs	with	high
efficiency	are	spending	more	time	doing	useful	work	and	less	time
synchronizing	and	communicating	than	programs	with	low	efficiency.
Figure	
12.36
shows	the	different	speedup	and	efficiency	measures	for
our	example	parallel	sum	program.	Efficiencies	over	90	percent	such	as
these	are	very	good,	but	do	not	be	fooled.	We	were	able	to	achieve	high
efficiency	because	our	problem	was	trivially	easy	to	parallelize.	In
practice,	this	is	not	usually	the	case.	Parallel	programming	has	been	an
active	area	of	research	for	decades.	With	the	advent	of	commodity	multi-
core	machines	whose	core	count	is	doubling	every	few	years,	parallel
programming	continues	to	be	a	deep,	difficult,	and	active	area	of
research.
There	is	another	view	of	speedup,	known	as	
weak	scaling
,	which
increases	the	problem	size	along	with	the	number	of	processors,	such
that	the	amount	of	work	performed	on	each	processor	is	held	constant	as
the	number	of	processors	increases.	With	this	formulation,	speedup	and
efficiency	are	expressed	in	terms	of	the	total	amount	of	work
E
p</h1>
<h1>S
p
p</h1>
<p>T
1
p
T
p</p>
<p>accomplished	per	unit	time.	For	example,	if	we	can	double	the	number	of
processors	and	do	twice	the	amount	of	work	per	hour,	then	we	are
enjoying	linear	speedup	and	100	percent	efficiency.
Weak	scaling	is	often	a	truer	measure	than	strong	scaling	because	it
more	accurately	reflects	our	desire	to	use	bigger	machines	to	do	more
work.	This	is	particularly	true	for	scientific	codes,	where	the	problem	size
can	be	easily	increased	and	where	bigger	problem	sizes	translate	directly
to	better	predictions	of	nature.	However,	there	exist	applications	whose
sizes	are	not	so	easily	increased,	and	for	these	applications	strong
scaling	is	more	appropriate.	For	example,	the	amount	of	work	performed
by	real-time	signal-processing	applications	is	often	determined	by	the
properties	of	the	physical	sensors	that	are	generating	the	signals.
Changing	the	total	amount	of	work	requires	using	different	physical
sensors,	which	might	not	be	feasible	or	necessary.	For	these
applications,	we	typically	want	to	use	parallelism	to	accomplish	a	fixed
amount	of	work	as	quickly	as	possible.
Practice	Problem	
12.11	
(solution	page	
1038
)
Fill	in	the	blanks	for	the	parallel	program	in	the	following	table.	Assume
strong	scaling.
Threads	(
t
)
1
2
4
Cores	(
p
)
1
2
4
Running	time	(
T
)
12
8
6
Speedup	(
S
p
)</p>
<hr />
<p>1.5</p>
<hr />
<p>p</p>
<p>Efficiency	(
E
)
100%</p>
<hr />
<p>50%
p</p>
<p>12.7	
Other	Concurrency	Issues
You	probably	noticed	that	life	got	much	more	complicated	once	we	were
asked	to	synchronize	accesses	to	shared	data.	So	far,	we	have	looked	at
techniques	for	mutual	exclusion	and	producer-consumer	synchronization,
but	this	is	only	the	tip	of	the	iceberg.	Synchronization	is	a	fundamentally
difficult	problem	that	raises	issues	that	simply	do	not	arise	in	ordinary
sequential	programs.	This	section	is	a	survey	(by	no	means	complete)	of
some	of	the	issues	you	need	to	be	aware	of	when	you	write	concurrent
programs.	To	keep	things	concrete,	we	will	couch	our	discussion	in	terms
of	threads.	Keep	in	mind,	however,	that	these	are	typical	of	the	issues
that	arise	when	concurrent	flows	of	any	kind	manipulate	shared
resources.
12.7.1	
Thread	Safety
When	we	program	with	threads,	we	must	be	careful	to	write	functions	that
have	a	property	called	thread	safety.	A	function	is	said	to	be	
thread-safe
if
and	only	if	it	will	always	produce	correct	results	when	called	repeatedly
from	multiple	concurrent	threads.	If	a	function	is	not	thread-safe,	then	we
say	it	is	
thread-unsafe
.
We	can	identify	four	(nondisjoint)	classes	of	thread-unsafe	functions:</p>
<h2>Class	1:	
Functions	that	do	not	protect	shared	variables.
We	have
already	encountered	this	problem	with	the	thread	function	in	
Figure
12.16
,	which</h2>
<h2 id="codeconcrandc"><a class="header" href="#codeconcrandc">code/conc/rand.c</a></h2>
<p>code/conc/rand.c
Figure	
12.37	
A	thread-unsafe	pseudorandom	number	generator.
(Based	on	[
61
])
increments	an	unprotected	global	counter	variable.	This	class	of
thread-unsafe	functions	is	relatively	easy	to	make	thread-safe:	protect</p>
<h2>the	shared	variables	with	synchronization	operations	such	as	
P
and
V
.	An	advantage	is	that	it	does	not	require	any	changes	in	the	calling
program.	A	disadvantage	is	that	the	synchronization	operations	slow
down	the	function.
Class	2:	
Functions	that	keep	state	across	multiple	invocations.
A
pseudorandom	number	generator	is	a	simple	example	of	this	class	of
thread-unsafe	functions.	Consider	the	pseudorandom	number
generator	package	in	
Figure	
12.37
.
The	
function	is	thread-unsafe	because	the	result	of	the	current
invocation	depends	on	an	intermediate	result	from	the	previous
iteration.	When	we	call	
repeatedly	from	a	single	thread	after
seeding	it	with	a	call	to	
,	we	can	expect	a	repeatable	sequence
of	numbers.	However,	this	assumption	no	longer	holds	if	multiple
threads	are	calling	
.
The	only	way	to	make	a	function	such	as	
thread-safe	is	to
rewrite	it	so	that	it	does	not	use	any	
data,	relying	instead	on
the	caller	to	pass	the	state	information	in	arguments.	The
disadvantage	is	that	the	programmer	is	now	forced	to	change	the
code	in	the	calling	routine	as	well.	In	a	large	program	where	there	are
potentially	hundreds	of	different	call	sites,	making	such	modifications
could	be	nontrivial	and	prone	to	error.
Class	3:	
Functions	that	return	a	pointer	to	a	static	variable.
Some
functions,	such	as	
and	
,	compute	a	result	in	a
variable	and	then	return	a	pointer	to	that	variable.	If	we	call
such	functions	from</h2>
<p>code/conc/ctime-ts.c</p>
<hr />
<p>code/conc/ctime-ts.c
Figure	
12.38	
Thread-safe	wrapper	function	for	the	C	standard
library	
function.
This	example	uses	the	lock-and-copy	technique	to	call	a	class	3
thread-unsafe	function.
concurrent	threads,	then	disaster	is	likely,	as	results	being	used	by
one	thread	are	silently	overwritten	by	another	thread.
There	are	two	ways	to	deal	with	this	class	of	thread-unsafe	functions.
One	option	is	to	rewrite	the	function	so	that	the	caller	passes	the
address	of	the	variable	in	which	to	store	the	results.	This	eliminates	all
shared	data,	but	it	requires	the	programmer	to	have	access	to	the
function	source	code.</p>
<p>If	the	thread-unsafe	function	is	difficult	or	impossible	to	modify	(e.g.,
the	code	is	very	complex	or	there	is	no	source	code	available),	then
another	option	is	to	use	the	
lock-and-copy
technique.	The	basic	idea
is	to	associate	a	mutex	with	the	thread-unsafe	function.	At	each	call
site,	lock	the	mutex,	call	the	thread-unsafe	function,	copy	the	result
returned	by	the	function	to	a	private	memory	location,	and	then	unlock
the	mutex.	To	minimize	changes	to	the	caller,	you	should	define	a
thread-safe	wrapper	function	that	performs	the	lock-and-copy	and
then	replace	all	calls	to	the	thread-unsafe	function	with	calls	to	the
wrapper.	For	example,	
Figure	
12.38
shows	a	thread-safe	wrapper
for	
that	uses	the	lock-and-copy	technique.
Class	4:	
Functions	that	call	thread-unsafe	functions.
If	a	function	
f
calls	a	thread-unsafe	function	
g
,	is	
f
thread-unsafe?	It	depends.	If	
g
is
a	class	2	function	that	relies	on	state	across	multiple	invocations,	then
f
is	also	thread-unsafe	and	there	is	no	recourse	short	of	rewriting	
g
.
However,	if	
g
is	a	class	1	or	class	3	function,	then	
f
can	still	be	thread-
safe	if	you	protect	the	call	site	and	any	resulting	shared	data	with	a
mutex.	We	see	a	good	example	of	this	in	
Figure	
12.38
,	where	we
use	lock-and-copy	to	write	a	thread-safe	function	that	calls	a	thread-
unsafe	function.
Figure	
12.39	
Relationships	between	the	sets	of	reentrant,	thread-
safe,	and	thread-unsafe	functions.</p>
<hr />
<h2 id="codeconcrand-rc"><a class="header" href="#codeconcrand-rc">code/conc/rand-r.c</a></h2>
<p>code/conc/rand-r.c
Figure	
12.40	
:	A	reentrant	version	of	the	
function	from
Figure	
12.37
.
12.7.2	
Reentrancy
There	is	an	important	class	of	thread-safe	functions,	known	as	
reentrant
functions
,	that	are	characterized	by	the	property	that	they	do	not
reference	
any
shared	data	when	they	are	called	by	multiple	threads.
Although	the	terms	
thread-safe
and	
reentrant
are	sometimes	used
(incorrectly)	as	synonyms,	there	is	a	clear	technical	distinction	that	is
worth	preserving.	
Figure	
12.39
shows	the	set	relationships	between
reentrant,	thread-safe,	and	thread-unsafe	functions.	The	set	of	all
functions	is	partitioned	into	the	disjoint	sets	of	thread-safe	and	thread-
unsafe	functions.	The	set	of	reentrant	functions	is	a	proper	subset	of	the
thread-safe	functions.</p>
<p>Reentrant	functions	are	typically	more	efficient	than	non-reentrant	thread-
safe	functions	because	they	require	no	synchronization	operations.
Furthermore,	the	only	way	to	convert	a	class	2	thread-unsafe	function
into	a	thread-safe	one	is	to	rewrite	it	so	that	it	is	reentrant.	For	example,
Figure	
12.40
shows	a	reentrant	version	of	the	
function	from
Figure	
12.37
.	The	key	idea	is	that	we	have	replaced	the	static	
variable	with	a	pointer	that	is	passed	in	by	the	caller.
Is	it	possible	to	inspect	the	code	of	some	function	and	declare	a	priori
that	it	is	reentrant?	Unfortunately,	it	depends.	If	all	function	arguments	are
passed	by	value	(i.e.,	no	pointers)	and	all	data	references	are	to	local
automatic	stack	variables	(i.e.,	no	references	to	static	or	global
variables),	then	the	function	is	
explicitly	reentrant
,	in	the	sense	that	we
can	assert	its	reentrancy	regardless	of	how	it	is	called.
However,	if	we	loosen	our	assumptions	a	bit	and	allow	some	parameters
in	our	otherwise	explicitly	reentrant	function	to	be	passed	by	reference
(i.e.,	we	allow	them	to	pass	pointers),	then	we	have	an	
implicitly	reentrant
function,	in	the	sense	that	it	is	only	reentrant	if	the	calling	threads	are
careful	to	pass	pointers	
to	nonshared	data.	For	example,	the	
function	in	
Figure	
12.40
is	implicitly	reentrant.
We	always	use	the	term	
reentrant
to	include	both	explicit	and	implicit
reentrant	functions.	However,	it	is	important	to	realize	that	reentrancy	is
sometimes	a	property	of	both	the	caller	and	the	callee,	and	not	just	the
callee	alone.
Practice	Problem	
12.12	
(solution	page</p>
<p>1038
)
The	
function	in	
Figure	
12.38
is	thread-safe	but	not
reentrant.	Explain.
12.7.3	
Using	Existing	Library
Functions	in	Threaded	Programs
Most	Linux	functions,	including	the	functions	defined	in	the	standard	C
library	(such	as	
,	and	
),	are	thread-
safe,	with	only	a	few	exceptions.	
Figure	
12.41
lists	some	common
exceptions.	(See	[
110
]	for	a	complete	list.)	The	
function	is	a
deprecated	function	(one	whose	use	is	discouraged)	for	parsing	strings.
The	
,	and	
functions	are	popular	functions	for
converting	back	and	forth	between	different	time	and	date	formats.	The
,	and	
functions	are	obsolete
network	programming	functions	that	have	been	replaced	by	the	reentrant
getaddrinfo,	
,	and	
functions,	respectively	(see
Chapter	
11
).	With	the	exceptions	of	
and	
,	they	are	of	the
class	3	variety	that	return	a	pointer	to	a	static	variable.	If	we	need	to	call
one	of	these	functions	in	a	threaded	program,	the	least	disruptive
approach	to	the	caller	is	to	lock	and	copy.	However,	the	lock-and-copy
approach	has	a	number	of	disadvantages.	First,	the	additional
synchronization	slows	down	the	program.	Second,	functions	that	return
pointers	to	complex	structures	of	structures	require	a	
deep	copy
of	the
structures	in	order	to	copy	the	entire	structure	hierarchy.	Third,	the	lock-</p>
<p>and-copy	approach	will	not	work	for	a	class	2	thread-unsafe	function
such	as	
that	relies	on	static	state	across	calls.
Thread-unsafe	function
Thread-unsafe	class
Linux	thread-safe	version
2
2
3
3
3
3
3
(none)
3
Figure	
12.41	
Common	thread-unsafe	library	functions.
Therefore,	Linux	systems	provide	reentrant	versions	of	most	thread-
unsafe	functions.	The	names	of	the	reentrant	versions	always	end	with
the	
suffix.	For	example,	the	reentrant	version	of	
is	called
.	We	recommend	using	these	functions	whenever	possible.
12.7.4	
Races</p>
<h2>A	
race
occurs	when	the	correctness	of	a	program	depends	on	one	thread
reaching	point	
x
in	its	control	flow	before	another	thread	reaches	point	
y
.
Races	usually	occur	because	programmers	assume	that	threads	will	take
some	particular	trajectory	through	the	execution	state	space,	forgetting
the	golden	rule	that	threaded	programs	must	work	correctly	for	any
feasible	trajectory.
An	example	is	the	easiest	way	to	understand	the	nature	of	races.
Consider	the	simple	program	in	
Figure	
12.42
.	The	main	thread
creates	four	peer	threads	and	passes	a	pointer	to	a	unique	integer	ID	to
each	one.	Each	peer	thread	copies	the</h2>
<p>code/conc/race.c</p>
<hr />
<p>code/conc/race.c
Figure	
12.42	
program	with	a	race.
ID	passed	in	its	argument	to	a	local	variable	(line	22)	and	then	prints	a
message	containing	the	ID.	It	looks	simple	enough,	but	when	we	run	this
program	on	our	system,	we	get	the	following	incorrect	result:
The	problem	is	caused	by	a	race	between	each	peer	thread	and	the	main
thread.	Can	you	spot	the	race?	Here	is	what	happens.	When	the	main</p>
<p>thread	creates	a	peer	thread	in	line	13,	it	passes	a	pointer	to	the	local
stack	variable	
i
.	At	this	point,	the	race	is	on	between	the	next	increment
of	
in	line	12	and	the	dereferencing	and	assignment	of	the	argument	in
line	22.	If	the	peer	thread	executes	line	22	before	the	main	thread
increments	
in	line	12,	then	the	
variable	gets	the	correct	ID.
Otherwise,	it	will	contain	the	ID	of	some	other	thread.	The	scary	thing	is
that	whether	we	get	the	correct	answer	depends	on	how	the	kernel
schedules	the	execution	of	the	threads.	On	our	system	it	fails,	but	on
other	systems	it	might	work	correctly,	leaving	the	programmer	blissfully
unaware	of	a	serious	bug.
To	eliminate	the	race,	we	can	dynamically	allocate	a	separate	block	for
each	integer	ID	and	pass	the	thread	routine	a	pointer	to	this	block,	as
shown	in	
Figure	
12.43
(lines	12−14).	Notice	that	the	thread	routine
must	free	the	block	in	order	to	avoid	a	memory	leak.
When	we	run	this	program	on	our	system,	we	now	get	the	correct	result:
Practice	Problem	
12.13	
(solution	page
1039
)</p>
<h2>In	
Figure	
12.43
,	we	might	be	tempted	to	free	the	allocated
memory	block	immediately	after	line	14	in	the	main	thread,	instead
of	freeing	it	in	the	peer	thread.	But	this	would	be	a	bad	idea.	Why?
Practice	Problem	
12.14	
(solution	page
1039
)
1
.	
In	
Figure	
12.43
,	we	eliminated	the	race	by	allocating	a	separate
block	for	each	integer	ID.	Outline	a	different	approach	that	does
not	call	the	
or	
functions.
2
.	
What	are	the	advantages	and	disadvantages	of	this	approach?</h2>
<p>code/conc/norace.c</p>
<hr />
<p>code/conc/norace.c
Figure	
12.43
A	correct	version	of	the	program	in	
Figure	
12.42
without	a	race.
12.7.5	
Deadlocks
Semaphores	introduce	the	potential	for	a	nasty	kind	of	run-time	error,
called	
deadlock
,	where	a	collection	of	threads	is	blocked,	waiting	for	a
condition	that	will	never	be	true.	The	progress	graph	is	an	invaluable	tool
for	understanding	deadlock.	For	example,	
Figure	
12.44
shows	the</p>
<p>progress	graph	for	a	pair	of	threads	that	use	two	semaphores	for	mutual
exclusion.	From	this	graph,	we	can	glean	some	important	insights	about
deadlock:
The	programmer	has	incorrectly	ordered	the	
P
and	
V
operations	such
that	the	forbidden	regions	for	the	two	semaphores	overlap.	If	some
execution	trajectory	happens	to	reach	the	
deadlock	state	d
,	then	no
further	progress	is
Figure	
12.44	
Progress	graph	for	a	program	that	can	deadlock.</p>
<p>possible	because	the	overlapping	forbidden	regions	block	progress	in
every	legal	direction.	In	other	words,	the	program	is	deadlocked
because	each	thread	is	waiting	for	the	other	to	do	a	
V
operation	that
will	never	occur.
The	overlapping	forbidden	regions	induce	a	set	of	states	called	the
deadlock	region
.	If	a	trajectory	happens	to	touch	a	state	in	the
deadlock	region,	then	deadlock	is	inevitable.	Trajectories	can	enter
deadlock	regions,	but	they	can	never	leave.
Deadlock	is	an	especially	difficult	issue	because	it	is	not	always
predictable.	Some	lucky	execution	trajectories	will	skirt	the	deadlock
region,	while	others	will	be	trapped	by	it.	
Figure	
12.44
shows	an
example	of	each.	The	implications	for	a	programmer	are	scary.	You
might	run	the	same	program	a	thousand	times	without	any	problem,
but	then	the	next	time	it	deadlocks.	Or	the	program	might	work	fine	on
one	machine	but	deadlock	on	another.	Worst	of	all,	the	error	is	often
not	repeatable	because	different	executions	have	different
trajectories.
Programs	deadlock	for	many	reasons,	and	preventing	them	is	a	difficult
problem	in	general.	However,	when	binary	semaphores	are	used	for
mutual	exclusion,	as	in	
Figure	
12.44
,	then	you	can	apply	the	following
simple	and	effective	rule	to	prevent	deadlocks:</p>
<p>Figure	
12.45	
Progress	graph	for	a	deadlock-free	program.
Mutex	lock	ordering	rule:	
Given	a	total	ordering	of	all	mutexes,	a
program	is	deadlock-free	if	each	thread	acquires	its	mutexes	in	order
and	releases	them	in	reverse	order.
For	example,	we	can	fix	the	deadlock	in	
Figure	
12.44
by	locking	
s
first,
then	
t
,	in	each	thread.	
Figure	
12.45
shows	the	resulting	progress
graph.
Practice	Problem	
12.15	
(solution	page</p>
<p>1039
)
Consider	the	following	program,	which	attempts	to	use	a	pair	of
semaphores	for	mutual	exclusion.
A
.	
Draw	the	progress	graph	for	this	program.
B
.	
Does	it	always	deadlock?
C
.	
If	so,	what	simple	change	to	the	initial	semaphore	values
will	eliminate	the	potential	for	deadlock?
D
.	
Draw	the	progress	graph	for	the	resulting	deadlock-free
program.</p>
<p>12.8	
Summary
A	concurrent	program	consists	of	a	collection	of	logical	flows	that	overlap
in	time.	In	this	chapter,	we	have	studied	three	different	mechanisms	for
building	concurrent	programs:	processes,	I/O	multiplexing,	and	threads.
We	used	a	concurrent	network	server	as	the	motivating	application
throughout.
Processes	are	scheduled	automatically	by	the	kernel,	and	because	of
their	separate	virtual	address	spaces,	they	require	explicit	IPC
mechanisms	in	order	to	share	data.	Event-driven	programs	create	their
own	concurrent	logical	flows,	which	are	modeled	as	state	machines,	and
use	I/O	multiplexing	to	explicitly	schedule	the	flows.	Because	the
program	runs	in	a	single	process,	sharing	data	between	flows	is	fast	and
easy.	Threads	are	a	hybrid	of	these	approaches.	Like	flows	based	on
processes,	threads	are	scheduled	automatically	by	the	kernel.	Like	flows
based	on	I/O	multiplexing,	threads	run	in	the	context	of	a	single	process,
and	thus	can	share	data	quickly	and	easily.
Regardless	of	the	concurrency	mechanism,	synchronizing	concurrent
accesses	to	shared	data	is	a	difficult	problem.	The	
P
and	
V
operations	on
semaphores	have	been	developed	to	help	deal	with	this	problem.
Semaphore	operations	can	be	used	to	provide	mutually	exclusive	access
to	shared	data,	as	well	as	to	schedule	access	to	resources	such	as	the
bounded	buffers	in	producer-consumer	systems	and	shared	objects	in
readers-writers	systems.	A	concurrent	prethreaded	echo	server	provides
a	compelling	example	of	these	usage	scenarios	for	semaphores.</p>
<p>Concurrency	introduces	other	difficult	issues	as	well.	Functions	that	are
called	by	threads	must	have	a	property	known	as	thread	safety.	We	have
identified	four	classes	of	thread-unsafe	functions,	along	with	suggestions
for	making	them	thread-safe.	Reentrant	functions	are	the	proper	subset
of	thread-safe	functions	that	do	not	access	any	shared	data.	Reentrant
functions	are	often	more	efficient	than	non-reentrant	functions	because
they	do	not	require	any	synchronization	primitives.	Some	other	difficult
issues	that	arise	in	concurrent	programs	are	races	and	dead	locks.
Races	occur	when	programmers	make	incorrect	assumptions	about	how
logical	flows	are	scheduled.	Deadlocks	occur	when	a	flow	is	waiting	for
an	event	that	will	never	happen.</p>
<p>Bibliographic	Notes
Semaphore	operations	were	introduced	by	Dijkstra	[
31
].	The	progress
graph	concept	was	introduced	by	Coffman	[
23
]	and	later	formalized	by
Carson	and	Reynolds	[
16
].	The	readers-writers	problem	was	introduced
by	Courtois	et	al	[
25
].	Operating	systems	texts	describe	classical
synchronization	problems	such	as	the	dining	philosophers,	sleeping
barber,	and	cigarette	smokers	problems	in	more	detail	
[
102
,	
106
,	
113
].
The	book	by	Butenhof	[
15
]	is	a	comprehensive	description	of	the	Posix
threads	interface.	The	paper	by	Birrell	[
7
]	is	an	excellent	introduction	to
threads	programming	and	its	pitfalls.	The	book	by	Reinders	[
90
]
describes	a	C/C++	library	that	simplifies	the	design	and	implementation
of	threaded	programs.	Several	texts	cover	the	fundamentals	of	parallel
programming	on	multi-core	systems	[
47
,	
71
].	Pugh	identifies	weaknesses
with	the	way	that	Java	threads	interact	through	memory	and	proposes
replacement	memory	models	[
88
].	Gustafson	proposed	the	weak-scaling
speedup	model	[
43
]	as	an	alternative	to	strong	scaling.</p>
<h2>Homework	Problems
12.16	
♦
Write	a	version	of	
(
Figure	
12.13
)	that	creates	and	reaps	
n
joinable	peer	threads,	where	
n
is	a	command-line	argument.
12.17	
♦
A
.	
The	program	in	
Figure	
12.46
has	a	bug.	The	thread	is
supposed	to	sleep	for	1	second	and	then	print	a	string.	However,
when	we	run	it	on	our	system,	nothing	prints.	Why?
B
.	
You	can	fix	this	bug	by	replacing	the	exit	function	in	line	10	with
one	of	two	different	Pthreads	function	calls.	Which	ones?</h2>
<p>code/conc/hellobug.c</p>
<hr />
<p>code/conc/hellobug.c
Figure	
12.46	
Buggy	program	for	
Problem	
12.17
.
12.18
Using	the	progress	graph	in	
Figure	
12.21
,	classify	the	following
trajectories	as	either	safe	or	unsafe.
A
.	
H
,	
L
,	
U
,	
H
,	
L
,	
S
,	
U
,	
S
,	
T
,	
T
B
.	
H
,	
H
,	
L
,	
U
,	
S
,	
L
,	
T
,	
U
,	
S
,	
T
C
.	
H
,	
L
,	
H
,	
L
,	
U
,	
S
,	
U
,	
S
,	
T
,	
T
2
2
2
1
1
2
1
1
1
2
2
1
1
1
1
2
1
2
2
2
1
1
2
2
2
2
1
1
1
2</p>
<p>12.19	
♦♦
The	solution	to	the	first	readers-writers	problem	in	
Figure	
12.26
gives
a	somewhat	weak	priority	to	readers	because	a	writer	leaving	its	critical
section	might	restart	a	waiting	writer	instead	of	a	waiting	reader.	Derive	a
solution	that	gives	stronger	priority	to	readers,	where	a	writer	leaving	its
critical	section	will	always	restart	a	waiting	reader	if	one	exists.
12.20	
♦♦♦
Consider	a	simpler	variant	of	the	readers-writers	problem	where	there	are
at	most	
N
readers.	Derive	a	solution	that	gives	equal	priority	to	readers
and	writers,	in	the	sense	that	pending	readers	and	writers	have	an	equal
chance	of	being	granted	access	to	the	resource.	
Hint:
You	can	solve	this
problem	using	a	single	counting	semaphore	and	a	single	mutex.
12.21	
♦♦♦♦
Derive	a	solution	to	the	second	readers-writers	problem,	which	favors
writers	instead	of	readers.
12.22	
♦♦</p>
<p>Test	your	understanding	of	the	
function	by	modifying	the	server	in
Figure	
12.6
so	that	it	echoes	at	most	one	text	line	per	iteration	of	the
main	server	loop.
12.23	
♦♦
The	event-driven	concurrent	echo	server	in	
Figure	
12.8
is	flawed
because	a	malicious	client	can	deny	service	to	other	clients	by	sending	a
partial	text	line.	Write	an	improved	version	of	the	server	that	can	handle
these	partial	text	lines	without	blocking.
12.24	
♦
The	functions	in	the	R
IO</p>
<p>I/O	package	(
Section	
10.5
)	are	thread-safe.
Are	they	reentrant	as	well?
12.25	
♦
In	the	prethreaded	concurrent	echo	server	in	
Figure	
12.28
,	each
thread	calls	the	
function	(
Figure	
12.29
).	Is	
thread-
safe?	Is	it	reentrant?	Why	or	why	not?
12.26	
♦♦♦</p>
<p>Use	the	lock-and-copy	technique	to	implement	a	thread-safe	non-
reentrant	version	of	
called	
.	A	correct
solution	will	use	a	deep	copy	of	the	
structure	protected	by	a
mutex.
12.27	
♦♦
Some	network	programming	texts	suggest	the	following	approach	for
reading	and	writing	sockets:	Before	interacting	with	the	client,	open	two
standard	I/O	streams	on	the	same	open	connected	socket	descriptor,	one
for	reading	and	one	for	writing:
When	the	server	finishes	interacting	with	the	client,	close	both	streams	as
follows:</p>
<p>However,	if	you	try	this	approach	in	a	concurrent	server	based	on
threads,	you	will	create	a	deadly	race	condition.	Explain.
12.28	
♦
In	
Figure	
12.45
,	does	swapping	the	order	of	the	two	
V
operations
have	any	effect	on	whether	or	not	the	program	deadlocks?	Justify	your
answer	by	drawing	the	progress	graphs	for	the	four	possible	cases:
Case1
Case2
Case3
Case	4
Thread
1
Thread
2
Thread
1
Thread
2
Thread
1
Thread
2
Thread
1
Thread
2
12.29	
♦
Can	the	following	program	deadlock?	Why	or	why	not?</p>
<p>12.30	
♦
Consider	the	following	program	that	deadlocks.</p>
<h2>A
.	
For	each	thread,	list	the	pairs	of	mutexes	that	it	holds
simultaneously.
B
.	
If	
a	&lt;	b	&lt;	c
,	which	threads	violate	the	mutex	lock	ordering	rule?
C
.	
For	these	threads,	show	a	new	lock	ordering	that	guarantees
freedom	from	deadlock.
12.31	
♦♦♦
Implement	a	version	of	the	standard	I/O	
function,	called	
,
that	times	out	and	returns	
if	it	does	not	receive	an	input	line	on
standard	input	within	5	seconds.	Your	function	should	be	implemented	in
a	package	called	
using	processes,	signals,	and	nonlocal
jumps.	It	should	not	use	the	Linux	alarm	function.	Test	your	solution	using
the	driver	program	in	
Figure	
12.47
.</h2>
<p>code/conc/tfgets-main.c</p>
<hr />
<p>code/conc/tfgets-main.c
Figure	
12.47	
Driver	program	for	Problems	12.31−12.33.
12.32	
♦♦♦
Implement	a	version	of	the	
function	from	
Problem	
12.31
that
uses	the	
function.	Your	function	should	be	implemented	in	a
package	called	
Test	your	solution	using	the	driver
program	from	
Problem	
12.31
.	You	may	assume	that	standard	input	is
assigned	to	descriptor	0.
12.33	
♦♦♦
Implement	a	threaded	version	of	the	
function	from	
Problem
12.31
.	Your	function	should	be	implemented	in	a	package	called
Test	your	solution	using	the	driver	program	from
Problem	
12.31
.</p>
<p>12.34	
♦♦♦
Write	a	parallel	threaded	version	of	an	
N
×	
M
matrix	multiplication	kernel.
Compare	the	performance	to	the	sequential	case.
12.35	
♦♦♦
Implement	a	concurrent	version	of	the	T
INY</p>
<p>Web	server	based	on
processes.	Your	solution	should	create	a	new	child	process	for	each	new
connection	request.	Test	your	solution	using	a	real	Web	browser.
12.36	
♦♦♦
Implement	a	concurrent	version	of	the	T
INY</p>
<p>Web	server	based	on	I/O
multiplexing.	Test	your	solution	using	a	real	Web	browser.
12.37	
♦♦♦
Implement	a	concurrent	version	of	the	T
INY</p>
<p>Web	server	based	on	threads.
Your	solution	should	create	a	new	thread	for	each	new	connection
request.	Test	your	solution	using	a	real	Web	browser.</p>
<p>12.38	
♦♦♦♦
Implement	a	concurrent	prethreaded	version	of	the	T
INY</p>
<p>Web	server.	Your
solution	should	dynamically	increase	or	decrease	the	number	of	threads
in	response	to	the	current	load.	One	strategy	is	to	double	the	number	of
threads	when	the	buffer	becomes	full,	and	halve	the	number	of	threads
when	the	buffer	becomes	empty.	Test	your	solution	using	a	real	Web
browser.
12.39	
♦♦♦♦
A	Web	proxy	is	a	program	that	acts	as	a	middleman	between	a	Web
server	and	browser.	Instead	of	contacting	the	server	directly	to	get	a	Web
page,	the	browser	contacts	the	proxy,	which	forwards	the	request	to	the
server.	When	the	server	replies	to	the	proxy,	the	proxy	sends	the	reply	to
the	browser.	For	this	lab,	you	will	write	a	simple	Web	proxy	that	filters	and
logs	requests:
A
.	
In	the	first	part	of	the	lab,	you	will	set	up	the	proxy	to	accept
requests,	parse	the	HTTP,	forward	the	requests	to	the	server,	and
return	the	results	to	the	browser.	Your	proxy	should	log	the	URLs
of	all	requests	in	a	log	file	on	disk,	and	it	should	also	block
requests	to	any	URL	contained	in	a	filter	file	on	disk.
B
.	
In	the	second	part	of	the	lab,	you	will	upgrade	your	proxy	to	deal
with	multiple	open	connections	at	once	by	spawning	a	separate
thread	to	handle	each	request.	While	your	proxy	is	waiting	for	a
remote	server	to	respond	to	a	request	so	that	it	can	serve	one</p>
<p>browser,	it	should	be	working	on	a	pending	request	from	another
browser.
Check	your	proxy	solution	using	a	real	Web	browser.</p>
<p>Solutions	to	Practice	Problems
Solution	to	Problem	
12.1	
(page
975
)
When	the	parent	forks	the	child,	it	gets	a	copy	of	the	connected
descriptor,	and	the	reference	count	for	the	associated	file	table	is
incremented	from	1	to	2.	When	the	parent	closes	its	copy	of	the
descriptor,	the	reference	count	is	decremented	from	2	to	1.	Since	the
kernel	will	not	close	a	file	until	the	reference	counter	in	its	file	table	goes
to	0,	the	child's	end	of	the	connection	stays	open.
Solution	to	Problem	
12.2	
(page
975
)
When	a	process	terminates	for	any	reason,	the	kernel	closes	all	open
descriptors.	Thus,	the	child's	copy	of	the	connected	file	descriptor	will	be
closed	automatically	when	the	child	exits.
Solution	to	Problem	
12.3	
(page</p>
<p>980
)
Recall	that	a	descriptor	is	ready	for	reading	if	a	request	to	read	1	byte
from	that	descriptor	would	not	block.	If	EOF	becomes	true	on	a
descriptor,	then	the	descriptor	is	ready	for	reading	because	the	read
operation	will	return	immediately	with	a	zero	return	code	indicating	EOF.
Thus,	typing	Ctrl+D	causes	the	
function	to	return	with	descriptor	0
in	the	ready	set.
Solution	to	Problem	
12.4	
(page
984
)
We	reinitialize	the	
variable	before	every	call	to	
because	it	serves	as	both	an	input	and	output	argument.	On	input,	it
contains	the	read	set.	On	output,	it	contains	the	ready	set.
Solution	to	Problem	
12.5	
(page
992
)
Since	threads	run	in	the	same	process,	they	all	share	the	same
descriptor	table.	No	matter	how	many	threads	use	the	connected
descriptor,	the	reference	count	for	the	connected	descriptor's	file	table	is
equal	to	1.	Thus,	a	single	
operation	is	sufficient	to	free	the	memory</p>
<p>resources	associated	with	the	connected	descriptor	when	we	are	through
with	it.
Solution	to	Problem	
12.6	
(page
995
)
The	main	idea	here	is	that	stack	variables	are	private,	whereas	global
and	static	variables	are	shared.	Static	variables	such	as	
are	a	little
tricky	because	the	sharing	is	limited	to	the	functions	within	their	scope—
in	this	case,	the	thread	routine.
A
.	
Here	is	the	table:
Variable	instance
Referenced	by
main	thread?
peer	thread	0?
peer	thread	1?
yes
yes
yes
no
yes
yes
yes
no
no
yes
yes
yes
no
yes
no
no
no
yes
Notes:</p>
<pre><code>A	global	variable	that	is	written	by	the	main	thread	and
</code></pre>
<p>read	by	the	peer	threads.
A	static	variable	with	only	one	instance	in	memory	that	is
read	and	written	by	the	two	peer	threads.
A	local	automatic	variable	stored	on	the	stack	of	the	main
thread.	Even	though	its	value	is	passed	to	the	peer	threads,
the	peer	threads	never	reference	it	on	the	stack,	and	thus	it	is
not	shared.
A	local	automatic	variable	stored	on	the	main	thread's
stack	and	referenced	indirectly	through	
by	both	peer
threads.
and	
Instances	of	a	local	automatic	variable
residing	on	the	stacks	of	peer	threads	0	and	1,	respectively.
B
.	
Variables	
,	and	
are	referenced	by	more	than	one
thread	and	thus	are	shared.
Solution	to	Problem	
12.7	
(page
998
)
The	important	idea	here	is	that	you	cannot	make	any	assumptions	about
the	ordering	that	the	kernel	chooses	when	it	schedules	your	threads.
Step
Thread
Instr.
1
2</p>
<p>1
1
H
—
—
0
2
1
L
0
—
0
3
2
H
—
—
0
4
2
L
—
0
0
5
2
U
—
1
0
6
2
S
—
1
1
7
1
U
1
—
1
8
1
S
1
—
1
9
1
T
1
—
1
10
2
T
—
1
1
Variable	
has	a	final	incorrect	value	of	1.
Solution	to	Problem	
12.8	
(page
1001
)
This	problem	is	a	simple	test	of	your	understanding	of	safe	and	unsafe
trajectories	in	progress	graphs.	Trajectories	such	as	A	and	C	that	skirt	the
critical	region	are	safe	and	will	produce	correct	results.
A
.	
H
,	
L
,	
U
,	
S
,	
H
,	
L
,	
U
,	
S
,	
T
,	
T
:	safe
1
1
2
2
2
2
1
1
1
2
1
1
1
1
2
2
2
2
2
1</p>
<p>B
.	
H
,	
L
,	
H
,	
L
,	
U
,	
S
,	
T
,	
U
,	
S
,	
T
:	unsafe
C
.	
H
,	
H
,	
L
,	
U
,	
S
,	
L
,	
U
,	
S
,	
T
,	
T
:	safe
Solution	to	Problem	
12.9	
(page
1006
)
A
.	
p
=	1,	
c
=	1,	
n
&gt;	1:	Yes,	the	mutex	semaphore	is	necessary
because	the	producer	and	consumer	can	concurrently	access	the
buffer.
B
.	
p
=	1,	
c
=	1,	
n
=	1:	No,	the	mutex	semaphore	is	not	necessary	in
this	case,	because	a	nonempty	buffer	is	equivalent	to	a	full	buffer.
When	the	buffer	contains	an	item,	the	producer	is	blocked.	When
the	buffer	is	empty,	the	consumer	is	blocked.	So	at	any	point	in
time,	only	a	single	thread	can	access	the	buffer,	and	thus	mutual
exclusion	is	guaranteed	without	using	the	mutex.
C
.	
p
&gt;	1,	
c
&gt;	1,	
n
=	1:	No,	the	mutex	semaphore	is	not	necessary	in
this	case	either,	by	the	same	argument	as	the	previous	case.
Solution	to	Problem	
12.10	
(page
1008
)
Suppose	that	a	particular	semaphore	implementation	uses	a	LIFO	stack
of	threads	for	each	semaphore.	When	a	thread	blocks	on	a	semaphore	in
a	
P
operation,	its	ID	is	pushed	onto	the	stack.	Similarly,	the	
V
operation
2
2
1
1
1
1
1
2
2
2
1
2
2
2
2
1
1
1
1
2</p>
<p>pops	the	top	thread	ID	from	the	stack	and	restarts	that	thread.	Given	this
stack	implementation,	an	adversarial	writer	in	its	critical	section	could
simply	wait	until	another	writer	blocks	on	the	semaphore	before	releasing
the	semaphore.	In	this	scenario,	a	waiting	reader	might	wait	forever	as
two	writers	passed	control	back	and	forth.
Notice	that	although	it	might	seem	more	intuitive	to	use	a	FIFO	queue
rather	than	a	LIFO	stack,	using	such	a	stack	is	not	incorrect	and	does	not
violate	the	semantics	of	the	
P
and	
V
operations.
Solution	to	Problem	
12.11	
(page
1020
)
This	problem	is	a	simple	sanity	check	of	your	understanding	of	speedup
and	parallel	efficiency:
Threads	(
t
)
1
2
4
Cores	(
p
)
1
2
4
Running	time	(
T
)
12
8
6
Speedup	(
S
)
1
1.5
2
Efficiency	(
E
)
100%
75%
50%
p
p
p</p>
<p>Solution	to	Problem	
12.12	
(page
1024
)
The	
function	is	not	reentrant,	because	each	invocation	shares
the	same	
variable	returned	by	the	
function.	However,	it	is
thread-safe	because	
the	accesses	to	the	shared	variable	are	protected
by	
P
and	
V
operations,	and	thus	are	mutually	exclusive.
Solution	to	Problem	
12.13	
(page
1026
)
If	we	free	the	block	immediately	after	the	call	to	
in	line	14,
then	we	will	introduce	a	new	race,	this	time	between	the	call	to	
in
the	main	thread	and	the	assignment	statement	in	line	24	of	the	thread
routine.
Solution	to	Problem	
12.14	
(page
1026
)
A
.	
Another	approach	is	to	pass	the	integer	
directly,	rather	than
passing	a	pointer	to	
:</p>
<p>In	the	thread	routine,	we	cast	the	argument	back	to	an	
and
assign	it	to	
:
B
.	
The	advantage	is	that	it	reduces	overhead	by	eliminating	the	calls
to	
and	
.	A	significant	disadvantage	is	that	it	assumes
that	pointers	are	at	least	as	large	as	
.	While	this	assumption
is	true	for	all	modern	systems,	it	might	not	be	true	for	legacy	or
future	systems.
Solution	to	Problem	
12.15	
(page
1029
)
A
.	
The	progress	graph	for	the	original	program	is	shown	in	
Figure
12.48
on	the	next	page.
B
.	
The	program	always	deadlocks,	since	any	feasible	trajectory	is
eventually	trapped	in	a	deadlock	state.
C
.	
To	eliminate	the	deadlock	potential,	initialize	the	binary	semaphore
to	1	instead	of	0.
D
.	
The	progress	graph	for	the	corrected	program	is	shown	in	
Figure
12.49
.</p>
<p>Figure	
12.48	
Progress	graph	for	a	program	that	deadlocks.</p>
<p>Figure	
12.49	
Progress	graph	for	the	corrected	deadlock-free
program.</p>
<p>Appendix	
A	
Error	Handling
Programmers	should	
always
check	the	error	codes	returned	by	system-
level	functions.	There	are	many	subtle	ways	that	things	can	go	wrong,
and	it	only	makes	sense	to	use	the	status	information	that	the	kernel	is
able	to	provide	us.	Unfortunately,	programmers	are	often	reluctant	to	do
error	checking	because	it	clutters	their	code,	turning	a	single	line	of	code
into	a	multi-line	conditional	statement.	Error	checking	is	also	confusing
because	different	functions	indicate	errors	in	different	ways.
We	were	faced	with	a	similar	problem	when	writing	this	text.	On	the	one
hand,	we	would	like	our	code	examples	to	be	concise	and	simple	to	read.
On	the	other	hand,	we	do	not	want	to	give	students	the	wrong	impression
that	it	is	OK	to	skip	error	checking.	To	resolve	these	issues,	we	have
adopted	an	approach	based	on	
error-handling	wrappers
that	was
pioneered	by	W.	Richard	Stevens	in	his	network	programming	text	[110].
The	idea	is	that	given	some	base	system-level	function	
,	we	define	a
wrapper	function	
with	identical	arguments,	but	with	the	first	letter
capitalized.	The	wrapper	calls	the	base	function	and	checks	for	errors.	If
it	detects	an	error,	the	wrapper	prints	an	informative	message	and
terminates	the	process.	Otherwise,	it	returns	to	the	caller.	Notice	that	if
there	are	no	errors,	the	wrapper	behaves	exactly	like	the	base	function.
Put	another	way,	if	a	program	runs	correctly	with	wrappers,	it	will	run
correctly	if	we	render	the	first	letter	of	each	wrapper	in	lowercase	and
recompile.</p>
<p>The	wrappers	are	packaged	in	a	single	source	file	(
)	that	is
compiled	and	linked	into	each	program.	A	separate	header	file	(
)
contains	the	function	prototypes	for	the	wrappers.
This	appendix	gives	a	tutorial	on	the	different	kinds	of	error	handling	in
Unix	systems	and	gives	examples	of	the	different	styles	of	error-handling
wrappers.	Copies	of	the	
and	
files	are	available	at	the
CS:APP	Web	site.</p>
<p>A.1	
Error	Handling	in	Unix	Systems
The	systems-level	function	calls	that	we	will	encounter	in	this	book	use
three	different	styles	for	returning	errors:	
Unix-style
,	
Posix-style
,	and	
GAI-
style
.
Unix-Style	Error	Handling
Functions	such	as	
and	
that	were	developed	in	the	early	days
of	Unix	(as	well	as	some	older	Posix	functions)	overload	the	function
return	value	with	both	error	codes	
and
useful	results.	For	example,	when
the	Unix-style	
function	encounters	an	error	(e.g.,	there	is	no	child
process	to	reap),	it	returns	-1	and	sets	the	global	variable	
to	an
error	code	that	indicates	the	cause	of	the	error.	If	
completes
successfully,	then	it	returns	the	useful	result,	which	is	the	PID	of	the
reaped	child.	Unix-style	error-handling	code	is	typically	of	the	following
form:</p>
<p>The	
function	returns	a	text	description	for	a	particular	value	of
.
Posix-Style	Error	Handling
Many	of	the	newer	Posix	functions	such	as	Pthreads	use	the	return	value
only	to	indicate	success	(zero)	or	failure	(nonzero).	Any	useful	results	are
returned	in	function	arguments	that	are	passed	by	reference.	We	refer	to
this	approach	as	
Posix-style	error	handling
.	For	example,	the	Posix-style
function	indicates	success	or	failure	with	its	return	value
and	returns	the	ID	of	the	newly	created	thread	(the	useful	result)	by
reference	in	its	first	argument.	Posix-style	error-handling	code	is	typically
of	the	following	form:
The	
function	returns	a	text	description	for	a	particular	value	of
.</p>
<p>GAI-Style	Error	Handling
The	
(GAI)	and	
functions	return	zero	on	success
and	a	nonzero	value	on	failure.	GAI	error-handling	code	is	typically	of	the
following	form:
The	
function	returns	a	text	description	for	a	particular	value
of	
.
Summary	of	Error-Reporting
Functions
Thoughout	this	book,	we	use	the	following	error-reporting	functions	to
accommodate	different	error-handling	styles.</p>
<p>As	their	names	suggest,	the	
,	and	
functions	report	Unix-style,	Posix-style,	and	GAI-style	errors	and	then
terminate.	The	
function	is	included	as	a	convenience	for
application	errors.	It	simply	prints	its	input	and	then	terminates.	
Figure
A.1
shows	the	code	for	the	error-reporting	functions.</p>
<h2>A.2	
Error-Handling	Wrappers
Here	are	some	examples	of	the	different	error-handling	wrappers.
Unix-style	error-handling	wrappers.	
Figure	
A.2
shows	the
wrapper	for	the	Unix-style	
function.	If	the	
returns	with	an
error,	the	wrapper	prints	an	informative	message	and	then	exits.
Otherwise,	it	returns	a	PID	to	the	caller.	
Figure	
A.3
shows	the
wrapper	for	the	Unix-style	kill	function.	Notice	that	this	function,	unlike
,	returns	void	on	success.
Posix-style	error-handling	wrappers.	
Figure	
A.4
shows	the
wrapper	for	the	Posix-style	
function.	Like	most	Posix-
style	functions,	it	does	not	overload	useful	results	with	error-return
codes,	so	the	wrapper	returns	void	on	success.
GAI-style	error-handling	wrappers.	
Figure	
A.5
shows	the	error-
handling	wrapper	for	the	GAI-style	
function.</h2>
<p>code/src/csapp.c</p>
<hr />
<h2>code/src/csapp.c
Figure	
A.1	
Error-reporting	functions.</h2>
<p>code/src/csapp.c</p>
<hr />
<h2>code/src/csapp.c
Figure	
A.2	
Wrapper	for	Unix-style	
function.</h2>
<h2 id="codesrccsappc"><a class="header" href="#codesrccsappc">code/src/csapp.c</a></h2>
<p>code/src/csapp.c</p>
<h2>Figure	
A.3	
Wrapper	for	Unix-style	
function.</h2>
<h2 id="codesrccsappc-1"><a class="header" href="#codesrccsappc-1">code/src/csapp.c</a></h2>
<h2>code/src/csapp.c
Figure	
A.4	
Wrapper	for	Posix-style	
function.</h2>
<p>code/src/csapp.c</p>
<hr />
<p>code/src/csapp.c
Figure	
A.5	
Wrapper	for	GAI-style	
function.</p>
<p>References
[1]	
Advanced	Micro	Devices,	Inc.	
Software	Optimization	Guide	for
AMD64	Processors
,	2005.	Publication	Number	25112.
[2]	
Advanced	Micro	Devices,	Inc.	
AMD64	Architecture	Programmer's
Manual,	Volume	1:	Application	Programming
,	2013.	Publication
Number	24592.
[3]	
Advanced	Micro	Devices,	Inc.	
AMD64	Architecture	Programmer's
Manual,	Volume	3:	General-Purpose	and	System	Instructions
,	2013.
Publication	Number	24594.
[4]	
Advanced	Micro	Devices,	Inc.	
AMD64	Architecture	Programmer's
Manual,	Volume	4:	128-Bit	and	256-Bit	Media	Instructions
,	2013.
Publication	Number	26568.
[5]	
K.	Arnold,	J.	Gosling,	and	D.	Holmes.	
The	Java	Programming
Language,	Fourth	Edition
.	Prentice	Hall,	2005.
[6]	
T.	Berners-Lee,	R.	Fielding,	and	H.	Frystyk.	Hypertext	transfer
protocol	-	HTTP/1.0.	RFC	1945,	1996.
[7]	
A.	Birrell.	An	introduction	to	programming	with	threads.	Technical
Report	35,	Digital	Systems	Research	Center,	1989.</p>
<p>[8]	
A.	Birrell,	M.	Isard,	C.	Thacker,	and	T.	Wobber.	A	design	for	high-
performance	flash	disks.	
SIGOPS	Operating	Systems	Review
41(2):88–93,	2007.
[9]	
G.	E.	Blelloch,	J.	T.	Fineman,	P.	B.	Gibbons,	and	H.	V.	Simhadri.
Scheduling	irregular	parallel	computations	on	hierarchical	caches.	In
Proceedings	of	the	23rd	Symposium	on	Parallelism	in	Algorithms	and
Architectures	(SPAA)
,	pages	355–366.	ACM,	June	2011.
[10]	
S.	Borkar.	Thousand	core	chips:	A	technology	perspective.	In
Proceedings	of	the	44th	Design	Automation	Conference
,	pages	746–
749.	ACM,	2007.
[11]	
D.	Bovet	and	M.	Cesati.	
Understanding	the	Linux	Kernel,	Third
Edition
.	O'Reilly	Media,	Inc.,	2005.
[12]	
A.	Demke	Brown	and	T.	Mowry.	Taming	the	memory	hogs:	Using
compiler-inserted	releases	to	manage	physical	memory	intelligently.
In	
Proceedings	of	the	4th	Symposium	on	Operating	Systems	Design
and	Implementation	(OSDI)
,	pages	31–44.	Usenix,	October	2000.
[13]	
R.	E.	Bryant.	Term-level	verification	of	a	pipelined	CISC
microprocessor.	Technical	Report	CMU-CS-05–195,	Carnegie	Mellon
University,	School	of	Computer	Science,	2005.
[14]	
R.	E.	Bryant	and	D.	R.	O'Hallaron.	Introducing	computer	systems
from	a	programmer's	perspective.	In	
Proceedings	of	the	Technical
Symposium	on	Computer	Science	Education	(SIGCSE)
,	pages	90–
94.	ACM,	February	2001.</p>
<p>[15]	
D.	Butenhof.	
Programming	with	Posix	Threads
.	Addison-Wesley,
1997.
[16]	
S.	Carson	and	P.	Reynolds.	The	geometry	of	semaphore	programs.
ACM	Transactions	on	Programming	Languages	and	Systems
9(1):25–53,	1987.
[17]	
J.	B.	Carter,	W.	C.	Hsieh,	L.	B.	Stoller,	M.	R.	Swanson,	L.	Zhang,	E.
L.	Brunvand,	A.	Davis,	C.-C.	Kuo,	R.	Kuramkote,	M.	A.	Parker,	L.
Schaelicke,	and	T.	Tateyama.	Impulse:	Building	a	smarter	memory
controller.	In	
Proceedings	of	the	5th	International	Symposium	on	High
Performance	Computer	Architecture	(HPCA)
,	pages	70–79.	ACM,
January	1999.
[18]	
K.	Chang,	D.	Lee,	Z.	Chishti,	A.	Alameldeen,	C.	Wilkerson,	Y.	Kim,
and	O.	Mutlu.	Improving	DRAM	performance	by	parallelizing
refreshes	with	accesses.	In	
Proceedings	of	the	20th	International
Symposium	on	High-Performance	Computer	Architecture	(HPCA)
.
ACM,	February	2014.
[19]	
S.	Chellappa,	F.	Franchetti,	and	M.	Püschel.	How	to	write	fast
numerical	code:	A	small	introduction.	In	
Generative	and
Transformational	Techniques	in	Software	Engineering	II
,	volume	5235
of	
Lecture	Notes	in	Computer	Science
,	pages	196–259.	Springer-
Verlag,	2008.
[20]	
P.	Chen,	E.	Lee,	G.	Gibson,	R.	Katz,	and	D.	Patterson.	RAID:	High-
performance,	reliable	secondary	storage.	
ACM	Computing	Surveys
26(2):145–185,	June	1994.</p>
<p>[21]	
S.	Chen,	P.	Gibbons,	and	T.	Mowry.	Improving	index	performance
through	prefetching.	In	
Proceedings	of	the	2001	ACM	SIGMOD
International	Conference	on	Management	of	Data
,	pages	235–246.
ACM,	May	2001.
[22]	
T.	Chilimbi,	M.	Hill,	and	J.	Larus.	Cache-conscious	structure	layout.
In	
Proceedings	of	the	1999	ACM	Conference	on	Programming
Language	Design	and	Implementation	(PLDI)
,	pages	1–12.	ACM,
May	1999.
[23]	
E.	Coffman,	M.	Elphick,	and	A.	Shoshani.	System	deadlocks.	
ACM
Computing	Surveys
3(2):67–78,	June	1971.
[24]	
D.	Cohen.	On	holy	wars	and	a	plea	for	peace.	
IEEE	Computer
14(10):48–54,	October	1981.
[25]	
P.	J.	Courtois,	F.	Heymans,	and	D.	L.	Parnas.	Concurrent	control
with	&quot;readers&quot;	and	&quot;writers.&quot;	
Communications	of	the	ACM
14(10):667–
668,	1971.
[26]	
C.	Cowan,	P.	Wagle,	C.	Pu,	S.	Beattie,	and	J.	Walpole.	Buffer
overflows:	Attacks	and	defenses	for	the	vulnerability	of	the	decade.	In
DARPA	Information	Survivability	Conference	and	Expo	(DISCEX)
,
volume	2,	pages	119–129,	March	2000.
[27]	
J.	H.	Crawford.	The	i486	CPU:	Executing	instructions	in	one	clock
cycle.	
IEEE	Micro
10(1):27–36,	February	1990.
[28]	
V.	Cuppu,	B.	Jacob,	B.	Davis,	and	T.	Mudge.	A	performance
comparison	of	contemporary	DRAM	architectures.	In	
Proceedings	of</p>
<p>the	26th	International	Symposium	on	Computer	Architecture	(ISCA)
,
pages	222–233,	ACM,	1999.
[29]	
B.	Davis,	B.	Jacob,	and	T.	Mudge.	The	new	DRAM	interfaces:
SDRAM,	RDRAM,	and	variants.	In	
Proceedings	of	the	3rd
International	Symposium	on	High	Performance	Computing	(ISHPC)
,
volume	1940	of	
Lecture	Notes	in	Computer	Science
,	pages	26–31.
Springer-Verlag,	October	2000.
[30]	
E.	Demaine.	Cache-oblivious	algorithms	and	data	structures.	In
Lecture	Notes	from	the	EEF	Summer	School	on	Massive	Data	Sets
.
BRICS,	University	of	Aarhus,	Denmark,	2002.
[31]	
E.	W.	Dijkstra.	Cooperating	sequential	processes.	Technical	Report
EWD-123,	Technological	University,	Eindhoven,	the	Netherlands,
1965.
[32]	
C.	Ding	and	K.	Kennedy.	Improving	cache	performance	of	dynamic
applications	through	data	and	computation	reorganizations	at	run
time.	In	
Proceedings	of	the	1999	ACM	Conference	on	Programming
Language	Design	and	Implementation	(PLDI)
,	pages	229–241.	ACM,
May	1999.
[33]	
M.	Dowson.	The	Ariane	5	software	failure.	
SIGSOFT	Software
Engineering	Notes
22(2):84,	1997.
[34]	
U.	Drepper.	User-level	IPv6	programming	introduction.	Available	at
http:/
/
www.akkadia.org/
drepper/
userapi-ipv6.html
,	2008.
[35]	
M.	W.	Eichen	and	J.	A.	Rochlis.	With	micro-	scope	and	tweezers:	An</p>
<p>analysis	of	the	Internet	virus	of	November,	1988.	In	
Proceedings	of
the	IEEE	Symposium	on	Research	in	Security	and	Privacy
,	pages
326–343.	IEEE,	1989.
[36]	
ELF-64	Object	File	Format,	Version	1.5	Draft	2
,	1998.	Available	at
http:/
/
www.uclibc.org/
docs/
elf-64-gen.pdf
.
[37]	
R.	Fielding,	J.	Gettys,	J.	Mogul,	H.	Frystyk,	L.	Masinter,	P.	Leach,
and	T.	Berners-Lee.	Hypertext	transfer	protocol	-	HTTP/1.1.	RFC
2616,	1999.
[38]	
M.	Frigo,	C.	E.	Leiserson,	H.	Prokop,	and	S.	Ramachandran.	Cache-
oblivious	algorithms.	In	
Proceedings	of	the	40th	IEEE	Symposium	on
Foundations	of	Computer	Science	(FOCS)
,	pages	285–297.	IEEE,
August	1999.
[39]	
M.	Frigo	and	V.	Strumpen.	The	cache	complexity	of	multithreaded
cache	oblivious	algorithms.	In	
Proceedings	of	the	18th	Symposium	on
Parallelism</p>
<p>in	Algorithms	and	Architectures	(SPAA)
,	pages	271–280.
ACM,	2006.
[40]	
G.	Gibson,	D.	Nagle,	K.	Amiri,	J.	Butler,	F.	Chang,	H.	Gobioff,	C.
Hardin,	E.	Riedel,	D.	Rochberg,	and	J.	Zelenka.	A	cost-effective,	high-
bandwidth	storage	architecture.	In	
Proceedings	of	the	8th
International	Conference	on	Architectural	Support	for	Programming
Languages	and	Operating	Systems	(ASPLOS)
,	pages	92–103.	ACM,
October	1998.
[41]	
G.	Gibson	and	R.	Van	Meter.	Network	attached	storage	architecture.
Communications	of	the	ACM
43(11):37–45,	November	2000.</p>
<p>[42]	
Google.	IPv6	Adoption.	Available	at	
http:/
/
www.google.com/
intl/
en/
ipv6/
statistics.html
.
[43]	
J.	Gustafson.	Reevaluating	Amdahl's	law.	
Communications	of	the
ACM
31(5):532–533,	August	1988.
[44]	
L.	Gwennap.	New	algorithm	improves	branch	prediction.
Microprocessor	Report
9(4),	March	1995.
[45]	
S.	P.	Harbison	and	G.	L.	Steele,	Jr.	
C,	A	Reference	Manual,	Fifth
Edition
.	Prentice	Hall,	2002.
[46]	
J.	L.	Hennessy	and	D.	A.	Patterson.	
Computer	Architecture:	A
Quantitative	Approach,	Fifth	Edition
.	Morgan	Kaufmann,	2011.
[47]	
M.	Herlihy	and	N.	Shavit.	
The	Art	of	Multi-	processor	Programming
.
Morgan	Kaufmann,	2008.
[48]	
C.	A.	R.	Hoare.	Monitors:	An	operating	system	structuring	concept.
Communications	of	the	ACM
17(10):549–557,	October	1974.
[49]	
Intel	Corporation.	
Intel	64	and	IA-32	Architectures	Optimization
Reference	Manual
.	Available	at	
http:/
/
www.intel.com/
content/
www/
us/
en/
processors/
architectures-software-developer-
manuals.html
.
[50]	
Intel	Corporation.	
Intel	64	and	IA-32	Architectures	Software
Developer's	Manual,	Volume	1:	Basic	Architecture
.	Available	at	
http:/
/
www.intel.com/
content/
www/
us/
en/
processors/
architectures-</p>
<p>software-developer-manuals.html
.
[51]	
Intel	Corporation.	
Intel	64	and	IA-32	Architectures	Software
Developer's	Manual,	Volume	2:	Instruction	Set	Reference.
Available
at	
http:/
/
www.intel.com/
content/
www/
us/
en/
processors/
architectures-software-developer-manuals.html
.
[52]	
Intel	Corporation.	
Intel	64	and	IA-32	Architectures	Software
Developer's	Manual,	Volume	3a:	System	Programming	Guide,	Part	1.
Available	at	
http:/
/
www.intel.com/
content/
www/
us/
en/
processors/
architectures-software-developer-manuals.html
.
[53]	
Intel	Corporation.	
Intel	Solid-State	Drive	730	Series:	Product
Specification.
Available	at	
http:/
/
www.intel.com/
content/
www/
us/
en/
solid-state-drives/
ssd-730-series-spec.html
.
[54]	
Intel	Corporation.	
Tool	Interface	Standards	Portable	Formats
Specification,	Version	1.1
,	1993.	Order	number	241597.
[55]	
F.	Jones,	B.	Prince,	R.	Norwood,	J.	Hartigan,	W.	Vogley,	C.	Hart,	and
D.	Bondurant.	Memory–-a	new	era	of	fast	dynamic	RAMs	(for	video
applications).	
IEEE	Spectrum
,	pages	43–45,	October	1992.
[56]	
R.	Jones	and	R.	Lins.	
Garbage	Collection:	Algorithms	for	Automatic
Dynamic	Memory	Management.
Wiley,	1996.
[57]	
M.	Kaashoek,	D.	Engler,	G.	Ganger,	H.	Briceo,	R.	Hunt,	D.	Maziers,
T.	Pinckney,	R.	Grimm,	J.	Jannotti,	and	K.	MacKenzie.	Application
performance	and	flexibility	on	Exokernel	systems.	In	
Proceedings	of</p>
<p>the	16th	ACM	Symposium	on	Operating	System	Principles	(SOSP)
,
pages	52–65.	ACM,	October	1997.
[58]	
R.	Katz	and	G.	Borriello.	
Contemporary	Logic	Design,	Second
Edition.
Prentice	Hall,	2005.
[59]	
B.	W.	Kernighan	and	R.	Pike.	
The	Practice	of	Programming.
Addison-Wesley,	1999.
[60]	
B.	Kernighan	and	D.	Ritchie.	
The	C	Programming	Language,	First
Edition.
Prentice	Hall,	1978.
[61]	
B.	Kernighan	and	D.	Ritchie.	
The	C	Programming	Language,	Second
Edition.
Prentice	Hall,	1988.
[62]	
Michael	Kerrisk.	
The	Linux	Programming	Interface.
No	Starch	Press,
2010.
[63]	
T.	Kilburn,	B.	Edwards,	M.	Lanigan,	and	F.	Sumner.	One-level
storage	system.	
IRE</p>
<p>Transactions	on	Electronic	Computers
EC-
11:223–235,	April	1962.
[64]	
D.	Knuth.	
The	Art	of	Computer	Programming,	Volume	1:
Fundamental	Algorithms,	Third	Edition.
Addison-Wesley,	1997.
[65]	
J.	Kurose	and	K.	Ross.	
Computer	Networking:	A	Top-Down
Approach,	Sixth	Edition.
Addison-Wesley,	2012.
[66]	
M.	Lam,	E.	Rothberg,	and	M.	Wolf.	The	cache	performance	and
optimizations	of	blocked	algorithms.	In	
Proceedings	of	the	4th</p>
<p>International	Conference	on	Architectural	Support	for	Programming
Languages	and	Operating	Systems	(ASPLOS)
,	pages	63–74.	ACM,
April	1991.
[67]	
D.	Lea.	A	memory	allocator.	Available	at	
http:/
/
gee.cs.oswego.edu/
dl/
html/
malloc.html
,	1996.
[68]	
C.	E.	Leiserson	and	J.	B.	Saxe.	Retiming	synchronous	circuitry.
Algorithmica
6(1–6),	June	1991.
[69]	
J.	R.	Levine.	
Linkers	and	Loaders.
Morgan	Kaufmann,	1999.
[70]	
David	Levinthal.	
Performance	Analysis	Guide	for	Intel	Core	i7
Processor	and	Intel	Xeon	5500	Processors.
Available	at
https://software.intel.com/sites/products/collateral/hpc/vtune/performance_analysis_guide.pdf
[71]	
C.	Lin	and	L.	Snyder.	
Principles	of	Parallel	Programming.
Addison
Wesley,	2008.
[72]	
Y.	Lin	and	D.	Padua.	Compiler	analysis	of	irregular	memory
accesses.	In	
Proceedings	of	the	2000	ACM	Conference	on
Programming	Language	Design	and	Implementation	(PLDI)
,	pages
157–168.	ACM,	June	2000.
[73]	
J.	L.	Lions.	Ariane	5	Flight	501	failure.	Technical	Report,	European
Space	Agency,	July	1996.
[74]	
S.	Macguire.	
Writing	Solid	Code.
Microsoft	Press,	1993.
[75]	
S.	A.	Mahlke,	W.	Y.	Chen,	J.	C.	Gyllenhal,	and	W.	W.	Hwu.	Compiler</p>
<p>code	transformations	for	superscalar-based	high-performance
systems.	In	
Proceedings	of	the	1992	ACM/IEEE	Conference	on
Supercomputing
,	pages	808–817.	ACM,	1992.
[76]	
E.	Marshall.	Fatal	error:	How	Patriot	over-	looked	a	Scud.	
Science
,
page	1347,	March	13,	1992.
[77]	
M.	Matz,	J.	Hubička,	A.	Jaeger,	and	M.	Mitchell.	System	V
application	binary	interface	AMD64	architecture	processor
supplement.	Technical	Report,	x86–64.org,	2013.	Available	at	
http:/
/
www.x86-64.org/
documentation_folder/
abi-0.99.pdf
.
[78]	
J.	Morris,	M.	Satyanarayanan,	M.	Conner,	J.	Howard,	D.	Rosenthal,
and	F.	Smith.	Andrew:	A	distributed	personal	computing	environment.
Communications	of	the	ACM
,	pages	184–201,	March	1986.
[79]	
T.	Mowry,	M.	Lam,	and	A.	Gupta.	Design	and	evaluation	of	a
compiler	algorithm	for	prefetching.	In	
Proceedings	of	the	5th
International	Conference	on	Architectural	Support	for	Programming
Languages	and	Operating	Systems	(ASPLOS)
,	pages	62–73.	ACM,
October	1992.
[80]	
S.	S.	Muchnick.	
Advanced	Compiler	Design	and	Implementation
.
Morgan	Kaufmann,	1997.
[81]	
S.	Nath	and	P.	Gibbons.	Online	maintenance	of	very	large	random
samples	on	flash	storage.	In	
Proceedings	of	VLDB
,	pages	970–983.
VLDB	Endowment,	August	2008.
[82]	
M.	Overton.	
Numerical	Computing	with	IEEE	Floating	Point</p>
<p>Arithmetic
.	SIAM,	2001.
[83]	
D.	Patterson,	G.	Gibson,	and	R.	Katz.	A	case	for	redundant	arrays	of
inexpensive	disks	(RAID).	In	
Proceedings	of	the	1998	ACM	SIGMOD
International	Conference	on	Management	of	Data
,	pages	109–116.
ACM,	June	1988.
[84]	
L.	Peterson	and	B.	Davie.	
Computer	Networks:	A	Systems
Approach,	Fifth	Edition
.	Morgan	Kaufmann,	2011.
[85]	
J.	Pincus	and	B.	Baker.	Beyond	stack	smashing:	Recent	advances	in
exploiting	buffer	overruns.	
IEEE	Security	and	Privacy
2(4):20–27,
2004.
[86]	
S.	Przybylski.	
Cache	and	Memory	Hierarchy	Design:	A	Performance-
Directed	Approach
.	Morgan	Kaufmann,	1990.
[87]	
W.	Pugh.	The	Omega	test:	A	fast	and	practical	integer	programming
algorithm	for	dependence	
analysis.	
Communications	of	the	ACM
35(8):102–114,	August	1992.
[88]	
W.	Pugh.	Fixing	the	Java	memory	model.	In	
Proceedings	of	the	ACM
Conference	on	Java	Grande
,	pages	89–98.	ACM,	June	1999.
[89]	
J.	Rabaey,	A.	Chandrakasan,	and	B.	Nikolic.	
Digital	Integrated
Circuits:	A	Design	Perspective,	Second	Edition
.	Prentice	Hall,	2003.
[90]	
J.	Reinders.	
Intel	Threading	Building	Blocks
.	O'Reilly,	2007.
[91]	
D.	Ritchie.	The	evolution	of	the	Unix	time-	sharing	system.	
AT&amp;T	Bell</p>
<p>Laboratories	Technical	Journal
63(6	Part	2):1577–1593,	October
1984.
[92]	
D.	Ritchie.	The	development	of	the	C	language.	In	
Proceedings	of
the	2nd	ACM	SIGPLAN	Conference	on	History	of	Programming
Languages
,	pages	201–208.	ACM,	April	1993.
[93]	
D.	Ritchie	and	K.	Thompson.	The	Unix	time-sharing	system.
Communications	of	the	ACM
17(7):365–367,	July	1974.
[94]	
M.	Satyanarayanan,	J.	Kistler,	P.	Kumar,	M.	Okasaki,	E.	Siegel,	and
D.	Steere.	Coda:	A	highly	available	file	system	for	a	distributed
workstation	environment.	
IEEE	Transactions	on	Computers
39(4):447–459,	April	1990.
[95]	
J.	Schindler	and	G.	Ganger.	Automated	disk	drive	characterization.
Technical	Report	CMU-	CS-99–176,	School	of	Computer	Science,
Carnegie	Mellon	University,	1999.
[96]	
F.	B.	Schneider	and	K.	P.	Birman.	The	monoculture	risk	put	into
context.	
IEEE	Security	and	Privacy
7(1):14–17,	January	2009.
[97]	
R.	C.	Seacord.	
Secure	Coding	in	C	and	C++,	Second	Edition
.
Addison-Wesley,	2013.
[98]	
R.	Sedgewick	and	K.	Wayne.	
Algorithms,	Fourth	Edition
.	Addison-
Wesley,	2011.
[99]	
H.	Shacham,	M.	Page,	B.	Pfaff,	E.-J.	Goh,	N.	Modadugu,	and	D.
Boneh.	On	the	effectiveness	of	address-space	randomization.	In</p>
<p>Proceedings	of	the	11th	ACM	Conference	on	Computer	and
Communications	Security	(CCS)
,	pages	298–307.	ACM,	2004.
[100]	
J.	P.	Shen	and	M.	Lipasti.	
Modern	Processor	Design:	Fundamentals
of	Superscalar	Processors.
McGraw	Hill,	2005.
[101]	
B.	Shriver	and	B.	Smith.	
The	Anatomy	of	a	High-Performance
Microprocessor:	A	Systems	Perspective.
IEEE	Computer	Society,
1998.
[102]	
A.	Silberschatz,	P.	Galvin,	and	G.	Gagne.	
Operating	Systems
Concepts,	Ninth	Edition.
Wiley,	2014.
[103]	
R.	Skeel.	Roundoff	error	and	the	Patriot	missile.	
SIAM	News
25(4):11,	July	1992.
[104]	
A.	Smith.	Cache	memories.	
ACM	Computing	Surveys
14(3),
September	1982.
[105]	
E.	H.	Spafford.	The	Internet	worm	program:	An	analysis.	Technical
Report	CSD-TR-823,	Department	of	Computer	Science,	Purdue
University,	1988.
[106]	
W.	Stallings.	
Operating	Systems:	Internals	and	Design	Principles,
Eighth	Edition.
Prentice	Hall,	2014.
[107]	
W.	R.	Stevens.	
TCP/IP	Illustrated,	Volume	3:	TCP	for	Transactions,
HTTP,	NNTP	and	the	Unix	Domain	Protocols.
Addison-Wesley,	1996.
[108]	
W.	R.	Stevens.	
Unix	Network	Programming:	Interprocess</p>
<p>Communications,	Second	Edition
,	volume	2.	Prentice	Hall,	1998.
[109]	
W.	R.	Stevens	and	K.	R.	Fall.	
TCP/IP	Illustrated,	Volume	1:	The
Protocols,	Second	Edition.
Addison-Wesley,	2011.
[110]	
W.	R.	Stevens,	B.	Fenner,	and	A.	M.	Rudoff.	
Unix	Network
Programming:	The	Sockets	Networking	API,	Third	Edition
,	volume	1.
Prentice	Hall,	2003.
[111]	
W.	R.	Stevens	and	S.	A.	Rago.	
Advanced	Programming	in	the	Unix
Environment,	Third	Edition.
Addison-Wesley,	2013.
[112]	
T.	Stricker	and	T.	Gross.	Global	address	space,	non-uniform
bandwidth:	A	memory	system	performance	characterization	of	parallel
systems.	In	
Proceedings	of	the	3rd	International	Symposium	on	High
Performance	Computer	Architecture	(HPCA)
,	pages	168–179.	IEEE,
February	1997.
[113]	
A.	S.	Tanenbaum	and	H.	Bos.	
Modern	Operating	Systems,	Fourth
Edition
.	Prentice	Hall,	2015.
[114]	
A.	S.	Tanenbaum	and	D.	Wetherall.	
Computer	Networks,	Fifth
Edition
.	Prentice	Hall,	2010.
[115]	
K.	P.	Wadleigh	and	I.	L.	Crawford.	
Software	Optimization	for	High-
Performance	Computing:	Creating	Faster	Applications
.	Prentice	Hall,
2000.
[116]	
J.	F.	Wakerly.	
Digital	Design	Principles	and	Practices,	Fourth
Edition
.	Prentice	Hall,	2005.</p>
<p>[117]	
M.	V.	Wilkes.	Slave	memories	and	dynamic	storage	allocation.	
IEEE
Transactions	on	Electronic	Computers
,	EC-14(2),	April	1965.
[118]	
P.Wilson,	M.	Johnstone,	M.	Neely,	and	D.	Boles.	Dynamic	storage
allocation:	A	survey	and	critical	review.	In	
International	Workshop	on
Memory	Management
,	volume	986	of	
Lecture	Notes	in	Computer
Science
,	pages	1–116.	Springer-Verlag,	1995.
[119]	
M.	Wolf	and	M.	Lam.	A	data	locality	algorithm.	In	
Proceedings	of	the
1991	ACM	Conference	on	Programming	Language	Design	and
Implementation	(PLDI)
,	pages	30–44,	June	1991.
[120]	
G.	R.	Wright	and	W.	R.	Stevens.	
TCP/IP	Illustrated,	Volume	2:	The
Implementation
.	Addison-Wesley,	1995.
[121]	
J.	Wylie,	M.	Bigrigg,	J.	Strunk,	G.	Ganger,	H.	Kiliccote,	and	P.
Khosla.	Survivable	information	storage	systems.	
IEEE	Computer
33:61–68,	August	2000.
[122]	
T.-Y.	Yeh	and	Y.	N.	Patt.	Alternative	implementation	of	two-level
adaptive	branch	prediction.	In	
Proceedings	of	the	19th	Annual
International	Symposium	on	Computer	Architecture	(ISCA)
,	pages
451–461.	ACM,	1998.</p>
<p>Index
Page	numbers	of	defining	references	are	
italicized
.	Entries	that	belong	to
a	hardware	or	software	system	are	followed	by	a	tag	in	brackets	that
identifies	the	system,	along	with	a	brief	description	to	jog	your	memory.
Here	is	the	list	of	tags	and	their	meanings.
[C]
C	language	construct
[C	Stdlib]
C	standard	library	function
[CS:APP]
Program	or	function	developed	in	this	text
[HCL]
HCL	language	construct
[Unix]
Unix	program,	function,	variable,	or	constant
[x86−64]
x86−64	machine-language	instruction
[Y86−64]
Y86−64	machine-language	instruction
!	[HCL]	
NOT
operation,	
373
$	for	immediate	operands,	
181
&amp;	[C]	address	of	operation
local	variables,	
248
logic	gates,	
373
pointers,	
48
,	
188
,	
257
,	
277</p>
<ul>
<li>[C]	dereference	pointer	operation,	
188
-&gt;	[C]	dereference	and	select	field	operation,	
266
.	(periods)	in	dotted-decimal	notation,	
926</li>
</ul>
<p>||	[HCL]	
OR
operation,	
373
&lt;	operator	for	left	hoinkies,	
909
&lt;&lt;	&quot;put	to&quot;	operator	(C++),	
890
&gt;	operator	for	right	hoinkies,	
909
&gt;&gt;	&quot;get	from&quot;	operator	(C++),	
890</p>
<ul>
<li>(two's-complement	addition),	
60
,	
90</li>
</ul>
<ul>
<li>(two's-complement	multiplication),	
60
,	
97
−
(two's-complement	negation),	
60
,	
95</li>
</ul>
<ul>
<li>(unsigned	addition),	
60
,	
85
,	
89</li>
</ul>
<ul>
<li>
<p>(unsigned	multiplication),	
60
,	
96
−
(unsigned	negation),	
60
,	
89
8086	microprocessor,	
167
8087	floating-point	coprocessor,	
109
,	
137
,	
167
80286	microprocessor,	
167
t
w
t
w
t
w
u
w
u
w
u
w</p>
<p>archive	files,	
686
object	file,	
673
Abel,	Niels	Henrik,	
89
abelian	group,	
89
ABI	(application	binary	interface),	
310
abort	exception	class,	
726
aborts,	
728
absolute	addressing	relocation	type,	
691
,	
693
–
694
absolute	pathnames,	
893
absolute	speedupof	parallel	programs,	
1019
abstract	operation	model	for	Core	i7,	
525
–
531
abstractions,	
27
[Unix]	wait	for	client	connection	request,	
933
,	
936
,	
936
–
937
access
disks,	
597
–
600
IA32	registers,	
179
–
180
main	memory,	
587
–
589
x86–64	registers
data	movement,	
182
–
189
operand	specifiers,	
180
–
182
access	permission	bits,	
894
access	time	for	disks,	
593
,	
593
–
595
accumulator	variable	expansion,	
570
accumulators,	multiple,	
536
–
541
Acorn	RISC	machine	(ARM)
ISAs,	
352
processor	architecture,	
363
actions,	signal,	
762
active	sockets,	
935</p>
</li>
</ul>
<p>actuator	arms,	
592
acyclic	networks,	
374
adapters,	
9
,	
597
ADD
[instruction	class]	add,	
192
function,	
981
,	
983
add	every	signal	to	signal	set	instruction,	
765
add	instruction,	
192
ADD
operation	in	execute	stage,	
408
add	signal	to	signal	set	instruction,	
765
[CS:APP]	CGI	adder,	
955
addition
floating	point,	
122
–
124
,	
302
two's	complement,	
90
,	
90
–
95
unsigned,	
84
–
90
,	
85
Y86–64,	
356
additive	inverse,	
52
[Y86–64]	add,	
356
,	
402
address	exceptions,	status	code	for,	
404
address	of	operator	(&amp;)	[C]
local	variables,	
248
logic	gates,	
373
pointers,	
48
,	
188
,	
257
,	
277
address	order	of	free	lists,	
863
address	partitioning	in	caches,	
615
,	
615
–
616
address-space	layout	randomization	(ASLR),	
285
,	
285
–
286
address	spaces,	
804
child	processes,	
741
linear,	
804
private,	
734</p>
<p>virtual,	
804
–
805
address	translation,	
804
caches	and	VM	integration,	
817
Core	i7,	
826
–
828
end-to-end,	
821
–
825
multi-level	page	tables,	
819
–
821
optimizing,	
830
overview,	
813
–
816
TLBs	for,	
817
–
819
addresses	and	addressing
byte	ordering,	
42
–
49
effective,	
690
flat,	
167
internet,	
922
invalid	address	status	code,	
364
I/O	devices,	
598
IP,	
924
,	
925
–
927
machine-level	programming,	
170
–
171
operands,	
181
out	of	bounds.	
See	
buffer	overflow
physical	vs.	virtual,	
803
–
804
pointers,	
257
,	
277
procedure	return,	
240
segmented,	
287
–
288
sockets,	
930
,	
933
–
934
structures,	
265
–
267
symbol	relocation,	
690
–
691
virtual,	
804
virtual	memory,	
34
Y86–64
,	
356
,	
359</p>
<p>addressing	modes,	
181
adjacency	matrices,	
660
[Y86–64]	status	code	indicating	invalid	address,	
364
Advanced	Micro	Devices	(AMD),	
165
,	
168
Intel	compatibility,	
168
x86–64.	
See	
x86–64	microprocessors
Advanced	Research	Projects	Administration	(ARPA),	
931
advanced	vector	extensions	(AVX)	instructions,	
294
,	
546
–
547
AFS	(Andrew	File	System),	
610
aggregate	data	types,	
171
aggregate	payloads,	
845
[x86–64]	low	order	8	of	register	
,	
180
[Unix]	schedule	alarm	to	self,	
762
,	
763
algebra,	Boolean,	
50
–
53
,	
52
aliasing	memory,	
499
,	
500
.align	directive,	
366
alignment
data,	
273
,	
273
–
276
memory	blocks,	
844
[Unix]	stack	storage	allocation	function,	
285
,	
290
,	
324
allocate	and	initialize	bounded	buffer	function,	
1007
allocate	heap	block	function,	
860
,	
861
allocate	heap	storage	function,	
840
allocated	bit,	
848
allocated	blocks
vs.	free,	
839
placement,	
849
allocation
blocks,	
860</p>
<p>dynamic	memory.	
See	
dynamic	memory	allocation
pages,	
810
allocators
block	allocation,	
860
block	freeing	and	coalescing,	
860
free	list	creation,	
857
–
859
free	list	manipulation,	
856
–
857
general	design,	
854
–
856
practice	problems,	
861
–
862
requirements	and	goals,	
844
–
845
styles,	
839
–
840
Alpha	(Compaq	Computer	Corp.)
RISC	processors,	
363
alternate	representations	of	signed	integers,	
68
[Y86–64]	function	code	for
instruction,	
404
ALUs	(arithmetic/logic	units),	
10
combinational	circuits,	
380
in	execute	stage,	
385
sequential	Y86–64	implementation,	
408
–
409
always	taken	branch	prediction	strategy,	
428
AMD	(Advanced	Micro	Devices),	
165
,	
168
Intel	compatibility,	
168
microprocessor	data	alignment,	
276
x86–64.	
See	
x86–64	microprocessors
Amdahl,	Gene,	
22
Amdahl's	law,	
22
,	
22
–
24
,	
562
,	
568
American	National	Standards	Institute	(ANSI),	
4
,	
35
ampersands	(&amp;)	address	operator,	
248</p>
<p>local	addresses,	
248
logic	gates,	
373
pointers,	
48
,	
188
,	
257
,	
277
AND
[instruction	class]	and,	
192
and	instruction,	
192
AND
operations
Boolean,	
51
–
52
execute	stage,	
408
HCL	expressions,	
374
–
375
logic	gates,	
373
logical,	
56
–
57
AND
packed	double	precision	instruction,	
305
AND
packed	single	precision	instruction,	
305
[Y86–64]	and,	
356
Andreesen,	Marc,	
949
Andrew	File	System	(AFS),	
610
anonymous	files,	
833
ANSI	(American	National	Standards	Institute),	
4
,	
35
[Y86–64]	status	code	for	normal	operation,	
363
[CS:APP]	reports	application	errors,	
1043
application	binary	interface	(ABI),	
310
applications,	loading	and	linking	shared	libraries	from,	
701
–
703
AR
Linux	archiver,	
686
,	
713
arbitrary	size	arithmetic,	
85
Archimedes,	
140
architecture
floating-point,	
293
,	
293
–
296
Y86.	
See	Y86–64	instruction	set	architecture
archives,	
686</p>
<p>areal	density	of	disks,	
591
areas
shared,	
834
swap,	
833
virtual	memory,	
830
arguments
function,	
750
Web	servers,	
953
–
954
arithmetic,	
33
,	
191
discussion,	
196
–
197
floating-point	code,	
302
–
304
integer.	
See	
integer	arithmetic
latency	and	issue	time,	
523
load	effective	address,	
191
–
193
pointers,	
257
–
258
,	
873
saturating,	
134
shift	operations,	
58
,	
104
–
106
,	
192
,	
194
–
196
special,	
197
–
200
unary	and	binary,	
194
–
196
arithmetic/logic	units	(ALUs),	
10
combinational	circuits,	
380
in	execute	stage,	
385
sequential	Y86–64	implementation,	
408
–
409
ARM	(Acorn	RISC	machine),	
43
ISAs,	
352
processor	architecture,	
363
ARM	A7	microprocessor,	
353
arms,	actuator,	
592
ARPA	(Advanced	Research	Projects	Administration),	
931</p>
<p>ARPANET,	
931
arrays,	
255
basic	principles,	
255
–
257
declarations,	
255
–
256
,	
263
DRAM,	
582
fixed-size,	
260
–
262
machine-code	representation,	
171
nested,	
258
–
260
pointer	arithmetic,	
257
–
258
pointer	relationships,	
48
,	
277
stride,	
606
variable-size,	
262
–
265
ASCII	standard,	
3
character	codes,	
49
limitations,	
50
function,	
1024
ASLR	(address-space	layout	randomization),	
285
,	
285
–
286
directive,	
178
directives,	
366
,	
5
,	
5
,	
164
,	
170
code,	
5
,	
164
with	C	programs,	
289
–
290
formatting,	
175
–
177
Y86–64,	
359
assembly	phase,	
5
associate	socket	address	with	descriptor	function,	
935
,	
935
associative	caches,	
624
–
626
associative	memory,	
625
associativity</p>
<p>caches,	
633
floating-point	addition,	
123
–
124
asterisks	(*)	dereference	pointer	operation,	
188
,	
257
,	
277
asymmetric	ranges	in	two's-complement	representation,	
66
,	
77
async-signal-safe	function,	
766
async-signal	safety,	
766
asynchronous	interrupts,	
726
atomic	reads	and	writes,	
770
ATT	assembly	code	format,	
177
,	
294
,	
311
argument	listing,	
306
condition	codes,	
201
–
202
instruction,	
199
vs.	Intel,	
177
operands,	
181
,	
192
Y86–64,	
356
automatic	variables,	
994
AVX	(advanced	vector	extensions)	instructions,	
276
,	
294
,	
546
–
547
[x86–64]	low	order	16	bits	of	register	
,	
180</p>
<p>B2T
(binary	to	two's-complement	conversion),	
60
,	
64
,	
72
,	
97
B2U
(binary	to	unsigned	conversion),	
60
,	
62
,	
72
,	
82
,	
97
background	processes,	
753
,	
753
–
756
backlogs	for	listening	sockets,	
935
backups	for	disks,	
611
backward	compatibility,	
35
backward	taken,	forward	not	taken	(BTFNT)	branch	prediction
strategy,	
428
bad	pointers	and	virtual	memory,	
870
–
871
[CS:APP]	improperly	synchronized	program,	
995
–
999
,	
996
bandwidth,	read,	
639
Barracuda	7400	drives,	
600
base	pointers,	
290
base	registers,	
181
[Unix]	Unix	shell	program,	
753
basic	blocks,	
569
Bell	Laboratories,	
35
Berkeley	sockets,	
932
Berners-Lee,	Tim,	
949
best-fit	block	placement	policy,	
849
,	
849
bi-endian	ordering	convention,	
43
biased	number	encoding,	
113
,	
113
–
117
biasing	in	division,	
106
big-endian	ordering	convention,	
42
,	
42
–
44
bigrams	statistics,	
565
bijections,	
64
,	
64
program,	
760
binary	files,	
3
,	
891
binary	notation,	
32</p>
<p>binary	points,	
110
,	
110
–
111
binary	representations
conversions
with	hexadecimal,	
36
–
37
signed	and	unsigned,	
70
–
76
to	two's	complement,	
64
,	
72
–
73
,	
97
to	unsigned,	
62
–
63
fractional,	
109
–
112
machine	language,	
194
binary	semaphores,	
1003
binary	tree	structure,	
270
–
271
[Unix]	associate	socket	address	with	descriptor,	
933
,	
935
,	
935
binding,	lazy,	
706
binutils	package,	
713
bistable	memory	cells,	
581
bit-level	operations,	
54
–
56
bit	representation	expansion,	
76
–
80
bit	vectors,	
51
,	
51
–
52
bits,	
3
overview,	
32
union	access	to,	
271
–
272
bitwise	operations,	
305
–
306
[x86–64]	low	order	8	of	register	
,	
180
block	and	unblock	signals	instruction,	
765
block	devices,	
892
block	offset	bits,	
616
block	pointers,	
856
block	size
caches,	
633</p>
<p>minimum,	
848
blocked	bit	vectors,	
759
blocked	signals,	
758
,	
759
,	
764
–
765
blocking
signals,	
764
–
765
for	temporal	locality,	
647
blocks
aligning,	
844
allocated,	
839
,	
849
vs.	cache	lines,	
634
caches,	
611
,	
611
–
612
,	
615
,	
633
coalescing,	
850
–
851
,	
860
epilogue,	
855
free	lists,	
847
–
849
freeing,	
860
heap,	
839
logical	disk,	
595
,	
595
–
596
,	
601
prologue,	
855
referencing	data	in,	
874
–
875
splitting,	
849
–
850
bodies,	response,	
952
[HCL]	bit-level	signal,	
374
Boole,	George,	
50
Boolean	algebra	and	functions,	
50
HCL,	
374
–
375
logic	gates,	
373
properties,	
52
working	with,	
50
–
53
Boolean	rings,	
52</p>
<p>bottlenecks,	
562
profilers,	
565
–
568
program	profiling,	
562
–
564
bottom	of	stack,	
190
boundary	tags,	
851
,	
851
–
854
,	
859
bounded	buffers,	
1004
,	
1005
–
1006
bounds
latency,	
518
,	
524
throughput,	
518
,	
524
[x86–64]	low	order	16	bits	of	register	
,	
180
[x86–64]	low	order	8	of	register	
,	
180
branch	prediction,	
519
,	
519
misprediction	handling,	
443
–
444
performance,	
549
–
553
Y86–64	pipelining,	
428
branch	prediction	logic,	
215
branches,	conditional,	
172
,	
209
assembly	form,	
211
condition	codes,	
201
–
202
condition	control,	
209
–
213
moves,	
214
–
220
,	
550
–
553
,	
232
–
238
command
in	
GDB
,	
280
with	
,	
233
break	
command	in	
GDB
,	
280
breakpoints,	
279
–
280
bridged	Ethernet,	
920
,	
921
bridges</p>
<p>Ethernet,	
920
I/O,	
587
browsers,	
948
,	
949
section,	
674
BTFNT	(backward	taken,	forward	not	taken)	branch	prediction
strategy,	
428
bubbles,	pipeline,	
434
,	
434
–
435
,	
459
–
460
buddies,	
865
buddy	systems,	
865
,	
865
buffer	overflow,	
279
execution	code	regions	limits	for,	
289
–
290
memory-related	bugs,	
871
overview,	
279
–
284
stack	corruption	detection	for,	
286
–
289
stack	randomization	for,	
284
–
286
vulnerabilities,	
7
buffered	I/O	functions,	
898
–
902
buffers
bounded,	
1004
,	
1005
–
1006
read,	
898
,	
900
–
901
store,	
557
–
558
streams,	
911
bus	transactions,	
587
buses,	
8
,	
587
designs,	
588
,	
598
I/O,	
596
memory,	
587
bypassing	for	data	hazards,	
436
–
439
byte	data	connections	in	hardware	diagrams,	
398</p>
<p>byte	order,	
42
–
49
disassembled	code,	
209
network,	
925
unions,	
272
bytes,	
3
,	
34
copying,	
133
range,	
36
register	operations,	
181
Y86	encoding,	
359
–
360
[x86–64]	low	order	16	bits	of	register	
,	
180</p>
<p>C	language
bit-level	operations,	
54
–
56
floating-point	representation,	
124
–
126
history,	
35
logical	operations,	
56
–
57
origins,	
4
shift	operations,	
57
–
59
static	libraries,	
684
–
688
C++	language,	
677
linker	symbols,	
680
objects,	
266
–
267
software	exceptions,	
723
–
724
,	
786
source	files,	
671
C	standard	library,	
4
–
5
,	
6
C11	standard,	
35
C90	standard,	
35
C99	standard,	
35
fixed	data	sizes,	
41
integral	data	types,	
67
cache	block	offset	(CO),	
823
cache	blocks,	
615
cache-friendly	code,	
633
–
639
,	
634
cache	lines
cache	sets,	
615
vs.	sets	and	blocks,	
634
cache-oblivious	algorithms,	
649
cache	set	index	(CI),	
823
cache	tags	(CT),	
823
cached	pages,	
806</p>
<p>caches	and	cache	memory,	
610
,	
615
address	translation,	
823
anatomy,	
631
associativity,	
633
cache-friendly	code,	
633
–
639
,	
634
data,	
520
,	
631
,	
631
direct-mapped.	
See	
direct-mapped	caches
DRAM,	
806
fully	associative,	
627
–
628
hits,	
612
importance,	
11
–
14
instruction,	
518
,	
631
,	
631
locality	in,	
605
,	
643
–
647
,	
810
managing,	
613
memory	mountains,	
639
–
643
misses,	
470
,	
612
,	
612
–
613
organization,	
615
–
617
overview,	
610
–
612
page	allocation,	
810
page	faults,	
808
,	
808
–
809
page	hits,	
808
page	tables,	
806
–
808
,	
807
performance,	
533
,	
631
–
633
,	
639
–
647
practice	problems,	
628
–
630
proxy,	
952
purpose,	
580
set	associative,	
624
,	
624
–
626
size,	
632
SRAM,	
806
symbols,	
617</p>
<p>virtual	memory	with,	
805
–
811
,	
817
write	issues,	
630
–
631
write	strategies,	
633
Y86–64	pipelining,	
469
–
470
[x86–64]	procedure	call,	
241
–
242
,	
357
[Y86–64]	instruction,	
404
,	
428
callee	procedures,	
251
callee-save	registers,	
251
,	
251
–
252
caller	procedures,	
251
caller-save	registers,	
251
,	
251
–
252
calling	environments,	
783
function	[C	Stdlib]	memory	allocation
declaration,	
134
dynamic	memory	allocation,	
841
security	vulnerability,	
100
–
101
[x86–64]	procedure	call,	
241
calls,	
17
,	
727
–
728
error	handling,	
737
–
738
Linux/x86–64	systems,	
730
–
731
in	performance,	
512
–
513
canary	values,	
286
–
287
canceling	mispredicted	branch	handling,	
444
capacity
caches,	
615
disks,	
591
,	
591
–
592
functional	units,	
523
capacity	misses,	
613
cards,	graphics,	
597
carriage	return	(CR)	characters,	
892</p>
<p>carry	flag	condition	code,	
201
,	
306
CAS	(column	access	strobe)	requests,	
583
case	expressions	in	HCL,	
378
,	
378
casting,	
44
explicit,	
75
floating-point	values,	
125
pointers,	
278
,	
854
signed	values,	
70
–
71
catching	signals,	
758
,	
761
,	
763
cells
DRAM,	
582
,	
583
SRAM,	
581
central	processing	units	(CPUs),	
9
,	
9
–
10
Core	i7.	
See	
Core	i7	microprocessors
early	instruction	sets,	
361
effective	cycle	time,	
602
embedded,	
363
Intel.	
See	
Intel	microprocessors
logic	design.	
See	
logic	design
many-core,	
471
multi-core,	
16
,	
24
–
25
,	
168
,	
605
,	
972
overview,	
352
–
354
pipelining.	
See	
pipelining
RAM,	
384
sequential	Y86	implementation.	
See	
sequential	Y86–64
implementation
superscalar,	
26
,	
471
,	
518
trends,	
602
–
603
Y86.	
See	
Y86–64	instruction	set	architecture
Cerf,	Vinton,	
931</p>
<p>CERT	(Computer	Emergency	Response	Team),	
100
[x86–64]	carry	flag	condition	code,	
201
,	
306
CGI	(common	gateway	interface)	program,	
953
,	
953
–
955
CGI	adder	function,	
955
chains,	proxy,	
952
[C]	data	types,	
40
,	
61
character	codes,	
49
character	devices,	
892
function,	
981
,	
984
child	processes,	
740
creating,	
741
–
743
default	behavior,	
744
error	conditions,	
745
–
746
exit	status,	
745
reaping,	
743
,	
743
–
749
function,	
746
–
749
CI	(cache	set	index),	
823
circuits
combinational,	
374
,	
374
–
380
retiming,	
421
sequential,	
381
CISC	(complex	instruction	set	computers),	
361
,	
361
–
363
[x86–64]	low	order	8	of	register	
,	
180
Clarke,	Dave,	
931
classes
data	hazards,	
435
exceptions,	
726
–
728
instructions,	
182
size,	
863</p>
<p>storage,	
994
–
995
clear	bit	in	descriptor	set	macro,	
978
clear	descriptor	set	macro,	
978
clear	signal	set	instruction,	
765
client-server	model,	
918
,	
918
–
919
[CS:APP]	T
INY
helper	function,	
959
–
960
clients
client-server	model,	
918
telnet,	
21
clock	signals,	
381
clocked	registers,	
401
–
402
clocking	in	logic	design,	
381
–
384
[Unix]	close	file,	
894
,	
894
–
895
close	operations	for	files,	
891
,	
894
–
895
close	shared	library	function,	
702
functions,	
905
[x86–64]	Sign	extend	
to	
,	
185
[x86–64]	move	if	unsigned	greater,	
217
[x86–64]	move	if	unsigned	greater	or	equal,	
217
[x86–64]	move	if	unsigned	less,	
217
[x86–64]	move	if	unsigned	less	or	equal,	
217
[Y86–64]	move	when	equal,	
357
[x86–64]	move	if	greater,	
217
,	
357
[x86–64]	move	if	greater	or	equal,	
217
,	
357
[x86–64]	move	if	less,	
217
,	
357
[x86–64]	move	if	less	or	equal,	
217
,	
357
[x86–64]	move	if	not	unsigned	greater,	
217
[x86–64]	move	if	unsigned	greater	or	equal,	
217</p>
<pre><code>[x86–64]	move	if	not	unsigned	less,	
</code></pre>
<p>217
[x86–64]	move	if	not	unsigned	less	or	equal,	
217
[x86–64]	move	if	not	equal,	
217
,	
357
[x86–64]	move	if	not	greater,	
217
[x86–64]	move	if	not	greater	or	equal,	
217
[x86–64]	move	if	not	less,	
217
[x86–64]	move	if	not	less	or	equal,	
217
[x86–64]	move	if	nonnegative,	
217
[x86–64]	move	if	not	zero,	
217
[x86–64]	move	if	even	parity,	
324
[x86–64]	move	if	negative,	
217
[x86–64]	move	if	zero,	
217
CMP
[instruction	class]	Compare,	
202
[x86–64]	compare	byte,	
202
[x86–64]	compare	double	word,	
202
[x86–64]	compare	double	word,	
202
[x86–64]	compare	word,	
202
cmtest	script,	
465
CO	(cache	block	offset),	
823
coalescing	blocks,	
860
with	boundary	tags,	
851
–
854
free,	
850
memory,	
847
Cocke,	John,	
361
code
performance	strategies,	
561
–
562
profilers,	
562
–
564
representing,	
49
–
50</p>
<p>self-modifying,	
435
Y86	instructions,	
358
,	
359
–
360
code	motion,	
508
code	segments,	
696
,	
697
–
698
Cohen,	Danny,	
43
cold	caches,	
612
cold	misses,	
612
Cold	War,	
931
collectors,	garbage,	
839
,	
866
basics,	
866
–
867
conservative,	
867
,	
869
–
870
Mark	&amp;	Sweep,	
867
–
870
column	access	strobe	(CAS)	requests,	
583</p>
<p>column-major	sum	function,	
636
combinational	circuits,	
374
,	
374
–
380
combinational	pipelines,	
412
–
414
,	
460
–
462
common	gateway	interface	(CGI)	program,	
953
,	
953
–
955
Compaq	Computer	Corp.	RISC	processors,	
363
compare	byte	instruction,	
202
compare	double	precision,	
306
compare	double	word	instruction,	
202
compare	instructions,	
202
compare	single	precision,	
306
compare	word	instruction,	
202
comparison	operations	for	floating-point	code,	
306
–
309
compilation	phase,	
5
compilation	systems,	
6
,	
6
–
7
compile	time,	
670
compile-time	interpositioning,	
708
–
709
compiler	drivers,	
4
,	
671
–
672
compilers,	
6
,	
164
optimizing	capabilities	and	limitations,	
498
–
502
process,	
169
–
170
purpose,	
171
complement	instruction,	
192
complex	instruction	set	computers	(CISC),	
361
,	
361
–
363
compulsory	misses,	
612
computation	stages	in	pipelining,	
421
–
422
computed	goto,	
233
Computer	Emergency	Response	Team	(CERT),	
100
computer	systems,	
2
concurrency,	
972
ECF	for,	
723</p>
<p>flow	synchronizing,	
776
–
778
and	parallelism,	
24
run,	
733
thread-level,	
24
–
26
concurrent	execution,	
733
concurrent	flow,	
733
,	
733
–
734
concurrent	processes,	
15
,	
16
concurrent	programming,	
972
–
973
deadlocks,	
1027
–
1030
with	I/O	multiplexing,	
978
–
985
library	functions	in,	
1024
–
1025
with	processes,	
973
–
977
races,	
1025
–
1027
reentrancy	issues,	
1023
–
1024
shared	variables,	
992
–
995
summary,	
1030
threads,	
985
–
992
for	parallelism,	
1013
–
1018
safety	issues,	
1020
–
1022
concurrent	programs,	
972
concurrent	servers,	
972
based	on	prethreading,	
1005
–
1013
based	on	processes,	
974
–
975
based	on	threads,	
991
–
992
condition	code	registers,	
171
hazards,	
435
SEQ	timing,	
401
–
402
condition	codes,	
201
,	
201
–
202
accessing,	
202
–
205
x86–64,	
201</p>
<p>Y86–64,	
355
–
357
condition	variables,	
1010
conditional	branches,	
172
,	
209
assembly	form,	
211
condition	codes,	
201
–
202
condition	control,	
209
–
213
moves,	
214
–
220
,	
550
–
553
,	
232
–
238
conflict	misses,	
613
,	
622
–
624
[Unix]	establish	connection	with	server,	
934
,	
934
–
935
connected	descriptors,	
936
,	
936
–
937
connections
EOF	on,	
948
Internet,	
925
,	
929
–
931
I/O	devices,	
596
–
597
persistent,	
952
conservative	garbage	collectors,	
867
,	
869
–
870
constant	words	in	Y86–64,	
359
constants
floating-point	code,	
304
–
305
free	lists,	
856
–
857
maximum	and	minimum	values,	
68
multiplication,	
101
–
103
for	ranges,	
67
–
68
Unix,	
746
content
dynamic,	
953
–
954
serving,	
949
Web,	
948
,	
949
–
950</p>
<p>context	switches,	
16
,	
736
–
737
contexts,	
736
processes,	
16
,	
732
thread,	
986
,	
993
command,	
280
Control	Data	Corporation	6600	processor,	
522
control	dependencies	in	pipelining,	
419
,	
429
control	flow,	
722
exceptional.	
See	
exceptional	control	flow	(ECF)
logical,	
732
,	
732
–
733
machine-language	procedures,	
239
control	hazards,	
429
control	logic	blocks,	
398
,	
398
,	
405
,	
426
control	logic	in	pipelining,	
455
control	mechanism	combinations,	
460
–
462
control	mechanisms,	
459
–
460
design	testing	and	verifying,	
465
implementation,	
462
–
464
special	cases,	
455
–
457
special	conditions,	
457
–
459
control	structures,	
200
–
201
condition	codes,	
200
–
205
conditional	branches,	
209
–
213
conditional	move	instructions,	
214
–
220
jumps,	
205
–
209
loops.	
See	
loops
statements,	
232
–
238
control	transfer,	
241
–
245
,	
722
controllers</p>
<p>disk,	
595
,	
595
–
596
I/O	devices,	
9
memory,	
583
,	
584
conventional	DRAMs,	
582
–
584
conversions
binary
with	hexadecimal,	
36
–
37
signed	and	unsigned,	
70
–
76
to	two's	complement,	
64
,	
72
–
73
,	
97
to	unsigned,	
62
–
63
floating	point,	
125
,	
296
–
301
lowercase,	
509
–
511
number	systems,	
36
–
39
convert	active	socket	to	listening	socket	function,	
935
convert	application-to-network	function,	
926
convert	double	precision	to	integer	instruction,	
297
convert	double	precision	to	quad-word	integer	instruction,	
297
convert	double	to	single	precision	instruction,	
299
convert	host	and	service	names	function,	
937
,	
937
–
940
convert	host-to-network	long	function,	
925
convert	host-to-network	short	function,	
925
convert	integer	to	double	precision	instruction,	
297
convert	integer	to	single	precision	instruction,	
297
convert	network-to-application	function,	
926
convert	network-to-host	long	function,	
925
convert	network-to-host	short	function,	
925
convert	packed	single	to	packed	double	precision	instruction,	
298
convert	quad-word	integer	to	double	precision	instruction,	
297
convert	quad-word	integer	to	single	precision	instruction,	
297
convert	quad	word	to	oct	word	instruction,	
198</p>
<p>convert	single	precision	to	integer	instruction,	
297
convert	single	precision	to	quad-word	integer	instruction,	
297
convert	single	to	double	precision	instruction,	
298
convert	socket	address	to	host	and	service	names	function,	
940
,
940
–
942
function,	
100
descriptor	function,	
909
function,	
86
–
87
copy-on-write	technique,	
835
,	
835
–
836
copying
bytes	in	memory,	
133
descriptor	tables,	
909
text	files,	
900
Core	2	microprocessors,	
168
,	
588
Core	i7	microprocessors,	
25
abstract	operation	model,	
525
–
531
address	translation,	
826
–
828
caches,	
631
Haswell,	
507
memory	mountain,	
641
Nehalem,	
168
page	table	entries,	
826
–
828
QuickPath	interconnect,	
588
virtual	memory,	
825
–
828
core	memory,	
757
cores	in	multi-core	processors,	
168
,	
605
,	
972
correct	signal	handling,	
770
–
774
counting	semaphores,	
1003
CPE	(cycles	per	element)	metric,	
502
,	
504
,	
507
–
508</p>
<pre><code>[CS:APP]	text	file	copy,	
</code></pre>
<p>900
CPI	(cycles	per	instruction)
five-stage	pipelines,	
471
in	performance	analysis,	
464
–
468
CPUs.	
See	
central	processing	units	(CPUs)
[x86–64]	convert	quad	word	to	oct	word,	
198
,	
199
CR	(carriage	return)	characters,	
892
CR3	register,	
826
Cray	1	supercomputer,	
353
create/change	environment	variable	function,	
752
create	child	process	function,	
740
,	
741
–
743
create	thread	function,	
988
critical	path	analysis,	
498
critical	paths,	
525
,	
529
critical	sections	in	progress	graphs,	
1000
CS:APP
header	files,	
746
wrapper	functions,	
738
,	
1041
[CS:APP]	CS:APP	wrapper	functions,	
738
,	
1041
[CS:APP]	CS:APP	header	file,	
738
,	
746
,	
1041
[Unix]	Unix	shell	program,	
753
CT	(cache	tags),	
823
ctest	script,	
465
function,	
1024
[CS:APP]	thread-safe	non-reentrant	wrapper	for	
,	
1022
Ctrl+C	key
nonlocal	jumps,	
785
signals,	
758
,	
761
,	
795
Ctrl+Z	key,	
761
,	
795</p>
<p>current	working	directory,	
892
[x86–64]	convert	double	to	single	precision,	
299
[x86–64]	convert	single	to	double	precision,	
298
cycles	per	element	(CPE)	metric,	
502
,	
504
,	
507
–
508
cycles	per	instruction	(CPI)
five-stage	pipelines,	
471
in	performance	analysis,	
464
–
468
cylinders
disk,	
591
spare,	
596
[x86–64]	low	order	16	bits	of	register	
,	
180</p>
<p>d-caches	(data	caches),	
520
,	
631
data
conditional	transfers,	
214
–
220
forwarding,	
436
–
439
,	
437
sizes,	
39
–
42
data	alignment,	
273
,	
273
–
276
data	caches	(d-caches),	
520
,	
631
data	dependencies	in	pipelining,	
419
,	
429
–
431
data-flow	graphs,	
525
–
530
data	formats	in	machine-level	programming,	
177
–
179
data	hazards,	
429
avoiding,	
441
–
444
classes,	
435
forwarding	for,	
436
–
439
load/use,	
439
–
441
stalling,	
433
–
436
Y86–64	pipelining,	
429
–
433
data	memory	in	SEQ	timing,	
401
data	movement	instructions,	
182
–
189
data	references
locality,	
606
–
607
PIC,	
704
–
705
section,	
674
data	segments,	
696
data	structures,	
265
data	alignment,	
273
–
276
structures,	
265
–
269
unions,	
269
–
273
data	transfer,	procedures,	
245
–
248</p>
<p>data	types.	
See	
types
database	transactions,	
919
datagrams,	
924
DDD
debugger	with	graphical	user	interface,	
279
DDR	SDRAM	(double	data-rate	synchronous	DRAM),	
586
deadlocks,	
1027
,	
1027
–
1030
deallocate	heap	storage	function,	
841
section,	
675
debugging,	
279
–
280
DEC
[instruction	class]	decrement,	
192
decimal	notation,	
32
decimal	system	conversions,	
37
–
39
declarations
arrays,	
255
–
256
,	
263
pointers,	
41
public	and	private,	
677
structures,	
265
–
269
unions,	
269
–
273
decode	stage
instruction	processing,	
385
,	
387
–
397
PIPE	processor,	
449
–
453
sequential	processing,	
400
Y86–64	implementation,	
406
–
408
Y86–64	pipelining,	
423
decoding	instructions,	
519
decrement	instruction,	
192
,	
194
deep	copies,	
1024
deep	pipelining,	
418
–
419
default	actions	with	signal,	
762</p>
<p>default	behavior	for	child	processes,	
744
default	function	code,	
404
deferred	coalescing,	
850
[C]	preprocessor	directive
command,	
280
delete	environment	variable	function,	
752
DELETE	method	in	HTTP,	
951
delete	signal	from	signal	set	instruction,	
765
delivering	signals,	
758
delivery	mechanisms	for	protocols,	
922
demand	paging,	
810
demand-zero	pages,	
833
demangling	process	(C++	and	Java),	
680
,	
680
denormalized	floating-point	value,	
114
,	
114
–
116
dependencies
control	in	pipelining	systems,	
419
,	
429
data	in	pipelining	systems,	
419
,	
429
–
431
reassociation	transformations,	
542
write/read,	
557
–
559</p>
<div style="break-before: page; page-break-before: always;"></div><p>dereferencing	pointers,	
48
,	
188
,	
257
,	
277
,	
870
–
871
descriptor	sets,	
977
,	
978
descriptor	tables,	
907
,	
909
descriptors,	
891
connected	and	listening,	
936
,	
936
–
937
socket,	
934
destination	hosts,	
922
detach	thread	function,	
990
detached	threads,	
989
detaching	threads,	
989
–
990</p>
<pre><code>[x86–64]	low	order	16	bits	of	register	
</code></pre>
<p>,	
180
diagrams
hardware,	
398
pipeline,	
413
Digital	Equipment	Corporation,	
56
Dijkstra,	Edsger,	
1001
–
1002
[x86–64]	low	order	8	of	register	
,	
180
DIMM	(dual	inline	memory	module),	
584
direct	jumps,	
206
direct-mapped	caches,	
617
conflict	misses,	
622
–
624
example,	
619
–
621
line	matching,	
618
line	replacement,	
619
set	selection,	
618
word	selection,	
619
direct	memory	access	(DMA),	
11
,	
598
directives,	assembler,	
176
,	
366
directories
description,	
891
,	
891
–
892
reading	contents,	
905
–
906
directory	streams,	
905
dirty	bits
in	cache,	
630
Core	i7,	
827
dirty	pages,	
827
command,	
280
disassemblers,	
44
,	
69
,	
173
,	
173
–
174
disks,	
589</p>
<p>accessing,	
597
–
600
anatomy,	
600
backups,	
611
capacity,	
591
,	
591
–
592
connecting,	
596
–
597
controllers,	
595
,	
595
–
596
geometry,	
590
–
591
logical	blocks,	
595
–
596
operation,	
592
–
595
trends,	
602
distributing	software,	
701
division
floating-point,	
302
instructions,	
198
–
200
Linux/x86–64	system	errors,	
729
by	powers	of	2,	
103
–
107
[x86–64]	unsigned	divide,	
198
,	
200
[x86–64]	low	order	8	of	register	
,	
180
[Unix]	close	shared	library,	
702
[Unix]	report	shared	library	error,	
702
DLL	(dynamic	link	library),	
699
[Unix]	open	shared	libary,	
701
[Unix]	get	address	of	shared	library	symbol,	
702
DMA	(direct	memory	access),	
11
,	
598
DMA	transfer,	
598
DNS	(domain	name	system),	
928
[C]	variant	of	
loop,	
220
–
223
statement,	
220
[CS:APP]	T
INY
helper	function,	
956
,	
958
,	
958
–
959</p>
<p>dollar	signs	($)	for	immediate	operands,	
181
domain	names,	
925
,	
927
–
929
domain	name	system	(DNS),	
928
[CS:APP]	vector	dot	product,	
622
dots	(.)	in	dotted-decimal	notation,	
926
dotted-decimal	notation,	
926
,	
926
[C]	double-precision	floating	point,	
124
,	
125
[C]	integer	data	type,	
41
double	data-rate	synchronous	DRAM	(DDR	SDRAM),	
586
floating-point	declaration,	
178
double-precision	addition	instruction,	
302
double-precision	division	instruction,	
302
double-precision	maximum	instruction,	
302
double-precision	minimum	instruction,	
302
double-precision	multiplication	instruction,	
302
double-precision	representation	C,	
41
,	
124
–
126
IEEE,	
113
,	
113
machine-level	data,	
178
double-precision	square	root	instruction,	
302
double-precision	subtraction	instruction,	
302
double	word	to	quad	word	instruction,	
199
double	words,	
177
DRAM.	
See	
dynamic	RAM	(DRAM)
DRAM	arrays,	
582
DRAM	cells,	
582
,	
583
drivers,	compiler,	
4
,	
671
–
672
dual	inline	memory	module	(DIMM),	
584
[Unix]	copy	file	descriptor,	
909
duplicate	symbol	names,	
680
–
684</p>
<p>dynamic	code,	
290
dynamic	content,	
701
,	
953
–
954
dynamic	link	libraries	(DLLs),	
699
dynamic	linkers,	
699
dynamic	linking,	
699
,	
699
–
701
dynamic	memory	allocation
allocated	block	placement,	
849
allocator	design,	
854
–
856
allocator	requirements	and	goals,	
844
–
845
coalescing	free	blocks,	
850
–
851
coalescing	with	boundary	tags,	
851
–
854
explicit	free	lists,	
862
–
863
fragmentation,	
846
heap	memory	requests,	
850
implementation	issues,	
846
–
847
implicit	free	lists,	
847
–
849
and	
functions,	
840
–
843
overview,	
839
–
840
purpose,	
843
–
844
segregated	free	lists,	
863
–
865
splitting	free	blocks,	
849
–
850
dynamic	memory	allocators,	
839
–
840
dynamic	RAM	(DRAM),	
9
,	
582
caches,	
806
,	
808
,	
808
–
809
conventional,	
582
–
584
enhanced,	
585
–
586
historical	popularity,	
586
modules,	
584
,	
585
vs.	SRAM,	
582</p>
<p>trends,	
602
–
603
dynamic	Web	content,	
949
[x86–64]	low	order	16	bits	of	register	
,	
180</p>
<p>E-way	set	associative	caches,	
624
–
625
[x86–64]	low	order	32	bits	of	register	
,	
180
[x86–64]	low	order	32	bits	of	register	
,	
180
[x86–64]	low	order	32	bits	of	register	
,	
180
ECF.	
See	
exceptional	control	flow	(ECF)
ECHILD	return	code,	
746
–
747
[CS:APP]	read	and	echo	input	lines,	
947
function,	
281
–
282
,	
287
[CS:APP]	counting	version	of	
,	
1012
[CS:APP]	echo	client,	
944
–
945
[CS:APP]	iterative	echo	server,	
936
–
937
,	
947
[CS:APP]	concurrent	echo	server	based	on	threads,
991
[CS:APP]	prethreaded	concurrent	echo	server,
1011
[x86–64]	low	order	32	bits	of	register	
,	
180
[x86–64]	low	order	32	bits	of	register	
,	
180
EDO	DRAM	(extended	data	out	DRAM),	
586
[x86–64]	low	order	32	bits	of	register	
,	
180
EEPROMs	(electrically	erasable
programmable	ROMs),	
587
effective	addresses,	
181
,	
690
effective	cycle	time,	
602
efficiency	of	parallel	programs,	
1019
,	
1019
EINTR	return	code,	
746
electrically	erasable	programmable	ROMs	(EEPROMs),	
587
ELF.	
See	
executable	and	linkable	format	(ELF)
EM64T	processors,	
168</p>
<p>embedded	processors,	
363
encapsulation,	
922
encodings	in	machine-level	programming,	
169
–
170
code	examples,	
172
–
175
code	overview,	
170
–
171
formatting,	
175
–
177
Y86–64	instructions,	
358
–
360
end-of-file	(EOF)	condition,	
891
,	
948
end	of	line	(EOL)	indicators,	
892
entry	points,	
696
,	
697
–
698
environment	variables	lists,	
751
–
752
EOF	(end-of-file)	condition,	
891
,	
948
EOL	(end	of	line)	indicators,	
892
ephemeral	ports,	
930
epilogue	blocks,	
855
EPIPE	error	return	code,	
964
erasable	programmable	ROMs	(EPROMs),	
587
[Unix]	Unix	error	variable,	
1042
error-correcting	codes	for	memory,	
582
error	handling
system	calls,	
737
–
738
Unix	systems,	
1042
–
1043
wrappers,	
738
,	
1041
,	
1043
–
1045
error-reporting	functions,	
737
errors
child	processes,	
745
–
746
link-time,	
7
off-by-one,	
872
race,	
776
,	
776
–
778</p>
<p>reporting,	
1043
synchronization,	
995
[x86–64]	low	order	32	bits	of	register	
,	
180
[x86–64]	low	order	32	bits	of	stack	pointer	register	
,	
180
establish	connection	with	server	functions,	
934
,	
934
–
935
,	
942
–
944
establish	listening	socket	function,	
944
,	
944
etest	script,	
465
Ethernet	segments,	
920
,	
920
Ethernet	technology,	
920
EUs	(execution	units),	
518
,	
520
[CS:APP]	shell	helper	routine,	
754
,	
755
event-driven	programs,	
980
based	on	I/O	multiplexing,	
980
–
985
based	on	threads,	
1013
events,	
723
scheduling,	
763
state	machines,	
980
evicting	blocks,	
612
exabytes,	
39
excepting	instructions,	
445
exception	handlers,	
724
,	
724
exception	handling
in	instruction	processing,	
385
Y86–64,	
363
–
364
,	
444
–
447
exception	numbers,	
725
exception	table	base	registers,	
725
exception	tables,	
725
,	
725
exceptional	control	flow	(ECF),	
722
exceptions,	
723
–
731</p>
<h2>importance,	
722
–
723
nonlocal	jumps,	
781
–
786
process	control.	
See	
processes
signals.	
See	
signals
summary,	
787
system	call	error	handling,	
737
–
738
exceptions,	
723
anatomy,	
723
–
724
asynchronous,	
726
classes,	
726
–
728
data	alignment,	
276
handling,	
724
–
726
Linux/x86–64	systems,	
729
–
731
status	code	for,	
404
synchronous,	
727
Y86,	
356
exclamation	points	!	for	
NOT
operation,	
373
EXCLUSIVE</h2>
<h2>OR
Boolean	operation,	
51
exclusive-or	instruction
x86–64,	
192
Y86–64,	
356
EXCLUSIVE</h2>
<p>OR
operation	in	execute	stage,	
408
exclusive-or	packed	double	precision	instruction,	
305
exclusive-or	packed	single	precision	instruction,	
305
executable	and	linkable	format	(ELF),	
673
executable	object	files,	
695
–
696
header	tables,	
674
,	
696
headers,	
674
–
675
relocation,	
690
symbol	tables,	
675
–
679</p>
<p>executable	code,	
170
executable	object	files,	
4
creating,	
672
description,	
672
fully	linked,	
696
loading,	
697
–
698
running,	
7
–
8
executable	object	programs,	
4
execute	access,	
289
execute	disable	bit,	
827
execute	stage
instruction	processing,	
385
,	
387
–
397
PIPE	processor,	
453
–
454
sequential	processing,	
400
sequential	Y86–64	implementation,	
408
–
409
Y86–64	pipelining,	
423
execution
concurrent,	
733
parallel,	
734
speculative,	
519
,	
519
,	
549
–
550
tracing,	
387
,	
394
–
395
,	
403
execution	code	regions,	
289
–
290
execution	units	(EUs),	
518
,	
520
[Unix]	load	program,	
750
arguments	and	environment	variables,	
750
–
752
child	processes,	
699
,	
701
loading	programs,	
697
running	programs,	
753
–
756
virtual	memory,	
836
–
837</p>
<pre><code>[C	Stdlib]	terminate	process,	
</code></pre>
<p>739
exit	status,	
739
,	
745
expanding	bit	representation,	
76
–
80
expansion	slots,	
597
explicit	allocator	requirements	and	goals,	
844
–
845
explicit	dynamic	memory	allocators,	
839
–
840
explicit	free	lists,	
862
–
863
explicit	thread	termination,	
988
explicit	waiting	for,	signals,	
778
–
781
explicitly	reentrant	functions,	
1023
exploit	code,	
284
exponents	in	floating-point	representation,	
112
[CS:APP]	allocator:	extend	heap,	
858
extended	data	out	DRAM	(EDO	DRAM),	
586
extended	precision	floating-point	representation,	
137
,	
137
external	exceptions	in	pipelining,	
444
external	fragmentation,	
846
,	
846</p>
<p>fall	through	in	
statements,	
233
false	fragmentation,	
850
fast	page	mode	DRAM	(FPM	DRAM),	
585
fault	exception	class,	
726
faulting	instructions,	
727
faults,	
728
Linux/x86–64	systems,	
729
,	
832
–
833
Y86–64	pipelining	caches,	
470
[Unix]	clear	bit	in	descriptor	set,	
977
,	
978
[Unix]	bit	turned	on	in	descriptor	set,	
977
,	
978
,	
980
[Unix]	set	bit	in	descriptor	set,	
977
,	
978
[Unix]	clear	descriptor	set,	
977
,	
978
feedback	in	pipelining,	
419
–
421
,	
425
feedback	paths,	
396
,	
419
fetch	file	metadata	function,	
903
fetch	stage
instruction	processing,	
384
,	
387
–
397
PIPE	processor,	
447
–
449
SEQ,	
404
–
406
sequential	processing,	
400
Y86–64	pipelining,	
423
fetches,	locality,	
607
–
608
function,	
282
Fibonacci	(Pisano),	
32
field-programmable	gate	arrays	(FPGAs),	
467
FIFOs,	
977
file	descriptors,	
891
file	position,	
891
file	tables,	
736
,	
906</p>
<p>FILE
type,	
911
filenames,	
891
files,	
19
as	abstraction,	
27
anonymous,	
833
binary,	
3
metadata,	
903
–
904
object.	
See	
object	files
register,	
10
,	
171
,	
358
–
359
,	
382
–
383
,	
401
,	
521
regular,	
833
sharing,	
906
–
908
system-level	I/O.	
See	
system-level	I/O
types,	
891
–
893
Unix,	
890
,	
890
–
891
FINGER
command,	
284
daemon,	
284
command,	
280
firmware,	
587
first-fit	block	placement	policy,	
849
,	
849
first-level	domain	names,	
927
first	readers-writers	problem,	
1008
fits,	segregated,	
863
,	
864
–
865
five-stage	pipelines,	
471
fixed-size	arithmetic,	
85
fixed-size	arrays,	
260
–
262
fixed-size	integer	types,	
41
,	
67
flash	memory,	
587
flash	translation	layers,	
600
–
601
flat	addressing,	
167</p>
<pre><code>[C]	single-precision	floating	point,	
</code></pre>
<p>124
floating-point	declaration,	
178
floating-point	code
architecture,	
293
,	
293
–
296
arithmetic	operations,	
302
–
304
bitwise	operations,	
305
–
306
comparison	operations,	
306
–
309
constants,	
304
–
305
movement	and	conversion	operations,	
296
–
301
observations,	
309
in	procedures,	
301
–
302
floating-point	representation	and	programs,	
108
–
109
arithmetic,	
33
C,	
124
–
126
denormalized	values,	
114
,	
114
–
116
encodings,	
32
extended	precision,	
137
,	
137
fractional	binary	numbers,	
109
–
112
IEEE,	
112
–
114
normalized	value,	
113
–
114
operations,	
122
–
124
overflow,	
127
pi,	
140
rounding,	
120
,	
120
–
122
special	values,	
115
support,	
40
x87	processors,	
167
flows
concurrent,	
733
,	
733
–
734</p>
<p>control,	
722
logical,	
732
,	
732
–
733
parallel,	
734
synchronizing,	
776
–
778
flushed	instructions,	
522
[Y86–64]	default	function	code,	
404
footers	of	blocks,	
851
[C]	general	loop	statement,	
228
–
232
guarded-do	translation,	
225
jump-to-middle	translation,	
223
forbidden	regions,	
1003
foreground	processes,	
753
[Unix]	create	child	process,	
740
child	processes,	
701
example,	
741
–
743
running	programs,	
753
–
756
virtual	memory,	
836
[CS:APP]	
example,	
741
formal	verification	in	pipelining,	
466
format	strings,	
47
formats	for	machine-level	data,	
177
–
179
formatted	disk	capacity,	
596
formatted	printing,	
47
formatting
disks,	
596
machine-level	code,	
175
–
177
forwarding
for	data	hazards,	
436
–
439
load,	
477</p>
<p>forwarding	priority,	
451
–
452
FPGAs	(field-programmable	gate	arrays),	
467
FPM	DRAM	(fast	page	mode	DRAM),	
585
[C	Stdlib]	function,	
47
fractional	binary	numbers,	
109
–
112
fractional	floating-point	representation,	
112
–
120
,	
137
fragmentation,	
846
dynamic	memory	allocation,	
846
false,	
850
frame	pointers,	
290
frames
Ethernet,	
920
stack,	
240
,	
240
–
241
,	
276
,	
290
–
293
free	blocks,	
839
coalescing,	
850
–
851
splitting,	
849
–
850
free	bounded	buffer	function,	
1007
[C	Stdlib]	deallocate	heap	storage,	
841
,	
841
–
843
interpositioning	libraries,	
708
wrappers	for,	
711
free	heap	block	function,	
860
free	heap	blocks,	referencing	data	in,	
874
–
875
free	lists
creating,	
857
–
859
dynamic	memory	allocation,	
847
–
849
explicit,	
862
–
863
implicit,	
848
manipulating,	
856
–
857
segregated,	
863
–
865</p>
<p>free	software,	
6
free	up	getaddrinfo	resources	function,	
937
[Unix]	free	up	getaddrinfo	resources,	
937
,	
938
FreeBSD	open-source	operating	system,	
86
–
87
freeing	blocks,	
860
Freescale
processor	family,	
352
RISC	design,	
361
front	side	bus	(FSB),	
588
[Unix]	fetch	file	metadata,	
903
full	duplex	connections,	
929
full	duplex	streams,	
912
fully	associative	caches,	
626
line	matching	and	word	selection,	
627
–
628
set	selection,	
627
fully	linked	executable	object	files,	
696
fully	pipelined	functional	units,	
523
function	calls
performance	strategies,	
561
PIC,	
705
–
707
function	part	in	Y86–64	instruction	specifier,	
358
functional	units,	
520
–
521
,	
523
–
524
functions
pointers	to,	
278
reentrant,	
766
,	
1023
static	libraries,	
684
–
688
system-level,	
730
thread-safe	and	thread-unsafe,	
1020
,	
1020
–
1022
wrapper,	
711</p>
<p>in	Y86	instructions,	
359</p>
<pre><code>[CS:APP]	reports	GAI-style	errors,	
</code></pre>
<p>1043
[Unix]	print	getaddrinfo	error	message,	
938
GAI-style	error	handling,	
1042
,	
1042
–
1043
gaps	between	disk	sectors,	
590
,	
596
garbage,	
866
garbage	collection,	
840
,	
866
garbage	collectors,	
840
,	
866
basics,	
866
–
867
conservative,	
867
,	
869
–
870
Mark&amp;Sweep,	
867
–
870
overview,	
865
–
866
gates,	logic,	
373
GCC
(GNU	compiler	collection)	compiler
code	formatting,	
175
–
176
inline	assembly,	
178
options,	
35
working	with,	
168
–
169
GDB
GNU	debugger,	
173
,	
279
,	
279
–
280
general	protection	faults,	
729
general-purpose	registers,	
179
,	
17
–
180
geometry	of	disks,	
590
–
591
get	address	of	shared	library	symbol	function,	
702
&quot;get	from&quot;	operator	(C++),	
890
GET	method	in	HTTP,	
951
get	parent	process	ID	function,	
739
get	process	group	ID	function,	
759
get	process	ID	function,	
739
get	thread	ID	function,	
988
[Unix]	convert	host	and	service	names,	
937
,	
937
–
940</p>
<pre><code>[C	Stdlib]	read	environment	variable,	
</code></pre>
<p>751
[Unix]	get	DNS	host	entry,	
1024
[Unix]	get	DNS	host	entry,	
1024
[Unix]	convert	socket	address	to	host	and	service	names,
940
,	
940
–
942
function	[C	Stdlib]	security	vulnerability,	
86
–
87
[Unix]	get	process	group	ID,	
759
[Unix]	get	process	ID,	
739
[Unix]	get	parent	process	ID,	
739
[Unix]	function,	
811
function,	
279
,	
281
–
282
GHz	(gigahertz),	
502
giga-instructions	per	second	(GIPS),	
413
gigabytes,	
592
gigahertz	(GHz),	
502
GIPS	(giga-instructions	per	second),	
413
global	IP	Internet.	
See	
Internet
Global	Offset	Table	(GOT),	
705
,	
705
–
707
global	symbols,	
675
global	variable	mapping,	
994
–
995
GNU	compiler	collection.	
See	
GCC
(GNU	compiler	collection)
compiler
GNU	project,	
6
GOT	(global	offset	table),	
705
,	
705
–
707
[C]	control	transfer	statement,	
210
,	
233
goto	code,	
210
GPROF
Unix	profiler,	
562
,	
562
–
563
gradual	underflow,	
115</p>
<p>granularity	of	concurrency,	
985
graphic	user	interfaces	for	debuggers,	
279
graphics	adapters,	
596
graphs
data-flow,	
525
–
530
process,	
741
,	
742
progress.	
See	
progress	graphs
reachability,	
866
greater	than	signs	&gt;
deferencing	operation,	
266
&quot;get	from&quot;	operator,	
890
right	hoinkies,	
909
groups
abelian,	
89
process,	
759
guard	values,	
286
guarded-do	translation,	
225</p>
<pre><code>header	files,	
</code></pre>
<p>686
half-precision	floating-point	representation,	
137
,	
137
[Y86–64]	halt	instruction	execution,	
357
code	for,	
404
–
405
exceptions,	
364
,	
444
–
447
in	pipelining,	
462
handlers
exception,	
724
,	
724
interrupt,	
726
signal,	
758
,	
763
handling	signals
blocking	and	unblocking,	
764
–
765
portable,	
774
–
775
hardware	caches.	
See	
caches	and	cache	memory
hardware	control	language	(HCL),	
372
Boolean	expressions,	
374
–
375
integer	expressions,	
376
–
380
logic	gates,	
373
hardware	description	languages	(HDLs),	
373
,	
467
hardware	exceptions,	
724
hardware	interrupts,	
726
hardware	management,	
14
–
15
hardware	organization,	
8
buses,	
8
I/O	devices,	
9
main	memory,	
9
processors,	
9
–
10
hardware	registers,	
381
–
384
hardware	structure	for	Y86–64,	
396
–
400</p>
<p>hardware	units,	
396
–
398
,	
401
hash	tables,	
567
–
568
Haswell	microarchitecture,	
825
Haswell	microprocessors,	
168
,	
215
,	
294
,	
507
,	
521
,	
523
hazards	in	pipelining,	
354
,	
429
avoiding,	
441
–
444
classes,	
435
forwarding	for,	
436
–
439
load/use,	
439
–
441
overview,	
429
–
433
stalling	for,	
433
–
436
HCL	(hardware	control	language),	
372
Boolean	expressions,	
374
–
375
integer	expressions,	
376
–
380
logic	gates,	
373
HDLs	(hardware	description	languages),	
373
,	
467
head	crashes,	
593
HEAD	method	in	HTTP,	
951
header	files
static	libraries,	
687
system,	
746
header	tables	in	ELF,	
674
,	
696
headers
blocks,	
847
Ethernet,	
920
request,	
951
response,	
952
heap,	
18
,	
18
–
19
,	
839
dynamic	memory	allocation,	
839
–
840
Linux	systems,	
697</p>
<p>referencing	data	in,	
874
–
875
requests,	
850
[CS:APP]	C	hello	program,	
2
,	
10
–
12
command,	
280
helper	functions,	sockets	interface,	
942
–
944
Hennessy,	John,	
361
,	
471
heterogeneous	data	structures,	
265
data	alignment,	
273
–
276
structures,	
265
–
269
unions,	
269
–
273
hexadecimal	(hex)	notation,	
36
,	
36
–
39
hierarchies
domain	name,	
927
storage	devices,	
14
,	
14
,	
609
–
614
high-level	design	performance	strategies,	
561
hit	rates,	
631
hit	time,	
631
hits
cache,	
612
,	
631
write,	
630
[x86–64]	halt	instruction	execution,	
357
[Y86–64]	status	code	indicating	
instruction,	
364
hoinkies,	
909
,	
910
holding	mutexes,	
1003
Horner,	William,	
530
Horner's	method,	
530
host	bus	adapters,	
597
host	bus	interfaces,	
597
host	entries,	
928</p>
<p>host	information	program	command,	
926
HOSTNAME
command,	
926
hosts
client-server	model,	
919
network,	
922
number	of,	
930
sockets	interface,	
937
–
942
htest	script,	
465
HTML	(hypertext	markup	language),	
948
,	
948
–
949
[Unix]	convert	host-to-network	long,	
925
[Unix]	convert	host-to-network	short,	
925
HTTP.	
See	
hypertext	transfer	protocol	(HTTP)
hubs,	
920
hyperlinks,	
948
hypertext	markup	language	(HTML),	
948
,	
948
–
949
hypertext	transfer	protocol	(HTTP),	
948
dynamic	content,	
953
–
954
methods,	
951
–
952
requests,	
951
,	
951
–
952
responses,	
952
,	
952
–
953
transactions,	
950
–
951
hyperthreading,	
24
,	
168
Hyper	Transport	interconnect,	
588</p>
<p>i-caches	(instruction	caches),	
518
,	
631
source	files,	
671
i386	microprocessor,	
167
i486	microprocessor,	
167
IA32	(Intel	Architecture	32-bit)	microprocessors,	
45
,	
168
machine	language,	
165
–
166
registers,	
179
–
180
[Y86–64]	immediate	add,	
369
IBM
Freescale	microprocessors,	
352
,	
361
out-of-order	processing,	
522
RISC	design,	
361
–
363
[Y86–64]	instruction	code	for	
instruction,	
404
ICANN	(Internet	Corporation	for	Assigned	Names	and	Numbers),	
927
icode	(instruction	code),	
384
,	
405
ICUs	(instruction	control	units),	
518
identifiers,	register,	
358
[x86–64]	signed	divide,	
199
[x86–64]	signed	divide,	
198
IDs	(identifiers)
processes,	
739
–
740
register,	
358
–
359
IEEE.	
See	
Institute	for	Electrical	and	Electronics	Engineers
(IEEE)
[C]	conditional	statement,	
211
–
213
ifun	(instruction	function),	
384
,	
405
[Y86–64]	instruction	code	for	
instruction,	
404
[Y86–64]	instruction	code	for	
instruction,	
404
ijk	matrix	multiplication,	
644
–
646
,	
645</p>
<pre><code>[Y86–64]	instruction	code	for	jump	instructions,	
</code></pre>
<p>404
ikj	matrix	multiplication,	
644
–
646
,	
645
illegal	instruction	exceptions,	
404
signal,	
405
immediate	add	instruction,	
369
immediate	coalescing,	
850
immediate	offset,	
181
immediate	operands,	
181
immediate	to	register	move	instruction,	
356
implicit	dynamic	memory	allocators,	
840
implicit	free	lists,	
847
–
849
,	
848
implicit	thread	termination,	
988
implicitly	reentrant	functions,	
1023
implied	leading	1	representation,	
114
[Y86–64]	instruction	code	for	
instruction,	
404
IMUL
[instruction	class]	multiply,	
192
[x86–64]	signed	multiply,	
198
,	
198
[HCL]	set	membership	test,	
381
[Unix]	IP	address	structure,	
925
INC
[instruction	class]	increment,	
192
include	files,	
686
[C]	preprocessor	directive,	
170
instruction,	
194
increment	instruction,	
192
,	
194
indefinite	integer	values,	
125
file,	
950
index	registers,	
181
indexes	for	direct-mapped	caches,	
622
–
624
indirect	jumps,	
206
,	
234</p>
<p>inefficiencies	in	loops,	
508
–
512
[Unix]	convert	network-to-application,	
1024
[Unix]	convert	network-to-application,	
926
[Unix]	convert	application-to-network,	
926
infinity
constants,	
124
representation,	
114
–
115
command,	
280
command,	
280
information,	
2
–
4
information	access	with	x86–64
registers,	
179
–
180
data	movement,	
182
–
189
operand	specifiers,	
180
–
182
information	storage,	
34
addressing	and	byte	ordering,	
42
–
49
bit-level	operations,	
54
–
56
Boolean	algebra,	
50
–
53
code,	
49
–
50
data	sizes,	
39
–
42
disks.	
See	
disks
floating	point.	
See	
floating-point	representation	and	programs
hexadecimal,	
36
–
39
integers.	
See	
integers
locality.	
See	
locality
logical	operations,	
56
–
57
memory.	
See	
memory
segregated,	
863
shift	operations,	
57
–
59</p>
<p>strings,	
49
summary,	
648
function,	
743
function,	
981
,	
983
initial	state	in	progress	graphs,	
999
initialize	nonlocal	handler	jump	function,	
783
initialize	nonlocal	jump	functions,	
783
initialize	read	buffer	function,	
898
,	
900
initialize	semaphore	function,	
1002
initialize	thread	function,	
990
initializing	threads,	
990
inline	assembly,	
178
inline	substitution,	
501
inlining,	
501
[Y86–64]	instruction	code	for	
instruction,	
404
input	events,	
980
input/output.	
See	
I/O	(input/output)
insert	item	in	bounded	buffer	function,	
1007
install	portable	handler	function,	
775
installing	signal	handlers,	
763
Institute	for	Electrical	and	Electronics	Engineers	(IEEE)
description,	
109
floating-point	representation	and	programs,	
112
–
114
denormalized,	
114
normalized,	
113
–
114
special	values,	
115
Standard	754,	
109
standards,	
109
Posix	standards,	
16</p>
<pre><code>signal,	
</code></pre>
<p>405
–
406
instruction	caches	(i-caches),	
518
,	
631
instruction	code	(icode),	
384
,	
405
instruction	control	units	(ICUs),	
518
instruction	function	(ifun),	
384
,	
405
instruction-level	parallelism,	
26
,	
497
,	
518
,	
562
instruction	memory	in	SEQ	timing,	
401
instruction	set	architectures	(ISAs),	
10
,	
27
,	
170
,	
352
instruction	set	simulators,	
366
instructions
classes,	
182
decoding,	
518
excepting,	
445
fetch	locality,	
607
–
608
issuing,	
427
–
428
jump,	
10
,	
205
–
209
load,	
10
low-level.	
See	
machine-level	programming
move,	
214
–
220
,	
550
–
553
operate,	
10
pipelining,	
468
–
469
,	
549
privileged,	
735
store,	
10
update,	
9
–
10
Y86–64.	
See	
Y86–64	instruction	set	architecture
instructions	per	cycle	(IPC),	
471
[C]	integer	data	type,	
40
[HCL]	integer	signal,	
376
data	types,	integral,	
61</p>
<pre><code>constant,	maximum	signed	integer,	
</code></pre>
<p>68
constant,	minimum	signed	integer,	
68
[Unix]	fixed-size,	
41
integer	arithmetic,	
84
,	
192
division	by	powers	of	2,	
103
–
107
multiplication	by	constants,	
101
–
103
overview,	
107
–
108
two's	complement	addition,	
90
–
95
two's	complement	multiplication,	
97
–
101
two's	complement	negation,	
95
unsigned	addition,	
84
–
90
integer	bits	in	floating-point	representation,	
137
integer	expressions	in	HCL,	
376
–
380
integer	indefinite	values,	
125
integer	operation	instruction,	
404
integer	registers	in	x86–64,	
179
–
180
integers,	
32
,	
59
–
60
arithmetic	operations.	
See	
integer	arithmetic
bit-level	operations,	
54
–
56
bit	representation	expansion,	
76
–
80
byte	order,	
43
–
44
data	types,	
60
–
62
shift	operations,	
57
–
59
signed	and	unsigned	conversions,	
70
–
76
signed	vs.	unsigned	guidelines,	
83
–
84
truncating,	
81
–
82
two's	complement	representation,	
64
–
70
unsigned	encoding,	
62
–
64
integral	data	types,	
60
,	
60
–
62</p>
<p>integration	of	caches	and	VM,	
817
Intel	assembly-code	format,	
177
,	
294
,	
311
Intel	Corporation,	
165
Intel	microprocessors
8086,	
26
,	
167
80286,	
167
Core	2,	
168
,	
588
Core	i7.	
See	
Core	i7	microprocessors
data	alignment,	
276
evolution,	
167
–
168
floating-point	representation,	
137
Haswell,	
168
,	
215
,	
294
,	
523
i386,	
167
i486,	
167
northbridge	and	southbridge	chipsets,	
588
out-of-order	processing,	
522
Pentium,	
167
Pentium	II,	
167
Pentium	III,	
167
–
168
Pentium	4,	
168
Pentium	4E,	
168
PentiumPro,	
167
,	
522
Sandy	Bridge,	
168
x86–64.	
See	
x86–64	microprocessors
Y86–64.	
See	
Y86–64	instruction	set	architecture
interconnected	networks	(internets),	
921
,	
921
–
922
interfaces
bus,	
588
host	bus,	
597
interlocks,	load,	
441</p>
<p>internal	exceptions	in	pipelining,	
444
internal	fragmentation,	
846
internal	read	function,	
901
International	Standards	Organization	(ISO),	
4
,	
35
Internet,	
921
connections,	
929
–
931
domain	names,	
927
–
929
IP	addresses,	
925
–
927
organization,	
924
–
925
origins,	
931
internet	addresses,	
922
Internet	Corporation	for	Assigned	Names	and	Numbers	(ICANN),	
927
Internet	domain	names,	
925
Internet	Domain	Survey
,	
930
Internet	hosts,	number	of,	
930
Internet	Protocol	(IP),	
924
Internet	Software	Consortium,	
930
Internet	worms,	
284
internets	(interconnected	networks),	
921
,	
921
–
922
interpositioning	libraries,	
707
,	
707
–
708
compile-time,	
708
–
709
link-time,	
708
,	
710
run-time,	
710
–
712
interpretation	of	bit	patterns,	
32
interprocess	communication	(IPC),	
977
interrupt	handlers,	
726
interruptions,	
764
interrupts,	
726
,	
726
–
727
interval	counting	schemes,	
564</p>
<pre><code>[C]	maximum	value	of	
</code></pre>
<p>N
-bit	signed	data	type,	
67
[C]	minimum	value	of	
N
-bit	signed	data	type,	
67
[C]	
N
-bit	signed	integer	data	type,	
67
fixed-size	integer	types,	
198
invalid	address	status	code,	
364
invariants,	semaphore,	
1002
I/O	(input/output),	
9
,	
890
memory-mapped,	
598
ports,	
598
redirection,	
909
,	
909
–
910
system-level.	
See	
system-level	I/O
Unix,	
19
,	
890
,	
890
–
891
I/O	bridges,	
587
I/O	buses,	
588
,	
596
,	
598
I/O	devices,	
9
addressing,	
598
connecting,	
596
–
597
I/O	multiplexing,	
973
concurrent	programming	with,	
978
–
985
event-driven	servers	based	on,	
980
–
985
pros	and	cons,	
985
[Y86–64]	instruction	code	for	integer	operation	instruction,	
404
IP	(Internet	Protocol),	
924
IP	address	structure,	
925
,	
926
IP	addresses,	
924
,	
925
–
927
IPC	(instructions	per	cycle),	
471
IPC	(interprocess	communication),	
977
iPhone	5S,	
353
[Y86–64]	instruction	code	for	
instruction,	
404</p>
<pre><code>[Y86–64]	instruction	code	for	
instruction,	
</code></pre>
<p>404
IPv6,	
925
[Y86–64]	instruction	code	for	
instruction,	
404
[Y86–64]	instruction	code	for	
instruction,	
404
[Y86–64]	immediate	to	register	move,	
356
,	
404
[Y86–64]	instruction	code	for	
instruction,	
404
ISAs	(instruction	set	architectures),	
10
,	
27
,	
170
,	
352
ISO	(International	Standards	Organization),	
4
,	
35
ISO	C11	C	standard,	
35
ISO	C90	C	standard,	
35
ISO	C99	C	standard,	
35
,	
41
,	
324
integral	data	types,	
67
static	libraries,	
684
–
688
function,	
869
issue	time	for	arithmetic	operations,	
523
issuing	instructions,	
427
–
428
iterative	servers,	
946
iterative	sorting	routines,	
567</p>
<pre><code>[x86–64]	jump	if	unsigned	greater,	
</code></pre>
<p>206
[x86–64]	jump	if	unsigned	greater	or	equal,	
206
Java	language,	
677
byte	code,	
310
linker	symbols,	
680
numeric	ranges,	
68
objects,	
266
–
267
software	exceptions,	
723
–
724
,	
786
threads,	
1030
Java	monitors,	
1010
Java	Native	Interface	(JNI),	
704
[x86–64]	jump	if	unsigned	less,	
206
[x86–64]	jump	if	unsigned	less	or	equal,	
206
[Y86–64]	jump	when	equal,	
357
,	
394
[x86–64]	jump	if	greater,	
206
,	
357
[x86–64]	jump	if	greater	or	equal,	
206
,	
357
jik	matrix	multiplication,	
644
–
646
,	
645
jki	matrix	multiplication,	
644
–
646
,	
645
[x86–64]	jump	if	less,	
206
,	
357
[x86–64]	jump	if	less	or	equal,	
206
,	
357
[x86–64]	jump	unconditionally,	
206
,	
357
[x86–64]	jump	if	not	unsigned	greater,	
206
[x86–64]	jump	if	unsigned	greater	or	equal,	
206
[x86–64]	jump	if	not	unsigned	less,	
206
[x86–64]	jump	if	not	unsigned	less	or	equal,	
206
[x86–64]	jump	if	not	equal,	
206
,	
357
[x86–64]	jump	if	not	greater,	
206
[x86–64]	jump	if	not	greater	or	equal,	
206</p>
<p>JNI	(Java	Native	Interface),	
704
[x86–64]	jump	if	not	less,	
206
[x86–64]	jump	if	not	less	or	equal,	
206
[x86–64]	jump	if	nonnegative,	
206
[x86–64]	jump	if	not	zero,	
206
jobs,	
760
joinable	threads,	
989
[x86–64]	jump	when	parity	flag	set,	
306
[x86–64]	jump	if	negative,	
206
jtest	script,	
465
jump	if	greater	instruction,	
206
,	
357
jump	if	greater	or	equal	instruction,	
206
,	
357
jump	if	less	instruction,	
206
,	
357
jump	if	less	or	equal	instruction,	
206
,	
357
jump	if	negative	instruction,	
206
jump	if	nonnegative	instruction,	
206
jump	if	not	equal	instruction,	
206
,	
357
jump	if	not	greater	instruction,	
206
jump	if	not	greater	or	equal	instruction,	
206
jump	if	not	less	instruction,	
206
jump	if	not	less	or	equal	instruction,	
206
jump	if	not	unsigned	greater	instruction,	
206
jump	if	not	unsigned	less	instruction,	
206
jump	if	not	unsigned	less	or	equal	instruction,	
206
jump	if	not	zero	instruction,	
206
jump	if	unsigned	greater	instruction,	
206
jump	if	unsigned	greater	or	equal	instruction,	
206
jump	if	unsigned	less	instruction,	
206
jump	if	unsigned	less	or	equal	instruction,	
206</p>
<p>jump	if	zero	instruction,	
206
jump	instructions,	
10
,	
205
–
209
,	
404
direct,	
206
indirect,	
206
,	
234
instruction	code	for,	
404
nonlocal,	
723
,	
781
,	
781
–
786
targets,	
206
jump	tables,	
233
,	
234
–
235
,	
725
jump-to-middle	translation,	
223
jump	unconditionally	instruction,	
206
,	
206
jump	when	equal	instruction,	
357
jump	when	parity	flag	set	instruction,	
306
just-in-time	compilation,	
290
,	
310
[x86–64]	jump	if	zero,	
206</p>
<p>k
×	1	loop	unrolling,	
531
k
×	1
a
loop	unrolling,	
544
k
×	
k
loop	unrolling,	
539
–
540
K&amp;R	(C	book),	
4
Kahan,	William,	
109
Kahn,	Robert,	
931
kernel	mode
exception	handlers,	
726
processes,	
734
–
736
,	
735
system	calls,	
728
kernels,	
17
,	
19
,	
698
exception	numbers,	
725
virtual	memory,	
830
–
831
Kernighan,	Brian,	
2
,	
4
,	
16
,	
35
,	
278
,	
914
Kerrisk,	Michael,	
914
keyboard,	signals	from,	
760
–
761
kij	matrix	multiplication,	
644
–
646
,	
645
[Unix]	send	signal,	
761
command	in	
GDB
debugger,	
280
[CS:APP]	
example,	
761
kji	matrix	multiplication,	
644
–
646
,	
645
Knuth,	Donald,	
849
,	
851
[Unix]	Unix	shell	program,	
753</p>
<h2>l	suffix,	
179
L1	cache,	
13
,	
615
L2	cache,	
13
,	
615
L3	cache,	
615
labels	for	jump	instructions,	
205
LANs	(local	area	networks),	
920
,	
920
–
922
last-in,	first	out	discipline,	
189
last-in	first-out	(LIFO)	free	list	order,	
863
latency
arithmetic	operations,	
523
,	
524
disks,	
594
instruction,	
413
load	operations,	
554
–
555
pipelining,	
412
latency	bounds,	
518
,	
524
lazy	binding,	
706
LD
Unix	static	linker,	
672
LD</h2>
<p>LINUX
.
SO
linker,	
699
environment	variable,	
710
–
712
LDD
tool,	
713
instruction,	
102
leaf	procedures,	
241
leaks,	memory,	
875
,	
992
[x86–64]	load	effective	address,	
191
,	
191
–
192
,	
277
least-frequently-used	(LFU)	replacement	policies,	
626
least-recently-used	(LRU)	replacement	policies,	
612
,	
626
least	squares	fit,	
502
,	
504
[x86–64]	prepare	stack	for	return	instruction,	
292
left	hoinkies	(&lt;),	
910</p>
<p>length	of	strings,	
83
less	than	signs	&lt;
left	hoinkies,	
909
&quot;put	to&quot;	operator,	
890
levels
optimization,	
498
storage,	
609
–
610
LF	(line	feed)	characters,	
892
LFU	(least-frequently-used)	replacement	policies,	
626
library,	
911
,	
698
libraries
in	concurrent	programming,	
1024
–
1025
header	files,	
83
interpositioning,	
707
,	
707
–
712
shared,	
19
,	
699
,	
699
–
701
standard	I/O,	
911
static,	
684
,	
684
–
688
LIFO	(last-in	first-out)	free	list	order,	
863
file	for	numeric	limit	declarations,	
67
–
68
,	
77
line	feed	(LF)	characters,	
892
line	matching
direct-mapped	caches,	
618
fully	associative	caches,	
626
set	associative	caches,	
625
–
626
line	replacement
direct-mapped	caches,	
619
set	associative	caches,	
626
section,	
675</p>
<p>linear	address	spaces,	
804
link-time	errors,	
7
link-time	interpositioning,	
708
,	
710
linkers	and	linking,	
5
,	
164
,	
170
compiler	drivers,	
671
–
672
dynamic,	
699
,	
699
–
701
library	interpositioning,	
707
,	
707
–
712
object	files,	
673
,	
673
–
674
executable,	
695
–
698
loading,	
697
–
698
relocatable,	
674
–
675
tools	for,	
713
overview,	
670
–
671
position-independent	code,	
704
–
707
relocation,	
689
–
695
shared	libraries	from	applications,	
701
–
703
static,	
672
summary,	
713
–
714
symbol	resolution,	
679
–
689
symbol	tables,	
675
–
679
virtual	memory	for,	
811
–
812
linking	phase,	
6
links	in	directories,	
891
Linux	operating	system,	
20
,	
45
code	segments,	
697
–
698
dynamic	linker	interfaces,	
702
and	ELF,	
673
exceptions,	
729
–
731
files,	
891
–
893
signals,	
756</p>
<p>static	libraries,	
685
–
686
virtual	memory,	
830
–
833
Lisp	language,	
85
[Unix]	convert	active	socket	to	listening	socket,	
935
listening	descriptors,	
936
–
937
listening	sockets,	
935
little-endian	ordering	convention,	
42
,	
42
–
44
load	effective	address	instruction,	
191
–
193
,	
277
load	forwarding	in	PIPE,	
477
load	instructions,	
10
load	interlocks,	
441
load	operations
example,	
588
process,	
519
–
520
load	penalty	in	CPI,	
467
load	performance	of	memory,	
554
–
555
load	program	function,	
750
load-store	architecture	in	CISC	vs.	RISC,	
362
load	time	for	code,	
670
load/use	data	hazards,	
439
,	
439
–
441
loaders,	
672
,	
697
loading
concepts,	
699
executable	object	files,	
697
–
698
process,	
697
programs,	
750
–
752
shared	libraries	from	applications,	
701
–
703
virtual	memory	for,	
812
local	area	networks	(LANs),	
920
,	
920
–
922</p>
<p>local	automatic	variables,	
994
local	registers,	
527
local	static	variables,	
994
,	
994
–
995
local	storage
registers,	
251
–
253
stack,	
248
–
251
local	symbols,	
676
locality,	
13
,	
580
,	
604
–
605
blocking	for,	
647
caches,	
643
–
647
,	
810
exploiting,	
647
forms,	
604
,	
614
instruction	fetches,	
607
–
608
program	data	references,	
606
–
607
summary,	
608
–
609
function,	
1024
lock-and-copy	technique,	
1022
,	
1022
locking	mutexes
lock	ordering	rule,	
1029
for	semaphores,	
1003
logic	design,	
372
combinational	circuits,	
374
–
380
,	
413
logic	gates,	
373
,	
373
memory	and	clocking,	
381
–
384
set	membership,	
380
–
381
logic	gates,	
373
logic	synthesis,	
355
,	
373
,	
467
logical	blocks
disks,	
595
,	
595
–
596</p>
<p>SSDs,	
601
logical	control	flow,	
732
,	
732
–
733
logical	operations,	
56
–
57
,	
191
discussion,	
196
–
197
load	effective	address,	
191
–
193
shift,	
58
,	
104
,	
192
,	
194
–
196
special,	
197
–
200
unary	and	binary,	
194
[C]	integer	data	type,	
40
–
41
,	
61
–
62
double	[C]	extended-precision	floating	point,	
125
,	
137
floating-point	declaration,	
178
long	words	in	machine-level	data,	
179
[C	Stdlib]	nonlocal	jump,	
723
,	
783
,	
783
loop	registers,	
527
loop	unrolling,	
502
,	
504
,	
531
Core	i7,	
572
k
×	1,	
531
k
×	1
a
,	
544
k
×	
k
,	
539
–
540
overview,	
531
–
535
with	reassociation	transformations,	
541
–
543
loopback	addresses,	
928
loops,	
220
,	
220
–
223
,	
228
–
232
inefficiencies,	
508
–
512
reverse	engineering,	
222
segments,	
526
–
527
for	spatial	locality,	
643
–
647</p>
<p>,	
223
–
228
low-level	instructions.	
See	
machine-level	programming
low-level	optimizations,	
562
lowercase	conversions,	
509
–
511
LRU	(least-recently-used)	replacement	policies,	
612
,	
626
command,	
892
[Unix]	function,	
896
–
897
lvalue	(C)	assignable	value	for	pointers,	
277</p>
<p>Mac	OS	X	(Apple	Macintosh)	operating	system,	
27
machine	checks,	
729
machine	code,	
164
machine-level	programming
arithmetic.	
See	
arithmetic
arrays.	
See	
arrays
buffer	overflow.	
See	
buffer	overflow
control.	
See	
control	structures
data-flow	graphs	from,	
525
–
529
data	formats,	
177
–
179
data	movement	instructions,	
182
–
189
encodings,	
169
–
177
floating	point.	
See	
floating-point	code
GDB
debugger,	
279
–
280
heterogeneous	data	structures.	
See	
heterogeneous	data
structures
historical	perspective,	
166
–
169
information	access,	
179
–
180
instructions,	
4
operand	specifiers,	
180
–
182
overview,	
164
–
166
pointer	principles,	
278
procedures.	
See	
procedures
x86–64.	
See	
x86–64	microprocessors
macros	for	storage	allocators,	
856
–
857
main	memory,	
9
accessing,	
587
–
589
memory	modules,	
584
main	threads,	
986</p>
<pre><code>[C	Stdlib]	allocate	heap	storage,	
</code></pre>
<p>35
,	
324
,	
697
,	
839
–
840
,	
840
alignment	with,	
276
declaration,	
134
–
135
dynamic	memory	allocation,	
840
–
843
interpositioning	libraries,	
708
wrappers	for,	
711
command,	
48
mandatory	alignment,	
276
mangling	process	(C++	and	Java),	
680
many-core	processors,	
471
map	disk	object	into	memory	function,	
837
mapping
memory.	
See	
memory	mapping
variables,	
994
–
995
mark	phase	in	Mark&amp;Sweep,	
867
Mark&amp;Sweep	algorithm,	
866
Mark&amp;Sweep	garbage	collectors,	
867
,	
867
–
870
masking	operations,	
55
matrices
adjacency,	
660
multiplying,	
643
–
647
maximum	floating-point	instructions,	
302
maximum	two's	complement	number,	
66
maximum	unsigned	number	function,	
63
maximum	values,	constants	for,	
68
McCarthy,	John,	
866
McIlroy,	Doug,	
16
media	instructions,	
294
[CS:APP]	heap	model,	
855</p>
<pre><code>[CS:APP]	sbrk	emulator,	
</code></pre>
<p>855
membership,	set,	
380
–
381
[Unix]	copy	bytes	from	one	region	of	memory	to	another,	
133
memory,	
580
accessing,	
587
–
589
aliasing,	
499
,	
500
associative,	
625
caches.	
See	
caches	and	cache	memory
copying	bytes	in,	
133
data	alignment	in,	
273
–
276
data	hazards,	
435
design,	
384
dynamic.	
See	
dynamic	memory	allocation
hazards,	
435
hierarchy,	
14
,	
14
,	
609
–
614
leaks,	
875
,	
992
load	performance,	
554
–
555
in	logic	design,	
361
–
364
machine-language	procedures,	
239
machine-level	programming,	
170
main,	
9
,	
584
,	
587
–
589
mapping.	
See	
memory	mapping
nonvolatile,	
587
performance,	
553
–
561
pipelining,	
469
–
470
protecting,	
289
,	
812
–
813
RAM.	
See	
random	access	memory	(RAM)
ROM,	
587
threads,	
993
–
994</p>
<p>trends,	
602
–
604
virtual.	
See	
virtual	memory	(VM)
Y86,	
356
memory	buses,	
587
memory	controllers,	
583
,	
584
memory	management	units	(MMUs),	
804
,	
807
memory-mapped	I/O,	
598
memory	mapping,	
812
areas,	
833
,	
833
function,	
836
–
837
function,	
836
in	loading,	
699
objects,	
833
–
836
user-level,	
837
–
839
memory	mountains,	
639
Core	i7	microprocessors,	
641
overview,	
639
–
643
memory	references
operands,	
181
out	of	bounds.	
See	
buffer	overflow
in	performance,	
514
–
517
memory	stage
instruction	processing,	
385
,	
387
–
397
PIPE	processor,	
454
–
455
sequential	processing,	
400
sequential	Y86–64	implementation,	
409
–
411
Y86–64	pipelining,	
423
memory	system,	
580
memory	utilization,	
845
,	
845</p>
<pre><code>function,	declaration,	
</code></pre>
<p>134
–
135
metadata,	
903
,	
903
–
904
metastable	states,	
581
methods
hypertext	transfer	protocol,	
951
–
952
objects,	
267
micro-operations,	
519
microarchitecture,	
10
,	
517
microprocessors.	
See	
central	processing	units	(CPUs)
Microsoft	Windows	operating	system,	
45
MIME	(multipurpose	internet	mail	extensions)	types,	
949
minimum	block	size,	
848
minimum	floating-point	instructions,	
302
minimum	two's	complement	number,	
66
minimum	values
constants,	
68
two's	complement	representation,	
66
mispredicted	branches
handling,	
443
–
444
performance	penalties,	
467
,	
520
,	
549
–
553
miss	rates,	
631
misses,	caches,	
470
,	
612
kinds,	
612
–
613
penalties,	
632
,	
806
rates,	
631
command,	
892
[CS:APP]	allocator:	boundary	tag	coalescing,	
860
[CS:APP]	allocator:	free	heap	block,	
860
[CS:APP]	matrix	multiply	
ijk,</p>
<p>645</p>
<pre><code>[CS:APP]	matrix	multiply	
</code></pre>
<p>ikj,</p>
<p>645
[CS:APP]	allocator:	initialize	heap,	
858
[CS:APP]	matrix	multiply	
jik
,	
645
[CS:APP]	matrix	multiply	
jki
,	
645
[CS:APP]	matrix	multiply	
kij
,	
645
[CS:APP]	matrix	multiply	
kji,</p>
<p>645
[CS:APP]	allocator:	allocate	heap	block,	
860
,	
861
[Unix]	map	disk	object	into	memory,	
837
,	
837
–
839
MMUs	(memory	management	units),	
804
,	
807
MMX	media	instructions,	
167
,	
294
Mockapetris,	Paul,	
931
mode	bits,	
735
modern	processor	performance,	
518
–
531
modes
kernel,	
726
,	
728
processes,	
734
–
736
,	
735
user,	
726
,	
728
modified	sequential	processor	implementation,	
421
–
422
modular	arithmetic,	
85
–
86
,	
89
modules
DRAM,	
584
,	
585
object,	
673
monitors,	Java,	
1010
monotonicity	assumption,	
846
monotonicity	property,	
124
Moore,	Gordon,	
169
Moore's	Law,	
169
,	
169
MOSAIC
browser,	
949
motherboards,	
9</p>
<p>Motorola	RISC	processors,	
363
MOV
[instruction	class]	move	data,	
182
,	
182
–
183
[x86–64]	move	absolute	quad	word,	
183
,	
183
[x86–64]	move	byte,	
183
move	absolute	quad	word	instruction,	
183
,	
183
move	aligned,	packed	double	precision	instruction,	
296
move	aligned,	packed	single	precision	instruction,	
296
move	and	sign-extend	instruction,	
184
,	
185
move	byte	instruction,	
183
move	data	instructions,	
182
–
189
move	double	precision	instruction,	
296
move	double	word	instruction,	
183
move	if	even	parity	instruction,	
324
move	if	greater	instruction,	
217
,	
357
move	if	greater	or	equal	instruction,	
217
,	
357
move	if	less	instruction,	
217
,	
357
move	if	less	or	equal	instruction,	
217
,	
357
move	if	negative	instruction,	
217
move	if	nonnegative	instruction,	
217
move	if	not	equal	instruction,	
217
,	
357
move	if	not	greater	instruction,	
217
move	if	not	greater	or	equal	instruction,	
217
move	if	not	less	instruction,	
217
move	if	not	less	or	equal	instruction,	
217
move	if	not	unsigned	greater	instruction,	
217
move	if	not	unsigned	less	instruction,	
217
move	if	not	unsigned	less	or	equal	instruction,	
217
move	if	not	zero	instruction,	
217
move	if	unsigned	greater	instruction,	
217</p>
<p>move	if	unsigned	greater	or	equal	instruction,	
217
move	if	unsigned	less	instruction,	
217
move	if	unsigned	less	or	equal	instruction,	
217
move	if	zero	instruction,	
217
move	instructions,	conditional,	
214
–
220
,	
550
–
553
move	quad	word	instruction,	
183
move	sign-extended	byte	to	double	word	instruction,	
185
move	sign-extended	byte	to	quad	word	instruction,	
185
move	sign-extended	byte	to	word	instruction,	
185
move	sign-extended	double	word	to	quad	word	instruction,	
185
move	sign-extended	word	to	double	word	instruction,	
185
move	sign-extended	word	to	quad	word	instruction,	
185
move	single	precision	instruction,	
296
move	when	equal	instruction,	
357
move	with	zero	extension	instruction,	
184
,	
184
move	word	instruction,	
183
move	zero-extended	byte	to	double	word	instruction,	
184
move	zero-extended	byte	to	quad	word	instruction,	
184
move	zero-extended	byte	to	word	instruction,	
184
move	zero-extended	word	to	double	word	instruction,	
184
move	zero-extended	word	to	quad	word	instruction,	
184
movement	operations,	floating-point	code,	
296
–
301
[x86–64]	move	double	word,	
183
[x86–64]	move	quad	word,	
183
MOVS
[instruction	class]	move	and	sign-extend,	
184
,	
185
[x86–64]	move	sign-extended	byte	to	double	word,	
185
[x86–64]	move	sign-extended	byte	to	quad	word,	
185
[x86–64]	move	sign-extended	byte	to	word,	
185
[x86–64]	move	sign-extended	double	word	to	quad	word,	
185</p>
<pre><code>[x86–64]	move	sign-extended	word	to	double	word,	
</code></pre>
<p>185
[x86–64]	move	sign-extended	word	to	quad	word,	
185
[x86–64]	move	word,	
183
MOVZ
[instruction	class]	move	with	zero	extension,	
184
,	
184
[x86–64]	move	zero-extended	byte	to	double	word,	
184
[x86–64]	move	zero-extended	byte	to	quad	word,	
184
[x86–64]	move	zero-extended	byte	to	word,	
184
[x86–64]	move	zero-extended	word	to	double	word,	
184
[x86–64]	move	zero-extended	word	to	quad	word,	
184
instruction,	
404
[x86–64]	unsigned	multiply,	
198
,	
198
multi-core	processors,	
16
,	
24
–
25
,	
168
,	
605
,	
972
multi-level	page	tables,	
819
–
821
multi-threading,	
17
–
18
,	
25
Multics,	
16
multicycle	instructions,	
468
–
469
multidimensional	arrays,	
258
–
260
multiple	accumulators	in	parallelism,	
536
–
541
multiple	zone	recording,	
592
multiplexing,	I/O,	
973
concurrent	programming	with,	
978
–
985
event-driven	servers	based	on,	
980
–
985
pros	and	cons,	
985
multiplexors,	
374
,	
374
–
375
HCL	with	case	expression,	
378
word-level,	
378
–
380
multiplication
constants,	
101
–
103
floating	point,	
124
,	
302</p>
<p>instructions,	
198
matrices,	
643
–
647
two's	complement,	
97
–
101
unsigned,	
96
–
97
,	
198
,	
198
multiply	instruction,	
192
multiported	random	access	memory,	
382
multiprocessor	systems,	
24
multipurpose	internet	mail	extensions	(MIME)	types,	
949
multitasking,	
733
multiway	branch	statements,	
232
–
238
[Unix]	unmap	disk	object,	
839
mutexes
lock	ordering	rule,	
1029
Pthreads,	
1010
for	semaphores,	
1003
mutual	exclusion
progress	graphs,	
1000
semaphores	for,	
1002
–
1004
mutually	exclusive	access,	
1000</p>
<pre><code>(newline	character),	
</code></pre>
<p>3
,	
891
n
-gram	statistics,	
565
named	pipes,	
892
names
domain,	
925
,	
927
–
929
mangling	and	demangling	processes	(C++	and	Java),	
680
,	
680
protocols,	
922
types,	
47
Y86–64	pipelines,	
427
NaN
(not	a	number)
constants,	
124
floating	point,	
306
representation,	
114
,	
115
nanoseconds	(ns),	
502
National	Science	Foundation	(NSF),	
931
signal,	
405
signal,	
405
NEG
[instruction	class]	negate,	
192
negate	instruction,	
192
negation,	two's	complement,	
95
negative	overflow,	
90
,	
90
–
91
nested	arrays,	
258
–
260
nested	structures,	
268
network	adapters,	
597
network	byte	order,	
925
network	clients,	
21
,	
918
Network	File	System	(NFS),	
610
network	programming,	
918
client-server	model,	
918
–
919</p>
<p>Internet.	
See	
Internet
networks,	
919
–
923
sockets	interface.	
See	
sockets	interface
summary,	
964
–
965
T
INY
Web	server,	
956
–
964
Web	servers,	
948
–
956
network	servers,	
21
,	
918
networks,	
20
–
21
acyclic,	
374
LANs,	
920
,	
920
–
922
WANs,	
921
,	
921
–
922
never	taken	(NT)	branch	prediction	strategy,	
428
newline	character	(
),	
3
,	
891
next-fit	block	placement	policy,	
849
,	
849
command,	
280
NFS	(Network	File	System),	
610
NM
tool,	
713
no-execute	(NX)	memory	protection,	
289
no	operation	nop	instruction,	
286
,	
404
instruction	code	for,	
405
pipelining,	
430
–
431
in	stack	randomization,	
286
no-write-allocate	approach,	
630
nodes,	root,	
866
nondeterminism,	
748
nondeterministic	behavior,	
748
nonexistent	variables,	referencing,	
874
nonlocal	jumps,	
723
,	
781
,	
781
–
786
nonuniform	partitioning,	
416
–
418</p>
<p>nonvolatile	memory,	
586
[x86–64]	no	operation	instruction,	
286
,	
404
instruction	code	for,	
405
pipelining,	
430
–
431
in	stack	randomization,	
286
nop	sleds,	
286
[CS:APP]	Pthreads	program	without	a	race,	
1027
normal	operation	status	code,	
364
,	
404
normalized	values,	floating-point,	
113
,	
113
–
114
northbridge	chipsets,	
588
not	a	number	
(NaN)
constants,	
124
floating	point,	
306
representation,	
114
,	
115
NOT
[instruction	class]	complement,	
192
NOT
operation
Boolean,	
51
–
52
C	operators,	
56
–
57
logic	gates,	
373
ns	(nanoseconds),	
502
NSF	(National	Science	Foundation),	
931
NSFNET,	
931
NSLOOKUP
program,	
928
[Unix]	convert	network-to-host	long,	
925
[Unix]	convert	network-to-host	short,	
925
number	systems	conversions.	
See	
conversions
numeric	limit	declarations,	
77
numeric	ranges
C	standards,	
61</p>
<p>integral	types,	
60
–
62
Java	standard,	
68
NX	(no-execute)	memory	protection,	
289</p>
<pre><code>files,	
</code></pre>
<p>173
,	
672
optimization	flag,	
170
optimization	flag,	
170
OBJDUMP</p>
<p>GNU
machine-code	file	reader,	
173
,	
279
,	
692
,	
713
object	code,	
170
,	
173
object	files,	
173
executable.	
See	
executable	object	files
formats,	
673
forms,	
673
relocatable,	
5
,	
672
,	
673
–
675
shared,	
673
tools,	
713
object	modules,	
673
objects
C++	and	Java,	
266
memory-mapped,	
833
–
836
private,	
834
,	
834
program,	
34
shared,	
699
,	
833
–
836
as	
,	
266
–
267
oct	word,	
197
,	
197
–
198
[x86–64]	overflow	flag	condition	code,	
201
,	
355
off-by-one	errors,	
872
offsets
GOTs,	
705
,	
705
–
707
memory	references,	
181
PPOs,	
814
unions,	
270
VPOs,	
814</p>
<pre><code>optimization	flag,	
</code></pre>
<p>170
,	
563
one-operand	multiply	instructions,	
198
ones'-complement	representation,	
68
[Unix]	open	file,	
891
,	
893
–
895
[CS:APP]	establish	connection	with	server,	
942
,
942
–
944
[CS:APP]	establish	a	listening	socket,	
944
,	
944
open	operations	for	files,	
891
,	
893
–
895
open	shared	library	function,	
701
open-source	operating	systems,	
86
–
87
functions,	
905
operand	specifiers,	
180
–
182
operate	instruction,	
10
operating	systems	(OS),	
15
files,	
19
hardware	management,	
14
–
15
kernels,	
19
Linux,	
20
,	
45
processes,	
15
–
17
threads,	
17
–
18
Unix,	
35
virtual	memory,	
18
–
19
Windows,	
45
operations
bit-level,	
54
–
56
logical,	
56
–
57
shift,	
57
–
59
optest	script,	
465
optimization</p>
<p>address	translation,	
830
compiler,	
170
levels,	
498
program	performance.	
See	
performance
optimization	blockers,	
496
–
497
,	
500
OPTIONS	method,	
951
OR
[instruction	class]	or,	
192
OR
operation
Boolean,	
51
–
52
C	operators,	
56
–
57
HCL	expressions,	
374
–
375
logic	gates,	
373
order,	bytes,	
42
–
49
disassembled	code,	
210
network,	
925
unions,	
272
origin	servers,	
952
OS.	
See	
operating	systems	(OS)
Ossanna,	Joe,	
16
out-of-bounds	memory	references.	
See	
buffer	overflow
out-of-order	execution,	
518
five-stage	pipelines,	
471
history,	
522
overflow
arithmetic,	
87
,	
87
–
89
,	
134
buffer.	
See	
buffer	overflow
floating-point	values,	
127
identifying,	
92
–
93
infinity	representation,	
115
multiplication,	
102</p>
<p>negative,	
90
,	
90
–
91
operations,	
32
positive,	
90
,	
90
–
91
overflow	flag	condition	code,	
201
,	
355
overloaded	functions	(C++	and	Java),	
680</p>
<p>P	semaphore	operation,	
1001
,	
1001
–
1002
P	[CS:APP]	wrapper	function	for	Posix	sem_wait,	
1002
P6	microarchitecture,	
167
PA	(physical	addresses),	
803
vs.	virtual,	
803
–
804
Y86–64,	
356
packages,	processor,	
825
packet	headers,	
922
packets,	
922
padding
alignment,	
274
–
275
blocks,	
847
page	faults
DRAM	caches,	
808
,	
808
–
809
Linux/x86–64	systems,	
729
,	
832
–
833
memory	caches,	
470
pipelining	caches,	
808
page	frames,	
805
page	hits	in	caches,	
808
page	table	base	registers	(PTBRs),	
814
page	table	entries	(PTEs),	
807
,	
807
–
808
Core	i7,	
826
–
828
TLBs	for,	
817
–
821
,	
823
page	table	entry	addresses	(PTEAs),	
817
page	tables,	
736
,	
823
caches,	
806
–
808
,	
807
multi-level,	
819
–
821
paged-in	pages,	
809
paged-out	pages,	
809
pages</p>
<p>allocation,	
810
demand	zero,	
833
dirty,	
827
physical,	
805
,	
805
–
806
SSDs,	
601
virtual,	
289
,	
805
,	
805
–
806
paging
demand,	
810
description,	
809
parallel	execution,	
734
parallel	flows,	
734
,	
734
parallel	programs,	
1013
parallelism,	
24
,	
536
instruction-level,	
26
,	
497
,	
518
,	
562
multiple	accumulators,	
536
–
541
reassociation	transformations,	
541
–
546
SIMD,	
26
,	
546
–
547
thread-level,	
26
threads	for,	
1013
–
1018
parent	directories,	
892
parent	processes,	
739
,	
739
–
740
parity	flag	condition	code,	
178
,	
306
[CS:APP]	T
INY
helper	function,	
960
[CS:APP]	shell	helper	routine,	
756
partitioning
addresses,	
615
–
616
nonuniform	in	pipelining,	
416
–
418
passing	data
machine-language	procedures,	
239</p>
<p>pointers	to	structures,	
266
pathnames,	
893
Patterson,	David,	
361
,	
471
[Unix]	suspend	until	signal	arrives,	
750
payloads
aggregate,	
845
Ethernet,	
920
protocol,	
922
PC.	
See	
program	counters	(PCs)
PC-relative	addressing
jumps,	
207
,	
207
–
209
symbol	references,	
690
,	
692
–
693
Y86–64,	
359
PC	selection	stage	in	PIPE	processor,	
447
–
449
PC	update	stage
instruction	processing,	
385
,	
387
–
395
sequential	processing,	
400
sequential	Y86–64	implementation,	
411
PCI	(peripheral	component	interconnect),	
598
PCIe	(PCI	express),	
598
PE	(Portable	Executable)	format,	
673
peak	utilization	metric,	
844
–
845
,	
845
peer	threads,	
986
pending	bit	vectors,	
759
pending	signals,	
758
Pentium	II	microprocessor,	
167
Pentium	III	microprocessor,	
167
–
168
Pentium	4	microprocessor,	
168
Pentium	4E	microprocessor,	
168</p>
<p>Pentium	microprocessor,	
167
PentiumPro	microprocessor,	
167
,	
522
performance,	
6
Amdahl's	law,	
22
–
24
basic	strategies,	
561
–
562
bottlenecks,	
562
–
568
branch	prediction	and	misprediction	penalties,	
549
–
553
caches,	
553
,	
631
–
633
,	
639
–
647
compiler	capabilities	and	limitations,	
498
–
502
expressing,	
502
–
504
limiting	factors,	
548
–
553
loop	inefficiencies,	
508
–
512
loop	unrolling,	
531
,	
531
–
535
memory,	
553
–
561
memory	references,	
514
–
517
modern	processors,	
518
–
531
overview,	
496
–
498
parallelism.	
See	
parallelism
procedure	calls,	
512
–
513
program	example,	
504
–
508
program	profiling,	
562
–
564
register	spilling,	
548
–
549
results	summary,	
547
–
548
sequential	Y86–64	implementation,	
412
summary,	
568
–
569
Y86–64	pipelining,	
464
–
468
periods	(.)	in	dotted-decimal	notation,	
926
persistent	connections	in	HTTP,	
952
P
F
[x86–64]	parity	flag	condition	code,	
178
,	
306
physical	address	spaces,	
804</p>
<p>physical	addresses	(PA),	
803
vs.	virtual,	
803
–
804
Y86–64,	
356
physical	page	numbers	(PPNs),	
814
physical	page	offset	(PPO),	
814
physical	pages	(PPs),	
805
,	
805
–
806
pi	in	floating-point	representation,	
140
PIC	(position-independent	code),	
704
data	references,	
704
–
705
function	calls,	
705
–
707
picoseconds	(ps),	
413
,	
502
PIDs	(process	IDs),	
739
pins,	DRAM,	
582
–
583
PIPE–	processor,	
421
,	
422
,	
426
–
430
PIPE	processor	stages,	
439
–
440
,	
447
decode	and	write-back,	
449
–
453
execute,	
453
–
454
memory,	
454
–
455
PC	selection	and	fetch,	
447
–
449
pipelining,	
26
,	
215
,	
412
bubble,	
434
combinational,	
412
–
414
deep,	
418
–
419
diagram,	
413
five-stage,	
471
functional	units,	
523
–
524
instruction,	
549
limitations,	
416
–
418
nonuniform	partitioning,	
416
–
418
operation,	
414
–
416</p>
<p>registers,	
413
,	
427
store	operation,	
555
–
556
systems	with	feedback,	
419
–
421
Y86–64.	
See	
Y86–64	pipelined	implementations
pipes,	
977
Pisano,	Leonardo	(Fibonacci),	
32
placement
memory	blocks,	
847
,	
849
policies,	
612
,	
849
platters,	disk,	
590
,	
591
PLT	(procedure	linkage	table),	
706
,	
706
–
707
PMAP
tool,	
786
point-to-point	connections,	
929
pointers,	
34
arithmetic,	
257
–
258
,	
873
arrays	relationship	to,	
48
,	
277
block,	
856
creating,	
48
,	
188
declaring,	
41
dereferencing,	
48
,	
188
,	
257
,	
277
,	
870
–
871
examples,	
188
to	functions,	
278
machine-level	data,	
177
principles,	
278
role,	
36
stack,	
239
to	structures,	
266
virtual	memory,	
870
–
873
,	
48</p>
<p>polynomial	evaluation,	
530
,	
530
,	
572
–
573
pools	of	peer	threads,	
987
pop	instructions	in	x86–64	models,	
372</p>
<p>pop	operations	on	stack,	
189
,	
189
–
191
[Y86–64]	pop	instruction,	
190
,	
190
,	
357
behavior	of,	
371
code	for,	
404
run-time	stack,	
239
portability	and	data	type	size,	
41
Portable	Executable	(PE)	format,	
673
portable	signal	handling,	
774
–
775
ports
Ethernet,	
920
Internet,	
930
I/O,	
598
register	files,	
382
[Y86–64]	directive,	
366
position-independent	code	(PIC),	
704
data	references,	
704
–
705
function	calls,	
705
–
707
positive	overflow,	
90
,	
90
–
91
[CS:APP]	reports	Posix-style	errors,	
1043
Posix	standards,	
16
Posix-style	error	handling,	
1042
,	
1043
Posix	threads,	
987
,	
987
–
988
POST	method,	
951
–
953
PowerPC
processor	family,	
352
,	
361
RISC	design,	
361
–
363
powers	of	2,	division	by,	
103
–
107
PPNs	(physical	page	numbers),	
814
PPO	(physical	page	offset),	
814</p>
<p>PPs	(physical	pages),	
805
,	
805
–
806
precedence	of	shift	operations,	
59
precision,	floating-point,	
113
,	
137
prediction
branch,	
215
misprediction	penalties,	
549
–
553
Y86–64	pipelining,	
422
,	
427
–
429
preempted	processes,	
733
prefetching	mechanism,	
641
–
642
prefix	sums,	
502
,	
503
,	
561
,	
573
prepare	stack	for	return	instruction,	
292
preprocessors,	
5
,	
170
prethreading,	
1005
–
1013
,	
1008
primary	inputs	in	logic	gates,	
374
principle	of	locality,	
604
,	
604
command,	
280
print	getaddrinfo	error	message	function,	
938
[C	Stdlib]	formatted	printing	function
formatted	printing,	
47
numeric	values	with,	
75
printing,	formatted,	
47
priorities
PIPE	processor	forwarding	sources,	
451
–
452
write	ports,	
408
private	address	space,	
734
private	areas,	
834
private	copy-on-write	structures,	
836
private	declarations	(C++	and	Java),	
677
private	objects,	
834
,	
834</p>
<p>privileged	instructions,	
735
filesystem,	
735
,	
735
–
736
,	
786
procedure	linkage	table	(PLT),	
706
,	
706
–
707
procedure	return	instruction,	
357
procedures,	
238
–
239
call	performance,	
512
–
513
control	transfer,	
241
–
245
data	transfer,	
245
–
248
floating-point	code	in,	
301
–
302
recursive,	
253
–
255
register	usage	conventions,	
251
–
253
run-time	stack,	
239
–
241
process	contexts,	
16
,	
736
process	graphs,	
741
,	
742
process	groups,	
759
process	IDs,	
739
process	tables,	
736
processes,	
15
,	
732
,	
738
background,	
753
child,	
740
concurrent	flow,	
732
–
734
,	
733
concurrent	programming	with,	
973
–
977
concurrent	servers	based	on,	
974
–
975
context	switches,	
736
–
737
creating	and	terminating,	
739
–
743
default	behavior,	
744
error	conditions,	
745
–
746
exit	status,	
745
foreground,	
753</p>
<p>group,	
759
IDs,	
739
–
740
loading	programs,	
699
,	
750
–
752
overview,	
15
–
17
parent,	
739
,	
740
preempted,	
733
private	address	space,	
734
vs.	programs,	
753
pros	and	cons,	
975
reaping,	
743
,	
743
–
749
running	programs,	
750
–
756
sleeping,	
749
–
750
tools,	
786
–
787
user	and	kernel	modes,	
734
–
735
function,	
746
–
749
zombie,	
743
processor-memory	gap,	
13
,	
604
processor	packages,	
825
processor	states,	
723
processors.	
See	
central	processing	units	(CPUs)
producer-consumer	problem,	
1004
,	
1005
–
1006
profilers	code,	
497
profiling,	program,	
562
–
564
program	counters	(PCs),	
9
,	
44
in	fetch	stage,	
384
hazards,	
435
machine-language	procedures,	
239
,	
171
SEQ	timing,	
401</p>
<p>Y86–64	instruction	set	architecture,	
356
Y86–64	pipelining,	
423
,	
427
–
429
program	data	references	locality,	
606
–
607
program	header	tables,	
696
,	
696
program	registers
clocked,	
381
–
384
data	hazards,	
435
Y86–64,	
355
–
356
programmable	ROMs	(PROMs),	
587
programmer-visible	state,	
355
,	
355
–
356
programs
code	and	data,	
18
concurrent.	
See	
concurrent	programming
forms,	
4
–
5
loading	and	running,	
750
–
752
machine-level.	
See	
machine-level	programming
objects,	
34
vs.	processes,	
753
profiling,	
562
–
564
running,	
10
–
12
,	
753
–
756
Y86–64,	
364
–
370
progress	graphs,	
999
,	
999
–
1001
deadlock	regions,	
1027
–
1028
,	
1028
forbidden	regions,	
1003
limitations,	
1004
prologue	blocks,	
855
PROMs	(programmable	ROMs),	
587
protection,	memory,	
812
–
813
protocol	software,	
922
protocols,	
922</p>
<p>proxy	caches,	
952
proxy	chains,	
952
ps	(picoseconds),	
413
,	
502
PS
tool,	
786
pseudorandom	number	generator	functions,	
1021
[CS:APP]	parallel	sum	program	using	array,	
1016
[CS:APP]	parallel	sum	program	using	local	variables,
1017
[CS:APP]	parallel	sum	program	using	mutex,	
1015
PTBRs	(page	table	base	registers),	
814
PTEAs	(page	table	entry	addresses),	
817
PTEs	(page	table	entries),	
807
,	
807
–
808
Core	i7,	
826
–
828
TLBs	for,	
817
–
821
,	
823
[Unix]	terminate	another	thread,	
989
[Unix]	create	a	thread,	
988
[Unix]	detach	thread,	
990
,	
990
[Unix]	terminate	current	thread,	
989
[Unix]	reap	a	thread,	
989
[Unix]	initialize	a	thread,	
990
,	
1012
[Unix]	get	thread	ID,	
988
Pthreads,	
987
,	
987
–
988
,	
1010
public	declarations	(C++	and	Java),	
677
push	instructions	in	x86–64	models,	
372
push	operations	on	stack,	
189
,	
189
–
191
[x86–64]	push	quad	word,	
173
,	
190
,	
190
,	
357
code	for,	
404
processing	steps,	
370
–
371
,	
392</p>
<p>run-time	stack,	
239
PUT	method	in	HTTP,	
951
&quot;put	to&quot;	operator	(C++),	
890
function,	
566
quad	words,	
177
QuickPath	interconnect,	
588
,	
826
command,	
280</p>
<pre><code>(absolute	addressing),	
</code></pre>
<p>691
(PC-relative	addressing),	
690
symbol	table	entry,	
677
and	Unix,	
673
[Y86–64]	program	register,	
180
,	
355
[x86–64]	low	order	32	bits	of	register	
,	
180
[x86–64]	low	order	16	bits	of	register	
,	
180
[Y86–64]	program	register,	
180
,	
355
[x86–64]	low	order	32	bits	of	register	
,	
180
[x86–64]	low	order	16	bits	of	register	
,	
180
[Y86–64]	program	register,	
180
,	
355
[x86–64]	low	order	32	bits	of	register	
,	
180
[x86–64]	low	order	16	bits	of	register	
,	
180
[Y86–64]	program	register,	
180
,	
355
[x86–64]	low	order	32	bits	of	register	
,	
180
[x86–64]	low	order	16	bits	of	register	
,	
180
[Y86–64]	program	register,	
180
,	
355
[x86–64]	low	order	32	bits	of	register	
,	
180
[x86–64]	low	order	16	bits	of	register	
,	
180
[Y86–64]	program	register,	
180
,	
355
[x86–64]	low	order	32	bits	of	register	
,	
180
[x86–64]	low	order	16	bits	of	register	
,	
180
[Y86–64]	program	register,	
180
,	
355
[x86–64]	low	order	32	bits	of	register	
,	
180
[x86–64]	low	order	16	bits	of	register	
,	
180
[x86–64]	program	register,	
180
,	
355
[x86–64]	low	order	32	bits	of	register	
,	
180</p>
<pre><code>[x86–64]	low	order	16	bits	of	register	
</code></pre>
<p>,	
180
[CS:APP]	program	with	a	race,	
1025
race	conditions,	
776
,	
992
concurrent	programming,	
1025
,	
1025
–
1027
signals,	
776
–
778
RAM.	
See	
random	access	memory	(RAM)
[CS:APP]	pseudorandom	number	generator,	
1021
,	
1024
function,	
1024
random	access	memory	(RAM),	
381
,	
581
dynamic.	
See	
dynamic	RAM	(DRAM)
multiported,	
382
processors,	
384
SEQ	timing,	
401
static.	
See	
static	RAM	(SRAM)
random	operations	in	SSDs,	
600
random	replacement	policies,	
612
ranges
asymmetric,	
66
,	
77
bytes,	
36
constants	for,	
67
–
68
data	types,	
40
integral	types,	
60
–
62
Java	standard,	
68
RAS	(row	access	strobe)	requests,	
583
[Y86–64]	program	register,	
180
,	
355
[Y86–64]	program	register,	
180
,	
355
[Y86–64]	program	register,	
180
,	
355
[Y86–64]	program	register,	
180
,	
355
[Y86–64]	program	register,	
180
,	
355</p>
<pre><code>[Y86–64]	program	register,	
</code></pre>
<p>180
,	
355
reachability	graphs,	
866
reachable	nodes,	
866
read	access,	
289
read	and	echo	input	lines	function,	
947
read	bandwidth,	
639
read	environment	variable	function,	
751
read/evaluate	steps,	
753
[Unix]	read	file,	
895
,	
895
–
897
read-only	memory	(ROM),	
586
read-only	register,	
527
read	operations
buffered,	
898
,	
900
–
901
disk	sectors,	
597
–
599
file	metadata,	
903
–
904
files,	
891
,	
895
–
897
SSDs,	
601
unbuffered,	
897
–
898
uninitialized	memory,	
871
read	ports,	
382
[CS:APP]	T
INY
helper	function,	
960
read	sets,	
978
read	throughput,	
639
read	transactions
descriptions,	
587
example	of,	
588
–
589
read/write	heads,	
592
functions,	
905
READELF
GNU	object	file	reader,	
678
,	
713</p>
<p>readers-writers	problem,	
1006
,	
1008
reading
directory	contents,	
905
–
906
disk	sectors,	
597
function,	
903
function,	
903
ready	read	descriptors,	
978
ready	sets,	
978
function,	
841
reap	thread	function,	
989
reaping
child	processes,	
743
,	
743
–
749
threads,	
989
rearranging	signals	in	pipelining,	
426
–
427
reassociation	transformations,	
541
,	
541
–
546
,	
570
receiving	signals,	
758
,	
762
–
764
recording	density,	
591
recording	zones,	
592
recursive	procedures,	
253
–
255
redirection	of	I/O,	
909
,	
909
–
910
reduced	instruction	set	computers	(RISC),	
361
VS
.
CISC,	
361
–
363
SPARC	processors,	
471
reentrancy	issues,	
1023
–
1024
reentrant	functions,	
766
,	
1023
reference	bits,	
827
reference	counts,	
906
reference	machines,	
507
referencing</p>
<p>data	in	free	heap	blocks,	
874
–
875
nonexistent	variables,	
874
refresh,	DRAM,	
582
regions,	deadlock,	
1027
–
1028
,	
1028
register	files,	
10
,	
358
contents,	
382
–
383
,	
521
purpose,	
358
–
359
SEQ	timing,	
401
register	identifier	(ID),	
358
–
359
register	operands,	
181
register	specifier	bytes	in	Y86–64	instruction,	
358
register	to	memory	move	instruction,	
356
register	to	register	move	instruction,	
356
registers,	
9
clocked,	
381
data	hazards,	
435
data	transfer,	
245
–
248
hardware,	
381
–
384
local,	
527
local	storage,	
251
–
253
loop,	
527
pipeline,	
413
,	
427
program,	
355
–
356
,	
381
–
384
,	
435
read-only,	
527
register	files,	
171
renaming,	
522
spilling,	
548
–
549
updating	conventions,	
179
write-only,	
527
x86–64	integer,	
179
,	
179
–
180</p>
<p>Y86–64,	
359
,	
422
–
426
regular	files,	
833
,	
891
section,	
675
section,	
675
relabeling	signals,	
426
–
427
relative	pathnames,	
893
relative	speedup	in	parallel	programs,	
1019
reliable	connections,	
930
relocatable	object	files,	
5
,	
672
,	
673
–
675
relocation,	
673
,	
689
–
690
algorithm,	
691
entries,	
690
,	
690
–
691
PC-relative	references,	
692
–
693
practice	problems,	
694
–
695
remove	item	from	bounded	buffer	function,	
1007
renaming	registers,	
522
[x86–64]	string	repeat	instruction	used	as	no-op,	
208
replacement	policies,	
613
replacing	blocks,	
612
report	shared	library	error	function,	
702
reporting	errors,	
1043
request	headers	in	HTTP,	
951
request	lines	in	HTTP,	
951
requests
client-server	model,	
918
HTTP,	
951
,	
951
–
952
requests	for	comments	(RFCs),	
965
reset	configuration	in	pipelining,	
460
resident	sets,	
810</p>
<p>resources
client-server	model,	
918
shared,	
1004
–
1008
[Y86–64]	register	ID	for	
,	
404
response	bodies	in	HTTP,	
952
response	headers	in	HTTP,	
952
response	lines	in	HTTP,	
952
responses
client-server	model,	
918
HTTP,	
952
,	
952
–
953
[CS:APP]	nonlocal	jump	example,	
785
restrictions,	alignment,	
273
–
276
[Y86–64]	procedure	return,	
357
[x86–64]	return	from	procedure	call,	
208
,	
241
–
242
instruction,	
404
processing	steps,	
395
Y86–64	pipelining,	
428
–
429
,	
455
–
457
,	
461
–
462
retiming	circuits,	
421
retirement	units,	
521
[x86–64]	return	from	procedure,	
241
return	addresses,	
241
predicting,	
429
procedures,	
240
return	penalty	in	CPI,	
467
reverse	engineering
loops,	
222
machine	code,	
165
revolutions	per	minute	(RPM),	
590
RFCs	(requests	for	comments),	
965</p>
<p>ridges	in	memory	mountains,	
641
right	hoinkies	(&gt;),	
910
right	shift	operations,	
57
–
58
,	
192
rings,	Boolean,	
52
RIO
[CS:APP]	Robust	I/O	package,	
897
buffered	functions,	
898
–
902
origins,	
903
unbuffered	functions,	
897
–
898
[CS:APP]	internal	read	function,	
901
[CS:APP]	init	read	buffer,	
898
,	
900
[CS:APP]	robust	buffered	read,	
898
,	
902
[CS:APP]	robust	unbuffered	read,	
897
,	
897
–
899
,	
901
,	
903
[CS:APP]	robust	buffered	read,	
898
,	
902
[CS:APP]	read	buffer,	
900
[CS:APP]	robust	unbuffered	write,	
897
,	
897
–
899
,	
903
[x86–64]	program	counter,	
171
program	counter,	
171
RISC	(reduced	instruction	set	computers),	
361
VS
.
CISC,	
361
–
363
SPARC	processors,	
471
Ritchie,	Dennis,	
2
,	
4
,	
16
,	
35
,	
914
command,	
892
[Y86–64]	register	to	memory	move,	
356
,	
390
,	
404
[Y86–64]	ID	for	indicating	no	register,	
404
Roberts,	Lawrence,	
931
robust	buffered	read	functions,	
898
,	
902
Robust	I/O	(
RIO
)	package,	
897
buffered	functions,	
898
–
902</p>
<p>origins,	
903
unbuffered	functions,	
897
–
898
robust	unbuffered	read	function,	
897
,	
897
–
899
robust	unbuffered	write	function,	
897
,	
897
–
899
section,	
674
ROM	(read-only	memory),	
586
root	directory,	
892
root	nodes,	
866
rotating	disks	term,	
591
rotational	latency	of	disks,	
594
rotational	rate	of	disks,	
590
round-down	mode,	
121
,	
121
round-to-even	mode,	
120
,	
120
–
121
,	
124
round-to-nearest	mode,	
120
,	
120
round-toward-zero	mode,	
120
,	
120
–
121
round-up	mode,	
121
,	
121
rounding
in	division,	
105
–
106
floating-point	representation,	
120
–
122
rounding	modes,	
120
,	
120
–
122
routers,	Ethernet,	
921
routines,	thread,	
987
row	access	strobe	(RAS)	requests,	
583
row-major	array	order,	
258
,	
606
row-major	sum	function,	
635
,	
635
RPM	(revolutions	per	minute),	
590
[Y86–64]	register	to	register	move,	
356
,	
404
[x86–64]	program	register,	
180
[Y86–64]	stack	pointer	program	register	
179
–
180
,	
355</p>
<pre><code>command,	
</code></pre>
<p>280
run	concurrency,	
733
run	time
interpositioning,	
710
–
712
linking,	
670
shared	libraries,	
699
stacks,	
171
,	
239
–
241
running
in	parallel,	
734
processes,	
739
programs,	
10
–
12
,	
750
–
756</p>
<pre><code>assembly	language	files,	
</code></pre>
<p>672
[CS:APP]	shorthand	for	
sockaddr,	
933
[Y86–64]	status	code	for	address	exception,	
404
safe	optimization,	
498
,	
498
–
499
safe	signal	handling,	
766
–
770
safe	trajectories	in	progress	graphs,	
1000
safely	emit	error	message	and	terminate	instruction,	
766
,	
768
safely	emit	long	int	instruction,	
766
,	
768
safely	emit	string	instruction,	
766
,	
768
SAL
[instruction	class]	shift	left,	
192
[x86–64]	shift	left,	
195
[x86–64]	shift	left,	
195
[x86–64]	shift	left,	
195
Sandy	Bridge	microprocessor,	
168
[Y86–64]	status	code	for	normal	operation,	
404
SAR
[instruction	class]	shift	arithmetic	right,	
192
,	
195
SATA	interfaces,	
597
saturating	arithmetic,	
134
[C	Stdlib]	extend	the	heap,	
841
,	
841
emulator,	
855
heap	memory,	
850
[CS:APP]	shared	bounded	buffer	package,	
1005
,	
1006
[CS:APP]	free	bounded	buffer,	
1007
[CS:APP]	allocate	and	init	bounded	buffer,	
1007
[CS:APP]	insert	item	in	a	bounded	buffer,	
1007
[CS:APP]	remove	item	from	bounded	buffer,	
1007
[CS:APP]	bounded	buffer	used	by	S
BUF
package,	
1006
scalar	code	performance	summary,	
547
–
548</p>
<p>scalar	format	data,	
294
scalar	instructions,	
296
scale	factor	in	memory	references,	
181
scaling	parallel	programs,	
1019
,	
1019
–
1020
function,	
870
–
871
schedule	alarm	to	self	function,	
762
schedulers,	
736
scheduling,	
736
events,	
763
shared	resources,	
1004
–
1008
SCSI	interfaces,	
597
SDRAM	(synchronous	DRAM),	
586
second-level	domain	names,	
928
second	readers-writers	problem,	
1008
sectors,	disk,	
590
,	
590
–
592
access	time,	
593
–
595
gaps,	
596
reading,	
597
–
599
security	monoculture,	
285
security	vulnerabilities,	
7
function,	
86
–
87
XDR	library,	
100
seeds	for	pseudorandom	number	generators,	
1021
seek	operations,	
593
,	
891
seek	time	for	disks,	
593
,	
593
segmentation	faults,	
729
segmented	addressing,	
287
–
288
segments
code,	
696
,	
697
–
698</p>
<p>data,	
696
Ethernet,	
920
,	
920
loops,	
526
–
527
virtual	memory,	
830
segregated	fits,	
863
,	
864
–
865
segregated	free	lists,	
863
–
865
segregated	storage,	
863
[Unix]	wait	for	I/O	events,	
977
self-loops,	
980
self-modifying	code,	
435
[Unix]	initialize	semaphore,	
1002
[Unix]	V	operation,	
1002
[Unix]	P	operation,	
1002
semaphores,	
1001
,	
1001
–
1002
concurrent	server	example,	
1005
–
1013
for	mutual	exclusion,	
1002
–
1004
for	scheduling	shared	resources,	
1004
–
1008
sending	signals,	
735
,	
759
–
762
separate	compilation,	
670
SEQ+	pipelined	implementations,	
421
,	
421
–
422
SEQ	Y86–64	processor	design.
See	
sequential	Y86–64	implementation
sequential	circuits,	
381
sequential	execution,	
200
–
201
sequential	operations	in	SSDs,	
600
sequential	reference	patterns,	
606
sequential	Y86–64	implementation,	
384
,	
421
decode	and	write-back	stage,	
406
–
408
execute	stage,	
408
–
409</p>
<p>fetch	stage,	
404
–
406
hardware	structure,	
396
–
400
instruction	processing	stages,	
384
–
395
memory	stage,	
409
–
411
PC	update	stage,	
411
performance,	
412
SEQ+	implementations,	
421
,	
421
–
422
timing,	
400
–
403
[CS:APP]	T
INY
helper	function,	
963
–
964
[CS:APP]	T
INY
helper	function,	
961
–
963
servers,	
21
client-server	model,	
918
concurrent.	
See	
concurrent	servers
network,	
21
Web.	
See	
Web	servers
service	conversions	in	sockets	interface,	
937
–
942
services	in	client-server	model,	
918
serving
dynamic	content,	
953
–
954
Web	content,	
949
set	associative	caches,	
624
line	matching	and	word	selection,	
625
–
626
line	replacement,	
625
set	selection,	
625
,	
625
set	bit	in	descriptor	set	macro,	
978
set	index	bits,	
615
,	
615
–
616
set	on	equal	instruction,	
203
set	on	greater	instruction,	
203
set	on	greater	or	equal	instruction,	
203</p>
<p>set	on	less	instruction,	
203
set	on	less	or	equal	instruction,	
203
set	on	negative	instruction,	
203
set	on	nonnegative	instruction,	
203
set	on	not	equal	instruction,	
203
set	on	not	greater	instruction,	
203
set	on	not	greater	or	equal	instruction,	
203
set	on	not	less	instruction,	
203
set	on	not	less	or	equal	instruction,	
203
set	on	not	zero	instruction,	
203
set	on	unsigned	greater	instruction,	
203
set	on	unsigned	greater	or	equal	instruction,	
203
set	on	unsigned	less	instruction,	
203
set	on	unsigned	less	or	equal	instruction,	
203
set	on	unsigned	not	greater	instruction,	
203
set	on	unsigned	not	less	instruction,	
203
set	on	unsigned	not	less	or	equal	instruction,	
203
set	on	zero	instruction,	
203
set	process	group	ID	function,	
759
set	selection
direct-mapped	caches,	
618
fully	associative	caches,	
625
set	associative	caches,	
625
[x86–64]	set	on	unsigned	greater,	
203
[x86–64]	set	on	unsigned	greater	or	equal,	
203
[x86–64]	set	on	unsigned	less,	
203
[x86–64]	set	on	unsigned	less	or	equal,	
203
[x86–64]	set	on	equal,	
203
[Unix]	create/change	environment	variable,	
752</p>
<pre><code>[x86–64]	set	on	greater,	
</code></pre>
<p>203
[x86–64]	set	on	greater	or	equal,	
203
[C	Stdlib]	init	nonlocal	jump,	
723
,	
781
,	
783
[CS:APP]	nonlocal	jump	example,	
784
[x86–64]	set	on	less,	
203
[x86–64]	set	on	less	or	equal,	
203
[x86–64]	set	on	unsigned	not	greater,	
203
[x86–64]	set	on	unsigned	not	less	or	equal,	
203
[x86–64]	set	on	unsigned	not	less,	
203
[x86–64]	set	on	unsigned	not	less	or	equal,	
203
[x86–64]	set	on	not	equal,	
203
[x86–64]	set	on	not	greater,	
203
[x86–64]	set	on	not	greater	or	equal,	
203
[x86–64]	set	on	not	less,	
203
[x86–64]	set	on	not	less	or	equal,	
203
[x86–64]	set	on	nonnegative,	
203
[x86–64]	set	on	not	zero,	
203
[Unix]	set	process	group	ID,	
759
sets
VS
.	cache	lines,	
634
membership,	
380
–
381
[x86–64]	set	on	negative,	
203
[x86–64]	set	on	zero,	
203
[x86–64]	sign	flag	condition	code,	
201
,	
355
[Unix]	Unix	shell	program,	
753
Shannon,	Claude,	
51
shared	areas,	
834</p>
<p>shared	libraries,	
19
,	
699
dynamic	linking	with,	
699
–
701
loading	and	linking	from	applications,	
701
–
703
shared	object	files,	
673
shared	objects,	
699
,	
833
–
836
,	
834
shared	resources,	scheduling,	
1004
–
1008
shared	variables,	
992
–
995
,	
993
sharing
files,	
906
–
908
virtual	memory	for,	
812
[CS:APP]	sharing	in	Pthreads	programs,	
993
[CS:APP]	shell	main	routine,	
754
shells,	
7
,	
753
shift	arithmetic	right	instruction,	
192
shift	left	instruction,	
192
shift	logical	right	instruction,	
192
shift	operations,	
57
,	
57
–
59
for	division,	
103
–
107
machine	language,	
194
–
196
for	multiplication,	
101
–
103
shift	arithmetic	right	instruction,	
192
shift	left	instruction,	
192
shift	logical	right	instruction,	
192
SHL
[instruction	class]	shift	left,	
192
,	
195
[Y86–64]	status	code	for	
,	
404
short	counts,	
895
[C]	integer	data	type,	
40
,	
61
SHR
[instruction	class]	shift	logical	right,	
192
,	
195
[x86–64]	low	order	16	bits	of	register	
,	
180</p>
<p>side	effects,	
500
type,	
770
[Unix]	install	portable	handler,	
775
[Unix]	add	signal	to	signal	set,	
765
[Unix]	delete	signal	from	signal	set,	
765
[Unix]	clear	a	signal	set,	
765
[Unix]	add	every	signal	to	signal	set,	
765
[CS:APP]	catches	SIGINT	signal,	
763
[Unix]	test	signal	set	membership,	
765
[Unix]	init	nonlocal	jump,	
783
,	
785
sign	bits
floating-point	representation,	
137
two's	complement	representation,	
64
sign	extension,	
77
,	
77
,	
183
–
184
sign	flag	condition	code,	
201
,	
355
sign-magnitude	representation,	
68
[CS:APP]	portable	version	of	
,	
775
signal	handlers,	
758
installing,	
763
writing,	
766
–
775
Y86–64,	
364
[CS:APP]	flawed	signal	handler,	
771
[CS:APP]	flawed	signal	handler,	
772
signals,	
722
,	
756
–
758
blocking	and	unblocking,	
764
–
765
correct	handling,	
770
–
774
enabling	and	disabling,	
52
flow	synchronizing,	
776
–
778</p>
<p>portable	handling,	
774
–
775
processes,	
739
receiving,	
762
,	
762
–
764
safe	handling,	
766
–
770
sending,	
758
,	
759
–
762
terminology,	
758
–
759
waiting	for,	
778
–
781
Y86–64	pipelined	implementations,	
426
–
427
[C]	integer	data	type,	
41
signed	divide	instruction,	
198
,	
199
signed	integers,	
32
,	
40
,	
61
–
62
,	
67
alternate	representations,	
68
shift	operations,	
58
two's	complement	encoding,	
64
–
70
unsigned	conversions,	
70
–
76
signed	multiply	instruction,	
198
,	
198
signed	number	representation
guidelines,	
83
–
84
ones'	complement,	
68
sign	magnitude,	
68
signed	size	type,	
896
significands	in	floating-point	representation,	
112
signs	for	floating-point	representation,	
112
,	
112
–
113
SIGPIPE	signal,	
964
[Unix]	block	and	unblock	signals,	
765
,	
781
[Unix]	init	nonlocal	handler	jump,	
781
,	
785
[Unix]	wait	for	a	signal,	
781
[x86–64]	low	order	8	of	register	
,	
180
SimAquarium	game,	
637
–
638</p>
<p>SIMD	(single-instruction,	multiple-data)	parallelism,	
26
,	
294
,	
546
,	
547
SIMD	streaming	extensions	(SSE)	instructions,	
276
simple	segregated	storage,	
863
,	
863
–
864
simplicity	in	instruction	processing,	
385
simulated	concurrency,	
24
simultaneous	multi-threading,	
25
single-bit	data	connections,	
398
single-instruction,	multiple-data	(SIMD)	parallelism,	
26
,	
294
,	
546
–
547
single-precision	floating-point	representation
IEEE,	
113
,	
113
machine-level	data,	
178
support	for,	
41
[Y86–64]	status	code	for	illegal	instruction	exception,	
404
[CS:APP]	safely	emit	error	message	and	terminate,	
766
,
768
[CS:APP]	safely	emit	string,	
768
[CS:APP]	safely	emit	long	int,	
766
,	
768
[CS:APP]	safely	emit	string,	
766
,	
768
[CS:APP]	safely	emit	string,	
768
size
blocks,	
848
caches,	
632
–
633
data,	
39
–
42
word,	
8
,	
39</p>
<p>size	classes,	
863
[Unix]	unsigned	size	type	for	designating	sizes,	
44
,	
83
–
84
,	
86
,
99
,	
896
SIZE
tool,	
713
[C]	compute	size	of	object,	
45
,	
129
–
131
,	
133
slashes	(/)	for	root	directory,	
892
[Unix]	suspend	process,	
749
slow	system	calls,	
774
shared	object	file,	
699
[Unix]	generic	socket	address	structure,	
933
[Unix]	Internet-style	socket	address	structure,	
933
socket	addresses,	
930
socket	descriptors,	
912
,	
934
function,	
934
socket	pairs,	
930
sockets,	
892
,	
930
sockets	interface,	
932
,	
932
–
933
function,	
936
–
937
address	structures,	
933
–
934
function,	
935
function,	
934
–
935
example,	
944
–
947
helper	functions,	
942
–
944
host	and	service	conversions,	
937
–
942
function,	
935
function,	
934
–
935
function,	
934
Software	Engineering	Institute,	
100</p>
<p>software	exceptions
C++	and	Java,	
786
ECF	for,	
723
–
724
VS
.	hardware,	
724
Solaris	Sun	Microsystems	operating	system,	
16
,	
45
solid	state	disks	(SSDs),	
591
,	
600
benefits,	
587
operation,	
600
–
602
sorting	performance,	
566
–
567
source	files,	
3
source	hosts,	
922
source	programs,	
3
southbridge	chipsets,	
588
Soviet	Union,	
931
[x86–64]	low	order	16	bits	of	stack	pointer	register	
,	
180
SPARC
five-stage	pipelines,	
471
RISC	processors,	
363
Sun	Microsystems	processor,	
45
spare	cylinders,	
596
spatial	locality,	
604
caches,	
643
–
647
exploiting,	
614
special	arithmetic	operations,	
197
–
200
special	control	conditions	in	Y86–64	pipelining
detecting,	
457
–
459
handling,	
455
–
457
specifiers,	operand,	
180
–
182
speculative	execution,	
519
,	
519
,	
549
–
550</p>
<p>speedup	of	parallel	programs,	
1018
,	
1018
–
1019
spilling,	register,	
548
–
549
spin	loops,	
778
spindles,	disks,	
590
[x86–64]	low	order	8	of	stack	pointer	register	
,	
180
splitting
free	blocks,	
849
–
850
memory	blocks,	
847
[C	Stdlib]	function,	
47
,	
282
Sputnik,	
931
[x86–64]	double-precision	square	root,	
302
[x86–64]	single-precision	square	root,	
302
square	root	floating-point	instructions,	
302
squashing	mispredicted	branch	handling,	
444
SRAM	(static	RAM),	
13
,	
581
,	
581
–
582
cache.	
See	
caches	and
cache	memory</p>
<p>VS
.	DRAM,	
582
trends,	
602
–
603
SRAM	cells,	
581
[CS:APP]	pseudorandom	number	generator	seed,	
1021
SSDs	(solid	state	disks),	
591
,	
600
benefits,	
587
operation,	
600
–
602
SSE	(streaming	SIMD	extensions)	instructions,	
167
–
168
,	
294
alignment	exceptions,	
276
parallelism,	
546
–
547
[Unix]	signed	size	type,	
896
stack	corruption	detection,	
286
–
289
stack	frames,	
240
,	
240
–
241
alignment	on,	
276</p>
<p>variable-size,	
290
–
293
stack	pointers,	
239
stack	protectors,	
286
–
287
stack	randomization,	
284
–
286
stack	storage	allocation	function,	
290
,	
324
stacks,	
19
,	
189
,	
189
–
191
bottom,	
190
buffer	overflow,	
871
with	
function,	
751
–
752
local	storage,	
248
–
251
machine-level	programming,	
171
overflow.	
See	
buffer	overflow
recursive	procedures,	
253
–
255
run	time,	
239
–
241
top,	
190
Y86–64	pipelining,	
429
stages,	SEQ,	
384
–
395
decode	and	write-back,	
406
–
408
execute,	
408
–
409
fetch,	
404
–
406
memory	stage,	
409
–
411
PC	update,	
411
stalling
for	data	hazards,	
442
pipeline,	
433
–
436
,	
459
–
460
Stallman,	Richard,	
6
,	
16
standard	C	library,	
4
,	
4
–
5
standard	error	files,	
891
standard	I/O	library,	
911
,	
911</p>
<p>standard	input	files,	
891
standard	output	files,	
891
Standard	Unix	Specification,	
16
,	
698
starvation	in	readers-writers	problem,	
1008
[Unix]	fetch	file	metadata,	
903–
904
state	machines,	
980
states
bistable	memory,	
581
deadlock,	
1027
processor,	
723
programmer-visible,	
355
,	
355
–
356
progress	graphs,	
999
state	machines,	
980
static	libraries,	
684
,	
684
–
688
static	linkers,	
672
static	linking,	
672
static	RAM	(SRAM),	
13
,	
581
–
582
cache.	
See	
caches	and	cache	memory
VS
.	DRAM,	
582
trends,	
602
–
603
[C]	variable	and	function	attribute,	
676
,	
677
,	
994
static	variables,	
994
,	
994
–
995
static	Web	content,	
949
status	code	registers,	
435
status	codes
HTTP,	
953
Y86–64,	
363
–
364
,	
364
status	messages	in	HTTP,	
953</p>
<p>status	register	hazards,	
435
[Unix]	constant	for	standard	error	descriptor,	
891
stream,	
911
[Unix]	constant	for	standard	input	descriptor,	
891
stream,	
911
file,	
67
[Unix]	standard	I/O	library	header	file,	
84
,	
86
,	
4
,	
4
–
5
[Unix]	constant	for	standard	output	descriptor,	
891
stream,	
911
command,	
280
command,	
280
Stevens,	W.	Richard,	
903
,	
914
,	
965
,	
1041
stopped	processes,	
739
storage.	
See	also	
information	storage
device	hierarchy,	
14
registers,	
251
–
253
stack,	
248
–
251
storage	classes	for	variables,	
994
–
995
store	buffers,	
557
–
558
store	instructions,	
10
store	operations
example,	
588
processors,	
521
store	performance	of	memory,	
555
–
561
STRACE
tool,	
786
straight-line	code,	
200
–
201
[C	Stdlib]	string	concatenation	function,	
282</p>
<pre><code>[C	Stdlib]	string	copy	function,	
</code></pre>
<p>282
streaming	SIMD	extensions	(SSE)	instructions,	
167
–
168
,	
294
alignment	exceptions,	
276
parallelism,	
546
–
547
streams,	
911
buffers,	
911
directory,	
905
full	duplex,	
912
function,	
738
stride-1	reference	patterns,	
606
stride-
k
reference	patterns,	
606
string	concatenation	function,	
282
string	copy	function,	
282
string	generation	function,	
282
strings
in	buffer	overflow,	
279
,	
281
length,	
83
lowercase	conversions,	
509
–
511
representing,	
49
STRINGS
tool,	
713
STRIP
tool,	
713
[C	Stdlib]	string	length	function,	
83
,	
509
–
511
strong	scaling,	
1019
strong	symbols,	
680
section,	
675
[C	Stdlib]	string	function,	
1024
[C]	structure	data	type,	
265
structures
address,	
933
–
934</p>
<p>heterogeneous.	
See	
heterogeneous	data	structures
machine-level	programming,	
171
SUB
[instruction	class]	subtract,	
192
subdomains,	
927
[Y86–64]	subtract,	
356
,	
388
substitution,	inline,	
501
subtract	instruction,	
192
subtract	operation	in	execute	stage,	
408
subtraction,	floating-point,	
302
[CS:APP]	column-major	sum,	
636
[CS:APP]	row-major	sum,	
635
,	
635
[CS:APP]	vector	sum,	
634
,	
635
–
636
Sun	Microsystems,	
45
five-stage	pipelines,	
471
RISC	processors,	
363
security	vulnerability,	
100
supercells,	
582
,	
582
–
583
superscalar	processors,	
26
,	
471
,	
518
supervisor	mode,	
735
surfaces,	disks,	
590
,	
595
suspend	process	function,	
749
suspend	until	signal	arrives	function,	
750
suspended	processes,	
739
swap	areas,	
833
swap	files,	
833
swap	space,	
833
swapped-in	pages,	
809
swapped-out	pages,	
809
swapping	pages,	
809</p>
<p>sweep	phase	in	Mark&amp;Sweep	garbage	collectors,	
867
Swift,	Jonathan,	
43
[C]	multiway	branch	statement,	
232
–
238
switches,	context,	
736
–
737
symbol	resolution,	
673
,	
679
duplicate	symbol	names,	
680
–
684
static	libraries,	
684
–
688
symbol	tables,	
675
,	
675
–
679
symbolic	links,	
892
symbolic	methods,	
466
symbols
address	translation,	
814
caches,	
617
global,	
675
local,	
676
relocation,	
689
–
695
strong	and	weak,	
680
section,	
675
synchronization
flow,	
776
–
778
Java	threads,	
1010
progress	graphs,	
1000
threads,	
995
–
999
progress	graphs,	
999
–
1001
with	semaphores.	
See	
semaphores
synchronization	errors,	
995
synchronous	DRAM	(SDRAM),	
586
synchronous	exceptions,	
727
filesystem,	
736</p>
<pre><code>function,	
</code></pre>
<p>730
system	bus,	
587
system	calls,	
17
,	
727
,	
727
–
728
error	handling,	
737
–
738
Linux/x86–64	systems,	
730
–
731
slow,	
774
system-level	functions,	
730
system-level	I/O
closing	files,	
894
–
895
file	metadata,	
903
–
904
I/O	redirection,	
909
–
910
opening	files,	
893
–
895
packages	summary,	
911
–
913
reading	files,	
895
–
897
RIO
package,	
897
–
903
sharing	files,	
906
–
908
standard,	
911
summary,	
913
–
914
Unix	I/O,	
890
–
891
writing	files,	
896
–
897
system	startup	function,	
698
System	V	Unix,	
16
semaphores,	
977
shared	memory,	
977</p>
<p>T2B
(two's	complement	to	binary	conversion),	
60
,	
65
,	
71
T2U
(two's	complement	to	unsigned	conversion),	
60
,	
71
,	
71
–
73
tables
descriptor,	
907
,	
909
exception,	
725
,	
725
GOTs,	
705
,	
705
–
707
hash,	
567
–
568
header,	
674
,	
696
jump,	
233
,	
234
–
235
,	
725
page,	
736
,	
806
–
808
,	
807
,	
819
–
821
,	
823
program	header,	
696
,	
696
symbol,	
675
,	
675
–
679
tag	bits,	
615
,	
616
tags,	boundary,	
851
,	
851
–
854
,	
859
Tanenbaum,	Andrew	S.,	
20
target	functions	in	interpositioning	libraries,	
708
targets,	jump,	
206
,	
206
–
209
TCP	(Transmission	Control	Protocol),	
924
TCP/IP	(Transmission	Control	Protocol/Internet	Protocol),	
924
[Unix]	Unix	shell	program,	
753
TELNET
remote	login	program,	
950
,	
950
–
951
temporal	locality,	
604
blocking	for,	
647
exploiting,	
614
terminate	another	thread	function,	
989
terminate	current	thread	function,	
989
terminate	process	function,	
739
terminated	processes,	
739
terminating</p>
<p>processes,	
739
–
743
threads,	
988
–
989
TEST
[instruction	class]	Test,	
202
test	byte	instruction,	
202
test	double	word	instruction,	
202
test	instructions,	
202
test	quad	word	instruction,	
202
test	signal	set	membership	instruction,	
765
test	word	instruction,	
202
[x86–64]	test	byte,	
202
testing	Y86–64	pipeline	design,	
465
[x86–64]	test	double	word,	
202
[x86–64]	test	quad	word,	
202
[x86–64]	test	word,	
202
text	files,	
3
,	
891
,	
892
,	
900
text	lines,	
891
,	
898
text	representation
ASCII,	
49
Unicode,	
50
section,	
674
Thompson,	Ken,	
16
thrashing
direct-mapped	caches,	
622
,	
622
–
623
pages,	
810
thread	contexts,	
986
,	
993
thread	IDs	(TIDs),	
986
thread-level	concurrency,	
24
–
26
thread-level	parallelism,	
26
thread	routines,	
987
,	
988</p>
<p>thread-safe	functions,	
1020
,	
1020
–
1022
thread-unsafe	functions,	
1020
,	
1020
–
1022
threads,	
17
,	
18
,	
973
,	
985
–
986
concurrent	server	based	on,	
991
–
992
creating,	
988
detaching,	
989
–
990
execution	model,	
986
–
987
initializing,	
990
library	functions	for,	
1024
–
1025
mapping	variables	in,	
994
–
995
memory	models,	
993
–
994
for	parallelism,	
1013
–
1018
Posix,	
987
–
988
races,	
1025
–
1027
reaping,	
989
safety	issues,	
1020
–
1022
shared	variables	with,	
992
–
995
,	
993
synchronizing,	
995
–
999
progress	graphs,	
999
–
1001
with	semaphores.	
See	
semaphores
terminating,	
988
–
989
three-stage	pipelines,	
414
–
416
throughput,	
524
dynamic	memory	allocators,	
845
pipelining	for.	
See	
pipelining	read,</p>
<p>639
throughput	bounds,	
518
,	
524
TIDs	(thread	IDs),	
986
time	slicing,	
733
timing,	SEQ,	
400
–
403</p>
<pre><code>[CS:APP]	Web	server,	
</code></pre>
<p>956
,	
956
–
964
TLB	index	(TLBI),	
817
TLB	tags	(TLBT),	
817
,	
823
TLBI	(TLB	index),	
817
TLBs	(translation	lookaside	buffers),	
470
,	
817
,	
817
–
825
TLBT	(TLB	tags),	
817
,	
823
TMax
(maximum	two's	complement	number),	
60
,	
65
,	
66
TMin
(minimum	two's	complement	number),	
60
,	
65
,	
66
,	
77
top	of	stack,	
190
,	
190
TOP
tool,	
786
topological	sorts	of	vertices,	
742
Torvalds,	Linus,	
20
touching	pages,	
833
TRACE	method,	
951
tracing	execution,	
387
,	
394
–
395
,	
403
track	density	of	disks,	
591
tracks,	disk,	
590
,	
595
trajectories	in	progress	graphs,	
1000
,	
1000
transactions
bus,	
587
,	
588
–
589
client-server	model,	
918
client-server	vs.	database,	
919
HTTP,	
950
–
953
transfer	time	for	disks,	
594
transfer	units,	
612
transferring	control,	
241
–
245
transformations,	reassociation,	
541
,	
541
–
546
,	
570
transistors	in	Moore's	Law,	
169
transitions</p>
<p>progress	graphs,	
999
state	machines,	
980
translating	programs,	
4
–
5
translation
address.	
See	
address	translation
statements,	
233
translation	lookaside	buffers	(TLBs),	
470
,	
817
,	
817
–
825
Transmission	Control	Protocol	(TCP),	
924
Transmission	Control	Protocol/Internet	Protocol	(TCP/IP),	
924
trap	exception	class,	
727
traps,	
727
,	
727
–
728
tree	height	reduction,	
570
tree	structure,	
270
–
271
truncating	numbers,	
81
–
82
two-operand	multiply	instructions,	
198
two-way	parallelism,	
536
–
537
two's-complement	representation
addition,	
90
–
95
asymmetric	range,	
66
,	
77
bit-level	representation,	
96
encodings,	
32
minimum	value,	
65
multiplication,	
97
–
101
negation,	
95
signed	and	unsigned	conversions,	
70
–
74
signed	numbers,	
64
,	
64
–
70
[C]	type	definition,	
44
,	
47
types
conversions.	
See	
conversions</p>
<p>floating	point,	
124
–
126
integral,	
60
,	
60
–
62
machine-level,	
171
,	
177
–
178
MIME,	
949
naming,	
47
pointers,	
36
,	
277
pointers	associated	with,	
34</p>
<p>U2B
(unsigned	to	binary	conversion),	
60
,	
64
,	
71
,	
74
U2T
(unsigned	to	two's-complement	conversion),	
60
,	
71
,	
73
,	
82
[x86–64]	compare	double	precision,	
306
[x86–64]	compare	single	precision,	
306
UDP	(Unreliable	Datagram	Protocol),	
924
constant,	maximum	unsigned	integer,	
68
[C]	maximum	value	of	
N
-bit	unsigned	data	type,	
67
[C]	
N
-bit	unsigned	integer	data	type,	
67
function,	
894
–
895
UMax
(maximum	unsigned	number),	
63
,	
66
–
67
unallocated	pages,	
805
unary	operations,	
194
unblocking	signals,	
764
–
765
unbuffered	input	and	output,	
897
–
898
uncached	pages,	
806
unconditional	jump	instruction,	
357
underflow,	gradual,	
115
Unicode	characters,	
50
unified	caches,	
631
uniform	resource	identifiers	(URIs),	
951
uninitialized	memory,	reading,	
871
unions,	
44
,	
269
–
273
uniprocessor	systems,	
16
,	
24
United	States,	ARPA	creation	in,	
931
universal	resource	locators	(URLs),	
949
Universal	Serial	Bus	(USB),	
596
Unix	4.xBSD,	
16
,	
932
[CS:APP]	reports	Unix-style	errors,	
738
,	
738
,	
1043
Unix	IPC,	
977</p>
<p>Unix	operating	systems,	
16
,	
16
,	
35
constants,	
746
error	handling,	
1043
,	
1043
I/O,	
19
,	
890
,	
890
–
891
Unix	signals,	
759
unlocking	mutexes,	
1003
unmap	disk	object	function,	
839
unordered,	floating-point	comparison	outcome,	
306
unpack	and	interleave	low	packed	double	precision	instruction,	
298
unpack	and	interleave	low	packed	single	precision	instruction,	
298
Unreliable	Datagram	Protocol	(UDP),	
924
unrolling
k
×	1,	
531
k
×	1
a
,	
544
×	
,	
539
–
540
loops,	
502
,	
504
,	
531
,	
531
–
535
,	
572
unsafe	regions	in	progress	graphs,	
1000
unsafe	trajectories	in	progress	graphs,	
1000
[Unix]	delete	environment	variable,	
752
[C]	integer	data	type,	
41
,	
61
unsigned	representations,	
83
–
84
addition,	
84
–
90
conversions,	
70
–
76
division,	
198
,	
199
encodings,	
32
,	
62
–
64
integers,	
40
maximum	value,	
63
multiplication,	
96
–
97
,	
198
,	
198
size	type,	
896</p>
<p>update	instructions,	
9
–
10
URIs	(uniform	resource	identifiers),	
951
URLs	(universal	resource	locators),	
949
USB	(Universal	Serial	Bus),	
596
user-level	memory	mapping,	
837
–
839
user	mode,	
726
processes,	
734
–
736
,	
735
regular	functions	in,	
728
user	stack,	
19
UTF-8	characters,	
50</p>
<pre><code>[CS:APP]	wrapper	function	for	Posix	sem_post,	
</code></pre>
<p>1002
v-node	tables,	
906
V	semaphore	operation,	
1001
,	
1001
–
1002
VA.	
See	
virtual	addresses	(VA)
[x86–64]	double-precision	addition,	
302
[x86–64]	single-precision	addition,	
302
VALGRIND
program,	
569
valid	bit
cache	lines,	
615
page	tables,	
807
values,	pointers,	
36
,	
277
[x86–64]	and	packed	double	precision,	
305
[x86–64]	and	packed	single	precision,	</p>
<div style="break-before: page; page-break-before: always;"></div><p>305
variable-size	stack	frames,	
290
–
293
variable-size	arrays,	
262
–
265
variables
mapping,	
994
–
995
nonexistent,	
874
shared,	
992
–
995
,	
993
storage	classes,	
994
–
995
VAX	computers	(Digital	Equipment	Corporation),	Boolean	operations,
56
[x86–64]	convert	packed	single	to	packed	double	precision,
298
[x86–64]	convert	integer	to	double	precision,	
297
[x86–64]	convert	quad-word	integer	to	double	precision,
297
[x86–64]	convert	integer	to	single	precision,	
297
[x86–64]	convert	quad-word	integer	to	single	precision,</p>
<p>297
[x86–64]	convert	double	precision	to	integer,	
297
[x86–64]	convert	double	precision	to	quad-word	integer,
297
[x86–64]	convert	single	precision	to	integer,	
297
[x86–64]	convert	single	precision	to	quad-word	integer,
297
[x86–64]	double-precision	division,	
302
[x86–64]	single-precision	division,	
302
vector	data	types,	
26
,	
504
–
507
vector	dot	product	function,	
622
vector	registers,	
171
,	
546
vector	sum	function,	
634
,	
635
–
636
vectors,	bit,	
51
,	
51
–
52
verification	in	pipelining,	
466
Verilog	hardware	description	language	for	logic	design,	
373
Y86–64	pipelining	implementation,	
467
vertical	bars	||	for	
OR
operation,	
373
VHDL	hardware	description	language,	
373
victim	blocks,	
612
Video	RAM	(VRAM),	
586
virtual	address	spaces,	
18
,	
34
,	
804
virtual	addresses	(VA)
machine-level	programming,	
170
–
171
vs.	physical,	
803
–
804
Y86–64,	
356
virtual	machines
as	abstraction,	
27
Java	byte	code,	
310</p>
<p>virtual	memory	(VM),	
15
,	
18
,	
34
,	
802
as	abstraction,	
27
address	spaces,	
804
–
805
address	translation.	
See	
address	translation
bugs,	
870
–
875
for	caching,	
805
–
811
characteristics,	
802
–
803
Core	i7,	
825
–
828
dynamic	memory	allocation.	
See	
dynamic	memory	allocation
garbage	collection,	
865
–
870
Linux,	
830
–
833
in	loading,	
699
managing,	
839
mapping.	
See	
memory	mapping
for	memory	management,	
811
–
812
for	memory	protection,	
812
–
813
overview,	
18
–
19
physical	vs.	virtual	addresses,	
803
–
804
summary,	
875
–
876
virtual	page	numbers	(VPNs),	
814
virtual	page	offset	(VPO),	
814
virtual	pages	(VPs),	
289
,	
805
,	
805
–
806
viruses,	
285
–
286
VLOG	implementation	of	Y86–64
pipelining,	
467
VM.	
See	
virtual	memory	(VM)
[x86–64]	double-precision	maximum,	
302
[x86–64]	single-precision	maximum,	
302
[x86–64]	double-precision	minimum,	
302</p>
<pre><code>[x86–64]	single-precision	minimum,	
</code></pre>
<h2>302
[x86–64]	move	aligned,	packed	double	precision,	
296
[x86–64]	move	aligned,	packed	single	precision,	
296
[x86–64]	move	double	precision,	
296
[x86–64]	move	single	precision,	
296
[x86–64]	double-precision	multiplication,	
302
[x86–64]	single-precision	multiplication,	
302
[C]	untyped	pointers,	
48
[C]	volatile	type	qualifier,	
769
–
770
VP	(virtual	pages),	
289
,	
805
,	
805
–
806
VPNs	(virtual	page	numbers),	
814
VPO	(virtual	page	offset),	
814
VRAM	(video	RAM),	
586
[x86–64]	double-precision	subtraction,	
302
[x86–64]	single-precision	subtraction,	
302
VTUNE
program,	
569
vulnerabilities,	security,	
86
–
87
[x86–64]	unpack	and	interleave	low	packed	double
precision,	
298
[x86–64]	unpack	and	interleave	low	packed	single
precision,	
298
[x86–64]	
EXCLUSIVE</h2>
<h2>OR
packed	double	precision,	
305
[x86–64]	
EXCLUSIVE</h2>
<p>OR
packed	single	precision,	
305</p>
<pre><code>[Unix]	wait	for	child	process,	
</code></pre>
<p>746
wait	for	child	process	functions,	
744
,	
746
–
749
wait	for	client	connection	request	function,	
936
,	
936
–
937
wait	for	signal	instruction,	
781
file,	
746
wait	sets,	
744
,	
744
waiting	for	signals,	
778
–
781
[Unix]	wait	for	child	process,	
743
,	
746
–
749
[CS:APP]	
example,	
747
[CS:APP]	
example,	
749
WANs	(wide	area	networks),	
921
,	
921
–
922
warming	up	caches,	
612
WCONTINUED	constant,	
744
weak	scaling,	
1019
,	
1020
weak	symbols,	
680
wear	leveling	logic,	
601
Web	clients,	
948
,	
948
Web	servers,	
701
,	
948
basics,	
948
–
949
dynamic	content,	
953
–
954
HTTP	transactions,	
950
–
953
example,	
956
–
964
Web	content,	
949
–
950
well-known	ports,	
930
well-known	service	names,	
930
[C]	loop	statement,	
223
–
228
wide	area	networks	(WANs),	
921
,	
921
–
922
WIFEXITED	constant,	
745
WIFEXITSTATUS	constant,	
745</p>
<p>WIFSIGNALED	constant,	
745
WIFSTOPPED	constant,	
745
Windows	Microsoft	operating	system,	
27
,	
45
wire	names	in	hardware	diagrams,	
398
WNOHANG	constant,	
744
–
745
word-level	combinational	circuits,	
376
–
380
word	selection
direct-mapped	caches,	
619
fully	associative	caches,	
627
–
628
set	associative	caches,	
625
–
626
word	size,	
8
,	
39
words,	
8
,	
177
working	sets,	
613
,	
810
world-wide	data	connections	in	hardware	diagrams,	
398
World	Wide	Web,	
949
worm	programs,	
284
–
286
wrapper	functions,	
711
error	handling,	
738
,	
1041
,	
1043
–
1045
interpositioning	libraries,	
708
write	access,	
289
write-allocate	approach,	
630
write-back	approach,	
630
write-back	stage
instruction	processing,	
385
,	
387
–
397
PIPE	processor,	
449
–
453
sequential	processing,	
400
sequential	Y86–64	implementation,	
406
–
408
[Unix]	write	file,	
895
,	
896
–
897
write	hits,	
630</p>
<p>write	issues	for	caches,	
630
–
631
write-only	register,	
527
write	operations	for	files,	
891
,	
896
–
897
write	ports
priorities,	
408
register	files,	
382
write/read	dependencies,	
557
–
559
write	strategies	for	caches,	
633
write-through	approach,	
630
write	transactions,	
587
,	
588
–
589
function,	
903
writers	in	readers-writers	problem,	
1006
,	
1008
writing
signal	handlers,	
766
–
775
SSD	oprations,	
600
WSTOPSIG	constant,	
745
WTERMSIG	constant,	
745
WUNTRACED	constant,	
744
–
745</p>
<h2>x86	Intel	microprocessor	line,	
166
x86–64	instruction	set	architecture	vs.	Y86–64,	
360
x86–64	microprocessors,	
168
array	access,	
256
conditional	move	instructions,	
214
–
220
data	alignment,	
276
exceptions,	
729
–
731
Intel-compatible	64-bit	microprocessors,	
45
machine	language,	
165
–
166
registers
data	movement,	
182
–
189
operand	specifiers,	
180
–
182
vs.	Y86–64,	
365
–
366
x87	microprocessors,	
167
XDR	library	security	vulnerability,	
100
[x86–64]	16-byte	media	register.	Subregion	of	YMM,	
295
,	return	floating-point	value	register,	
299
,	
301
XMM,	SSE	vector	registers,	
294
–
296
XOR
[instruction	class]	
EXCLUSIVE</h2>
<h2>OR
,	
192
[Y86–64]	
EXCLUSIVE</h2>
<p>OR
,	
356</p>
<p>Y86–64	instruction	set	architecture,	
353
–
354
details,	
370
–
372
exception	handling,	
363
–
364
hazards,	
435
instruction	encoding,	
358
–
360
instruction	set,	
356
–
358
programmer-visible	state,	
355
–
356
programs,	
364
–
370
sequential	implementation.
See	
sequential	Y86–64	implementation
vs.	x86–64,	
360
Y86–64	pipelined	implementations,	
421
computation	stages,	
421
–
422
control	logic.	
See	
control	logic	in	pipelining
exception	handling,	
444
–
447
hazards.	
See	
hazards	in	pipelining
memory	system	interfacing,	
469
–
470
multicycle	instructions,	
468
–
469
performance	analysis,	
464
–
468
predicted	values,	
427
–
429
register	insertions,	
422
–
426
signals,	
426
–
427
stages.	
See	
PIPE	processor	stages
testing,	
465
verification,	
466
Verilog,	
467
YAS
Y86–64	assembler,	
366
YIS
Y86–64	instruction	set	simulator,	
366
[x86–64]	32-byte	media	register,	
295</p>
<p>YMM,	AVX	vector	registers,	
294
–
296</p>
<p>zero	extension,	
77
zero	flag	condition	code,	
201
,	
306
,	
355
[x86–64]	zero	flag	condition	code,	
201
,	
306
,	
355
zombie	processes,	
743
,	
743
–
744
,	
770
zones,	recording,	
592</p>
<p>Contents
1
.	
Computer	Systems	
A	Programmer's	Perspective
2
.	
Computer	Systems	
A	Programmer's	Perspective
3
.	
MasteringEngineering
4
.	
Contents
5
.	
Preface
A
.	
Assumptions	about	the	Reader's	Background
B
.	
How	to	Read	the	Book
C
.	
Book	Overview
D
.	
New	to	This	Edition
E
.	
Origins	of	the	Book
F
.	
For	Instructors:	Courses	Based	on	the	Book
G
.	
For	Instructors:	Classroom-Tested	Laboratory	Exercises
6
.	
About	the	Authors
7
.	
Chapter	
1	
A	Tour	of	Computer	Systems
A
.	
1.1	
Information	Is	Bits	+	Context
B
.	
1.2	
Programs	Are	Translated	by	Other	Programs	into	Different
Forms
C
.	
1.3	
It	Pays	to	Understand	How	Compilation	Systems	Work
D
.	
1.4	
Processors	Read	and	Interpret	Instructions	Stored	in
Memory
1
.	
1.4.1	
Hardware	Organization	of	a	System
a
.	
Buses
b
.	
I/O	Devices
c
.	
Main	Memory
®</p>
<p>d
.	
Processor
2
.	
1.4.2	
Running	the	
Program
E
.	
1.5	
Caches	Matter
F
.	
1.6	
Storage	Devices	Form	a	Hierarchy
G
.	
1.7	
The	Operating	System	Manages	the	Hardware
1
.	
1.7.1	
Processes
2
.	
1.7.2	
Threads
3
.	
1.7.3	
Virtual	Memory
4
.	
1.7.4	
Files
H
.	
1.8	
Systems	Communicate	with	Other	Systems	Using
Networks
I
.	
1.9	
Important	Themes
1
.	
1.9.1	
Amdahl's	Law
a
.	
Practice	Problem	
1.1	
(solution	page	28)
b
.	
Practice	Problem	
1.2	
(solution	page	28)
2
.	
1.9.2	
Concurrency	and	Parallelism
a
.	
Thread-Level	Concurrency
b
.	
Instruction-Level	Parallelism
c
.	
Single-Instruction,	Multiple-Data	(SIMD)	Parallelism
3
.	
1.9.3	
The	Importance	of	Abstractions	in	Computer
Systems
J
.	
1.10	
Summary
K
.	
Bibliographic	Notes</p>
<p>8
.	
Part	
I	
Program	Structure	and	Execution
A
.	
Chapter	
2	
Representing	and	Manipulating	Information
1
.	
2.1	
Information	Storage
a
.	
2.1.1	
Hexadecimal	Notation
a
.	
Practice	Problem	
2.1
(solution	page	143)
b
.	
Practice	Problem	
2.2
(solution	page	143)
c
.	
Practice	Problem	
2.3
(solution	page	144)
d
.	
Practice	Problem	
2.4
(solution	page	144)
b
.	
2.1.2	
Data	Sizes
c
.	
2.1.3	
Addressing	and	Byte	Ordering
a
.	
Practice	Problem	
2.5
(solution	page	144)
b
.	
Practice	Problem	
2.6
(solution	page	145)
d
.	
2.1.4	
Representing	Strings
a
.	
Practice	Problem	
2.7
(solution	page	145)
e
.	
2.1.5	
Representing	Code
f
.	
2.1.6	
Introduction	to	Boolean	Algebra
a
.	
Practice	Problem	
2.8
(solution	page	145)
b
.	
Practice	Problem	
2.9
(solution	page	146)
g
.	
2.1.7	
Bit-Level	Operations	in	C
a
.	
Practice	Problem	
2.10
(solution	page	146)
b
.	
Practice	Problem	
2.11
(solution	page	146)
c
.	
Practice	Problem	
2.12
(solution	page	146)
d
.	
Practice	Problem	
2.13
(solution	page	147)
h
.	
2.1.8	
Logical	Operations	in	C
a
.	
Practice	Problem	
2.14
(solution	page	147)</p>
<p>b
.	
Practice	Problem	
2.15
(solution	page	148)
i
.	
2.1.9	
Shift	Operations	in	C
a
.	
Practice	Problem	
2.16
(solution	page	148)
2
.	
2.2	
Integer	Representations
a
.	
2.2.1	
Integral	Data	Types
b
.	
2.2.2	
Unsigned	Encodings
c
.	
2.2.3	
Two's-Complement	Encodings
a
.	
Practice	Problem	
2.17
(solution	page	148)
b
.	
Practice	Problem	
2.18
(solution	page	149)
d
.	
2.2.4	
Conversions	between	Signed	and	Unsigned
a
.	
Practice	Problem	
2.19
(solution	page	149)
b
.	
Practice	Problem	
2.20
(solution	page	149)
e
.	
2.2.5	
Signed	versus	Unsigned	in	C
a
.	
Practice	Problem	
2.21
(solution	page	149)
f
.	
2.2.6	
Expanding	the	Bit	Representation	of	a	Number
a
.	
Practice	Problem	
2.22
(solution	page	150)
b
.	
Practice	Problem	
2.23
(solution	page	150)
g
.	
2.2.7	
Truncating	Numbers
a
.	
Practice	Problem	
2.24
(solution	page	150)
h
.	
2.2.8	
Advice	on	Signed	versus	Unsigned
a
.	
Practice	Problem	
2.25
(solution	page	151)
b
.	
Practice	Problem	
2.26
(solution	page	151)</p>
<p>3
.	
2.3	
Integer	Arithmetic
a
.	
2.3.1	
Unsigned	Addition
a
.	
Practice	Problem	
2.27
(solution	page	152)
b
.	
Practice	Problem	
2.28
(solution	page	152)
b
.	
2.3.2	
Two's-Complement	Addition
a
.	
Practice	Problem	
2.29
(solution	page	152)
b
.	
Practice	Problem	
2.30
(solution	page	153)
c
.	
Practice	Problem	
2.31
(solution	page	153)
d
.	
Practice	Problem	
2.32
(solution	page	153)
c
.	
2.3.3	
Two's-Complement	Negation
a
.	
Practice	Problem	
2.33
(solution	page	153)
d
.	
2.3.4	
Unsigned	Multiplication
e
.	
2.3.5	
Two's-Complement	Multiplication
a
.	
Practice	Problem	
2.34
(solution	page	153)
b
.	
Practice	Problem	
2.35
(solution	page	154)
c
.	
Practice	Problem	
2.36
(solution	page	154)
d
.	
Practice	Problem	
2.37
(solution	page	155)
f
.	
2.3.6	
Multiplying	by	Constants
a
.	
Practice	Problem	
2.38
(solution	page	155)
b
.	
Practice	Problem	
2.39
(solution	page	156)
c
.	
Practice	Problem	
2.40
(solution	page	156)
d
.	
Practice	Problem	
2.41
(solution	page	156)
g
.	
2.3.7	
Dividing	by	Powers	of	2
a
.	
Practice	Problem	
2.42
(solution	page	156)
b
.	
Practice	Problem	
2.43
(solution	page	157)</p>
<p>h
.	
2.3.8	
Final	Thoughts	on	Integer	Arithmetic
a
.	
Practice	Problem	
2.44
(solution	page	157)
4
.	
2.4	
Floating	Point
a
.	
2.4.1	
Fractional	Binary	Numbers
a
.	
Practice	Problem	
2.45
(solution	page	157)
b
.	
Practice	Problem	
2.46
(solution	page	158)
b
.	
2.4.2	
IEEE	Floating-Point	Representation
c
.	
2.4.3	
Example	Numbers
a
.	
Practice	Problem	
2.47
(solution	page	158)
b
.	
Practice	Problem	
2.48
(solution	page	159)
c
.	
Practice	Problem	
2.49
(solution	page	159)
d
.	
2.4.4	
Rounding
a
.	
Practice	Problem	
2.50
(solution	page	159)
b
.	
Practice	Problem	
2.51
(solution	page	159)
c
.	
Practice	Problem	
2.52
(solution	page	160)
e
.	
2.4.5	
Floating-Point	Operations
f
.	
2.4.6	
Floating	Point	in	C
a
.	
Practice	Problem	
2.53
(solution	page	160)
b
.	
Practice	Problem	
2.54
(solution	page	160)
5
.	
2.5	
Summary
6
.	
Bibliographic	Notes
7
.	
Homework	Problems
a
.	
2.55	
♦
b
.	
2.56	
♦</p>
<p>c
.	
2.57	
♦
d
.	
2.58	
♦♦
e
.	
2.59	
♦♦
f
.	
2.60	♦♦
g
.	
Bit-Level	Integer	Coding	Rules
h
.	
2.61	♦♦
i
.	
2.62	♦♦♦
j
.	
2.63	♦♦♦
k
.	
2.64	♦
l
.	
2.65	♦♦♦♦
m
.	
2.66	♦♦♦♦
n
.	
2.67	♦♦
o
.	
2.68	♦♦
p
.	
2.69	♦♦♦
q
.	
2.70	♦♦
r
.	
2.71
s
.	
2.72
t
.	
2.73
u
.	
2.74
v
.	
2.75
w
.	
2.76
x
.	
2.77
y
.	
2.78
z
.	
2.79
aa
.	
2.80
ab
.	
2.81
ac
.	
2.82
ad
.	
2.83
ae
.	
2.84
af
.	
2.85</p>
<p>ag
.	
2.86
ah
.	
2.87
ai
.	
2.88
aj
.	
2.89
ak
.	
2.90
al
.	
2.91
am
.	
Bit-Level	Floating-Point	Coding	Rules
an
.	
2.92	♦♦
ao
.	
2.94
ap
.	
2.95
aq
.	
2.96
ar
.	
2.97
B
.	
Chapter	
3	
Machine-Level	Representation	of	Programs
1
.	
3.1	
A	Historical	Perspective
2
.	
3.2	
Program	Encodings
a
.	
3.2.1	
Machine-Level	Code
b
.	
3.2.2	
Code	Examples
c
.	
3.2.3	
Notes	on	Formatting
3
.	
3.3	
Data	Formats
4
.	
3.4	
Accessing	Information
a
.	
3.4.1	
Operand	Specifiers
a
.	
Practice	Problem	
3.1	
(solution	page	325)
b
.	
3.4.2	
Data	Movement	Instructions
a
.	
Practice	Problem	
3.2	
(solution	page	325)
b
.	
Practice	Problem	
3.3	
(solution	page	326)
c
.	
3.4.3	
Data	Movement	Example</p>
<p>a
.	
Practice	Problem	
3.4	
(solution	page	326)
b
.	
Practice	Problem	
3.5	
(solution	page	327)
d
.	
3.4.4	
Pushing	and	Popping	Stack	Data
5
.	
3.5	
Arithmetic	and	Logical	Operations
a
.	
3.5.1	
Load	Effective	Address
a
.	
Practice	Problem	
3.6	
(solution	page	327)
b
.	
Practice	Problem	
3.7	
(solution	page	328)
b
.	
3.5.2	
Unary	and	Binary	Operations
a
.	
Practice	Problem	
3.8	
(solution	page	328)
c
.	
3.5.3	
Shift	Operations
a
.	
Practice	Problem	
3.9	
(solution	page	328)
d
.	
3.5.4	
Discussion
a
.	
Practice	Problem	
3.10	
(solution	page	329)
b
.	
Practice	Problem	
3.11	
(solution	page	329)
e
.	
3.5.5	
Special	Arithmetic	Operations
a
.	
Practice	Problem	
3.12	
(solution	page	329)
6
.	
3.6	
Control
a
.	
3.6.1	
Condition	Codes
b
.	
3.6.2	
Accessing	the	Condition	Codes
a
.	
Practice	Problem	
3.13	
(solution	page	330)
b
.	
Practice	Problem	
3.14	
(solution	page	330)
c
.	
3.6.3	
Jump	Instructions</p>
<p>d
.	
3.6.4	
Jump	Instruction	Encodings
a
.	
Practice	Problem	
3.15	
(solution	page	330)
e
.	
3.6.5	
Implementing	Conditional	Branches	with
Conditional	Control
a
.	
Practice	Problem	
3.16	
(solution	page	331)
b
.	
Practice	Problem	
3.17	
(solution	page	331)
c
.	
Practice	Problem	
3.18	
(solution	page	332)
f
.	
3.6.6	
Implementing	Conditional	Branches	with
Conditional	Moves
a
.	
Practice	Problem	
3.19	
(solution	page	332)
b
.	
Practice	Problem	
3.20	
(solution	page	333)
c
.	
Practice	Problem	
3.21	
(solution	page	333)
g
.	
3.6.7	
Loops
a
.	
Do-While	Loops
a
.	
Practice	Problem	
3.22	
(solution	page	333)
b
.	
Practice	Problem	
3.23	
(solution	page	334)
b
.	
While	Loops
a
.	
Practice	Problem	
3.24	
(solution	page	335)
b
.	
Practice	Problem	
3.25	
(solution	page	335)
c
.	
Practice	Problem	
3.26	
(solution	page	336)
c
.	
For	Loops
a
.	
Practice	Problem	
3.27	
(solution	page	336)
b
.	
Practice	Problem	
3.28	
(solution	page	336)
c
.	
Practice	Problem	
3.29	
(solution	page	337)</p>
<p>h
.	
3.6.8	
Switch	Statements
a
.	
Practice	Problem	
3.30	
(solution	page	338)
b
.	
Practice	Problem	
3.31	
(solution	page	338)
7
.	
3.7	
Procedures
a
.	
3.7.1	
The	Run-Time	Stack
b
.	
3.7.2	
Control	Transfer
a
.	
Practice	Problem	
3.32	
(solution	page	339)
c
.	
3.7.3	
Data	Transfer
a
.	
Practice	Problem	
3.33	
(solution	page	339)
d
.	
3.7.4	
Local	Storage	on	the	Stack
e
.	
3.7.5	
Local	Storage	in	Registers
a
.	
Practice	Problem	
3.34	
(solution	page	340)
f
.	
3.7.6	
Recursive	Procedures
a
.	
Practice	Problem	
3.35	
(solution	page	340)
8
.	
3.8	
Array	Allocation	and	Access
a
.	
3.8.1	
Basic	Principles
a
.	
Practice	Problem	
3.36	
(solution	page	341)
b
.	
3.8.2	
Pointer	Arithmetic
a
.	
Practice	Problem	
3.37	
(solution	page	341)
c
.	
3.8.3	
Nested	Arrays
a
.	
Practice	Problem	
3.38	
(solution	page	341)
d
.	
3.8.4	
Fixed-Size	Arrays</p>
<p>a
.	
Practice	Problem	
3.39	
(solution	page	342)
b
.	
Practice	Problem	
3.40	
(solution	page	342)
e
.	
3.8.5	
Variable-Size	Arrays
9
.	
3.9	
Heterogeneous	Data	Structures
a
.	
3.9.1	
Structures
a
.	
Practice	Problem	
3.41	
(solution	page	343)
b
.	
Practice	Problem	
3.42	
(solution	page	343)
b
.	
3.9.2	
Unions
a
.	
Practice	Problem	
3.43	
(solution	page	344)
c
.	
3.9.3	
Data	Alignment
a
.	
Practice	Problem	
3.44	
(solution	page	345)
b
.	
Practice	Problem	
3.45	
(solution	page	345)
10
.	
3.10	
Combining	Control	and	Data	in	Machine-Level
Programs
a
.	
3.10.1	
Understanding	Pointers
b
.	
3.10.2	
Life	in	the	Real	World:	Using	the	
GDB</p>
<p>Debugger
c
.	
3.10.3	
Out-of-Bounds	Memory	References	and	Buffer
Overflow
a
.	
Practice	Problem	
3.46	
(solution	page	346)
d
.	
3.10.4	
Thwarting	Buffer	Overflow	Attacks
a
.	
Stack	Randomization
a
.	
Practice	Problem	
3.47	
(solution	page	347)
b
.	
Stack	Corruption	Detection</p>
<p>a
.	
Practice	Problem	
3.48	
(solution	page	347)
c
.	
Limiting	Executable	Code	Regions
e
.	
3.10.5	
Supporting	Variable-Size	Stack	Frames
a
.	
Practice	Problem	
3.49	
(solution	page	347)
11
.	
3.11	
Floating-Point	Code
a
.	
3.11.1	
Floating-Point	Movement	and	Conversion
Operations
a
.	
Practice	Problem	
3.50	
(solution	page	347)
b
.	
Practice	Problem	
3.51	
(solution	page	348)
b
.	
3.11.2	
Floating-Point	Code	in	Procedures
a
.	
Practice	Problem	
3.52	
(solution	page	348)
c
.	
3.11.3	
Floating-Point	Arithmetic	Operations
a
.	
Practice	Problem	
3.53	
(solution	page	348)
b
.	
Practice	Problem	
3.54	
(solution	page	349)
d
.	
3.11.4	
Defining	and	Using	Floating-Point	Constants
a
.	
Practice	Problem	
3.55	
(solution	page	349)
e
.	
3.11.5	
Using	Bitwise	Operations	in	Floating-Point	Code
a
.	
Practice	Problem	
3.56	
(solution	page	350)
f
.	
3.11.6	
Floating-Point	Comparison	Operations
a
.	
Practice	Problem	
3.57	
(solution	page	350)
g
.	
3.11.7	
Observations	about	Floating-Point	Code</p>
<p>12
.	
3.12	
Summary
13
.	
Bibliographic	Notes
14
.	
Homework	Problems
a
.	
3.58
b
.	
3.59
c
.	
3.60
d
.	
3.61
e
.	
3.62
f
.	
3.63
g
.	
3.64
h
.	
3.65
i
.	
3.66
j
.	
3.67
k
.	
3.68
l
.	
3.69
m
.	
3.70
n
.	
3.71
o
.	
3.72
p
.	
3.73
q
.	
3.74
r
.	
3.75
C
.	
Chapter	
4	
Processor	Architecture
1
.	
4.1	
The	Y86-64	Instruction	Set	Architecture
a
.	
4.1.1	
Programmer-Visible	State
b
.	
4.1.2	
Y86-64	Instructions
c
.	
4.1.3	
Instruction	Encoding
a
.	
Practice	Problem	
4.1	
(solution	page	480)
b
.	
Practice	Problem	
4.2	
(solution	page	481)</p>
<p>d
.	
4.1.4	
Y86-64	Exceptions
e
.	
4.1.5	
Y86-64	Programs
a
.	
Practice	Problem	
4.3	
(solution	page	482)
b
.	
Practice	Problem	
4.4	
(solution	page	482)
c
.	
Practice	Problem	
4.5	
(solution	page	483)
d
.	
Practice	Problem	
4.6	
(solution	page	483)
f
.	
4.1.6	
Some	Y86-64	Instruction	Details
a
.	
Practice	Problem	
4.7	
(solution	page	484)
b
.	
Practice	Problem	
4.8	
(solution	page	484)
2
.	
4.2	
Logic	Design	and	the	Hardware	Control	Language	HCL
a
.	
4.2.1	
Logic	Gates
b
.	
4.2.2	
Combinational	Circuits	and	HCL	Boolean
Expressions
a
.	
Practice	Problem	
4.9	
(solution	page	484)
c
.	
4.2.3	
Word-Level	Combinational	Circuits	and	HCL
Integer	Expressions
a
.	
Practice	Problem	
4.10	
(solution	page	484)
b
.	
Practice	Problem	
4.11	
(solution	page	484)
c
.	
Practice	Problem	
4.12	
(solution	page	484)
d
.	
4.2.4	
Set	Membership
e
.	
4.2.5	
Memory	and	Clocking
3
.	
4.3	
Sequential	Y86-64	Implementations
a
.	
4.3.1	
Organizing	Processing	into	Stages
a
.	
Practice	Problem	
4.13	
(solution	page	485)</p>
<p>b
.	
Practice	Problem	
4.14	
(solution	page	486)
c
.	
Practice	Problem	
4.15	
(solution	page	486)
d
.	
Practice	Problem	
4.16	
(solution	page	486)
e
.	
Practice	Problem	
4.17	
(solution	page	486)
f
.	
Practice	Problem	
4.18	
(solution	page	487)
b
.	
4.3.2	
SEQ	Hardware	Structure
c
.	
4.3.3	
SEQ	Timing
d
.	
4.3.4	
SEQ	Stage	Implementations
a
.	
Fetch	Stage
a
.	
Practice	Problem	
4.19	
(solution	page	487)
b
.	
Decode	and	Write-Back	Stages
a
.	
Practice	Problem	
4.20	
(solution	page	488)
b
.	
Practice	Problem	
4.21	
(solution	page	488)
c
.	
Practice	Problem	
4.22	
(solution	page	488)
c
.	
Execute	Stage
a
.	
Practice	Problem	
4.23	
(solution	page	488)
b
.	
Practice	Problem	
4.24	
(solution	page	488)
d
.	
Memory	Stage
a
.	
Practice	Problem	
4.25	
(solution	page	488)
b
.	
Practice	Problem	
4.26	
(solution	page	489)
c
.	
Practice	Problem	
4.27	
(solution	page	489)
e
.	
PC	Update	Stage
f
.	
Surveying	SEQ
4
.	
4.4	
General	Principles	of	Pipelining</p>
<p>a
.	
4.4.1	
Computational	Pipelines
b
.	
4.4.2	
A	Detailed	Look	at	Pipeline	Operation
c
.	
4.4.3	
Limitations	of	Pipelining
a
.	
Nonuniform	Partitioning
a
.	
Practice	Problem	
4.28	
(solution	page	489)
b
.	
Diminishing	Returns	of	Deep	Pipelining
a
.	
Practice	Problem	
4.29	
(solution	page	490)
d
.	
4.4.4	
Pipelining	a	System	with	Feedback
5
.	
4.5	
Pipelined	Y86-64	Implementations
a
.	
4.5.1	
SEQ+:	Rearranging	the	Computation	Stages
b
.	
4.5.2	
Inserting	Pipeline	Registers
c
.	
4.5.3	
Rearranging	and	Relabeling	Signals
d
.	
4.5.4	
Next	PC	Prediction
e
.	
4.5.5	
Pipeline	Hazards
a
.	
Avoiding	Data	Hazards	by	Stalling
b
.	
Avoiding	Data	Hazards	by	Forwarding
c
.	
Load/Use	Data	Hazards
d
.	
Avoiding	Control	Hazards
f
.	
4.5.6	
Exception	Handling
g
.	
4.5.7	
PIPE	Stage	Implementations
a
.	
PC	Selection	and	Fetch	Stage
a
.	
Practice	Problem	
4.30	
(solution	page	490)
b
.	
Decode	and	Write-Back	Stages
a
.	
Practice	Problem	
4.31	
(solution	page	490)
b
.	
Practice	Problem	
4.32	
(solution	page	490)</p>
<p>c
.	
Practice	Problem	
4.33	
(solution	page	491)
d
.	
Practice	Problem	
4.34	
(solution	page	491)
c
.	
Execute	Stage
a
.	
Practice	Problem	
4.35	
(solution	page	491)
d
.	
Memory	Stage
a
.	
Practice	Problem	
4.36	
(solution	page	492)
h
.	
4.5.8	
Pipeline	Control	Logic
a
.	
Desired	Handling	of	Special	Control	Cases
b
.	
Detecting	Special	Control	Conditions
c
.	
Pipeline	Control	Mechanisms
d
.	
Combinations	of	Control	Conditions
a
.	
Practice	Problem	
4.37	
(solution	page	492)
b
.	
Practice	Problem	
4.38	
(solution	page	492)
e
.	
Control	Logic	Implementation
a
.	
Practice	Problem	
4.39	
(solution	page	493)
b
.	
Practice	Problem	
4.40	
(solution	page	493)
c
.	
Practice	Problem	
4.41	
(solution	page	493)
d
.	
Practice	Problem	
4.42	
(solution	page	493)
i
.	
4.5.9	
Performance	Analysis
a
.	
Practice	Problem	
4.43	
(solution	page	494)
b
.	
Practice	Problem	
4.44	
(solution	page	494)
j
.	
4.5.10	
Unfinished	Business
a
.	
Multicycle	Instructions
b
.	
Interfacing	with	the	Memory	System</p>
<p>6
.	
4.6	
Summary
a
.	
4.6.1	
Y86-64	Simulators
7
.	
Bibliographic	Notes
8
.	
Homework	Problems
a
.	
4.45
b
.	
4.46
c
.	
4.47
d
.	
4.48
e
.	
4.49
f
.	
4.50
g
.	
4.51
h
.	
4.52
i
.	
4.53
j
.	
4.54
k
.	
4.55
l
.	
4.56
m
.	
4.57
n
.	
4.58
o
.	
4.59
D
.	
Chapter	
5	
Optimizing	Program	Performance
1
.	
5.1	
Capabilities	and	Limitations	of	Optimizing	Compilers
a
.	
Practice	Problem	
5.1	
(solution	page	573)
2
.	
5.2	
Expressing	Program	Performance
a
.	
Practice	Problem	
5.2	
(solution	page	573)
3
.	
5.3	
Program	Example</p>
<p>4
.	
5.4	
Eliminating	Loop	Inefficiencies
a
.	
Practice	Problem	
5.3	
(solution	page	573)
5
.	
5.5	
Reducing	Procedure	Calls
6
.	
5.6	
Eliminating	Unneeded	Memory	References
a
.	
Practice	Problem	
5.4	
(solution	page	574)
7
.	
5.7	
Understanding	Modern	Processors
a
.	
5.7.1	
Overall	Operation
b
.	
5.7.2	
Functional	Unit	Performance
c
.	
5.7.3	
An	Abstract	Model	of	Processor	Operation
a
.	
From	Machine-Level	Code	to	Data-Flow	Graphs
b
.	
Other	Performance	Factors
a
.	
Practice	Problem	
5.5	
(solution	page	575)
b
.	
Practice	Problem	
5.6	
(solution	page	575)
8
.	
5.8	
Loop	Unrolling
a
.	
Practice	Problem	
5.7	
(solution	page	575)
9
.	
5.9	
Enhancing	Parallelism
a
.	
5.9.1	
Multiple	Accumulators
b
.	
5.9.2	
Reassociation	Transformation
a
.	
Practice	Problem	
5.8	
(solution	page	576)
10
.	
5.10	
Summary	of	Results	for	Optimizing	Combining	Code
11
.	
5.11	
Some	Limiting	Factors
a
.	
5.11.1	
Register	Spilling
b
.	
5.11.2	
Branch	Prediction	and	Misprediction	Penalties
a
.	
Do	Not	Be	Overly	Concerned	about	Predictable
Branches</p>
<p>b
.	
Write	Code	Suitable	for	Implementation	with
Conditional	Moves
a
.	
Practice	Problem	
5.9	
(solution	page	576)
12
.	
5.12	
Understanding	Memory	Performance
a
.	
5.12.1	
Load	Performance
b
.	
5.12.2	
Store	Performance
a
.	
Practice	Problem	
5.10	
(solution	page	577)
b
.	
Practice	Problem	
5.11	
(solution	page	577)
c
.	
Practice	Problem	
5.12	
(solution	page	577)
13
.	
5.13	
Life	in	the	Real	World:	Performance	Improvement
Techniques
14
.	
5.14	
Identifying	and	Eliminating	Performance	Bottlenecks</p>
<p>1
.	
A
.	
1
.	
a
.	
5.14.1	
Program	Profiling
b
.	
5.14.2	
Using	a	Profiler	to	Guide	Optimization
2
.	
5.15	
Summary
3
.	
Bibliographic	Notes
4
.	
Homework	Problems
a
.	
5.13	
♦♦
b
.	
5.14	
♦
c
.	
5.15	
♦
d
.	
5.16	
♦
e
.	
5.17	
♦♦
f
.	
5.18	
♦♦♦
g
.	
5.19	
♦♦♦
B
.	
Chapter	
6	
The	Memory	Hierarchy
1
.	
6.1	
Storage	Technologies
a
.	
6.1.1	
Random	Access	Memory
a
.	
Static	RAM
b
.	
Dynamic	RAM
c
.	
Conventional	DRAMs
d
.	
Memory	Modules
a
.	
Practice	Problem	
6.1	
(solution	page	660)
e
.	
Enhanced	DRAMs
f
.	
Nonvolatile	Memory
g
.	
Accessing	Main	Memory</p>
<p>b
.	
6.1.2	
Disk	Storage
a
.	
Disk	Geometry
b
.	
Disk	Capacity
a
.	
Practice	Problem	
6.2	
(solution	page	661)
c
.	
Disk	Operation
a
.	
Practice	Problem	
6.3	
(solution	page	661)
d
.	
Logical	Disk	Blocks
a
.	
Practice	Problem	
6.4	
(solution	page	661)
e
.	
Connecting	I/O	Devices
f
.	
Accessing	Disks
c
.	
6.1.3	
Solid	State	Disks
a
.	
Practice	Problem	
6.5	
(solution	page	662)
d
.	
6.1.4	
Storage	Technology	Trends
a
.	
Practice	Problem	
6.6	
(solution	page	662)
2
.	
6.2	
Locality
a
.	
6.2.1	
Locality	of	References	to	Program	Data
b
.	
6.2.2	
Locality	of	Instruction	Fetches
c
.	
6.2.3	
Summary	of	Locality
a
.	
Practice	Problem	
6.7	
(solution	page	662)
b
.	
Practice	Problem	
6.8	
(solution	page	663)
3
.	
6.3	
The	Memory	Hierarchy
a
.	
6.3.1	
Caching	in	the	Memory	Hierarchy
a
.	
Cache	Hits</p>
<p>b
.	
Cache	Misses
c
.	
Kinds	of	Cache	Misses
d
.	
Cache	Management
b
.	
6.3.2	
Summary	of	Memory	Hierarchy	Concepts
4
.	
6.4	
Cache	Memories
a
.	
6.4.1	
Generic	Cache	Memory	Organization
a
.	
Practice	Problem	
6.9	
(solution	page	663)
b
.	
6.4.2	
Direct-Mapped	Caches
a
.	
Set	Selection	in	Direct-Mapped	Caches
b
.	
Line	Matching	in	Direct-Mapped	Caches
c
.	
Word	Selection	in	Direct-Mapped	Caches
d
.	
Line	Replacement	on	Misses	in	Direct-Mapped
Caches
e
.	
Putting	It	Together:	A	Direct-Mapped	Cache	in
Action
f
.	
Conflict	Misses	in	Direct-Mapped	Caches
a
.	
Practice	Problem	
6.10	
(solution	page	663)
b
.	
Practice	Problem	
6.11	
(solution	page	663)
c
.	
6.4.3	
Set	Associative	Caches
a
.	
Set	Selection	in	Set	Associative	Caches
b
.	
Line	Matching	and	Word	Selection	in	Set
Associative	Caches
c
.	
Line	Replacement	on	Misses	in	Set	Associative
Caches
d
.	
6.4.4	
Fully	Associative	Caches</p>
<p>a
.	
Set	Selection	in	Fully	Associative	Caches
b
.	
Line	Matching	and	Word	Selection	in	Fully
Associative	Caches
a
.	
Practice	Problem	
6.12	
(solution	page	663)
b
.	
Practice	Problem	
6.13	
(solution	page	664)
c
.	
Practice	Problem	
6.14	
(solution	page	664)
d
.	
Practice	Problem	
6.15	
(solution	page	664)
e
.	
Practice	Problem	
6.16	
(solution	page	665)
e
.	
6.4.5	
Issues	with	Writes
f
.	
6.4.6	
Anatomy	of	a	Real	Cache	Hierarchy
g
.	
6.4.7	
Performance	Impact	of	Cache	Parameters
a
.	
Impact	of	Cache	Size
b
.	
Impact	of	Block	Size
c
.	
Impact	of	Associativity
d
.	
Impact	of	Write	Strategy
5
.	
6.5	
Writing	Cache-Friendly	Code
a
.	
Practice	Problem	
6.17	
(solution	page	665)
b
.	
Practice	Problem	
6.18	
(solution	page	666)
c
.	
Practice	Problem	
6.19	
(solution	page	666)
d
.	
Practice	Problem	
6.20	
(solution	page	666)
6
.	
6.6	
Putting	It	Together:	The	Impact	of	Caches	on	Program
Performance
a
.	
6.6.1	
The	Memory	Mountain
a
.	
Practice	Problem	
6.21	
(solution	page	666)
b
.	
6.6.2	
Rearranging	Loops	to	Increase	Spatial	Locality
c
.	
6.6.3	
Exploiting	Locality	in	Your	Programs</p>
<p>7
.	
6.7	
Summary
8
.	
Bibliographic	Notes
9
.	
Homework	Problems
a
.	
6.22
b
.	
6.23
c
.	
6.24
d
.	
6.25
e
.	
6.26
f
.	
6.27
g
.	
6.28
h
.	
6.29
i
.	
6.30
j
.	
6.31
k
.	
6.32
l
.	
6.33
m
.	
6.34
n
.	
6.35
o
.	
6.36
p
.	
6.37
q
.	
6.38
r
.	
6.39
s
.	
6.40
t
.	
6.41
u
.	
6.42
v
.	
6.43
w
.	
6.44
x
.	
6.45
y
.	
6.46</p>
<p>2
.	
Part	
II	
Running	Programs	on	a	System
A
.	
Chapter	
7	
Linking
1
.	
7.1	
Compiler	Drivers
2
.	
7.2	
Static	Linking
3
.	
7.3	
Object	Files
4
.	
7.4	
Relocatable	Object	Files
5
.	
7.5	
Symbols	and	Symbol	Tables
a
.	
Practice	Problem	
7.1	
(solution	page	717)
6
.	
7.6	
Symbol	Resolution
a
.	
7.6.1	
How	Linkers	Resolve	Duplicate	Symbol	Names
a
.	
Practice	Problem	
7.2	
(solution	page	718)
b
.	
7.6.2	
Linking	with	Static	Libraries
c
.	
7.6.3	
How	Linkers	Use	Static	Libraries	to	Resolve
References
a
.	
Practice	Problem	
7.3	
(solution	page	718)
7
.	
7.7	
Relocation
a
.	
7.7.1	
Relocation	Entries
b
.	
7.7.2	
Relocating	Symbol	References
a
.	
Relocating	PC-Relative	References
b
.	
Relocating	Absolute	References
a
.	
Practice	Problem	
7.4	
(solution	page	718)
b
.	
Practice	Problem	
7.5	
(solution	page	718)
8
.	
7.8	
Executable	Object	Files
9
.	
7.9	
Loading	Executable	Object	Files
10
.	
7.10	
Dynamic	Linking	with	Shared	Libraries
11
.	
7.11	
Loading	and	Linking	Shared	Libraries	from</p>
<p>Applications
12
.	
7.12	
Position-Independent	Code	(PIC)
13
.	
7.13	
Library	Interpositioning
a
.	
7.13.1	
Compile-Time	Interpositioning
b
.	
7.13.2	
Link-Time	Interpositioning
c
.	
7.13.3	
Run-Time	Interpositioning
14
.	
7.14	
Tools	for	Manipulating	Object	Files
15
.	
7.15	
Summary
16
.	
Bibliographic	Notes
17
.	
Homework	Problems
a
.	
7.6	
♦
b
.	
7.7	
♦
c
.	
7.8	
♦
d
.	
7.9	
♦
e
.	
7.10	
♦♦
f
.	
7.11	
♦♦
g
.	
7.12	
♦♦
h
.	
7.13	
♦♦
B
.	
Chapter	
8	
Exceptional	Control	Flow
1
.	
8.1	
Exceptions
a
.	
8.1.1	
Exception	Handling
b
.	
8.1.2	
Classes	of	Exceptions
a
.	
Interrupts
b
.	
Traps	and	System	Calls
c
.	
Faults
d
.	
Aborts
c
.	
8.1.3	
Exceptions	in	Linux/x86-64	Systems</p>
<p>a
.	
Linux/x86-64	Faults	and	Aborts
b
.	
Linux/x86-64	System	Calls
2
.	
8.2	
Processes
a
.	
8.2.1	
Logical	Control	Flow
b
.	
8.2.2	
Concurrent	Flows
a
.	
Practice	Problem	
8.1	
(solution	page	795)
c
.	
8.2.3	
Private	Address	Space
d
.	
8.2.4	
User	and	Kernel	Modes
e
.	
8.2.5	
Context	Switches
3
.	
8.3	
System	Call	Error	Handling
4
.	
8.4	
Process	Control
a
.	
8.4.1	
Obtaining	Process	IDs
b
.	
8.4.2	
Creating	and	Terminating	Processes
a
.	
Practice	Problem	
8.2	
(solution	page	795)
c
.	
8.4.3	
Reaping	Child	Processes
a
.	
Determining	the	Members	of	the	Wait	Set
b
.	
Modifying	the	Default	Behavior
c
.	
Checking	the	Exit	Status	of	a	Reaped	Child
d
.	
Error	Conditions
a
.	
Practice	Problem	
8.3	
(solution	page	797)
e
.	
The	
Function
f
.	
Examples	of	Using	
a
.	
Practice	Problem	
8.4	
(solution	page	797)</p>
<p>d
.	
8.4.4	
Putting	Processes	to	Sleep
a
.	
Practice	Problem	
8.5	
(solution	page	797)
e
.	
8.4.5	
Loading	and	Running	Programs
a
.	
Practice	Problem	
8.6	
(solution	page	797)
f
.	
8.4.6	
Using	
and	
to	Run	Programs
5
.	
8.5	
Signals
a
.	
8.5.1	
Signal	Terminology
b
.	
8.5.2	
Sending	Signals
a
.	
Process	Groups
b
.	
Sending	Signals	with	the	
Program
c
.	
Sending	Signals	from	the	Keyboard
d
.	
Sending	Signals	with	the	
Function
e
.	
Sending	Signals	with	the	
Function
c
.	
8.5.3	
Receiving	Signals
a
.	
Practice	Problem	
8.7	
(solution	page	798)
d
.	
8.5.4	
Blocking	and	Unblocking	Signals
e
.	
8.5.5	
Writing	Signal	Handlers
a
.	
Safe	Signal	Handling
b
.	
Correct	Signal	Handling
a
.	
Practice	Problem	
8.8	
(solution	page	799)
c
.	
Portable	Signal	Handling
f
.	
8.5.6	
Synchronizing	Flows	to	Avoid	Nasty	Concurrency</p>
<p>Bugs
g
.	
8.5.7	
Explicitly	Waiting	for	Signals
6
.	
8.6	
Nonlocal	Jumps
7
.	
8.7	
Tools	for	Manipulating	Processes
8
.	
8.8	
Summary
9
.	
Bibliographic	Notes
10
.	
Homework	Problems
a
.	
8.9	
♦
b
.	
8.10	
♦
c
.	
8.11	
♦
d
.	
8.12	
♦
e
.	
8.13	
♦
f
.	
8.14	
♦
g
.	
8.15	
♦
h
.	
8.16	
♦
i
.	
8.17	
♦
j
.	
8.18	
♦♦
k
.	
8.19	
♦♦
l
.	
8.20	
♦♦
m
.	
8.21	
♦♦
n
.	
8.22	
♦♦♦
o
.	
8.23	
♦♦
p
.	
8.24	
♦♦♦
q
.	
8.25	
♦♦♦
r
.	
8.26	
♦♦♦♦
C
.	
Chapter	
9	
Virtual	Memory
1
.	
9.1	
Physical	and	Virtual	Addressing
2
.	
9.2	
Address	Spaces</p>
<p>a
.	
Practice	Problem	
9.1	
(solution	page	880)
3
.	
9.3	
VM	as	a	Tool	for	Caching
a
.	
9.3.1	
DRAM	Cache	Organization
b
.	
9.3.2	
Page	Tables
a
.	
Practice	Problem	
9.2	
(solution	page	881)
c
.	
9.3.3	
Page	Hits
d
.	
9.3.4	
Page	Faults
e
.	
9.3.5	
Allocating	Pages
f
.	
9.3.6	
Locality	to	the	Rescue	Again
4
.	
9.4	
VM	as	a	Tool	for	Memory	Management
5
.	
9.5	
VM	as	a	Tool	for	Memory	Protection
6
.	
9.6	
Address	Translation
a
.	
Practice	Problem	
9.3	
(solution	page	881)
b
.	
9.6.1	
Integrating	Caches	and	VM
c
.	
9.6.2	
Speeding	Up	Address	Translation	with	a	TLB
d
.	
9.6.3	
Multi-Level	Page	Tables
e
.	
9.6.4	
Putting	It	Together:	End-to-End	Address
Translation
a
.	
Practice	Problem	
9.4	
(solution	page	881)
7
.	
9.7	
Case	Study:	The	Intel	Core	i7/Linux	Memory	System
a
.	
9.7.1	
Core	i7	Address	Translation
b
.	
9.7.2	
Linux	Virtual	Memory	System
a
.	
Linux	Virtual	Memory	Areas
b
.	
Linux	Page	Fault	Exception	Handling
8
.	
9.8	
Memory	Mapping</p>
<p>a
.	
9.8.1	
Shared	Objects	Revisited
b
.	
9.8.2	
The	
Function	Revisited
c
.	
9.8.3	
The	execve	Function	Revisited
d
.	
9.8.4	
User-Level	Memory	Mapping	with	the	
Function
a
.	
Practice	Problem	
9.5	
(solution	page	882)
9
.	
9.9	
Dynamic	Memory	Allocation
a
.	
9.9.1	
The	
and	
Functions
b
.	
9.9.2	
Why	Dynamic	Memory	Allocation?
c
.	
9.9.3	
Allocator	Requirements	and	Goals
d
.	
9.9.4	
Fragmentation
e
.	
9.9.5	
Implementation	Issues
f
.	
9.9.6	
Implicit	Free	Lists
a
.	
Practice	Problem	
9.6	
(solution	page	883)
g
.	
9.9.7	
Placing	Allocated	Blocks
h
.	
9.9.8	
Splitting	Free	Blocks
i
.	
9.9.9	
Getting	Additional	Heap	Memory
j
.	
9.9.10	
Coalescing	Free	Blocks
k
.	
9.9.11	
Coalescing	with	Boundary	Tags
a
.	
Practice	Problem	
9.7	
(solution	page	883)
l
.	
9.9.12	
Putting	It	Together:	Implementing	a	Simple
Allocator
a
.	
General	Allocator	Design
b
.	
Basic	Constants	and	Macros	for	Manipulating	the
Free	List
c
.	
Creating	the	Initial	Free	List</p>
<p>d
.	
Freeing	and	Coalescing	Blocks
e
.	
Allocating	Blocks
a
.	
Practice	Problem	
9.8	
(solution	page	884)
b
.	
Practice	Problem	
9.9	
(solution	page	884)
m
.	
9.9.13	
Explicit	Free	Lists
n
.	
9.9.14	
Segregated	Free	Lists
a
.	
Simple	Segregated	Storage
a
.	
Practice	Problem	
9.10	
(solution	page	885)
b
.	
Segregated	Fits
c
.	
Buddy	Systems
10
.	
9.10	
Garbage	Collection
a
.	
9.10.1	
Garbage	Collector	Basics
b
.	
9.10.2	
Mark&amp;Sweep	Garbage	Collectors
c
.	
9.10.3	
Conservative	Mark&amp;Sweep	for	C	Programs
11
.	
9.11	
Common	Memory-Related	Bugs	in	C	Programs
a
.	
9.11.1	
Dereferencing	Bad	Pointers
b
.	
9.11.2	
Reading	Uninitialized	Memory
c
.	
9.11.3	
Allowing	Stack	Buffer	Overflows
d
.	
9.11.4	
Assuming	That	Pointers	and	the	Objects	They
Point	to	Are	the	Same	Size
e
.	
9.11.5	
Making	Off-by-One	Errors
f
.	
9.11.6	
Referencing	a	Pointer	Instead	of	the	Object	It
Points	To
g
.	
9.11.7	
Misunderstanding	Pointer	Arithmetic
h
.	
9.11.8	
Referencing	Nonexistent	Variables
i
.	
9.11.9	
Referencing	Data	in	Free	Heap	Blocks</p>
<p>j
.	
9.11.10	
Introducing	Memory	Leaks
12
.	
9.12	
Summary
13
.	
Bibliographic	Notes
14
.	
Homework	Problems
a
.	
9.11
b
.	
9.12
c
.	
9.13
d
.	
9.14
e
.	
9.15
f
.	
9.16
g
.	
9.17
h
.	
9.18
i
.	
9.19
j
.	
9.20
3
.	
Part	
III	
Interaction	and	Communication	between	Programs
A
.	
Chapter	
10	
System-Level	I/O
1
.	
10.1	
Unix	I/O
2
.	
10.2	
Files
3
.	
10.3	
Opening	and	Closing	Files
a
.	
Practice	Problem	
10.1	
(solution	page	915)
4
.	
10.4	
Reading	and	Writing	Files
5
.	
10.5	
Robust	Reading	and	Writing	with	the	R
IO
Package
a
.	
10.5.1	
Unbuffered	Input	and	Output	Functions
b
.	
10.5.2	
Buffered	Input	Functions
6
.	
10.6	
Reading	File	Metadata</p>
<p>7
.	
10.7	
Reading	Directory	Contents
8
.	
10.8	
Sharing	Files
a
.	
Practice	Problem	
10.2	
(solution	page	915)
b
.	
Practice	Problem	
10.3	
(solution	page	915)
9
.	
10.9	
I/O	Redirection
a
.	
Practice	Problem	
10.4	
(solution	page	915)
b
.	
Practice	Problem	
10.5	
(solution	page	916)
10
.	
10.10	
Standard	I/O
11
.	
10.11	
Putting	It	Together:	Which	I/O	Functions	Should	I
Use?
12
.	
10.12	
Summary
13
.	
Bibliographic	Notes
14
.	
Homework	Problems
a
.	
10.6
b
.	
10.7
c
.	
10.8
d
.	
10.9
e
.	
10.10
B
.	
Chapter	
11	
Network	Programming
1
.	
11.1	
The	Client-Server	Programming	Model
2
.	
11.2	
Networks
3
.	
11.3	
The	Global	IP	Internet
a
.	
11.3.1	
IP	Addresses
a
.	
Practice	Problem	
11.1	
(solution	page	966)
b
.	
Practice	Problem	
11.2	
(solution	page	967)
c
.	
Practice	Problem	
11.3	
(solution	page	967)</p>
<p>b
.	
11.3.2	
Internet	Domain	Names
c
.	
11.3.3	
Internet	Connections
4
.	
11.4	
The	Sockets	Interface
a
.	
11.4.1	
Socket	Address	Structures
b
.	
11.4.2	
The	
Function
c
.	
11.4.3	
The	
Function
d
.	
11.4.4	
The	
Function
e
.	
11.4.5	
The	
Function
f
.	
11.4.6	
The	
Function
g
.	
11.4.7	
Host	and	Service	Conversion
a
.	
The	
Function
b
.	
The	
Function
a
.	
Practice	Problem	
11.4	
(solution	page	968)
h
.	
11.4.8	
Helper	Functions	for	the	Sockets	Interface
a
.	
The	
Function
b
.	
The	
Function
i
.	
11.4.9	
Example	Echo	Client	and	Server
5
.	
11.5	
Web	Servers
a
.	
11.5.1	
Web	Basics
b
.	
11.5.2	
Web	Content
c
.	
11.5.3	
HTTP	Transactions
a
.	
HTTP	Requests
b
.	
HTTP	Responses
d
.	
11.5.4	
Serving	Dynamic	Content</p>
<p>a
.	
How	Does	the	Client	Pass	Program	Arguments	to
the	Server?
b
.	
How	Does	the	Server	Pass	Arguments	to	the	Child?
c
.	
How	Does	the	Server	Pass	Other	Information	to	the
Child?
d
.	
Where	Does	the	Child	Send	Its	Output?
a
.	
Practice	Problem	
11.5	
(solution	page	969)
6
.	
11.6	
Putting	It	Together:	The	T
INY</p>
<p>Web	Server
7
.	
11.7	
Summary
8
.	
Bibliographic	Notes
9
.	
Homework	Problems
a
.	
11.6
b
.	
11.7
c
.	
11.8
d
.	
11.9
e
.	
11.10
f
.	
11.11
g
.	
11.12
h
.	
11.13
C
.	
Chapter	
12	
Concurrent	Programming
1
.	
12.1	
Concurrent	Programming	with	Processes
a
.	
12.1.1	
A	Concurrent	Server	Based	on	Processes
b
.	
12.1.2	
Pros	and	Cons	of	Processes
a
.	
Practice	Problem	
12.1	
(solution	page	1036)
b
.	
Practice	Problem	
12.2	
(solution	page	1036)
2
.	
12.2	
Concurrent	Programming	with	I/O	Multiplexing
a
.	
Practice	Problem	
12.3	
(solution	page	1036)</p>
<p>b
.	
12.2.1	
A	Concurrent	Event-Driven	Server	Based	on	I/O
Multiplexing
a
.	
Practice	Problem	
12.4	
(solution	page	1036)
c
.	
12.2.2	
Pros	and	Cons	of	I/O	Multiplexing
3
.	
12.3	
Concurrent	Programming	with	Threads
a
.	
12.3.1	
Thread	Execution	Model
b
.	
12.3.2	
Posix	Threads
c
.	
12.3.3	
Creating	Threads
d
.	
12.3.4	
Terminating	Threads
e
.	
12.3.5	
Reaping	Terminated	Threads
f
.	
12.3.6	
Detaching	Threads
g
.	
12.3.7	
Initializing	Threads
h
.	
12.3.8	
A	Concurrent	Server	Based	on	Threads
a
.	
Practice	Problem	
12.5	
(solution	page	1036)
4
.	
12.4	
Shared	Variables	in	Threaded	Programs
a
.	
12.4.1	
Threads	Memory	Model
b
.	
12.4.2	
Mapping	Variables	to	Memory
c
.	
12.4.3	
Shared	Variables
a
.	
Practice	Problem	
12.6	
(solution	page	1036)
5
.	
12.5	
Synchronizing	Threads	with	Semaphores
a
.	
Practice	Problem	
12.7	
(solution	page	1037)
b
.	
12.5.1	
Progress	Graphs
a
.	
Practice	Problem	
12.8	
(solution	page	1038)
c
.	
12.5.2	
Semaphores
d
.	
12.5.3	
Using	Semaphores	for	Mutual	Exclusion</p>
<p>e
.	
12.5.4	
Using	Semaphores	to	Schedule	Shared
Resources
a
.	
Producer-Consumer	Problem
a
.	
Practice	Problem	
12.9	
(solution	page	1038)
b
.	
Readers-Writers	Problem
a
.	
Practice	Problem	
12.10	
(solution	page	1038)
f
.	
12.5.5	
Putting	It	Together:	A	Concurrent	Server	Based
on	Prethreading
6
.	
12.6	
Using	Threads	for	Parallelism
7
.	
12.7	
Other	Concurrency	Issues
a
.	
12.7.1	
Thread	Safety
b
.	
12.7.2	
Reentrancy
a
.	
Practice	Problem	
12.12	
(solution	page	1038)
c
.	
12.7.3	
Using	Existing	Library	Functions	in	Threaded
Programs
d
.	
12.7.4	
Races
a
.	
Practice	Problem	
12.13	
(solution	page	1039)
b
.	
Practice	Problem	
12.14	
(solution	page	1039)
e
.	
12.7.5	
Deadlocks
a
.	
Practice	Problem	
12.15	
(solution	page	1039)
8
.	
12.8	
Summary
9
.	
Bibliographic	Notes
10
.	
Homework	Problems
a
.	
12.16	
♦</p>
<p>b
.	
12.17	
♦
c
.	
12.18
d
.	
12.19	
♦♦
e
.	
12.20	
♦♦♦
f
.	
12.21	
♦♦♦♦
g
.	
12.22	
♦♦
h
.	
12.23	
♦♦
i
.	
12.24	
♦
j
.	
12.25	
♦
k
.	
12.26	
♦♦♦
l
.	
12.27	
♦♦
m
.	
12.28	
♦
n
.	
12.29	
♦
o
.	
12.30	
♦
p
.	
12.31	
♦♦♦
q
.	
12.32	
♦♦♦
r
.	
12.33	
♦♦♦
s
.	
12.34	
♦♦♦
t
.	
12.35	
♦♦♦
u
.	
12.36	
♦♦♦
v
.	
12.37	
♦♦♦
w
.	
12.38	
♦♦♦♦
x
.	
12.39	
♦♦♦♦
4
.	
Appendix	
A	
Error	Handling
A
.	
A.1	
Error	Handling	in	Unix	Systems
B
.	
A.2	
Error-Handling	Wrappers
5
.	
References
6
.	
Index</p>
<p>List	of	Illustrations
1
.	
Figure	1	A	typical	code	example.
2
.	
Figure	2	Five	systems	courses	based	on	the	CS:APP	book.
3
.	
Figure	1.1	The	hello	program.
4
.	
Figure	1.2	The	ASCII	text	representation	of	hello.c.
5
.	
Figure	1.3	The	compilation	system.
6
.	
Figure	1.4	Hardware	organization	of	a	typical	system.
7
.	
Figure	1.5	Reading	the	hello	command	from	the	keyboard.
8
.	
Figure	1.6	Loading	the	executable	from	disk	into	main
memory.
9
.	
Figure	1.7	Writing	the	output	string	from	memory	to	the
display.
10
.	
Figure	1.8	Cache	memories.
11
.	
Figure	1.9	An	example	of	a	memory	hierarchy.
12
.	
Figure	1.10	Layered	view	of	a	computer	system.
13
.	
Figure	1.11	Abstractions	provided	by	an	operating	system.
14
.	
Figure	1.12	Process	context	switching.
15
.	
Figure	1.13	Process	virtual	address	space.
16
.	
Figure	1.14	A	network	is	another	I/O	device.
17
.	
Figure	1.15	Using	telnet	to	run	hello	remotely	over	a	network.
18
.	
Figure	1.16	Categorizing	different	processor	configurations.
19
.	
Figure	1.17	Multi-core	processor	organization.
20
.	
Figure	1.18	Some	abstractions	provided	by	a	computer
system.
21
.	
Figure	2.1	Specifying	different	versions	of	C	to	GCC.
22
.	
Figure	2.2	Hexadecimal	notation.</p>
<p>23
.	
Figure	2.3	Typical	sizes	(in	bytes)	of	basic	C	data	types.
24
.	
Figure	2.4	Code	to	print	the	byte	representation	of	program
objects.
25
.	
Figure	2.5	Byte	representation	examples.
26
.	
Figure	2.6	Byte	representations	of	different	data	values.
27
.	
Figure	2.7	Operations	of	Boolean	algebra.
28
.	
Figure	2.8	Terminology	for	integer	data	and	arithmetic
operations.
29
.	
Figure	2.9	Typical	ranges	for	C	integral	data	types	for	32-bit
programs.
30
.	
Figure	2.10	Typical	ranges	for	C	integral	data	types	for	64-bit
programs.
31
.	
Figure	2.11	Guaranteed	ranges	for	C	integral	data	types.
32
.	
Figure	2.12	Unsigned	number	examples	for
33
.	
Figure	2.13	Two's-complement	number	examples	for
34
.	
Figure	2.14	Important	numbers.
35
.	
Figure	2.15	Two's-complement	representations	of	12,345	and
–12,345,	and	unsigned	representation	of	53,191.
36
.	
Figure	2.16	Comparing	unsigned	and	two's-complement
representations	for
37
.	
Figure	2.17	Conversion	from	two's	complement	to	unsigned.
38
.	
Figure	2.18	Conversion	from	unsigned	to	two's	complement.
39
.	
Figure	2.19	Effects	of	C	promotion	rules.
40
.	
Figure	2.20	Examples	of	sign	extension	from	w	=	3	to	w	=	4.
41
.	
Figure	2.21	Integer	addition.
42
.	
Figure	2.22	Relation	between	integer	addition	and	unsigned
addition.
43
.	
Figure	2.23	Unsigned	addition.
44
.	
Figure	2.24	Relation	between	integer	and	two's-complement
addition.</p>
<p>45
.	
Figure	2.25	Two's-complement	addition	examples.
46
.	
Figure	2.26	Two's-complement	addition.
47
.	
Figure	2.27	Three-bit	unsigned	and	two's-complement
multiplication	examples.
48
.	
Figure	2.28	Dividing	unsigned	numbers	by	powers	of	2.
49
.	
Figure	2.29	Applying	arithmetic	right	shift.
50
.	
Figure	2.30	Dividing	two's-complement	numbers	by	powers	of
2.
51
.	
Figure	2.31	Fractional	binary	representation.
52
.	
Figure	2.32	Standard	floating-point	formats.
53
.	
Figure	2.33	Categories	of	single-precision	floating-point
values.
54
.	
Figure	2.34	Representable	values	for	6-bit	floating-point
format.
55
.	
Figure	2.35	Example	nonnegative	values	for	8-bit	floating-
point	format.
56
.	
Figure	2.36	Examples	of	nonnegative	floating-point	numbers.
57
.	
Figure	2.37	Illustration	of	rounding	modes	for	dollar	rounding.
58
.	
Figure	3.1	Sizes	of	C	data	types	in	x86-64.
59
.	
Figure	3.2	Integer	registers.
60
.	
Figure	3.3	Operand	forms.
61
.	
Figure	3.4	Simple	data	movement	instructions.
62
.	
Figure	3.5	Zero-extending	data	movement	instructions.
63
.	
Figure	3.6	Sign-extending	data	movement	instructions.
64
.	
Figure	3.7	C	and	assembly	code	for	exchange	routine.
65
.	
Figure	3.8	Push	and	pop	instructions.
66
.	
Figure	3.9	Illustration	of	stack	operation.
67
.	
Figure	3.10	Integer	arithmetic	operations.
68
.	
Figure	3.11	C	and	assembly	code	for	arithmetic	function.
69
.	
Figure	3.12	Special	arithmetic	operations.</p>
<p>70
.	
Figure	3.13	Comparison	and	test	instructions.
71
.	
Figure	3.14	The	set	instructions.
72
.	
Figure	3.15	The	jump	instructions.
73
.	
Figure	3.16	Compilation	of	conditional	statements.
74
.	
Figure	3.17	Compilation	of	conditional	statements	using
conditional	assignment.
75
.	
Figure	3.18	The	conditional	move	instructions.
76
.	
Figure	3.19	Code	for	do-while	version	of	factorial	program.
77
.	
Figure	3.20	C	and	assembly	code	for	while	version	of	factorial
using	jump-to-middle	translation.
78
.	
Figure	3.21	C	and	assembly	code	for	while	version	of	factorial
using	guarded-do	translation.
79
.	
Figure	3.22	Example	switch	statement	and	its	translation	into
extended	C.
80
.	
Figure	3.23	Assembly	code	for	switch	statement	example	in
Figure	3.22.
81
.	
Figure	3.24	Assembly	code	and	jump	table	for	Problem	3.31.
82
.	
Figure	3.25	General	stack	frame	structure.
83
.	
Figure	3.26	Illustration	of	call	and	ret	functions.
84
.	
Figure	3.27	Detailed	execution	of	program	involving
procedure	calls	and	returns.
85
.	
Figure	3.28	Registers	for	passing	function	arguments.
86
.	
Figure	3.29	Example	of	function	with	multiple	arguments	of
different	types.
87
.	
Figure	3.30	Stack	frame	structure	for	function	proc.
88
.	
Figure	3.31	Example	of	procedure	definition	and	call.
89
.	
Figure	3.32	Example	of	code	to	call	function	proc,	defined	in
Figure	3.29.
90
.	
Figure	3.33	Stack	frame	for	function	call_proc.
91
.	
Figure	3.34	Code	demonstrating	use	of	callee-saved	registers.</p>
<p>92
.	
Figure	3.35	Code	for	recursive	factorial	program.
93
.	
Figure	3.36	Elements	of	array	in	row-major	order.
94
.	
Figure	3.37	Original	and	optimized	code	to	compute	element	i,
k	of	matrix	product	for	fixed-length	arrays.
95
.	
Figure	3.38	Original	and	optimized	code	to	compute	element	i,
k	of	matrix	product	for	variable-size	arrays.
96
.	
Figure	3.39	Example	gdb	commands.
97
.	
Figure	3.40	Stack	organization	for	echo	function.
98
.	
Figure	3.41	C	and	disassembled	code	for	Practice	Problem
3.46.
99
.	
Figure	3.42	Stack	organization	for	echo	function	with	stack
protector	enabled.
100
.	
Figure	3.43	Function	requiring	the	use	of	a	frame	pointer.
101
.	
Figure	3.44	Stack	frame	structure	for	function	vframe.
102
.	
Figure	3.45	Media	registers.
103
.	
Figure	3.46	Floating-point	movement	instructions.
104
.	
Figure	3.47	Two-operand	floating-point	conversion
operations.
105
.	
Figure	3.48	Three-operand	floating-point	conversion
operations.
106
.	
Figure	3.49	Scalar	floating-point	arithmetic	operations.
107
.	
Figure	3.50	Bitwise	operations	on	packed	data.
108
.	
Figure	3.51	Illustration	of	conditional	branching	in	floating-
point	code.
109
.	
Figure	3.52	Assembly	code	for	Problem	3.62.
110
.	
Figure	3.53	Disassembled	code	for	Problem	3.63.
111
.	
Figure	3.54	Code	for	Problem	3.72.
112
.	
Figure	4.1	Y86-64	programmer-visible	state.
113
.	
Figure	4.2	Y86-64	instruction	set.
114
.	
Figure	4.3	Function	codes	for	Y86-64	instruction	set.</p>
<p>115
.	
Figure	4.4	Y86-64	program	register	identifiers.
116
.	
Figure	4.5	Y86-64	status	codes.
117
.	
Figure	4.6	Comparison	of	Y86-64	and	x86-64	assembly
programs.
118
.	
Figure	4.7	Sample	program	written	in	Y86-64	assembly	code.
119
.	
Figure	4.8	Output	of	yas	assembler.
120
.	
Figure	4.9	Logic	gate	types.
121
.	
Figure	4.10	Combinational	circuit	to	test	for	bit	equality.
122
.	
Figure	4.11	Single-bit	multiplexor	circuit.
123
.	
Figure	4.12	Word-level	equality	test	circuit.
124
.	
Figure	4.13	Word-level	multiplexor	circuit.
125
.	
Figure	4.14	Four-way	multiplexor.
126
.	
Figure	4.15	Arithmetic/logic	unit	(ALU).
127
.	
Figure	4.16	Register	operation.
128
.	
Figure	4.17	Sample	Y86-64	instruction	sequence.
129
.	
Figure	4.18	Computations	in	sequential	implementation	of
Y86-64	instructions	OPq,	rrmovq,	and	irmovq.
130
.	
Figure	4.19	Computations	in	sequential	implementation	of
Y86-64	instructions	rmmovq	and	mrmovq.
131
.	
Figure	4.20	Computations	in	sequential	implementation	of
Y86-64	instructions	pushq	and	popq.
132
.	
Figure	4.21	Computations	in	sequential	implementation	of
Y86-64	instructions	jXX,	call,	and	ret.
133
.	
Figure	4.22	Abstract	view	of	SEQ,	a	sequential
implementation.
134
.	
Figure	4.23	Hardware	structure	of	SEQ,	a	sequential
implementation.
135
.	
Figure	4.24	Identifying	the	different	computation	steps	in	the
sequential	implementation.
136
.	
Figure	4.25	Tracing	two	cycles	of	execution	by	SEQ.</p>
<p>137
.	
Figure	4.26	Constant	values	used	in	HCL	descriptions.
138
.	
Figure	4.27	SEQ	fetch	stage.
139
.	
Figure	4.28	SEQ	decode	and	write-back	stage.
140
.	
Figure	4.29	SEQ	execute	stage.
141
.	
Figure	4.30	SEQ	memory	stage.
142
.	
Figure	4.31	SEQ	PC	update	stage.
143
.	
Figure	4.32	Unpipelined	computation	hardware.
144
.	
Figure	4.33	Three-stage	pipelined	computation	hardware.
145
.	
Figure	4.34	Three-stage	pipeline	timing.
146
.	
Figure	4.35	One	clock	cycle	of	pipeline	operation.
147
.	
Figure	4.36	Limitations	of	pipelining	due	to	nonuniform	stage
delays.
148
.	
Figure	4.37	Limitations	of	pipelining	due	to	overhead.
149
.	
Figure	4.38	Limitations	of	pipelining	due	to	logical
dependencies.
150
.	
Figure	4.39	Shifting	the	timing	of	the	PC	computation.
151
.	
Figure	4.40	SEQ+	hardware	structure.
152
.	
Figure	4.41	Hardware	structure	of	PIPE—,	an	initial	pipelined
implementation.
153
.	
Figure	4.42	Example	of	instruction	flow	through	pipeline.
154
.	
Figure	4.43	Pipelined	execution	of	prog1	without	special
pipeline	control.
155
.	
Figure	4.44	Pipelined	execution	of	prog2	without	special
pipeline	control.
156
.	
Figure	4.45	Pipelined	execution	of	prog3	without	special
pipeline	control.
157
.	
Figure	4.46	Pipelined	execution	of	prog4	without	special
pipeline	control.
158
.	
Figure	4.47	Pipelined	execution	of	prog2	using	stalls.
159
.	
Figure	4.48	Pipelined	execution	of	prog4	using	stalls.</p>
<p>160
.	
Figure	4.49	Pipelined	execution	of	prog2	using	forwarding.
161
.	
Figure	4.50	Pipelined	execution	of	prog3	using	forwarding.
162
.	
Figure	4.51	Pipelined	execution	of	prog4	using	forwarding.
163
.	
Figure	4.52	Hardware	structure	of	PIPE,	our	final	pipelined
implementation.
164
.	
Figure	4.53	Example	of	load/use	data	hazard.
165
.	
Figure	4.54	Handling	a	load/use	hazard	by	stalling.
166
.	
Figure	4.55	Simplified	view	of	ret	instruction	processing.
167
.	
Figure	4.56	Processing	mispredicted	branch	instructions.
168
.	
Figure	4.57	PIPE	PC	selection	and	fetch	logic.
169
.	
Figure	4.58	PIPE	decode	and	write-back	stage	logic.
170
.	
Figure	4.59	Demonstration	of	forwarding	priority.
171
.	
Figure	4.60	PIPE	execute	stage	logic.
172
.	
Figure	4.61	PIPE	memory	stage	logic.
173
.	
Figure	4.62	Detailed	processing	of	the	ret	instruction.
174
.	
Figure	4.63	Processing	invalid	memory	reference	exception.
175
.	
Figure	4.64	Detection	conditions	for	pipeline	control	logic.
176
.	
Figure	4.65	Additional	pipeline	register	operations,
177
.	
Figure	4.66	Actions	for	pipeline	control	logic.
178
.	
Figure	4.67	Pipeline	states	for	special	control	conditions.
179
.	
Figure	4.68	PIPE	pipeline	control	logic.
180
.	
Figure	4.69	Switch	statements	can	be	translated	into	Y86-64
code.
181
.	
Figure	4.70	Execute	and	memory	stages	capable	of	load
forwarding.
182
.	
Figure	4.71	Solution	for	Problem	4.10.
183
.	
Figure	5.1	Prefix-sum	functions.
184
.	
Figure	5.2	Performance	of	prefix-sum	functions.
185
.	
Figure	5.3	Vector	abstract	data	type.
186
.	
Figure	5.4	Implementation	of	vector	abstract	data	type.</p>
<p>187
.	
Figure	5.5	Initial	implementation	of	combining	operation.
188
.	
Figure	5.6	Improving	the	efficiency	of	the	loop	test.
189
.	
Figure	5.7	Lowercase	conversion	routines.
190
.	
Figure	5.8	Comparative	performance	of	lowercase	conversion
routines.
191
.	
Figure	5.9	Eliminating	function	calls	within	the	loop.
192
.	
Figure	5.10	Accumulating	result	in	temporary.
193
.	
Figure	5.11	Block	diagram	of	an	out-of-order	processor.
194
.	
Figure	5.12	Latency,	issue	time,	and	capacity	characteristics
of	reference	machine	operations.
195
.	
Figure	5.13	Graphical	representation	of	inner-loop	code	for
combine4
196
.	
Figure	5.14	Abstracting	combine4	operations	as	a	data-flow
graph.
197
.	
Figure	5.15	Data-flow	representation	of	computation	by	n
iterations	of	the	inner	loop	of	combine4.
198
.	
Figure	5.16	Applying	2	×	1	loop	unrolling.
199
.	
Figure	5.17	CPE	performance	for	different	degrees	of	k	×	1
loop	unrolling.
200
.	
Figure	5.18	Graphical	representation	of	inner-loop	code	for
combine5.
201
.	
Figure	5.19	Abstracting	combine5	operations	as	a	data-flow
graph.
202
.	
Figure	5.20	Data-flow	representation	of	combine5	operating
on	a	vector	of	length	n.
203
.	
Figure	5.21	Applying	2	×	2	loop	unrolling.
204
.	
Figure	5.22	Graphical	representation	of	inner-loop	code	for
combine6.
205
.	
Figure	5.23	Abstracting	combine6	operations	as	a	data-flow
graph.</p>
<p>206
.	
Figure	5.24	Data-flow	representation	of	combine6	operating
on	a	vector	of	length	n.
207
.	
Figure	5.25	CPE	performance	of	k	×	k	loop	unrolling.
208
.	
Figure	5.26	Applying	2	×	1a	unrolling.
209
.	
Figure	5.27	Graphical	representation	of	inner-loop	code	for
combine7.
210
.	
Figure	5.28	Abstracting	combine7	operations	as	a	data-flow
graph.
211
.	
Figure	5.29	Data-flow	representation	of	combine7	operating
on	a	vector	of	length	n.
212
.	
Figure	5.30	CPE	performance	for	k	×	1a	loop	unrolling.
213
.	
Figure	5.31	Linked	list	function.
214
.	
Figure	5.32	Function	to	set	array	elements	to	0.
215
.	
Figure	5.33	Code	to	write	and	read	memory	locations,	along
with	illustrative	executions.
216
.	
Figure	5.34	Detail	of	load	and	store	units.
217
.	
Figure	5.35	Graphical	representation	of	inner-loop	code	for
write_read.
218
.	
Figure	5.36	Abstracting	the	operations	for	write_read.
219
.	
Figure	5.37	Data-flow	representation	of	function	write_read.
220
.	
Figure	5.38	Profile	results	for	different	versions	of	bigram-
frequency	counting	program.
221
.	
Figure	5.39	Data	dependencies	among	multiplication
operations	for	cases	in	Problem	5.8.
222
.	
Figure	6.1	Inverted	pendulum.
223
.	
Figure	6.2	Characteristics	of	DRAM	and	SRAM	memory.
224
.	
Figure	6.3	High-level	view	of	a	128-bit	16	×	8	DRAM	chip.
225
.	
Figure	6.4	Reading	the	contents	of	a	DRAM	supercell.
226
.	
Figure	6.5	Reading	the	contents	of	a	memory	module.
227
.	
Figure	6.6	Example	bus	structure	that	connects	the	CPU	and</p>
<p>main	memory.
228
.	
Figure	6.7	Memory	read	transaction	for	a	load	operation:
movq	A,	%rax.
229
.	
Figure	6.8	Memory	write	transaction	for	a	store	operation:
movq	%rax,	A.
230
.	
Figure	6.9	Disk	geometry.
231
.	
Figure	6.10	Disk	dynamics.
232
.	
Figure	6.11	Example	bus	structure	that	connects	the	CPU,
main	memory,	and	I/O	devices.
233
.	
Figure	6.12	Reading	a	disk	sector.
234
.	
Figure	6.13	Solid	state	disk	(SSD).
235
.	
Figure	6.14	Performance	characteristics	of	a	commercial	solid
state	disk.
236
.	
Figure	6.15	Storage	and	processing	technology	trends.
237
.	
Figure	6.16	The	gap	between	disk,	DRAM,	and	CPU	speeds.
238
.	
Figure	6.17	(a)	A	function	with	good	locality,	(b)	Reference
pattern	for	vector	v	(N	=	8).
239
.	
Figure	6.18	(a)	Another	function	with	good	locality,	(b)
Reference	pattern	for	array	a	(M	=	2,	N	=	3).
240
.	
Figure	6.19	(a)	A	function	with	poor	spatial	locality,	(b)
Reference	pattern	for	array	a	(M	=	2,	N	=	3).
241
.	
Figure	6.20	Code	examples	for	Practice	Problem	6.8.
242
.	
Figure	6.21	The	memory	hierarchy.
243
.	
Figure	6.22	The	basic	principle	of	caching	in	a	memory
hierarchy.
244
.	
Figure	6.23	The	ubiquity	of	caching	in	modern	computer
systems.
245
.	
Figure	6.24	Typical	bus	structure	for	cache	memories.
246
.	
Figure	6.25	General	organization	of	cache	(S,	E,	B,	m).
247
.	
Figure	6.26	Summary	of	cache	parameters.</p>
<p>248
.	
Figure	6.27	Direct-mapped	cache	(E	=	1).
249
.	
Figure	6.28	Set	selection	in	a	direct-mapped	cache.
250
.	
Figure	6.29	Line	matching	and	word	selection	in	a	direct-
mapped	cache.
251
.	
Figure	6.30	4-bit	address	space	for	example	direct-mapped
cache.
252
.	
Figure	6.31	Why	caches	index	with	the	middle	bits.
253
.	
Figure	6.32	Set	associative	cache	(1	&lt;	E	&lt;	C/B).
254
.	
Figure	6.33	Set	selection	in	a	set	associative	cache.
255
.	
Figure	6.34	Line	matching	and	word	selection	in	a	set
associative	cache.
256
.	
Figure	6.35	Fully	associative	cache	(E	=	C/B).
257
.	
Figure	6.36	Set	selection	in	a	fully	associative	cache.
258
.	
Figure	6.37	Line	matching	and	word	selection	in	a	fully
associative	cache.
259
.	
Figure	6.38	Intel	Core	i7	cache	hierarchy.
260
.	
Figure	6.39	Characteristics	of	the	Intel	Core	i7	cache
hierarchy.
261
.	
Figure	6.40	Functions	that	measure	and	compute	read
throughput.
262
.	
Figure	6.41	A	memory	mountain.
263
.	
Figure	6.42	Ridges	of	temporal	locality	in	the	memory
mountain.
264
.	
Figure	6.43	A	slope	of	spatial	locality.
265
.	
Figure	6.44	Six	versions	of	matrix	multiply.
266
.	
Figure	6.45	Analysis	of	matrix	multiply	inner	loops.
267
.	
Figure	6.46	Core	i7	matrix	multiply	performance.
268
.	
Figure	6.47	Functions	referenced	in	Problem	6.37.
269
.	
Figure	6.48	Figure	for	solution	to	Problem	6.17.
270
.	
Figure	7.1	Example	program	1.</p>
<p>271
.	
Figure	7.2	Static	linking.
272
.	
Figure	7.3	Typical	ELF	relocatable	object	file.
273
.	
Figure	7.4	ELF	symbol	table	entry.
274
.	
Figure	7.5	Example	program	for	Practice	Problem	7.1.
275
.	
Figure	7.6	Member	object	files	in	the	libvector	library.
276
.	
Figure	7.7	Example	program	2.
277
.	
Figure	7.8	Linking	with	static	libraries.
278
.	
Figure	7.9	ELF	relocation	entry.
279
.	
Figure	7.10	Relocation	algorithm.
280
.	
Figure	7.11	Code	and	relocation	entries	from	main.o.
281
.	
Figure	7.12	Relocated	.text	and	.data	sections	for	the
executable	file	prog.
282
.	
Figure	7.13	Typical	ELF	executable	object	file.
283
.	
Figure	7.14	Program	header	table	for	the	example	executable
prog.
284
.	
Figure	7.15	Linux	x86-64	run-time	memory	image.
285
.	
Figure	7.16	Dynamic	linking	with	shared	libraries.
286
.	
Figure	7.17	Example	program	3.
287
.	
Figure	7.18	Using	the	GOT	to	reference	a	global	variable.
288
.	
Figure	7.19	Using	the	PLT	and	GOT	to	call	external	functions.
289
.	
Figure	7.20	Compile-time	interpositioning	with	the	C
preprocessor.
290
.	
Figure	7.21	Link-time	interpositioning	with	the	--wrap	flag.
291
.	
Figure	7.22	Run-time	interpositioning	with	LD_PRELOAD.
292
.	
Figure	8.1	Anatomy	of	an	exception.
293
.	
Figure	8.2	Exception	table.
294
.	
Figure	8.3	Generating	the	address	of	an	exception	handler.
295
.	
Figure	8.4	Classes	of	exceptions.
296
.	
Figure	8.5	Interrupt	handling.
297
.	
Figure	8.6	Trap	handling.</p>
<p>298
.	
Figure	8.7	Fault	handling.
299
.	
Figure	8.8	Abort	handling.
300
.	
Figure	8.9	Examples	of	exceptions	in	x86-64	systems.
301
.	
Figure	8.10	Examples	of	popular	system	calls	in	Linux	x86-64
systems.
302
.	
Figure	8.11	Implementing	the	hello	program	directly	with
Linux	system	calls.
303
.	
Figure	8.12	Logical	control	flows.
304
.	
Figure	8.13	Process	address	space.
305
.	
Figure	8.14	Anatomy	of	a	process	context	switch.
306
.	
Figure	8.15	Using	fork	to	create	a	new	process.
307
.	
Figure	8.16	Process	graph	for	the	example	program	in	Figure
8.15.
308
.	
Figure	8.17	Process	graph	for	a	nested	fork.
309
.	
Figure	8.18	Using	the	waitpid	function	to	reap	zombie
children	in	no	particular	order.
310
.	
Figure	8.19	Using	waitpid	to	reap	zombie	children	in	the	order
they	were	created.
311
.	
Figure	8.20	Organization	of	an	argument	list.
312
.	
Figure	8.21	Organization	of	an	environment	variable	list.
313
.	
Figure	8.22	Typical	organization	of	the	user	stack	when	a	new
program	starts.
314
.	
Figure	8.23	The	main	routine	for	a	simple	shell	program.
315
.	
Figure	8.24	eval	evaluates	the	shell	command	line.
316
.	
Figure	8.25	parseline	parses	a	line	of	input	for	the	shell.
317
.	
Figure	8.26	Linux	signals.
318
.	
Figure	8.27	Signal	handling.
319
.	
Figure	8.28	Foreground	and	background	process	groups.
320
.	
Figure	8.29	Using	the	kill	function	to	send	a	signal	to	a	child.
321
.	
Figure	8.30	A	program	that	uses	a	signal	handler	to	catch	a</p>
<p>SIGINT	signal.
322
.	
Figure	8.31	Handlers	can	be	interrupted	by	other	handlers.
323
.	
Figure	8.32	Temporarily	blocking	a	signal	from	being
received.
324
.	
Figure	8.33	Async-signal-safe	functions.
325
.	
Figure	8.34	The	Sio	(Safe	I/O)	package	for	signal	handlers.
326
.	
Figure	8.35	A	safe	version	of	the	SICINT	handler	from	Figure
8.30.
327
.	
Figure	8.36	signal1.	This	program	is	flawed	because	it
assumes	that	signals	are	queued.
328
.	
Figure	8.37	signal2.	An	improved	version	of	Figure	8.36	that
correctly	accounts	for	the	fact	that	signals	are	not	queued.
329
.	
Figure	8.38	Signal.	A	wrapper	for	sigaction	that	provides
portable	signal	handling	on	Posix-compliant	systems.
330
.	
Figure	8.39	A	shell	program	with	a	subtle	synchronization
error.
331
.	
Figure	8.40	Using	sigprocmask	to	synchronize	processes.
332
.	
Figure	8.41	Waiting	for	a	signal	with	a	spin	loop.
333
.	
Figure	8.42	Waiting	for	a	signal	with	sigsuspend.
334
.	
Figure	8.43	Nonlocal	jump	example.
335
.	
Figure	8.44	A	program	that	uses	nonlocal	jumps	to	restart
itself	when	the	user	types	Ctrl+C.
336
.	
Figure	8.45	Counter	program	referenced	in	Problem	8.23.
337
.	
Figure	8.46	Sample	shell	session	for	Problem	8.26.
338
.	
Figure	8.47	Process	graph	for	Practice	Problem	8.2.
339
.	
Figure	8.48	Process	graph	for	Practice	Problem	8.3.
340
.	
Figure	8.49	Process	graph	for	Practice	Problem	8.4.
341
.	
Figure	9.1	A	system	that	uses	physical	addressing.
342
.	
Figure	9.2	A	system	that	uses	virtual	addressing.
343
.	
Figure	9.3	How	a	VM	system	uses	main	memory	as	a	cache.</p>
<p>344
.	
Figure	9.4	Page	table.
345
.	
Figure	9.5	VM	page	hit.
346
.	
Figure	9.6	VM	page	fault	(before).
347
.	
Figure	9.7	VM	page	fault	(after).
348
.	
Figure	9.8	Allocating	a	new	virtual	page.
349
.	
Figure	9.9	How	VM	provides	processes	with	separate	address
spaces.
350
.	
Figure	9.10	Using	VM	to	provide	page-level	memory
protection.
351
.	
Figure	9.11	Summary	of	address	translation	symbols.
352
.	
Figure	9.12	Address	translation	with	a	page	table.
353
.	
Figure	9.13	Operational	view	of	page	hits	and	page	faults.
354
.	
Figure	9.14	Integrating	VM	with	a	physically	addressed	cache.
355
.	
Figure	9.15	Components	of	a	virtual	address	that	are	used	to
access	the	TLB.
356
.	
Figure	9.16	Operational	view	of	a	TLB	hit	and	miss.
357
.	
Figure	9.17	A	two-level	page	table	hierarchy.
358
.	
Figure	9.18	Address	translation	with	a	k-level	page	table.
359
.	
Figure	9.19	Addressing	for	small	memory	system.
360
.	
Figure	9.20	TLB,	page	table,	and	cache	for	small	memory
system.
361
.	
Figure	9.21	The	Core	i7	memory	system.
362
.	
Figure	9.22	Summary	of	Core	i7	address	translation.
363
.	
Figure	9.23	Format	of	level	1,	level	2,	and	level	3	page	table
entries.
364
.	
Figure	9.24	Format	of	level	4	page	table	entries.
365
.	
Figure	9.25	Core	i7	page	table	translation.
366
.	
Figure	9.26	The	virtual	memory	of	a	Linux	process.
367
.	
Figure	9.27	How	Linux	organizes	virtual	memory.
368
.	
Figure	9.28	Linux	page	fault	handling.</p>
<p>369
.	
Figure	9.29	A	shared	object.
370
.	
Figure	9.30	A	private	copy-on-write	object.
371
.	
Figure	9.31	How	the	loader	maps	the	areas	of	the	user
address	space.
372
.	
Figure	9.32	Visual	interpretation	of	mmap	arguments.
373
.	
Figure	9.33	The	heap.
374
.	
Figure	9.34	Allocating	and	freeing	blocks	with	malloc	and
free.
375
.	
Figure	9.35	Format	of	a	simple	heap	block.
376
.	
Figure	9.36	Organizing	the	heap	with	an	implicit	free	list.
377
.	
Figure	9.37	Splitting	a	free	block	to	satisfy	a	three-word
allocation	request.
378
.	
Figure	9.38	An	example	of	false	fragmentation.
379
.	
Figure	9.39	Format	of	heap	block	that	uses	a	boundary	tag.
380
.	
Figure	9.40	Coalescing	with	boundary	tags.
381
.	
Figure	9.41	memlib.	c:	Memory	system	model.
382
.	
Figure	9.42	Invariant	form	of	the	implicit	free	list.
383
.	
Figure	9.43	Basic	constants	and	macros	for	manipulating	the
free	list.
384
.	
Figure	9.44	mm_init	creates	a	heap	with	an	initial	free	block.
385
.	
Figure	9.45	extend_heap	extends	the	heap	with	a	new	free
block.
386
.	
Figure	9.46	mm_free	frees	a	block	and	uses	boundary-tag
coalescing	to	merge	it	with	any	adjacent	free	blocks	in
constant	time.
387
.	
Figure	9.47	mm_malloc	allocates	a	block	from	the	free	list.
388
.	
Figure	9.48	Format	of	heap	blocks	that	use	doubly	linked	free
lists.
389
.	
Figure	9.49	A	garbage	collector's	view	of	memory	as	a
directed	graph.</p>
<p>390
.	
Figure	9.50	Integrating	a	conservative	garbage	collector	and	a
C	malloc	package.
391
.	
Figure	9.51	Pseudocode	for	the	mark	and	sweep	functions.
392
.	
Figure	9.52	Mark&amp;Sweep	example.
393
.	
Figure	9.53	Left	and	right	pointers	in	a	balanced	tree	of
allocated	blocks.
394
.	
Figure	10.1	Portion	of	the	Linux	directory	hierarchy.
395
.	
Figure	10.2	Access	permission	bits.
396
.	
Figure	10.3	Using	read	and	write	to	copy	standard	input	to
standard	output	1	byte	at	a	time.
397
.	
Figure	10.4	The	rio_readn	and	rio_writen	functions.
398
.	
Figure	10.5	Copying	a	text	file	from	standard	input	to
standard	output.
399
.	
Figure	10.6	A	read	buffer	of	type	rio_t	and	the	rio_readinitb
function	that	initializes	it.
400
.	
Figure	10.7	The	internal	rio_read	function.
401
.	
Figure	10.8	The	rio_readlineb	and	rio_readnb	functions.
402
.	
Figure	10.9	The	stat	structure.
403
.	
Figure	10.10	Querying	and	manipulating	a	file's	st_mode	bits.
404
.	
Figure	10.11	Reading	the	contents	of	a	directory.
405
.	
Figure	10.12	Typical	kernel	data	structures	for	open	files.
406
.	
Figure	10.13	File	sharing.
407
.	
Figure	10.14	How	a	child	process	inherits	the	parent's	open
files.
408
.	
Figure	10.15	Kernel	data	structures	after	redirecting	standard
output	by	calling	dup2(4,	1).
409
.	
Figure	10.16	Relationship	between	Unix	I/O,	standard	I/O,	and
Rio.
410
.	
Figure	11.1	A	client-server	transaction.
411
.	
Figure	11.2	Hardware	organization	of	a	network	host.</p>
<p>412
.	
Figure	11.3	Ethernet	segment.
413
.	
Figure	11.4	Bridged	Ethernet	segments.
414
.	
Figure	11.5	Conceptual	view	of	a	LAN.
415
.	
Figure	11.6	A	small	internet.
416
.	
Figure	11.7	How	data	travel	from	one	host	to	another	on	an
internet.
417
.	
Figure	11.8	Hardware	and	software	organization	of	an	Internet
application.
418
.	
Figure	11.9	IP	address	structure.
419
.	
Figure	11.10	Subset	of	the	Internet	domain	name	hierarchy.
420
.	
Figure	11.11	Anatomy	of	an	Internet	connection.
421
.	
Figure	11.12	Overview	of	network	applications	based	on	the
sockets	interface.
422
.	
Figure	11.13	Socket	address	structures.
423
.	
Figure	11.14	The	roles	of	the	listening	and	connected
descriptors.
424
.	
Figure	11.15	Data	structure	returned	by	getaddrinfo.
425
.	
Figure	11.16	The	addrinfo	structure	used	by	getaddrinfo.
426
.	
Figure	11.17	Hostinfo	displays	the	mapping	of	a	domain	name
to	its	associated	IP	addresses.
427
.	
Figure	11.18	open_clientfd:	Helper	function	that	establishes	a
connection	with	a	server.
428
.	
Figure	11.19	open_listenfd:	Helper	function	that	opens	and
returns	a	listening	descriptor.
429
.	
Figure	11.20	Echo	client	main	routine.
430
.	
Figure	11.21	Iterative	echo	server	main	routine.
431
.	
Figure	11.22	echo	function	that	reads	and	echoes	text	lines.
432
.	
Figure	11.23	Example	MIME	types.
433
.	
Figure	11.24	Example	of	an	HTTP	transaction	that	serves
static	content.</p>
<p>434
.	
Figure	11.25	Some	HTTP	status	codes.
435
.	
Figure	11.26	Examples	of	CGI	environment	variables.
436
.	
Figure	11.27	CGI	program	that	sums	two	integers.
437
.	
Figure	11.28	An	HTTP	transaction	that	serves	dynamic	HTML
content.
438
.	
Figure	11.29	The	Tiny	Web	server.
439
.	
Figure	11.30	Tiny	doit	handles	one	HTTP	transaction.
440
.	
Figure	11.31	Tiny	clienterror	sends	an	error	message	to	the
client.
441
.	
Figure	11.32	Tiny	read_requesthdrs	reads	and	ignores
request	headers.
442
.	
Figure	11.33	Tiny	parse_uri	parses	an	HTTP	URI.
443
.	
Figure	11.34	Tiny	serve_static	serves	static	content	to	a
client.
444
.	
Figure	11.35	Tiny	serve_dynamic	serves	dynamic	content	to	a
client.
445
.	
Figure	12.1	Step	1:	Server	accepts	connection	request	from
client.
446
.	
Figure	12.2	Step	2:	Server	forks	a	child	process	to	service	the
client.
447
.	
Figure	12.3	Step	3:	Server	accepts	another	connection
request.
448
.	
Figure	12.4	Step	4:	Server	forks	another	child	to	service	the
new	client.
449
.	
Figure	12.5	Concurrent	echo	server	based	on	processes.
450
.	
Figure	12.6	An	iterative	echo	server	that	uses	I/O
multiplexing.
451
.	
Figure	12.7	State	machine	for	a	logical	flow	in	a	concurrent
event-driven	echo	server.
452
.	
Figure	12.8	Concurrent	echo	server	based	on	I/O</p>
<p>multiplexing.
453
.	
Figure	12.9	init_pool	initializes	the	pool	of	active	clients.
454
.	
Figure	12.10	add_client	adds	a	new	client	connection	to	the
pool.
455
.	
Figure	12.11	check_clients	services	ready	client	connections.
456
.	
Figure	12.12	Concurrent	thread	execution.
457
.	
Figure	12.13	hello.c:	The	Pthreads	&quot;Hello,	world!&quot;	program.
458
.	
Figure	12.14	Concurrent	echo	server	based	on	threads.
459
.	
Figure	12.15	Example	program	that	illustrates	different
aspects	of	sharing.
460
.	
Figure	12.16	badcnt.c:	An	improperly	synchronized	counter
program.
461
.	
Figure	12.17	Assembly	code	for	the	counter	loop	(lines	40−41)
in	badcnt.c.
462
.	
Figure	12.18	Instruction	orderings	for	the	first	loop	iteration	in
badcnt.c.
463
.	
Figure	12.19	Progress	graph	for	the	first	loop	iteration	of
badcnt.c.
464
.	
Figure	12.20	An	example	trajectory.
465
.	
Figure	12.21	Safe	and	unsafe	trajectories.
466
.	
Figure	12.22	Using	semaphores	for	mutual	exclusion.
467
.	
Figure	12.23	Producer-consumer	problem.
468
.	
Figure	12.24	sbuf_t:	Bounded	buffer	used	by	the	Sbuf
package.
469
.	
Figure	12.25	Sbuf:	A	package	for	synchronizing	concurrent
access	to	bounded	buffers.
470
.	
Figure	12.26	Solution	to	the	first	readers-writers	problem.
471
.	
Figure	12.27	Organization	of	a	prethreaded	concurrent	server.
472
.	
Figure	12.28	A	prethreaded	concurrent	echo	server.
473
.	
Figure	12.29	echo_cnt:	A	version	of	echo	that	counts	all	bytes</p>
<p>received	from	clients.
474
.	
Figure	12.30	Relationships	between	the	sets	of	sequential,
concurrent,	and	parallel	programs.
475
.	
Figure	12.31	Main	routine	for	psum-mutex.
476
.	
Figure	12.32	Thread	routine	for	psum-mutex.
477
.	
Figure	12.33	Thread	routine	for	psum-array.
478
.	
Figure	12.34	Thread	routine	for	psum-local.
479
.	
Figure	12.35	Performance	of	psum-local	(Figure	12.34).
480
.	
Figure	12.36	Speedup	and	parallel	efficiency	for	the	execution
times	in	Figure	12.35.
481
.	
Figure	12.37	A	thread-unsafe	pseudorandom	number
generator.
482
.	
Figure	12.38	Thread-safe	wrapper	function	for	the	C	standard
library	ctime	function.
483
.	
Figure	12.39	Relationships	between	the	sets	of	reentrant,
thread-safe,	and	thread-unsafe	functions.
484
.	
Figure	12.40	rand_r:	A	reentrant	version	of	the	rand	function
from	Figure	12.37.
485
.	
Figure	12.41	Common	thread-unsafe	library	functions.
486
.	
Figure	12.42	program	with	a	race.
487
.	
Figure	12.43
488
.	
Figure	12.44	Progress	graph	for	a	program	that	can	deadlock.
489
.	
Figure	12.45	Progress	graph	for	a	deadlock-free	program.
490
.	
Figure	12.46	Buggy	program	for	Problem	12.17.
491
.	
Figure	12.47	Driver	program	for	Problems	12.31−12.33.
492
.	
Figure	12.48	Progress	graph	for	a	program	that	deadlocks.
493
.	
Figure	12.49	Progress	graph	for	the	corrected	deadlock-free
program.
494
.	
Figure	A.1	Error-reporting	functions.
495
.	
Figure	A.2	Wrapper	for	Unix-style	wait	function.</p>
<p>496
.	
Figure	A.3	Wrapper	for	Unix-style	kill	function.
497
.	
Figure	A.4	Wrapper	for	Posix-style	pthread_detach	function.
498
.	
Figure	A.5	Wrapper	for	GAI-style	getaddrinfo	function.</p>
<p>Landmarks
1
.	
Contents
2
.	
Frontmatter
3
.	
Start	of	Content
4
.	
backmatter
5
.	
List	of	Illustrations
1
.	
i
2
.	
ii
3
.	
iii
4
.	
iv
5
.	
v
6
.	
vi
7
.	
vii
8
.	
viii
9
.	
ix
10
.	
x
11
.	
xi
12
.	
xii
13
.	
xiii
14
.	
xiv
15
.	
xv
16
.	
xvi
17
.	
xvii
18
.	
xviii
19
.	
xix</p>
<p>20
.	
xx
21
.	
xxi
22
.	
xxii
23
.	
xxiii
24
.	
xxiv
25
.	
xxv
26
.	
xxvi
27
.	
xxvii
28
.	
xxviii
29
.	
xxix
30
.	
xxx
31
.	
xxxi
32
.	
xxxii
33
.	
xxxiii
34
.	
xxxiv
35
.	
xxxv
36
.	
xxxvi
37
.	
1
38
.	
2
39
.	
3
40
.	
4
41
.	
5
42
.	
6
43
.	
7
44
.	
8
45
.	
9
46
.	
10
47
.	
11
48
.	
12
49
.	
13</p>
<p>50
.	
14
51
.	
15
52
.	
16
53
.	
17
54
.	
18
55
.	
19
56
.	
20
57
.	
21
58
.	
22
59
.	
23
60
.	
24
61
.	
25
62
.	
26
63
.	
27
64
.	
28
65
.	
29
66
.	
30
67
.	
31
68
.	
32
69
.	
33
70
.	
34
71
.	
35
72
.	
36
73
.	
37
74
.	
38
75
.	
39
76
.	
40
77
.	
41
78
.	
42
79
.	
43</p>
<p>80
.	
44
81
.	
45
82
.	
46
83
.	
47
84
.	
48
85
.	
49
86
.	
50
87
.	
51
88
.	
52
89
.	
53
90
.	
54
91
.	
55
92
.	
56
93
.	
57
94
.	
58
95
.	
59
96
.	
60
97
.	
61
98
.	
62
99
.	
63
100
.	
64
101
.	
65
102
.	
66
103
.	
67
104
.	
68
105
.	
69
106
.	
70
107
.	
71
108
.	
72
109
.	
73</p>
<p>110
.	
74
111
.	
75
112
.	
76
113
.	
77
114
.	
78
115
.	
79
116
.	
80
117
.	
81
118
.	
82
119
.	
83
120
.	
84
121
.	
85
122
.	
86
123
.	
87
124
.	
88
125
.	
89
126
.	
90
127
.	
91
128
.	
92
129
.	
93
130
.	
94
131
.	
95
132
.	
96
133
.	
97
134
.	
98
135
.	
99
136
.	
100
137
.	
101
138
.	
102
139
.	
103</p>
<p>140
.	
104
141
.	
105
142
.	
106
143
.	
107
144
.	
108
145
.	
109
146
.	
110
147
.	
111
148
.	
112
149
.	
113
150
.	
114
151
.	
115
152
.	
116
153
.	
117
154
.	
118
155
.	
119
156
.	
120
157
.	
121
158
.	
122
159
.	
123
160
.	
124
161
.	
125
162
.	
126
163
.	
127
164
.	
128
165
.	
129
166
.	
130
167
.	
131
168
.	
132
169
.	
133</p>
<p>170
.	
134
171
.	
135
172
.	
136
173
.	
137
174
.	
138
175
.	
139
176
.	
140
177
.	
141
178
.	
142
179
.	
143
180
.	
144
181
.	
145
182
.	
146
183
.	
147
184
.	
148
185
.	
149
186
.	
150
187
.	
151
188
.	
152
189
.	
153
190
.	
154
191
.	
155
192
.	
156
193
.	
157
194
.	
158
195
.	
159
196
.	
160
197
.	
161
198
.	
162
199
.	
163</p>
<p>200
.	
164
201
.	
165
202
.	
166
203
.	
167
204
.	
168
205
.	
169
206
.	
170
207
.	
171
208
.	
172
209
.	
173
210
.	
174
211
.	
175
212
.	
176
213
.	
177
214
.	
178
215
.	
179
216
.	
180
217
.	
181
218
.	
182
219
.	
183
220
.	
184
221
.	
185
222
.	
186
223
.	
187
224
.	
188
225
.	
189
226
.	
190
227
.	
191
228
.	
192
229
.	
193</p>
<p>230
.	
194
231
.	
195
232
.	
196
233
.	
197
234
.	
198
235
.	
199
236
.	
200
237
.	
201
238
.	
202
239
.	
203
240
.	
204
241
.	
205
242
.	
206
243
.	
207
244
.	
208
245
.	
209
246
.	
210
247
.	
211
248
.	
212
249
.	
213
250
.	
214
251
.	
215
252
.	
216
253
.	
217
254
.	
218
255
.	
219
256
.	
220
257
.	
221
258
.	
222
259
.	
223</p>
<p>260
.	
224
261
.	
225
262
.	
226
263
.	
227
264
.	
228
265
.	
229
266
.	
230
267
.	
231
268
.	
232
269
.	
233
270
.	
234
271
.	
235
272
.	
236
273
.	
237
274
.	
238
275
.	
239
276
.	
240
277
.	
241
278
.	
242
279
.	
243
280
.	
244
281
.	
245
282
.	
246
283
.	
247
284
.	
248
285
.	
249
286
.	
250
287
.	
251
288
.	
252
289
.	
253</p>
<p>290
.	
254
291
.	
255
292
.	
256
293
.	
257
294
.	
258
295
.	
259
296
.	
260
297
.	
261
298
.	
262
299
.	
263
300
.	
264
301
.	
265
302
.	
266
303
.	
267
304
.	
268
305
.	
269
306
.	
270
307
.	
271
308
.	
272
309
.	
273
310
.	
274
311
.	
275
312
.	
276
313
.	
277
314
.	
278
315
.	
279
316
.	
280
317
.	
281
318
.	
282
319
.	
283</p>
<p>320
.	
284
321
.	
285
322
.	
286
323
.	
287
324
.	
288
325
.	
289
326
.	
290
327
.	
291
328
.	
292
329
.	
293
330
.	
294
331
.	
295
332
.	
296
333
.	
297
334
.	
298
335
.	
299
336
.	
300
337
.	
301
338
.	
302
339
.	
303
340
.	
304
341
.	
305
342
.	
306
343
.	
307
344
.	
308
345
.	
309
346
.	
310
347
.	
311
348
.	
312
349
.	
313</p>
<p>350
.	
314
351
.	
315
352
.	
316
353
.	
317
354
.	
318
355
.	
319
356
.	
320
357
.	
321
358
.	
322
359
.	
323
360
.	
324
361
.	
325
362
.	
326
363
.	
327
364
.	
328
365
.	
329
366
.	
330
367
.	
331
368
.	
332
369
.	
333
370
.	
334
371
.	
335
372
.	
336
373
.	
337
374
.	
338
375
.	
339
376
.	
340
377
.	
341
378
.	
342
379
.	
343</p>
<p>380
.	
344
381
.	
345
382
.	
346
383
.	
347
384
.	
348
385
.	
349
386
.	
350
387
.	
351
388
.	
352
389
.	
353
390
.	
354
391
.	
355
392
.	
356
393
.	
357
394
.	
358
395
.	
359
396
.	
360
397
.	
361
398
.	
362
399
.	
363
400
.	
364
401
.	
365
402
.	
366
403
.	
367
404
.	
368
405
.	
369
406
.	
370
407
.	
371
408
.	
372
409
.	
373</p>
<p>410
.	
374
411
.	
375
412
.	
376
413
.	
377
414
.	
378
415
.	
379
416
.	
380
417
.	
381
418
.	
382
419
.	
383
420
.	
384
421
.	
385
422
.	
386
423
.	
387
424
.	
388
425
.	
389
426
.	
390
427
.	
391
428
.	
392
429
.	
393
430
.	
394
431
.	
395
432
.	
396
433
.	
397
434
.	
398
435
.	
399
436
.	
400
437
.	
401
438
.	
402
439
.	
403</p>
<p>440
.	
404
441
.	
405
442
.	
406
443
.	
407
444
.	
408
445
.	
409
446
.	
410
447
.	
411
448
.	
412
449
.	
413
450
.	
414
451
.	
415
452
.	
416
453
.	
417
454
.	
418
455
.	
419
456
.	
420
457
.	
421
458
.	
422
459
.	
423
460
.	
424
461
.	
425
462
.	
426
463
.	
427
464
.	
428
465
.	
429
466
.	
430
467
.	
431
468
.	
432
469
.	
433</p>
<p>470
.	
434
471
.	
435
472
.	
436
473
.	
437
474
.	
438
475
.	
439
476
.	
440
477
.	
441
478
.	
442
479
.	
443
480
.	
444
481
.	
445
482
.	
446
483
.	
447
484
.	
448
485
.	
449
486
.	
450
487
.	
451
488
.	
452
489
.	
453
490
.	
454
491
.	
455
492
.	
456
493
.	
457
494
.	
458
495
.	
459
496
.	
460
497
.	
461
498
.	
462
499
.	
463</p>
<p>500
.	
464
501
.	
465
502
.	
466
503
.	
467
504
.	
468
505
.	
469
506
.	
470
507
.	
471
508
.	
472
509
.	
473
510
.	
474
511
.	
475
512
.	
476
513
.	
477
514
.	
478
515
.	
479
516
.	
480
517
.	
481
518
.	
482
519
.	
483
520
.	
484
521
.	
485
522
.	
486
523
.	
487
524
.	
488
525
.	
489
526
.	
490
527
.	
491
528
.	
492
529
.	
493</p>
<p>530
.	
494
531
.	
495
532
.	
496
533
.	
497
534
.	
498
535
.	
499
536
.	
500
537
.	
501
538
.	
502
539
.	
503
540
.	
504
541
.	
505
542
.	
506
543
.	
507
544
.	
508
545
.	
509
546
.	
510
547
.	
511
548
.	
512
549
.	
513
550
.	
514
551
.	
515
552
.	
516
553
.	
517
554
.	
518
555
.	
519
556
.	
520
557
.	
521
558
.	
522
559
.	
523</p>
<p>560
.	
524
561
.	
525
562
.	
526
563
.	
527
564
.	
528
565
.	
529
566
.	
530
567
.	
531
568
.	
532
569
.	
533
570
.	
534
571
.	
535
572
.	
536
573
.	
537
574
.	
538
575
.	
539
576
.	
540
577
.	
541
578
.	
542
579
.	
543
580
.	
544
581
.	
545
582
.	
546
583
.	
547
584
.	
548
585
.	
549
586
.	
550
587
.	
551
588
.	
552
589
.	
553</p>
<p>590
.	
554
591
.	
555
592
.	
556
593
.	
557
594
.	
558
595
.	
559
596
.	
560
597
.	
561
598
.	
562
599
.	
563
600
.	
564
601
.	
565
602
.	
566
603
.	
567
604
.	
568
605
.	
569
606
.	
570
607
.	
571
608
.	
572
609
.	
573
610
.	
574
611
.	
575
612
.	
576
613
.	
577
614
.	
578
615
.	
579
616
.	
580
617
.	
581
618
.	
582
619
.	
583</p>
<p>620
.	
584
621
.	
585
622
.	
586
623
.	
587
624
.	
588
625
.	
589
626
.	
590
627
.	
591
628
.	
592
629
.	
593
630
.	
594
631
.	
595
632
.	
596
633
.	
597
634
.	
598
635
.	
599
636
.	
600
637
.	
601
638
.	
602
639
.	
603
640
.	
604
641
.	
605
642
.	
606
643
.	
607
644
.	
608
645
.	
609
646
.	
610
647
.	
611
648
.	
612
649
.	
613</p>
<p>650
.	
614
651
.	
615
652
.	
616
653
.	
617
654
.	
618
655
.	
619
656
.	
620
657
.	
621
658
.	
622
659
.	
623
660
.	
624
661
.	
625
662
.	
626
663
.	
627
664
.	
628
665
.	
629
666
.	
630
667
.	
631
668
.	
632
669
.	
633
670
.	
634
671
.	
635
672
.	
636
673
.	
637
674
.	
638
675
.	
639
676
.	
640
677
.	
641
678
.	
642
679
.	
643</p>
<p>680
.	
644
681
.	
645
682
.	
646
683
.	
647
684
.	
648
685
.	
649
686
.	
650
687
.	
651
688
.	
652
689
.	
653
690
.	
654
691
.	
655
692
.	
656
693
.	
657
694
.	
658
695
.	
659
696
.	
660
697
.	
661
698
.	
662
699
.	
663
700
.	
664
701
.	
665
702
.	
666
703
.	
667
704
.	
668
705
.	
669
706
.	
670
707
.	
671
708
.	
672
709
.	
673</p>
<p>710
.	
674
711
.	
675
712
.	
676
713
.	
677
714
.	
678
715
.	
679
716
.	
680
717
.	
681
718
.	
682
719
.	
683
720
.	
684
721
.	
685
722
.	
686
723
.	
687
724
.	
688
725
.	
689
726
.	
690
727
.	
691
728
.	
692
729
.	
693
730
.	
694
731
.	
695
732
.	
696
733
.	
697
734
.	
698
735
.	
699
736
.	
700
737
.	
701
738
.	
702
739
.	
703</p>
<p>740
.	
704
741
.	
705
742
.	
706
743
.	
707
744
.	
708
745
.	
709
746
.	
710
747
.	
711
748
.	
712
749
.	
713
750
.	
714
751
.	
715
752
.	
716
753
.	
717
754
.	
718
755
.	
719
756
.	
720
757
.	
721
758
.	
722
759
.	
723
760
.	
724
761
.	
725
762
.	
726
763
.	
727
764
.	
728
765
.	
729
766
.	
730
767
.	
731
768
.	
732
769
.	
733</p>
<p>770
.	
734
771
.	
735
772
.	
736
773
.	
737
774
.	
738
775
.	
739
776
.	
740
777
.	
741
778
.	
742
779
.	
743
780
.	
744
781
.	
745
782
.	
746
783
.	
747
784
.	
748
785
.	
749
786
.	
750
787
.	
751
788
.	
752
789
.	
753
790
.	
754
791
.	
755
792
.	
756
793
.	
757
794
.	
758
795
.	
759
796
.	
760
797
.	
761
798
.	
762
799
.	
763</p>
<p>800
.	
764
801
.	
765
802
.	
766
803
.	
767
804
.	
768
805
.	
769
806
.	
770
807
.	
771
808
.	
772
809
.	
773
810
.	
774
811
.	
775
812
.	
776
813
.	
777
814
.	
778
815
.	
779
816
.	
780
817
.	
781
818
.	
782
819
.	
783
820
.	
784
821
.	
785
822
.	
786
823
.	
787
824
.	
788
825
.	
789
826
.	
790
827
.	
791
828
.	
792
829
.	
793</p>
<p>830
.	
794
831
.	
795
832
.	
796
833
.	
797
834
.	
798
835
.	
799
836
.	
800
837
.	
801
838
.	
802
839
.	
803
840
.	
804
841
.	
805
842
.	
806
843
.	
807
844
.	
808
845
.	
809
846
.	
810
847
.	
811
848
.	
812
849
.	
813
850
.	
814
851
.	
815
852
.	
816
853
.	
817
854
.	
818
855
.	
819
856
.	
820
857
.	
821
858
.	
822
859
.	
823</p>
<p>860
.	
824
861
.	
825
862
.	
826
863
.	
827
864
.	
828
865
.	
829
866
.	
830
867
.	
831
868
.	
832
869
.	
833
870
.	
834
871
.	
835
872
.	
836
873
.	
837
874
.	
838
875
.	
839
876
.	
840
877
.	
841
878
.	
842
879
.	
843
880
.	
844
881
.	
845
882
.	
846
883
.	
847
884
.	
848
885
.	
849
886
.	
850
887
.	
851
888
.	
852
889
.	
853</p>
<p>890
.	
854
891
.	
855
892
.	
856
893
.	
857
894
.	
858
895
.	
859
896
.	
860
897
.	
861
898
.	
862
899
.	
863
900
.	
864
901
.	
865
902
.	
866
903
.	
867
904
.	
868
905
.	
869
906
.	
870
907
.	
871
908
.	
872
909
.	
873
910
.	
874
911
.	
875
912
.	
876
913
.	
877
914
.	
878
915
.	
879
916
.	
880
917
.	
881
918
.	
882
919
.	
883</p>
<p>920
.	
884
921
.	
885
922
.	
886
923
.	
887
924
.	
888
925
.	
889
926
.	
890
927
.	
891
928
.	
892
929
.	
893
930
.	
894
931
.	
895
932
.	
896
933
.	
897
934
.	
898
935
.	
899
936
.	
900
937
.	
901
938
.	
902
939
.	
903
940
.	
904
941
.	
905
942
.	
906
943
.	
907
944
.	
908
945
.	
909
946
.	
910
947
.	
911
948
.	
912
949
.	
913</p>
<p>950
.	
914
951
.	
915
952
.	
916
953
.	
917
954
.	
918
955
.	
919
956
.	
920
957
.	
921
958
.	
922
959
.	
923
960
.	
924
961
.	
925
962
.	
926
963
.	
927
964
.	
928
965
.	
929
966
.	
930
967
.	
931
968
.	
932
969
.	
933
970
.	
934
971
.	
935
972
.	
936
973
.	
937
974
.	
938
975
.	
939
976
.	
940
977
.	
941
978
.	
942
979
.	
943</p>
<p>980
.	
944
981
.	
945
982
.	
946
983
.	
947
984
.	
948
985
.	
949
986
.	
950
987
.	
951
988
.	
952
989
.	
953
990
.	
954
991
.	
955
992
.	
956
993
.	
957
994
.	
958
995
.	
959
996
.	
960
997
.	
961
998
.	
962
999
.	
963
1000
.	
964
1001
.	
965
1002
.	
966
1003
.	
967
1004
.	
968
1005
.	
969
1006
.	
970
1007
.	
971
1008
.	
972
1009
.	
973</p>
<p>1010
.	
974
1011
.	
975
1012
.	
976
1013
.	
977
1014
.	
978
1015
.	
979
1016
.	
980
1017
.	
981
1018
.	
982
1019
.	
983
1020
.	
984
1021
.	
985
1022
.	
986
1023
.	
987
1024
.	
988
1025
.	
989
1026
.	
990
1027
.	
991
1028
.	
992
1029
.	
993
1030
.	
994
1031
.	
995
1032
.	
996
1033
.	
997
1034
.	
998
1035
.	
999
1036
.	
1000
1037
.	
1001
1038
.	
1002
1039
.	
1003</p>
<p>1040
.	
1004
1041
.	
1005
1042
.	
1006
1043
.	
1007
1044
.	
1008
1045
.	
1009
1046
.	
1010
1047
.	
1011
1048
.	
1012
1049
.	
1013
1050
.	
1014
1051
.	
1015
1052
.	
1016
1053
.	
1017
1054
.	
1018
1055
.	
1019
1056
.	
1020
1057
.	
1021
1058
.	
1022
1059
.	
1023
1060
.	
1024
1061
.	
1025
1062
.	
1026
1063
.	
1027
1064
.	
1028
1065
.	
1029
1066
.	
1030
1067
.	
1031
1068
.	
1032
1069
.	
1033</p>
<p>1070
.	
1034
1071
.	
1035
1072
.	
1036
1073
.	
1037
1074
.	
1038
1075
.	
1039
1076
.	
1040
1077
.	
1041
1078
.	
1042
1079
.	
1043
1080
.	
1044
1081
.	
1045
1082
.	
1046
1083
.	
1047
1084
.	
1048
1085
.	
1049
1086
.	
1050
1087
.	
1051
1088
.	
1052
1089
.	
1053
1090
.	
1054
1091
.	
1055
1092
.	
1056
1093
.	
1057
1094
.	
1058
1095
.	
1059
1096
.	
1060
1097
.	
1061
1098
.	
1062
1099
.	
1063</p>
<p>1100
.	
1064
1101
.	
1065
1102
.	
1066
1103
.	
1067
1104
.	
1068
1105
.	
1069
1106
.	
1070
1107
.	
1071
1108
.	
1072
1109
.	
1073
1110
.	
1074
1111
.	
1075
1112
.	
1076
1113
.	
1077
1114
.	
1078
1115
.	
1079
1116
.	
1080
1117
.	
1081
1118
.	
1082
1119
.	
1083
1120
.	
1084</p>
<div style="break-before: page; page-break-before: always;"></div><p>Current
Legacy
Getting Started
Using Discord
Commands Parameters and Tools
Writing Prompts
Working with Your Own Images
Styles and Aesthetics
Image Size and Aspect Ratio
Advanced Prompting Tools
Subscriptions
Using The Website
Policies
Terms of Service
Midjourney Trademark Policy
Privacy Policy
Data Deletion Request Form and FAQ
Data Deletion Request Form and FAQ
Light
Data Deletion Form
Privacy FAQ</p>
<p>Last Updated: October 2, 2023</p>
<p>What information does Midjourney collect from users?</p>
<p>We collect information that you provide to us while interacting with our services. This may include personal information that could be used to identify you, such as your username, the text or image prompts that you input, public chats, IP address, billing information for Stripe, your contact information, email address, and any other information that you share with us.</p>
<p>How long is personal data stored?</p>
<p>We only keep the information that you share with us for as long as necessary to serve an allowable purpose under the data protection regulations that apply to you. Some of these purposes include: providing you with the Midjourney services, maintaining and improving upon these services, helping you manage your account and the features that you can access, and other purposes laid out in our Privacy Policy. There may also be legal obligations that we need to comply with that may impact how long we retain your personal data.</p>
<p>Where is the personal data on Discord stored?</p>
<p>Discord is in control of all data you store on Discord. Please consult the Discord Privacy Policy for more information.</p>
<p>Where is the personal data that Midjourney handles stored?</p>
<p>The personal information you provide is stored on Midjourney servers in the US. We take careful measures to make sure that your personal information is treated securely, and that your information is not transferred to an organization or country that does not have the adequate measures in place to ensure its security.</p>
<p>Does Midjourney sell personal data?</p>
<p>No. We do not sell your personal information.</p>
<p>Who can submit a request to delete personal data?</p>
<p>If you use or have used the Midjourney services, you can submit a request to delete your personal data. There is no need for you to reside in or have used our services in any particular locations.</p>
<p>What information does Midjourney need in order to process a data deletion request?</p>
<p>In order to process your data deletion request, we need some information from you to help us locate the data, such as your username and email address associated with your account. Please review our data deletion request form for more information.</p>
<p>How long does a data deletion request take?</p>
<p>We will do our best to respond to your data deletion request within 45 days once we receive it. If we need more time, we will let you know, and you should hear back from us within 90 days. Once we are able to confirm that we possess the data you are requesting we delete, we will remove this information from our system within 1-2 weeks.</p>
<p>Previous
Privacy Policy
Midjourney is an independent research lab exploring new mediums of thought and expanding the imaginative powers of the human species. We are a small self-funded team focused on design, human infrastructure, and AI.
FOLLOW US: [F] [T] [R]
SUPPORT
For questions or support visit the Midjourney Discord support channels.
SITES
Midjourney Website
Midjourney Discord
Current
Legacy
Getting Started
Using Discord
Commands Parameters and Tools
Writing Prompts
Working with Your Own Images
Styles and Aesthetics
Image Size and Aspect Ratio
Advanced Prompting Tools
Subscriptions
Subscription Plans
Stealth Mode
Earn Free Hours
Fast, Relax, &amp; Turbo Modes
Contacting Support
Using The Website
Policies
Earn Free Hours
Light
Subscribers can earn a free Fast GPU hour each day by ranking images. A bonus Fast hour awarded to the top 2000 participants each day.</p>
<p>Bonus hours expire 30 days from the date they are awarded.
You must have an active subscription to participate.</p>
<p>How to Rank Images</p>
<p>Current subscribers can rank image pairs at midjourney.com/app/rank-pairs/ to earn free Fast GPU hours. Click on the image you like the most for each pair that is presented. The top 2000 daily image rankers receive an hour of free Fast GPU time. If you're among the daily top rankers, you will receive a direct message from the Midjourney Bot. Please note that these hours remain valid for 30 days and require an active subscription to use.</p>
<p>What Criteria Do I Use to Rank?</p>
<p>Rankings are based on personal preference, so choose the image you like the most based on appearance, effort, colors, concept, or theme.</p>
<p>Keep Track of Your Free Hours</p>
<p>You can see how many hours you have earned rating images on your Midjourney.com/account page in the Bonus category.</p>
<p>Previous
Stealth Mode
Next
Fast, Relax, &amp; Turbo Modes
Midjourney is an independent research lab exploring new mediums of thought and expanding the imaginative powers of the human species. We are a small self-funded team focused on design, human infrastructure, and AI.
FOLLOW US: [F] [T] [R]
SUPPORT
For questions or support visit the Midjourney Discord support channels.
SITES
Midjourney Website
Midjourney Discord
Current
Legacy
Getting Started
Using Discord
Commands Parameters and Tools
Writing Prompts
Working with Your Own Images
Styles and Aesthetics
Image Size and Aspect Ratio
Advanced Prompting Tools
Subscriptions
Using The Website
Policies
Terms of Service
Midjourney Trademark Policy
Privacy Policy
Data Deletion Request Form and FAQ
Midjourney Trademark Policy
Light</p>
<p>Last Updated: August 7, 2023</p>
<p>The goal of this policy is to protect customers by making it easy to readily identify products and services from Midjourney, Inc. Please read on to learn when and how you can properly use our logo, name, and other brand features (collectively, the “Midjourney Trademarks”).</p>
<p>What are the Midjourney Trademarks?</p>
<p>The Midjourney Trademarks include, but are not limited to, the following:</p>
<p>MIDJOURNEY™,
NIJI JOURNEY™,
NIJI·JOURNEY™,
にじジャーニー,
小船,</p>
<p>,</p>
<p>By using any Midjourney Trademarks, or by accessing the Midjourney platform, you acknowledge that Midjourney is the sole owner of the Trademarks and you agree not to interfere with Midjourney’s rights in the Trademarks, including challenging Midjourney’s use, registration of, or application to register such Trademarks. You agree that you will not harm, misuse, or bring into dispute any Midjourney Trademarks. You may not use any Midjourney Trademarks in metatags, search fields, hidden text without Midjourney’s prior written consent. It is never acceptable to use the Midjourney Trademarks in any form that has the purpose or effect of diverting or confusing consumers.</p>
<p>Please review these guidelines on a regular basis, as they may be updated from time to time. Midjourney reserves the right to revoke any permission and to change or restrict any use of the trademarks if we believe that such use is in violation of this policy or is harmful to the reputation of Midjourney or the Midjourney community.</p>
<p>When and How can you use a Midjourney Trademark?</p>
<p>Here is a non-exhaustive list of the correct ways to use a Midjourney Trademark.</p>
<p>Use Midjourney Trademarks to refer to Midjourney, its services, and products truthfully and accurately.</p>
<p>“Created using the Midjourney web app.”</p>
<p>Use Midjourney Trademarks exactly as shown in the list above, including capitalization and proper trademark symbol ™.</p>
<p>Please include this notice when you use the Midjourney’s Trademarks in accordance with this Policy:</p>
<p>“[Midjourney Trademark] is a trademark of Midjourney, Inc. We are not endorsed by or affiliated with Midjourney, Inc.”
Here are some ways you shouldn’t use our marks:
Do not use Midjourney Trademarks in the name of your business, product, service, app, publication, domain name, or other offering.
Do not use Midjourney wordmarks in plural or possessive forms.
Do not misspell the Midjourney Marks (including hyphens, spaces etc.)
Do not use marks, logos, company names, domain names, social media names, community names, or designs that are confusingly similar to Midjourney’s Trademarks. Deliberate misspellings and transliterations of the Midjourney Trademarks are also not permitted
Ex. Midj0urney
Do not use Midjourney’s Trademarks to incorrectly imply sponsorship, endorsement, or affiliation with Midjourney.
Do not use Midjourney’s Trademarks for any illegal, misleading, derogatory, or defamatory purposes.
Do not use Midjourney’s Trademarks more prominently than your company name, product, or service.
Do not use Midjourney’s Trademarks on merchandise for sale.
Do not use Midjourney’s Trademarks for any other form of commercial use, unless limited to a truthful descriptive reference.
Do not modify, distort, abbreviate, or combine Midjourney’s Trademarks with any other symbols, words, images, or slogans.</p>
<p>Can I use one of the Midjourney Trademarks when registering a domain name?</p>
<p>You may not use the Midjourney Trademarks in your own domain, if that use would likely confuse a relevant consumer. You may not use the Midjourney Trademarks in your product, app, or service, except to clearly refer to Midjourney services in a nominative fashion.</p>
<p>Can I use one of the Midjourney Trademarks when naming an application or game?</p>
<p>You may not use the Midjourney trademark in naming an application or a game, if that use would likely confuse a consumer.</p>
<p>What if I want to plan an online or in person event concerning Midjourney or with Midjourney community members?</p>
<p>To help guide your usage of our trademarks, please keep in mind the following: Meetups, events, online chats outside our Discord channel, or other in person or online events that are not sponsored or endorsed by Midjourney cannot misrepresent the event as such. In order to avoid this confusion, the event planner should promote the event as an unofficial event.</p>
<p>Midjourney is not responsible for your experience at these unofficial events, as it is not an official Midjourney event. In order to avoid any implication that Midjourney is associated or sponsoring the event, avoid prominent use the Midjourney Trademarks (e.g., in connection with the title of the event etc.) to advertise, promote, or otherwise talk about the event.</p>
<p>IMPORTANT NOTE: Nothing in this Midjourney policy statement shall be interpreted to allow any third parties to claim any association with the Midjourney offerings or to imply its approval or support by Midjourney for any third-party products or services.</p>
<p>If you have any questions at all about proper usage of the Midjourney Trademarks, please contact trademarks@midjourney.com</p>
<p>Previous
Terms of Service
Next
Privacy Policy
Midjourney is an independent research lab exploring new mediums of thought and expanding the imaginative powers of the human species. We are a small self-funded team focused on design, human infrastructure, and AI.
FOLLOW US: [F] [T] [R]
SUPPORT
For questions or support visit the Midjourney Discord support channels.
SITES
Midjourney Website
Midjourney Discord
Current
Legacy
Getting Started
Using Discord
Commands Parameters and Tools
Writing Prompts
Working with Your Own Images
Styles and Aesthetics
Image Size and Aspect Ratio
Advanced Prompting Tools
Subscriptions
Using The Website
Policies
Terms of Service
Midjourney Trademark Policy
Privacy Policy
Data Deletion Request Form and FAQ
Privacy Policy
Light</p>
<ol>
<li>Introduction</li>
</ol>
<p>This privacy policy (the “Policy”) applies to Midjourney, Inc., the Midjourney.com website, and the Midjourney image generation platform (the “Services”) . Midjourney, Inc. (“Midjourney”) is a communications technology incubator that provides image generation services to augment human creativity and foster social connection.</p>
<p>As used in this Policy, “personal data” means any information that relates to, describes, could be used to identify an individual, directly or indirectly.</p>
<p>Applicability: This Policy applies to personal data that Midjourney collects, uses, and discloses and which may include: (i) data collected through the Services, (ii) data collected through the process of training Midjourney machine learning algorithms, (iii) data collected through Midjourney websites, and (iv) data collected from third party sources. Third party sources may include, but not be limited to: public databases, commercial data sources, and the public internet. When you make purchases, we use third-party payment processors to collect credit card or other financial information. Midjourney does not store the credit card or payment information you provide, only confirmation that payment was made.</p>
<p>This Policy does not apply to the following information:</p>
<p>Personal Data about Midjourney employees and candidates, and certain contractors and agents acting in similar roles.</p>
<p>Changes: We may update this Policy from time-to-time to reflect changes in legal, regulatory, operational requirements, our practices, and other factors. Please check this Policy periodically for updates. If any of the changes are unacceptable to you, you should cease interacting with us. When required under applicable law, we will notify you of any changes to this Policy.</p>
<p>Definitions: Through this Policy, You, or Your means the individual accessing or using the Service, or the company, or other legal entity on behalf of which such individual is accessing or using the Service, as applicable. Company (referred to as either &quot;the Company&quot;, &quot;We&quot;, &quot;Us&quot; or &quot;Our&quot; in this Agreement) refers to Midjourney LLC. Usage Data refers to data collected automatically, either generated by the use of the Service or from the Service infrastructure itself (for example, the duration of a page visit).</p>
<ol start="2">
<li>Collecting and Using Your Personal Data</li>
</ol>
<p>2.1 Types of Data Collected
2.1.1 Personal Data
While interacting with the Services, You may provide certain personally identifiable information that could be used to contact or identify You. Personally identifiable information may include, but is not limited to:</p>
<p>Your user name for the Services
Text or image prompts you input into the Services, or public chats you maintain on the service
Your IP address
Usage Data
Tracking Technologies and Cookies
Contact Information
Organizational Information like your company title
Your Email
Cookies
Other data that you elect to send to Midjourney</p>
<p>2.2 Use of Your Personal Data
Midjourneymay use Personal Data for the following purposes:</p>
<p>To provide,maintain, and improve our Service, including to monitor the usage of our Service.
To manage Your account: to manage Your registration as a user of the Service. The Personal Data You provide can give You access to different functionalities of the Service that are available to You as a registered user.
For the performance of a contract: the development, compliance and undertaking of the purchase contract for the products, items or services You have purchased or of any other contract with Us through the Service.
To contact You: To contact You by email, telephone calls, SMS, or other equivalent forms of electronic communication, such as a mobile application's push notifications regarding updates or informative communications related to the functionalities, products or contracted services, including the security updates, when necessary or reasonable for their implementation.
To provide You with news, special offers and general information about other goods, services and events which we offer that are similar to those that you have already purchased or enquired about unless You have opted not to receive such information.
To manage Your requests: To attend and manage Your requests to Us.
For business transfers: We may use Your information to evaluate or conduct a merger, divestiture, restructuring, reorganization, dissolution, or other sale or transfer of some or all of Our assets, whether as a going concern or as part of bankruptcy, liquidation, or similar proceeding, in which Personal Data held by Us about our Service users is among the assets transferred.
For other purposes: We may use Your information for other purposes, such as data analysis, identifying usage trends, determining the effectiveness of our promotional campaigns and to evaluate and improve our Service, products, services, marketing and your experience.
2.3 Sharing of Your Personal Data</p>
<p>We may share Your personal information in the following situations:</p>
<p>With Service Providers, Third Party Vendors, Consultants, and other Business Partners: We may share Your personal information with these parties in order to provide services on our behalf, monitor and analyze the use of our services, contact You, and for the reasons stated in the Agreement. Service Providers to monitor and analyze the use of our Service, to contact You.
For business transfers: We may share or transfer Your personal information in connection with, or during negotiations of, any merger, sale of Company assets, financing, or acquisition of all or a portion of Our business to another company.
With Your consent: We may disclose Your personal information for any other purpose with Your consent.
With Law Enforcement: Under certain circumstances, Midjourney may be required to disclose Your Personal Data if required to do so by law or in response to valid requests by public authorities (e.g. a court or a government agency). To the extent we receive a request from Law Enforcement for Your personal data, we will promptly notify You and provide You with a copy of the request, unless we are legally prohibited from doing so.
With Other Parties in order to:
Comply with a legal obligation
Protect and defend the rights or property of the Company
Prevent or investigate possible wrongdoing in connection with the Service
Protect the personal safety of Users of the Service or the public
Protect against legal liability</p>
<p>2.4 Retention of Your Personal Data
The Company will retain Your Personal Data only for as long as is necessary for the purposes set out in this Privacy Policy. We will retain and use Your Personal Data to the extent necessary to comply with our legal obligations (for example, if we are required to retain your data to comply with applicable laws), resolve disputes, and enforce our legal agreements and policies.</p>
<p>The Company will also retain Usage Data for internal analysis purposes. Usage Data is generally retained for a shorter period of time, except when this data is used to strengthen the security or to improve the functionality of Our Service, or We are legally obligated to retain this data for longer time periods.</p>
<p>2.5 Transfer of Your Personal Data
Your information, including Personal Data, is processed at the Company's operating offices and in any other places where the parties involved in the processing are located. It means that this information may be transferred to — and maintained on — computers located outside of Your state, province, country or other governmental jurisdiction where the data protection laws may differ from those in Your jurisdiction.</p>
<p>Your consent to this Privacy Policy followed by Your submission of such information represents Your agreement to that transfer.</p>
<p>The Company will take all steps reasonably necessary to ensure that Your data is treated securely and in accordance with this Privacy Policy and no transfer of Your Personal Data will take place to an organization or a country unless there are adequate controls in place including the security of Your data and other personal information.</p>
<p>2.6 Security of Your Personal Data</p>
<p>The security of Your Personal Data is important to Us, but remember that no method of transmission over the Internet, or method of electronic storage is 100% secure. While We strive to use commercially acceptable means to protect Your Personal Data, We cannot guarantee its absolute security.</p>
<ol start="3">
<li>Children's Privacy</li>
</ol>
<p>Our Service does not address anyone under the age of 13. We do not knowingly collect personally identifiable information from anyone under the age of 13. If You are a parent or guardian and You are aware that Your child has provided Us with Personal Data, please contact Us. If We become aware that We have collected Personal Data from anyone under the age of 13 without verification of parental consent, We take steps to remove that information from Our servers.</p>
<p>If We need to rely on consent as a legal basis for processing Your information and Your country requires consent from a parent, We may require Your parent's consent before We collect and use that information.</p>
<ol start="4">
<li>Links to Other Websites</li>
</ol>
<p>Our Service may contain links to other websites that are not operated by Us. If You click on a third party link, You will be directed to that third party's site. We strongly advise You to review the Privacy Policy of every site You visit.</p>
<p>We have no control over and assume no responsibility for the content, privacy policies or practices of any third party sites or services.</p>
<ol start="5">
<li>Changes to this Privacy Policy</li>
</ol>
<p>We may update Our Privacy Policy from time to time. We will notify You of any changes by posting the new Privacy Policy on this page.</p>
<p>We will let You know via email and/or a prominent notice on Our Service, prior to the change becoming effective and update the &quot;Last updated&quot; date at the top of this Privacy Policy.</p>
<p>You are advised to review this Privacy Policy periodically for any changes. Changes to this Privacy Policy are effective when they are posted on this page.</p>
<ol start="6">
<li>Supplemental Terms and Conditions for Certain Regions</li>
</ol>
<p>Europe
If You are located in the European Economic Area (the “EEA”), Switzerland, or the United Kingdom (the “UK”) Our legal basis for collecting and using the Personal Data described in this Policy will depend on the personal data concerned and the specific context in which we collect it. However, we will normally collect personal data from you only where we have your consent to do so, where we need the personal data to perform a contract with you, or where the processing is in our legitimate interests and not overridden by your data protection interests or fundamental rights and freedoms. In some cases, we may also have a legal obligation to collect personal data from you.</p>
<p>Midjourney may share information internally or with third parties, as further described in this Policy. When we share Personal Data of individuals in the EEA, Switzerland or UK with third parties, we make use of a variety of legal mechanisms to safeguard the transfer including the European Commission-approved standard contractual clauses, as well as additional safeguards where appropriate.</p>
<p>Additionally You have the following data protection rights:</p>
<p>You can request access, correction, updates or deletion of your Personal Data.
You can object to our processing of your Personal Data, ask us to restrict processing of your Personal Data or request portability of your Personal Data.
If we have collected and processed your Personal Data with your consent, then you can withdraw your consent at any time. Withdrawing your consent will not affect the lawfulness of any processing we conducted prior to your withdrawal, nor will it affect processing of your Personal Data conducted in reliance on lawful processing grounds other than consent.
You have the right to complain to a data protection authority about our collection and use of your Personal Data.</p>
<p>In order to exercise Your rights please contact privacy@midjourney.com.</p>
<p>California
This Statement applies solely to residents of California or individuals whose information has been collected in California. Midjourney has adopted and included this notice to comply with the California Consumer Privacy Act of 2018 (“CCPA”). Any terms used in this Statement that are defined in the CCPA have the same meaning given therein.</p>
<p>INFORMATION WE COLLECT</p>
<h1 id="category-collected-disclosed"><a class="header" href="#category-collected-disclosed">Category	Collected?	Disclosed?</a></h1>
<p>1	Identifiers. Name, alias, postal address, unique personal identifier, online identifier, Internet Protocol (IP) address, email address, account name, social security number, driver’s license number, passport number, or other similar identifiers	YES	YES
2	Personal information categories under the California Customer Records statute (Cal. Civ. Code § 1798.80(e)). A name, signature, Social Security number, physical characteristics or description, address, telephone number, passport number, driver’s license or state identification card number, insurance policy number, education, employment, employment history, bank account number, credit card number, debit card number, or any other financial information, medical information, or health insurance information. Some personal information included in this category may overlap with other categories.	YES	YES
3	Protected classification characteristics under California or federal law. Age (40 years or older), race, color, ancestry, national origin, citizenship, religion or creed, marital status, medical condition, physical or mental disability, sex (including gender, gender identity, gender expression, pregnancy or childbirth and related medical conditions), sexual orientation, veteran or military status, genetic information (including familial genetic information).	NO	NO
4	Commercial information. Records of personal property, products or services purchased, obtained, or considered, or other purchasing or consuming histories or tendencies.	YES	YES
5	Biometric information. Genetic, physiological, behavioral, and biological characteristics, or activity patterns used to extract a template or other identifier or identifying information, such as, fingerprints, faceprints, and voiceprints, iris or retina scans, keystroke, gait, or other physical patterns, and sleep, health, or exercise data.	NO	NO
6	Internet or other similar network activity. Browsing history, search history, information on a consumer’s interaction with a website, application, or advertisement.	NO	NO
7	Geolocation data. Physical location or movements.	NO	NO
8	Sensory data. Audio, electronic, visual, thermal, olfactory, or similar information.	YES (only images you upload to our services)	YES
9	Professional or employment-related information. Current or past employment history or performance evaluations.	NO	NO
10	Education Information under California Family Educational Rights and Privacy Act (20 U.S.C. section 1232g, 34 C.F.R. Part 99) Information that is not “publicly available personally identifiable information” as defined in the California Family Educational Rights and Privacy Act (20 U.S.C. section 1232g, 34 C.F.R. Part 99). Includes education records directly related to a student maintained by an educational institution or party acting on its behalf, like grades, transcripts, class lists and student schedules, identification codes, financial information, or disciplinary records.	NO	NO
11	Inferences Conclusions that could be used to create a profile reflecting an individual’s preferences, characteristics, psychological trends, predispositions, behavior, attitudes, intelligence, abilities, aptitude.	YES.	YES</p>
<p>SELLING INFORMATION
Midjourney does not sell Your Personal Information, as defined under CCPA. If in the future we do sell your personal information, we will notify you and you may have the right to opt-out of such sale.</p>
<p>YOUR RIGHTS AND CHOICES
The CCPA provides individuals residing in California or whose Personal Information was collected in California with specific rights regarding their Personal information. The below describes your rights and how you may exercise them.</p>
<p>Access to Specific Information and Data Portability Rights
You have the right to request that Midjourney disclose certain information to you about our collection and use of your Personal Information over the past twelve (12) months. Once Midjourney receives and confirms your verifiable information access request, Midjourney must disclose to you: (i) the categories of Personal Information we collected about you; (ii) the categories of sources for the Personal Information we collected about you; (iii) our business or commercial purpose for collecting or, if applicable, selling that Personal Information; (iv) the categories of third parties with whom we share that Personal Information; (v) the specific data points or pieces of Personal Information we collected about you. If we disclosed for a business purpose or sold your Personal Information, Midjourney must also provide separate lists that: (x) identify the personal information categories that were sold to each category of recipient in connection with sales of your Personal Information; and (y) identify the personal information categories that were provided to each category of recipient in connection with business purposes disclosures of your Personal Information..</p>
<p>Deletion Request Rights
You have the right to request that Midjourney delete any of your Personal Information that we collected from you and/or retained. Unless subject to a certain limited exception, once Midjourney receives and confirms your verifiable data deletion request, we will delete (and direct our service providers to delete) your personal information from our records. Midjourney will notify you promptly if it determines it must deny your deletion request.</p>
<p>Do Not Sell Opt-Out Rights
You have the right to opt-out of any sales, as defined by the CCPA, of Personal Information by Midjourney. However, Midjourney does not sell your information.</p>
<p>EXERCISING YOUR RIGHTS
To exercise your access, data portability, and deletion or do not sell opt-out rights described above, you may submit a verifiable consumer request by any of the following means: By email to privacy@midjourney.com.</p>
<p>You may only make a verifiable consumer request for access or data portability up to two times within a 12-month period. You may make a verifiable do not sell opt-out request at any time. Any such request must: (i) provide sufficient information that allows Us to reasonably verify that you are the person about whom we collected personal information or an authorized representative thereof; and (ii) describe your request with sufficient detail such that we may understand, evaluate, and respond to it. Midjourney cannot respond to your request or provide you with personal information if we cannot verify your identity or authority to make the request and confirm the personal information relates to you. Making a verifiable consumer request does not require you to create an account with Midjourney. Midjourney will only use personal information provided in a verifiable consumer request to verify the requestor’s identity or authority to make the request. Only you or a person registered with the California Secretary of State that you authorize to act on your behalf may make a verifiable consumer request related to your Personal Information. You may also make a verifiable consumer request on behalf of your minor child.</p>
<p>Response Timing and Format
We endeavor to respond to a verifiable consumer request within 45 days of its receipt. If we require more time (up to 90 days), we will inform you in writing of the extension period and the reason for it. Midjourney will deliver any required or requested responses or other communications in writing to you by email. Any disclosures we provide will only cover the 12-month period preceding the verifiable consumer request’s receipt. If applicable, the response we provide will also explain any reasons we cannot comply with a request. For data portability requests, Midjourney will provide your personal information in a format that is readily usable and transferable. Midjourney does not charge a fee to process or respond to your verifiable consumer request unless such requests become excessive, repetitive, or manifestly unfounded or as otherwise permitted by the CCPA. If we determine that a request warrants charging a fee, we will notify you and provide you with a cost estimate before completing your request.</p>
<ol start="7">
<li>Contact Us</li>
</ol>
<p>If you have any questions about this Policy, or would otherwise like to exercise your rights under this Policy or applicable law, you can contact us at privacy@midjourney.com</p>
<p>Previous
Midjourney Trademark Policy
Next
Data Deletion Request Form and FAQ
Midjourney is an independent research lab exploring new mediums of thought and expanding the imaginative powers of the human species. We are a small self-funded team focused on design, human infrastructure, and AI.
FOLLOW US: [F] [T] [R]
SUPPORT
For questions or support visit the Midjourney Discord support channels.
SITES
Midjourney Website
Midjourney Discord
Current
Legacy
Getting Started
Using Discord
Commands Parameters and Tools
Writing Prompts
Working with Your Own Images
Styles and Aesthetics
Image Size and Aspect Ratio
Advanced Prompting Tools
Subscriptions
Using The Website
The Midjourney Web App
Policies
The Midjourney Web App
Light
Manage your subscription, view all of your images in one places, and browse images created by other Midjourney users at www.midjourney.com/
Previous
Contacting Support
Next
Terms of Service
Midjourney is an independent research lab exploring new mediums of thought and expanding the imaginative powers of the human species. We are a small self-funded team focused on design, human infrastructure, and AI.
FOLLOW US: [F] [T] [R]
SUPPORT
For questions or support visit the Midjourney Discord support channels.
SITES
Midjourney Website
Midjourney Discord
Current
Legacy
Getting Started
Using Discord
Commands Parameters and Tools
Writing Prompts
Working with Your Own Images
Styles and Aesthetics
Image Size and Aspect Ratio
Advanced Prompting Tools
Subscriptions
Using The Website
Policies
Terms of Service
Midjourney Trademark Policy
Privacy Policy
Data Deletion Request Form and FAQ
Terms of Service
Light</p>
<p>Version Effective Date: December 22, 2023</p>
<p>Thank you for using the Midjourney platform (the “Services”). These Terms of Service (the “Agreement”) explain what rights you have with respect to images and other assets which you might generate with the Service, or prompts you might enter into the Service (the “Assets”), your use of the Services, and other important topics like arbitration. Please read it carefully. Our privacy policy outlines how we handle your data here.</p>
<p>This Agreement is entered into by Midjourney Inc. and the entity or person agreeing to these terms (the &quot;Customer,&quot; “You” or “Your”) and govern the Customer's access to and use of the Services.</p>
<p>This Agreement is effective when the Customer is presented with this Agreement and proceeds to use the Services (the &quot;Effective Date&quot;) or to receive or distribute Assets. These terms may be updated and presented again to the Customer from time to time. Continued use of the Services constitutes acceptance of the updated terms. If You do not agree to this Agreement, please stop using the Services.</p>
<p>Other documents referenced here may also bind Customer’s use of the Services, including the Subscription Plans page and the Community Guidelines below.</p>
<ol>
<li>Service Availability and Quality</li>
</ol>
<p>We are constantly improving the Services to make them better. The Services are subject to modification and change, including but not limited to the art style of Assets, the algorithms used to generate the Assets, and features available to the Customer. No guarantees are made with respect to the Services’ quality, stability, uptime or reliability. Please do not create any dependencies on any attributes of the Services or the Assets. We will not be liable to You or Your downstream customers for any harm caused by Your dependency on the Service.</p>
<p>Both the Services and the Assets are provided to Customer on an &quot;AS IS&quot; BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied, including, without limitation, any warranties or conditions of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A PARTICULAR PURPOSE. You are solely responsible for determining the appropriateness of using or redistributing the Assets and assume any risks associated with use of the Services.</p>
<p>Midjourney reserves the right to suspend or ban Your access to the Services at any time, and for any reason. You may not use the Services for purposes of developing or offering competitive products or services. You may not reverse engineer the Services or the Assets. You may not use automated tools to access, interact with, or generate Assets through the Services. You may not resell or redistribute the Services or access to the Service. Only one user may use the Services per registered account. Each user of the Services may only have one account.</p>
<p>You may not use the Service to try to violate the intellectual property rights of others, including copyright, patent, or trademark rights. Doing so may subject you to penalties including legal action or a permanent ban from the Service.</p>
<p>We reserve the right to investigate complaints or reported violations of our Terms of Service and to take any action we deem appropriate including but not limited to reporting any suspected unlawful activity to law enforcement officials, regulators, or other third parties and disclosing any information necessary or appropriate to such persons or entities relating to user profiles, e-mail addresses, usage history, posted materials, IP addresses and traffic information.</p>
<ol start="2">
<li>Age Requirements</li>
</ol>
<p>By accessing the Services, You confirm that You are at least 13 years old and meet the minimum age of digital consent in Your country. If You are old enough to access the Services in Your country, but not old enough to have authority to consent to our terms, Your parent or guardian must agree to our terms on Your behalf.</p>
<p>Please ask Your parent or guardian to read these terms with You. If You are a parent or legal guardian, and You allow Your teenager to use the Services, then these terms also apply to You and You are responsible for Your teenager’s activity on the Services.</p>
<p>Midjourney tries to make its Services PG-13 and family friendly, but the Assets are generated by an artificial intelligence system based on user queries. This is new technology and it does not always work as expected. No guarantees are made as to the suitability of the Assets for the Customer.</p>
<ol start="3">
<li>Your Information</li>
</ol>
<p>By using the Services, You may provide Midjourney with personal information like Your email address, user name, billing information, favorites, image outputs, and text prompts that You enter, or sample images that You upload to the Service. Our privacy policy can be found here.</p>
<ol start="4">
<li>Content Rights
Your Rights</li>
</ol>
<p>You own all Assets You create with the Services to the fullest extent possible under applicable law. There are some exceptions:</p>
<p>Your ownership is subject to any obligations imposed by this Agreement and the rights of any third-parties.
If you are a company or any employee of a company with more than $1,000,000 USD a year in revenue, you must be subscribed to a “Pro” or “Mega” plan to own Your Assets.
If you upscale the images of others, these images remain owned by the original creators.</p>
<p>Please consult Your own lawyer if You want more information about the state of current intellectual property law in Your jurisdiction. Your ownership of the Assets you created persists even if in subsequent months You downgrade or cancel Your membership.</p>
<p>Rights You give to Midjourney</p>
<p>By using the Services, You grant to Midjourney, its successors, and assigns a perpetual, worldwide, non-exclusive, sublicensable no-charge, royalty-free, irrevocable copyright license to reproduce, prepare derivative works of, publicly display, publicly perform, sublicense, and distribute text and image prompts You input into the Services, as well as any Assets produced by You through the Service. This license survives termination of this Agreement by any party, for any reason.</p>
<p>Remixing and Stealth Mode</p>
<p>Please note: Midjourney is an open community which allows others to use and remix Your images and prompts whenever they are posted in a public setting. By default, Your images are publically viewable and remixable. As described below, You grant Midjourney a license to allow this.</p>
<p>If You purchased the Stealth feature as part of Your “Pro” or &quot;Mega&quot; subscription or through the previously available add-on, we agree to make best efforts not to publish any Assets You make in any situation where you have engaged stealth mode in the Services.</p>
<p>Please be aware that any image You make in a shared or open space, such as a Discord chatroom, is viewable by anyone in that chatroom, regardless of whether Stealth mode is engaged.</p>
<ol start="5">
<li>DMCA and Takedowns Policy</li>
</ol>
<p>Notification Procedures</p>
<p>We respect the intellectual property rights of others. If you believe that material located on or linked to by the Services violates your copyright or trademark, please send a notice of claimed infringement to takedown@midjourney.com with the subject “Takedown Request,” and include the following:</p>
<p>Your physical or electronic signature.
Identification of the copyrighted work (or mark) you believe to have been infringed or, if the claim involves multiple works, a representative list of such works.
Identification of the material you believe to be infringing in a sufficiently precise and detailed manner to allow us to locate that material.
Adequate information by which we can contact you (including your name, postal address, telephone number, and, if available, email address).
A statement that you have a good faith belief that use of the copyrighted material is not authorized by the copyright owner, its agent, or the law.
A statement that the information in the written notice is accurate.
A statement, under penalty of perjury, that you are authorized to act on behalf of the copyright owner.
If the copyright owner’s rights arise under the laws of a country other than the United States, please identify the country.</p>
<p>You may also send notices containing the above-required information to the following Address:</p>
<p>Midjourney, Inc.
Attn: Takedowns Department
611 Gateway Blvd. Ste 120
South San Francisco, CA, 94080-7066,
US</p>
<p>Upon receipt of a notice that complies with the foregoing, we reserve the right to remove or disable access to the accused material or disable any links to the material; notify the party accused of infringement that we have removed or disabled access to the identified material; and terminate access to and use of the Services for any user who engages in repeated acts of infringement.</p>
<p>Please be aware that if you knowingly misrepresent that material or activity on the Services is infringing your copyright, you may be held liable for damages (including costs and attorneys’ fees) under Section 512(f) of the DMCA.</p>
<p>Counter-Notification Procedures</p>
<p>If you believe that material was removed or access to it was disabled by mistake or misidentification, you may file a counter-notification with us by submitting a written notification to our copyright agent designated above. Such notification must include substantially the following:</p>
<p>Your physical or electronic signature.
An identification of the material that has been removed or to which access has been disabled and the location at which the material appeared before it was removed or access disabled.
Adequate information by which we can contact you (including your name, postal address, telephone number, and, if available, email address).
A statement under penalty of perjury by you that you have a good faith belief that the material identified above was removed or disabled as a result of a mistake or misidentification of the material to be removed or disabled.
A statement that you will consent to the jurisdiction of the Federal District Court for the judicial district in which your address is located (or if you reside outside the United States for any judicial district in which the Services may be found) and that you will accept service from the person (or an agent of that person) who provided us with the complaint at issue.
Our designated agent to receive counter notices is the same as the agent shown above.
The DMCA allows us to restore the removed content within 10-14 business days unless the complaining party initiates a court action against you during that time period and notifies us of the same.
Please be aware that if you knowingly materially misrepresent that material or activity on the Services was removed or disabled by mistake or misidentification, you may be held liable for damages (including costs and attorney’s; fees) under Section 512(f) of the DMCA.
6. Dispute Resolution and Governing Law</p>
<p>In the event a dispute, controversy, or claim arises out of or relating to these Terms (“Dispute”), the Dispute will be resolved by binding arbitration rather than in court. The parties will first try in good faith to settle any Dispute within 30 days after the Dispute arises. If the Dispute is not resolved within 30 days, it shall be resolved by arbitration by the American Arbitration Association’s International Centre for Dispute Resolution in accordance with its Expedited Commercial Rules in force as of the date of this Agreement (&quot;Rules&quot;). The parties will mutually select one arbitrator. The arbitration will be conducted in English in Santa Clara County, California, USA. Either party may apply to any competent court for injunctive relief necessary to protect its rights pending resolution of the arbitration. The arbitrator may order equitable or injunctive relief consistent with the remedies and limitations in the Agreement. The arbitral award will be final and binding on the parties and its execution may be presented in any competent court, including any court with jurisdiction over either party or any of its property.
Each party will bear its own lawyers’ and experts’ fees and expenses, regardless of the arbitrator’s final decision regarding the Dispute.</p>
<ol start="7">
<li>Unlimited Service and Rate Limiting</li>
</ol>
<p>If You purchase an unlimited plan, we will try to reasonably offer You unlimited access to the Services. However, we reserve the right to rate limit You to prevent quality decay or interruptions to other customers.</p>
<ol start="8">
<li>Payment and Billing</li>
</ol>
<p>We may invoice You for Your use of the Services through a third party payment service provider. The third party service provider’s terms of service shall govern and supersede this Agreement in case of conflict.</p>
<p>You are free to cancel Your plan at any time. We also reserve the right to terminate Your access to the Service for any reason, including for violation of the Community Guidelines or other inappropriate use of the Service. Any violation of Community Guidelines is a breach of this Agreement. You will not be refunded for the current subscription period, but You will not be charged after the current subscription period has ended. Additional information and policies regarding our subscription plans, including details on billing, taxes, refunds, and authorization, can be found here.</p>
<ol start="9">
<li>Community Guidelines
Be kind and respect each other and staff. Do not create images or use text prompts that are inherently disrespectful, aggressive, hateful, or otherwise abusive. Violence or harassment of any kind will not be tolerated.
No adult content or gore. Please avoid making visually shocking or disturbing content. We will block some text inputs automatically.
Respect others’ creations. Do not distribute or publicly repost the creations of others without their permission.
You may not use the Services to generate images for political campaigns, or to try to influence the outcome of an election.
You may not use the Services or the Assets to attempt to or to actually deceive or defraud anyone.
You may not use the Services for illegal activity nor may you upload images to our servers that involve illegal activity, or where the uploading itself may be illegal.
You may not intentionally mislead recipients of the Assets about their nature or source.
Respect others’ rights. Do not upload others’ private information.
Be careful about sharing. It’s OK to share Your creations outside of the Midjourney community but please consider how others might view Your content.
Banhammer. Any violations of these rules may lead to bans from our services. We are not a democracy. Behave respectfully or lose Your rights to use the Service.</li>
<li>Limitation of Liability and Indemnity</li>
</ol>
<p>We provide the service as is, and we make no promises or guarantees about it.</p>
<p>You understand and agree that we will not be liable to You or any third party for any loss of profits, use, goodwill, or data, or for any incidental, indirect, special, consequential or exemplary damages, however they arise.</p>
<p>You are responsible for Your use of the service. If You harm someone else or get into a dispute with someone else, we will not be involved.</p>
<p>If You knowingly infringe someone else’s intellectual property, and that costs us money, we’re going to come find You and collect that money from You. We might also do other stuff, like try to get a court to make You pay our legal fees. Don’t do it.</p>
<ol start="11">
<li>Miscellaneous
Force Majeure. Neither party will be liable for failure or delay in performance to the extent caused by circumstances beyond its reasonable control, including acts of God, natural disasters, terrorism, riots, or war.
No Agency. This Agreement does not create any agency, partnership, or joint venture between the parties.
Severability. If any part of this Agreement is invalid, illegal, or unenforceable, the rest of the Agreement will remain in effect.
No Third-Party Beneficiaries. This Agreement does not confer any benefits on any third party unless it expressly states that it does.
Survival. The sections and obligations in this Agreement that a reasonable person would expect to survive this agreement, will. Particularly the IP and privacy stuff.
Governing Law. These Terms shall be governed by the laws of the State of California, USA, without reference to conflict of law rules. All disputes will be governed by the arbitration agreement above.
Previous
The Midjourney Web App
Next
Midjourney Trademark Policy
Midjourney is an independent research lab exploring new mediums of thought and expanding the imaginative powers of the human species. We are a small self-funded team focused on design, human infrastructure, and AI.
FOLLOW US: [F] [T] [R]
SUPPORT
For questions or support visit the Midjourney Discord support channels.
SITES
Midjourney Website
Midjourney Discord
Current
Legacy
Getting Started
Using Discord
Commands Parameters and Tools
Writing Prompts
Working with Your Own Images
Styles and Aesthetics
Image Size and Aspect Ratio
Advanced Prompting Tools
Subscriptions
Subscription Plans
Stealth Mode
Earn Free Hours
Fast, Relax, &amp; Turbo Modes
Contacting Support
Using The Website
Policies
Contacting Support
Light
Most issues can be resolved without needing manual intervention. If you're still stuck, follow the instructions below to get help.
Self-Help</li>
</ol>
<p>Use the /ask command to get answers to many common support questions.</p>
<p>Tech Support and General Questions</p>
<p>We provide support directly within Discord if you have an issue with the Midjourney Bot or Midjourney.com website. You can find help in the #support channel, just above the newbies rooms. Volunteers and helpful community members run this channel and can't assist with billing issues.</p>
<p>Billing Support
Billing and Subscription Help</p>
<p>For billing and subscription questions, please visit help.midjourney.com.</p>
<p>Previous
Fast, Relax, &amp; Turbo Modes
Next
The Midjourney Web App
Midjourney is an independent research lab exploring new mediums of thought and expanding the imaginative powers of the human species. We are a small self-funded team focused on design, human infrastructure, and AI.
FOLLOW US: [F] [T] [R]
SUPPORT
For questions or support visit the Midjourney Discord support channels.
SITES
Midjourney Website
Midjourney Discord
Current
Legacy
Getting Started
Using Discord
Commands Parameters and Tools
Writing Prompts
Working with Your Own Images
Styles and Aesthetics
Image Size and Aspect Ratio
Advanced Prompting Tools
Subscriptions
Subscription Plans
Stealth Mode
Earn Free Hours
Fast, Relax, &amp; Turbo Modes
Contacting Support
Using The Website
Policies
Fast, Relax, &amp; Turbo Modes
Light
Midjourney uses powerful Graphics Processing Units (GPUs) to interpret and process each prompt. When you purchase a subscription to Midjourney, you are purchasing time on these GPUs.</p>
<p>Different subscription plans have different amounts of monthly GPU time.
This monthly subscription GPU time is Fast Mode time. Fast Mode tries to give you access to a GPU instantly. It's the default processing tier and uses your subscription's monthly GPU time.</p>
<p>How many GPU minutes do my generations cost?</p>
<p>The Average Job the Midjourney bot processes takes about one minute of GPU time to finish creating an image. Upscaling an image, using nonstandard aspect ratios, or older Midjourney Model Versions may take more time. Creating variations or using lower quality values will take less time.</p>
<p>A Job's time depends on the following factors:</p>
<pre><code>+ Lower Time	++ Average Time	+++ Higher Time
</code></pre>
<p>Job Type	Variations	/imagine	Upscale
Aspect Ratio	default (square)	tall or wide	
Quality Parameter	--q 0.25 or --q 0.5	default (--q 1)	--q 2 (for legacy Model Versions
Stop Parameter	--stop 10–--stop 99	default (--stop 100)	</p>
<p>Use /info before and after running a process to see how many of your remaining GPU minutes the generation used.</p>
<p>Fast vs. Relax Mode</p>
<p>Subscribers to the Standard, Pro, and Mega plans can create an unlimited number of images each month in Relax Mode. Relax Mode will not cost any GPU time, but Jobs will be placed into a queue based on how much you've used the system.</p>
<p>How long do I need to wait in Relax mode?</p>
<p>Jobs in Relax mode are placed in a queue to be processed as GPUs become available. Wait times for Relax are dynamic but generally range between 0–10 minutes per job. If you use Relax mode occasionally, you will have shorter wait times compared to subscribers that have used it more. This priority currently resets whenever you renew your monthly subscription.</p>
<p>Limitations</p>
<p>Permutation prompts, the --repeat parameter and the Legacy upscaler, Max Upscale are not available while useing Relax mode.</p>
<p>Run Out of Fast Time?</p>
<p>You can purchase more Fast Hours on your Midjourney.com/accounts page.</p>
<p>By default, images are generated using Fast Mode.
Unused monthly Fast GPU time does not roll over.
Fast Mode is automatically reactivated when the subscription renews.</p>
<p>Turbo Mode</p>
<p>Turbo Mode is available for subscribers who want extremely quick image generation. Turbo mode uses a high-speed experimental GPU pool. Jobs run in Turbo mode generate up to four times faster but consume twice as many subscription GPU minutes as a typical Fast Mode Job.</p>
<p>Turbo mode is only available with Midjourney Model Versions 5, 5.1, and 5.2.</p>
<p>If Turbo mode is selected, but the GPUs are unavailable, or it is incompatible with the selected model version, your job will run in Fast Mode instead.</p>
<p>Turbo Mode is an experimental feature, and availability and price may change at any time.</p>
<p>How to Switch Between Modes
Use the /fast /turbo or /relax command</p>
<p>Standard, Pro, and Mega plan subscribers can use the /fast or /relax commands to switch between modes.</p>
<p>Use the Settings Command</p>
<p>Use the /settings command and choose 🐢 Relax 🐇 Fast or ⚡ Turbo from the menu.</p>
<p>Use the --fast --turbo or --relax parameter</p>
<p>Standard, Pro, and Mega plan subscribers can use the --relax, --fast, or --turbo parameters at the end of a prompt to run a single job using Relax, Fast, or Turbo mode instead of changing their preferred setting.</p>
<p>Previous
Earn Free Hours
Next
Contacting Support
Midjourney is an independent research lab exploring new mediums of thought and expanding the imaginative powers of the human species. We are a small self-funded team focused on design, human infrastructure, and AI.
FOLLOW US: [F] [T] [R]
SUPPORT
For questions or support visit the Midjourney Discord support channels.
SITES
Midjourney Website
Midjourney Discord
Current
Legacy
Getting Started
Using Discord
Commands Parameters and Tools
Writing Prompts
Working with Your Own Images
Styles and Aesthetics
Image Size and Aspect Ratio
Advanced Prompting Tools
Subscriptions
Subscription Plans
Stealth Mode
Earn Free Hours
Fast, Relax, &amp; Turbo Modes
Contacting Support
Using The Website
Policies
Stealth Mode
Light
Midjourney is an open-by-default community, and all image generations are visible at midjourney.com, including images created in private discord servers, direct messages, and on the Midjourney web app.</p>
<p>Subscribers to the Pro and Mega plans have access to Stealth Mode. Stealth mode prevents your images from being visible to others on the Midjourney website. Use the /stealth and /public commands toggle between Stealth and Public mode.</p>
<p>Stealth Mode Only Prevents Others From Viewing Your Images on Midjourney.com!
Images generated in public channels are always visible to other users, even when using Stealth Mode. To prevent others from seeing an image you create using Stealth Mode, generate images in your Direct Messages or on a private Discord server.</p>
<p>If you cancel or downgrade your Midjourney subscription, previously created Stealth Mode images will remain unpublished.</p>
<p>How to Switch to Stealth Mode
Use the /stealth command in Discord.</p>
<p>Use the /settings command in Discord.</p>
<p>Use the /settings command to adjust your settings. The 🧍‍♂️Public button can be used to toggle between Stealth and Public mode.</p>
<p>When the 🧍‍♂️Public button is green, you are making images in Public mode.</p>
<p>When the 🧍‍♂️Public button is gray, you are making images in Stealth mode.</p>
<p>Select an image and Unpublish on the Web.</p>
<p>Unpublish an image on the Midjourney website when viewing one of your images in lightbox mode by selecting the ... next to your image and then clicking unpublish.</p>
<p>Use /Info to check if you are in Stealth Mode</p>
<p>Use the /info command to see information about your account. Visibility shows whether you are currently in Public or Stealth Mode.</p>
<p>Previous
Subscription Plans
Next
Earn Free Hours
Midjourney is an independent research lab exploring new mediums of thought and expanding the imaginative powers of the human species. We are a small self-funded team focused on design, human infrastructure, and AI.
FOLLOW US: [F] [T] [R]
SUPPORT
For questions or support visit the Midjourney Discord support channels.
SITES
Midjourney Website
Midjourney Discord
Current
Legacy
Getting Started
Using Discord
Commands Parameters and Tools
Writing Prompts
Working with Your Own Images
Styles and Aesthetics
Image Size and Aspect Ratio
Advanced Prompting Tools
Permutation Prompts
Remaster
Repeat
Seeds
Stop
Extra Features
Tile
Video
Subscriptions
Using The Website
Policies
Video
Light
Use the --video parameter to create a short movie of your initial image grid being generated. React to the finished job with the envelope ✉️ emoji to have the Midjourney Bot send a link to the video to your Direct Messages.</p>
<p>--video only works on image grids, not upscales.
--video works with Model Versions 5, 5.1, 5.2, and niji 5.
--video works with Legacy Model Versions 1, 2, 3, test, and testp.</p>
<p>Video Examples</p>
<p>Vibrant California Poppies</p>
<p>Botanical Sketch of Fanciful Ferns</p>
<p>How to Get a Video Link</p>
<p>Prompt example: /imagine prompt Vibrant California Poppies --video</p>
<p>1 Add --video to the end of your prompt.</p>
<p>2 Once the Job has finished, click Add Reaction</p>
<p>3 Select the ✉️ Envelope emoji.</p>
<p>4 The Midjourney bot will send a link to the video to your Direct Messages.</p>
<p>5 Click the link to view your video within a browser. Right-click or Long Press to download the video.</p>
<p>How to Use the Video Parameter</p>
<p>Add --video to the end of your prompt.</p>
<p>Previous
Tile
Next
Subscription Plans
Midjourney is an independent research lab exploring new mediums of thought and expanding the imaginative powers of the human species. We are a small self-funded team focused on design, human infrastructure, and AI.
FOLLOW US: [F] [T] [R]
SUPPORT
For questions or support visit the Midjourney Discord support channels.
SITES
Midjourney Website
Midjourney Discord
Current
Legacy
Getting Started
Using Discord
Commands Parameters and Tools
Writing Prompts
Working with Your Own Images
Styles and Aesthetics
Image Size and Aspect Ratio
Advanced Prompting Tools
Subscriptions
Subscription Plans
Stealth Mode
Earn Free Hours
Fast, Relax, &amp; Turbo Modes
Contacting Support
Using The Website
Policies
Subscription Plans
Light
Midjourney has four subscription tiers. Pay month-to-month or for the entire year for a 20% discount. Each subscription plan includes access to the Midjourney member gallery, the official Discord, general commercial usage terms, and more.
How to Subscribe</p>
<p>Use the /subscribe command to generate a personal link to the subscription page.
Or, go to Midjourney.com/account.
Or, select Manage Sub from the sidebar while logged into the Midjourney website.</p>
<p>Billing and Subscription Help</p>
<p>For billing and subscription questions, please visit help.midjourney.com.</p>
<p>Plan Comparison
Basic Plan	Standard Plan	Pro Plan	Mega Plan
Monthly Subscription Cost	$10	$30	$60	$120
Annual Subscription Cost	$96
($8 / month)	$288
($24 / month)	$576
($48 / month)	$1152
($96 / month)
Fast GPU Time	3.3 hr/month	15 hr/month	30 hr/month	60 hr/month
Relax GPU Time	-	Unlimited	Unlimited	Unlimited
Purchase Extra
GPU Time	$4/hr	$4/hr	$4/hr	$4/hr
Work Solo In Your
Direct Messages	✓	✓	✓	✓
Stealth Mode	-	-	✓	✓
Maximum Concurrent Jobs	3 Jobs
10 Jobs waiting in queue	3 Jobs
10 Jobs waiting in queue	12 Fast Jobs
3 Relaxed Jobs
10 Jobs in queue	12 Fast Jobs
3 Relaxed Jobs
10 Jobs in queue
Rate Images to Earn Free GPU Time	✓	✓	✓	✓
Usage Rights	General Commercial Terms*	General Commercial Terms*	General Commercial Terms*	General Commercial Terms*
If you have subscribed at any point, you are free to use your images in just about any way you want. You must purchase the Pro or Mega plan if you are a company making more than $1,000,000 USD in gross revenue per year. For complete details, please see the Terms of Service
Subscribe to a Plan</p>
<p>Go to https://www.midjourney.com/account/ or use the /subscribe command to generate a link to the subscription page.</p>
<p>Don't Share Your Link</p>
<p>The generated link is unique for each account and should never be shared with others!</p>
<p>Payment Methods</p>
<p>Only payment methods supported by Stripe are currently accepted: credit or debit cards issued by services like Mastercard, VISA, or American Express. Stripe is a PCI Service Provider Level 1, the most stringent level of certification available in the payments industry.
Google Pay, Apple Pay, and Cash App Pay are available in some regions.
PayPal, wire transfer, and similar methods are not supported.</p>
<p>Manage Your Plan</p>
<p>Manage your subscription plan at https://www.midjourney.com/account/.</p>
<p>Monthly Renewal
Unused monthly Fast GPU time does not roll over.
Fast mode is automatically reactivated when the subscription renews.</p>
<p>Switch Plans</p>
<p>Upgrade your plan at any time. When upgrading, you may choose whether the upgrade should be effective immediately or at the end of the current billing cycle. If you choose to upgrade immediately, you will be offered a prorated price based on the usage of the plan you are upgrading from. Downgrades are always effective at the end of the current billing cycle.</p>
<p>Proration?</p>
<p>Unused Fast GPU Time is credited towards your account when you choose to upgrade immediately. If a Basic subscriber upgrades immediately without using any GPU time, they receive a $10 credit towards their new plan. If a Standard subscriber upgrades after using 50% of their Fast GPU time, they receive a $15 credit towards their new plan.</p>
<p>Cancellation</p>
<p>Go to https://www.midjourney.com/account/ to cancel a subscription at any time. Cancellations are effective at the end of the current billing cycle. Subscription benefits like access to the community gallery and the bulk download tool are available until the end of the current billing cycle.</p>
<p>Your generated images and Midjourney.com/account page are not deleted if you cancel your subscription. You can resubscribe to a plan at any time by visiting https://www.midjourney.com/account/.</p>
<p>Refunds</p>
<p>Refunds are available for subscribers who have lifetime usage of less than 20 GPU minutes, including time used in Relax Mode. If you are eligible for a refund, the dialog box will automatically pop up when you click to cancel your account.</p>
<p>Buy More Fast Hours</p>
<p>Run out of Fast GPU and want to buy more before your monthly subscription renews? You can purchase additional Fast GPU hours for $4/hr. Hourly Prices are experimental and subject to change.</p>
<p>Purchase more Fast GPU hours on your Midjourney account page.</p>
<p>Purchased Fast GPU hours do not expire, but you must have an active subscription to use them.</p>
<p>Account Information</p>
<p>Use the /info command to view information about your account, including how much Fast time is left for the subscription period, lifetime usage stats, the number of queued or running jobs, and the subscription renewal date.</p>
<p>Previous
Video
Next
Stealth Mode
Midjourney is an independent research lab exploring new mediums of thought and expanding the imaginative powers of the human species. We are a small self-funded team focused on design, human infrastructure, and AI.
FOLLOW US: [F] [T] [R]
SUPPORT
For questions or support visit the Midjourney Discord support channels.
SITES
Midjourney Website
Midjourney Discord
Current
Legacy
Getting Started
Using Discord
Commands Parameters and Tools
Writing Prompts
Working with Your Own Images
Styles and Aesthetics
Image Size and Aspect Ratio
Advanced Prompting Tools
Permutation Prompts
Remaster
Repeat
Seeds
Stop
Extra Features
Tile
Video
Subscriptions
Using The Website
Policies
Tile
Light
The --tile parameter generates images that can be used as repeating tiles to create seamless patterns for fabrics, wallpapers and textures.</p>
<p>--tile works with Model Versions 1, 2, 3, test, testp, 5, 5.1, and 5.2 .
--tile only generates a single tile. Use a pattern making tool like this Seamless Pattern Checker to see the tile repeat.</p>
<p>Tile Examples
prompt scribble of moss on rocks --tile
prompt watercolor koi --tile
How to Use the Tile Parameter</p>
<p>Add --tile to the end of your prompt.</p>
<p>Previous
Stop
Next
Video
Midjourney is an independent research lab exploring new mediums of thought and expanding the imaginative powers of the human species. We are a small self-funded team focused on design, human infrastructure, and AI.
FOLLOW US: [F] [T] [R]
SUPPORT
For questions or support visit the Midjourney Discord support channels.
SITES
Midjourney Website
Midjourney Discord
Current
Legacy
Getting Started
Using Discord
Commands Parameters and Tools
Writing Prompts
Working with Your Own Images
Styles and Aesthetics
Image Size and Aspect Ratio
Advanced Prompting Tools
Permutation Prompts
Remaster
Repeat
Seeds
Stop
Extra Features
Subscriptions
Using The Website
Policies
Stop
Light
Use the --stop parameter to finish a Job partway through the process. Stopping a Job at an earlier percentage can create blurrier, less detailed results.</p>
<p>--stop accepts values: 10–100.
The default --stop value is 100.
--stop does not work while Upscaling.</p>
<p>Stop Comparison</p>
<p>prompt example: /imagine prompt splatter art painting of acorns --stop 90</p>
<p>--stop 10</p>
<p>--stop 20</p>
<p>--stop 30</p>
<p>--stop 40</p>
<p>--stop 50</p>
<p>--stop 60</p>
<p>--stop 70</p>
<p>--stop 80</p>
<p>--stop 90</p>
<p>--stop 100</p>
<p>How to Change the Stop Percentage
Use the --stop Parameter</p>
<p>Add --stop <value> to the end of your prompt.</p>
<p>Previous
Seeds
Next
Tile
Midjourney is an independent research lab exploring new mediums of thought and expanding the imaginative powers of the human species. We are a small self-funded team focused on design, human infrastructure, and AI.
FOLLOW US: [F] [T] [R]
SUPPORT
For questions or support visit the Midjourney Discord support channels.
SITES
Midjourney Website
Midjourney Discord
Current
Legacy
Getting Started
Using Discord
Commands Parameters and Tools
Writing Prompts
Working with Your Own Images
Styles and Aesthetics
Image Size and Aspect Ratio
Advanced Prompting Tools
Permutation Prompts
Remaster
Repeat
Seeds
Stop
Extra Features
Subscriptions
Using The Website
Policies
Repeat
Light
The --repeat or --r parameter runs a Job multiple times. Combine --repeat with other parameters, like --chaos to increase the pace of your visual exploration.</p>
<p>--repeat accepts values 2–4 for Basic subscribers
--repeat accepts values 2–10 for Standard subscribers
--repeat accepts values 2–40 for Pro and Mega subscribers
The --repeat parameter can only be used in Fast and Turbo GPU mode.
Using the redo (re-roll) 🔄 button on the results of a --repeat Job will only re-run the prompt once.</p>
<p>Use the --repeat or --r Parameter</p>
<p>Add --repeat <value> or --r <value> to the end of your prompt.</p>
<p>Previous
Remaster
Next
Seeds
Midjourney is an independent research lab exploring new mediums of thought and expanding the imaginative powers of the human species. We are a small self-funded team focused on design, human infrastructure, and AI.
FOLLOW US: [F] [T] [R]
SUPPORT
For questions or support visit the Midjourney Discord support channels.
SITES
Midjourney Website
Midjourney Discord
Current
Legacy
Getting Started
Using Discord
Commands Parameters and Tools
Writing Prompts
Working with Your Own Images
Styles and Aesthetics
Image Size and Aspect Ratio
Advanced Prompting Tools
Permutation Prompts
Remaster
Repeat
Seeds
Stop
Extra Features
Subscriptions
Using The Website
Policies
Seeds
Light
The Midjourney bot uses a seed number to create a field of visual noise, like television static, as a starting point to generate the initial image grids. Seed numbers are generated randomly for each image but can be specified with the --seed parameter. If you use the same seed number and prompt, you will get similar final images.
--seed accepts whole numbers 0–4294967295.
--seed values only influence the initial image grid.
Identical --seed values using Model Versions 1, 2, 3, test, and testp will produce images with similar composition, color, and details.
Identical --seed values using Model Versions 4, 5, and niji will produce nearly identical images.
Seed numbers are not static and should not be relied upon between sessions.
Seed Parameter</p>
<p>If no Seed is specified, Midjourney will use a randomly generated seed number, producing a wide variety of options each time a prompt is used.</p>
<p>Jobs run three times with random seeds:</p>
<p>prompt example: /imagine prompt celadon owl pitcher</p>
<p>Jobs run two times with --seed 123:</p>
<p>prompt example: /imagine prompt celadon owl pitcher --seed 123</p>
<p>How to Find a Job's Seed Number
Use a Discord Emoji Reaction</p>
<p>Find the seed number of a Job in discord by reacting with an ✉️ envelope emoji to a Job.</p>
<p>Use The Show Command to Bring Back Old Jobs</p>
<p>To get the seed number for a past image, copy the job ID and use the /show &lt;Job ID #&gt; command with that ID to revive the Job. You can then react to the newly regenerated Job with an ✉️ envelope emoji.</p>
<p>How To Change Seed Numbers
Use the --seed Parameter</p>
<p>Add --seed <value> to the end of your prompt.</p>
<p>Previous
Repeat
Next
Stop
Midjourney is an independent research lab exploring new mediums of thought and expanding the imaginative powers of the human species. We are a small self-funded team focused on design, human infrastructure, and AI.
FOLLOW US: [F] [T] [R]
SUPPORT
For questions or support visit the Midjourney Discord support channels.
SITES
Midjourney Website
Midjourney Discord
Current
Legacy
Getting Started
Using Discord
Commands Parameters and Tools
Writing Prompts
Working with Your Own Images
Styles and Aesthetics
Image Size and Aspect Ratio
Advanced Prompting Tools
Permutation Prompts
Remaster
Repeat
Seeds
Stop
Extra Features
Subscriptions
Using The Website
Policies
Remaster
Light
Remaster</p>
<p>The Remaster feature is available for upscaled images generated using earlier Midjourney Model Versions. It creates a new grid of images using the latest Midjourney Model, combining the original image's composition with the coherency of the newest Midjourney Model Version.</p>
<p>Remaster any previously upscaled job by clicking the 🆕 Remaster button beneath the original upscale.</p>
<p>To Remaster very old jobs, use the /show command to refresh that job in Discord.</p>
<p>prompt example: /imagine prompt Byzantine sandcastle cathedral --v 2</p>
<p>Original Model Version 2 Image
Remastered with Model Version 5.2
Previous
Permutation Prompts
Next
Repeat
Midjourney is an independent research lab exploring new mediums of thought and expanding the imaginative powers of the human species. We are a small self-funded team focused on design, human infrastructure, and AI.
FOLLOW US: [F] [T] [R]
SUPPORT
For questions or support visit the Midjourney Discord support channels.
SITES
Midjourney Website
Midjourney Discord
Current
Legacy
Getting Started
Using Discord
Commands Parameters and Tools
Writing Prompts
Working with Your Own Images
Styles and Aesthetics
Image Size and Aspect Ratio
Advanced Prompting Tools
Permutation Prompts
Remaster
Repeat
Seeds
Stop
Extra Features
Subscriptions
Using The Website
Policies
Permutation Prompts
Light
Permutation Prompts allow you to quickly generate variations of a Prompt with a single /imagine command. By including lists of options separated with commas , within curly braces {} in your prompt, you can create multiple versions of a prompt with different combinations of those options.</p>
<p>Basic Subscribers can create a maximum of 4 Jobs with a single Permutation Prompt.
Standard Subscribers can create a maximum of 10 Jobs with a single Permutation Prompt.
Pro and Mega Subscribers can create a maximum of 40 Jobs with a single Permutation Prompt.</p>
<p>You can use Permutation Prompts to create combinations and permutations involving any part of a Midjourney Prompt, including text, image prompts, parameters, or prompt weights.
Permutation prompts are only available while using Fast mode.</p>
<p>Permutation Prompt Basics</p>
<p>Separate your list of options within curly brackets {} to quickly create and process multiple prompt variations.</p>
<p>Prompt Example:
/imagine prompt a {red, green, yellow} bird creates and processes three Jobs.</p>
<p>/imagine prompt a red bird
/imagine prompt a green bird
/imagine prompt a yellow bird</p>
<p>GPU Minutes</p>
<p>The Midjourney Bot processes each Permutation Prompt variation as an individual Job. Each Job consumes GPU minutes.</p>
<p>Permutation Prompts will show a confirmation message before they begin processing.</p>
<p>Permutation Prompt Examples
Prompt Text Variations</p>
<p>The prompt /imagine prompt a naturalist illustration of a {pineapple, blueberry, rambutan, banana} bird will create and process four Jobs:</p>
<p>a naturalist illustration of a pineapple bird</p>
<p>a naturalist illustration of a blueberry bird</p>
<p>a naturalist illustration of a rambutan bird</p>
<p>a naturalist illustration of a banana bird</p>
<p>Prompt Parameter Variations</p>
<p>The prompt /imagine prompt a naturalist illustration of a fruit salad bird --ar {3:2, 1:1, 2:3, 1:2} will create and process four Jobs with different aspect ratios:</p>
<p>a naturalist illustration of a fruit salad bird --ar 3:2</p>
<p>a naturalist illustration of a fruit salad bird --ar 1:1</p>
<p>a naturalist illustration of a fruit salad bird --ar 2:3</p>
<p>a naturalist illustration of a fruit salad bird --ar 1:2</p>
<p>The prompt /imagine prompt a naturalist illustration of a fruit salad bird --{v 5, niji, test} will create and process three Jobs using different Midjourney Model Versions:</p>
<p>a naturalist illustration of a fruit salad bird --v 5</p>
<p>a naturalist illustration of a fruit salad bird --niji</p>
<p>a naturalist illustration of a fruit salad bird --test</p>
<p>Multiple and Nested Permutations</p>
<p>It is possible to use multiple sets of bracketed options in a single prompt.
/imagine prompt a {red, green} bird in the {jungle, desert} creates and processes four Jobs.</p>
<p>/imagine prompt a red bird in the jungle
/imagine prompt a red bird in the desert
/imagine prompt a green bird in the jungle
/imagine prompt a green bird in the desert</p>
<p>It is also possible to nest sets of bracketed options inside other sets of brackets within a single prompt:</p>
<p>Example: /imagine prompt A {sculpture, painting} of a {seagull {on a pier, on a beach}, poodle {on a sofa, in a truck}}.</p>
<p>/imagine prompt A sculpture of a seagull on a pier.
/imagine prompt A sculpture of a seagull on a beach.
/imagine prompt A sculpture of a poodle on a sofa.
/imagine prompt A sculpture of a poodle in a truck.
/imagine prompt A painting of a seagull on a pier.
/imagine prompt A painting of a seagull on a beach.
/imagine prompt A painting of a poodle on a sofa.
/imagine prompt A painting of a poodle in a truck.</p>
<p>Escape Character</p>
<p>If you want to include a , within the curly brackets that does not act as a separator place a backslash \ directly before it.</p>
<p>imagine prompt {red, pastel, yellow} bird produces three Jobs
/imagine prompt a red bird
/imagine prompt a pastel bird
/imagine prompt a yellow bird</p>
<p>imagine prompt {red, pastel , yellow} bird produces two Jobs
/imagine prompt a red bird
/imagine prompt a pastel, yellow bird</p>
<p>Previous
Advanced Prompting Tools
Next
Remaster
Midjourney is an independent research lab exploring new mediums of thought and expanding the imaginative powers of the human species. We are a small self-funded team focused on design, human infrastructure, and AI.
FOLLOW US: [F] [T] [R]
SUPPORT
For questions or support visit the Midjourney Discord support channels.
SITES
Midjourney Website
Midjourney Discord
Current
Legacy
Getting Started
Using Discord
Commands Parameters and Tools
Writing Prompts
Working with Your Own Images
Styles and Aesthetics
Image Size and Aspect Ratio
Aspect Ratios
Pan
Upscalers
Understanding Image Size
Zoom Out
Advanced Prompting Tools
Subscriptions
Using The Website
Policies
Zoom Out
Light
The Zoom Out option allows you to extend the canvas of an upscaled image beyond its original boundaries without changing the content of the original image. The newly expanded canvas will be filled-in using guidance from the prompt and the original image.</p>
<p>Zoom Out does not increase the maximum 1024px x 1024 px size of an image.</p>
<p>Zoom Out</p>
<p>🔎 Zoom Out 2X 🔎 Zoom Out 1.5X buttons will appear after upscaling an image.</p>
<p>Prompt: vibrant California poppies</p>
<p>Starting Image
Zoom Out 1.5X
Zoom Out 2X</p>
<p>Make Square</p>
<p>With Make Square you can adjust the aspect ratio of a non-square image to make it square. If the original aspect ratio is wide (landscape), it will be expanded vertically. If it is tall (portrait), it will be expanded horizontally. The emoji ↔️ ↕️ next to the ↔️ Make Square button also indicates which way the image will be expanded. The ↔️ Make Square button will appear underneath non-square upscaled images.</p>
<p>Prompt: vibrant California poppies</p>
<p>Starting Image 
↕️ Make Square 
Starting Image 
↔️ Make Square </p>
<p>Custom Zoom</p>
<p>The 🔎 Custom Zoom button lets you choose how much to zoom out on an image. 🔎 Custom Zoom button under an upscaled image will pop up a dialogue box where you enter a custom value for --zoom. --zoom accepts values between 1-2.</p>
<p>Change the Aspect Ratio of an Upscaled Image</p>
<p>You can use --zoom 1 in this Custom Zoom pop-up box to change the aspect ratio, with the --ar parameter, without zooming out.</p>
<p>Change Your Prompt with Custom Zoom</p>
<p>🔎 Custom Zoom allows you to change the prompt before you expand your image, giving you finer control over the finished image. For example, changing the prompt to &quot;A framed picture on the wall&quot; gives this result:</p>
<p>Starting Image</p>
<p>prompt: vibrant California poppies</p>
<p>Custom Zoom + Changed Prompt</p>
<p>Zoom Out prompt: A framed picture on the wall --zoom 2</p>
<p>Previous
Understanding Image Size
Next
Advanced Prompting Tools
Midjourney is an independent research lab exploring new mediums of thought and expanding the imaginative powers of the human species. We are a small self-funded team focused on design, human infrastructure, and AI.
FOLLOW US: [F] [T] [R]
SUPPORT
For questions or support visit the Midjourney Discord support channels.
SITES
Midjourney Website
Midjourney Discord
Current
Legacy
Getting Started
Using Discord
Commands Parameters and Tools
Writing Prompts
Working with Your Own Images
Styles and Aesthetics
Image Size and Aspect Ratio
Aspect Ratios
Pan
Upscalers
Understanding Image Size
Zoom Out
Advanced Prompting Tools
Subscriptions
Using The Website
Policies
Aspect Ratios
Light
The --aspect or --ar parameter changes the aspect ratio of the generated image. An aspect ratio is the width-to-height ratio of an image. It is typically expressed as two numbers separated by a colon, such as 7:4 or 4:3.</p>
<p>A square image has equal width and height, described as a 1:1 aspect ratio. The image could be 1000px × 1000px, or 1500px × 1500px, and the aspect ratio would still be 1:1. A computer screen might have a ratio of 16:10. The width is 1.6 times longer than the height. So the image could be 1600px × 1000px, 4000px × 2000px, 320px x 200px, etc.</p>
<p>The default aspect ratio is 1:1.
--aspect must use whole numbers. Use 139:100 instead of 1.39:1.
The aspect ratio impacts the shape and composition of a generated image.
Some aspect ratios may be slightly changed when upscaling.</p>
<p>Max Aspect Ratios</p>
<p>Different Midjourney Version Models have different maximum aspect ratios.</p>
<pre><code>Version 5	Version 4	niji 5
</code></pre>
<p>Ratios	any*	1:2 to 2:1	any*</p>
<p>The --ar parameter will accept any aspect ratio from 1:1 (square) up to the maximum aspect ratio for each model. However, final outputs may be slightly modified during image generation or upscaling.</p>
<ul>
<li>Aspect ratios greater than 2:1 are experimental and may produce unpredicatble results.</li>
</ul>
<p>prompt example: imagine/ prompt vibrant california poppies --ar 5:4</p>
<p>Common Midjourney Aspect Ratios</p>
<p>--aspect 1:1 Default aspect ratio.
--aspect 5:4 Common frame and print ratio.
--aspect 3:2 Common in print photography.
--aspect 7:4 Close to HD TV screens and smartphone screens.</p>
<p>Changing the Aspect Ratio of an Image</p>
<p>Do you love an image you have generated but wish it taller or wider? You can use the 🔎 Zoom Out buttons on any upscaled image to change the aspect ratio of your image. The Midjourney Bot will fill in the new space with additional content informed by your prompt and the original image.</p>
<p>How to Set the Aspect Ratio
Use Aspect Ratio Parameters</p>
<p>Add --aspect <value>:<value>, or --ar <value>:<value> to the end of your prompt.</p>
<p>Previous
Image Size and Aspect Ratio
Next
Pan
Midjourney is an independent research lab exploring new mediums of thought and expanding the imaginative powers of the human species. We are a small self-funded team focused on design, human infrastructure, and AI.
FOLLOW US: [F] [T] [R]
SUPPORT
For questions or support visit the Midjourney Discord support channels.
SITES
Midjourney Website
Midjourney Discord
Current
Legacy
Getting Started
Using Discord
Commands Parameters and Tools
Writing Prompts
Working with Your Own Images
Styles and Aesthetics
Image Size and Aspect Ratio
Aspect Ratios
Pan
Upscalers
Understanding Image Size
Zoom Out
Advanced Prompting Tools
Subscriptions
Using The Website
Policies
Understanding Image Size
Light
There are lots of terms for how &quot;big&quot; an image is. Many people use words like resolution, file size, pixel count, dots per inch, and high resolution interchangeably. Understanding what these words mean can help you get the results you want from Midjourney images.</p>
<p>This guide is a simplified, non-technical explanation of image size, dimensions, resolutions, and DPI.</p>
<p>Image Size Basics</p>
<p>Examples will use this simple sailboat picture.</p>
<p>What is an image file?</p>
<p>Image files like .jpg, .png, or .gif are instructions for creating an image.
Each image is a mosaic of small colored tiles (pixels) that form the picture.</p>
<p>Think about image files as a set of instructions on how to arrange colored pixels to create an image.</p>
<p>What is File Size?</p>
<p>The file size is directly related to the amount of information inside the file. In the boat picture, the file contains 10 rows and 10 columns, or 100 pieces of information. If each colored dot is one byte of information, the image file would be 100 bytes.</p>
<p>If the file has a grid of 200 x 400 pieces of color information, it would be an 80,000-byte file (200*400=80,000) or 80,000-byte/1000 = 80-kilobyte file.</p>
<p>Image Dimensions</p>
<p>People generally don't talk about the file size of an image in kilobytes or megabytes. They talk about the image's dimensions (the number of columns and rows information) The boat.jpg example above is a 10x10 pixel image. 1024x1024, 720p, and 4K are all shorthand ways of talking about the pixel dimensions of an image.
​
There are no &quot;high-resolution&quot; image files when working on monitors, phones, TVs, or other screens. The only thing that matters is how much information you have (the image dimensions) vs. the size of the display.
The boat.jpg image might be a great size on a vintage flip phone, fine on your tablet, and not enough for your brand-new monitor.</p>
<p>The same image dimension viewed on different screens.</p>
<p>DPI Dots Per Inch</p>
<p>Dots Per Inch, Pixels Per Inch, Dots Per Centimeter, and Points Per Inch all refer to generally the same concept. The amount of information in a given length. 300 &quot;dots&quot; per inch means there are 300 pieces of color information (pixels, printed CMYK dots, etc.) in each inch.
DPI is NOT a measurement that is useful on screens. The dimensions of the file are what matters on screen. DPI is VERY important when printing. The general standard for a great-looking print is 300 dots of color in every inch. 300DPI is the point where the individual dots are so closely packed together you can't distinguish individual dots, and the picture looks smooth and crisp.</p>
<p>What DPI is this file?</p>
<p>It depends! Knowing that your file only contains instructions on how to make an image, the DPI is only determined when you tell the printer how closely or loosely that information is packed together.
Think about building the file with Legos. You can take the same set of instructions, and if you have a pile of small 1x1 bricks, you pack your information into a smaller space than if you start with a pile of 2x2 bricks.</p>
<p>The same &quot;file instructions&quot; using two sizes of tiles.</p>
<p>A Simplified Example of the Math</p>
<p>For this example, you have a 1200x1200 pixel image. What DPI is that image? It depends on how large you want it to print out.
1200/300 = 4. A 1200x1200 pixel image would produce a great looking 4in x 4in image.
1200/150 = 8. A 1200x1200 pixel image would produce an OK-looking 8in x 8in image.
1200/100 = 12. A 1200x1200 pixel image would produce a pretty bad looking 12in x 12in image.</p>
<p>Midjourney Image Sizes</p>
<p>The default size of a Midjourney image using the current default Model Version is 1024 x 1024 pixels. You can use the upscale tool, to increase the file size to 2048 x 2048 or 4096 x 4096 pixels. Using other aspect ratios will produce images with different dimensions but the same general file size.</p>
<p>Open your Midjourney images in your web browser or download them from Midjourney.com for maximum file size.</p>
<p>Can You Print a Midjourney Image?</p>
<p>Absolutely! The quality of your print simply follows the above rules of DPI. If you want a quality print, you need 300 pieces of information in every inch. So the default 1024x1024 pixel midjourney image would produce a good quality 3.4 in x 3.4 in print (1024 / 300 = 3.41). An image that has been upscaled to 4096x4096 pixels would produce a good quality 13.7 in x 13.7 in print (4096 / 300 = 13.65)</p>
<p>Previous
Upscalers
Next
Zoom Out
Midjourney is an independent research lab exploring new mediums of thought and expanding the imaginative powers of the human species. We are a small self-funded team focused on design, human infrastructure, and AI.
FOLLOW US: [F] [T] [R]
SUPPORT
For questions or support visit the Midjourney Discord support channels.
SITES
Midjourney Website
Midjourney Discord
Current
Legacy
Getting Started
Using Discord
Commands Parameters and Tools
Writing Prompts
Working with Your Own Images
Styles and Aesthetics
Stylize
Style
Style Tuner
Chaos
Weird
Image Size and Aspect Ratio
Advanced Prompting Tools
Subscriptions
Using The Website
Policies
Weird
Light
Explore unconventional aesthetics with the experimental --weird or --w parameter. This parameter introduces quirky and offbeat qualities to your generated images, resulting in unique and unexpected outcomes.</p>
<p>--weird accepts values: 0–3000.
The default --weird value is 0.
--weird is a highly experimental feature. What's weird may change over time
--weird is compatible with Midjourney Model Versions 5, 5.1, 5.2 and niji 5
--weird is not fully compatible with seeds</p>
<p>The Influence of Weird on Jobs</p>
<p>The optimal --weird value is dependent on the prompt and requires experimentation. Try starting with smaller values, such as 250 or 500, and then go up/down from there. If you want a generation to be conventionally attractive and weird, try mixing higher --stylize values with --weird. Try starting with similar values for both. Example /imagine prompt cyanotype cat --stylize 250 --weird 250.</p>
<p>prompt example: /imagine prompt cyanotype cat --weird 250</p>
<p>--weird 0
--weird 250
--weird 500
--weird 1000</p>
<p>prompt example: /imagine prompt lithograph potato --weird 250</p>
<p>--weird 0
--weird 250
--weird 500
--weird 1000</p>
<p>prompt example: /imagine prompt clockwork chicken --weird 250</p>
<p>--weird 0
--weird 250
--weird 500
--weird 1000</p>
<p>What's the difference between --weird, --chaos, and --stylize?</p>
<p>--chaos controls how diverse the initial grid images are from each other.
--stylize controls how strongly Midjourney's default aesthetic is applied.
--weird controls how unusual an image is compared to previous Midjourney images.</p>
<p>How to Use the Weird Parameter</p>
<p>Add --weird <value> or --w <value> to the end of your prompt.</p>
<p>Previous
Chaos
Next
Image Size and Aspect Ratio
Midjourney is an independent research lab exploring new mediums of thought and expanding the imaginative powers of the human species. We are a small self-funded team focused on design, human infrastructure, and AI.
FOLLOW US: [F] [T] [R]
SUPPORT
For questions or support visit the Midjourney Discord support channels.
SITES
Midjourney Website
Midjourney Discord
Current
Legacy
Getting Started
Using Discord
Commands Parameters and Tools
Writing Prompts
Working with Your Own Images
Styles and Aesthetics
Image Size and Aspect Ratio
Aspect Ratios
Pan
Upscalers
Understanding Image Size
Zoom Out
Advanced Prompting Tools
Subscriptions
Using The Website
Policies
Pan
Light
The Pan option allows you to expand the canvas of an image in a chosen direction without changing the content of the original image. The newly expanded canvas will be filled in using guidance from the prompt and the original image.</p>
<p>Pan allows you to increase the image resolution beyond the maximum 1024px x 1024px dimensions in one direction.
Pan is compatible with Midjourney Model Version 5, 5.1, 5.2, and niji 5</p>
<p>Panning an Image</p>
<p>The Pan buttons ⬅️ ➡️ ⬆️ ⬇️ will appear after upscaling an image. When panning, only the closest 512 pixels to the side of the image, along with the prompt, are used to determine the new section.</p>
<p>Prompt: A vibrant fantasy landscape</p>
<p>Starting Image
Pan ⬆️
Pan ⬇️
Pan ⬅️
Pan ➡️</p>
<p>After panning an image once, you can only pan that image again in the same direction (horizontal/vertical). You may continue to pan in that direction as often as you like. After panning several times, the image may become too large to send on Discord. When this happens, you will be sent a link to the image instead.</p>
<p>Pan ⬅️ and ➡️ several times</p>
<p>Pan with Remix Mode</p>
<p>Panning supports Remix Mode. This allows you to change your prompt when panning.</p>
<p>Starting Image</p>
<p>Starting prompt: A vibrant fantasy landscape</p>
<p>Pan ➡️ + Remixed Prompt</p>
<p>Pan ➡️ +  Remix prompt: A vibrant fantasy city</p>
<p>Previous
Aspect Ratios
Next
Upscalers
Midjourney is an independent research lab exploring new mediums of thought and expanding the imaginative powers of the human species. We are a small self-funded team focused on design, human infrastructure, and AI.
FOLLOW US: [F] [T] [R]
SUPPORT
For questions or support visit the Midjourney Discord support channels.
SITES
Midjourney Website
Midjourney Discord
Current
Legacy
Getting Started
Using Discord
Commands Parameters and Tools
Writing Prompts
Working with Your Own Images
Styles and Aesthetics
Image Size and Aspect Ratio
Aspect Ratios
Pan
Upscalers
Understanding Image Size
Zoom Out
Advanced Prompting Tools
Subscriptions
Using The Website
Policies
Upscalers
Light
The current Midjourney and Niji Model Versions produce grids of 1024 x 1024 pixel images. Use the U1 U2 U3 U4 buttons under each image grid to separate your selected image. You can then use the Upscale (2x) or Upscale (4x) tools to increase the size of your image.</p>
<p>Upscale tools use your subscription's GPU minutes. Using Upscale 2X on an image takes roughly twice as long as generating an initial image grid. Using Upscale 4X on an image takes roughly six times as long as generating an initial image grid.</p>
<p>Upscale tools are not compatible with the pan tool or the tile parameter.</p>
<p>How to Use the Upscale Tools</p>
<ol>
<li>Generate an Image</li>
</ol>
<p>Create an image using the /imagine command.</p>
<ol start="2">
<li>Select an Image</li>
</ol>
<p>Use a U button to separate your selected image from the grid.</p>
<ol start="3">
<li>Select Upscale</li>
</ol>
<p>Click on the Upscale button to upscale your image. The upscaler will double the size of your image.</p>
<p>Upscale Comparison</p>
<p>Prompt: chiaroscuro rooster portrait
Original 1024 by 1024 pixel image.</p>
<p>Detail from original image</p>
<p>Original image</p>
<p>After Upscale (2x) to 2048 x 2048 px</p>
<p>Upscale Comparison</p>
<p>Prompt: 1960s pop-art acrylic of redwoods
Original 1024 by 1024 pixel image.</p>
<p>Detail from the original image</p>
<p>Original image</p>
<p>After Upscale (4x) to 4096 x 4096 px</p>
<p>Upscale Tips</p>
<p>Upscale older Jobs by refreshing them with the /show command.</p>
<p>Learn More</p>
<p>Learn more about image sizes, dimensions, and DPI.</p>
<p>Legacy Upscalers</p>
<p>Earlier Midjourney model versions generated grids of lower-resolution images. You can use a legacy Midjourney upscaler on these images to increase the size and add additional details. There are multiple legacy upscale models available for upscaling images made with earlier Midjourney models. Using a legacy upscaler uses your subscription's GPU minutes.</p>
<p>Previous
Pan
Next
Understanding Image Size
Midjourney is an independent research lab exploring new mediums of thought and expanding the imaginative powers of the human species. We are a small self-funded team focused on design, human infrastructure, and AI.
FOLLOW US: [F] [T] [R]
SUPPORT
For questions or support visit the Midjourney Discord support channels.
SITES
Midjourney Website
Midjourney Discord
Current
Legacy
Getting Started
Using Discord
Commands Parameters and Tools
Writing Prompts
Working with Your Own Images
Styles and Aesthetics
Stylize
Style
Style Tuner
Chaos
Weird
Image Size and Aspect Ratio
Advanced Prompting Tools
Subscriptions
Using The Website
Policies
Chaos
Light
The --chaos or --c parameter influences how varied the initial image grids are. High --chaos values will produce more unusual and unexpected results and compositions. Lower --chaos values have more reliable, repeatable results.</p>
<p>--chaos accepts values 0–100.
The default --chaos value is 0.</p>
<p>The Influence of Chaos on Jobs
No --chaos value</p>
<p>Using a very low --chaos value, or not specifying a value, will produce initial image grids that are similar each time a Job is run.</p>
<p>prompt example: /imagine prompt: watermelon owl hybrid --c 0</p>
<p>Low --chaos values</p>
<p>Using a low --chaos value will produce initial image grids that are slightly varied each time a Job is run.</p>
<p>prompt example: /imagine prompt: watermelon owl hybrid --c 10</p>
<p>Moderate --chaos values</p>
<p>Using a moderate --chaos value will produce initial image grids that are varied each time a Job is run.</p>
<p>prompt example: /imagine prompt: watermelon owl hybrid --c 25</p>
<p>High --chaos Values</p>
<p>Using a higher --chaos value will produce initial image grids that are more varied and unexpected each time a Job is run.</p>
<p>prompt example: /imagine prompt: watermelon owl hybrid --c 50</p>
<p>Very High --chaos Values</p>
<p>Using extremely high --chaos values will produce initial image grids that are varied and have unexpected compositions or artistic mediums each time a Job is run.</p>
<p>prompt example: /imagine prompt: watermelon owl hybrid --c 80</p>
<p>How to Change the Chaos Value
Use the --chaos or --c Parameter</p>
<p>Add --chaos <value> or --c <value> to the end of your prompt.</p>
<p>Previous
Style Tuner
Next
Weird
Midjourney is an independent research lab exploring new mediums of thought and expanding the imaginative powers of the human species. We are a small self-funded team focused on design, human infrastructure, and AI.
FOLLOW US: [F] [T] [R]
SUPPORT
For questions or support visit the Midjourney Discord support channels.
SITES
Midjourney Website
Midjourney Discord
Current
Legacy
Getting Started
Using Discord
Commands Parameters and Tools
Writing Prompts
Working with Your Own Images
Styles and Aesthetics
Stylize
Style
Style Tuner
Chaos
Weird
Image Size and Aspect Ratio
Advanced Prompting Tools
Subscriptions
Using The Website
Policies
Style
Light
The --style parameter replaces the default aesthetic of some Midjourney Model Versions. Adding a style parameter can help you create more photo-realistic images, cinematic scenes, or cuter characters.</p>
<p>Default Model Version 5.2 and the previous version 5.1 accept --style raw.
Model Version Niji 5 accepts --style cute --style scenic --style original or --style expressive</p>
<p>Model Version 5.2 Styles</p>
<p>The current default Model Version 5.2 and the previous model version 5.1 have one style parameter, --style raw. --style raw uses an alternative model that may work well for users already comfortable with prompting who want more control over their images. Images made with --style raw have less automatic beautification applied, which can result in a more accurate match when prompting for specific styles.</p>
<p>Model Version 5.2
--v 5.2</p>
<p>ice cream icon</p>
<p>--v 5.2 --style raw</p>
<p>ice cream icon --style raw</p>
<p>--v 5.2 </p>
<p>child's drawing of a cat</p>
<p>--v 5.2 --style raw</p>
<p>child's drawing of a cat --style raw</p>
<p>Niji 5 Styles</p>
<p>Niji Model Version 5 can also use different aesthetics with --style options to achieve unique looks. Try --style cute, --style scenic, --style original , or --style expressive.</p>
<p>Niji Style Parameters</p>
<p>--style cute creates charming and adorable characters, props, and settings.
--style expressive has a more sophisticated illustrated feeling.
--style original uses the original Niji Model Version 5, which was the default before May 26th, 2023.
--style scenic makes beautiful backgrounds and cinematic character moments in the context of their fantastical surroundings.</p>
<p>niji 5 default</p>
<p>guinea pig wearing a flower crown --niji 5</p>
<p>--style original</p>
<p>guinea pig wearing a flower crown --niji 5 --style original</p>
<p>--style cute</p>
<p>guinea pig wearing a flower crown --niji 5 --style cute</p>
<p>--style expressive</p>
<p>guinea pig wearing a flower crown --niji 5 --style expressive</p>
<p>--style scenic</p>
<p>guinea pig wearing a flower crown --niji 5 --style scenic</p>
<p>niji 5 default</p>
<p>pastel fields of oxalis --niji 5</p>
<p>--style original</p>
<p>pastel fields of oxalis --niji 5 --style original</p>
<p>--style cute</p>
<p>pastel fields of oxalis --niji 5 --style cute</p>
<p>--style expressive</p>
<p>pastel fields of oxalis --niji 5 --style expressive</p>
<p>--style scenic</p>
<p>pastel fields of oxalis --niji 5 --style scenic</p>
<p>How to Use Styles
Use the --style Parameter</p>
<p>Add --style <style name> to the end of your prompt.</p>
<p>Use the Settings Command</p>
<p>Type /settings and select 🔧 Raw from the menu to append --style raw to all prompts.</p>
<p>Previous
Stylize
Next
Style Tuner
Midjourney is an independent research lab exploring new mediums of thought and expanding the imaginative powers of the human species. We are a small self-funded team focused on design, human infrastructure, and AI.
FOLLOW US: [F] [T] [R]
SUPPORT
For questions or support visit the Midjourney Discord support channels.
SITES
Midjourney Website
Midjourney Discord
Current
Legacy
Getting Started
Using Discord
Commands Parameters and Tools
Writing Prompts
Working with Your Own Images
Styles and Aesthetics
Stylize
Style
Style Tuner
Chaos
Weird
Image Size and Aspect Ratio
Advanced Prompting Tools
Subscriptions
Using The Website
Policies
Style Tuner
Light
Personalize the appearance of your Midjourney images using the Style Tuner. Use the /tune command to generate a range of sample images showing different visual styles based on your prompt. Choose your favorite images, and you'll receive a unique code you can use to customize the look of future Jobs.</p>
<p>Share your Style Tuner and Codes with others to share, explore, and experiment with different aesthetics.</p>
<p>/tune and codes are only compatible with Midjourney Model Version 5.2
/tune is only available while in Fast Mode.
--style parameters created with the Style Tuner are compatible with --stylize values between 20–1000.</p>
<p>How to Use the Style Tuner</p>
<ol>
<li>Generate Your Custom Style Tuner</li>
</ol>
<p>Create a Style Tuner page using the /tune command.</p>
<ol start="2">
<li>Select Your Preferred Options
Style Directions: Choose the number of image pairs you want to see in your Style Tuner (16, 32, 64, or 128 pairs).
Default Mode: Select the style mode for your sample images (Default or Raw). If you typically do not use the --style raw parameter with your prompts, choose &quot;default.&quot;</li>
</ol>
<p>Use an Exisiting Style Tuner</p>
<p>If another user has previously generated a Style Tuner with your prompt, you will receive a link to that Tuner. Click the link to access the existing Style Tuner. Using a previously generated Style Tuner does not use your subscription's GPU minutes.</p>
<ol start="3">
<li>Submit your Job
Click the Submit button.
Confirm your submission.</li>
</ol>
<p>Your Style Tuner generates a pair of images for each Style Direction. A Style Tuner with 16 directions will generate 32 images. A Style Tuner with 128 directions will generate 256 images. Generating these images uses your subscription's Fast GPU time.</p>
<ol start="4">
<li>Open Your Custom Style Tuner
When your Style Tuner is ready, the Midjourney Bot will send you a direct message with a link to your Tuner.
Click the link to open your Style Tuner in your web browser.</li>
</ol>
<p>Try this Style Tuner: https://tuner.midjourney.com/ejYLCOY</p>
<ol start="5">
<li>Select images</li>
</ol>
<p>Your Style Tuner will show rows of image pairs, each representing a distinct visual direction for your prompt. Click on the image you prefer in each pair. If you don't feel strongly about either image, leave the empty middle box selected.</p>
<ol start="6">
<li>Copy Your Code</li>
</ol>
<p>The Style Tuner generates a code you can add to your prompts with the --style <code> Parameter. Learn more about parameters.</p>
<p>To copy your prompt and Parameter</p>
<p>Find your customized code at the bottom of the page.
Click the Copy button to copy your original prompt and newly generated --style <code> parameter.</p>
<p>You can share your Style Tuner page with friends and generate new codes without using any additional GPU minutes!</p>
<ol start="7">
<li>
<p>Generate an Image
Return to Discord
Use the /imagine command and paste your copied prompt and --style <code> parameter into the prompt field.
Generate your image</p>
</li>
<li>
<p>Use Additional Midjourney Tools</p>
</li>
</ol>
<p>Take your image further by using other Midjourney tools like Upscale, Pan, Zoom-Out, Remix, or Vary-Region.</p>
<ol start="9">
<li>Experiment and Explore
Use your style code with a new prompt:</li>
</ol>
<p>The Style Tuner you create uses your initial prompt to create sample images and help you visualize the impact of your choices. However, the generated codes can be used with any prompt. Remember that styles and prompts always work together to generate an image, so a style code may not transfer as intended to other prompts.</p>
<p>Experiment: Style codes and prompts interact in complex ways. A code may have a strong effect on one prompt and a subtle effect on a similar prompt. The images you choose in your Style Tuner can combine in unexpected and creative ways. Use style codes as a tool to explore new looks and visuals.
Generate more codes: You can return to your Style Tuner page at any time to change your selections and create new codes.
Share style codes: You can share or use style codes created by friends. Check out the Midjourney style-tuner channel to share your codes and discover new styles!
Find a Style Tuner page: Find the Style Tuner page for any style code by adding it to this URL: https://tuner.midjourney.com/code/StyleCodeHere.</p>
<ol start="10">
<li>Save and Reuse Your Codes
Use the /settings command and turn on 📌Sticky Style. Sticky Style will save the last --style parameter used in your personal suffix, so you don't have to repeat the code on future prompts. Change codes by using a new --style or unselecting 📌Sticky Style.
Use custom options to store your favorite codes.
Or, create your own Discord server to organize your images, prompts, image references, and Style Tuner codes.</li>
</ol>
<p>Style Tune Examples</p>
<p>prompt vibrant california poppies
All images were made by style created using this Style Tuner</p>
<p>Random Codes</p>
<p>Use the --style random parameter to apply a random 32 base styles Style Tuner code to your prompt. You can also use --style random-16, --style random-64 or --style random-128 to use random results from other lengths of tuners.</p>
<p>--random simulates Style Tuner code with random selections chosen for 75% of the image pairs. You can adjust this percentage by adding a number to the end of the --random parameter. For example, --style random-32-15 simulates a 32-pair tuner with 15% of the image pairs selected, --style random-128-80 simulates a 128-pair tuner with 80% of the image pairs selected.</p>
<p>Combine Codes</p>
<p>Combine multiple codes in one parameter with a hyphen, --style code1-code2.
Combine multiple codes and style raw: --style raw-code1-code2</p>
<p>Style Tuner and --stylize</p>
<p>The --stylize parameter adjusts the influence of the --style parameter on your generated images. If you're not seeing the desired effect from your code, consider combining it with higher stylization values, like --stylize 250, or --stylize 500.</p>
<p>Comparison</p>
<p>prompt vibrant California poppies --style fdeQ4zOX5jd --stylize 250</p>
<p>--stylize 20</p>
<p>--stylize 100 (default)</p>
<p>--stylize 250</p>
<p>--stylize 750</p>
<p>Style Raw</p>
<p>Combine your custom style code with Midjourney Style Raw by using --style raw-<code>.
Example: To use Style Raw and --style fjo5S8BgMoV use --style raw-fjo5S8BgMoV.</p>
<p>Style Tuner and Niji Model Version</p>
<p>Style Tuner codes created with the Midjourney Bot are not compatible with the Niji Model version accessed through the Midjourney Bot. To create a Style Tuner or Code for Niji Model version, join the Niji Discord community and interact with the Niji Bot in the same way you interact with the Midjourney Bot. Your Midjourney subscription gives you access to the Niji community and Bot.</p>
<p>Technical Details</p>
<p>/tune is compatible with prompts that include the following:
--aspect
--chaos
--tile
multi prompts</p>
<p>/tune and style codes are not compatible with image prompts that do not include a text prompt.</p>
<p>If your /tune command does not return a clickable link, check that Embeds and Link Previews is enabled in your Discord App Settings</p>
<p>Previous
Style
Next
Chaos
Midjourney is an independent research lab exploring new mediums of thought and expanding the imaginative powers of the human species. We are a small self-funded team focused on design, human infrastructure, and AI.
FOLLOW US: [F] [T] [R]
SUPPORT
For questions or support visit the Midjourney Discord support channels.
SITES
Midjourney Website
Midjourney Discord
Current
Legacy
Getting Started
Using Discord
Commands Parameters and Tools
Writing Prompts
Working with Your Own Images
Styles and Aesthetics
Stylize
Style
Style Tuner
Chaos
Weird
Image Size and Aspect Ratio
Advanced Prompting Tools
Subscriptions
Using The Website
Policies
Stylize
Light
The Midjourney Bot has been trained to produce images that favor artistic color, composition, and forms. The --stylize or --s parameter influences how strongly this training is applied. Low stylization values produce images that closely match the prompt but are less artistic. High stylization values create images that are very artistic but less connected to the prompt.</p>
<p>--stylize's default value is 100 and accepts integer values 0–1000 when using the current model</p>
<p>Different Midjourney Version Models have different stylize ranges.</p>
<pre><code>Version 5, 5.1, 5.2	Version 4	niji 5
</code></pre>
<p>Stylize default	100	100	100
Stylize Range	0–1000	0–1000	0–1000</p>
<p>Common Stylize Settings
Influence of Stylize on Model Version 5.2</p>
<p>prompt example: /imagine prompt child's drawing of a cat --s 100</p>
<p>--stylize 0
--stylize 50
Equal to 🖌️ Style Low
--stylize 100 (default)
Equal to 🖌️ Style Med</p>
<p>--stylize 250
Equal to 🖌️ Style High
--stylize 500
--stylize 750
Equal to 🖌️ Style Very High</p>
<p>prompt example: /imagine prompt colorful risograph of a fig --s 100</p>
<p>--stylize 50</p>
<p>Equal to 🖌️ Style Low</p>
<p>--stylize 100 (default)</p>
<p>Equal to 🖌️ Style Med</p>
<p>--stylize 250</p>
<p>Equal to 🖌️ Style High</p>
<p>--stylize 750</p>
<p>Equal to 🖌️ Style Very High</p>
<p>Midjourney Model Version 5.2 is more sensitive to different stylize values. If you previously used very high stylize values, they may require adjustment for this model version. We recommend reducing your stylize value to 20% of your previous value. For example, if you were using --stylize 1000 --V 5.1, try using --stylize 200 instead.</p>
<p>Influence of Stylize on Model Version 5.1</p>
<p>prompt example: /imagine prompt colorful risograph of a fig --v 5.1 --s 100</p>
<p>--stylize 50</p>
<p>🖌️ Style Low</p>
<p>--stylize 100 (default)</p>
<p>🖌️ Style Med</p>
<p>--stylize 250</p>
<p>🖌️ Style High</p>
<p>--stylize 750</p>
<p>🖌️ Style Very High</p>
<p>Influence of Stylize on Niji 5</p>
<p>prompt example: /imagine prompt colorful risograph of a fig --niji 5 --s 100</p>
<p>--stylize 50</p>
<p>🖌️ Style Low</p>
<p>--stylize 100 (default)</p>
<p>🖌️ Style Med</p>
<p>--stylize 250</p>
<p>🖌️ Style High</p>
<p>--stylize 750</p>
<p>🖌️ Style Very High</p>
<p>Influence of Stylize on Model V4</p>
<p>prompt example: /imagine prompt illustrated figs --v 4 --s 100</p>
<p>--stylize 50</p>
<p>🖌️ Style Low</p>
<p>--stylize 100 (default)</p>
<p>🖌️ Style Med</p>
<p>--stylize 250</p>
<p>🖌️ Style High</p>
<p>--stylize 750</p>
<p>🖌️ Style Very High</p>
<p>How to Switch Stylization Values
Use the Stylize Parameter</p>
<p>Add --stylize <value> or --s <value> to the end of your prompt.</p>
<p>Use the Settings Command</p>
<p>Type /settings and select your preferred stylize value from the menu.</p>
<p>🖌️ Style Low 🖌️ Style Med 🖌️ Style High 🖌️ Style Very High
Previous
Styles and Aesthetics
Next
Style
Midjourney is an independent research lab exploring new mediums of thought and expanding the imaginative powers of the human species. We are a small self-funded team focused on design, human infrastructure, and AI.
FOLLOW US: [F] [T] [R]
SUPPORT
For questions or support visit the Midjourney Discord support channels.
SITES
Midjourney Website
Midjourney Discord
Current
Legacy
Getting Started
Using Discord
Commands Parameters and Tools
Writing Prompts
Working with Your Own Images
Blend
Image Prompts
Describe
Styles and Aesthetics
Image Size and Aspect Ratio
Advanced Prompting Tools
Subscriptions
Using The Website
Policies
Describe
Light
The /describe command allows you to upload an image and generate four possible prompts based on that image. Use the /describe command to explore new vocabulary and aesthetic movements.</p>
<p>/describe generates prompts that are inspirational and suggestive, it cannot be used to recreate an uploaded image exactly.
/describe returns the aspect ratio for uploaded images.</p>
<p>Previous
Image Prompts
Next
Styles and Aesthetics
Midjourney is an independent research lab exploring new mediums of thought and expanding the imaginative powers of the human species. We are a small self-funded team focused on design, human infrastructure, and AI.
FOLLOW US: [F] [T] [R]
SUPPORT
For questions or support visit the Midjourney Discord support channels.
SITES
Midjourney Website
Midjourney Discord
Current
Legacy
Getting Started
Using Discord
Commands Parameters and Tools
Writing Prompts
Working with Your Own Images
Blend
Image Prompts
Describe
Styles and Aesthetics
Image Size and Aspect Ratio
Advanced Prompting Tools
Subscriptions
Using The Website
Policies
Image Prompts
Light
You can use images as part of a prompt to influence a Job's composition, style, and colors. Images prompts can be used alone or with text prompts—experiment with combining images with different styles for the most exciting results.</p>
<p>To add images to a prompt, type or paste the web address where the image is stored online. The address must end in an extension like .png, .gif, or .jpg. After adding image addresses, add any additional text and parameters to complete the prompt.</p>
<p>Image prompts go at the front of a prompt.
Prompts must have two images or one image and text to work.
An image URL must be a direct link to an online image.
Your file should end in .png, .gif, .webp, .jpg, or .jpeg.
In most browsers, right-click or long-press an image and select Copy Image Address to get the URL.
The /blend command is a simplified image prompting process optimized for mobile users.</p>
<p>Upload an image to Discord</p>
<p>To incorporate an image into your prompt, you need a direct image link that ends with .png, .gif, .webp, .jpg, or .jpeg. If the image is on your computer or phone, you can send it as a message to the Midjourney Bot first to generate a link.</p>
<p>How To Upload Your Image
Paste the image into the chat with Midjourney Bot. Press Enter to send your image.
Depending on your platform, follow these steps to obtain the image link</p>
<p>Discord Desktop App: Right-click on the image and select &quot;Copy Link&quot; (NOT &quot;Copy Message Link&quot;).</p>
<p>Discord Web App: Click to expand the image, then right-click and choose &quot;Copy image address.&quot;</p>
<p>Discord Mobile App: Tap and hold on the image, then select &quot;Copy Media Link.&quot;</p>
<p>If none of these methods work, you can always click to expand the image, and at the bottom, select &quot;Open in Browser&quot; so you can copy and paste the image's URL.</p>
<p>Add an Image URL to Your Prompt</p>
<p>To add an image to a prompt, begin typing /imagine as usual. After the prompt box appears, drag the image file into the prompt box to add the image's URL, or right-click and paste the link within the prompt box.</p>
<p>Privacy Notes</p>
<p>Upload images in your Direct Messages with the Midjourney Bot to prevent other server users from seeing an image.
Image prompts are visible on the Midjourney website unless a user has Stealth Mode.</p>
<p>Examples
Starting Images</p>
<p>Statue of Apollo</p>
<p>Vintage Flower Illustration</p>
<p>Ernst Haeckel's Jellyfish</p>
<p>Ernst Haeckel's Lichen</p>
<p>Hokusai's The Great Wave</p>
<p>Midjourney Model Version 4
Statue + Flowers
+ </p>
<p>Statue + Jellyfish</p>
<ul>
<li></li>
</ul>
<p>Statue + Lichen</p>
<ul>
<li></li>
</ul>
<p>Statue + Wave</p>
<ul>
<li></li>
</ul>
<p>Statue + Lichen + Flowers</p>
<ul>
<li>
<ul>
<li></li>
</ul>
</li>
</ul>
<p>Midjourney Model Version 5
Statue + Flowers
+ </p>
<p>Statue + Jellyfish</p>
<ul>
<li></li>
</ul>
<p>Statue + Lichen</p>
<ul>
<li></li>
</ul>
<p>Statue + Wave</p>
<ul>
<li></li>
</ul>
<p>Statue + Lichen + Flowers</p>
<ul>
<li>
<ul>
<li></li>
</ul>
</li>
</ul>
<p>Aspect Ratio Tip</p>
<p>Crop images to the same aspect ratio as your final image for the best results.</p>
<p>Image Weight Parameter</p>
<p>Use the image weight parameter --iw to adjust the importance of the image vs. text portion of a prompt. The default value is used when no --iw is specified. Higher --iw values mean the image prompt will have more impact on the finished job.</p>
<p>See the Multi Prompts page for more information about the relative importance between parts of a prompt.</p>
<p>Different Midjourney Version Models have different image weight ranges.</p>
<pre><code>Version 5	Version 4	niji 5
</code></pre>
<p>Image Weight Default	1	NA	1
Image Weight Range	0–2	NA	0–2</p>
<p>prompt example: /imagine prompt flowers.jpg birthday cake --iw .5</p>
<p>Image Prompt</p>
<p>--iw .5</p>
<p>--iw .75</p>
<p>--iw 1</p>
<p>--iw 1.25</p>
<p>--iw 1.5</p>
<p>--iw 1.75</p>
<p>--iw 2</p>
<p>Technical Details</p>
<p>Prompts that only use images and no text are not compatible with the --stylize, or --weird parameters.</p>
<p>Previous
Blend
Next
Describe
Midjourney is an independent research lab exploring new mediums of thought and expanding the imaginative powers of the human species. We are a small self-funded team focused on design, human infrastructure, and AI.
FOLLOW US: [F] [T] [R]
SUPPORT
For questions or support visit the Midjourney Discord support channels.
SITES
Midjourney Website
Midjourney Discord
Current
Legacy
Getting Started
Using Discord
Commands Parameters and Tools
Writing Prompts
Working with Your Own Images
Blend
Image Prompts
Describe
Styles and Aesthetics
Image Size and Aspect Ratio
Advanced Prompting Tools
Subscriptions
Using The Website
Policies
Blend
Light
The /blend command allows you to upload 2–5 images quickly and then looks at the concepts and aesthetics of each image and merges them into a novel new image.</p>
<p>/blend is the same as using multiple image prompts with /imagine, but the interface is optimized for easy use on mobile devices.
/blend works with up to 5 images. To use more than 5 images in a prompt use image prompts with /imagine
/blend does not work with text prompts. To use text and image prompts together, use image prompts and text with /imagine</p>
<p>/blend Options</p>
<p>After typing the /blend command, you will be prompted to upload two photos. Drag and drop images from your hard drive or add images from your photo library when using a mobile device. To add more images, select the optional/options field and select image3, image4, or image5. The /blend command may take longer to start than other commands because your images must be uploaded before the Midjourney Bot can process your request.</p>
<p>Blended images have a default 1:1 aspect ratio, but you can use the optional dimensions field to select between a square aspect ratio (1:1), portrait aspect ration (2:3), or landscape aspect ratio (3:2).</p>
<p>Custom suffixes are added to the end of /blend prompts, like any other /imagine prompt. Aspect ratios specified as part of the /blend command override aspect ratios within a custom suffix.</p>
<p>Blending Tip</p>
<p>For the best results, upload images that are the same aspect ratio as your desired result.</p>
<p>How to Use /blend</p>
<p>Previous
Working with Your Own Images
Next
Image Prompts
Midjourney is an independent research lab exploring new mediums of thought and expanding the imaginative powers of the human species. We are a small self-funded team focused on design, human infrastructure, and AI.
FOLLOW US: [F] [T] [R]
SUPPORT
For questions or support visit the Midjourney Discord support channels.
SITES
Midjourney Website
Midjourney Discord
Current
Legacy
Getting Started
Using Discord
Commands Parameters and Tools
Writing Prompts
Prompts
Describe
No
Image Prompts
Multi Prompts
Remix
Shorten
Working with Your Own Images
Styles and Aesthetics
Image Size and Aspect Ratio
Advanced Prompting Tools
Subscriptions
Using The Website
Policies
Shorten
Light
The /shorten command analyzes your prompt, highlights some of your prompt's most influential words, and suggests unnecessary words you could remove. With this command, you can optimize your prompt by focusing on essential terms.</p>
<p>/shorten is not compatible with multi prompts or the --no parameter</p>
<p>Analyze a Prompt with /Shorten</p>
<p>The Midjourney bot analyzes your prompt by breaking it down into smaller units known as tokens. These tokens can be phrases, words, or even syllables. The Midjourney bot converts these tokens into a format it can understand. It uses them with the associations and patterns learned during its training to guide how your image is generated. Think of tokens as the building blocks that help the Midjourney bot make sense of the input and create the desired visual output.</p>
<p>Long prompts with unnecessary words, lengthy descriptions, poetic phrases, or direct addressing of the bot (&quot;Please make me an image,&quot; &quot;Thank you for your help, Midjourney Bot!&quot;) can lead to unexpected elements being added to your images.</p>
<p>The /shorten command can help you discover the most important words in your prompt and what words you can omit.</p>
<p>Shorten Command Example</p>
<p>If you want to create a pile of sprinkle covered donuts you might try the prompt:
Please create a whimsical majestic tower of donuts, intricately crafted and adorned with a mesmerizing array of colorful sprinkles. Bring this sugary masterpiece to life, ensuring every detail is rendered in stunning magical realism. Thank you!</p>
<p>If you use /shorten command with the above prompt the Midjourney Bot will return the following information:</p>
<p>IMPORTANT TOKENS
Please create a whimsical majestic tower of donuts, intricately crafted and adorned with a mesmerizing array of colorful sprinkles. Bring this sugary masterpiece to life, ensuring every detail is rendered in stunning magical realism. Thank you!</p>
<p>SHORTENED PROMPTS
1️⃣ Please, majestic tower of donuts, crafted, array of colorful sprinkles, sugary masterpiece, rendered, magical realism</p>
<p>2️⃣ Please, majestic tower of donuts, colorful sprinkles, sugary masterpiece, rendered, magical realism</p>
<p>3️⃣ majestic tower of donuts, colorful sprinkles, sugary, magical realism</p>
<p>4️⃣ majestic tower of donuts, colorful sprinkles, magical</p>
<p>5️⃣ tower of donuts, sprinkles</p>
<p>The most Important tokens in your prompt are highlighted in bold, the least important are stikethroughed. You will also be given 5 possible shorter prompts based on this information.</p>
<p>Shortened Option 1️⃣ </p>
<p>Please, majestic tower of donuts, crafted, array of colorful sprinkles, sugary masterpiece, rendered, magical realism</p>
<p>Shortened Option 2️⃣ </p>
<p>Please, majestic tower of donuts, colorful sprinkles, sugary masterpiece, rendered, magical realism</p>
<p>Shortened Option 3️⃣ </p>
<p>majestic tower of donuts, colorful sprinkles, sugary, magical realism</p>
<p>Shortened Option 4️⃣ </p>
<p>majestic tower of donuts, colorful sprinkles, magical</p>
<p>Shortened Option 5️⃣ </p>
<p>tower of donuts, sprinkles</p>
<p>Analyzing the Results</p>
<p>The shortest prompt, Option 5️⃣ : tower of donuts, sprinkles produced an image closest to the original goal. Many of the filler words like &quot;whimsical,&quot; &quot;mesmerizing,&quot; and &quot;masterpiece&quot; could be omitted. Learning that &quot;tower&quot; and &quot;magical&quot; were considered important tokens helps explain why some images were generated with fairytale castle elements. Learning this provided a clue that &quot;magical&quot; should be removed from the prompt if the goal was to create a stack of delicious donuts.</p>
<p>The /shorten command is a tool to help you explore how the Midjourney Bot interprets tokens and experiment with words, but it may not work for all subjects and styles of prompting.</p>
<p>How to Use Shorten</p>
<p>Use the /shorten <your prompt> command in any Bot Channel to get information on your prompt</p>
<p>Previous
Remix
Next
Working with Your Own Images
Midjourney is an independent research lab exploring new mediums of thought and expanding the imaginative powers of the human species. We are a small self-funded team focused on design, human infrastructure, and AI.
FOLLOW US: [F] [T] [R]
SUPPORT
For questions or support visit the Midjourney Discord support channels.
SITES
Midjourney Website
Midjourney Discord
Current
Legacy
Getting Started
Using Discord
Commands Parameters and Tools
Writing Prompts
Prompts
Describe
No
Image Prompts
Multi Prompts
Remix
Shorten
Working with Your Own Images
Styles and Aesthetics
Image Size and Aspect Ratio
Advanced Prompting Tools
Subscriptions
Using The Website
Policies
Remix
Light
Use Remix Mode to change prompts, parameters, model versions, or aspect ratios between variations. Remix will take the general composition of your starting image and use it as part of the new Job. Remixing can help change the setting or lighting of an image, evolve a subject, or achieve tricky compositions.</p>
<p>Remix is an experimental feature that may change or be removed at any time.</p>
<p>Using Remix</p>
<p>Activate Remix mode with the /prefer remix command or by using the /settings command and toggling the 🎛️ Remix Mode button. Remix changes the behavior of the variation buttons (V1, V2, V3, V4) under image grids. When Remix is enabled, it allows you to edit your prompt during each variation. To Remix an upscale select 🪄 Make Variations.</p>
<p>When Remix is enabled, Variation buttons turn green when used instead of blue.
You can switch Model Versions when using Remix.
When you are done with Remix use the /settings or /prefer remix command to turn it off.
Create a standard image variation when Remix is active by not modifying the prompt in the pop-up window.</p>
<p>Step 1</p>
<p>line-art stack of pumpkins</p>
<p>Turn on Remix mode.</p>
<p>Select an image grid or upscaled image to Remix.</p>
<p>Step 2</p>
<p>Remix</p>
<p>Select &quot;Make Variations.&quot;</p>
<p>Modify or enter a new prompt in the pop-up.</p>
<p>Results</p>
<p>pile of cartoon owls</p>
<p>The Midjourney Bot generates an image using the new prompt with influence from the original image.</p>
<p>Starting Image</p>
<p>line-art stack of pumpkins</p>
<p>Model Change</p>
<p>line-art stack of pumpkins --test</p>
<p>Subject Change</p>
<p>balloon-animal shaped stack of pumpkins&quot;</p>
<p>Medium Change</p>
<p>vibrant illustrated stack of fruit</p>
<p>Using Parameters with Remix</p>
<p>You can add or remove Parameters while using Remix mode, but you must use valid parameter combinations. Changing /imagine prompt illustrated stack of pumpkins --version 3 --stylize 10000 to illustrated stack of pumpkins --version 4 --stylize 10000 will return an error because Midjourney Model Version 4 is incompatible with the Stylize parameter.</p>
<p>Only parameters that normally influence variations will work while using Remix:</p>
<pre><code>Affects Initial
</code></pre>
<p>Generation	Affects Variations
and Remix
Aspect Ratio*	✓	✓
Chaos	✓	
Image Weight	✓	
No	✓	✓
Quality	✓	
Seed	✓	
Same Seed	✓	
Stop	✓	✓
Stylize	✓	
Tile	✓	✓
Video	✓	✓
Changing aspect ratios with Remix will stretch an image. It will not extend the canvas, add missing details, or fix a bad crop.
How to Activate Remix
Use the Settings Command</p>
<p>Type /settings and select Remix from the pop-up.
🎛️ Remix</p>
<p>Use the Prefer Remix Command</p>
<p>Type /prefer remix to toggle Remix mode on and off.</p>
<p>Previous
Multi Prompts
Next
Shorten
Midjourney is an independent research lab exploring new mediums of thought and expanding the imaginative powers of the human species. We are a small self-funded team focused on design, human infrastructure, and AI.
FOLLOW US: [F] [T] [R]
SUPPORT
For questions or support visit the Midjourney Discord support channels.
SITES
Midjourney Website
Midjourney Discord
Current
Legacy
Getting Started
Using Discord
Commands Parameters and Tools
Writing Prompts
Prompts
Describe
No
Image Prompts
Multi Prompts
Remix
Shorten
Working with Your Own Images
Styles and Aesthetics
Image Size and Aspect Ratio
Advanced Prompting Tools
Subscriptions
Using The Website
Policies
Image Prompts
Light
You can use images as part of a prompt to influence a Job's composition, style, and colors. Images prompts can be used alone or with text prompts—experiment with combining images with different styles for the most exciting results.</p>
<p>To add images to a prompt, type or paste the web address where the image is stored online. The address must end in an extension like .png, .gif, or .jpg. After adding image addresses, add any additional text and parameters to complete the prompt.</p>
<p>Image prompts go at the front of a prompt.
Prompts must have two images or one image and text to work.
An image URL must be a direct link to an online image.
Your file should end in .png, .gif, .webp, .jpg, or .jpeg.
In most browsers, right-click or long-press an image and select Copy Image Address to get the URL.
The /blend command is a simplified image prompting process optimized for mobile users.</p>
<p>Upload an image to Discord</p>
<p>To incorporate an image into your prompt, you need a direct image link that ends with .png, .gif, .webp, .jpg, or .jpeg. If the image is on your computer or phone, you can send it as a message to the Midjourney Bot first to generate a link.</p>
<p>How To Upload Your Image
Paste the image into the chat with Midjourney Bot. Press Enter to send your image.
Depending on your platform, follow these steps to obtain the image link</p>
<p>Discord Desktop App: Right-click on the image and select &quot;Copy Link&quot; (NOT &quot;Copy Message Link&quot;).</p>
<p>Discord Web App: Click to expand the image, then right-click and choose &quot;Copy image address.&quot;</p>
<p>Discord Mobile App: Tap and hold on the image, then select &quot;Copy Media Link.&quot;</p>
<p>If none of these methods work, you can always click to expand the image, and at the bottom, select &quot;Open in Browser&quot; so you can copy and paste the image's URL.</p>
<p>Add an Image URL to Your Prompt</p>
<p>To add an image to a prompt, begin typing /imagine as usual. After the prompt box appears, drag the image file into the prompt box to add the image's URL, or right-click and paste the link within the prompt box.</p>
<p>Privacy Notes</p>
<p>Upload images in your Direct Messages with the Midjourney Bot to prevent other server users from seeing an image.
Image prompts are visible on the Midjourney website unless a user has Stealth Mode.</p>
<p>Examples
Starting Images</p>
<p>Statue of Apollo</p>
<p>Vintage Flower Illustration</p>
<p>Ernst Haeckel's Jellyfish</p>
<p>Ernst Haeckel's Lichen</p>
<p>Hokusai's The Great Wave</p>
<p>Midjourney Model Version 4
Statue + Flowers
+ </p>
<p>Statue + Jellyfish</p>
<ul>
<li></li>
</ul>
<p>Statue + Lichen</p>
<ul>
<li></li>
</ul>
<p>Statue + Wave</p>
<ul>
<li></li>
</ul>
<p>Statue + Lichen + Flowers</p>
<ul>
<li>
<ul>
<li></li>
</ul>
</li>
</ul>
<p>Midjourney Model Version 5
Statue + Flowers
+ </p>
<p>Statue + Jellyfish</p>
<ul>
<li></li>
</ul>
<p>Statue + Lichen</p>
<ul>
<li></li>
</ul>
<p>Statue + Wave</p>
<ul>
<li></li>
</ul>
<p>Statue + Lichen + Flowers</p>
<ul>
<li>
<ul>
<li></li>
</ul>
</li>
</ul>
<p>Aspect Ratio Tip</p>
<p>Crop images to the same aspect ratio as your final image for the best results.</p>
<p>Image Weight Parameter</p>
<p>Use the image weight parameter --iw to adjust the importance of the image vs. text portion of a prompt. The default value is used when no --iw is specified. Higher --iw values mean the image prompt will have more impact on the finished job.</p>
<p>See the Multi Prompts page for more information about the relative importance between parts of a prompt.</p>
<p>Different Midjourney Version Models have different image weight ranges.</p>
<pre><code>Version 5	Version 4	niji 5
</code></pre>
<p>Image Weight Default	1	NA	1
Image Weight Range	0–2	NA	0–2</p>
<p>prompt example: /imagine prompt flowers.jpg birthday cake --iw .5</p>
<p>Image Prompt</p>
<p>--iw .5</p>
<p>--iw .75</p>
<p>--iw 1</p>
<p>--iw 1.25</p>
<p>--iw 1.5</p>
<p>--iw 1.75</p>
<p>--iw 2</p>
<p>Technical Details</p>
<p>Prompts that only use images and no text are not compatible with the --stylize, or --weird parameters.</p>
<p>Previous
No
Next
Multi Prompts
Midjourney is an independent research lab exploring new mediums of thought and expanding the imaginative powers of the human species. We are a small self-funded team focused on design, human infrastructure, and AI.
FOLLOW US: [F] [T] [R]
SUPPORT
For questions or support visit the Midjourney Discord support channels.
SITES
Midjourney Website
Midjourney Discord
Current
Legacy
Getting Started
Using Discord
Commands Parameters and Tools
Writing Prompts
Prompts
Describe
No
Image Prompts
Multi Prompts
Remix
Shorten
Working with Your Own Images
Styles and Aesthetics
Image Size and Aspect Ratio
Advanced Prompting Tools
Subscriptions
Using The Website
Policies
Multi Prompts
Light
The Midjourney Bot can blend multiple concepts using :: as a separator. Using a multi-prompt allows you to assign relative importance to the concept in the prompt, helping you control how they are blended together.
Multi-Prompt Basics</p>
<p>Adding a double colon :: to a prompt indicates to the Midjourney Bot that it should consider each part of the prompt individually. For the prompt space ship both words are considered together, and the Midjourney Bot produces images of sci-fi spaceships. If the prompt is separated into two parts, space:: ship, both concepts are considered separately, then blended together creating a sailing ship traveling through space.</p>
<p>There is no space between the double colons ::
Multi-prompts work with Model Versions 1, 2, 3, 4, '5, niji, and niji 5
Any parameters are still added to the very end of the prompt.</p>
<p>space ship</p>
<p>space ship is considered as a single thought.</p>
<p>space:: ship</p>
<p>space and ship are considered separate thoughts</p>
<p>cheese cake painting</p>
<p>cheese cake painting is considered together, producing a painted image of a cheesecake.</p>
<p>cheese:: cake painting</p>
<p>cheese is considered separately from cake painting, producing images of painted cakes made of cheeses.</p>
<p>cheese:: cake:: painting</p>
<p>cheese, cake, and painting are considered separately, producing tiered cakes, made of cheeses with common classical painting compositions and elements.</p>
<p>Prompt Weights</p>
<p>When a double colon :: is used to separate a prompt into different parts, you can add a number immediately after the double colon to assign the relative importance to that part of the prompt.</p>
<p>In the example below, the prompt space:: ship produced a sailing ship traveling through space. Changing the prompt to space::2 ship makes the word space twice as important as the word ship, producing images of space that have ships as a supporting element.</p>
<p>[Model Versions] 1, 2, 3 only accept whole numbers as weights
[Model Versions] 4, niji 4, niji 5, 5, 5.1, and 5.2 and can accept decimal places for weights
Non-specified weights default to 1.</p>
<p>space:: ship</p>
<p>space and ship are considered as separate thoughts</p>
<p>space::2 ship</p>
<p>space is twice as important as ship</p>
<p>Weights are normalized:
space:: ship is the same as space::1 ship, space:: ship::1,space::2 ship::2, space::100 ship::100, etc.
cheese::2 cake is the same as cheese::4 cake::2, cheese::100 cake::50 etc.
cheese:: cake:: painting is the same as cheese::1 cake::1 painting::1, cheese::1 cake:: painting::, cheese::2 cake::2 painting::2 etc.</p>
<p>Negative Prompt Weights</p>
<p>Negative weights can be added to parts of a multi-prompt to help remove unwanted elements.
The sum of all weights must be a positive number.</p>
<p>still life gouache painting</p>
<p>a range of objects appear in the still life</p>
<p>still life gouache painting:: fruit::-.5</p>
<p>The still life has fewer fruits</p>
<p>The --no Parameter</p>
<p>The --no parameter is the same as weighing part of a multi prompt to &quot;-.5&quot; vibrant tulip fields:: red::-.5 is the same as vibrant tulip fields --no red.</p>
<p>Previous
Image Prompts
Next
Remix
Midjourney is an independent research lab exploring new mediums of thought and expanding the imaginative powers of the human species. We are a small self-funded team focused on design, human infrastructure, and AI.
FOLLOW US: [F] [T] [R]
SUPPORT
For questions or support visit the Midjourney Discord support channels.
SITES
Midjourney Website
Midjourney Discord
Current
Legacy
Getting Started
Using Discord
Commands Parameters and Tools
Writing Prompts
Prompts
Describe
No
Image Prompts
Multi Prompts
Remix
Shorten
Working with Your Own Images
Styles and Aesthetics
Image Size and Aspect Ratio
Advanced Prompting Tools
Subscriptions
Using The Website
Policies
No
Light
The No parameter tells the Midjourney Bot what not to include in your image.</p>
<p>--no accepts multiple words separated with commas: --no item1, item2, item3, item4</p>
<p>--No Comparison</p>
<p>still life gouache painting</p>
<p>a range of objects appear in the still life</p>
<p>still life gouache painting --no fruit</p>
<p>The still life has fewer fruits</p>
<p>--no vs. Don't</p>
<p>The Midjourney Bot considers any word within the prompt as something you would like to see generated in the final image. Prompting still life gouache painting without any fruit or still life gouache painting dont add fruit! are more likely to produce pictures that include fruits because the relationship between &quot;without&quot; or &quot;don't&quot; and the &quot;fruit&quot; is not interpreted by the Midjourney Bot in the same way a human reader would understand it. To improve your results, focus your prompt on what you do want to see in the image and use the &quot;--no&quot; parameter to specify concepts you don't want to include.</p>
<p>still life gouache painting</p>
<p>a range of objects appear in the still life</p>
<p>still life gouache painting don't add fruit</p>
<p>more fruit is present in the final image.</p>
<p>Multi Prompting</p>
<p>The --no parameter is the same as weighing part of a multi prompt to &quot;-.5&quot; still life gouache painting:: fruit::-.5 is the same as still life gouache painting --no fruit.</p>
<p>How to Use the No Paramter</p>
<p>Add --no item1, item2, item3 to the end of your prompt.</p>
<p>Previous
Describe
Next
Image Prompts
Midjourney is an independent research lab exploring new mediums of thought and expanding the imaginative powers of the human species. We are a small self-funded team focused on design, human infrastructure, and AI.
FOLLOW US: [F] [T] [R]
SUPPORT
For questions or support visit the Midjourney Discord support channels.
SITES
Midjourney Website
Midjourney Discord
Current
Legacy
Getting Started
Using Discord
Commands Parameters and Tools
Writing Prompts
Prompts
Describe
No
Image Prompts
Multi Prompts
Remix
Shorten
Working with Your Own Images
Styles and Aesthetics
Image Size and Aspect Ratio
Advanced Prompting Tools
Subscriptions
Using The Website
Policies
Describe
Light
The /describe command allows you to upload an image and generate four possible prompts based on that image. Use the /describe command to explore new vocabulary and aesthetic movements.</p>
<p>/describe generates prompts that are inspirational and suggestive, it cannot be used to recreate an uploaded image exactly.
/describe returns the aspect ratio for uploaded images.</p>
<p>Previous
Prompts
Next
No
Midjourney is an independent research lab exploring new mediums of thought and expanding the imaginative powers of the human species. We are a small self-funded team focused on design, human infrastructure, and AI.
FOLLOW US: [F] [T] [R]
SUPPORT
For questions or support visit the Midjourney Discord support channels.
SITES
Midjourney Website
Midjourney Discord
Current
Legacy
Getting Started
Using Discord
Commands Parameters and Tools
Writing Prompts
Prompts
Describe
No
Image Prompts
Multi Prompts
Remix
Shorten
Working with Your Own Images
Styles and Aesthetics
Image Size and Aspect Ratio
Advanced Prompting Tools
Subscriptions
Using The Website
Policies
Prompts
Light
A Prompt is a short text phrase that the Midjourney Bot interprets to produce an image. The Midjourney Bot breaks down the words and phrases in a prompt into smaller pieces, called tokens, that can be compared to its training data and then used to generate an image. A well-crafted prompt can help make unique and exciting images.
Basic Prompts</p>
<p>A basic prompt can be as simple as a single word, phrase or emoji</p>
<p>Prompting Tip!</p>
<p>The Midjourney Bot works best with simple, short sentences that describe what you want to see. Avoid long lists of requests. Instead of: Show me a picture of lots of blooming California poppies, make them bright, vibrant orange, and draw them in an illustrated style with colored pencils Try: Bright orange California poppies drawn with colored pencils</p>
<p>Advanced Prompts</p>
<p>More advanced prompts can include one or more image URLs, multiple text phrases, and one or more parameters</p>
<p>Image Prompts</p>
<p>Image URLs can be added to a prompt to influence the style and content of the finished result. Image URLs always go at the front of a prompt.</p>
<p>Read more about Image Prompts</p>
<p>Prompt Text</p>
<p>The text description of what image you want to generate. See below for prompting information and tips. Well-written prompts help generate amazing images.</p>
<p>Parameters</p>
<p>Parameters change how an image generates. Parameters can change aspect ratios, models, upscalers, and lots more. Parameters go at the end of the prompt.</p>
<p>Read more about Parameters</p>
<p>Prompting Notes
Prompt Length</p>
<p>Prompts can be very simple. Single words (or even an emoji!) will produce an image. Very short prompts will rely heavily on Midjourney’s default style, so a more descriptive prompt is better for a unique look. However, super-long prompts aren’t always better. Concentrate on the main concepts you want to create.</p>
<p>Grammar</p>
<p>The Midjourney Bot does not understand grammar, sentence structure, or words like humans. Word choice also matters. More specific synonyms work better in many circumstances. Instead of big, try gigantic, enormous, or immense. Remove words when possible. Fewer words mean each word has a more powerful influence. Use commas, brackets, and hyphens to help organize your thoughts, but know the Midjourney Bot will not reliably interpret them. The Midjourney Bot does not consider capitalization.</p>
<p>Midjourney Model Version 4 is slightly better than other models at interpreting traditional sentence structure.</p>
<p>Focus on What you Want</p>
<p>It is better to describe what you want instead of what you don’t want. If you ask for a party with “no cake,” your image will probably include a cake. If you want to ensure an object is not in the final image, try advance prompting using the --no parameter.</p>
<p>Think About What Details Matter</p>
<p>Anything left unsaid may surprise you. Be as specific or vague as you want, but anything you leave out will be randomized. Being vague is a great way to get variety, but you may not get the specific details you want.</p>
<p>Try to be clear about any context or details that are important to you. Think about:</p>
<p>Subject: person, animal, character, location, object, etc.
Medium: photo, painting, illustration, sculpture, doodle, tapestry, etc.
Environment: indoors, outdoors, on the moon, in Narnia, underwater, the Emerald City, etc.
Lighting: soft, ambient, overcast, neon, studio lights, etc
Color: vibrant, muted, bright, monochromatic, colorful, black and white, pastel, etc.
Mood: Sedate, calm, raucous, energetic, etc.
Composition: Portrait, headshot, closeup, birds-eye view, etc.</p>
<p>Use Collective Nouns</p>
<p>Plural words leave a lot to chance. Try specific numbers. &quot;Three cats&quot; is more specific than &quot;cats.&quot; Collective nouns also work, “flock of birds” instead of &quot;birds.”</p>
<p>Previous
Writing Prompts
Next
Describe
Midjourney is an independent research lab exploring new mediums of thought and expanding the imaginative powers of the human species. We are a small self-funded team focused on design, human infrastructure, and AI.
FOLLOW US: [F] [T] [R]
SUPPORT
For questions or support visit the Midjourney Discord support channels.
SITES
Midjourney Website
Midjourney Discord
Current
Legacy
Getting Started
Using Discord
Commands Parameters and Tools
Commands
Parameters
Tools
Vary Region
Vary Region + Remix
Upscalers
Style Tuner
Pan
Zoom Out
Writing Prompts
Working with Your Own Images
Styles and Aesthetics
Image Size and Aspect Ratio
Advanced Prompting Tools
Subscriptions
Using The Website
Policies
Zoom Out
Light
The Zoom Out option allows you to extend the canvas of an upscaled image beyond its original boundaries without changing the content of the original image. The newly expanded canvas will be filled-in using guidance from the prompt and the original image.</p>
<p>Zoom Out does not increase the maximum 1024px x 1024 px size of an image.</p>
<p>Zoom Out</p>
<p>🔎 Zoom Out 2X 🔎 Zoom Out 1.5X buttons will appear after upscaling an image.</p>
<p>Prompt: vibrant California poppies</p>
<p>Starting Image
Zoom Out 1.5X
Zoom Out 2X</p>
<p>Make Square</p>
<p>With Make Square you can adjust the aspect ratio of a non-square image to make it square. If the original aspect ratio is wide (landscape), it will be expanded vertically. If it is tall (portrait), it will be expanded horizontally. The emoji ↔️ ↕️ next to the ↔️ Make Square button also indicates which way the image will be expanded. The ↔️ Make Square button will appear underneath non-square upscaled images.</p>
<p>Prompt: vibrant California poppies</p>
<p>Starting Image 
↕️ Make Square 
Starting Image 
↔️ Make Square </p>
<p>Custom Zoom</p>
<p>The 🔎 Custom Zoom button lets you choose how much to zoom out on an image. 🔎 Custom Zoom button under an upscaled image will pop up a dialogue box where you enter a custom value for --zoom. --zoom accepts values between 1-2.</p>
<p>Change the Aspect Ratio of an Upscaled Image</p>
<p>You can use --zoom 1 in this Custom Zoom pop-up box to change the aspect ratio, with the --ar parameter, without zooming out.</p>
<p>Change Your Prompt with Custom Zoom</p>
<p>🔎 Custom Zoom allows you to change the prompt before you expand your image, giving you finer control over the finished image. For example, changing the prompt to &quot;A framed picture on the wall&quot; gives this result:</p>
<p>Starting Image</p>
<p>prompt: vibrant California poppies</p>
<p>Custom Zoom + Changed Prompt</p>
<p>Zoom Out prompt: A framed picture on the wall --zoom 2</p>
<p>Previous
Pan
Next
Writing Prompts
Midjourney is an independent research lab exploring new mediums of thought and expanding the imaginative powers of the human species. We are a small self-funded team focused on design, human infrastructure, and AI.
FOLLOW US: [F] [T] [R]
SUPPORT
For questions or support visit the Midjourney Discord support channels.
SITES
Midjourney Website
Midjourney Discord
Current
Legacy
Getting Started
Using Discord
Commands Parameters and Tools
Commands
Parameters
Tools
Vary Region
Vary Region + Remix
Upscalers
Style Tuner
Pan
Zoom Out
Writing Prompts
Working with Your Own Images
Styles and Aesthetics
Image Size and Aspect Ratio
Advanced Prompting Tools
Subscriptions
Using The Website
Policies
Pan
Light
The Pan option allows you to expand the canvas of an image in a chosen direction without changing the content of the original image. The newly expanded canvas will be filled in using guidance from the prompt and the original image.</p>
<p>Pan allows you to increase the image resolution beyond the maximum 1024px x 1024px dimensions in one direction.
Pan is compatible with Midjourney Model Version 5, 5.1, 5.2, and niji 5</p>
<p>Panning an Image</p>
<p>The Pan buttons ⬅️ ➡️ ⬆️ ⬇️ will appear after upscaling an image. When panning, only the closest 512 pixels to the side of the image, along with the prompt, are used to determine the new section.</p>
<p>Prompt: A vibrant fantasy landscape</p>
<p>Starting Image
Pan ⬆️
Pan ⬇️
Pan ⬅️
Pan ➡️</p>
<p>After panning an image once, you can only pan that image again in the same direction (horizontal/vertical). You may continue to pan in that direction as often as you like. After panning several times, the image may become too large to send on Discord. When this happens, you will be sent a link to the image instead.</p>
<p>Pan ⬅️ and ➡️ several times</p>
<p>Pan with Remix Mode</p>
<p>Panning supports Remix Mode. This allows you to change your prompt when panning.</p>
<p>Starting Image</p>
<p>Starting prompt: A vibrant fantasy landscape</p>
<p>Pan ➡️ + Remixed Prompt</p>
<p>Pan ➡️ +  Remix prompt: A vibrant fantasy city</p>
<p>Previous
Style Tuner
Next
Zoom Out
Midjourney is an independent research lab exploring new mediums of thought and expanding the imaginative powers of the human species. We are a small self-funded team focused on design, human infrastructure, and AI.
FOLLOW US: [F] [T] [R]
SUPPORT
For questions or support visit the Midjourney Discord support channels.
SITES
Midjourney Website
Midjourney Discord
Current
Legacy
Getting Started
Using Discord
Commands Parameters and Tools
Commands
Parameters
Tools
Vary Region
Vary Region + Remix
Upscalers
Style Tuner
Pan
Zoom Out
Writing Prompts
Working with Your Own Images
Styles and Aesthetics
Image Size and Aspect Ratio
Advanced Prompting Tools
Subscriptions
Using The Website
Policies
Style Tuner
Light
Personalize the appearance of your Midjourney images using the Style Tuner. Use the /tune command to generate a range of sample images showing different visual styles based on your prompt. Choose your favorite images, and you'll receive a unique code you can use to customize the look of future Jobs.</p>
<p>Share your Style Tuner and Codes with others to share, explore, and experiment with different aesthetics.</p>
<p>/tune and codes are only compatible with Midjourney Model Version 5.2
/tune is only available while in Fast Mode.
--style parameters created with the Style Tuner are compatible with --stylize values between 20–1000.</p>
<p>How to Use the Style Tuner</p>
<ol>
<li>Generate Your Custom Style Tuner</li>
</ol>
<p>Create a Style Tuner page using the /tune command.</p>
<ol start="2">
<li>Select Your Preferred Options
Style Directions: Choose the number of image pairs you want to see in your Style Tuner (16, 32, 64, or 128 pairs).
Default Mode: Select the style mode for your sample images (Default or Raw). If you typically do not use the --style raw parameter with your prompts, choose &quot;default.&quot;</li>
</ol>
<p>Use an Exisiting Style Tuner</p>
<p>If another user has previously generated a Style Tuner with your prompt, you will receive a link to that Tuner. Click the link to access the existing Style Tuner. Using a previously generated Style Tuner does not use your subscription's GPU minutes.</p>
<ol start="3">
<li>Submit your Job
Click the Submit button.
Confirm your submission.</li>
</ol>
<p>Your Style Tuner generates a pair of images for each Style Direction. A Style Tuner with 16 directions will generate 32 images. A Style Tuner with 128 directions will generate 256 images. Generating these images uses your subscription's Fast GPU time.</p>
<ol start="4">
<li>Open Your Custom Style Tuner
When your Style Tuner is ready, the Midjourney Bot will send you a direct message with a link to your Tuner.
Click the link to open your Style Tuner in your web browser.</li>
</ol>
<p>Try this Style Tuner: https://tuner.midjourney.com/ejYLCOY</p>
<ol start="5">
<li>Select images</li>
</ol>
<p>Your Style Tuner will show rows of image pairs, each representing a distinct visual direction for your prompt. Click on the image you prefer in each pair. If you don't feel strongly about either image, leave the empty middle box selected.</p>
<ol start="6">
<li>Copy Your Code</li>
</ol>
<p>The Style Tuner generates a code you can add to your prompts with the --style <code> Parameter. Learn more about parameters.</p>
<p>To copy your prompt and Parameter</p>
<p>Find your customized code at the bottom of the page.
Click the Copy button to copy your original prompt and newly generated --style <code> parameter.</p>
<p>You can share your Style Tuner page with friends and generate new codes without using any additional GPU minutes!</p>
<ol start="7">
<li>
<p>Generate an Image
Return to Discord
Use the /imagine command and paste your copied prompt and --style <code> parameter into the prompt field.
Generate your image</p>
</li>
<li>
<p>Use Additional Midjourney Tools</p>
</li>
</ol>
<p>Take your image further by using other Midjourney tools like Upscale, Pan, Zoom-Out, Remix, or Vary-Region.</p>
<ol start="9">
<li>Experiment and Explore
Use your style code with a new prompt:</li>
</ol>
<p>The Style Tuner you create uses your initial prompt to create sample images and help you visualize the impact of your choices. However, the generated codes can be used with any prompt. Remember that styles and prompts always work together to generate an image, so a style code may not transfer as intended to other prompts.</p>
<p>Experiment: Style codes and prompts interact in complex ways. A code may have a strong effect on one prompt and a subtle effect on a similar prompt. The images you choose in your Style Tuner can combine in unexpected and creative ways. Use style codes as a tool to explore new looks and visuals.
Generate more codes: You can return to your Style Tuner page at any time to change your selections and create new codes.
Share style codes: You can share or use style codes created by friends. Check out the Midjourney style-tuner channel to share your codes and discover new styles!
Find a Style Tuner page: Find the Style Tuner page for any style code by adding it to this URL: https://tuner.midjourney.com/code/StyleCodeHere.</p>
<ol start="10">
<li>Save and Reuse Your Codes
Use the /settings command and turn on 📌Sticky Style. Sticky Style will save the last --style parameter used in your personal suffix, so you don't have to repeat the code on future prompts. Change codes by using a new --style or unselecting 📌Sticky Style.
Use custom options to store your favorite codes.
Or, create your own Discord server to organize your images, prompts, image references, and Style Tuner codes.</li>
</ol>
<p>Style Tune Examples</p>
<p>prompt vibrant california poppies
All images were made by style created using this Style Tuner</p>
<p>Random Codes</p>
<p>Use the --style random parameter to apply a random 32 base styles Style Tuner code to your prompt. You can also use --style random-16, --style random-64 or --style random-128 to use random results from other lengths of tuners.</p>
<p>--random simulates Style Tuner code with random selections chosen for 75% of the image pairs. You can adjust this percentage by adding a number to the end of the --random parameter. For example, --style random-32-15 simulates a 32-pair tuner with 15% of the image pairs selected, --style random-128-80 simulates a 128-pair tuner with 80% of the image pairs selected.</p>
<p>Combine Codes</p>
<p>Combine multiple codes in one parameter with a hyphen, --style code1-code2.
Combine multiple codes and style raw: --style raw-code1-code2</p>
<p>Style Tuner and --stylize</p>
<p>The --stylize parameter adjusts the influence of the --style parameter on your generated images. If you're not seeing the desired effect from your code, consider combining it with higher stylization values, like --stylize 250, or --stylize 500.</p>
<p>Comparison</p>
<p>prompt vibrant California poppies --style fdeQ4zOX5jd --stylize 250</p>
<p>--stylize 20</p>
<p>--stylize 100 (default)</p>
<p>--stylize 250</p>
<p>--stylize 750</p>
<p>Style Raw</p>
<p>Combine your custom style code with Midjourney Style Raw by using --style raw-<code>.
Example: To use Style Raw and --style fjo5S8BgMoV use --style raw-fjo5S8BgMoV.</p>
<p>Style Tuner and Niji Model Version</p>
<p>Style Tuner codes created with the Midjourney Bot are not compatible with the Niji Model version accessed through the Midjourney Bot. To create a Style Tuner or Code for Niji Model version, join the Niji Discord community and interact with the Niji Bot in the same way you interact with the Midjourney Bot. Your Midjourney subscription gives you access to the Niji community and Bot.</p>
<p>Technical Details</p>
<p>/tune is compatible with prompts that include the following:
--aspect
--chaos
--tile
multi prompts</p>
<p>/tune and style codes are not compatible with image prompts that do not include a text prompt.</p>
<p>If your /tune command does not return a clickable link, check that Embeds and Link Previews is enabled in your Discord App Settings</p>
<p>Previous
Upscalers
Next
Pan
Midjourney is an independent research lab exploring new mediums of thought and expanding the imaginative powers of the human species. We are a small self-funded team focused on design, human infrastructure, and AI.
FOLLOW US: [F] [T] [R]
SUPPORT
For questions or support visit the Midjourney Discord support channels.
SITES
Midjourney Website
Midjourney Discord
Current
Legacy
Getting Started
Using Discord
Commands Parameters and Tools
Commands
Parameters
Tools
Vary Region
Vary Region + Remix
Upscalers
Style Tuner
Pan
Zoom Out
Writing Prompts
Working with Your Own Images
Styles and Aesthetics
Image Size and Aspect Ratio
Advanced Prompting Tools
Subscriptions
Using The Website
Policies
Upscalers
Light
The current Midjourney and Niji Model Versions produce grids of 1024 x 1024 pixel images. Use the U1 U2 U3 U4 buttons under each image grid to separate your selected image. You can then use the Upscale (2x) or Upscale (4x) tools to increase the size of your image.</p>
<p>Upscale tools use your subscription's GPU minutes. Using Upscale 2X on an image takes roughly twice as long as generating an initial image grid. Using Upscale 4X on an image takes roughly six times as long as generating an initial image grid.</p>
<p>Upscale tools are not compatible with the pan tool or the tile parameter.</p>
<p>How to Use the Upscale Tools</p>
<ol>
<li>Generate an Image</li>
</ol>
<p>Create an image using the /imagine command.</p>
<ol start="2">
<li>Select an Image</li>
</ol>
<p>Use a U button to separate your selected image from the grid.</p>
<ol start="3">
<li>Select Upscale</li>
</ol>
<p>Click on the Upscale button to upscale your image. The upscaler will double the size of your image.</p>
<p>Upscale Comparison</p>
<p>Prompt: chiaroscuro rooster portrait
Original 1024 by 1024 pixel image.</p>
<p>Detail from original image</p>
<p>Original image</p>
<p>After Upscale (2x) to 2048 x 2048 px</p>
<p>Upscale Comparison</p>
<p>Prompt: 1960s pop-art acrylic of redwoods
Original 1024 by 1024 pixel image.</p>
<p>Detail from the original image</p>
<p>Original image</p>
<p>After Upscale (4x) to 4096 x 4096 px</p>
<p>Upscale Tips</p>
<p>Upscale older Jobs by refreshing them with the /show command.</p>
<p>Learn More</p>
<p>Learn more about image sizes, dimensions, and DPI.</p>
<p>Legacy Upscalers</p>
<p>Earlier Midjourney model versions generated grids of lower-resolution images. You can use a legacy Midjourney upscaler on these images to increase the size and add additional details. There are multiple legacy upscale models available for upscaling images made with earlier Midjourney models. Using a legacy upscaler uses your subscription's GPU minutes.</p>
<p>Previous
Vary Region + Remix
Next
Style Tuner
Midjourney is an independent research lab exploring new mediums of thought and expanding the imaginative powers of the human species. We are a small self-funded team focused on design, human infrastructure, and AI.
FOLLOW US: [F] [T] [R]
SUPPORT
For questions or support visit the Midjourney Discord support channels.
SITES
Midjourney Website
Midjourney Discord
Current
Legacy
Getting Started
Using Discord
Commands Parameters and Tools
Commands
Parameters
Tools
Vary Region
Vary Region + Remix
Upscalers
Style Tuner
Pan
Zoom Out
Writing Prompts
Working with Your Own Images
Styles and Aesthetics
Image Size and Aspect Ratio
Advanced Prompting Tools
Subscriptions
Using The Website
Policies
Vary Region + Remix
Light
Combine Remix Mode and the Midjourney Vary Region editor to select and regenerate specific parts of an upscaled image using a new or modified prompt.</p>
<p>This tool requires you to use up-to-date Discord client versions, if you do not see the Vary Region button, try updating your Discord client.</p>
<p>The Vary (Region) button appears after a Midjourney image has been upscaled.
Regional variations results are guided by the content in your original image, the area you select, and the modified prompt used.
Vary (Region) is compatible with Midjourney Model Versions V5.0, V5.1, V5.2, and niji 5</p>
<p>How to Use Remix Mode with Vary (Region)</p>
<ol>
<li>Enable Remix Mode</li>
</ol>
<p>Use the /settings command and select 🎛️ Remix from the pop-up.</p>
<ol start="2">
<li>Generate an Image</li>
</ol>
<p>Create an image using the /imagine command.</p>
<ol start="3">
<li>Upscale the Image</li>
</ol>
<p>Use the U buttons to upscale your selected image.</p>
<ol start="4">
<li>Select Vary Region</li>
</ol>
<p>Click the 🖌️ Vary (Region) button to open the editing interface.</p>
<ol start="5">
<li>
<p>Select Areas to Regenerate
Choose the freehand or rectangular selection tools in the lower left of the Editor.
Select the areas of your image that you want to regenerate.
The size of your selection will affect your results. Larger selections give the Midjourney Bot more room to generate new creative details. Smaller selections will result in smaller, more subtle changes.
Note: You cannot edit an existing selection but can undo multiple steps using the undo button in the upper right corner.</p>
</li>
<li>
<p>Modify Your Prompt</p>
</li>
</ol>
<p>Describe what you want to generate in the selected areas with an updated prompt. Your revised prompt should focus on the details you want to introduce or change.</p>
<ol start="7">
<li>Submit Your Job</li>
</ol>
<p>Click the Submit → button to send your request to Midjourney Bot. The Vary Region editor can now be closed, and you can return to Discord while your Job is processed.</p>
<p>Note You can use the Editor Button underneath an upscaled image multiple times to experiment with different selections and prompts.</p>
<ol start="8">
<li>View Your Results</li>
</ol>
<p>The Midjourney Bot will process your Job and generate a new image grid using the information from your original image and the guidance of your new prompt.</p>
<p>New Image Grid</p>
<p>Original vs. Varied Comparison</p>
<ol start="8">
<li>Upscale and Vary Region again</li>
</ol>
<p>You can upscale one of your new images and use the Vary Region Editor again to continue refining your image.</p>
<p>Selection
Result</p>
<p>Updated Prompt: hot air balloon lithograph</p>
<p>Result</p>
<p>Updated Prompt: castle in the meadow lithograph</p>
<p>Vary Region + Remix Prompting Tips</p>
<p>Selections The size of your selection affects the outcome. Larger selections provide the Midjourney Bot with more contextual information, which can improve the scaling and context of new additions. But selecting too much may lead to the newly generated elements blending or replacing parts of the original image you wished to preserve.</p>
<p>Prompts Experiment with how you modify your prompt when Using Vary Region + Remix Mode. Prompts should focus on what you want to happen in the selected area. The Midjourney Bot also considers the existing image when generating a selected area, so shorter focused prompts often are more effective. Midjourney Prompts should not be conversational. Instead of &quot;Please change the meadow trail into a beautiful stream,&quot; be direct and prompt &quot;meadow stream.&quot;</p>
<p>Work in Small Steps If you want to change many parts of an image, work on one part at a time. This way, you can create a focused prompt for each part.</p>
<p>Vary Region + Remix Examples</p>
<p>Original Image</p>
<p>Prompt: gouache alligator in sunglasses</p>
<p>Selection</p>
<p>Updated Prompt: gouache alligator in green sunglasses</p>
<p>Result</p>
<p>Updated Prompt: gouache alligator in green sunglasses</p>
<p>Original Image</p>
<p>Prompt: scratchboard apple tree branch</p>
<p>Selection</p>
<p>Updated Prompt: rainbow scratchboard apple tree branch</p>
<p>Result</p>
<p>Updated Prompt: rainbow scratchboard apple tree branch</p>
<p>Technical Details</p>
<p>Jobs generated using Vary (Region) + Remix Mode will honor the following parameters:
--chaos
--fast
--iw
--no
--stylize
--relax
--style
--version
--video
--weird</p>
<p>Previous
Vary Region
Next
Upscalers
Midjourney is an independent research lab exploring new mediums of thought and expanding the imaginative powers of the human species. We are a small self-funded team focused on design, human infrastructure, and AI.
FOLLOW US: [F] [T] [R]
SUPPORT
For questions or support visit the Midjourney Discord support channels.
SITES
Midjourney Website
Midjourney Discord
Current
Legacy
Getting Started
Using Discord
Commands Parameters and Tools
Commands
Parameters
Tools
Vary Region
Vary Region + Remix
Upscalers
Style Tuner
Pan
Zoom Out
Writing Prompts
Working with Your Own Images
Styles and Aesthetics
Image Size and Aspect Ratio
Advanced Prompting Tools
Subscriptions
Using The Website
Policies
Vary Region
Light
Use the Midjourney Vary Region editor to select and regenerate specific parts of an upscaled image.</p>
<p>The Vary (Region) button appears after a Midjourney image has been upscaled.
Regional variations are guided by the content in your original image and the area you select.
Vary (Region) is compatible with Midjourney Model Versions V5.0, V5.1, V5.2, and niji 5</p>
<p>How to Use Vary Region</p>
<ol>
<li>Generate an Image</li>
</ol>
<p>Create an image using the /imagine command.</p>
<ol start="2">
<li>Upscale the Image</li>
</ol>
<p>Use the U buttons to upscale your selected image.</p>
<ol start="3">
<li>Select Vary Region</li>
</ol>
<p>Click on the 🖌️ Vary (Region) button to open the editing interface.</p>
<ol start="4">
<li>
<p>Select Areas to Regenerate
Choose the freehand or rectangular selection tools in the lower left of the Editor.
Select the areas of your image that you want to regenerate.
The size of your selection will affect your results. Larger selections give the Midjourney Bot more room to generate new creative details. Smaller selections will result in smaller, more subtle changes.
Note: You cannot edit an existing selection but can undo multiple steps using the undo button in the upper right corner.</p>
</li>
<li>
<p>Submit Your Job</p>
</li>
</ol>
<p>Click the Submit → button to send your request to Midjourney Bot. The Vary Region editor can now be closed, and you can return to Discord while your Job is processed.</p>
<p>Note You can use the 🖌️ Vary (Region) button underneath an upscaled image multiple times to experiment with different selections. Your previous selection will be preserved. You can continue to add to this existing selection or use the undo button to clear your selection.</p>
<ol start="6">
<li>View Your Results</li>
</ol>
<p>The Midjourney Bot will process your Job and generate a new image grid of variations within the area you selected.</p>
<p>Vary Region Examples</p>
<p>Upscaled Image</p>
<p>Prompt: colorful candy brooches</p>
<p>Selection</p>
<p>Result</p>
<p>Upscaled Image</p>
<p>Prompt: architectural drawing of a house</p>
<p>Selection</p>
<p>Result</p>
<p>Vary Region + Remix Mode</p>
<p>You can use Remix mode with the Vary Region editor to update your prompt while regenerating specific parts of your image. Read about using Remix Mode with the Vary Region editor.</p>
<p>Original Image</p>
<p>Prompt: gouache alligator in sunglasses</p>
<p>Selection</p>
<p>Updated Prompt: gouache alligator in green sunglasses</p>
<p>Result</p>
<p>Updated Prompt: gouache alligator in green sunglasses</p>
<p>Technical Details</p>
<p>Jobs generated using Vary (Region) will honor the following parameters:
--chaos
--fast
--iw
--no
--stylize
--relax
--style
--version
--video
--weird</p>
<p>Previous
Weird
Next
Vary Region + Remix
Midjourney is an independent research lab exploring new mediums of thought and expanding the imaginative powers of the human species. We are a small self-funded team focused on design, human infrastructure, and AI.
FOLLOW US: [F] [T] [R]
SUPPORT
For questions or support visit the Midjourney Discord support channels.
SITES
Midjourney Website
Midjourney Discord
Current
Legacy
Getting Started
Using Discord
Commands Parameters and Tools
Commands
Parameters
Parameter List
Aspect Ratios
Chaos
No
Quality
Repeat
Seeds
Stop
Style
Stylize
Tile
Version
Video
Weird
Tools
Writing Prompts
Working with Your Own Images
Styles and Aesthetics
Image Size and Aspect Ratio
Advanced Prompting Tools
Subscriptions
Using The Website
Policies
Weird
Light
Explore unconventional aesthetics with the experimental --weird or --w parameter. This parameter introduces quirky and offbeat qualities to your generated images, resulting in unique and unexpected outcomes.</p>
<p>--weird accepts values: 0–3000.
The default --weird value is 0.
--weird is a highly experimental feature. What's weird may change over time
--weird is compatible with Midjourney Model Versions 5, 5.1, 5.2 and niji 5
--weird is not fully compatible with seeds</p>
<p>The Influence of Weird on Jobs</p>
<p>The optimal --weird value is dependent on the prompt and requires experimentation. Try starting with smaller values, such as 250 or 500, and then go up/down from there. If you want a generation to be conventionally attractive and weird, try mixing higher --stylize values with --weird. Try starting with similar values for both. Example /imagine prompt cyanotype cat --stylize 250 --weird 250.</p>
<p>prompt example: /imagine prompt cyanotype cat --weird 250</p>
<p>--weird 0
--weird 250
--weird 500
--weird 1000</p>
<p>prompt example: /imagine prompt lithograph potato --weird 250</p>
<p>--weird 0
--weird 250
--weird 500
--weird 1000</p>
<p>prompt example: /imagine prompt clockwork chicken --weird 250</p>
<p>--weird 0
--weird 250
--weird 500
--weird 1000</p>
<p>What's the difference between --weird, --chaos, and --stylize?</p>
<p>--chaos controls how diverse the initial grid images are from each other.
--stylize controls how strongly Midjourney's default aesthetic is applied.
--weird controls how unusual an image is compared to previous Midjourney images.</p>
<p>How to Use the Weird Parameter</p>
<p>Add --weird <value> or --w <value> to the end of your prompt.</p>
<p>Previous
Video
Next
Vary Region
Midjourney is an independent research lab exploring new mediums of thought and expanding the imaginative powers of the human species. We are a small self-funded team focused on design, human infrastructure, and AI.
FOLLOW US: [F] [T] [R]
SUPPORT
For questions or support visit the Midjourney Discord support channels.
SITES
Midjourney Website
Midjourney Discord
Current
Legacy
Getting Started
Using Discord
Commands Parameters and Tools
Commands
Parameters
Parameter List
Aspect Ratios
Chaos
No
Quality
Repeat
Seeds
Stop
Style
Stylize
Tile
Version
Video
Weird
Tools
Writing Prompts
Working with Your Own Images
Styles and Aesthetics
Image Size and Aspect Ratio
Advanced Prompting Tools
Subscriptions
Using The Website
Policies
Video
Light
Use the --video parameter to create a short movie of your initial image grid being generated. React to the finished job with the envelope ✉️ emoji to have the Midjourney Bot send a link to the video to your Direct Messages.</p>
<p>--video only works on image grids, not upscales.
--video works with Model Versions 5, 5.1, 5.2, and niji 5.
--video works with Legacy Model Versions 1, 2, 3, test, and testp.</p>
<p>Video Examples</p>
<p>Vibrant California Poppies</p>
<p>Botanical Sketch of Fanciful Ferns</p>
<p>How to Get a Video Link</p>
<p>Prompt example: /imagine prompt Vibrant California Poppies --video</p>
<p>1 Add --video to the end of your prompt.</p>
<p>2 Once the Job has finished, click Add Reaction</p>
<p>3 Select the ✉️ Envelope emoji.</p>
<p>4 The Midjourney bot will send a link to the video to your Direct Messages.</p>
<p>5 Click the link to view your video within a browser. Right-click or Long Press to download the video.</p>
<p>How to Use the Video Parameter</p>
<p>Add --video to the end of your prompt.</p>
<p>Previous
Version
Next
Weird
Midjourney is an independent research lab exploring new mediums of thought and expanding the imaginative powers of the human species. We are a small self-funded team focused on design, human infrastructure, and AI.
FOLLOW US: [F] [T] [R]
SUPPORT
For questions or support visit the Midjourney Discord support channels.
SITES
Midjourney Website
Midjourney Discord
Current
Legacy
Getting Started
Using Discord
Commands Parameters and Tools
Commands
Parameters
Parameter List
Aspect Ratios
Chaos
No
Quality
Repeat
Seeds
Stop
Style
Stylize
Tile
Version
Video
Weird
Tools
Writing Prompts
Working with Your Own Images
Styles and Aesthetics
Image Size and Aspect Ratio
Advanced Prompting Tools
Subscriptions
Using The Website
Policies
Version
Light
Midjourney routinely releases new model versions to improve efficiency, coherency, and quality. The latest model is the default, but other models can be used by adding the --version or --v parameter or by using the /settings command and selecting a model version. Each model excels at producing different types of images.</p>
<p>--version accepts the values 1, 2, 3, 4, 5, 5.1, and 5.2
--version can be abbreviated --v
--v 5.2 is the current default model.</p>
<p>Model Version 5.2</p>
<p>The Midjourney V5.2 model is the newest and most advanced, released June 2023. To use this model, add the --v 5.2 parameter to the end of a prompt, or use the /settings command and select 5️⃣ MJ Version 5.2</p>
<p>Default Model 06/22/23–current</p>
<p>This model produces more detailed, sharper results with better colors, contrast, and compositions. It also has a slightly better understanding of prompts than earlier models and is more responsive to the full range of the --stylize parameter.</p>
<p>Prompt: vibrant California poppies --v 5.2
Prompt: high contrast surreal collage --v 5.2</p>
<p>Model Version 5.2 + Style Raw Parameter</p>
<p>Midjourney Model Versions 5.1 and 5.2 can be fine-tuned with the --style raw parameter to reduce the Midjourney default aesthetic.</p>
<p>Read more about the Midjourney --style parameter.</p>
<p>default --v 5.2</p>
<p>vibrant California poppies</p>
<p>--v 5.2 --style raw</p>
<p>vibrant California poppies --style raw</p>
<p>default --v 5.2</p>
<p>high contrast surreal collage</p>
<p>--v 5.2 --style raw</p>
<p>high contrast surreal collage --style raw</p>
<p>Model Version 5.1</p>
<p>The Midjourney V5.1 was released on May 4th, 2023. To use this model, add the --v 5.1 parameter to the end of a prompt, or use the /settings command and select 5️⃣ MJ Version 5.1</p>
<p>Default Model 05/03/23–06/22/23</p>
<p>This model has a stronger default aesthetic than earlier versions, making it easier to use with simple text prompts. It also has high Coherency, excels at accurately interpreting natural language prompts, produces fewer unwanted artifacts and borders, has increased image sharpness, and supports advanced features like repeating patterns with --tile.</p>
<p>Prompt: vibrant California poppies --v 5.1
Prompt: high contrast surreal collage --v 5.1</p>
<p>Model Version 5.0</p>
<p>The Midjourney V5.0 model produces more photographic generations than the V5.1 model. This model produces images that closely match the prompt but may require longer prompts to achieve your desired aesthetic.</p>
<p>Default Model 03/30/23–05/03/23</p>
<p>To use this model, add the --v 5 parameter to the end of a prompt, or use the /settings command and select 5️⃣ MJ Version 5</p>
<p>Prompt: vibrant California poppies --v 5
Prompt: high contrast surreal collage --v 5</p>
<p>Model Version 4</p>
<p>The Midjourney V4 was the default model from Nov 2022–May 2023. This model featured an entirely new codebase and brand-new AI architecture designed by Midjourney and trained on the new Midjourney AI supercluster. Model Version 4 had increased knowledge of creatures, places, and objects compared to previous models.</p>
<p>This model has very high Coherency and excels with Image Prompts.</p>
<p>Prompt: vibrant California poppies
Prompt: high contrast surreal collage</p>
<p>Niji Model 5</p>
<p>The Niji model is a collaboration between Midjourney and Spellbrush tuned to produce anime and illustrative styles with vastly more knowledge of anime, anime styles, and anime aesthetics. It's excellent at dynamic and action shots and character-focused compositions.</p>
<p>To use this model, add the --niji 5 parameter to the end of a prompt, or use the /settings command and select 🍏 Niji version 5</p>
<p>This model is sensitive to the --stylize parameter. Experiment with different stylization ranges to fine-tune your images.</p>
<p>Niji Style Parameters</p>
<p>Niji Model Version 5 can also be fine-tuned with --style parameters to achieve unique looks. Try --style cute, --style scenic, --style original (uses the original Niji Model Version 5, which was the default before May 26th, 2023), or --style expressive.</p>
<p>default --niji 5</p>
<p>birds perching on a twig --niji 5</p>
<p>--style original</p>
<p>birds perching on a twig --niji 5 --style original</p>
<p>--style cute</p>
<p>birds perching on a twig --niji 5 --style cute</p>
<p>--style expressive</p>
<p>birds perching on a twig --niji 5 --style expressive</p>
<p>--style scenic</p>
<p>birds perching on a twig --niji 5 --style scenic</p>
<p>Niji 5 vs. Midjourney Version 5.2
--v 5.2</p>
<p>vibrant California poppies</p>
<p>--niji 5</p>
<p>vibrant California poppies --niji 5</p>
<p>--v 5.2</p>
<p>birds sitting on a twig</p>
<p>--niji 5</p>
<p>birds sitting on a twig --niji 5</p>
<p>How to Switch Models
Use the Version or Test Parameter</p>
<p>Add --v 4 --v 5 --v 5.1 --v 5.1 --style raw --v 5.2 --v 5.2 --style raw --niji 5 --niji 5 --style cute --niji 5 --style expressive --niji 5 --style original or --niji 5 --style scenic to the end of your prompt.</p>
<p>Use the Settings Command</p>
<p>Type /settings and select your preferred version from the menu.</p>
<p>4️⃣ MJ Version 4 5️⃣ MJ Version 5 5️⃣ MJ Version 5.1 5️⃣ MJ Version 5.2 🍎 Niji Version 5
Legacy Models</p>
<p>You can experiment with early Midjourney models using the --version parameter Read more about Midjourney Legacy Models.</p>
<p>Previous
Tile
Next
Video
Midjourney is an independent research lab exploring new mediums of thought and expanding the imaginative powers of the human species. We are a small self-funded team focused on design, human infrastructure, and AI.
FOLLOW US: [F] [T] [R]
SUPPORT
For questions or support visit the Midjourney Discord support channels.
SITES
Midjourney Website
Midjourney Discord
Current
Legacy
Getting Started
Using Discord
Commands Parameters and Tools
Commands
Parameters
Parameter List
Aspect Ratios
Chaos
No
Quality
Repeat
Seeds
Stop
Style
Stylize
Tile
Version
Video
Weird
Tools
Writing Prompts
Working with Your Own Images
Styles and Aesthetics
Image Size and Aspect Ratio
Advanced Prompting Tools
Subscriptions
Using The Website
Policies
Tile
Light
The --tile parameter generates images that can be used as repeating tiles to create seamless patterns for fabrics, wallpapers and textures.</p>
<p>--tile works with Model Versions 1, 2, 3, test, testp, 5, 5.1, and 5.2 .
--tile only generates a single tile. Use a pattern making tool like this Seamless Pattern Checker to see the tile repeat.</p>
<p>Tile Examples
prompt scribble of moss on rocks --tile
prompt watercolor koi --tile
How to Use the Tile Parameter</p>
<p>Add --tile to the end of your prompt.</p>
<p>Previous
Stylize
Next
Version
Midjourney is an independent research lab exploring new mediums of thought and expanding the imaginative powers of the human species. We are a small self-funded team focused on design, human infrastructure, and AI.
FOLLOW US: [F] [T] [R]
SUPPORT
For questions or support visit the Midjourney Discord support channels.
SITES
Midjourney Website
Midjourney Discord
Current
Legacy
Getting Started
Using Discord
Commands Parameters and Tools
Commands
Parameters
Parameter List
Aspect Ratios
Chaos
No
Quality
Repeat
Seeds
Stop
Style
Stylize
Tile
Version
Video
Weird
Tools
Writing Prompts
Working with Your Own Images
Styles and Aesthetics
Image Size and Aspect Ratio
Advanced Prompting Tools
Subscriptions
Using The Website
Policies
Stylize
Light
The Midjourney Bot has been trained to produce images that favor artistic color, composition, and forms. The --stylize or --s parameter influences how strongly this training is applied. Low stylization values produce images that closely match the prompt but are less artistic. High stylization values create images that are very artistic but less connected to the prompt.</p>
<p>--stylize's default value is 100 and accepts integer values 0–1000 when using the current model</p>
<p>Different Midjourney Version Models have different stylize ranges.</p>
<pre><code>Version 5, 5.1, 5.2	Version 4	niji 5
</code></pre>
<p>Stylize default	100	100	100
Stylize Range	0–1000	0–1000	0–1000</p>
<p>Common Stylize Settings
Influence of Stylize on Model Version 5.2</p>
<p>prompt example: /imagine prompt child's drawing of a cat --s 100</p>
<p>--stylize 0
--stylize 50
Equal to 🖌️ Style Low
--stylize 100 (default)
Equal to 🖌️ Style Med</p>
<p>--stylize 250
Equal to 🖌️ Style High
--stylize 500
--stylize 750
Equal to 🖌️ Style Very High</p>
<p>prompt example: /imagine prompt colorful risograph of a fig --s 100</p>
<p>--stylize 50</p>
<p>Equal to 🖌️ Style Low</p>
<p>--stylize 100 (default)</p>
<p>Equal to 🖌️ Style Med</p>
<p>--stylize 250</p>
<p>Equal to 🖌️ Style High</p>
<p>--stylize 750</p>
<p>Equal to 🖌️ Style Very High</p>
<p>Midjourney Model Version 5.2 is more sensitive to different stylize values. If you previously used very high stylize values, they may require adjustment for this model version. We recommend reducing your stylize value to 20% of your previous value. For example, if you were using --stylize 1000 --V 5.1, try using --stylize 200 instead.</p>
<p>Influence of Stylize on Model Version 5.1</p>
<p>prompt example: /imagine prompt colorful risograph of a fig --v 5.1 --s 100</p>
<p>--stylize 50</p>
<p>🖌️ Style Low</p>
<p>--stylize 100 (default)</p>
<p>🖌️ Style Med</p>
<p>--stylize 250</p>
<p>🖌️ Style High</p>
<p>--stylize 750</p>
<p>🖌️ Style Very High</p>
<p>Influence of Stylize on Niji 5</p>
<p>prompt example: /imagine prompt colorful risograph of a fig --niji 5 --s 100</p>
<p>--stylize 50</p>
<p>🖌️ Style Low</p>
<p>--stylize 100 (default)</p>
<p>🖌️ Style Med</p>
<p>--stylize 250</p>
<p>🖌️ Style High</p>
<p>--stylize 750</p>
<p>🖌️ Style Very High</p>
<p>Influence of Stylize on Model V4</p>
<p>prompt example: /imagine prompt illustrated figs --v 4 --s 100</p>
<p>--stylize 50</p>
<p>🖌️ Style Low</p>
<p>--stylize 100 (default)</p>
<p>🖌️ Style Med</p>
<p>--stylize 250</p>
<p>🖌️ Style High</p>
<p>--stylize 750</p>
<p>🖌️ Style Very High</p>
<p>How to Switch Stylization Values
Use the Stylize Parameter</p>
<p>Add --stylize <value> or --s <value> to the end of your prompt.</p>
<p>Use the Settings Command</p>
<p>Type /settings and select your preferred stylize value from the menu.</p>
<p>🖌️ Style Low 🖌️ Style Med 🖌️ Style High 🖌️ Style Very High
Previous
Style
Next
Tile
Midjourney is an independent research lab exploring new mediums of thought and expanding the imaginative powers of the human species. We are a small self-funded team focused on design, human infrastructure, and AI.
FOLLOW US: [F] [T] [R]
SUPPORT
For questions or support visit the Midjourney Discord support channels.
SITES
Midjourney Website
Midjourney Discord
Current
Legacy
Getting Started
Using Discord
Commands Parameters and Tools
Commands
Parameters
Parameter List
Aspect Ratios
Chaos
No
Quality
Repeat
Seeds
Stop
Style
Stylize
Tile
Version
Video
Weird
Tools
Writing Prompts
Working with Your Own Images
Styles and Aesthetics
Image Size and Aspect Ratio
Advanced Prompting Tools
Subscriptions
Using The Website
Policies
Repeat
Light
The --repeat or --r parameter runs a Job multiple times. Combine --repeat with other parameters, like --chaos to increase the pace of your visual exploration.</p>
<p>--repeat accepts values 2–4 for Basic subscribers
--repeat accepts values 2–10 for Standard subscribers
--repeat accepts values 2–40 for Pro and Mega subscribers
The --repeat parameter can only be used in Fast and Turbo GPU mode.
Using the redo (re-roll) 🔄 button on the results of a --repeat Job will only re-run the prompt once.</p>
<p>Use the --repeat or --r Parameter</p>
<p>Add --repeat <value> or --r <value> to the end of your prompt.</p>
<p>Previous
Quality
Next
Seeds
Midjourney is an independent research lab exploring new mediums of thought and expanding the imaginative powers of the human species. We are a small self-funded team focused on design, human infrastructure, and AI.
FOLLOW US: [F] [T] [R]
SUPPORT
For questions or support visit the Midjourney Discord support channels.
SITES
Midjourney Website
Midjourney Discord
Current
Legacy
Getting Started
Using Discord
Commands Parameters and Tools
Commands
Parameters
Parameter List
Aspect Ratios
Chaos
No
Quality
Repeat
Seeds
Stop
Style
Stylize
Tile
Version
Video
Weird
Tools
Writing Prompts
Working with Your Own Images
Styles and Aesthetics
Image Size and Aspect Ratio
Advanced Prompting Tools
Subscriptions
Using The Website
Policies
Seeds
Light
The Midjourney bot uses a seed number to create a field of visual noise, like television static, as a starting point to generate the initial image grids. Seed numbers are generated randomly for each image but can be specified with the --seed parameter. If you use the same seed number and prompt, you will get similar final images.
--seed accepts whole numbers 0–4294967295.
--seed values only influence the initial image grid.
Identical --seed values using Model Versions 1, 2, 3, test, and testp will produce images with similar composition, color, and details.
Identical --seed values using Model Versions 4, 5, and niji will produce nearly identical images.
Seed numbers are not static and should not be relied upon between sessions.
Seed Parameter</p>
<p>If no Seed is specified, Midjourney will use a randomly generated seed number, producing a wide variety of options each time a prompt is used.</p>
<p>Jobs run three times with random seeds:</p>
<p>prompt example: /imagine prompt celadon owl pitcher</p>
<p>Jobs run two times with --seed 123:</p>
<p>prompt example: /imagine prompt celadon owl pitcher --seed 123</p>
<p>How to Find a Job's Seed Number
Use a Discord Emoji Reaction</p>
<p>Find the seed number of a Job in discord by reacting with an ✉️ envelope emoji to a Job.</p>
<p>Use The Show Command to Bring Back Old Jobs</p>
<p>To get the seed number for a past image, copy the job ID and use the /show &lt;Job ID #&gt; command with that ID to revive the Job. You can then react to the newly regenerated Job with an ✉️ envelope emoji.</p>
<p>How To Change Seed Numbers
Use the --seed Parameter</p>
<p>Add --seed <value> to the end of your prompt.</p>
<p>Previous
Repeat
Next
Stop
Midjourney is an independent research lab exploring new mediums of thought and expanding the imaginative powers of the human species. We are a small self-funded team focused on design, human infrastructure, and AI.
FOLLOW US: [F] [T] [R]
SUPPORT
For questions or support visit the Midjourney Discord support channels.
SITES
Midjourney Website
Midjourney Discord
Current
Legacy
Getting Started
Using Discord
Commands Parameters and Tools
Commands
Parameters
Parameter List
Aspect Ratios
Chaos
No
Quality
Repeat
Seeds
Stop
Style
Stylize
Tile
Version
Video
Weird
Tools
Writing Prompts
Working with Your Own Images
Styles and Aesthetics
Image Size and Aspect Ratio
Advanced Prompting Tools
Subscriptions
Using The Website
Policies
Stop
Light
Use the --stop parameter to finish a Job partway through the process. Stopping a Job at an earlier percentage can create blurrier, less detailed results.</p>
<p>--stop accepts values: 10–100.
The default --stop value is 100.
--stop does not work while Upscaling.</p>
<p>Stop Comparison</p>
<p>prompt example: /imagine prompt splatter art painting of acorns --stop 90</p>
<p>--stop 10</p>
<p>--stop 20</p>
<p>--stop 30</p>
<p>--stop 40</p>
<p>--stop 50</p>
<p>--stop 60</p>
<p>--stop 70</p>
<p>--stop 80</p>
<p>--stop 90</p>
<p>--stop 100</p>
<p>How to Change the Stop Percentage
Use the --stop Parameter</p>
<p>Add --stop <value> to the end of your prompt.</p>
<p>Previous
Seeds
Next
Style
Midjourney is an independent research lab exploring new mediums of thought and expanding the imaginative powers of the human species. We are a small self-funded team focused on design, human infrastructure, and AI.
FOLLOW US: [F] [T] [R]
SUPPORT
For questions or support visit the Midjourney Discord support channels.
SITES
Midjourney Website
Midjourney Discord
Current
Legacy
Getting Started
Using Discord
Commands Parameters and Tools
Commands
Parameters
Parameter List
Aspect Ratios
Chaos
No
Quality
Repeat
Seeds
Stop
Style
Stylize
Tile
Version
Video
Weird
Tools
Writing Prompts
Working with Your Own Images
Styles and Aesthetics
Image Size and Aspect Ratio
Advanced Prompting Tools
Subscriptions
Using The Website
Policies
Style
Light
The --style parameter replaces the default aesthetic of some Midjourney Model Versions. Adding a style parameter can help you create more photo-realistic images, cinematic scenes, or cuter characters.</p>
<p>Default Model Version 5.2 and the previous version 5.1 accept --style raw.
Model Version Niji 5 accepts --style cute --style scenic --style original or --style expressive</p>
<p>Model Version 5.2 Styles</p>
<p>The current default Model Version 5.2 and the previous model version 5.1 have one style parameter, --style raw. --style raw uses an alternative model that may work well for users already comfortable with prompting who want more control over their images. Images made with --style raw have less automatic beautification applied, which can result in a more accurate match when prompting for specific styles.</p>
<p>Model Version 5.2
--v 5.2</p>
<p>ice cream icon</p>
<p>--v 5.2 --style raw</p>
<p>ice cream icon --style raw</p>
<p>--v 5.2 </p>
<p>child's drawing of a cat</p>
<p>--v 5.2 --style raw</p>
<p>child's drawing of a cat --style raw</p>
<p>Niji 5 Styles</p>
<p>Niji Model Version 5 can also use different aesthetics with --style options to achieve unique looks. Try --style cute, --style scenic, --style original , or --style expressive.</p>
<p>Niji Style Parameters</p>
<p>--style cute creates charming and adorable characters, props, and settings.
--style expressive has a more sophisticated illustrated feeling.
--style original uses the original Niji Model Version 5, which was the default before May 26th, 2023.
--style scenic makes beautiful backgrounds and cinematic character moments in the context of their fantastical surroundings.</p>
<p>niji 5 default</p>
<p>guinea pig wearing a flower crown --niji 5</p>
<p>--style original</p>
<p>guinea pig wearing a flower crown --niji 5 --style original</p>
<p>--style cute</p>
<p>guinea pig wearing a flower crown --niji 5 --style cute</p>
<p>--style expressive</p>
<p>guinea pig wearing a flower crown --niji 5 --style expressive</p>
<p>--style scenic</p>
<p>guinea pig wearing a flower crown --niji 5 --style scenic</p>
<p>niji 5 default</p>
<p>pastel fields of oxalis --niji 5</p>
<p>--style original</p>
<p>pastel fields of oxalis --niji 5 --style original</p>
<p>--style cute</p>
<p>pastel fields of oxalis --niji 5 --style cute</p>
<p>--style expressive</p>
<p>pastel fields of oxalis --niji 5 --style expressive</p>
<p>--style scenic</p>
<p>pastel fields of oxalis --niji 5 --style scenic</p>
<p>How to Use Styles
Use the --style Parameter</p>
<p>Add --style <style name> to the end of your prompt.</p>
<p>Use the Settings Command</p>
<p>Type /settings and select 🔧 Raw from the menu to append --style raw to all prompts.</p>
<p>Previous
Stop
Next
Stylize
Midjourney is an independent research lab exploring new mediums of thought and expanding the imaginative powers of the human species. We are a small self-funded team focused on design, human infrastructure, and AI.
FOLLOW US: [F] [T] [R]
SUPPORT
For questions or support visit the Midjourney Discord support channels.
SITES
Midjourney Website
Midjourney Discord
Current
Legacy
Getting Started
Using Discord
Commands Parameters and Tools
Commands
Parameters
Parameter List
Aspect Ratios
Chaos
No
Quality
Repeat
Seeds
Stop
Style
Stylize
Tile
Version
Video
Weird
Tools
Writing Prompts
Working with Your Own Images
Styles and Aesthetics
Image Size and Aspect Ratio
Advanced Prompting Tools
Subscriptions
Using The Website
Policies
Quality
Light
The --quality or --q parameter changes how much time is spent generating an image. Higher-quality settings take longer to process and produce more details. Higher values also mean more GPU minutes are used per job. The quality setting does not impact resolution.</p>
<p>The default --quality value is 1.
--quality only accepts the values: .25, .5, and 1 for the current model. Larger values are rounded down to 1.
--quality only influences the initial image generation.
--quality works with Model Versions 4, 5, 5.1, 5.2, and niji 5.</p>
<p>The Influence of Quality on Jobs</p>
<p>Higher --quality settings aren't always better. Sometimes a lower --quality settings can produce better results—depending on the image you're trying to create. Lower --quality settings might be best for a gestural abstract look. Higher --quality values may improve the look of architectural images that benefit from many details. Choose the setting that best matches the kind of image you're hoping to create.</p>
<p>Version Quality Compatibility
Model Version	Quality .25	Quality .5	Quality 1
Version 5, 5.1, and 5.2	✓	✓	✓
Version 4	✓	✓	✓
niji 5	✓	✓	✓</p>
<p>Quality Comparison</p>
<p>Prompt example: /imagine prompt detailed peony illustration --q .25</p>
<p>--quality .25</p>
<p>quickest results, least detailed results</p>
<p>4× faster and ¼ the GPU minutes.
--quality .5</p>
<p>less detailed results</p>
<p>2× faster and ½ the GPU minutes.
--quality 1</p>
<p>the default setting</p>
<p>How to Use the Quality Parameter
Use the --quality or --q Parameter</p>
<p>Add --quality <value> or --q <value> to the end of your prompt.</p>
<p>Previous
No
Next
Repeat
Midjourney is an independent research lab exploring new mediums of thought and expanding the imaginative powers of the human species. We are a small self-funded team focused on design, human infrastructure, and AI.
FOLLOW US: [F] [T] [R]
SUPPORT
For questions or support visit the Midjourney Discord support channels.
SITES
Midjourney Website
Midjourney Discord
Current
Legacy
Getting Started
Using Discord
Commands Parameters and Tools
Commands
Parameters
Parameter List
Aspect Ratios
Chaos
No
Quality
Repeat
Seeds
Stop
Style
Stylize
Tile
Version
Video
Weird
Tools
Writing Prompts
Working with Your Own Images
Styles and Aesthetics
Image Size and Aspect Ratio
Advanced Prompting Tools
Subscriptions
Using The Website
Policies
No
Light
The No parameter tells the Midjourney Bot what not to include in your image.</p>
<p>--no accepts multiple words separated with commas: --no item1, item2, item3, item4</p>
<p>--No Comparison</p>
<p>still life gouache painting</p>
<p>a range of objects appear in the still life</p>
<p>still life gouache painting --no fruit</p>
<p>The still life has fewer fruits</p>
<p>--no vs. Don't</p>
<p>The Midjourney Bot considers any word within the prompt as something you would like to see generated in the final image. Prompting still life gouache painting without any fruit or still life gouache painting dont add fruit! are more likely to produce pictures that include fruits because the relationship between &quot;without&quot; or &quot;don't&quot; and the &quot;fruit&quot; is not interpreted by the Midjourney Bot in the same way a human reader would understand it. To improve your results, focus your prompt on what you do want to see in the image and use the &quot;--no&quot; parameter to specify concepts you don't want to include.</p>
<p>still life gouache painting</p>
<p>a range of objects appear in the still life</p>
<p>still life gouache painting don't add fruit</p>
<p>more fruit is present in the final image.</p>
<p>Multi Prompting</p>
<p>The --no parameter is the same as weighing part of a multi prompt to &quot;-.5&quot; still life gouache painting:: fruit::-.5 is the same as still life gouache painting --no fruit.</p>
<p>How to Use the No Paramter</p>
<p>Add --no item1, item2, item3 to the end of your prompt.</p>
<p>Previous
Chaos
Next
Quality
Midjourney is an independent research lab exploring new mediums of thought and expanding the imaginative powers of the human species. We are a small self-funded team focused on design, human infrastructure, and AI.
FOLLOW US: [F] [T] [R]
SUPPORT
For questions or support visit the Midjourney Discord support channels.
SITES
Midjourney Website
Midjourney Discord
Current
Legacy
Getting Started
Using Discord
Commands Parameters and Tools
Commands
Parameters
Parameter List
Aspect Ratios
Chaos
No
Quality
Repeat
Seeds
Stop
Style
Stylize
Tile
Version
Video
Weird
Tools
Writing Prompts
Working with Your Own Images
Styles and Aesthetics
Image Size and Aspect Ratio
Advanced Prompting Tools
Subscriptions
Using The Website
Policies
Chaos
Light
The --chaos or --c parameter influences how varied the initial image grids are. High --chaos values will produce more unusual and unexpected results and compositions. Lower --chaos values have more reliable, repeatable results.</p>
<p>--chaos accepts values 0–100.
The default --chaos value is 0.</p>
<p>The Influence of Chaos on Jobs
No --chaos value</p>
<p>Using a very low --chaos value, or not specifying a value, will produce initial image grids that are similar each time a Job is run.</p>
<p>prompt example: /imagine prompt: watermelon owl hybrid --c 0</p>
<p>Low --chaos values</p>
<p>Using a low --chaos value will produce initial image grids that are slightly varied each time a Job is run.</p>
<p>prompt example: /imagine prompt: watermelon owl hybrid --c 10</p>
<p>Moderate --chaos values</p>
<p>Using a moderate --chaos value will produce initial image grids that are varied each time a Job is run.</p>
<p>prompt example: /imagine prompt: watermelon owl hybrid --c 25</p>
<p>High --chaos Values</p>
<p>Using a higher --chaos value will produce initial image grids that are more varied and unexpected each time a Job is run.</p>
<p>prompt example: /imagine prompt: watermelon owl hybrid --c 50</p>
<p>Very High --chaos Values</p>
<p>Using extremely high --chaos values will produce initial image grids that are varied and have unexpected compositions or artistic mediums each time a Job is run.</p>
<p>prompt example: /imagine prompt: watermelon owl hybrid --c 80</p>
<p>How to Change the Chaos Value
Use the --chaos or --c Parameter</p>
<p>Add --chaos <value> or --c <value> to the end of your prompt.</p>
<p>Previous
Aspect Ratios
Next
No
Midjourney is an independent research lab exploring new mediums of thought and expanding the imaginative powers of the human species. We are a small self-funded team focused on design, human infrastructure, and AI.
FOLLOW US: [F] [T] [R]
SUPPORT
For questions or support visit the Midjourney Discord support channels.
SITES
Midjourney Website
Midjourney Discord
Current
Legacy
Getting Started
Using Discord
Commands Parameters and Tools
Commands
Parameters
Parameter List
Aspect Ratios
Chaos
No
Quality
Repeat
Seeds
Stop
Style
Stylize
Tile
Version
Video
Weird
Tools
Writing Prompts
Working with Your Own Images
Styles and Aesthetics
Image Size and Aspect Ratio
Advanced Prompting Tools
Subscriptions
Using The Website
Policies
Aspect Ratios
Light
The --aspect or --ar parameter changes the aspect ratio of the generated image. An aspect ratio is the width-to-height ratio of an image. It is typically expressed as two numbers separated by a colon, such as 7:4 or 4:3.</p>
<p>A square image has equal width and height, described as a 1:1 aspect ratio. The image could be 1000px × 1000px, or 1500px × 1500px, and the aspect ratio would still be 1:1. A computer screen might have a ratio of 16:10. The width is 1.6 times longer than the height. So the image could be 1600px × 1000px, 4000px × 2000px, 320px x 200px, etc.</p>
<p>The default aspect ratio is 1:1.
--aspect must use whole numbers. Use 139:100 instead of 1.39:1.
The aspect ratio impacts the shape and composition of a generated image.
Some aspect ratios may be slightly changed when upscaling.</p>
<p>Max Aspect Ratios</p>
<p>Different Midjourney Version Models have different maximum aspect ratios.</p>
<pre><code>Version 5	Version 4	niji 5
</code></pre>
<p>Ratios	any*	1:2 to 2:1	any*</p>
<p>The --ar parameter will accept any aspect ratio from 1:1 (square) up to the maximum aspect ratio for each model. However, final outputs may be slightly modified during image generation or upscaling.</p>
<ul>
<li>Aspect ratios greater than 2:1 are experimental and may produce unpredicatble results.</li>
</ul>
<p>prompt example: imagine/ prompt vibrant california poppies --ar 5:4</p>
<p>Common Midjourney Aspect Ratios</p>
<p>--aspect 1:1 Default aspect ratio.
--aspect 5:4 Common frame and print ratio.
--aspect 3:2 Common in print photography.
--aspect 7:4 Close to HD TV screens and smartphone screens.</p>
<p>Changing the Aspect Ratio of an Image</p>
<p>Do you love an image you have generated but wish it taller or wider? You can use the 🔎 Zoom Out buttons on any upscaled image to change the aspect ratio of your image. The Midjourney Bot will fill in the new space with additional content informed by your prompt and the original image.</p>
<p>How to Set the Aspect Ratio
Use Aspect Ratio Parameters</p>
<p>Add --aspect <value>:<value>, or --ar <value>:<value> to the end of your prompt.</p>
<p>Previous
Parameter List
Next
Chaos
Midjourney is an independent research lab exploring new mediums of thought and expanding the imaginative powers of the human species. We are a small self-funded team focused on design, human infrastructure, and AI.
FOLLOW US: [F] [T] [R]
SUPPORT
For questions or support visit the Midjourney Discord support channels.
SITES
Midjourney Website
Midjourney Discord
Current
Legacy
Getting Started
Using Discord
Commands Parameters and Tools
Commands
Command List
Blend
Describe
User Info
Settings and Presets
Show Job
Shorten
Parameters
Tools
Writing Prompts
Working with Your Own Images
Styles and Aesthetics
Image Size and Aspect Ratio
Advanced Prompting Tools
Subscriptions
Using The Website
Policies
Shorten
Light
The /shorten command analyzes your prompt, highlights some of your prompt's most influential words, and suggests unnecessary words you could remove. With this command, you can optimize your prompt by focusing on essential terms.</p>
<p>/shorten is not compatible with multi prompts or the --no parameter</p>
<p>Analyze a Prompt with /Shorten</p>
<p>The Midjourney bot analyzes your prompt by breaking it down into smaller units known as tokens. These tokens can be phrases, words, or even syllables. The Midjourney bot converts these tokens into a format it can understand. It uses them with the associations and patterns learned during its training to guide how your image is generated. Think of tokens as the building blocks that help the Midjourney bot make sense of the input and create the desired visual output.</p>
<p>Long prompts with unnecessary words, lengthy descriptions, poetic phrases, or direct addressing of the bot (&quot;Please make me an image,&quot; &quot;Thank you for your help, Midjourney Bot!&quot;) can lead to unexpected elements being added to your images.</p>
<p>The /shorten command can help you discover the most important words in your prompt and what words you can omit.</p>
<p>Shorten Command Example</p>
<p>If you want to create a pile of sprinkle covered donuts you might try the prompt:
Please create a whimsical majestic tower of donuts, intricately crafted and adorned with a mesmerizing array of colorful sprinkles. Bring this sugary masterpiece to life, ensuring every detail is rendered in stunning magical realism. Thank you!</p>
<p>If you use /shorten command with the above prompt the Midjourney Bot will return the following information:</p>
<p>IMPORTANT TOKENS
Please create a whimsical majestic tower of donuts, intricately crafted and adorned with a mesmerizing array of colorful sprinkles. Bring this sugary masterpiece to life, ensuring every detail is rendered in stunning magical realism. Thank you!</p>
<p>SHORTENED PROMPTS
1️⃣ Please, majestic tower of donuts, crafted, array of colorful sprinkles, sugary masterpiece, rendered, magical realism</p>
<p>2️⃣ Please, majestic tower of donuts, colorful sprinkles, sugary masterpiece, rendered, magical realism</p>
<p>3️⃣ majestic tower of donuts, colorful sprinkles, sugary, magical realism</p>
<p>4️⃣ majestic tower of donuts, colorful sprinkles, magical</p>
<p>5️⃣ tower of donuts, sprinkles</p>
<p>The most Important tokens in your prompt are highlighted in bold, the least important are stikethroughed. You will also be given 5 possible shorter prompts based on this information.</p>
<p>Shortened Option 1️⃣ </p>
<p>Please, majestic tower of donuts, crafted, array of colorful sprinkles, sugary masterpiece, rendered, magical realism</p>
<p>Shortened Option 2️⃣ </p>
<p>Please, majestic tower of donuts, colorful sprinkles, sugary masterpiece, rendered, magical realism</p>
<p>Shortened Option 3️⃣ </p>
<p>majestic tower of donuts, colorful sprinkles, sugary, magical realism</p>
<p>Shortened Option 4️⃣ </p>
<p>majestic tower of donuts, colorful sprinkles, magical</p>
<p>Shortened Option 5️⃣ </p>
<p>tower of donuts, sprinkles</p>
<p>Analyzing the Results</p>
<p>The shortest prompt, Option 5️⃣ : tower of donuts, sprinkles produced an image closest to the original goal. Many of the filler words like &quot;whimsical,&quot; &quot;mesmerizing,&quot; and &quot;masterpiece&quot; could be omitted. Learning that &quot;tower&quot; and &quot;magical&quot; were considered important tokens helps explain why some images were generated with fairytale castle elements. Learning this provided a clue that &quot;magical&quot; should be removed from the prompt if the goal was to create a stack of delicious donuts.</p>
<p>The /shorten command is a tool to help you explore how the Midjourney Bot interprets tokens and experiment with words, but it may not work for all subjects and styles of prompting.</p>
<p>How to Use Shorten</p>
<p>Use the /shorten <your prompt> command in any Bot Channel to get information on your prompt</p>
<p>Previous
Show Job
Next
Parameter List
Midjourney is an independent research lab exploring new mediums of thought and expanding the imaginative powers of the human species. We are a small self-funded team focused on design, human infrastructure, and AI.
FOLLOW US: [F] [T] [R]
SUPPORT
For questions or support visit the Midjourney Discord support channels.
SITES
Midjourney Website
Midjourney Discord
Current
Legacy
Getting Started
Using Discord
Commands Parameters and Tools
Commands
Parameters
Parameter List
Aspect Ratios
Chaos
No
Quality
Repeat
Seeds
Stop
Style
Stylize
Tile
Version
Video
Weird
Tools
Writing Prompts
Working with Your Own Images
Styles and Aesthetics
Image Size and Aspect Ratio
Advanced Prompting Tools
Subscriptions
Using The Website
Policies
Parameter List
Light
Parameters are options added to a prompt that change how an image generates. Parameters can change an image's Aspect Ratios, switch between Midjourney Model Versions, change which Upscaler is used, and lots more.</p>
<p>Parameters are always added to the end of a prompt. You can add multiple parameters to each prompt.</p>
<p>Using an Apple device?</p>
<p>Many Apple devices automatically change double hyphens (--) to an em-dash (—). Midjourney accepts both!</p>
<p>Basic Parameters
Aspect Ratios</p>
<p>--aspect, or --ar Change the aspect ratio of a generation.</p>
<p>Chaos</p>
<p>--chaos &lt;number 0–100&gt; Change how varied the results will be. Higher values produce more unusual and unexpected generations.</p>
<p>Fast</p>
<p>--fast override your current setting and run a single job using Fast Mode.</p>
<p>Image Weight</p>
<p>--iw &lt;0–2&gt; Sets image prompt weight relative to text weight. The default value is 1.</p>
<p>No</p>
<p>--no Negative prompting, --no plants would try to remove plants from the image.</p>
<p>Quality</p>
<p>--quality &lt;.25, .5, or 1&gt;, or --q &lt;.25, .5, or 1&gt; How much rendering quality time you want to spend. The default value is 1. Higher values use more GPU minutes; lower values use less.</p>
<p>Random</p>
<p>--style random, add a random 32 base styles Style Tuner code to your prompt. You can also use --style random-16, --style random-64 or --style random-128 to use random results from other lengths of Style Tuners.</p>
<p>Relax</p>
<p>--relax override your current setting and run a single job using Relax Mode.</p>
<p>Repeat</p>
<p>--repeat &lt;1–40&gt;, or --r &lt;1–40&gt; Create multiple Jobs from a single prompt. --repeat is useful for quickly rerunning a job multiple times.</p>
<p>Seed</p>
<p>--seed &lt;integer between 0–4294967295&gt; The Midjourney bot uses a seed number to create a field of visual noise, like television static, as a starting point to generate the initial image grids. Seed numbers are generated randomly for each image but can be specified with the --seed or --sameseed parameter. Using the same seed number and prompt will produce similar ending images.</p>
<p>Stop</p>
<p>--stop &lt;integer between 10–100&gt; Use the --stop parameter to finish a Job partway through the process. Stopping a Job at an earlier percentage can create blurrier, less detailed results.</p>
<p>Style</p>
<p>--style <raw> Switch between versions of the Midjourney Model Version 5.1 and 5.2.
--style &lt;4a, 4b, or 4c&gt; Switch between versions of the Midjourney Model Version 4.
--style &lt;cute, expressive, original, or scenic&gt; Switch between versions of the Niji Model Version 5.
Use the /tune command to create a Style Tuner and generate custom style codes.</p>
<p>Stylize</p>
<p>--stylize <number>, or --s <number> parameter influences how strongly Midjourney's default aesthetic style is applied to Jobs.</p>
<p>Tile</p>
<p>--tile parameter generates images that can be used as repeating tiles to create seamless patterns.</p>
<p>Turbo</p>
<p>--turbo override your current setting and run a single job using Turbo Mode.</p>
<p>Weird</p>
<p>--weird &lt;number 0–3000&gt;, or --w &lt;number 0–3000&gt; Explore unusual aesthetics with the experimental --weird parameter.</p>
<p>Default Values (Model Version 5.2)
Aspect Ratio	Chaos	Quality	Seed	Stop	Stylize
Default Value
1:1	0	1	Random	100	100
Range
any	0–100	.25 .5, or 1	whole numbers 0–4294967295	10–100	0–1000
Aspect ratios greater than 2:1 are experimental and may produce unpredictable results.</p>
<p>Default Values (Model Version 6)
Aspect Ratio	Chaos	Quality	Seed	Stop	Stylize
Default Value
1:1	0	1	Random	100	100
Range
1:3–3:1	0–100	.25 .5 or 1	whole numbers 0–4294967295	10–100	0–1000</p>
<p>Model Version Parameters</p>
<p>Midjourney routinely releases new model versions to improve efficiency, coherency, and quality. Different models excel at different types of images.</p>
<p>Niji</p>
<p>--niji &lt;4, or 5&gt; An alternative model focused on anime-style images.</p>
<p>Version</p>
<p>--version &lt;1, 2, 3, 4, 5.0, 5.1, 5.2, or 6&gt; or --v &lt;1, 2, 3, 4, 5.0, 5.1, 5.2, or 6&gt; Use a different version of the Midjourney algorithm.</p>
<p>Legacy Parameters</p>
<p>Some parameters only work with earlier Midjourney Models.
Read More about legacy parameters here.</p>
<p>Compatibility
Model Version &amp; Parameter Compatability
Affects initial generation	Affects variations + remix	Ver. 6	Ver. 5.2	Niji 5
Max Aspect Ratio	✓	✓	any	any	any
Chaos	✓		0–100	0–100	0–100
Image Weight	✓		0–3
default=1	.5–2
default=1	.5–2
default=1
No	✓	✓	✓	✓	✓
Quality	✓		.25, .5, or 1	.25, .5, or 1	.25, .5, or 1
Repeat	✓		✓	✓	✓
Seed	✓		✓	✓	✓
Stop	✓	✓	10–100	10–100	10–100
Style			raw	raw	cute, expressive, original
and scenic
Stylize	✓		0–1000
default=100	0–1000
default=100	0–1000
default=100)
Tile	✓	✓	✓	✓	✓
Video	✓		✓	✓	✓
Weird	✓		✓	✓	✓
Previous
Shorten
Next
Aspect Ratios
Midjourney is an independent research lab exploring new mediums of thought and expanding the imaginative powers of the human species. We are a small self-funded team focused on design, human infrastructure, and AI.
FOLLOW US: [F] [T] [R]
SUPPORT
For questions or support visit the Midjourney Discord support channels.
SITES
Midjourney Website
Midjourney Discord
Current
Legacy
Getting Started
Using Discord
Commands Parameters and Tools
Commands
Command List
Blend
Describe
User Info
Settings and Presets
Show Job
Shorten
Parameters
Tools
Writing Prompts
Working with Your Own Images
Styles and Aesthetics
Image Size and Aspect Ratio
Advanced Prompting Tools
Subscriptions
Using The Website
Policies
Show Job
Light
You can use the /show command with the unique Job ID to move a job to another server or channel, revive a lost job, or refresh an old job to make new variations, upscale, or use newer parameters and features.</p>
<p>/show only works on your own jobs.</p>
<p>Find a Job ID</p>
<p>Job IDs are unique identifiers used for each image generated by Midjourney.
Job IDs look like this: 9333dcd0-681e-4840-a29c-801e502ae424and can be found in the first part of all image filenames, in the URLs on the website, and in an image's filename.</p>
<p>On The Web</p>
<p>You can find the Job ID of any image in your member gallery by selecting ... &gt; Copy... &gt; Job ID.</p>
<p>From The URL</p>
<p>Job IDs are the last part of the URL when viewing an image in your midjourney gallery.
https://www.midjourney.com/app/users/381590592095911946/?jobId=9333dcd0-681e-4840-a29c-801e502ae424.</p>
<p>From The File Name</p>
<p>Job IDs are the last part of the file name when viewing an image you have downloaded from your gallery.
User_cat_cloud_spirit_9333dcd0-681e-4840-a29c-801e502ae424.png</p>
<p>Using a Discord Emoji Reaction</p>
<p>React with the envelope emoji ✉️ to send a completed Job to direct messages. The direct message will include the image's seed number and Job ID. The ✉️ reaction only works with your own Jobs.</p>
<p>How to Use Show</p>
<p>Use the /show &lt;Job ID #&gt; in any Bot Channel to revive a Job.</p>
<p>Previous
Settings and Presets
Next
Shorten
Midjourney is an independent research lab exploring new mediums of thought and expanding the imaginative powers of the human species. We are a small self-funded team focused on design, human infrastructure, and AI.
FOLLOW US: [F] [T] [R]
SUPPORT
For questions or support visit the Midjourney Discord support channels.
SITES
Midjourney Website
Midjourney Discord
Current
Legacy
Getting Started
Using Discord
Commands Parameters and Tools
Commands
Command List
Blend
Describe
User Info
Settings and Presets
Show Job
Shorten
Parameters
Tools
Writing Prompts
Working with Your Own Images
Styles and Aesthetics
Image Size and Aspect Ratio
Advanced Prompting Tools
Subscriptions
Using The Website
Policies
Settings and Presets
Light
The /settings command provides toggle buttons for common options like model version, style value, quality value, and upscaler version. Settings also has toggles for the /stealth and /public commands.</p>
<p>Parameters added to the end of a prompt will override selections made using /settings.</p>
<p>Model Version Dropdown</p>
<p>Select the Midjourney Model Version to use when generating images. Use The Latest Model is the default selection and will always use the newest Midjourney Model Version.</p>
<p>Use the latest Model (V5.2) 5️⃣ Mjourney Model V5.2 5️⃣ Mjourney Model V5.1 🍎 Niji Model V5 5️⃣ Mjourney Model V5.0 🌈 Niji Model V4 4️⃣ Mjourney Model V4 3️⃣ Mjourney Model V3 2️⃣ Mjourney Model V2 1️⃣ Mjourney Model V1</p>
<p>Style Raw Parameter</p>
<p>Midjourney Model Versions 5.1 and 5.2 can be fine-tuned with the --style raw parameter to reduce the Midjourney default aesthetic. This toggle is not available if other model versions are selected.</p>
<p>🔧 Raw Mode</p>
<p>Stylize Parameter</p>
<p>The Midjourney Bot has been trained to produce images that favor artistic color, composition, and forms. The --stylize or --s parameter influences how strongly this training is applied. Low stylization values produce images that closely match the prompt but are less artistic. High stylization values create images that are very artistic but less connected to the prompt.</p>
<p>🖌️ Stylize Low 🖌️ Stylize Med 🖌️ Stylize High 🖌️ Stylize Very High</p>
<p>Stylize Low = --s 50, Stylize Med = --s 100, Stylize High = --s 250, Stylize Very High = --s 750,</p>
<p>Public and Stealth Mode</p>
<p>Toggle between Public and Stealth modes. Corresponds to the /public and /stealth commands.</p>
<p>🧍‍♂️Public</p>
<p>Remix Mode</p>
<p>Use Remix mode to change prompts, parameters, model versions, or aspect ratios between variations. Remix will take the general composition of your starting image and use it as part of the new Job. Remixing can help change the setting or lighting of an image, evolve a subject, or achieve tricky compositions.</p>
<p>🎛️ Remix</p>
<p>High and Low Variation Mode</p>
<p>Toggle between High Variation and Low Variation mode.</p>
<p>🎨 High Variation Mode 🎨 Low Variation Mode</p>
<p>Sticky Style</p>
<p>Sticky Style will save the last --style code parameter used in your personal suffix, so you don't have to repeat the code on future prompts. Change codes by using a new --style or unselecting Sticky Style.</p>
<p>📌 Sticky Style</p>
<p>Turbo, Fast, and Relax Mode</p>
<p>Toggle between Turbo, Fast and Relaxed modes. Corresponds to the /turbo, /fast, and /relax commands and the --turbo, --fast, and --relax parameters.</p>
<p>⚡ Turbo Mode 🐇 Fast Mode 🐢 Relax Mode</p>
<p>Reset Settings</p>
<p>Return to default settings.</p>
<p>Reset Settings</p>
<p>Custom Preferences</p>
<p>Create custom options using prefer commands to add commonly used parameters to the end of prompts automatically.
/prefer option Create or manage a custom option.
/prefer option list View your current custom options.
/prefer suffix specify a suffix to add to the end of every prompt.</p>
<p>Automatically Direct Message Results</p>
<p>Use /prefer auto_dm to turn on or off automatic sending of finished jobs to your Direct Message.</p>
<p>Prefer Option</p>
<p>/prefer option set <name> <value> Creates a custom parameter that you can use to add multiple parameters to the end of prompts quickly.</p>
<p>/prefer option set mine --hd --ar 7:4 creates an option called &quot;mine&quot; that translates to --hd --ar 7:4.</p>
<p>Using /imagine prompt vibrant California poppies --mine, is interpreted as /imagine prompt vibrant California poppies --hd --ar 7:4.</p>
<p>Leave the &quot;value&quot; field empty to delete an option.</p>
<p>/prefer option list list all options created with prefer option set. Users can have up to 20 custom options.</p>
<p>To delete a custom option, use /prefer option set <name to delete> and leave the value field blank.</p>
<p>Prefer Suffix</p>
<p>/prefer suffix automatically appends the specified suffix after all prompts.
Use the /settings command and select Reset Settings to clear a preferred suffix.</p>
<p>Command example: /prefer suffix --uplight --video</p>
<p>Only Parameters can be used with /prefer suffix,
prefer suffix --no orange is accepted
prefer suffix orange::-1 is not accepted</p>
<p>Previous
User Info
Next
Show Job
Midjourney is an independent research lab exploring new mediums of thought and expanding the imaginative powers of the human species. We are a small self-funded team focused on design, human infrastructure, and AI.
FOLLOW US: [F] [T] [R]
SUPPORT
For questions or support visit the Midjourney Discord support channels.
SITES
Midjourney Website
Midjourney Discord
Current
Legacy
Getting Started
Using Discord
Commands Parameters and Tools
Commands
Command List
Blend
Describe
User Info
Settings and Presets
Show Job
Shorten
Parameters
Tools
Writing Prompts
Working with Your Own Images
Styles and Aesthetics
Image Size and Aspect Ratio
Advanced Prompting Tools
Subscriptions
Using The Website
Policies
User Info
Light
Use the /info command to see information about your current queued and running jobs, subscription type, renewal date, and more.</p>
<p>Subscription</p>
<p>The Subscription section shows which plan you are subscribed to and your next renewal date.</p>
<p>Job Mode</p>
<p>Shows whether you are currently in Fast or Relaxed Mode. Relax Mode is only available to Standard and Pro Plan subscribers.</p>
<p>Visibility Mode</p>
<p>Shows whether you are currently in Public or Stealth Mode. Stealth Mode is only available to Pro Plan subscribers.</p>
<p>Fast Time Remaining</p>
<p>Shows your remaining Fast GPU time for the month. Fast GPU time resets monthly and does not carry over.</p>
<p>Lifetime Usage</p>
<p>Shows your lifetime Midjourney stats. Images include all types of generations (initial image grids, upscales, variations, remix, etc).</p>
<p>Relaxed Usage</p>
<p>Shows your Relax Mode usage for the month. Heavy Relax Mode users will experience slightly slower queue times. Relaxed usage amounts reset monthly.</p>
<p>Queued Jobs</p>
<p>Lists all Jobs queued to run. A maximum of seven jobs can be queued at the same time.</p>
<p>Running Jobs</p>
<p>Lists all Jobs currently running. A maximum of three jobs can run at the same time.</p>
<p>Use the Info Command</p>
<p>Type /info in any Bot Channel or your Direct Messages. Only you will be able to see your Info popup.</p>
<p>Previous
Describe
Next
Settings and Presets
Midjourney is an independent research lab exploring new mediums of thought and expanding the imaginative powers of the human species. We are a small self-funded team focused on design, human infrastructure, and AI.
FOLLOW US: [F] [T] [R]
SUPPORT
For questions or support visit the Midjourney Discord support channels.
SITES
Midjourney Website
Midjourney Discord
Current
Legacy
Getting Started
Using Discord
Commands Parameters and Tools
Commands
Command List
Blend
Describe
User Info
Settings and Presets
Show Job
Shorten
Parameters
Tools
Writing Prompts
Working with Your Own Images
Styles and Aesthetics
Image Size and Aspect Ratio
Advanced Prompting Tools
Subscriptions
Using The Website
Policies
Describe
Light
The /describe command allows you to upload an image and generate four possible prompts based on that image. Use the /describe command to explore new vocabulary and aesthetic movements.</p>
<p>/describe generates prompts that are inspirational and suggestive, it cannot be used to recreate an uploaded image exactly.
/describe returns the aspect ratio for uploaded images.</p>
<p>Previous
Blend
Next
User Info
Midjourney is an independent research lab exploring new mediums of thought and expanding the imaginative powers of the human species. We are a small self-funded team focused on design, human infrastructure, and AI.
FOLLOW US: [F] [T] [R]
SUPPORT
For questions or support visit the Midjourney Discord support channels.
SITES
Midjourney Website
Midjourney Discord
Current
Legacy
Getting Started
Using Discord
Commands Parameters and Tools
Commands
Command List
Blend
Describe
User Info
Settings and Presets
Show Job
Shorten
Parameters
Tools
Writing Prompts
Working with Your Own Images
Styles and Aesthetics
Image Size and Aspect Ratio
Advanced Prompting Tools
Subscriptions
Using The Website
Policies
Blend
Light
The /blend command allows you to upload 2–5 images quickly and then looks at the concepts and aesthetics of each image and merges them into a novel new image.</p>
<p>/blend is the same as using multiple image prompts with /imagine, but the interface is optimized for easy use on mobile devices.
/blend works with up to 5 images. To use more than 5 images in a prompt use image prompts with /imagine
/blend does not work with text prompts. To use text and image prompts together, use image prompts and text with /imagine</p>
<p>/blend Options</p>
<p>After typing the /blend command, you will be prompted to upload two photos. Drag and drop images from your hard drive or add images from your photo library when using a mobile device. To add more images, select the optional/options field and select image3, image4, or image5. The /blend command may take longer to start than other commands because your images must be uploaded before the Midjourney Bot can process your request.</p>
<p>Blended images have a default 1:1 aspect ratio, but you can use the optional dimensions field to select between a square aspect ratio (1:1), portrait aspect ration (2:3), or landscape aspect ratio (3:2).</p>
<p>Custom suffixes are added to the end of /blend prompts, like any other /imagine prompt. Aspect ratios specified as part of the /blend command override aspect ratios within a custom suffix.</p>
<p>Blending Tip</p>
<p>For the best results, upload images that are the same aspect ratio as your desired result.</p>
<p>How to Use /blend</p>
<p>Previous
Command List
Next
Describe
Midjourney is an independent research lab exploring new mediums of thought and expanding the imaginative powers of the human species. We are a small self-funded team focused on design, human infrastructure, and AI.
FOLLOW US: [F] [T] [R]
SUPPORT
For questions or support visit the Midjourney Discord support channels.
SITES
Midjourney Website
Midjourney Discord
Current
Legacy
Getting Started
Using Discord
Commands Parameters and Tools
Commands
Command List
Blend
Describe
User Info
Settings and Presets
Show Job
Shorten
Parameters
Tools
Writing Prompts
Working with Your Own Images
Styles and Aesthetics
Image Size and Aspect Ratio
Advanced Prompting Tools
Subscriptions
Using The Website
Policies
Command List
Light
You can interact with the Midjourney Bot on Discord by typing a Command. Commands are used to create images, change default settings, monitor user info, and perform other helpful tasks.</p>
<p>Midjourney Commands can be used in any Bot Channel, on private Discord servers where the Midjourney Bot has been permitted to operate, or in a direct message with the Midjourney Bot.</p>
<p>Commands
/ask</p>
<p>Get an answer to a question.</p>
<p>/blend</p>
<p>Easily blend two images together.</p>
<p>/daily_theme</p>
<p>Toggle notification pings for the #daily-theme channel update</p>
<p>/docs</p>
<p>Use in the official Midjourney Discord server to quickly generate a link to topics covered in this user guide!</p>
<p>/describe</p>
<p>Writes four example prompts based on an image you upload.</p>
<p>/faq</p>
<p>Use in the official Midjourney Discord server to quickly generate a link to popular prompt craft channel FAQs.</p>
<p>/fast</p>
<p>Switch to Fast mode.</p>
<p>/help</p>
<p>Shows helpful basic information and tips about the Midjourney Bot.</p>
<p>/imagine</p>
<p>Generate an image using a prompt</p>
<p>/info</p>
<p>View information about your account and any queued or running jobs.</p>
<p>/prefer option</p>
<p>Create or manage a custom option.</p>
<p>/prefer option list</p>
<p>View your current custom options.</p>
<p>/prefer suffix</p>
<p>Specify a suffix to add to the end of every prompt.</p>
<p>/prefer variability</p>
<p>Toggle between High and Low variations when using the V1 V2 V3 and V4 buttons underneath an image grid.</p>
<p>/public</p>
<p>For Pro Plan Subscribers: switch to Public Mode</p>
<p>/relax</p>
<p>Switch to Relax mode.</p>
<p>/remix</p>
<p>Toggle Remix mode.</p>
<p>/settings</p>
<p>View and adjust the Midjourney Bot's settings</p>
<p>/shorten</p>
<p>Submit a long prompt and receive suggestions on how to make it more concise.</p>
<p>/show</p>
<p>Use an images Job ID to regenerate the Job within Discord.</p>
<p>/stealth</p>
<p>For Pro Plan Subscribers: switch to Stealth Mode</p>
<p>/subscribe</p>
<p>Generate a personal link for a user's account page.</p>
<p>/tune</p>
<p>Generate a Style Tuner based on your prompt. The Style Tuner lets you make your own Midjourney style and customize the look of your Jobs.</p>
<p>/turbo</p>
<p>Switch to Turbo mode.</p>
<p>Deprecated</p>
<p>/private (replaced with '/stealth')
/pixels
/idea</p>
<p>Previous
Add the Bot to Your Server
Next
Blend
Midjourney is an independent research lab exploring new mediums of thought and expanding the imaginative powers of the human species. We are a small self-funded team focused on design, human infrastructure, and AI.
FOLLOW US: [F] [T] [R]
SUPPORT
For questions or support visit the Midjourney Discord support channels.
SITES
Midjourney Website
Midjourney Discord
Current
Legacy
Getting Started
Using Discord
Discord Interface
Direct Messages
Discord Emoji Reactions
Add the Bot to Your Server
Commands Parameters and Tools
Writing Prompts
Working with Your Own Images
Styles and Aesthetics
Image Size and Aspect Ratio
Advanced Prompting Tools
Subscriptions
Using The Website
Policies
Add the Bot to Your Server
Light
Add the Midjourney Bot to any Discord Server to create with friends, or make a personal Discord Server to organize your work.</p>
<p>Midjourney images generated on private servers are still subject to Midjourney's Community Guidelines.</p>
<p>Images generated on private servers are still visible to other users on midjourney.com.</p>
<p>To use the Midjourney Bot in any server, users must have an active Midjourney subscription.</p>
<p>Create Your Own Server</p>
<p>It's easy to create your own private Discord server to organize your work, notes and inspiration. Learn how to create your own Discord Server.</p>
<p>Add App</p>
<p>Select the Midjourney Bot from the User List, and then click Add App:</p>
<p>Choose your server from the Select a Server drop down</p>
<p>Click Authorize</p>
<p>Bot Management
Restrict the Bot to Specific Channels</p>
<p>If you do not want the Midjourney Bot to be usable in specific channels, go to Edit Channel, select the Permissions tab, select @everyone under Roles/Members and Uncheck &quot;Use Application Commands.&quot;
Users who try to use the Midjourney Bot in a channel with these permissions disabled will not see the command.</p>
<p>For more granular control, go to Settings, select Integrations, the Permissions tab, the + for the Midjourney Bot, and Manage Integration.
slash command permissions can then be set for individual roles and channels.</p>
<p>Get Status Updates and Announcements on Your Server</p>
<p>Follow the Midjourney Official Discord #announcements, #community updates, and #status channels on your server to stay up-to-date with Midjourney.</p>
<p>Previous
Discord Emoji Reactions
Next
Command List
Midjourney is an independent research lab exploring new mediums of thought and expanding the imaginative powers of the human species. We are a small self-funded team focused on design, human infrastructure, and AI.
FOLLOW US: [F] [T] [R]
SUPPORT
For questions or support visit the Midjourney Discord support channels.
SITES
Midjourney Website
Midjourney Discord
Current
Legacy
Getting Started
Using Discord
Discord Interface
Direct Messages
Discord Emoji Reactions
Add the Bot to Your Server
Commands Parameters and Tools
Writing Prompts
Working with Your Own Images
Styles and Aesthetics
Image Size and Aspect Ratio
Advanced Prompting Tools
Subscriptions
Using The Website
Policies
Discord Emoji Reactions
Light
React with different emojis to Midjourney Jobs to send images to your Direct Messages, cancel a job in progress, or delete an image.</p>
<p>React to a generation with an emoji to trigger actions from the Midjourney Bot.</p>
<p>❌ Cancel and Delete</p>
<p>❌ Cancels or deletes a Job at any time. Reacting to a job with an X emoji also removes the Job from the Midjourney website. If you want to delete an image from the Midjourney webpage but can't find the image within Discord, use the /show command to revive the Job. The ❌ reaction only works with your own Jobs.</p>
<p>✉️ Send to Direct Messages</p>
<p>React with the envelope emoji ✉️ to send a completed Job to direct messages. The direct message will include the image's seed number and Job ID. If the envelope emoji is used for an image grid the grid will be sent as individual images. The ✉️ reaction only works with your own Jobs.</p>
<p>Previous
Direct Messages
Next
Add the Bot to Your Server
Midjourney is an independent research lab exploring new mediums of thought and expanding the imaginative powers of the human species. We are a small self-funded team focused on design, human infrastructure, and AI.
FOLLOW US: [F] [T] [R]
SUPPORT
For questions or support visit the Midjourney Discord support channels.
SITES
Midjourney Website
Midjourney Discord
Current
Legacy
Getting Started
Using Discord
Discord Interface
Direct Messages
Discord Emoji Reactions
Add the Bot to Your Server
Commands Parameters and Tools
Writing Prompts
Working with Your Own Images
Styles and Aesthetics
Image Size and Aspect Ratio
Advanced Prompting Tools
Subscriptions
Using The Website
Policies
Direct Messages
Light
Midjourney subscribers can work one-on-one with the Midjourney Bot in their Discord Direct Messages.</p>
<p>Images made within your direct messages are still be subject to content and moderation rules and will be visible on your Midjourney website gallery.</p>
<p>How to Message the Bot</p>
<p>Click on the Midjourney Bot from the Member List (or anywhere you see the Midjourney Bot's name)
Send any message to the Midjourney Bot.
This will begin your Direct Message conversation with the Midjourney Bot.</p>
<p>Find Your Direct Messages
Discord Mobile
Discord Desktop
Troubleshooting</p>
<p>If you don’t see messages from the Bot, adjust your privacy settings:</p>
<p>Discord Desktop
Right-click the Midjourney server icon
Select More Options
Toggle &quot;Allow direct messages from server members.&quot; on
Discord Mobile
Long-press the Midjourney server icon
Select Privacy Settings
Scroll down to the &quot;Allow direct messages from server members&quot; toggle.
Previous
Discord Interface
Next
Discord Emoji Reactions
Midjourney is an independent research lab exploring new mediums of thought and expanding the imaginative powers of the human species. We are a small self-funded team focused on design, human infrastructure, and AI.
FOLLOW US: [F] [T] [R]
SUPPORT
For questions or support visit the Midjourney Discord support channels.
SITES
Midjourney Website
Midjourney Discord
Current
Legacy
Getting Started
Using Discord
Discord Interface
Direct Messages
Discord Emoji Reactions
Add the Bot to Your Server
Commands Parameters and Tools
Writing Prompts
Working with Your Own Images
Styles and Aesthetics
Image Size and Aspect Ratio
Advanced Prompting Tools
Subscriptions
Using The Website
Policies
Discord Interface
Light
You can interact with the Midjourney Bot on Discord. The Midjourney server has channels for working collaboratively, technical and billing support, official announcements, offering feedback, and discussions. The community is supportive, encouraging, and eager to share their knowledge with users just starting out.
Midjourney Discord: https://discord.gg/midjourney
Discord Interface</p>
<p>Server List</p>
<p>Direct Messages
Work one-on-one with the Midjourney Bot in your Direct Messages for a quieter experience.</p>
<p>Midjourney Official Server
The official Midjourney server with channels for collaboration and billing/technical support.</p>
<p>Channel List</p>
<p>#support
Visit this channel for billing and technical support from Midjourney Guides.</p>
<p>#newbies
Visit any #newbies channel to create images.</p>
<p>Member List</p>
<p>Midjourney Bot
The bot that produces images with the /imagine command</p>
<p>Moderators and Guides
Moderators and Guides can help with billing and technical problems in the support channels.</p>
<p>Image Grids</p>
<p>The /imagine command produces a grid of low-resolution image options based on your prompt. Use the buttons under each image grid to create variations of an image, upscale an image, or rerun the last Midjourney Bot action.</p>
<p>Upscale Buttons</p>
<p>U1 U2 U3 U4 buttons separate an image from the image grid, allowing you easily download that image or use additional tools like Zoom Out, or Pan.</p>
<p>When using legacy Midjourney model versions the U buttons would upscale an image generating a larger version of the selected image and adding more details, which uses Fast GPU time.</p>
<p>Redo</p>
<p>🔄 The redo (re-roll) button reruns a job. In this case, it would rerun the original prompt producing a new grid of images.</p>
<p>Variation Buttons</p>
<p>V1 V2 V3 V4 V buttons create incremental variations of the selected grid image. Creating a variation generates a new image grid similar to the chosen image's overall style and composition.</p>
<p>Upscaled Images
🪄 Make Variations Web ↗️ ❤️ Favorite</p>
<p>Make Variations: creates a variation of the upscaled image and generates a new grid of four options.</p>
<p>Web: Open the image in your gallery on Midjourney.com</p>
<p>Favorite: tag your best images to easily find them on the Midjourney website.</p>
<p>Direct Messages</p>
<p>If the #general or #newbie channels are moving too fast, Midjouney subscribers can work one-on-one with the Midjourney Bot in their Discord Direct Messages.
Learn how to direct message the Midjourney Bot</p>
<p>Emoji Reactions</p>
<p>React with different emojis to Midjourney Jobs to send images to your Direct Messages, cancel a job in progress, or delete an image.
Learn how to use emoji reactions.</p>
<p>Daily Theme Channel</p>
<p>Participate in a fun themed group image generation in the #daily-theme channel. Look for the day's theme next to the channel's name. All generations must include one of the day's keywords.</p>
<p>Turn off the daily theme notification</p>
<p>Want to avoid the daily notification from the daily theme channel? Use the /daily_theme command to turn off notifications for that channel.</p>
<p>Previous
Version
Next
Direct Messages
Midjourney is an independent research lab exploring new mediums of thought and expanding the imaginative powers of the human species. We are a small self-funded team focused on design, human infrastructure, and AI.
FOLLOW US: [F] [T] [R]
SUPPORT
For questions or support visit the Midjourney Discord support channels.
SITES
Midjourney Website
Midjourney Discord
Current
Legacy
Getting Started
Quick Start
Community Guidelines
Next Steps
Variations
Vary Region
Vary Region + Remix
Upscalers
Version
Using Discord
Commands Parameters and Tools
Writing Prompts
Working with Your Own Images
Styles and Aesthetics
Image Size and Aspect Ratio
Advanced Prompting Tools
Subscriptions
Using The Website
Policies
Version
Light
Midjourney routinely releases new model versions to improve efficiency, coherency, and quality. The latest model is the default, but other models can be used by adding the --version or --v parameter or by using the /settings command and selecting a model version. Each model excels at producing different types of images.</p>
<p>--version accepts the values 1, 2, 3, 4, 5, 5.1, and 5.2
--version can be abbreviated --v
--v 5.2 is the current default model.</p>
<p>Model Version 5.2</p>
<p>The Midjourney V5.2 model is the newest and most advanced, released June 2023. To use this model, add the --v 5.2 parameter to the end of a prompt, or use the /settings command and select 5️⃣ MJ Version 5.2</p>
<p>Default Model 06/22/23–current</p>
<p>This model produces more detailed, sharper results with better colors, contrast, and compositions. It also has a slightly better understanding of prompts than earlier models and is more responsive to the full range of the --stylize parameter.</p>
<p>Prompt: vibrant California poppies --v 5.2
Prompt: high contrast surreal collage --v 5.2</p>
<p>Model Version 5.2 + Style Raw Parameter</p>
<p>Midjourney Model Versions 5.1 and 5.2 can be fine-tuned with the --style raw parameter to reduce the Midjourney default aesthetic.</p>
<p>Read more about the Midjourney --style parameter.</p>
<p>default --v 5.2</p>
<p>vibrant California poppies</p>
<p>--v 5.2 --style raw</p>
<p>vibrant California poppies --style raw</p>
<p>default --v 5.2</p>
<p>high contrast surreal collage</p>
<p>--v 5.2 --style raw</p>
<p>high contrast surreal collage --style raw</p>
<p>Model Version 5.1</p>
<p>The Midjourney V5.1 was released on May 4th, 2023. To use this model, add the --v 5.1 parameter to the end of a prompt, or use the /settings command and select 5️⃣ MJ Version 5.1</p>
<p>Default Model 05/03/23–06/22/23</p>
<p>This model has a stronger default aesthetic than earlier versions, making it easier to use with simple text prompts. It also has high Coherency, excels at accurately interpreting natural language prompts, produces fewer unwanted artifacts and borders, has increased image sharpness, and supports advanced features like repeating patterns with --tile.</p>
<p>Prompt: vibrant California poppies --v 5.1
Prompt: high contrast surreal collage --v 5.1</p>
<p>Model Version 5.0</p>
<p>The Midjourney V5.0 model produces more photographic generations than the V5.1 model. This model produces images that closely match the prompt but may require longer prompts to achieve your desired aesthetic.</p>
<p>Default Model 03/30/23–05/03/23</p>
<p>To use this model, add the --v 5 parameter to the end of a prompt, or use the /settings command and select 5️⃣ MJ Version 5</p>
<p>Prompt: vibrant California poppies --v 5
Prompt: high contrast surreal collage --v 5</p>
<p>Model Version 4</p>
<p>The Midjourney V4 was the default model from Nov 2022–May 2023. This model featured an entirely new codebase and brand-new AI architecture designed by Midjourney and trained on the new Midjourney AI supercluster. Model Version 4 had increased knowledge of creatures, places, and objects compared to previous models.</p>
<p>This model has very high Coherency and excels with Image Prompts.</p>
<p>Prompt: vibrant California poppies
Prompt: high contrast surreal collage</p>
<p>Niji Model 5</p>
<p>The Niji model is a collaboration between Midjourney and Spellbrush tuned to produce anime and illustrative styles with vastly more knowledge of anime, anime styles, and anime aesthetics. It's excellent at dynamic and action shots and character-focused compositions.</p>
<p>To use this model, add the --niji 5 parameter to the end of a prompt, or use the /settings command and select 🍏 Niji version 5</p>
<p>This model is sensitive to the --stylize parameter. Experiment with different stylization ranges to fine-tune your images.</p>
<p>Niji Style Parameters</p>
<p>Niji Model Version 5 can also be fine-tuned with --style parameters to achieve unique looks. Try --style cute, --style scenic, --style original (uses the original Niji Model Version 5, which was the default before May 26th, 2023), or --style expressive.</p>
<p>default --niji 5</p>
<p>birds perching on a twig --niji 5</p>
<p>--style original</p>
<p>birds perching on a twig --niji 5 --style original</p>
<p>--style cute</p>
<p>birds perching on a twig --niji 5 --style cute</p>
<p>--style expressive</p>
<p>birds perching on a twig --niji 5 --style expressive</p>
<p>--style scenic</p>
<p>birds perching on a twig --niji 5 --style scenic</p>
<p>Niji 5 vs. Midjourney Version 5.2
--v 5.2</p>
<p>vibrant California poppies</p>
<p>--niji 5</p>
<p>vibrant California poppies --niji 5</p>
<p>--v 5.2</p>
<p>birds sitting on a twig</p>
<p>--niji 5</p>
<p>birds sitting on a twig --niji 5</p>
<p>How to Switch Models
Use the Version or Test Parameter</p>
<p>Add --v 4 --v 5 --v 5.1 --v 5.1 --style raw --v 5.2 --v 5.2 --style raw --niji 5 --niji 5 --style cute --niji 5 --style expressive --niji 5 --style original or --niji 5 --style scenic to the end of your prompt.</p>
<p>Use the Settings Command</p>
<p>Type /settings and select your preferred version from the menu.</p>
<p>4️⃣ MJ Version 4 5️⃣ MJ Version 5 5️⃣ MJ Version 5.1 5️⃣ MJ Version 5.2 🍎 Niji Version 5
Legacy Models</p>
<p>You can experiment with early Midjourney models using the --version parameter Read more about Midjourney Legacy Models.</p>
<p>Previous
Upscalers
Next
Discord Interface
Midjourney is an independent research lab exploring new mediums of thought and expanding the imaginative powers of the human species. We are a small self-funded team focused on design, human infrastructure, and AI.
FOLLOW US: [F] [T] [R]
SUPPORT
For questions or support visit the Midjourney Discord support channels.
SITES
Midjourney Website
Midjourney Discord
Current
Legacy
Getting Started
Quick Start
Community Guidelines
Next Steps
Variations
Vary Region
Vary Region + Remix
Upscalers
Version
Using Discord
Commands Parameters and Tools
Writing Prompts
Working with Your Own Images
Styles and Aesthetics
Image Size and Aspect Ratio
Advanced Prompting Tools
Subscriptions
Using The Website
Policies
Upscalers
Light
The current Midjourney and Niji Model Versions produce grids of 1024 x 1024 pixel images. Use the U1 U2 U3 U4 buttons under each image grid to separate your selected image. You can then use the Upscale (2x) or Upscale (4x) tools to increase the size of your image.</p>
<p>Upscale tools use your subscription's GPU minutes. Using Upscale 2X on an image takes roughly twice as long as generating an initial image grid. Using Upscale 4X on an image takes roughly six times as long as generating an initial image grid.</p>
<p>Upscale tools are not compatible with the pan tool or the tile parameter.</p>
<p>How to Use the Upscale Tools</p>
<ol>
<li>Generate an Image</li>
</ol>
<p>Create an image using the /imagine command.</p>
<ol start="2">
<li>Select an Image</li>
</ol>
<p>Use a U button to separate your selected image from the grid.</p>
<ol start="3">
<li>Select Upscale</li>
</ol>
<p>Click on the Upscale button to upscale your image. The upscaler will double the size of your image.</p>
<p>Upscale Comparison</p>
<p>Prompt: chiaroscuro rooster portrait
Original 1024 by 1024 pixel image.</p>
<p>Detail from original image</p>
<p>Original image</p>
<p>After Upscale (2x) to 2048 x 2048 px</p>
<p>Upscale Comparison</p>
<p>Prompt: 1960s pop-art acrylic of redwoods
Original 1024 by 1024 pixel image.</p>
<p>Detail from the original image</p>
<p>Original image</p>
<p>After Upscale (4x) to 4096 x 4096 px</p>
<p>Upscale Tips</p>
<p>Upscale older Jobs by refreshing them with the /show command.</p>
<p>Learn More</p>
<p>Learn more about image sizes, dimensions, and DPI.</p>
<p>Legacy Upscalers</p>
<p>Earlier Midjourney model versions generated grids of lower-resolution images. You can use a legacy Midjourney upscaler on these images to increase the size and add additional details. There are multiple legacy upscale models available for upscaling images made with earlier Midjourney models. Using a legacy upscaler uses your subscription's GPU minutes.</p>
<p>Previous
Vary Region + Remix
Next
Version
Midjourney is an independent research lab exploring new mediums of thought and expanding the imaginative powers of the human species. We are a small self-funded team focused on design, human infrastructure, and AI.
FOLLOW US: [F] [T] [R]
SUPPORT
For questions or support visit the Midjourney Discord support channels.
SITES
Midjourney Website
Midjourney Discord
Current
Legacy
Getting Started
Quick Start
Community Guidelines
Next Steps
Variations
Vary Region
Vary Region + Remix
Upscalers
Version
Using Discord
Commands Parameters and Tools
Writing Prompts
Working with Your Own Images
Styles and Aesthetics
Image Size and Aspect Ratio
Advanced Prompting Tools
Subscriptions
Using The Website
Policies
Vary Region + Remix
Light
Combine Remix Mode and the Midjourney Vary Region editor to select and regenerate specific parts of an upscaled image using a new or modified prompt.</p>
<p>This tool requires you to use up-to-date Discord client versions, if you do not see the Vary Region button, try updating your Discord client.</p>
<p>The Vary (Region) button appears after a Midjourney image has been upscaled.
Regional variations results are guided by the content in your original image, the area you select, and the modified prompt used.
Vary (Region) is compatible with Midjourney Model Versions V5.0, V5.1, V5.2, and niji 5</p>
<p>How to Use Remix Mode with Vary (Region)</p>
<ol>
<li>Enable Remix Mode</li>
</ol>
<p>Use the /settings command and select 🎛️ Remix from the pop-up.</p>
<ol start="2">
<li>Generate an Image</li>
</ol>
<p>Create an image using the /imagine command.</p>
<ol start="3">
<li>Upscale the Image</li>
</ol>
<p>Use the U buttons to upscale your selected image.</p>
<ol start="4">
<li>Select Vary Region</li>
</ol>
<p>Click the 🖌️ Vary (Region) button to open the editing interface.</p>
<ol start="5">
<li>
<p>Select Areas to Regenerate
Choose the freehand or rectangular selection tools in the lower left of the Editor.
Select the areas of your image that you want to regenerate.
The size of your selection will affect your results. Larger selections give the Midjourney Bot more room to generate new creative details. Smaller selections will result in smaller, more subtle changes.
Note: You cannot edit an existing selection but can undo multiple steps using the undo button in the upper right corner.</p>
</li>
<li>
<p>Modify Your Prompt</p>
</li>
</ol>
<p>Describe what you want to generate in the selected areas with an updated prompt. Your revised prompt should focus on the details you want to introduce or change.</p>
<ol start="7">
<li>Submit Your Job</li>
</ol>
<p>Click the Submit → button to send your request to Midjourney Bot. The Vary Region editor can now be closed, and you can return to Discord while your Job is processed.</p>
<p>Note You can use the Editor Button underneath an upscaled image multiple times to experiment with different selections and prompts.</p>
<ol start="8">
<li>View Your Results</li>
</ol>
<p>The Midjourney Bot will process your Job and generate a new image grid using the information from your original image and the guidance of your new prompt.</p>
<p>New Image Grid</p>
<p>Original vs. Varied Comparison</p>
<ol start="8">
<li>Upscale and Vary Region again</li>
</ol>
<p>You can upscale one of your new images and use the Vary Region Editor again to continue refining your image.</p>
<p>Selection
Result</p>
<p>Updated Prompt: hot air balloon lithograph</p>
<p>Result</p>
<p>Updated Prompt: castle in the meadow lithograph</p>
<p>Vary Region + Remix Prompting Tips</p>
<p>Selections The size of your selection affects the outcome. Larger selections provide the Midjourney Bot with more contextual information, which can improve the scaling and context of new additions. But selecting too much may lead to the newly generated elements blending or replacing parts of the original image you wished to preserve.</p>
<p>Prompts Experiment with how you modify your prompt when Using Vary Region + Remix Mode. Prompts should focus on what you want to happen in the selected area. The Midjourney Bot also considers the existing image when generating a selected area, so shorter focused prompts often are more effective. Midjourney Prompts should not be conversational. Instead of &quot;Please change the meadow trail into a beautiful stream,&quot; be direct and prompt &quot;meadow stream.&quot;</p>
<p>Work in Small Steps If you want to change many parts of an image, work on one part at a time. This way, you can create a focused prompt for each part.</p>
<p>Vary Region + Remix Examples</p>
<p>Original Image</p>
<p>Prompt: gouache alligator in sunglasses</p>
<p>Selection</p>
<p>Updated Prompt: gouache alligator in green sunglasses</p>
<p>Result</p>
<p>Updated Prompt: gouache alligator in green sunglasses</p>
<p>Original Image</p>
<p>Prompt: scratchboard apple tree branch</p>
<p>Selection</p>
<p>Updated Prompt: rainbow scratchboard apple tree branch</p>
<p>Result</p>
<p>Updated Prompt: rainbow scratchboard apple tree branch</p>
<p>Technical Details</p>
<p>Jobs generated using Vary (Region) + Remix Mode will honor the following parameters:
--chaos
--fast
--iw
--no
--stylize
--relax
--style
--version
--video
--weird</p>
<p>Previous
Vary Region
Next
Upscalers
Midjourney is an independent research lab exploring new mediums of thought and expanding the imaginative powers of the human species. We are a small self-funded team focused on design, human infrastructure, and AI.
FOLLOW US: [F] [T] [R]
SUPPORT
For questions or support visit the Midjourney Discord support channels.
SITES
Midjourney Website
Midjourney Discord
Current
Legacy
Getting Started
Quick Start
Community Guidelines
Next Steps
Variations
Vary Region
Vary Region + Remix
Upscalers
Version
Using Discord
Commands Parameters and Tools
Writing Prompts
Working with Your Own Images
Styles and Aesthetics
Image Size and Aspect Ratio
Advanced Prompting Tools
Subscriptions
Using The Website
Policies
Vary Region
Light
Use the Midjourney Vary Region editor to select and regenerate specific parts of an upscaled image.</p>
<p>The Vary (Region) button appears after a Midjourney image has been upscaled.
Regional variations are guided by the content in your original image and the area you select.
Vary (Region) is compatible with Midjourney Model Versions V5.0, V5.1, V5.2, and niji 5</p>
<p>How to Use Vary Region</p>
<ol>
<li>Generate an Image</li>
</ol>
<p>Create an image using the /imagine command.</p>
<ol start="2">
<li>Upscale the Image</li>
</ol>
<p>Use the U buttons to upscale your selected image.</p>
<ol start="3">
<li>Select Vary Region</li>
</ol>
<p>Click on the 🖌️ Vary (Region) button to open the editing interface.</p>
<ol start="4">
<li>
<p>Select Areas to Regenerate
Choose the freehand or rectangular selection tools in the lower left of the Editor.
Select the areas of your image that you want to regenerate.
The size of your selection will affect your results. Larger selections give the Midjourney Bot more room to generate new creative details. Smaller selections will result in smaller, more subtle changes.
Note: You cannot edit an existing selection but can undo multiple steps using the undo button in the upper right corner.</p>
</li>
<li>
<p>Submit Your Job</p>
</li>
</ol>
<p>Click the Submit → button to send your request to Midjourney Bot. The Vary Region editor can now be closed, and you can return to Discord while your Job is processed.</p>
<p>Note You can use the 🖌️ Vary (Region) button underneath an upscaled image multiple times to experiment with different selections. Your previous selection will be preserved. You can continue to add to this existing selection or use the undo button to clear your selection.</p>
<ol start="6">
<li>View Your Results</li>
</ol>
<p>The Midjourney Bot will process your Job and generate a new image grid of variations within the area you selected.</p>
<p>Vary Region Examples</p>
<p>Upscaled Image</p>
<p>Prompt: colorful candy brooches</p>
<p>Selection</p>
<p>Result</p>
<p>Upscaled Image</p>
<p>Prompt: architectural drawing of a house</p>
<p>Selection</p>
<p>Result</p>
<p>Vary Region + Remix Mode</p>
<p>You can use Remix mode with the Vary Region editor to update your prompt while regenerating specific parts of your image. Read about using Remix Mode with the Vary Region editor.</p>
<p>Original Image</p>
<p>Prompt: gouache alligator in sunglasses</p>
<p>Selection</p>
<p>Updated Prompt: gouache alligator in green sunglasses</p>
<p>Result</p>
<p>Updated Prompt: gouache alligator in green sunglasses</p>
<p>Technical Details</p>
<p>Jobs generated using Vary (Region) will honor the following parameters:
--chaos
--fast
--iw
--no
--stylize
--relax
--style
--version
--video
--weird</p>
<p>Previous
Variations
Next
Vary Region + Remix
Midjourney is an independent research lab exploring new mediums of thought and expanding the imaginative powers of the human species. We are a small self-funded team focused on design, human infrastructure, and AI.
FOLLOW US: [F] [T] [R]
SUPPORT
For questions or support visit the Midjourney Discord support channels.
SITES
Midjourney Website
Midjourney Discord
Current
Legacy
Getting Started
Quick Start
Community Guidelines
Next Steps
Variations
Vary Region
Vary Region + Remix
Upscalers
Version
Using Discord
Commands Parameters and Tools
Writing Prompts
Working with Your Own Images
Styles and Aesthetics
Image Size and Aspect Ratio
Advanced Prompting Tools
Subscriptions
Using The Website
Policies
Variations
Light
Create subtle or strong variations of a generated image using the the V1 V2 V3 V4 buttons under each image grid, or use the Vary (Strong) and Vary (Subtle) buttons under an upscaled image.</p>
<p>Control the amount of variation created with these buttons using the 🎨 High Variation Mode and 🎨 Low Variation Mode settings. You can also create variations of upscaled images using the 🪄 Vary (Strong) or 🪄 Vary (Subtle) buttons that appear under upscaled images. 🪄 Vary (Region) modifies a selected area within a generated image without changing the entire image. Read more about Vary Region</p>
<p>🎨 High Variation Mode and 🎨 Low Variation Mode affect Remix Mode
🎨 High Variation Mode is the default setting.
🪄 Vary (Strong) and 🪄 Vary (Subtle) are availabe with Model Version 5.2 and Niji 5. Older models have a single 🪄 Make Variations button.</p>
<p>Variation Comparison</p>
<p>With 🎨 High Variation Mode and 🪄 Vary (Strong), Using the Variation buttons will produce a new images that may change the composition, number of elements, colors, and the type of details within the image. High Variation Mode is useful for creating multiple concepts based on a single generated image.</p>
<p>🎨 Low Variation Mode and 🪄 Vary (Subtle) produce variations that retain the main composition of the original image but introduce subtle changes to its details. This mode helps refine or make slight adjustments to an image.</p>
<p>Prompt example: imagine/ prompt mosaic estuary</p>
<p>Original Upscaled Image
Image grid using 🪄Vary (Subtle)
Image grid using 🪄Vary (Strong)</p>
<p>Prompt example: imagine/ prompt Murrine glass vegetables</p>
<p>Original Upscaled Image
Image grid using 🪄Vary (Subtle)
Image grid using 🪄Vary (Strong)</p>
<p>How to Change the Amount of Variation
Use the Vary Buttons</p>
<p>Use the 🪄 Vary (Strong) or 🪄 Vary (Subtle) buttons under upscaled images to create strong or subtle variations of that image.</p>
<p>Use the Settings Command</p>
<p>Use the /settings command and select 🎨 High Variation Mode or 🎨 Low Variation Mode from the menu to set a preference for more or less varied results when using the V1 V2 V3 V4 buttons.</p>
<p>Use the /prefer variability command</p>
<p>Toggle between using 🎨 High Variation Mode and 🎨 Low Variation Mode when using the V1 V2 V3 and V4 buttons underneath an image grid.</p>
<p>Previous
Explore Prompting
Next
Vary Region
Midjourney is an independent research lab exploring new mediums of thought and expanding the imaginative powers of the human species. We are a small self-funded team focused on design, human infrastructure, and AI.
FOLLOW US: [F] [T] [R]
SUPPORT
For questions or support visit the Midjourney Discord support channels.
SITES
Midjourney Website
Midjourney Discord
Current
Legacy
Getting Started
Quick Start
Community Guidelines
Next Steps
Prompts
Explore Prompting
Variations
Vary Region
Vary Region + Remix
Upscalers
Version
Using Discord
Commands Parameters and Tools
Writing Prompts
Working with Your Own Images
Styles and Aesthetics
Image Size and Aspect Ratio
Advanced Prompting Tools
Subscriptions
Using The Website
Policies
Explore Prompting
Light
Even short single-word prompts will produce beautiful images in Midjourney's default style, but you can create more interesting personalized results by combining concepts like artistic medium, historical periods, location, and more.
Pick A Medium</p>
<p>Break out the paint, crayons, scratchboard, printing presses, glitter, ink, and colored paper. One of the best ways to generate a stylish image is by specifying an artistic medium.</p>
<p>prompt example: /imagine prompt <any art style> style cat</p>
<p>Block Print 
Folk Art 
Cyanotype 
Graffiti 
Paint-by-Numbers 
Risograph 
Ukiyo-e 
Pencil Sketch 
Watercolor 
Pixel Art 
Blacklight Painting 
Cross Stitch 
Get Specific</p>
<p>More precise words and phrases will help create an image with exactly the right look and feel.</p>
<p>prompt example: /imagine prompt <style> sketch of a cat</p>
<p>Life Drawing 
Continuous Line 
Loose Gestural 
Blind Contour 
Value Study 
Charcoal Sketch </p>
<p>Time Travel</p>
<p>Different eras have distinct visual styles.</p>
<p>prompt example: /imagine prompt <decade> cat illustration</p>
<p>1700s 
1800s 
1900s 
1910s 
1920s 
1930s 
1940s 
1950s 
1960s 
1970s 
1980s 
1990s </p>
<p>Emote</p>
<p>Use emotion words to give characters personality.</p>
<p>prompt example: /imagine prompt <emotion> cat</p>
<p>Determined 
Happy 
Sleepy 
Angry 
Shy 
Embarassed </p>
<p>Get Colorful</p>
<p>A full spectrum of possibilities.</p>
<p>prompt example: /imagine prompt <color word> colored cat</p>
<p>Millennial Pink 
Acid Green 
Desaturated 
Canary Yellow 
Peach 
Two Toned 
Pastel 
Mauve 
Ebony 
Neutral 
Day Glo 
Green Tinted </p>
<p>Environmental Exploration</p>
<p>Different environments can set unique moods.</p>
<p>prompt example: /imagine prompt <location> cat</p>
<p>Tundra 
Salt Flat 
Jungle 
Desert 
Mountain 
Cloud Forest 
Previous
Prompts
Next
Variations
Midjourney is an independent research lab exploring new mediums of thought and expanding the imaginative powers of the human species. We are a small self-funded team focused on design, human infrastructure, and AI.
FOLLOW US: [F] [T] [R]
SUPPORT
For questions or support visit the Midjourney Discord support channels.
SITES
Midjourney Website
Midjourney Discord
Current
Current
Legacy
Getting Started
Quick Start
Community Guidelines
Next Steps
Prompts
Explore Prompting
Variations
Vary Region
Vary Region + Remix
Upscalers
Version
Using Discord
Commands Parameters and Tools
Writing Prompts
Working with Your Own Images
Styles and Aesthetics
Image Size and Aspect Ratio
Advanced Prompting Tools
Subscriptions
Using The Website
Policies
Prompts
Light
A Prompt is a short text phrase that the Midjourney Bot interprets to produce an image. The Midjourney Bot breaks down the words and phrases in a prompt into smaller pieces, called tokens, that can be compared to its training data and then used to generate an image. A well-crafted prompt can help make unique and exciting images.
Basic Prompts</p>
<p>A basic prompt can be as simple as a single word, phrase or emoji</p>
<p>Prompting Tip!</p>
<p>The Midjourney Bot works best with simple, short sentences that describe what you want to see. Avoid long lists of requests. Instead of: Show me a picture of lots of blooming California poppies, make them bright, vibrant orange, and draw them in an illustrated style with colored pencils Try: Bright orange California poppies drawn with colored pencils</p>
<p>Advanced Prompts</p>
<p>More advanced prompts can include one or more image URLs, multiple text phrases, and one or more parameters</p>
<p>Image Prompts</p>
<p>Image URLs can be added to a prompt to influence the style and content of the finished result. Image URLs always go at the front of a prompt.</p>
<p>Read more about Image Prompts</p>
<p>Prompt Text</p>
<p>The text description of what image you want to generate. See below for prompting information and tips. Well-written prompts help generate amazing images.</p>
<p>Parameters</p>
<p>Parameters change how an image generates. Parameters can change aspect ratios, models, upscalers, and lots more. Parameters go at the end of the prompt.</p>
<p>Read more about Parameters</p>
<p>Prompting Notes
Prompt Length</p>
<p>Prompts can be very simple. Single words (or even an emoji!) will produce an image. Very short prompts will rely heavily on Midjourney’s default style, so a more descriptive prompt is better for a unique look. However, super-long prompts aren’t always better. Concentrate on the main concepts you want to create.</p>
<p>Grammar</p>
<p>The Midjourney Bot does not understand grammar, sentence structure, or words like humans. Word choice also matters. More specific synonyms work better in many circumstances. Instead of big, try gigantic, enormous, or immense. Remove words when possible. Fewer words mean each word has a more powerful influence. Use commas, brackets, and hyphens to help organize your thoughts, but know the Midjourney Bot will not reliably interpret them. The Midjourney Bot does not consider capitalization.</p>
<p>Midjourney Model Version 4 is slightly better than other models at interpreting traditional sentence structure.</p>
<p>Focus on What you Want</p>
<p>It is better to describe what you want instead of what you don’t want. If you ask for a party with “no cake,” your image will probably include a cake. If you want to ensure an object is not in the final image, try advance prompting using the --no parameter.</p>
<p>Think About What Details Matter</p>
<p>Anything left unsaid may surprise you. Be as specific or vague as you want, but anything you leave out will be randomized. Being vague is a great way to get variety, but you may not get the specific details you want.</p>
<p>Try to be clear about any context or details that are important to you. Think about:</p>
<p>Subject: person, animal, character, location, object, etc.
Medium: photo, painting, illustration, sculpture, doodle, tapestry, etc.
Environment: indoors, outdoors, on the moon, in Narnia, underwater, the Emerald City, etc.
Lighting: soft, ambient, overcast, neon, studio lights, etc
Color: vibrant, muted, bright, monochromatic, colorful, black and white, pastel, etc.
Mood: Sedate, calm, raucous, energetic, etc.
Composition: Portrait, headshot, closeup, birds-eye view, etc.</p>
<p>Use Collective Nouns</p>
<p>Plural words leave a lot to chance. Try specific numbers. &quot;Three cats&quot; is more specific than &quot;cats.&quot; Collective nouns also work, “flock of birds” instead of &quot;birds.”</p>
<p>Previous
Community Guidelines
Next
Explore Prompting
Midjourney is an independent research lab exploring new mediums of thought and expanding the imaginative powers of the human species. We are a small self-funded team focused on design, human infrastructure, and AI.
FOLLOW US: [F] [T] [R]
SUPPORT
For questions or support visit the Midjourney Discord support channels.
SITES
Midjourney Website
Midjourney Discord
Current
Legacy
Getting Started
Quick Start
Community Guidelines
Next Steps
Variations
Vary Region
Vary Region + Remix
Upscalers
Version
Using Discord
Commands Parameters and Tools
Writing Prompts
Working with Your Own Images
Styles and Aesthetics
Image Size and Aspect Ratio
Advanced Prompting Tools
Subscriptions
Using The Website
Policies
Community Guidelines
Light
Midjourney is an open-by-default community. To keep the platform accessible and welcoming to the broadest number of users, content must be PG-13.
Be kind and respect each other and staff. Do not create images or use text prompts that are inherently disrespectful, aggressive, or otherwise abusive. Violence or harassment of any kind will not be tolerated.
No adult content or gore. Please avoid making visually shocking or disturbing content. We will block some text inputs automatically.
Do not publicly repost the creations of others without their permission.
Be careful about sharing. It’s OK to share your creations outside of the Midjourney community, but please consider how others might view your content.
Any violations of these rules may lead to bans from our services. We are not a democracy. Behave respectfully or lose your rights to use the Service.</p>
<p>These rules apply to all content, including images made in private servers, using Private Mode, and in direct messages with the Midjourney Bot. See the Discord #rules channel for the most up-to-date information.</p>
<p>Notes
What is Considered Gore?</p>
<p>Gore includes images of detached body parts of humans or animals, cannibalism, blood, violence (images of shooting or bombing someone, for instance), deformed bodies, severed limbs, pestilence, etc.</p>
<p>What's NSFW or Adult Content?</p>
<p>Avoid nudity, sexual organs, fixation on naked breasts, people in showers or on toilets, sexual imagery, fetishes, etc.</p>
<p>Other Offensive Content</p>
<p>Other things may be deemed offensive or abusive because they can be viewed as racist, homophobic, disturbing, or in some way derogatory to a community. This includes offensive or inflamatory images of celebrities or public figures. Content guidelines are continually reviewed and may be modified as the Midjourney community grows.</p>
<p>About the Community Standards</p>
<p>Finding a balance between facilitating artistic expression and creating a safe and welcoming community for a broad group of users is challenging. Everyone has a right to an opinion about what images are beautiful, intriguing, or inspiring, and there are as many individual interpretations of what content is offensive, inappropriate, or harmful. We also recognize that art has a long history of pushing the status quo to challenge social conventions, inspire discussion, and spark change.</p>
<p>When deciding on our community guidelines for Midjourney, we considered many factors. As a new and rapidly developing technology, it is important to consider how the broader public will perceive and be introduced to this technology. Midjourney is a more participatory image-making tool than a paintbrush or pencil. The Midjourney team does not wish to participate in creating pornographic, offensive, or harmful images. Most important, however, was our desire to create a welcoming and inclusive community for all users. To that end, we have established guidelines prohibiting certain behaviors and language on our platform.</p>
<p>We understand that the balance we decided on may not be popular with everyone. Still, we hope you will respect our efforts to create a positive and supportive community for all. We encourage you to express yourself creatively but to do so in a way that is respectful and considerate of others.</p>
<p>Users who repeatedly violate the terms of service may be warned by a community moderator, given a time-out from accessing the bot, or be blocked from the service.</p>
<p>Self-Policing and Reporting</p>
<p>Occasionally prompts will unintentionally produce non-PG-13 content. Please self-police these images by using the ❌ emoji reaction or by right-clicking selecting Apps and then clicking Cancel Job to delete the image.</p>
<p>Users may flag any image for moderator review by right-clicking selecting Apps, and then clicking Report Job within Discord or the website by selecting ... under an image and clicking Report.</p>
<p>Report Images in Discord
Report Images on the Website</p>
<p>Automation</p>
<p>In order to maintain the highest quality experience for all users, Midjourney accounts are designed for individual use and each user may maintain only one account. Midjourney does not provide an API to access the bot through third-party apps or scripts, and automating interactions with the Midjourney Bot is strictly prohibited according to our Terms of Service. Accounts who do not comply with these rules may be blocked.</p>
<p>Previous
Quick Start
Next
Prompts
Midjourney is an independent research lab exploring new mediums of thought and expanding the imaginative powers of the human species. We are a small self-funded team focused on design, human infrastructure, and AI.
FOLLOW US: [F] [T] [R]
SUPPORT
For questions or support visit the Midjourney Discord support channels.
SITES
Midjourney Website
Midjourney Discord
Current
Legacy
Getting Started
Quick Start
Community Guidelines
Next Steps
Variations
Vary Region
Vary Region + Remix
Upscalers
Version
Using Discord
Commands Parameters and Tools
Writing Prompts
Working with Your Own Images
Styles and Aesthetics
Image Size and Aspect Ratio
Advanced Prompting Tools
Subscriptions
Using The Website
Policies
Quick Start
Light
Use the Midjourney bot to generate stunning images from simple text prompts in seconds. Work directly in Discord. No specialized hardware or software is required.
Code of Conduct</p>
<p>Don't be a jerk.
Don't use our tools to make images that could inflame, upset, or cause drama. That includes gore and adult content.
Be respectful to other people and the team.</p>
<p>Making Images with Midjourney</p>
<ol>
<li>Log In To Discord</li>
</ol>
<p>Access the Midjourney Bot through Discord via web browser, mobile app, or desktop app. Ensure you have a verified Discord account before joining the Midjourney Discord server.</p>
<p>Follow these guides to create or verify your Discord account:
Create Discord Account
Verify Discord Account</p>
<ol start="2">
<li>Subscribe to a Midjourney Plan</li>
</ol>
<p>To start generating images with Midjourney, you'll need to subscribe to a plan.</p>
<p>Visit Midjourney.com/account.
Sign in using your verified Discord account.
Choose a subscription plan that suits your needs.</p>
<p>Go to Subscription Plans for information on pricing and the features available with each tier.</p>
<ol start="3">
<li> Join the Midjourney Server on Discord</li>
</ol>
<p>To start interacting with the Midjourney Bot, join the Midjourney Server</p>
<p>Open Discord and locate the server list on the left-hand sidebar.
Press the + button at the bottom of the server list.
In the pop-up window, click the Join a Server button.
Paste or type the following URL: http://discord.gg/midjourney and press Join.</p>
<p>For additional information, Learn more about Discord servers.</p>
<ol start="4">
<li>Go to any #General or #Newbie Channel</li>
</ol>
<p>After joining the Midjourney server on Discord, you'll see several channels listed in the sidebar.</p>
<p>On the Midjourney Server</p>
<p>Locate and select any channel labeled general-# or newbie-#. These channels are designed for beginners to start using the Midjourney bot. The Midjourney Bot will not generate images in other channels.</p>
<p>On Other Servers</p>
<p>You can generate images with the Midjourney Bot on any Discord server that has invited the Midjourney Bot. Look for instructions on your server on where to use the Bot.</p>
<p>5. Use the /imagine Command</p>
<p>About Discord Commands
Interact with the Midjourney Bot on Discord using a Command. Commands are used to create images, change default settings, monitor user info, and perform other helpful tasks. The /imagine command generates a unique image from a short text description (known as a Prompt). Learn more about Prompts</p>
<p>How to Use /imagine</p>
<p>Type '/imagine prompt:' in the message field. You can also select the /imagine command from the list of available slash commands that pop up when you type '/'.
Type a description of the image you want to create in the prompt field.
Send your message. The Bot will interpret your text prompt and begin generating the images.
Respect the Community Guidelines. Community guidelines apply wherever the Midjourney Bot is used.</p>
<p>6. Accept the Terms Of Service</p>
<p>Before generating any images, the Midjourney Bot will prompt you to accept the Terms Of Service. You must agree to these terms to proceed with image creation.</p>
<p>7. Image Generation Process</p>
<p>After submitting a text prompt, the Midjourney Bot processes your request, creating four unique image options within a minute. This process utilizes advanced Graphics Processing Units (GPUs), and each image generation counts towards the GPU time included with your Midjourney subscription. To monitor your available GPU time (Fast Time Remaining) use the /info command.</p>
<p>8. Select an Image or Create Variations</p>
<p>Once your initial image grid has been generated, two rows of buttons become available underneath your image grid.</p>
<p>U1 U2 U3 U4 Image Selection</p>
<p>In earlier versions of Midjourney, the U buttons were used for upscaling images. With the latest model, images are immediately generated at 1024 x 1024 pixels size. Now, the 'U' buttons help you separate out your chosen image from the grid, making it easier to download and giving you access to additional editing and generation tools.</p>
<p>🔄 Re-run or Re-roll a Job</p>
<p>The 🔄 button re-runs a Job. In this case, it would re-run the original prompt producing a new grid of images.</p>
<p>V1 V2 V3 V4 Image Variation</p>
<p>The V buttons are used to create variations of the selected image. Each button generates a new image grid that maintains the general style and composition of the selected image.</p>
<p>9. Enhance or Modify Your Image</p>
<p>After singling out an image, an expanded set of options becomes available.</p>
<p>🪄 Vary (Strong) 🪄 Vary (Subtle)</p>
<p>Create a stronger or subtle variation of your selected image, generating a new grid of four options.</p>
<p>🔍 Zoom Out 2x 🔍 Zoom Out 1.5x 🔍 Custom Zoom</p>
<p>Zoom Out of your image, extending the canvas its original boundaries without changing the content of the original image. The newly expanded canvas will be filled-in using guidance from the prompt and the original image.</p>
<p>⬅️ ➡️ ⬆️ ⬇️</p>
<p>The Pan buttons allow you to expand the canvas of an image in a chosen direction without changing the content of the original image. The newly expanded canvas will be filled in using guidance from the prompt and the original image.</p>
<p>❤️ Favorite</p>
<p>tag your best images to easily find them on the Midjourney website.</p>
<p>Web ↗️</p>
<p>Open the image in your gallery on Midjourney.com</p>
<p>10. Save Your Image</p>
<p>Click on the image to open it to full size, and then right-click and choose Save image. On mobile, long-tap the image and then tap the download icon in the top right corner.</p>
<p>All images are immediately available to view on midjourney.com/imagine</p>
<p>Next Steps</p>
<p>Direct Messaging the Midjourney Bot
Are you having difficulty finding yourself while working with other users on the Midjourney server? You can interact with the Midjourney Bot one-on-one in a direct message. Images created within your direct messages are still subject to content and moderation rules and are visible on the Midjourney website.</p>
<p>Learn More About Prompts
Learn how to write effective and creative prompts,
Explore how descriptions of artistic mediums, locations, and time periods change an image.</p>
<p>Blend Your Own Images
Learn how to upload and blend your own images using the /blend command.</p>
<p>Midjourney Bot Assistance</p>
<p>Need help or have questions? Try these commands:
/help Displays useful information and tips about the Midjourney Bot.
/ask Provides answers to questions about the Midjourney Bot.</p>
<p>You can also visit the #support channel on the Midjourney Discord for additional assistance.</p>
<p>Billing and Subscription Queries</p>
<p>For billing and subscription questions, please visit help.midjourney.com.</p>
<p>Next
Community Guidelines
Midjourney is an independent research lab exploring new mediums of thought and expanding the imaginative powers of the human species. We are a small self-funded team focused on design, human infrastructure, and AI.
FOLLOW US: [F] [T] [R]
SUPPORT
For questions or support visit the Midjourney Discord support channels.
SITES
Midjourney Website
Midjourney Discord
Current
Legacy
Getting Started
Quick Start
Community Guidelines
Next Steps
Variations
Vary Region
Vary Region + Remix
Upscalers
Version
Using Discord
Commands Parameters and Tools
Writing Prompts
Working with Your Own Images
Styles and Aesthetics
Image Size and Aspect Ratio
Advanced Prompting Tools
Subscriptions
Using The Website
Policies
Quick Start
Light
Use the Midjourney bot to generate stunning images from simple text prompts in seconds. Work directly in Discord. No specialized hardware or software is required.
Code of Conduct</p>
<p>Don't be a jerk.
Don't use our tools to make images that could inflame, upset, or cause drama. That includes gore and adult content.
Be respectful to other people and the team.</p>
<p>Making Images with Midjourney</p>
<ol>
<li>Log In To Discord</li>
</ol>
<p>Access the Midjourney Bot through Discord via web browser, mobile app, or desktop app. Ensure you have a verified Discord account before joining the Midjourney Discord server.</p>
<p>Follow these guides to create or verify your Discord account:
Create Discord Account
Verify Discord Account</p>
<ol start="2">
<li>Subscribe to a Midjourney Plan</li>
</ol>
<p>To start generating images with Midjourney, you'll need to subscribe to a plan.</p>
<p>Visit Midjourney.com/account.
Sign in using your verified Discord account.
Choose a subscription plan that suits your needs.</p>
<p>Go to Subscription Plans for information on pricing and the features available with each tier.</p>
<ol start="3">
<li> Join the Midjourney Server on Discord</li>
</ol>
<p>To start interacting with the Midjourney Bot, join the Midjourney Server</p>
<p>Open Discord and locate the server list on the left-hand sidebar.
Press the + button at the bottom of the server list.
In the pop-up window, click the Join a Server button.
Paste or type the following URL: http://discord.gg/midjourney and press Join.</p>
<p>For additional information, Learn more about Discord servers.</p>
<ol start="4">
<li>Go to any #General or #Newbie Channel</li>
</ol>
<p>After joining the Midjourney server on Discord, you'll see several channels listed in the sidebar.</p>
<p>On the Midjourney Server</p>
<p>Locate and select any channel labeled general-# or newbie-#. These channels are designed for beginners to start using the Midjourney bot. The Midjourney Bot will not generate images in other channels.</p>
<p>On Other Servers</p>
<p>You can generate images with the Midjourney Bot on any Discord server that has invited the Midjourney Bot. Look for instructions on your server on where to use the Bot.</p>
<p>5. Use the /imagine Command</p>
<p>About Discord Commands
Interact with the Midjourney Bot on Discord using a Command. Commands are used to create images, change default settings, monitor user info, and perform other helpful tasks. The /imagine command generates a unique image from a short text description (known as a Prompt). Learn more about Prompts</p>
<p>How to Use /imagine</p>
<p>Type '/imagine prompt:' in the message field. You can also select the /imagine command from the list of available slash commands that pop up when you type '/'.
Type a description of the image you want to create in the prompt field.
Send your message. The Bot will interpret your text prompt and begin generating the images.
Respect the Community Guidelines. Community guidelines apply wherever the Midjourney Bot is used.</p>
<p>6. Accept the Terms Of Service</p>
<p>Before generating any images, the Midjourney Bot will prompt you to accept the Terms Of Service. You must agree to these terms to proceed with image creation.</p>
<p>7. Image Generation Process</p>
<p>After submitting a text prompt, the Midjourney Bot processes your request, creating four unique image options within a minute. This process utilizes advanced Graphics Processing Units (GPUs), and each image generation counts towards the GPU time included with your Midjourney subscription. To monitor your available GPU time (Fast Time Remaining) use the /info command.</p>
<p>8. Select an Image or Create Variations</p>
<p>Once your initial image grid has been generated, two rows of buttons become available underneath your image grid.</p>
<p>U1 U2 U3 U4 Image Selection</p>
<p>In earlier versions of Midjourney, the U buttons were used for upscaling images. With the latest model, images are immediately generated at 1024 x 1024 pixels size. Now, the 'U' buttons help you separate out your chosen image from the grid, making it easier to download and giving you access to additional editing and generation tools.</p>
<p>🔄 Re-run or Re-roll a Job</p>
<p>The 🔄 button re-runs a Job. In this case, it would re-run the original prompt producing a new grid of images.</p>
<p>V1 V2 V3 V4 Image Variation</p>
<p>The V buttons are used to create variations of the selected image. Each button generates a new image grid that maintains the general style and composition of the selected image.</p>
<p>9. Enhance or Modify Your Image</p>
<p>After singling out an image, an expanded set of options becomes available.</p>
<p>🪄 Vary (Strong) 🪄 Vary (Subtle)</p>
<p>Create a stronger or subtle variation of your selected image, generating a new grid of four options.</p>
<p>🔍 Zoom Out 2x 🔍 Zoom Out 1.5x 🔍 Custom Zoom</p>
<p>Zoom Out of your image, extending the canvas its original boundaries without changing the content of the original image. The newly expanded canvas will be filled-in using guidance from the prompt and the original image.</p>
<p>⬅️ ➡️ ⬆️ ⬇️</p>
<p>The Pan buttons allow you to expand the canvas of an image in a chosen direction without changing the content of the original image. The newly expanded canvas will be filled in using guidance from the prompt and the original image.</p>
<p>❤️ Favorite</p>
<p>tag your best images to easily find them on the Midjourney website.</p>
<p>Web ↗️</p>
<p>Open the image in your gallery on Midjourney.com</p>
<p>10. Save Your Image</p>
<p>Click on the image to open it to full size, and then right-click and choose Save image. On mobile, long-tap the image and then tap the download icon in the top right corner.</p>
<p>All images are immediately available to view on midjourney.com/imagine</p>
<p>Next Steps</p>
<p>Direct Messaging the Midjourney Bot
Are you having difficulty finding yourself while working with other users on the Midjourney server? You can interact with the Midjourney Bot one-on-one in a direct message. Images created within your direct messages are still subject to content and moderation rules and are visible on the Midjourney website.</p>
<p>Learn More About Prompts
Learn how to write effective and creative prompts,
Explore how descriptions of artistic mediums, locations, and time periods change an image.</p>
<p>Blend Your Own Images
Learn how to upload and blend your own images using the /blend command.</p>
<p>Midjourney Bot Assistance</p>
<p>Need help or have questions? Try these commands:
/help Displays useful information and tips about the Midjourney Bot.
/ask Provides answers to questions about the Midjourney Bot.</p>
<p>You can also visit the #support channel on the Midjourney Discord for additional assistance.</p>
<p>Billing and Subscription Queries</p>
<p>For billing and subscription questions, please visit help.midjourney.com.</p>
<p>Next
Community Guidelines
Midjourney is an independent research lab exploring new mediums of thought and expanding the imaginative powers of the human species. We are a small self-funded team focused on design, human infrastructure, and AI.
FOLLOW US: [F] [T] [R]
SUPPORT
For questions or support visit the Midjourney Discord support channels.
SITES
Midjourney Website
Midjourney Discord</p>
<div style="break-before: page; page-break-before: always;"></div><p>The Big Book of Prompts </p>
<p>AiTuts.com</p>
<p>Copyright © 2022 AiTuts.com </p>
<p>All rights reserved. </p>
<p>Dedicated to the old masters.</p>
<p>They will never be surpassed.</p>
<p>CONTENTS </p>
<p>Foreword </p>
<p>I  Game Art </p>
<p>II  Illustration </p>
<p>III  Architecture </p>
<p>IV  Arts &amp; Crafts </p>
<p>i </p>
<p>1 </p>
<p>11 </p>
<p>28 </p>
<p>39 </p>
<p>V  Portrait Photography  61 </p>
<p>VI  General Photography  67 </p>
<p>VII  Graphic Design </p>
<p>VIII  Film </p>
<p>IX  Fashion </p>
<p>X </p>
<p>Concept Art </p>
<p>XI </p>
<p>Traditional Games </p>
<p>XII </p>
<p>3D </p>
<p>79 </p>
<p>93 </p>
<p>103 </p>
<p>111 </p>
<p>118 </p>
<p>125 </p>
<p>FOREWORD </p>
<p>The history of art is the history of humanity itself. From cave paintings of the 
day’s hunt to AI-generated art that can depict complicated subjects limited 
only by our imaginations, every era in human history is marked by our need to 
make art. </p>
<p>As you’ve undoubtedly noticed, AI Art took a massive leap forward in 2022. 
I believe future history books will look to the innovations in this decade as some 
of  the  most  important  in  human  progress,  rivaling  the  significance  of  the 
Gutenberg printing press. </p>
<p>There’s a learning curve that comes with anything new and using generative 
AI tools is no exception. Except in this case, the curve seems to be flattening as 
the tools advance.    Users of Midjourney newest version, V4, all say that it’s 
much easier to get what you want. This trend will continue in the future. </p>
<p>As  such,  this  isn’t  a  book  to  tell  you  exactly  what  prompts  to  use.  The 
prompts will change as the tools advance. This is a book for inspiring you to 
use these tools in ways that you might not have thought of. All prompts have 
been included to help you experiment faster.</p>
<p>I’ve divided the book by thematic sections, so you can browse the topics that 
you are most interested in. It’s designed so you can flip through quickly until 
something grabs your attention.</p>
<p>It's an exciting time to be alive for artists and art lovers. We’re standing at the 
edge of a massive precipice, and no one knows what’s going to happen next. 
It’s exhilarating, stimulating, and a bit scary.</p>
<p>Really, we’re just here to enjoy the ride. I hope this book can help you turn </p>
<p>your imagination into reality.</p>
<p>— Aitu Ma,<br />
San Diego, December 2022 </p>
<p>The Big Book of Prompts </p>
<p>SECTION I </p>
<p>GAME ART </p>
<p>https://aituts.com </p>
<p>1 </p>
<p>The Big Book of Prompts </p>
<ol>
<li>PIXEL ART </li>
</ol>
<p>Versatile pixel art prompts for delicious pixel backgrounds.</p>
<p>Try switching between 8-bit pixel art / 16-bit pixel art / 32-bit pixel art. This 
really does make a difference on the fidelity of the final image! </p>
<p>16-bit pixel art, outside of café on rainy day, light coming from </p>
<p>windows, cinematic still, hdr </p>
<p>https://aituts.com </p>
<p>2 </p>
<p>The Big Book of Prompts </p>
<p>16-bit pixel art, island in the clouds, by studio ghibli, cinematic </p>
<p>still, hdr </p>
<p>16-bit pixel art, cute interior of Japanese apartment, soft colors </p>
<p>https://aituts.com </p>
<p>3 </p>
<p>16-bit pixel art, neo tokyo cityscape 2048, cinematic still </p>
<p>32-bit pixel art, neo tokyo street in the rain, neon signs, gritty, 2048 </p>
<p>https://aituts.com</p>
<p>4 </p>
<p>The Big Book of Prompts </p>
<ol start="2">
<li>MOBILE GAME CHARACTERS </li>
</ol>
<p>The Clash of Clans style has become a mainstay of mobile games in the past 
few  years.  Generate  Clash  style  characters,  perfect  for  adding  a  touch  of 
personality to your game world. </p>
<p>Tyrion Lannister, isometric, full body, blender 3d, style of artstation </p>
<p>and behance, Disney Pixar, cute </p>
<p>https://aituts.com </p>
<p>5 </p>
<p>The Big Book of Prompts </p>
<p>Daenerys Targaryen, isometric, full body, blender 3d, style of artstation </p>
<p>and behance, Disney Pixar, Mobile game character, clash royale, cute </p>
<p>Sansa Stark, isometric, full body, game character, Clash Royale, blender </p>
<p>3d, style of artstation and behance, Vector art </p>
<p>https://aituts.com </p>
<p>6 </p>
<p>Lord Commander Jon Snow, full 
body, blender 3d, artstation and 
behance, Disney Pixar, Mobile 
game character, isometric, clash 
royale, cute </p>
<p>Jamie Lannister, full body, 
blender 3d, artstation and 
behance, Disney Pixar, Mobile 
game character, isometric, clash 
royale, cute </p>
<p>Bran Stark, full body, sitting, 
game, blender 3d, style of 
behance. Style of Clash Royale, 
isometric </p>
<p>White direwolf game of thrones, 
blender 3d, style of artstation 
and behance. Style of Clash of 
Clans and Disney Pixar, 
isometric Vector art </p>
<p>https://aituts.com</p>
<p>7 </p>
<p>The Big Book of Prompts </p>
<ol start="3">
<li>GAME ITEM COLLECTIONS </li>
</ol>
<p>Assemble  collections  of  multiple  items  for  use  in  3D  games,  with  a  mobile 
gaming aesthetic. </p>
<p>game sheet of different types of swords and axes, light background, clay </p>
<p>render, oily, shiny, bevel, blender, style of Hearthstone </p>
<p>https://aituts.com </p>
<p>8 </p>
<p>The Big Book of Prompts </p>
<p>sheet of shiny treasure chests 
with gold coins, clay, render, 
game icons, game asset, blender, 
oily, shiny, bevel, smooth 
rendering, hearthstone style </p>
<p>game sheet of different types of 
enchanted potions, light 
background, clay, oily, shiny, 
game icon, blender, style of 
Hearthstone </p>
<p>sheet of cafe pastries, game 
asset, game icon, clay render, 
blender, oily, shiny, bevel, 
smooth rendering, style of 
Hearthstone </p>
<p>sheet of different types of 
swords and axes, clay render, 
blender, oily, shiny, bevel, 
smooth rendering, style of 
Hearthstone </p>
<p>https://aituts.com </p>
<p>9 </p>
<p>The Big Book of Prompts </p>
<p>game sheet of paladin armor, light background, clay render, oily, shiny, </p>
<p>bevel, blender, style of Hearthstone </p>
<p>game sheet of wizard robes, light background, clay render, oily, shiny, </p>
<p>bevel, blender, style of Hearthstone </p>
<p>https://aituts.com </p>
<p>10 </p>
<p>The Big Book of Prompts </p>
<p>SECTION II </p>
<p>ILLUSTRATION </p>
<p>https://aituts.com </p>
<p>11 </p>
<p>4. DREAMY WATERCOLOR SCENES </p>
<p>Transform  any  subject  into  beautiful,  dreamy  watercolor  paintings,  that  will 
transport you a simpler, peaceful world. A very versatile prompt that I loved to 
experiment with! </p>
<p>light watercolor, children playing at the beach, bright, white </p>
<p>background, few details, dreamy Studio Ghibli </p>
<p>https://aituts.com</p>
<p>12 </p>
<p>The Big Book of Prompts </p>
<p>light watercolor, children playing by the riverbank, white background, </p>
<p>few details, dreamy, Studio Ghibli </p>
<p>light watercolor, inside of a church with beautiful stained glass </p>
<p>windows, bright, white background, few details, dreamy, Studio Ghibli </p>
<p>https://aituts.com </p>
<p>13 </p>
<p>The Big Book of Prompts </p>
<p>light watercolor, boats at the 
harbor, white background, few 
details, dreamy, Studio Ghibli </p>
<p>light watercolor, Bamboo forest, 
bright, side view,simple white 
background,Small river, few 
details, Ghibli, fairy tale </p>
<p>light watercolor, cat and a dog 
sleeping, bright, white 
background, few details, dreamy, 
Studio Ghibli </p>
<p>light watercolor, outside of a 
coffeeshop, bright, white 
background, few details, dreamy, 
Studio Ghibli </p>
<p>https://aituts.com </p>
<p>14 </p>
<p>The Big Book of Prompts </p>
<p>light watercolor, children playing at the beach, bright, white </p>
<p>background, few details, dreamy, Studio Ghibli </p>
<p>light watercolor, glass fishbowl containing tropical fish, white </p>
<p>background, few details, dreamy, Studio Ghibli </p>
<p>https://aituts.com </p>
<p>15 </p>
<p>The Big Book of Prompts </p>
<ol start="5">
<li>SIMPLE TECH BRAND 
ILLUSTRATIONS </li>
</ol>
<p>Create sleek and modern tech brand illustrations with a minimal aesthetic. </p>
<p>simple minimal tech illustration, man jogging by the waterfront, by slack </p>
<p>and dropbox, style of behance </p>
<p>https://aituts.com </p>
<p>16 </p>
<p>The Big Book of Prompts </p>
<p>tech illustration, woman at desk 
surrounded by succulents, simple 
minimal, by slack and dropbox, 
style of behance </p>
<p>simple minimal tech 
illustration, man getting into a 
car, by slack and dropbox, style 
of behance </p>
<p>tech illustration, woman leaving 
her home, simple minimal, by 
slack and dropbox, style of 
behance </p>
<p>tech illustration, boy running 
with dog, simple minimal, by 
slack and dropbox, style of 
behance </p>
<p>https://aituts.com </p>
<p>17 </p>
<p>The Big Book of Prompts </p>
<ol start="6">
<li>DETAILED TECH BRAND 
ILLUSTRATIONS </li>
</ol>
<p>Generate modern and appealing illustrations for tech brands, with a simple and 
vector-based aesthetic. </p>
<p>generic productivity illustration for a tech company, by slack and </p>
<p>behance </p>
<p>https://aituts.com </p>
<p>18 </p>
<p>The Big Book of Prompts </p>
<p>user being inspired by the possibilities of an app, illustration for a </p>
<p>tech company, by slack and dropbox, style of behance </p>
<p>user being inspired by the possibilities of an app, illustration for a </p>
<p>tech company, by slack and dropbox, style of behance </p>
<p>https://aituts.com </p>
<p>19 </p>
<p>The Big Book of Prompts </p>
<p>users at a coffeeshop, 
illustration for a tech company, 
by slack and dropbox, style of 
behance </p>
<p>Falling in love with your 
company culture, illustration 
for a tech company, by slack and 
dropbox, style of behance </p>
<p>generic productivity 
illustration for a tech company, 
by slack and behance </p>
<p>productivity illustration for a 
tech company, by slack and 
dropbox, style of behance </p>
<p>https://aituts.com </p>
<p>20 </p>
<p>7.  19-CENTRY BOTANICAL 
ILLUSTRATIONS </p>
<p>Create  detailed,  beautiful  19th-century  botanical  illustrations  in  the  style  of 
Pierre-Joseph Redoute and other Titans of botanical illustration. </p>
<p>pumpkin varieties, botanical illustration, white background, style of </p>
<p>Pierre-Joseph Redoute </p>
<p>https://aituts.com</p>
<p>21 </p>
<p>The Big Book of Prompts </p>
<p>botanical illustration, white background, types of tulips </p>
<p>different types of mushrooms, botanical illustration, white background, </p>
<p>style of Pierre-Joseph Redoute </p>
<p>https://aituts.com </p>
<p>22 </p>
<p>The Big Book of Prompts </p>
<p>peach tree branch, botanical 
illustration, white background, 
style of Margaret Mee </p>
<p>cherry varieties, botanical 
illustration, white background, 
style of Pierre-Joseph Redoute </p>
<p>pinecone varieties, botanical 
illustration, white background, 
style of Margaret Mee </p>
<p>The Butterflies of Europe, 
botanical illustration, white 
background, style of Pierre-
Joseph Redoute </p>
<p>https://aituts.com </p>
<p>23 </p>
<p>The Big Book of Prompts </p>
<ol start="8">
<li>FLORAL TATTOO DESIGNS </li>
</ol>
<p>Create beautiful and artistic floral tattoos that look great straight out of the box. </p>
<p>minimalist rose tattoo design, lines, minimal, black and white, white </p>
<p>background </p>
<p>https://aituts.com </p>
<p>24 </p>
<p>The Big Book of Prompts </p>
<p>rose tattoo design, lines, minimal, black and white, white background </p>
<p>minimalist lily tattoo design, lines, minimal, black and white, white </p>
<p>background </p>
<p>https://aituts.com </p>
<p>25 </p>
<p>The Big Book of Prompts </p>
<ol start="9">
<li>MINIMAL LINE &amp; SHAPE TATTOO 
DESIGNS </li>
</ol>
<p>Create  unique  and  minimalist  tattoos  using  lines  and  shapes  in  abstract 
compositions. </p>
<p>minimal tattoo, symmetrical, line, dots, square, triangle, circle, black </p>
<p>and white, white background </p>
<p>https://aituts.com </p>
<p>26 </p>
<p>The Big Book of Prompts </p>
<p>minimal tattoo, symmetrical, line, dots, square, triangle, circle, black </p>
<p>and white, white background </p>
<p>minimal tattoo, symmetrical, line, dots, square, triangle, circle, black </p>
<p>https://aituts.com </p>
<p>27 </p>
<p>and white, white background </p>
<p>The Big Book of Prompts </p>
<p>SECTION III </p>
<p>ARCHITECTURE </p>
<p>https://aituts.com </p>
<p>28 </p>
<p>The Big Book of Prompts </p>
<ol start="10">
<li>MOUNTAIN EMBEDDED 
ARCHITECTURE </li>
</ol>
<p>Create stunning images of structures built and integrated into mountains and 
cliffs, by award winning architects. The ideal residence for villains. </p>
<p>resort embed into a cliff designed by Kengo Kuma, architectural </p>
<p>photography, style of archillect, futurism, modernist architecture </p>
<p>https://aituts.com </p>
<p>29 </p>
<p>The Big Book of Prompts </p>
<p>modern resort, staggered and 
embedded into the side of a 
mountain, architectural 
photography, style of 
archillect, futurism, modernist 
architecture </p>
<p>hotel designed by Ando Tadao, 
organic, embed into cliffside, 
architectural photography, style 
of archillect, futurism, 
modernist architecture </p>
<p>modern villa embedded into the 
side of a cliff, architectural 
photography, style of 
archillect, futurism </p>
<p>resort embed into a cliff 
designed by Kengo Kuma, 
architectural photography, style 
of archillect, futurism, 
modernist architecture </p>
<p>https://aituts.com </p>
<p>30 </p>
<p>11.  MODERNIST ORGANIC 
ARCHITECTURE </p>
<p>Generate architectural designs by award winning architects, for structures that 
are organically embedded into the terrain. </p>
<p>organic house embedded into the hilly terrain designed by Kengo Kuma, </p>
<p>architectural photography, style of archillect, futurism, modernist </p>
<p>architecture </p>
<p>https://aituts.com</p>
<p>31 </p>
<p>The Big Book of Prompts </p>
<p>organic house embedded into the hilly terrain designed by Kengo Kuma, </p>
<p>architectural photography, style of archillect, futurism, modernist </p>
<p>architecture </p>
<p>organic resort embed into a series of hills, designed by Ando Tadao, </p>
<p>architectural photography, style of archillect, futurism, modernist </p>
<p>architecture3d, style of artstation and behance, Vector art </p>
<p>https://aituts.com </p>
<p>32 </p>
<p>The Big Book of Prompts </p>
<p>organic house embedded into a grassy hill, designed by Kazuyo Sejima and </p>
<p>Ryue Nishizawa, architectural photography, style of archillect, futurism, </p>
<p>modernist architecture </p>
<p>museum designed by Ando Tadao, organic, embed into side of a hill, </p>
<p>architectural photography, style of archillect, futurism, modernist </p>
<p>https://aituts.com </p>
<p>architecture </p>
<p>33 </p>
<p>The Big Book of Prompts </p>
<ol start="12">
<li></li>
</ol>
<p>INTERIOR DESIGN RENDERINGS </p>
<p>Get inspiration for modern interior design with this highly versatile prompt. </p>
<p>Interior Design, a perspective of a study, modernist, large windows with </p>
<p>natural light, Light colors, plants, modern furniture, modern interior </p>
<p>design </p>
<p>https://aituts.com </p>
<p>34 </p>
<p>The Big Book of Prompts </p>
<p>Interior Design, a perspective of a study, modernist, large windows with </p>
<p>natural light, Light colors, plants, modern furniture, modern interior </p>
<p>design </p>
<p>Interior Design, a perspective of a living room and a kitchen with an </p>
<p>island, large windows with natural light, Light colors, vegetation, </p>
<p>modern furniture, skylight, modern minimalistic design </p>
<p>https://aituts.com </p>
<p>35 </p>
<p>The Big Book of Prompts </p>
<p>Interior Design, a perspective of a living room and a kitchen with an </p>
<p>island, large windows with natural light, Light colors, vegetation, </p>
<p>modern furniture, skylight, modern minimalistic design </p>
<p>Interior Design, a perspective of a creative loft, large windows with </p>
<p>natural light, industrial ceiling, modern furniture, skylight </p>
<p>https://aituts.com </p>
<p>36 </p>
<p>The Big Book of Prompts </p>
<ol start="13">
<li>CUTE TINY ISOMETRIC 
ARCHITECTURE </li>
</ol>
<p>Generate tiny and intricate isometric architecture by award-winning architects. </p>
<p>isometric clean art of a museum designed by kengo kuma, blender </p>
<p>https://aituts.com </p>
<p>37 </p>
<p>The Big Book of Prompts </p>
<p>isometric clean art of exterior 
of condo designed by kengo kuma, 
blender </p>
<p>isometric clean art of exterior 
of small apartment designed by 
kengo kuma, tiny model </p>
<p>isometric clean art of exterior 
of condo designed by kengo kuma, 
blender </p>
<p>isometric clean art of outside 
of airport designed by eero 
saarinen, blender </p>
<p>https://aituts.com </p>
<p>38 </p>
<p>The Big Book of Prompts </p>
<p>SECTION IV </p>
<p>ARTS &amp; CRAFTS </p>
<p>https://aituts.com </p>
<p>39 </p>
<p>14.  LAYERED PAPER ART </p>
<p>Create layered paper art with a ton of dimensionality and personality. </p>
<p>You can also add beautiful lighting effects! </p>
<p>Hogwarts, castle on a hill, from Harry Potter, layered paper art, </p>
<p>diorama, shadowbox, volumetric lighting, product photography </p>
<p>https://aituts.com</p>
<p>40 </p>
<p>The Big Book of Prompts </p>
<p>Harry potter on broomstick in sky, layered paper craft, paper art, </p>
<p>diorama </p>
<p>hagrid on motorcycle in the air, layered paper craft, diorama, paper art, </p>
<p>https://aituts.com </p>
<p>hdr </p>
<p>41 </p>
<p>The Big Book of Prompts </p>
<p>paper material, dumbledore in 
his study, layered paper craft, 
diorama </p>
<p>ron weasley wearing christmas 
sweater, layered paper craft, 
diorama </p>
<p>hermione granger with books, 
layered paper craft, diorama </p>
<p>Diagon alley from Harry Potter, 
layered paper craft, paper art, 
diorama, underground </p>
<p>https://aituts.com </p>
<p>42 </p>
<p>The Big Book of Prompts </p>
<p>royal crest of gryffindor lion, 
layered paper craft, diorama, 
paper art </p>
<p>royal crest of ravenclaw eagle, 
layered paper craft, diorama, 
paper art </p>
<p>crest of slytherin, symmetrical 
serpent, layered paper craft, 
diorama, paper art </p>
<p>crest of hufflepuff, symmetrical 
badger head, layered paper 
craft, diorama, paper art </p>
<p>https://aituts.com </p>
<p>43 </p>
<p>The Big Book of Prompts </p>
<ol start="15">
<li>VINTAGE US POSTAL STAMPS </li>
</ol>
<p>Generate a distinct vintage United States postage stamp featuring any subject 
of your choice. </p>
<p>vintage United States Postage Stamp, 2 cent stamp, Obi Wan Kenobi, blue </p>
<p>ink, line engraving, intaglio </p>
<p>https://aituts.com </p>
<p>44 </p>
<p>The Big Book of Prompts </p>
<p>vintage United States Postage 
Stamp, 2 cent stamp, Dark Vader, 
red ink, line engraving, 
intaglio </p>
<p>vintage United States Postage 
Stamp, 2 cent stamp, Princess 
Leia, brown ink, line engraving, 
intaglio </p>
<p>vintage United States Postage 
Stamp, 3 cent stamp, Master Yoda 
using the force, green ink, line 
engraving, intaglio </p>
<p>vintage United States Postage 
Stamp, 3 cent stamp, 
Stormtrooper full body, purple 
ink, line engraving, intaglio </p>
<p>https://aituts.com </p>
<p>45 </p>
<p>The Big Book of Prompts </p>
<p>vintage 2 cent postage stamp of 
Abe Lincoln, red ink, line 
engraving, intaglio </p>
<p>vintage 2 cent postage stamp of 
Captain Jack Sparrow, blue ink, 
line engraving, intaglio </p>
<p>clean vintage 5 cent postage 
stamp of Usain Bolt doing a 
victory pose, green ink, line 
engraving, intaglio </p>
<p>vintage 2 cent postage stamp of 
Louis and Clark exploring, 
orange ink, line engraving, 
intaglio </p>
<p>https://aituts.com </p>
<p>46 </p>
<p>The Big Book of Prompts </p>
<ol start="16">
<li></li>
</ol>
<p>INFINITE HOLIDAY PATTERNS </p>
<p>Add  some  festive  cheer  to  your  designs  with  this  simple  repeating  pattern 
prompt!</p>
<p>The sides match up, so you can make wrapping paper and other print designs 
up to any dimensions. </p>
<p>simple seamless doodle halloween themed pattern </p>
<p>https://aituts.com </p>
<p>47 </p>
<p>The Big Book of Prompts </p>
<p>simple seamless doodle easter 
themed pattern </p>
<p>simple seamless doodle christmas 
themed pattern </p>
<p>simple seamless doodle halloween 
themed pattern </p>
<p>simple seamless valentines day 
themed pattern </p>
<p>https://aituts.com </p>
<p>48 </p>
<p>The Big Book of Prompts </p>
<ol start="17">
<li>T-SHIRT GRAPHICS </li>
</ol>
<p>Create  bold  Neon/Synthwave  T-shirt  graphics  that  are  perfect  for  print-on-
demand services. </p>
<p>16-bit pixel art, outside of café on rainy day, by studio ghibli, </p>
<p>cinematic still </p>
<p>https://aituts.com </p>
<p>49 </p>
<p>The Big Book of Prompts </p>
<p>tshirt vector, darth vader 
synthwave, vivid colors, 
detailed </p>
<p>tshirt vector, malibu miami vice 
monkey, vivid colors, detailed </p>
<p>tshirt vector, 80s retrowave 
dinosaur, vivid, pink and blue 
lighting, 90s </p>
<p>tshirt vector, terminator 
graphic, synthwave, vivid 
colors, detailed </p>
<p>https://aituts.com </p>
<p>50 </p>
<p>The Big Book of Prompts </p>
<ol start="18">
<li>SUPER CUTE ANIMAL STICKERS </li>
</ol>
<p>Create adorable animal stickers that are perfect for producing and selling on 
Etsy and other online marketplaces. </p>
<p>sticker design, super cute baby pixar style white rabbit, wearing a cyan </p>
<p>sweater, vector </p>
<p>https://aituts.com </p>
<p>51 </p>
<p>The Big Book of Prompts </p>
<p>sticker, A super cute baby pixar 
style fox, vector </p>
<p>sticker, A super cute baby pixar 
style elephant, vector </p>
<p>sticker, A super cute baby pixar 
style dolphin, vector </p>
<p>sticker design, super cute baby 
pixar style otter, wearing a 
beanie, vector </p>
<p>https://aituts.com </p>
<p>52 </p>
<p>The Big Book of Prompts </p>
<ol start="19">
<li>PLANNING &amp; ORGANIZATION 
STICKERS </li>
</ol>
<p>Design  playful  organization  stickers  that  help  people  plan  their  busy  lives. 
Perfect for selling on Etsy and similar sites. </p>
<p>sticker design, lined paper for writing, cute, vector </p>
<p>https://aituts.com </p>
<p>53 </p>
<p>The Big Book of Prompts </p>
<p>sticker design, lined paper for 
writing, cute, vector</p>
<p>sticker design, a sheet of lined 
paper, cute, vector </p>
<p>sticker design, lined paper for 
writing cute, vector </p>
<p>sticker design, lined paper for 
writing, cute, vector </p>
<p>https://aituts.com </p>
<p>54 </p>
<p>The Big Book of Prompts </p>
<ol start="20">
<li>GRAYSCALE COLORING PAGES </li>
</ol>
<p>Generate clean and crisp grayscale coloring book pages that can be printed for 
use at home, or turned into coloring book products to sell. </p>
<p>clean coloring book page of a lion, black and white </p>
<p>https://aituts.com </p>
<p>55 </p>
<p>The Big Book of Prompts </p>
<p>clean coloring book page of a boot 
with a flower growing out of it, 
black and white </p>
<p>clean coloring book page of a 
flying whale </p>
<p>clean coloring book page of a 
Canada Goose, black and white </p>
<p>Clean coloring book illustration of 
a Nothern Cardinal, black and white </p>
<p>https://aituts.com </p>
<p>56 </p>
<p>The Big Book of Prompts </p>
<ol start="21">
<li>SIGNET RINGS </li>
</ol>
<p>Create mockups of cool and unique ring prototypes with this simple prompt. </p>
<p>Silver Direwolf signet ring </p>
<p>https://aituts.com </p>
<p>57 </p>
<p>The Big Book of Prompts </p>
<p>royal family crest, signet ring </p>
<p>royal lion, signet ring </p>
<p>https://aituts.com </p>
<p>58 </p>
<p>The Big Book of Prompts </p>
<ol start="22">
<li>CULTURAL PATTERNS </li>
</ol>
<p>Create repeating patterns inspired by different cultures. </p>
<p>repeating japanese pattern </p>
<p>https://aituts.com </p>
<p>59 </p>
<p>The Big Book of Prompts </p>
<p>repeating Chinese pattern </p>
<p>repeating Nordic pattern </p>
<p>https://aituts.com </p>
<p>60 </p>
<p>The Big Book of Prompts </p>
<p>SECTION V </p>
<p>PORTRAIT PHOTOGRAPHY </p>
<p>https://aituts.com </p>
<p>61 </p>
<p>The Big Book of Prompts </p>
<ol start="23">
<li>INTIMATE BLACK &amp; WHITE 
PORTRAITS </li>
</ol>
<p>Award-winning, intimate and expressive black and white portrait photography. </p>
<p>Jean-Michel Basquiat, half body, black and white portrait photography, </p>
<p>depth of field, f2.8, 50mm lens, exquisite detail, hdr, deep shadows, </p>
<p>award-winning photography, high-sharpnes </p>
<p>https://aituts.com </p>
<p>62 </p>
<p>The Big Book of Prompts </p>
<p>black and white portrait photography, Amy Winehouse, depth of field, </p>
<p>f2.8, 50mm lens, exquisite detail, hdr, deep shadows, award-winning </p>
<p>photography, high-sharpness </p>
<p>Kurt Cobain, portrait photography, 3 quarter lighting, depth of field, </p>
<p>f2.8, 50mm lens, exquisite detail, intricately-detailed, award-winning </p>
<p>photography, high-contrast, High-sharpness </p>
<p>https://aituts.com </p>
<p>63 </p>
<p>The Big Book of Prompts </p>
<p>brad pitt, black and white portrait photography, 3 quarter lighting, </p>
<p>depth of field, f2.8, 50mm lens, exquisite detail, intricately-detailed, </p>
<p>award-winning photography, high-contrast, High-sharpness </p>
<p>tom hanks, black and white portrait photography, 3 quarter lighting, </p>
<p>depth of field, f2.8, 50mm lens, exquisite detail, intricately-detailed, </p>
<p>award-winning photography, high-contrast, High-sharpness</p>
<p>https://aituts.com </p>
<p>64 </p>
<p>24.  DETAILED CLOSE-UP PORTRAITS </p>
<p>Close  up  portraits  of  people  with  striking  depth-of-field  effects  (a  sharp 
foreground and blurry background). </p>
<p>close up portrait of an white haired old lady, Exquisite detail, 30-</p>
<p>megapixel, 4k, 85-mm-lens, sharp-focus, f:8, ISO 100, shutter-speed </p>
<p>1:125, diffuse-back-lighting, award-winning photograph, small-catchlight, </p>
<p>High-sharpness, facial-symmetry </p>
<p>https://aituts.com</p>
<p>65 </p>
<p>The Big Book of Prompts </p>
<p>close up portrait of a generic female, Exquisite detail, 30-megapixel, 4k, 85-
mm-lens, sharp-focus, f:8, ISO 100, shutter-speed 1:125, diffuse-back-lighting, 
award-winning photograph, small-catchlight, High-sharpness, facial-symmetry </p>
<p>close up portrait of an white haired old man, Exquisite detail, 30-megapixel, 
4k, 85-mm-lens, sharp-focus, f:8, ISO 100, shutter-speed 1:125, diffuse-back-
lighting, award-winning photograph, small-catchlight, High-sharpness, facial-
symmetry </p>
<p>https://aituts.com </p>
<p>66 </p>
<p>The Big Book of Prompts </p>
<ol start="25">
<li>VINTAGE POLAROID PHOTOS </li>
</ol>
<p>For some reason – vintage polaroids always come out looking great! </p>
<p>blonde girl leaning on table and smiling at camera, detailed facial </p>
<p>features, detailed eyes, polaroid, 1991 </p>
<p>https://aituts.com </p>
<p>67 </p>
<p>The Big Book of Prompts </p>
<p>Brother and sister playing in the yard, early 2000s, flash photography, polaroid </p>
<p>woman playing solitaire at a desk,fuji color film, polaroid, 1999 </p>
<p>https://aituts.com </p>
<p>68 </p>
<p>The Big Book of Prompts </p>
<p>SECTION VI </p>
<p>PHOTOGRAPHY </p>
<p>https://aituts.com </p>
<p>69 </p>
<p>The Big Book of Prompts </p>
<ol start="26">
<li>BEAUTIFUL, REALISTIC 
LANDSCAPE PHOTOGRAPHY </li>
</ol>
<p>Create stunning, award-winning landscape photos. From sweeping mountain 
vistas to tranquil forest scenes, this prompt will help you capture the beauty of 
the natural world. </p>
<p>The Grand Canyon at sunrise, Canon RF 16mm f:2.8 STM Lens, hyper </p>
<p>realistic photography, style of unsplash and National Geographic </p>
<p>https://aituts.com </p>
<p>70 </p>
<p>The Big Book of Prompts </p>
<p>Sea of Endless waves with the sparkling reflection of the sun, golden </p>
<p>hour, Canon RF 16mm f:2.8 STM Lens, hyperrealistic photography, style of </p>
<p>unsplash and National Geographic </p>
<p>hills of the scotland highlands, misty fog, Canon RF 16mm f:2.8 STM Lens, </p>
<p>award winning photography, by national geographic and upsplash </p>
<p>https://aituts.com </p>
<p>71 </p>
<p>The Big Book of Prompts </p>
<p>beach during the golden hour, Canon RF 16mm f:2.8 STM Lens, </p>
<p>hyperrealistic photography, style of unsplash and National Geographic </p>
<p>Arctic beautiful landscape, Canon RF 16mm f:2.8 STM Lens, hyperrealistic </p>
<p>photography style of unsplash and National Geographic </p>
<p>https://aituts.com </p>
<p>72 </p>
<p>The Big Book of Prompts </p>
<p>Cherry Blossoms in Hokkaido in the wintertime, Canon RF 16mm f:2.8 STM </p>
<p>Lens, hyperrealistic photography, style of unsplash and National </p>
<p>Geographic </p>
<p>view of the northern lights at night time, seen in Alaska, Canon RF 16mm </p>
<p>f:2.8 STM Lens, hyperrealistic photography, style of unsplash and </p>
<p>https://aituts.com </p>
<p>73 </p>
<p>National Geographic</p>
<p>27.  HYPER-REALISTIC WATER 
EFFECTS </p>
<p>Capture the beauty and power of water with this realistic water prompt with 
radiant sunlight effects. </p>
<p>ball of water suspended in the air, ripples and splash on surface, </p>
<p>sunlight gleaming, with sparkling crisp radiant reflections, Canon 35mm </p>
<p>lens, hyperrealistic photography, style of unsplash </p>
<p>https://aituts.com</p>
<p>74 </p>
<p>The Big Book of Prompts </p>
<p>ball of water suspended in the air, 
ripples and splash on surface, with 
sparkling, crisp radiant 
reflections, sunlight gleaming, 
Canon 35mm lens, hyperrealistic 
photography, style of unsplash </p>
<p>ball of water suspended in the air, 
ripples and splash on surface, with 
sparkling, crisp radiant 
reflections, sunlight gleaming, 
Canon 35mm lens, hyperrealistic 
photography, style of unsplash </p>
<p>water splash, with sparkling, 
crisp radiant reflections, 
sunlight gleaming, Canon 35mm 
lens, hyperrealistic 
photography, style of unsplash </p>
<p>water with sparkling, crisp 
radiant reflections, sunlight 
gleaming, Canon 35mm lens 
hyperrealistic photography, 
style of unsplash </p>
<p>https://aituts.com </p>
<p>75 </p>
<p>The Big Book of Prompts </p>
<ol start="28">
<li>CUTE MINI-SUCCULENTS &amp; 
PLANTS </li>
</ol>
<p>Generate  tiny  succulents  and  plants  with  a  close-up  miniature  photography 
style. </p>
<p>2 fingers holding cute mini succulent in a pot, light background, depth </p>
<p>of field f2.8, 50mm lens, tilt shift photography </p>
<p>https://aituts.com </p>
<p>76 </p>
<p>The Big Book of Prompts </p>
<p>cute mini Haworthia cymbiformis 
plant in a pot, light pastel 
background, depth of field f2.8 
3.5, 50mm lens </p>
<p>cute mini Aloe plant in a pot, 
white background, depth of field 
f2.8 3.5, 50mm lens </p>
<p>cute mini Aloe plant in a pot, 
white background, depth of field 
f2.8 3.5, 50mm lens </p>
<p>cute mini Kalanchoe plant in a pot, 
pastel background, depth of field 
f2.8 3.5, 50mm lens </p>
<p>https://aituts.com </p>
<p>77 </p>
<p>The Big Book of Prompts </p>
<p>cute mini African Violet plant in a 
pot, white background, depth of 
field f2.8 3.5, 50mm lens </p>
<p>cute mini Pilea plant in a pot, 
white background, f2.8 3.5, 50mm 
lens, tilt shift photography </p>
<p>cute mini Peperomia Watermelon in a 
pot, white background, depth of 
field f2.8 3.5, 50mm lens </p>
<p>cute mini Lucky Bamboo plant in a 
pot, white background, depth of 
field f2.8 3.5, 50mm lens </p>
<p>https://aituts.com </p>
<p>78 </p>
<p>The Big Book of Prompts </p>
<ol start="29">
<li>POLISHED WOODEN OBJECTS </li>
</ol>
<p>Generate beautiful, realistic polished wood sculptures. Change the wood type 
and see how the texture differs! </p>
<p>starwars helmet made of polished walnut burl and blackwood, dynamic </p>
<p>contrast, depth mapped </p>
<p>https://aituts.com </p>
<p>79 </p>
<p>The Big Book of Prompts </p>
<p>Buddha made of lacquered polished walnut burl and Mahogany, dynamic </p>
<p>contrast, depth mapped </p>
<p>Warrior Guan Yu made of lacquered polished walnut burl and Mahogany, </p>
<p>https://aituts.com </p>
<p>80 </p>
<p>dynamic contrast, depth mapped </p>
<p>The Big Book of Prompts </p>
<p>SECTION VII </p>
<p>GRAPHIC DESIGN </p>
<p>https://aituts.com </p>
<p>81 </p>
<p>The Big Book of Prompts </p>
<ol start="30">
<li>MINIMAL JAPANESE STYLE LOGO </li>
</ol>
<p>Minimalist  Japanese/Asian  style  logos  that  are  perfect  for  any  business  or 
brand. </p>
<p>logo of a shark, minimal, style of japanese book cover </p>
<p>https://aituts.com </p>
<p>82 </p>
<p>The Big Book of Prompts </p>
<p>logo of taiwanese brand cup of tea, 
minimal, style of japanese book 
cover </p>
<p>logo of cup of coffee latte, 
minimal, style of japanese book 
cover </p>
<p>logo of a rooster, minimal, style 
of japanese illustration </p>
<p>logo of Direwolf, house stark, 
simple, style of Japanese book 
cover </p>
<p>https://aituts.com </p>
<p>83 </p>
<p>The Big Book of Prompts </p>
<ol start="31">
<li></li>
</ol>
<p>IMDB MOVIE POSTERS </p>
<p>Create a movie poster for any movie with this extraordinarily simple prompt. </p>
<p>movie poster for the empire strikes back on hoth </p>
<p>https://aituts.com </p>
<p>84 </p>
<p>The Big Book of Prompts </p>
<p>movie poster for fight club </p>
<p>movie poster for the godfather </p>
<p>movie poster for batman the dark 
knight </p>
<p>movie poster for city of god </p>
<p>https://aituts.com </p>
<p>85 </p>
<p>The Big Book of Prompts </p>
<p>movie poster for interstellar </p>
<p>movie poster for the matrix </p>
<p>movie poster for forrest gump </p>
<p>movie poster for pulp fiction </p>
<p>https://aituts.com </p>
<p>86 </p>
<p>32.  MINIMAL GRAPHIC 
STORYTELLING POSTERS </p>
<p>Generate simple and effective graphic design movie posters that tell a bigger 
story with just a few elements. </p>
<p>movie poster for the godfather, by Saul Bass </p>
<p>https://aituts.com</p>
<p>87 </p>
<p>The Big Book of Prompts </p>
<p>movie poster for Gone Girl, by Saul Bass </p>
<p>movie poster for Apocalypse Now, by Saul Bass </p>
<p>https://aituts.com </p>
<p>88 </p>
<p>The Big Book of Prompts </p>
<ol start="33">
<li>IOS APP ICONS </li>
</ol>
<p>Make simple and effective vector icons for your next iOS app </p>
<p>squared with round edges mobile app logo design, flat vector app icon of </p>
<p>a cute shiba inu face, minimalistic, white background </p>
<p>https://aituts.com </p>
<p>89 </p>
<p>The Big Book of Prompts </p>
<p>squared with round edges mobile 
app logo design, flat vector app 
icon of an open box, 
minimalistic, white background </p>
<p>squared with round edges mobile 
app logo design, flat vector app 
icon of a skull, minimalistic, 
white background </p>
<p>squared with round edges mobile 
app logo design, flat vector app 
icon of a rocket, minimalistic, 
white background </p>
<p>squared with round edges mobile 
app logo design, flat vector app 
icon of a cute onigiri, 
minimalistic, white background </p>
<p>https://aituts.com </p>
<p>90 </p>
<p>The Big Book of Prompts </p>
<ol start="34">
<li>APP MOCKUPS WITH DEVICE </li>
</ol>
<p>Get mockups of any app you can imagine on a phone. </p>
<p>Showcase your wildest app design ideas! </p>
<p>photography of an iphone [with a modern user interface plant </p>
<p>identification app on the screen] inspired by Behance and Figma and </p>
<p>dribbble </p>
<p>https://aituts.com </p>
<p>91 </p>
<p>The Big Book of Prompts </p>
<p>photography of an iphone [with a 
modern user interface plant 
identification app on the 
screen] inspired by Behance and 
Figma and dribbble </p>
<p>photography of an iphone [with a 
modern user interface plant 
identification app on the 
screen] inspired by Behance and 
Figma and dribbble </p>
<p>photography of an iphone [with a 
modern user interface home 
buying app on the display] 
inspired by Behance and Figma 
and dribbble </p>
<p>photography of an iphone [with a 
modern user interface plant 
identification app on the 
screen] inspired by Behance and 
Figma and dribbble </p>
<p>https://aituts.com </p>
<p>92 </p>
<p>The Big Book of Prompts </p>
<ol start="35">
<li>WEB DESIGN MOCKUPS WITH 
DEVICE </li>
</ol>
<p>Web design mockups inside Macbooks! </p>
<p>photo of macbook M1 with [ a modern user interface of plant </p>
<p>identification app on the screen] inspired by Behance and Figma and </p>
<p>dribbble </p>
<p>https://aituts.com </p>
<p>93 </p>
<p>The Big Book of Prompts </p>
<p>photo of macbook m1 with [modern web user interface of lego website, </p>
<p>style of dribbble and Behance and Figma ] on the screen </p>
<p>photo of macbook m1 with [modern web user interface of nutrition website, </p>
<p>style of dribbble and Behance and Figma ] on the screen </p>
<p>https://aituts.com </p>
<p>94 </p>
<p>The Big Book of Prompts </p>
<p>SECTION VIII </p>
<p>FILM </p>
<p>https://aituts.com </p>
<p>95 </p>
<p>The Big Book of Prompts </p>
<ol start="36">
<li>STUDIO GHIBLI ANIME STILLS </li>
</ol>
<p>Generate retro film stills from old Studio Ghibli anime. </p>
<p>DVD screengrab from studio ghibli movie, beautiful countryside with </p>
<p>poppies in the foreground, clouds on blue sky, directed by Hayao </p>
<p>Miyazaki, retro anime </p>
<p>https://aituts.com </p>
<p>96 </p>
<p>The Big Book of Prompts </p>
<p>DVD screengrab from studio ghibli movie, the founding fathers signing the </p>
<p>declaration of independence, directed by Hayao Miyazaki, retro anime </p>
<p>DVD screengrab from studio ghibli movie, castle in the sky, aerial shot, </p>
<p>flying machines, aesthetic, designed by Hayao Miyazaki, retro anime</p>
<p>https://aituts.com </p>
<p>97 </p>
<p>DVD screengrab from studio ghibli movie, star wars imperial star </p>
<p>destroyer, directedd by Hayao Miyazaki, retro anime </p>
<p>DVD screengrab from studio ghibli movie, stormtroopers marching inside </p>
<p>the hangar of an imperior star destroyer, directed by Hayao Miyazaki, </p>
<p>retro anime </p>
<p>https://aituts.com</p>
<p>98 </p>
<p>The Big Book of Prompts </p>
<ol start="37">
<li>STILLS FROM WES ANDERSON 
FILMS </li>
</ol>
<p>Generate film stills with Wes Anderson’s distinct visual style. </p>
<p>still from film, Daenerys Targaryen and her dragons, directed by Wes </p>
<p>Anderson, quirky costume design </p>
<p>https://aituts.com </p>
<p>99 </p>
<p>The Big Book of Prompts </p>
<p>still from film, Joffrey Baratheon sitting on the throne, from game of </p>
<p>thrones, directed by Wes Anderson, quirky costume design </p>
<p>still from film, ned stark game of thrones, directed by Wes Anderson, </p>
<p>quirky costume design </p>
<p>https://aituts.com </p>
<p>100 </p>
<p>The Big Book of Prompts </p>
<ol start="38">
<li>STILLS FROM BLACK AND WHITE 
FILMS </li>
</ol>
<p>Generate stills from old black and white films. </p>
<p>still from black and white old movie, film grain, live action pokemon, </p>
<p>talking to nurse at pokemon center, directed by Alfred Hitchcock </p>
<p>https://aituts.com </p>
<p>101 </p>
<p>The Big Book of Prompts </p>
<p>still from black and white old movie, film grain, pokemon live action, </p>
<p>directed by Alfred Hitchcock </p>
<p>still from black and white old movie, film grain, pokemon live action, </p>
<p>directed by Alfred Hitchcock </p>
<p>https://aituts.com </p>
<p>102 </p>
<p>The Big Book of Prompts </p>
<ol start="39">
<li>STILLS FROM CLAYMATION FILMS 
BY TIM BURTON </li>
</ol>
<p>Generate stop-motion claymation film stills with Tim Burton’s distinct visual 
style. </p>
<p>DVD screengrab from stop motion movie, claymation, Professor Snape </p>
<p>teaching class, directed by Tim Burton </p>
<p>https://aituts.com </p>
<p>103 </p>
<p>The Big Book of Prompts </p>
<p>DVD screengrab from stop motion movie, claymation, hogwarts castle, </p>
<p>directed by Tim Burton </p>
<p>DVD screengrab from stop motion movie, claymation, harry potter and ron </p>
<p>weasley, directed by Tim Burton </p>
<p>https://aituts.com </p>
<p>104 </p>
<p>The Big Book of Prompts </p>
<p>SECTION IX </p>
<p>FASHION </p>
<p>https://aituts.com </p>
<p>105 </p>
<p>The Big Book of Prompts </p>
<ol start="40">
<li>VINTAGE FASHION COVER 
SHOOTS </li>
</ol>
<p>VOGUE-style fashion photoshoots that evoke a bygone era. </p>
<p>model is dressed in vintage clothing and accessories, with a backdrop </p>
<p>that evokes a bygone era, fashion shoot, style of vogue </p>
<p>https://aituts.com </p>
<p>106 </p>
<p>The Big Book of Prompts </p>
<p>model is dressed in vintage clothing and accessories, with a backdrop </p>
<p>that evokes a bygone era, fashion shoot, style of vogue </p>
<p>model is dressed in vintage clothing and accessories, with a backdrop </p>
<p>that evokes a bygone era, fashion shoot, style of vogue</p>
<p>https://aituts.com </p>
<p>107 </p>
<p>41.  FUTURISTIC SNEAKER CONCEPTS </p>
<p>Design futuristic sneaker concepts inspired by any theme or idea. </p>
<p>futuristic yeezy foam runner, inspired by dark souls </p>
<p>https://aituts.com</p>
<p>108 </p>
<p>The Big Book of Prompts </p>
<p>futuristic sneaker inspired by egyptian mythology, by nike </p>
<p>futuristic sneaker inspired by greek mythology, by nike </p>
<p>https://aituts.com </p>
<p>109 </p>
<p>The Big Book of Prompts </p>
<p>futuristic footwear, inspired by military special forces, by Nike </p>
<p>futuristic footwear, inspired by Starcraft, by Nike </p>
<p>https://aituts.com </p>
<p>110 </p>
<p>The Big Book of Prompts </p>
<ol start="42">
<li>FASHION MOODBOARDS </li>
</ol>
<p>Start new fashion concepts with moodboards that draw on a wide variety of 
influences. </p>
<p>fashion moodboard for bohemian fashion line, in the style of hip hop </p>
<p>https://aituts.com </p>
<p>111 </p>
<p>The Big Book of Prompts </p>
<p>fashion moodboard for techwear 
by nike, inspired by arctic </p>
<p>fashion moodboard for techwear 
by nike, inspired by athletic 
clothing </p>
<p>fashion moodboard for earth tone 
clothes inspired by the outdoors 
and hiking </p>
<p>fashion moodboard for earth tone 
clothes inspired by the outdoors 
and hiking </p>
<p>https://aituts.com </p>
<p>112 </p>
<p>The Big Book of Prompts </p>
<p>SECTION X </p>
<p>CONCEPT ART </p>
<p>https://aituts.com </p>
<p>113 </p>
<p>The Big Book of Prompts </p>
<ol start="43">
<li>CHARACTER CONCEPT SHEET </li>
</ol>
<p>Versatile  pixel  art  prompts  for  delicious  pixel  backgrounds.  Try  switching 
between: 8 bit pixel art / 16 bit pixel art / 32 bit pixel art. This really does makes 
a difference on the fidelity of the final image! </p>
<p>Female wearing cybernetic exoskeleton character design, concept design </p>
<p>sheet, white background, style of yoji shinkawa </p>
<p>https://aituts.com </p>
<p>114 </p>
<p>The Big Book of Prompts </p>
<p>space mercenary character design, concept design sheet, white background, </p>
<p>style of yoji shinkawa </p>
<p>male wearing cybernetic exoskeleton character design, concept design </p>
<p>sheet, white background, style of craig mullins </p>
<p>https://aituts.com </p>
<p>115 </p>
<p>The Big Book of Prompts </p>
<p>james bond, character design, 
concept design sheet, white 
background, style of Yoji 
Shinkawa </p>
<p>flaming skull and suit character 
design, concept design sheet, 
white background </p>
<p>special ops with cyborg arm 
character design, concept design 
sheet, white background, style 
of yoji shinkawa </p>
<p>robot character design, concept 
design sheet, white background </p>
<p>https://aituts.com </p>
<p>116 </p>
<p>The Big Book of Prompts </p>
<ol start="44">
<li>CHARACTER CONCEPT CLOSE-UP </li>
</ol>
<p>Create a close-up concept sheet for both original and existing characters.</p>
<p>mecha pilot female, short hair, close up character design, multiple </p>
<p>concept designs, concept design sheet, white background, style of Yoji </p>
<p>Shinkawa </p>
<p>https://aituts.com </p>
<p>117 </p>
<p>The Big Book of Prompts </p>
<p>lion head man character design, concept design sheet, white background, </p>
<p>style of yoji shinkawa </p>
<p>dwarven warlord, close up character design, multiple concept designs, </p>
<p>concept design sheet, white background, style of Yoshitaka Amano </p>
<p>https://aituts.com </p>
<p>118 </p>
<p>The Big Book of Prompts </p>
<p>female military officer, close 
up character design, multiple 
concept designs, concept design 
sheet, white background, style 
of Hideo Kojima </p>
<p>robot butler, head, close up 
character design, multiple 
concept designs, concept design 
sheet, white background, style 
of Yoji Shinkawa </p>
<p>wizened old female 
fortuneteller, head, close up 
character design, multiple 
concept designs, concept design 
sheet, white background, style 
of Yoshitaka Amano </p>
<p>white hair female close up 
character design, multiple 
concept designs, concept design 
sheet, white background, style 
of Yoshitaka Amano </p>
<p>https://aituts.com </p>
<p>119 </p>
<p>SECTION XI </p>
<p>TRADITIONAL GAMES </p>
<p>https://aituts.com</p>
<p>120 </p>
<p>45.  WARHAMMER STYLE TABLETOP 
MINIATURES </p>
<p>Bring  your  toy  ideas  to  life  with  this  stunning  prompt  that  creates  product 
photos of Warhammer style miniatures. </p>
<p>epic miniature of Holy Paladin knight, hand painted, toy, white </p>
<p>background, studio lighting, product photography </p>
<p>https://aituts.com</p>
<p>121 </p>
<p>The Big Book of Prompts </p>
<p>warhammer miniature of Lich King, hand painted, plastic, detailed, white </p>
<p>background, studio lighting, product photography </p>
<p>warhammer miniature made of Orc, white background, studio lighting, 35 mm </p>
<p>https://aituts.com </p>
<p>lens </p>
<p>122 </p>
<p>The Big Book of Prompts </p>
<p>warhammer miniature of weary 
Battle cyborg, hand painted, 
plastic, detailed, white 
background, studio lighting, 
product photography </p>
<p>warhammer figurine of space 
marine with sword, hand painted, 
plastic, detailed, white 
background, studio lighting, 
product photography </p>
<p>warhammer miniature of Dark 
Magician, hand painted, plastic, 
detailed, white background, 
studio lighting, product 
photography </p>
<p>warhammer miniature of a row of 
ancient chinese knights, hand 
painted, plastic, detailed, 
white background, studio 
lighting, product photography </p>
<p>https://aituts.com </p>
<p>123 </p>
<p>The Big Book of Prompts </p>
<p>warhammer figurine of zerg hydralisk, hand painted plastic, detailed, </p>
<p>white background, studio lighting, product photography </p>
<p>warhammer miniature of a Steakpunk flying ship, hand painted, plastic, </p>
<p>detailed, white background, studio lighting, product photography </p>
<p>https://aituts.com </p>
<p>124 </p>
<p>The Big Book of Prompts </p>
<ol start="46">
<li>ART DECO TAROT CARDS </li>
</ol>
<p>Design a deck of tarot cards with the art deco style of none other than Alphonse 
Mucha. </p>
<p>tarot card illustration, the knight of cups, style of Alphonse Mucha </p>
<p>https://aituts.com </p>
<p>125 </p>
<p>The Big Book of Prompts </p>
<p>tarot card illustration, the lovers, style of Alphonse Mucha </p>
<p>tarot card illustration, the hermit, style of Alphonse Mucha </p>
<p>https://aituts.com </p>
<p>126 </p>
<p>The Big Book of Prompts </p>
<p>SECTION XII </p>
<p>3D </p>
<p>https://aituts.com </p>
<p>127 </p>
<p>The Big Book of Prompts </p>
<ol start="47">
<li>CLEAN ISOMETRIC ART </li>
</ol>
<p>Generate clean and stylish isometric images for any theme or concept. </p>
<p>Isometric clean image of an old Japanese ryokan with a small open air hot </p>
<p>spring </p>
<p>https://aituts.com </p>
<p>128 </p>
<p>The Big Book of Prompts </p>
<p>Isometric clean pixel art image cutaway of inside of japanese bath house </p>
<p>Isometric clean pixel art image of outside of shrine </p>
<p>https://aituts.com </p>
<p>129 </p>
<p>The Big Book of Prompts </p>
<p>Isometric clean pixel art image 
cutaway of inside of cable car </p>
<p>Isometric clean pixel art image 
of outside of cute design studio </p>
<p>Isometric clean pixel art image 
of outside of old apartment 
building </p>
<p>Isometric clean pixel art image 
of a old Japanese gate </p>
<p>https://aituts.com </p>
<p>130 </p>
<p>The Big Book of Prompts </p>
<ol start="48">
<li>ISOMETRIC CUTAWAY </li>
</ol>
<p>Generate  isometric  cutaways  of  any  object  you  can  imagine.  You  might  be 
surprised at what’s inside! </p>
<p>Isometric clean pixel art image cutaway of inside of a Pineapple </p>
<p>https://aituts.com </p>
<p>131 </p>
<p>The Big Book of Prompts </p>
<p>Isometric clean pixel art image cutaway of inside of Planet earth </p>
<p>Isometric clean pixel art image cutaway of inside of mecha cockpit </p>
<p>https://aituts.com </p>
<p>132 </p>
<p>The Big Book of Prompts </p>
<p>Isometric clean pixel art image 
cutaway of inside of melon </p>
<p>Isometric clean pixel art image 
cutaway of inside of a dragon 
slayer guild fused with a 
mechanical dragon </p>
<p>Isometric clean pixel art image 
cutaway of inside of tree fort </p>
<p>Isometric clean pixel art image 
cutaway of inside of fox </p>
<p>https://aituts.com </p>
<p>133 </p>
<p>The Big Book of Prompts </p>
<ol start="49">
<li>FUTURISTIC VEHICLE CONCEPTS </li>
</ol>
<p>Futuristic vehicles inspired by anything. </p>
<p>futuristic motorcycle concept inspired by Egyptian mythology </p>
<p>https://aituts.com </p>
<p>134 </p>
<p>The Big Book of Prompts </p>
<p>futuristic motorcycle concept inspired by arctic snowmobile.png </p>
<p>futuristic motorcycle concept inspired by Akira motorcycle </p>
<p>https://aituts.com </p>
<p>135 </p>
<p>The Big Book of Prompts </p>
<ol start="50">
<li>CUTE MATTE CLAY EMOJIS </li>
</ol>
<p>Generate cute matte clay isometric emojis of any subject. </p>
<p>Tiny cute isometric vespa emoji, soft lighting, soft pastel colors, 3d </p>
<p>icon clay render, blender 3d, pastel background </p>
<p>https://aituts.com </p>
<p>136 </p>
<p>The Big Book of Prompts </p>
<p>Tiny cute isometric outside of 
office emoji, soft lighting, soft 
pastel colors, 3d icon clay render, 
blender 3d, pastel background, 
physically based rendering </p>
<p>Tiny cute isometric cabbage emoji, 
soft lighting, soft colors, matte 
clay, blender 3d, pastel background </p>
<p>Tiny cute isometric little dog 
emoji, soft lighting, soft colors, 
matte clay, blender 3d, pastel 
background </p>
<p>Tiny cute isometric cruise ship 
emoji, soft lighting, soft pastel 
colors, 3d icon clay render, 
blender 3d, pastel background, 
physically based rendering </p>
<p>https://aituts.com </p>
<p>137 </p>
<p>The Big Book of Prompts </p>
<ol start="51">
<li>SUPER CUTE ANIMALS WEARING 
KNIT WOOL HATS </li>
</ol>
<p>Cute winter animals wearing cozy knit caps, perfect for various design collateral 
for the holiday season. </p>
<p>snowing winter, super cute baby pixar style white fairy bunny, shiny </p>
<p>snow-white fluffy, big bright eyes, wearing a woolly cyan hat, delicate </p>
<p>and fine, high detailed, bright color, natural light, simple background, </p>
<p>octane render, ultra wide angle, 8K </p>
<p>https://aituts.com </p>
<p>138 </p>
<p>The Big Book of Prompts </p>
<p>snowing winter, super cute baby pixar style white fairy bear, shiny snow-white </p>
<p>fluffy, big bright eyes, wearing a woolly cyan hat, delicate and fine, high </p>
<p>detailed, bright color, natural light, simple background, octane render, ultra </p>
<p>wide angle, 8K </p>
<p>snowing winter, super cute baby pixar style white fairy wolf, shiny snow-white </p>
<p>fluffy, big bright eyes, wearing a woolly cyan hat, delicate and fine, high </p>
<p>detailed, bright color, natural light, simple background, octane render, ultra </p>
<p>wide angle, 8K</p>
<p>https://aituts.com </p>
<p>139 </p>
<p>The Big Book of Prompts </p>
<p>ABOUT THE AUTHOR </p>
<p>Yubin “Aitu” Ma has loved art and computers since he was a kid. He was formerly a software engineer 
at IBM and a slew of small startups. He holds a BA in Computer Science from Cornell University. </p>
<p>He’s done illustrations for brands like Angelist, Grubmarket and Yelp. He feels like generative AI is the 
best thing to happen to artists since sliced bread and thinks it is sad that many artists have such a 
negative reaction to this incredible imagination factory. </p>
<p>He, for one, welcomes our new AI overlords. </p>
<p>https://aituts.com </p>
<p>140 </p>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->


                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">

            </nav>

        </div>




        <script>
            window.playground_copyable = true;
        </script>



        <script src="clipboard.min.js"></script>
        <script src="highlight.js"></script>
        <script src="book.js"></script>

        <!-- Custom JS scripts -->

        <script>
        window.addEventListener('load', function() {
            window.setTimeout(window.print, 100);
        });
        </script>

    </div>
    </body>
</html>
