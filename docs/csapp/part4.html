<!DOCTYPE HTML>
<html lang="en" class="sidebar-visible no-js light">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>Part4 - KnowledgeBase</title>


        <!-- Custom HTML head -->
        
        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff" />

        <link rel="icon" href="../favicon.svg">
        <link rel="shortcut icon" href="../favicon.png">
        <link rel="stylesheet" href="../css/variables.css">
        <link rel="stylesheet" href="../css/general.css">
        <link rel="stylesheet" href="../css/chrome.css">
        <link rel="stylesheet" href="../css/print.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="../FontAwesome/css/font-awesome.css">
        <link rel="stylesheet" href="../fonts/fonts.css">

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" href="../highlight.css">
        <link rel="stylesheet" href="../tomorrow-night.css">
        <link rel="stylesheet" href="../ayu-highlight.css">

        <!-- Custom theme stylesheets -->

    </head>
    <body>
    <div id="body-container">
        <!-- Provide site root to javascript -->
        <script>
            var path_to_root = "../";
            var default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? "navy" : "light";
        </script>

        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script>
            try {
                var theme = localStorage.getItem('mdbook-theme');
                var sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script>
            var theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            var html = document.querySelector('html');
            html.classList.remove('no-js')
            html.classList.remove('light')
            html.classList.add(theme);
            html.classList.add('js');
        </script>

        <!-- Hide / unhide sidebar before it is displayed -->
        <script>
            var html = document.querySelector('html');
            var sidebar = null;
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            } else {
                sidebar = 'hidden';
            }
            html.classList.remove('sidebar-visible');
            html.classList.add("sidebar-" + sidebar);
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <div class="sidebar-scrollbox">
                <ol class="chapter"><li class="chapter-item expanded "><a href="../HaskellProgramming/HaskellProgramming.html"><strong aria-hidden="true">1.</strong> Haskell Programming</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="../HaskellProgramming/part1.html"><strong aria-hidden="true">1.1.</strong> Part1</a></li><li class="chapter-item expanded "><a href="../HaskellProgramming/part2.html"><strong aria-hidden="true">1.2.</strong> Part2</a></li><li class="chapter-item expanded "><a href="../HaskellProgramming/part3.html"><strong aria-hidden="true">1.3.</strong> Part3</a></li><li class="chapter-item expanded "><a href="../HaskellProgramming/part4.html"><strong aria-hidden="true">1.4.</strong> Part4</a></li><li class="chapter-item expanded "><a href="../HaskellProgramming/part5.html"><strong aria-hidden="true">1.5.</strong> Part5</a></li></ol></li><li class="chapter-item expanded "><a href="../csapp/csapp.html"><strong aria-hidden="true">2.</strong> csapp</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="../csapp/part1.html"><strong aria-hidden="true">2.1.</strong> Part1</a></li><li class="chapter-item expanded "><a href="../csapp/part2.html"><strong aria-hidden="true">2.2.</strong> Part2</a></li><li class="chapter-item expanded "><a href="../csapp/part3.html"><strong aria-hidden="true">2.3.</strong> Part3</a></li><li class="chapter-item expanded "><a href="../csapp/part4.html" class="active"><strong aria-hidden="true">2.4.</strong> Part4</a></li><li class="chapter-item expanded "><a href="../csapp/part5.html"><strong aria-hidden="true">2.5.</strong> Part5</a></li><li class="chapter-item expanded "><a href="../csapp/part6.html"><strong aria-hidden="true">2.6.</strong> Part6</a></li><li class="chapter-item expanded "><a href="../csapp/part7.html"><strong aria-hidden="true">2.7.</strong> Part7</a></li><li class="chapter-item expanded "><a href="../csapp/part8.html"><strong aria-hidden="true">2.8.</strong> Part8</a></li><li class="chapter-item expanded "><a href="../csapp/part9.html"><strong aria-hidden="true">2.9.</strong> Part9</a></li><li class="chapter-item expanded "><a href="../csapp/part10.html"><strong aria-hidden="true">2.10.</strong> Part10</a></li></ol></li><li class="chapter-item expanded "><a href="../combined_html_page.html"><strong aria-hidden="true">3.</strong> midjourney</a></li></ol>
            </div>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle"></div>
        </nav>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky bordered">
                    <div class="left-buttons">
                        <button id="sidebar-toggle" class="icon-button" type="button" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </button>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="light">Light</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                    </div>

                    <h1 class="menu-title">KnowledgeBase</h1>

                    <div class="right-buttons">
                        <a href="../print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>

                    </div>
                </div>


                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script>
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <pre><code>on	lines	5–7.	
</code></pre>
<p>Hint:
Think	about	the	bit-level
representation	of	–16	and	its	effect	in	the	
instruction	of
line	6.
1
2
1
2
2
1
1
1
2</p>
<p>B
.	
Explain,	in	mathematical	terms,	the	logic	in	the	computation
of	
p
on	lines	8–10.	
Hint:
You	may	want	to	refer	to	the
discussion	on	division	by	powers	of	2	in	
Section	
2.3.7
.
C
.	
For	the	following	values	of	
n
and	
s
,	trace	the	execution	of
the	code	to	determine	what	the	resulting	values	would	be
for	
s
,	
p
,	
e
,	and	
e
.
n
s
s
p
e
e
5
2,065</p>
<hr />
<hr />
<hr />
<hr />
<p>6
2,064</p>
<hr />
<hr />
<hr />
<hr />
<p>D
.	
What	alignment	properties	does	this	code	guarantee	for	the
values	of	
s
and	
p
?
1
2
1
2
1
2
1
2
2</p>
<p>3.11	
Floating-Point	Code
The	
floating-point	architecture
for	a	processor	consists	of	the	different
aspects	that	affect	how	programs	operating	on	floating-point	data	are
mapped	onto	the	machine,	including
How	floating-point	values	are	stored	and	accessed.	This	is	typically
via	some	form	of	registers.
The	instructions	that	operate	on	floating-point	data.
The	conventions	used	for	passing	floating-point	values	as	arguments
to	functions	and	for	returning	them	as	results.
The	conventions	for	how	registers	are	preserved	during	function	calls
—for	example,	with	some	registers	designated	as	caller	saved,	and
others	as	callee	saved.
To	understand	the	x86-64	floating-point	architecture,	it	is	helpful	to	have	a
brief	historical	perspective.	Since	the	introduction	of	the	Pentium/MMX	in
1997,	both	Intel	and	AMD	have	incorporated	successive	generations	of
media
instructions	to	support	graphics	and	image	processing.	These
instructions	originally	focused	on	allowing	multiple	operations	to	be
performed	in	a	parallel	mode	known	as	
single	instruction,	multiple	data
,
or	
SIMD
(pronounced	sim-dee).	In	this	mode	the	same	operation	is
performed	on	a	number	of	different	data	values	in	parallel.	Over	the
years,	there	has	been	a	progression	of	these	extensions.	The	names
have	changed	through	a	series	of	major	revisions	from	MMX	to	SSE	(for
&quot;streaming	SIMD	extensions&quot;)	and	most	recently	AVX	(for	&quot;advanced
vector	extensions&quot;).	Within	each	generation,	there	have	also	been</p>
<p>different	versions.	Each	of	these	extensions	manages
datainsetsofregisters,	referredto	as&quot;MM&quot;	registers	for	MMX,	&quot;XMM&quot;	for
SSE,	and	&quot;YMM&quot;	for	AVX,	ranging	from	64	bits	for	MM	registers,	to	128
for	XMM,	to	256	for	YMM.	So,	for	example,	each	YMM	register	can	hold
eight	32-bit	values,	or	four	64-bit	values,	where	these	values	can	be
either	integer	or	floating	point.
Starting	with	SSE2,	introduced	with	the	Pentium	4	in	2000,	the	media
instructions	have	included	ones	to	operate	on	
scalar
floating-point	data,
using	single	values	in	the	low-order	32	or	64	bits	of	XMM	or	YMM
registers.	This	scalar	mode	provides	a	set	of	registers	and	instructions
that	are	more	typical	of	the	way	other	processors	support	floating	point.
All	processors	capable	of	executing	x86-64	code	support	SSE2	or	higher,
and	hence	x86-64	floating	point	is	based	on	SSE	or	AVX,	including
conventions	for	passing	procedure	arguments	and	return	values	
[77]
.
Our	presentation	is	based	on	AVX2,	the	second	version	of	AVX,
introduced	with	the	Core	i7	Haswell	processor	in	2013.	G
CC
will	generate
AVX2	code	when	given	the	command-line	parameter	
.	Code
based	on	the	different	versions	of	SSE,	as	well	as	the	first	version	of
AVX,	is	conceptually	similar,	although	they	differ	in	the	instruction	names
and	formats.	We	present	only	instructions	that	arise	in	compiling	floating-
point	programs	with	
GCC
.	These	are,	for	the	most	part,	the	scalar	AVX
instructions,	although	we	document	occasions	where	instructions
intended	for	operating	on	entire	data	vectors	arise.	A	more	complete
coverage	of	how	to	exploit	the	SIMD	capabilities	of	SSE	and	AVX	is
presented	in	Web	Aside	
OPT
:
SIMD
on	page	546.	Readers	may	wish	to	refer
to	the	AMD	and	Intel	documentation	for	the	individual	instructions	
[4,</p>
<p>51]
.
As	with	integer	operations,	note	that	the	ATT	format	we	use	in	our</p>
<p>presentation	differs	from	the	Intel	format	used	in	these	documents.	In
particular,	the	instruction	operands	are	listed	in	a	different	order	in	these
two	versions.
Figure	
3.45	
Media	registers.
These	registers	are	used	to	hold	floating-point	data.	Each	YMM	register
holds	32	bytes.	The	low-order	16	bytes	can	be	accessed	as	an	XMM
register.
As	is	illustrated	in	
Figure	
3.45
,	the	AVX	floating-point	architecture</p>
<p>allows	data	to	be	stored	in	16	YMM	registers,	named	
.	Each
YMM	register	is	256	bits	(32	bytes)	long.	When	operating	on	scalar	data,
these	registers	only	hold	floating-point	data,	and	only	the	low-order	32
bits	(for	float)	or	64	bits	(for	double)	are	used.	The	assembly	code	refers
to	the	registers	by	their	SSE	XMM	register	names	
,	where
each	XMM	register	is	the	low-order	128	bits	(16	bytes)	of	the
corresponding	YMM	register.
Instruction
Source
Destination
Description
M
X
Move	single	precision
X
M
Move	single	precision
M
X
Move	double	precision
X
M
Move	double	precision
X
X
Move	aligned,	packed	single	precision
X
X
Move	aligned,	packed	double	precision
Figure	
3.46	
Floating-point	movement	instructions.
These	operations	transfer	values	between	memory	and	registers,	as	well
as	between	pairs	of	registers.	(
X
:	XMM	register	(e.g.,	
);	
M
:	32-bit
memory	range;	
M
:	64-bit	memory	range)
3.11.1	
Floating-Point	Movement	and
Conversion	Operations
32
32
64
64
32
64</p>
<p>Figure	
3.46
shows	a	set	of	instructions	for	transferring	floating-point
data	between	memory	and	XMM	registers,	as	well	as	from	one	XMM
register	to	another	without	any	conversions.	Those	that	reference
memory	are	
scalar
instructions,	meaning	that	they	operate	on	individual,
rather	than	packed,	data	values.	The	data	are	held	either	in	memory
(indicated	in	the	table	as	
M
and	
M
)	or	in	XMM	registers	(shown	in	the
table	as	
X
).	These	instructions	will	work	correctly	regardless	of	the
alignment	of	data,	although	the	code	optimization	guidelines	recommend
that	32-bit	memory	data	satisfy	a	4-byte	alignment	and	that	64-bit	data
satisfy	an	8-byte	alignment.	Memory	references	are	specified	in	the	same
way	as	for	the	integer	
MOV</p>
<p>instructions,	with	all	of	the	different	possible
combinations	of	displacement,	base	register,	index	register,	and	scaling
factor.
G
CC</p>
<p>uses	the	scalar	movement	operations	only	to	transfer	data	from
memory	to	an	XMM	register	or	from	an	XMM	register	to	memory.	For
transferring	data	between	two	XMM	registers,	it	uses	one	of	two	different
instructions	for	copying	the	entire	contents	of	one	XMM	register	to
another—namely,	
for	single-precision	and	
for	double-
precision	values.	For	these	cases,	whether	the	program	copies	the	entire
register	or	just	the	low-order	value	affects	neither	the	program
functionality	nor	the	execution	speed,	and	so	using	these	instructions
rather	than	ones	specific	to	scalar	data	makes	no	real	difference.	The
letter	`
'	in	these	instruction	names	stands	for	&quot;aligned.&quot;	When	used	to
read	and	write	memory,	they	will	cause	an	exception	if	the	address	does
not	satisfy	a	16-byte	alignment.	For	transferring	between	two	registers,
there	is	no	possibility	of	an	incorrect	alignment.
32
64</p>
<p>As	an	example	of	the	different	floating-point	move	operations,	consider
the	C	function
Instruction
Source
Destination
Description
X
/
M
R
Convert	with	truncation	single	precision	to
integer
X
/
M
R
Convert	with	truncation	double	precision	to
integer
X
/
M
R
Convert	with	truncation	single	precision	to	quad
word	integer
X
/
M
R
Convert	with	truncation	double	precision	to
quad	word	integer
Figure	
3.47	
Two-operand	floating-point	conversion	operations.
These	convert	floating-point	data	to	integers.	(
X
:	XMM	register	(e.g.,
);	
R
:	32-bit	general-purpose	register	(e.g.,	
);	
R
:	64-bit
general-purpose	register	(e.g.,	
);	
M
:	32-bit	memory	range;	
M
:	64-
bit	memory	range)
32
32
64
32
32
64
64
64
32
64
32
64</p>
<p>Instruction
Source
1
Source
2
Destination
Description
M
/
R
X
X
Convert	integer	to	single	precision
M
/
R
X
X
Convert	integer	to	double	precision
M
/
R
X
X
Convert	quad	word	integer	to	single
precision
M
/
R
X
X
Convert	quad	word	integer	to	double
precision
Figure	
3.48	
Three-operand	floating-point	conversion	operations.
These	instructions	convert	from	the	data	type	of	the	first	source	to	the
data	type	of	the	destination.	The	second	source	value	has	no	effect	on
the	low-order	bytes	of	the	result.	(
X
:	XMM	register	(e.g.,	
);	
M
:	32-
bit	memory	range;	
M
:	64-bit	memory	range)
and	its	associated	x86-64	assembly	code
32
32
32
32
32
64
64
32
64</p>
<p>We	can	see	in	this	example	the	use	of	the	
instruction	to	copy
data	from	one	register	to	another	and	the	use	of	the	
instruction	to
copy	data	from	memory	to	an	XMM	register	and	from	an	XMM	register	to
memory.
Figures	
3.47
and	
3.48
show	sets	of	instructions	for	converting
between	floating-point	and	integer	data	types,	as	well	as	between
different	floating-point	formats.	These	are	all	scalar	instructions	operating
on	individual	data	values.	Those	in	
Figure	
3.47
convert	from	a
floating-point	value	read	from	either	an	XMM	register	or	memory	and
write	the	result	to	a	general-purpose	register	(e.g.,	
,	etc.).
When	converting	floating-point	values	to	integers,	they	perform
truncation
,	rounding	values	toward	zero,	as	is	required	by	C	and	most
other	programming	languages.
The	instructions	in	
Figure	
3.48
convert	from	integer	to	floating	point.
They	use	an	unusual	three-operand	format,	with	two	sources	and	a
destination.	The	
first	operand	is	read	from	memory	or	from	a	general-
purpose	register.	For	our	purposes,	we	can	ignore	the	second	operand,
since	its	value	only	affects	the	upper	bytes	of	the	result.	The	destination
must	be	an	XMM	register.	In	common	usage,	both	the	second	source	and
the	destination	operands	are	identical,	as	in	the	instruction
This	instruction	reads	a	long	integer	from	register	
,	converts	it	to
data	type	double,	and	stores	the	result	in	the	lower	bytes	of	XMM	register</p>
<p>.
Finally,	for	converting	between	two	different	floating-point	formats,	current
versions	of	
GCC</p>
<p>generate	code	that	requires	separate	documentation.
Suppose	the	low-order	4	bytes	of	
hold	a	single-precision	value;
then	it	would	seem	straightforward	to	use	the	instruction
to	convert	this	to	a	double-precision	value	and	store	the	result	in	the
lower	8	bytes	of	register	
.	Instead,	we	find	the	following	code
generated	by	
GCC
:
The	
instruction	is	normally	used	to	interleave	the	values	in	two
XMM	registers	and	store	them	in	a	third.	That	is,	if	one	source	register
contains	words	[
s
,	
s
,	
s
,	
s
]	and	the	other	contains	words	[
d
,	
d
,	
d
,	
d
],
then	the	value	of	the	destination	register	will	be	[
s
,	
d
,	
s
,	
d
].	In	the	code
above,	we	see	the	same	register	being	used	for	all	three	operands,	and
3
2
1
0
3
2
1
0
1
1
0
0</p>
<p>so	if	the	original	register	held	values	[
x
,	
x
,	
x
,	
x
],	then	the	instruction	will
update	the	register	to	hold	values	[
x
,	
x
,	
x
,	
x
].	The	
instruction
expands	the	two	low-order	single-precision	values	in	the	source	XMM
register	to	be	the	two	double-precision	values	in	the	destination	XMM
register.	Applying	this	to	the	result	of	the	preceding	
instruction
would	give	values	[
dx
,	
dx
],	where	
dx
is	the	result	of	converting	
x
to
double	precision.	That	is,	the	net	effect	of	the	two	instructions	is	to
convert	the	original	single-precision	value	in	the	low-order	4	bytes	of
to	double	precision	and	store	two	copies	of	it	in	
.	It	is	unclear
why	
GCC</p>
<p>generates	this	code.	There	is	neither	benefit	nor	need	to	have
the	value	duplicated	within	the	XMM	register.
G
CC</p>
<p>generates	similar	code	for	converting	from	double	precision	to	single
precision:
Suppose	these	instructions	start	with	register	
holding	two	double-
precision	values	[
x
,	
x
].	Then	the	
instruction	will	set	it	to	[
x
,	
x
].
The	
instruction	will	convert	these	values	to	single	precision,
pack	them	into	the	low-order	half	of	the	register,	and	set	the	upper	half	to
0,	yielding	a	result	[0.0,	0.0,	
x
,	
x
]	(recall	that	floating-point	value	0.0	is
3
2
1
0
1
1
0
0
0
0
0
1
0
0
0
0
0</p>
<p>represented	by	a	bit	pattern	of	all	zeros).	Again,	there	is	no	clear	value	in
computing	the	conversion	from	one	precision	to	another	this	way,	rather
than	by	using	the	single	instruction
As	an	example	of	the	different	floating-point	conversion	operations,
consider	the	C	function
and	its	associated	x86-64	assembly	code</p>
<p>All	of	the	arguments	to	
are	passed	through	the	general-purpose
registers,	since	they	are	either	integers	or	pointers.	The	result	is	returned
in	register	
.	As	is	documented	in	
Figure	
3.45
,	this	is	the
designated	return	register	for	float	or	double	values.	In	this	code,	we	see
a	number	of	the	movement	and	conversion	instructions	of	
Figures
3.46
–
3.48
,	as	well	as	
GCC
's	preferred	method	of	converting	from
single	to	double	precision.
Practice	Problem	
3.50	
(solution	page	
347
)
For	the	following	C	code,	the	expressions	
all	map	to	the
program	values	
,	and	
:</p>
<p>Determine	the	mapping,	based	on	the	following	x86-64	code	for
the	function:</p>
<p>Practice	Problem	
3.51	
(solution	page	
348
)
The	following	C	function	converts	an	argument	of	type	
to	a
return	value	of	type	
,	where	these	two	types	are	defined
using	
:
For	execution	on	x86-64,	assume	that	argument	
is	either	in
or	in	the	appropriately	named	portion	of	register	
(i.e.,
or	
).	One	or	two	instructions	are	to	be	used	to	perform
the	type	conversion	and	to	copy	the	value	to	the	appropriately
named	portion	of	register	
(integer	result)	or	
(floating-
point	result).	Show	the	instruction(s),	including	the	source	and
destination	registers.
T
T
Instruction(s)
long
double
vcvtsi2sdq	%rdi,	%xmm0
double
int</p>
<hr />
<p>double
float</p>
<hr />
<p>long
float</p>
<hr />
<p>x
y</p>
<p>float
long</p>
<hr />
<p>3.11.2	
Floating-Point	Code	in
Procedures
With	x86-64,	the	XMM	registers	are	used	for	passing	floating-point
arguments	to	functions	and	for	returning	floating-point	values	from	them.
As	is	illustrated	in	
Figure	
3.45
,	the	following	conventions	are
observed:
Up	to	eight	floating-point	arguments	can	be	passed	in	XMM	registers
.	These	registers	are	used	in	the	order	the	arguments	are
listed.	Additional	floating-point	arguments	can	be	passed	on	the	stack.
A	function	that	returns	a	floating-point	value	does	so	in	register	
.
All	XMM	registers	are	caller	saved.	The	callee	may	overwrite	any	of
these	registers	without	first	saving	it.
When	a	function	contains	a	combination	of	pointer,	integer,	and	floating-
point	arguments,	the	pointers	and	integers	are	passed	in	general-
purpose	registers,	while	the	floating-point	values	are	passed	in	XMM
registers.	This	means	that	the	mapping	of	arguments	to	registers
depends	on	both	their	types	and	their	ordering.	Here	are	several
examples:</p>
<p>This	function	would	have	
in	
in	
and	
in	
.
This	function	would	have	the	same	register	assignment	as	function	
.
This	function	would	have	
in	
in	
,	and	
in	
Practice	Problem	
3.52	
(solution	page	
348
)
For	each	of	the	following	function	declarations,	determine	the
register	assignments	for	the	arguments:
A
.	
B
.	
C
.	
D
.	
3.11.3	
Floating-Point	Arithmetic</p>
<p>Operations
Figure	
3.49
documents	a	set	of	scalar	AVX2	floating-point	instructions
that	perform	arithmetic	operations.	Each	has	either	one	(
S
)	or	two	(
S
,
S
)	source	operands	and	a	destination	operand	
D
.	The	first	source
operand	
S
can	be	either	an	XMM	register	or	a	memory	location.	The
second	source	operand	and	the	destination	operands	must	be	XMM
registers.	Each	operation	has	an	instruction	for	single	precision	and	an
instruction	for	double	precision.	The	result	is	stored	in	the	destination
register.
As	an	example,	consider	the	following	floating-point	function:
The	x86-64	code	is	as	follows:
1
1
2
1</p>
<p>Single
Double
Effect
Description
D
←	
S
+
S
Floating-point	add
D
←	
S
-
S
Floating-point	subtract
D
←	
S
×	
S
Floating-point	multiply
D
←	
S
/
S
Floating-point	divide
D
←	max(
S
,	
S
)
Floating-point	maximum
D
←	min(
S
,	
S
)
Floating-point	minimum
Floating-point	square	root
Figure	
3.49	
Scalar	floating-point	arithmetic	operations.
These	have	either	one	or	two	source	operands	and	a	destination
operand.
2
1
2
1
2
1
2
1
2
1
2
1
D
←
S
1</p>
<p>The	three	floating-point	arguments	
,	and	
are	passed	in	XMM
registers	
,	while	integer	argument	
is	passed	in	register
.	The	standard	two-instruction	sequence	is	used	to	convert	argument
to	double	(lines	2-3).	Another	conversion	instruction	is	required	to
convert	argument	
to	double	(line	5).	The	function	value	is	returned	in
register	
.
Practice	Problem	
3.53	
(solution	page	
348
)
For	the	following	C	function,	the	types	of	the	four	arguments	are
defined	by	
:
When	compiled,	
GCC</p>
<p>generates	the	following	code:</p>
<p>Determine	the	possible	combinations	of	types	of	the	four
arguments	(there	may	be	more	than	one).
Practice	Problem	
3.54	
(solution	page	
349
)
Function	
has	the	following	prototype:
G
CC</p>
<p>generates	the	following	code	for	the	function:</p>
<p>Write	a	C	version	of	
.
3.11.4	
Defining	and	Using	Floating-
Point	Constants
Unlike	integer	arithmetic	operations,	AVX	floating-point	operations	cannot
have	immediate	values	as	operands.	Instead,	the	compiler	must	allocate
and	initialize	storage	for	any	constant	values.	The	code	then	reads	the
values	from	memory.	This	is	illustrated	by	the	following	Celsius	to
Fahrenheit	conversion	function:
The	relevant	parts	of	the	x86-64	assembly	code	are	as	follows:</p>
<p>We	see	that	the	function	reads	the	value	1.8	from	the	memory	location
labeled	
and	the	value	32.0	from	the	memory	location	labeled	
Looking	at	the	values	associated	with	these	labels,	we	see	that	each	is
specified	by	a	pair	of	.long	declarations	with	the	values	given	in	decimal.
How	should	these	be	interpreted	as	floating-point	values?	Looking	at	the
declaration	labeled	.
,	we	see	that	the	two	values	are	3435973837
(
)	and	1073532108	(
.)	Since	the	machine	uses
little-endian	byte	ordering,	the	first	value	gives	the	low-order	4	bytes,
while	the	second	gives	the	high-order	4	bytes.	From	the	high-order	bytes,
we	can	extract	an	exponent	field	of	
(1023),	from	which	we	subtract
a	bias	of	1023	to	get	an	exponent	of	0.	Concatenating	the	fraction	bits	of
the	two	values,	we	get	a	fraction	field	of	
,	which	can	be
shown	to	be	the	fractional	binary	representation	of	0.8,	to	which	we	add
the	implied	leading	one	to	get	1.8.
Single
Double
Effect
Description</p>
<h2>D
←	
S
^	
S
Bitwise	
EXCLUSIVE</h2>
<p>OR
D
←	
S
&amp;	
S
Bitwise	
AND
Figure	
3.50	
Bitwise	operations	on	packed	data.
These	instructions	perform	Boolean	operations	on	all	128	bits	in	an	XMM
register.
Practice	Problem	
3.55	
(solution	page	
349
)
Show	how	the	numbers	declared	at	label	
encode	the	number
32.0.
3.11.5	
Using	Bitwise	Operations	in
Floating-Point	Code
At	times,	we	find	
GCC</p>
<p>generating	code	that	performs	bitwise	operations	on
XMM	registers	to	implement	useful	floating-point	results.	
Figure	
3.50
shows	some	relevant	instructions,	similar	to	their	counterparts	for
operating	on	general-purpose	registers.	These	operations	all	act	on
packed	data,	meaning	that	they	update	the	entire	destination	XMM
register,	applying	the	bitwise	operation	to	all	the	data	in	the	two	source
registers.	Once	again,	our	only	interest	for	scalar	data	is	the	effect	these
instructions	have	on	the	low-order	4	or	8	bytes	of	the	destination.	These
operations	are	often	simple	and	convenient	ways	to	manipulate	floating-
point	values,	as	is	explored	in	the	following	problem.
2
1
2
1</p>
<p>Practice	Problem	
3.56	
(solution	page	
350
)
Consider	the	following	C	function,	where	
is	a	macro	defined
with	#define:
Below,	we	show	the	AVX2	code	generated	for	different	definitions
of	
,	where	value	x	is	held	in	
.	All	of	them	correspond	to
some	useful	operation	on	floating-point	values.	Identify	what	the
operations	are.	Your	answers	will	require	you	to	understand	the	bit
patterns	of	the	constant	words	being	retrieved	from	memory.
A
.	
B
.	</p>
<p>C
.	
3.11.6	
Floating-Point	Comparison
Operations
AVX2	provides	two	instructions	for	comparing	floating-point	values:
Instruction
Based	on
Description</p>
<h2>S
,	
S
S</h2>
<p>S
Compare	single	precision</p>
<h2>S
,	
S
S</h2>
<p>S
Compare	double	precision
These	instructions	are	similar	to	the	
CMP</p>
<p>instructions	(see	
Section	
3.6
),
in	that	they	compare	operands	
S
and	
S
(but	in	the	opposite	order	one
might	expect)	and	set	the	condition	codes	to	indicate	their	relative	values.
As	with	
,	they	follow	the	ATT-format	convention	of	listing	the
1
2
2
1
1
2
2
1
1
2</p>
<p>operands	in	reverse	order.	Argument	
S
must	be	in	an	XMM	register,
while	
S
can	be	either	in	an	XMM	register	or	in	memory.
The	floating-point	comparison	instructions	set	three	condition	codes:	the
zero	flag	
,	the	carry	flag	
,	and	the	parity	flag	
.	We	did	not
document	the	parity	flag	in	
Section	
3.6.1
,	because	it	is	not	commonly
found	in	
GCC
-generated	x86	code.	For	integer	operations,	this	flag	is	set
when	the	most	recent	arithmetic	or	logical	operation	yielded	a	value
where	the	least	significant	byte	has	even	parity	(i.e.,	an	even	number	of
ones	in	the	byte).	For	floating-point	comparisons,	however,	the	flag	is	set
when	either	operand	is	
NaN
.	By	convention,	any	comparison	in	C	is
considered	to	fail	when	one	of	the	arguments	is	
NaN
,	and	this	flag	is
used	to	detect	such	a	condition.	For	example,	even	the	comparison	
yields	0	when	
is	
NaN
.
The	condition	codes	are	set	as	follows:
Ordering	
S
:
S
Unordered
1
1
1
S
&lt;	
S
1
0
0
S
=	
S
0
1
0
S
&gt;	
S
0
0
0
The	
unordered
case	occurs	when	either	operand	is	
NaN
.	This	can	be
detected	with	the	parity	flag.	Commonly,	the	
(for	&quot;jump	on	parity&quot;)
instruction	is	used	to	conditionally	jump	when	a	floating-point	comparison
yields	an	unordered	result.	Except	for	this	case,	the	values	of	the	carry
2
1
2
1
2
1
2
1
2
1</p>
<p>and	zero	flags	are	the	same	as	those	for	an	unsigned	comparison:	
is
set	when	the	two	operands	are	equal,	and	
is
(a)	C	code
(b)	Generated	assembly	code</p>
<p>Figure	
3.51	
Illustration	of	conditional	branching	in	floating-point
code.</p>
<p>set	when	
S
&lt;	
S
.	Instructions	such	as	
and	
are	used	to
conditionally	jump	on	various	combinations	of	these	flags.
As	an	example	of	floating-point	comparisons,	the	C	function	of	
Figure
3.51(a)
classifies	argument	
according	to	its	relation	to	0.0,	returning
an	enumerated	type	as	the	result.	Enumerated	types	in	C	are	encoded	as
integers,	and	so	the	possible	function	values	are:	
,	and	
.	This	final	outcome	occurs	when	the	value	of	
is
NaN
.
G
CC</p>
<p>generates	the	code	shown	in	
Figure	
3.51(b)
for	
.	The
code	is	not	very	efficient—it	compares	
to	0.0	three	times,	even	though
the	required	information	could	be	obtained	with	a	single	comparison.	It
also	generates	floating	point	constant	0.0	twice—once	using	
,	and
once	by	reading	the	value	from	memory.	Let	us	trace	the	flow	of	the
function	for	the	four	possible	comparison	results:
x	&lt;	0.0	The	ja	branch	on	line	4	will	be	taken,	jumping	to	the	end	with	a
return	value	of	0.
x	=	0.0	The	
(line	4)	and	
(line	6)	branches	will	not	be	taken,	but
the	
branch	(line	8)	will,	returning	with	
equal	to	1.
x	&gt;	0.0	None	of	the	three	branches	will	be	taken.	The	set	be	(line	11)
will	yield	0,	and	this	will	be	incremented	by	the	
instruction	(line
13)	to	give	a	return	value	of	2.
x	=	
NaN
The	jp	branch	(line	6)	will	be	taken.	The	third	
instruction	(line	10)	will	set	both	the	carry	and	the	zero	flag,	and	so
the	set	be	instruction	(line	11)	and	the	following	instruction	will	set
2
1</p>
<pre><code>to	1.	This	gets	incremented	by	the	
instruction	(line	13)	to
</code></pre>
<p>give	a	return	value	of	3.
In	Homework	
Problems	
3.73
and	
3.74
,	you	are	challenged	to	hand-
generate	more	efficient	implementations	of	
.
Practice	Problem	
3.57	
(solution	page	
350
)
Function	
has	the	following	prototype:
For	this	function,	
GCC</p>
<p>generates	the	following	code:</p>
<p>Write	a	C	version	of	
.
3.11.7	
Observations	about	Floating-
Point	Code
We	see	that	the	general	style	of	machine	code	generated	for	operating
on	floating-point	data	with	AVX2	is	similar	to	what	we	have	seen	for
operating	on	integer	data.	Both	use	a	collection	of	registers	to	hold	and
operate	on	values,	and	they	use	these	registers	for	passing	function
arguments.
Of	course,	there	are	many	complexities	in	dealing	with	the	different	data
types	and	the	rules	for	evaluating	expressions	containing	a	mixture	of
data	types,	and	AVX2	code	involves	many	more	different	instructions	and
formats	than	is	usually	seen	with	functions	that	perform	only	integer
arithmetic.
AVX2	also	has	the	potential	to	make	computations	run	faster	by
performing	parallel	operations	on	packed	data.	Compiler	developers	are</p>
<p>working	on	automating	the	conversion	of	scalar	code	to	parallel	code,	but
currently	the	most	reliable	way	to	achieve	higher	performance	through
parallelism	is	to	use	the	extensions	to	the	C	language	supported	by	
GCC
for	manipulating	vectors	of	data.	See	Web	Aside	
OPT
:
SIMD</p>
<p>on	page	546	to
see	how	this	can	be	done.</p>
<p>3.12	
Summary
In	this	chapter,	we	have	peered	beneath	the	layer	of	abstraction	provided
by	the	C	language	to	get	a	view	of	machine-level	programming.	By
having	the	compiler	generate	an	assembly-code	representation	of	the
machine-level	program,	we	gain	insights	into	both	the	compiler	and	its
optimization	capabilities,	along	with	the	machine,	its	data	types,	and	its
instruction	set.	In	
Chapter	
5
,	we	will	see	that	knowing	the
characteristics	of	a	compiler	can	help	when	trying	to	write	programs	that
have	efficient	mappings	onto	the	machine.	We	have	also	gotten	amore
complete	picture	of	how	the	program	stores	data	in	different	memory
regions.	In	
Chapter	
12
,	we	will	see	many	examples	where	application
programmers	need	to	know	whether	a	program	variable	is	on	the	run-
time	stack,	in	some	dynamically	allocated	data	structure,	or	part	of	the
global	program	data.	Understanding	how	programs	map	onto	machines
makes	it	easier	to	understand	the	differences	between	these	kinds	of
storage.
Machine-level	programs,	and	their	representation	by	assembly	code,
differ	in	many	ways	from	C	programs.	There	is	minimal	distinction
between	different	data	types.	The	program	is	expressed	as	a	sequence
of	instructions,	each	of	which	performs	a	single	operation.	Parts	of	the
program	state,	such	as	registers	and	the	run-time	stack,	are	directly
visible	to	the	programmer.	Only	low-level	operations	are	provided	to
support	data	manipulation	and	program	control.	The	compiler	must	use
multiple	instructions	to	generate	and	operate	on	different	data	structures
and	to	implement	control	constructs	such	as	conditionals,	loops,	and</p>
<p>procedures.	We	have	covered	many	different	aspects	of	C	and	how	it
gets	compiled.	We	have	seen	that	the	lack	of	bounds	checking	in	C
makes	many	programs	prone	to	buffer	overflows.	This	has	made	many
systems	vulnerable	to	attacks	by	malicious	intruders,	although	recent
safeguards	provided	by	the	run-time	system	and	the	compiler	help	make
programs	more	secure.
We	have	only	examined	the	mapping	of	C	onto	x86-64,	but	much	of	what
we	have	covered	is	handled	in	a	similar	way	for	other	combinations	of
language	and	machine.	For	example,	compiling	C++	is	very	similar	to
compiling	C.	In	fact,	early	implementations	of	C++	first	performed	a
source-to-source	conversion	from	C++	to	C	and	generated	object	code
by	running	a	C	compiler	on	the	result.	C++	objects	are	represented	by
structures,	similar	to	a	C	
.	Methods	are	represented	by	pointers	to
the	code	implementing	the	methods.	By	contrast,	Java	is	implemented	in
an	entirely	different	fashion.	The	object	code	of	Java	is	a	special	binary
representation	known	as	
Java	byte	code
.	This	code	can	be	viewed	as	a
machine-level	program	for	a	
virtual	machine
.	As	its	name	suggests,	this
machine	is	not	implemented	directly	in	hardware.	Instead,	software
interpreters	process	the	byte	code,	simulating	the	behavior	of	the	virtual
machine.	Alternatively,	an	approach	known	as	
just-in-time	compilation
dynamically	translates	byte	code	sequences	into	machine	instructions.
This	approach	provides	faster	execution	when	code	is	executed	multiple
times,	such	as	in	loops.	The	advantage	of	using	byte	code	as	the	low-
level	representation	of	a	program	is	that	the	same	code	can	be
&quot;executed&quot;	on	many	different	machines,	whereas	the	machine	code	we
have	considered	runs	only	on	x86-64	machines.</p>
<p>Bibliographic	Notes
Both	Intel	and	AMD	provide	extensive	documentation	on	their
processors.	This	includes	general	descriptions	of	an	assembly-language
programmer's	view	of	the	hardware	
[2,</p>
<p>50]
,	as	well	as	detailed
references	about	the	individual	instructions	
[3,</p>
<p>51]
.	Reading	the
instruction	descriptions	is	complicated	by	the	facts	that	(1)	all
documentation	is	based	on	the	Intel	assembly-code	format,	(2)	there	are
many	variations	for	each	instruction	due	to	the	different	addressing	and
execution	modes,	and	(3)	there	are	no	illustrative	examples.	Still,	these
remain	the	authoritative	references	about	the	behavior	of	each
instruction.
The	organization	x86-64.org	has	been	responsible	for	defining	the
application	binary	interface
(ABI)	for	x86-64	code	running	on	Linux
systems	
[77]
.	This	interface	describes	details	for	procedure	linkages,
binary	code	files,	and	a	number	of	other	features	that	are	required	for
machine-code	programs	to	execute	properly.
As	we	have	discussed,	the	ATT	format	used	by	
GCC</p>
<p>is	very	different	from
the	Intel	format	used	in	Intel	documentation	and	by	other	compilers
(including	the	Microsoft	compilers).
Muchnick's	book	on	compiler	design	
[80]
is	considered	the	most
comprehensive	reference	on	code-optimization	techniques.	It	covers
many	of	the	techniques	we	discuss	here,	such	as	register	usage
conventions.</p>
<p>Much	has	been	written	about	the	use	of	buffer	overflow	to	attack	systems
over	the	Internet.	Detailed	analyses	of	the	1988	Internet	worm	have	been
published	by	Spafford	
[105]
as	well	as	by	members	of	the	team	at	MIT
who	helped	stop	its	spread	
[35]
.	Since	then	a	number	of	papers	and
projects	have	generated	ways	both	to	create	and	to	prevent	buffer
overflow	attacks.	Seacord's	book	
[97]
provides	a	wealth	of	information
about	buffer	overflow	and	other	attacks	on	code	generated	by	C
compilers.</p>
<p>Homework	Problems
3.58
For	a	function	with	prototype
GCC</p>
<p>generates	the	following	assembly	code:
Parameters	
,	and	
are	passed	in	registers	
,	and
.	The	code	stores	the	return	value	in	register	
.</p>
<p>Write	C	code	for	
that	will	have	an	effect	equivalent	to	the
assembly	code	shown.
3.59
The	following	code	computes	the	128-bit	product	of	two	64-bit
signed	values	
x
and	
y
and	stores	the	result	in	memory:
G
CC</p>
<p>generates	the	following	assembly	code	implementing	the
computation:</p>
<p>This	code	uses	three	multiplications	for	the	multiprecision
arithmetic	required	to	implement	128-bit	arithmetic	on	a	64-bit
machine.	Describe	the	algorithm	used	to	compute	the	product,	and
annotate	the	assembly	code	to	show	how	it	realizes	your
algorithm.	
Hint:
When	extending	arguments	of	
x
and	
y
to	128	bits,
they	can	be	rewritten	as	
x
=	2
·	
x
+	
x
and	
y
=	2
·	
y
+	
y
,	where
x
,	
x
,	
y
,	and	
y
are	64-bit	values.	Similarly,	the	128-bit	product	can
be	written	as	
p
=	2
·	
p
+	
p
,	where	
p
and	
p
are	64-bit	values.
Show	how	the	code	computes	the	values	of	
p
and	
p
in	terms	of
x
,	
x
,	
y
,	and	
y
.
3.60
Consider	the	following	assembly	code:
64
h
l
64
h
l
h
l
h
l
64
h
l
h
l
h
l
h
l
h
l</p>
<p>The	preceding	code	was	generated	by	compiling	C	code	that	had
the	following	overall	form:
Your	task	is	to	fill	in	the	missing	parts	of	the	C	code	to	get	a
program	equivalent	to	the	generated	assembly	code.	Recall	that
the	result	of	the	function	is	returned	in	register	
.	You	will	find	it</p>
<p>helpful	to	examine	the	assembly	code	before,	during,	and	after	the
loop	to	form	a	consistent	mapping	between	the	registers	and	the
program	variables.
A
.	
Which	registers	hold	program	values	
,	and
B
.	
What	are	the	initial	values	of	
and	
C
.	
What	is	the	test	condition	for	
D
.	
How	does	
get	updated?
E
.	
How	does	
get	updated?
F
.	
Fill	in	all	the	missing	parts	of	the	C	code.
3.61
In	
Section	
3.6.6
,	we	examined	the	following	code	as	a
candidate	for	the	use	of	conditional	data	transfer:
We	showed	a	trial	implementation	using	a	conditional	move
instruction	but	argued	that	it	was	not	valid,	since	it	could	attempt	to
read	from	a	null	address.
Write	a	C	function	
that	has	the	same	behavior	as	
,
except	that	it	can	be	compiled	to	use	conditional	data	transfer.</p>
<p>When	compiled,	the	generated	code	should	use	a	conditional
move	instruction	rather	than	one	of	the	jump	instructions.
3.62
The	code	that	follows	shows	an	example	of	branching	on	an
enumerated	type	value	in	a	switch	statement.	Recall	that
enumerated	types	in	C	are	simply	a	way	to	introduce	a	set	of
names	having	associated	integer	values.	By	default,	the	values
assigned	to	the	names	count	from	zero	upward.	In	our	code,	the
actions	associated	with	the	different	case	labels	have	been
omitted.</p>
<p>The	part	of	the	generated	assembly	code	implementing	the
different	actions	is	shown	in	
Figure	
3.52
.	The	annotations
indicate	the	argument	locations,	the	register	values,	and	the	case
labels	for	the	different	jump	destinations.
Fill	in	the	missing	parts	of	the	C	code.	It	contained	one	case	that
fell	through	to	another—try	to	reconstruct	this.
3.63
This	problem	will	give	you	a	chance	to	reverse	engineer	a	
statement	from	disassembled	machine	code.	In	the	following
procedure,	the	body	of	the	
statement	has	been	omitted:</p>
<p>Figure	
3.52	
Assembly	code	for	
Problem	
3.62
.
This	code	implements	the	different	branches	of	a	switch
statement.
Figure	
3.53
shows	the	disassembled	machine	code	for	the
procedure.
The	jump	table	resides	in	a	different	area	of	memory.	We	can	see
from	the	indirect	jump	on	line	5	that	the	jump	table	begins	at
address	
.	Using	the	
GDB	
debugger,	we	can	examine	the
six	8-byte	words	of	memory	comprising	the	jump	table	with	the
command	</p>
<p>GDB	
prints	the	following:
Fill	in	the	body	of	the	switch	statement	with	C	code	that	will	have
the	same	behavior	as	the	machine	code.</p>
<p>Figure	
3.53	
Disassembled	code	for	
Problem	
3.63
.
3.64
Consider	the	following	source	code,	where	
R
,	
S
,	and	
T
are
constants	declared	with	
:</p>
<p>In	compiling	this	program,	
GCC</p>
<p>generates	the	following	assembly
code:</p>
<p>A
.	
Extend	
Equation	
3.1
from	two	dimensions	to	three	to
provide	a	formula	for	the	location	of	array	element	
.
B
.	
Use	your	reverse	engineering	skills	to	determine	the	values
of	
R
,	
S
,	and	
T
based	on	the	assembly	code.
3.65
The	following	code	transposes	the	elements	of	an	
M
×	
M
array,
where	
M
is	a	constant	defined	by	
:
When	compiled	with	optimization	level	–01,	
GCC</p>
<p>generates	the
following	code	for	the	inner	loop	of	the	function:</p>
<p>We	can	see	that	
GCC</p>
<p>has	converted	the	array	indexing	to	pointer
code.
A
.	
Which	register	holds	a	pointer	to	array	element	
B
.	
Which	register	holds	a	pointer	to	array	element	
C
.	
What	is	the	value	of	
M
?
3.66
Consider	the	following	source	code,	where	
and	
are	macro
expressions	declared	with	
that	compute	the	dimensions	of
array	A	in	terms	of	parameter	
n
.	This	code	computes	the	sum	of
the	elements	of	column	
j
of	the	array.</p>
<p>In	compiling	this	program,	
GCC</p>
<p>generates	the	following	assembly
code:</p>
<p>Use	your	reverse	engineering	skills	to	determine	the	definitions	of
and	
.
3.67
For	this	exercise,	we	will	examine	the	code	generated	by	
GCC</p>
<p>for
functions	that	have	structures	as	arguments	and	return	values,
and	from	this	see	how	these	language	features	are	typically
implemented.
The	following	C	code	has	a	function	process	having	structures	as
argument	and	return	values,	and	a	function	eval	that	calls	process:</p>
<p>G
CC</p>
<p>generates	the	following	code	for	these	two	functions:</p>
<p>A
.	
We	can	see	on	line	2	of	function	eval	that	it	allocates	104
bytes	on	the	stack.	Diagram	the	stack	frame	for	
,
showing	the	values	that	it	stores	on	the	stack	prior	to	calling
.
B
.	
What	value	does	
pass	in	its	call	to	
C
.	
How	does	the	code	for	
access	the	elements	of
structure	arguments?
D
.	
How	does	the	code	for	
set	the	fields	of	result
structure	
E
.	
Complete	your	diagram	of	the	stack	frame	for	
,
showing	how	
accesses	the	elements	of	structure	
following	the	return	from	process.</p>
<p>F
.	
What	general	principles	can	you	discern	about	how
structure	values	are	passed	as	function	arguments	and	how
they	are	returned	as	function	results?
3.68
In	the	following	code,	
A
and	
B
are	constants	defined	with	</p>
<p>G
CC</p>
<p>generates	the	following	code	for	
What	are	the	values	of	
A
and	
B
?	(The	solution	is	unique.)
3.69
You	are	charged	with	maintaining	a	large	C	program,	and	you
come	across	the	following	code:</p>
<p>The	declarations	of	the	compile-time	constant	
and	the
structure	
are	in	a	file	for	which	you	do	not	have	the
necessary	access	privilege.	Fortunately,	you	have	a	copy	of	the
version	of	code,	which	you	are	able	to	disassemble	with	the
OBJDUMP</p>
<p>program,	yielding	the	following	disassembly:
Using	your	reverse	engineering	skills,	deduce	the	following:
A
.	
The	value	of	
B
.	
A	complete	declaration	of	structure	
Assume	that
the	only	fields	in	this	structure	are	
and	
,	and	that	both
of	these	contain	signed	values.</p>
<p>3.70
Consider	the	following	union	declaration:
This	declaration	illustrates	that	structures	can	be	embedded	within
unions.
The	following	function	(with	some	expressions	omitted)	operates
on	a	linked	list	having	these	unions	as	list	elements:
A
.	
What	are	the	offsets	(in	bytes)	of	the	following	fields:</p>
<p>B
.	
How	many	total	bytes	does	the	structure	require?
C
.	
The	compiler	generates	the	following	assembly	code	for
On	the	basis	of	this	information,	fill	in	the	missing
expressions	in	the	code	for	</p>
<p>Hint:
Some	union
references	can	have	ambiguous	interpretations.	These
ambiguities	get	resolved	as	you	see	where	the	references
lead.	There	
is	only	one	answer	that	does	not	perform	any
casting	and	does	not	violate	any	type	constraints.</p>
<p>3.71
Write	a	function	
that	reads	a	line	from	standard	input
and	writes	it	to	standard	output.	Your	implementation	should	work
for	an	input	line	of	arbitrary	length.	You	may	use	the	library
function	
,	but	you	must	make	sure	your	function	works
correctly	even	when	the	input	line	requires	more	space	than	you
have	allocated	for	your	buffer.	Your	code	should	also	check	for
error	conditions	and	return	when	one	is	encountered.	Refer	to	the
definitions	of	the	standard	I/O	functions	for	documentation	
[45,
61]
.
3.72
Figure	
3.54(a)
shows	the	code	for	a	function	that	is	similar	to
function	
(
Figure	
3.43(a)
).	We	used	
to	illustrate
the	use	of	a	frame	pointer	in	managing	variable-size	stack	frames.
The	new	function	
allocates	space	for	local
(a)	C	code</p>
<p>(b)	Portions	of	generated	assembly	code
⋮
Figure	
3.54	
Code	for	
Problem	
3.72
.
This	function	is	similar	to	that	of	
Figure	
3.43
.
array	p	by	calling	library	function	
.	This	function	is	similar	to
the	more	commonly	used	function	malloc,	except	that	it	allocates
space	on	the	run-time	stack.	The	space	is	automatically
deallocated	when	the	executing	procedure	returns.</p>
<p>Figure	
3.54(b)
shows	the	part	of	the	assembly	code	that	sets
up	the	frame	pointer	and	allocates	space	for	local	variables	
and
.	It	is	very	similar	to	the	corresponding	code	for	
.	Let	us
use	the	same	notation	as	in	
Problem	
3.49
:	The	stack	pointer	is
set	to	values	
s
at	line	4	and	
s
at	line	7.	The	start	address	of	array
is	set	to	value	
p
at	line	9.	Extra	space	
e
may	arise	between	
s
and	
p
,	and	extra	space	
e
may	arise	between	the	end	of	array	
and	
s
.
A
.	
Explain,	in	mathematical	terms,	the	logic	in	the	computation
of	
s
.
B
.	
Explain,	in	mathematical	terms,	the	logic	in	the	computation
of	
p
.
C
.	
Find	values	of	
n
and	
s
that	lead	to	minimum	and	maximum
values	of	
e
.
D
.	
What	alignment	properties	does	this	code	guarantee	for	the
values	of	
s
and	
p
?
3.73
Write	a	function	in	assembly	code	that	matches	the	behavior	of	the
function	
in	
Figure	
3.51
.	Your	code	should	contain
only	one	floating-point	comparison	instruction,	and	then	it	should
use	conditional	branches	to	generate	the	correct	result.	Test	your
code	on	all	2
possible	argument	values.	Web	Aside	
ASM:EASM
on	page	178	describes	how	to	incorporate	functions	written	in
assembly	code	into	C	programs.
1
2
2
2
1
1
2
1
1
2
32</p>
<p>3.74
Write	a	function	in	assembly	code	that	matches	the	behavior	of	the
function	
in	
Figure	
3.51
.	Your	code	should	contain
only	one	floating-point	comparison	instruction,	and	then	it	should
use	conditional	moves	to	generate	the	correct	result.	You	might
want	to	make	use	of	the	instruction	
(move	if	even	parity).
Test	your	code	on	all	2
possible	argument	values.	Web	Aside
ASM:EASM	
on	page	178	describes	how	to	incorporate	functions
written	in	assembly	code	into	C	programs.
3.75
ISO	C99	includes	extensions	to	support	complex	numbers.	Any
floating-point	type	can	be	modified	with	the	keyword	complex.
Here	are	some	sample	functions	that	work	with	complex	data	and
that	call	some	of	the	associated	library	functions:
32</p>
<p>When	compiled,	
GCC</p>
<p>generates	the	following	assembly	code	for
these	functions:
Based	on	these	examples,	determine	the	following:
A
.	
How	are	complex	arguments	passed	to	a	function?
B
.	
How	are	complex	values	returned	from	a	function?</p>
<p>Solutions	to	Practice	Problems
Solution	to	Problem	
3.1	
(page
182
)
This	exercise	gives	you	practice	with	the	different	operand	forms.
Operand
Value
Comment
Register
Absolute	address
Immediate
Address	
Address	
Address	
Address	
Address	
Address	</p>
<p>Solution	to	Problem	
3.2	
(page
185
)
As	we	have	seen,	the	assembly	code	generated	by	
GCC</p>
<p>includes
suffixes	on	the	instructions,	while	the	disassembler	does	not.	Being
able	to	switch	between	these	
two	forms	is	an	important	skill	to	learn.
One	important	feature	is	that	memory	references	in	x86-64	are	always
given	with	quad	word	registers,	such	as	
,	even	if	the	operand	is	a
byte,	single	word,	or	double	word.
Here	is	the	code	written	with	suffixes:
Solution	to	Problem	
3.3	
(page
186
)</p>
<p>Since	we	will	rely	on	
GCC</p>
<p>to	generate	most	of	our	assembly	code,
being	able	to	write	correct	assembly	code	is	not	a	critical	skill.
Nonetheless,	this	exercise	will	help	you	become	more	familiar	with	the
different	instruction	and	operand	types.
Here	is	the	code	with	explanations	of	the	errors:
Solution	to	Problem	
3.4	
(page
187
)
This	exercise	gives	you	more	experience	with	the	different	data
movement	instructions	and	how	they	relate	to	the	data	types	and
conversion	rules	of	C.	The	nuances	of	conversions	of	both</p>
<p>signedness	and	size,	as	well	as	integral	promotion,	add	challenge	to
this	problem.
Instruction
Comments
Read	8	bytes
Store	8	bytes
Convert	char	to	int
Store	4	bytes
Convert	char	to	int
Store	4	bytes
Read	byte	and	zero-
extend
Store	8	bytes
Read	4	bytes
Store	low-order	byte
Read	4	bytes
Store	low-order	byte
Read	byte	and	sign-
extend
Store	2	bytes</p>
<p>Solution	to	Problem	
3.5	
(page
189
)
Reverse	engineering	is	a	good	way	to	understand	systems.	In	this
case,	we	want	to	reverse	the	effect	of	the	C	compiler	to	determine
what	C	code	gave	rise	to	this	assembly	code.	The	best	way	is	to	run	a
&quot;simulation,&quot;	starting	with	values	
,	and	
at	the	locations
designated	by	pointers	
,	and	
,	respectively.	We	would	then
get	the	following	behavior:
From	this,	we	can	generate	the	following	C	code:</p>
<p>Solution	to	Problem	
3.6	
(page
192
)
This	exercise	demonstrates	the	versatility	of	the	
instruction	and
gives	you	more	practice	in	deciphering	the	different	operand	forms.
Although	the	operand	forms	are	classified	as	type	&quot;Memory&quot;	in	
Figure
3.3
,	no	memory	access	occurs.
Instruction
Result
6+
x
x
+
y
x
+	4
y
7	+	9
x</p>
<p>10	+	4
y
9	+
x
+	2
y
Solution	to	Problem	
3.7	
(page
193
)
Again,	reverse	engineering	proves	to	be	a	useful	way	to	learn	the
relationship	between	C	code	and	the	generated	assembly	code.
The	best	way	to	solve	problems	of	this	type	is	to	annotate	the	lines	of
assembly	code	with	information	about	the	operations	being
performed.	Here	is	a	sample:
From	this,	it	is	easy	to	generate	the	missing	expression:</p>
<p>Solution	to	Problem	
3.8	
(page
194
)
This	problem	gives	you	a	chance	to	test	your	understanding	of
operands	and	the	arithmetic	instructions.	The	instruction	sequence	is
designed	so	that	the	result	of	each	instruction	does	not	affect	the
behavior	of	subsequent	ones.
Instruction
Destination
Value
Solution	to	Problem	
3.9	
(page
195
)</p>
<p>This	exercise	gives	you	a	chance	to	generate	a	little	bit	of	assembly
code.	The	solution	code	was	generated	by	
GCC
.	By	loading	parameter
in	register	
,	it	can	then	use	byte	register	
to	specify	the	shift
amount	for	the	
instruction.	It	might	seem	odd	to	use	a	
instruction,	given	that	
is	eight	bytes	long,	but	keep	in	mind	that	only
the	least	significant	byte	is	required	to	specify	the	shift	amount.
Solution	to	Problem	
3.10	
(page
196
)
This	problem	is	fairly	straightforward,	since	the	assembly	code	follows
the	structure	of	the	C	code	closely.</p>
<p>Solution	to	Problem	
3.11	
(page
197
)
A
.	
This	instruction	is	used	to	set	register	
to	zero,	exploiting	the
property	that	
x
^	
x
=	0	for	any	
x
.	It	corresponds	to	the	C	statement
=	0.
B
.	
A	more	direct	way	of	setting	register	
to	zero	is	with	the
instruction	
C
.	
Assembling	and	disassembling	this	code,	however,	we	find	that
the	version	with	
requires	only	3	bytes,	while	the	version	with
requires	7.	Other	ways	to	set	
to	zero	rely	on	the
property	that	any	instruction	that	updates	the	lower	4	bytes	will
cause	the	high-order	bytes	to	be	set	to	zero.	Thus,	we	could	use
either	
(2	bytes)	or	
(5	bytes).
Solution	to	Problem	
3.12	
(page
200
)
We	can	simply	replace	the	
instruction	with	one	that	sets	register
to	
,	and	use	
rather	than	
as	our	division</p>
<p>instruction,	yielding	the	following	code:
Solution	to	Problem	
3.13	
(page
204
)
It	is	important	to	understand	that	assembly	code	does	not	keep	track
of	the	type	of	a	program	value.	Instead,	the	different	instructions
determine	the	operand	sizes	and	whether	they	are	signed	or
unsigned.	When	mapping	from	instruction	sequences	back	to	C	code,
we	must	do	a	bit	of	detective	work	to	infer	the	data	types	of	the
program	values.</p>
<p>A
.	
The	suffix	<code> '	and	the	register	identifiers	indicate	32-bit operands,	while	the	comparison	is	for	a	two's-complement	&lt;. We	can	infer	that	 must	be	 B .	 The	suffix	</code>
'	and	the	register	identifiers	indicate	16-bit
operands,	while	the	comparison	is	for	a	two's-complement	&gt;=.
We	can	infer	that	
must	be	
C
.	
The	suffix	<code> '	and	the	register	identifiers	indicate	8-bit operands,	while	the	comparison	is	for	an	unsigned	&lt;=.	We	can infer	that	 must	be	 D .	 The	suffix	</code>
'	and	the	register	identifiers	indicate	64-bit
operands,	while	the	comparison	is	for	!=,	which	is	the	same
whether	the	arguments	are	signed,	unsigned,	or	pointers.	We
can	infer	that	
could	be	either	
,	or
some	form	of	pointer.
Solution	to	Problem	
3.14	
(page
205
)
This	problem	is	similar	to	
Problem	
3.13
,	except	that	it	involves	
TEST
instructions	rather	than	
CMP</p>
<p>instructions.
A
.	
The	suffix	`
'	and	the	register	identifiers	indicate	a	64-bit
operand,	while	the	comparison	is	for	&gt;=,	which	must	be	signed.
We	can	infer	that	
must	be	
.</p>
<p>B
.	
The	suffix	<code> '	and	the	register	identifier	indicate	a	16-bit operand,	while	the	comparison	is	for	==,	which	is	the	same	for signed	or	unsigned.	We	can	infer	that	 must	be	either C .	 The	suffix	</code>
'	and	the	register	identifier	indicate	an	8-bit
operand,	while	the	comparison	is	for	unsigned	&gt;.	We	can	infer
that	
must	be	
D
.	
The	suffix	`
'	and	the	register	identifier	indicate	32-bit
operands,	while	the	comparison	is	for	&lt;.	We	can	infer	that
must	be	
Solution	to	Problem	
3.15	
(page
209
)
This	exercise	requires	you	to	examine	disassembled	code	in	detail
and	reason	about	the	encodings	for	jump	targets.	It	also	gives	you
practice	in	hexadecimal	arithmetic.
A
.	
The	
instruction	has	as	its	target	
As	the
original	disassembled	code	shows,	this	is	</p>
<p>B
.	
The	
instruction	has	as	its	target	
–	12	(since
is	the	1-byte	two's-complement	representation	of	–	12).
As	the	original	disassembled	code	shows,	this	is	
C
.	
According	to	the	annotation	produced	by	the	disassembler,	the
jump	target	is	at	absolute	address	
.	According	to	the
byte	encoding,	this	must	be	at	an	address	
bytes	beyond
that	of	the	pop	instruction.	Subtracting	these	gives	address
.	Noting	that	the	encoding	of	the	
instruction
requires	2	bytes,	it	must	be	located	at	address	
.	These
are	confirmed	by	examining	the	original	disassembly:
D
.	
Reading	the	bytes	in	reverse	order,	we	can	see	that	the	target
offset	is	
,	or	decimal	-141.	Adding	this	to	
(the	address	of	the	nop	instruction)	gives	address	</p>
<p>Solution	to	Problem	
3.16	
(page
212
)
Annotating	assembly	code	and	writing	C	code	that	mimics	its	control
flow	are	good	first	steps	in	understanding	assembly-language
programs.	This	problem	gives	you	practice	for	an	example	with	simple
control	flow.	It	also	gives	you	a	chance	to	examine	the	implementation
of	logical	operations.
A
.	
Here	is	the	C	code:
B
.	
The	first	conditional	branch	is	part	of	the	implementation	of	the
&amp;&amp;	expression.	If	the	test	for	
being	non-null	fails,	the	code
will	skip	the	test	of	a	&gt;	*p.</p>
<p>Solution	to	Problem	
3.17	
(page
212
)
This	is	an	exercise	to	help	you	think	about	the	idea	of	a	general
translation	rule	and	how	to	apply	it.
A
.	
Converting	to	this	alternate	form	involves	only	switching	around
a	few	lines	of	the	code:
B
.	
In	most	respects,	the	choice	is	arbitrary.	But	the	original	rule
works	better	for	the	common	case	where	there	is	no	else
statement.	For	this	case,	we	can	simply	modify	the	translation
rule	to	be	as	follows:</p>
<p>A	translation	based	on	the	alternate	rule	is	more	cumbersome.
Solution	to	Problem	
3.18	
(page
213
)
This	problem	requires	that	you	work	through	a	nested	branch
structure,	where	you	will	see	how	our	rule	for	translating	
statements	has	been	applied.	On	the	whole,	the	machine	code	is	a
straightforward	translation	of	the	C	code.</p>
<p>Solution	to	Problem	
3.19	
(page
216
)
This	problem	reinforces	our	method	of	computing	the	misprediction
penalty.
A
.	
We	can	apply	our	formula	directly	to	get	
T
=	2(31	–	16)	=	30.
B
.	
When	misprediction	occurs,	the	function	will	require	around
cycles.
Solution	to	Problem	
3.20	
(page
219
)
This	problem	provides	a	chance	to	study	the	use	of	conditional
moves.
A
.	
The	operator	is	`/'.	We	see	this	is	an	example	of	dividing	by	a
power	of	3	by	right	shifting	(see	
Section	
2.3.7
).	Before
MP
16</p>
<ul>
<li></li>
</ul>
<h1 id="30"><a class="header" href="#30">30</a></h1>
<p>46</p>
<h1>shifting	by	
,	we	must	add	a	bias	of	
when	the
dividend	is	negative.
B
.	
Here	is	an	annotated	version	of	the	assembly	code:
The	program	creates	a	temporary	value	equal	to	
,	in
anticipation	of	
x
being	negative	and	therefore	requiring	biasing.
The	
instruction	conditionally	changes	this	number	to	
x
when	
,	and	then	it	is	shifted	by	3	to	generate	
x
/8.
Solution	to	Problem	
3.21	
(page
219
)
This	problem	is	similar	to	
Problem	
3.18
,	except	that	some	of	the
conditionals	have	been	implemented	by	conditional	data	transfers.
Although	it	might	seem	daunting	to	fit	this	code	into	the	framework	of
k</h1>
<h1>3
2
k
−
1</h1>
<p>7
x</p>
<ul>
<li></li>
</ul>
<p>7
x
≥
0</p>
<p>the	original	C	code,	you	will	find	that	it	follows	the	translation	rules
fairly	closely.
Solution	to	Problem	
3.22	
(page
221
)
A
.	
If	we	build	up	a	table	of	factorials	computed	with	data	type	
,	we
get	the	following:
n
n
!
OK?
1
1
Y
2
2
Y</p>
<p>3
6
Y
4
24
Y
5
120
Y
6
720
Y
7
5,040
Y
8
40,320
Y
9
362,880
Y
10
3,628,800
Y
11
39,916,800
Y
12
479,001,600
Y
13
1,932,053,504
N
We	can	see	that	the	computation	of	13!	has	overflowed.	As	we
learned	in	
Problem	
2.35
,	when	we	get	value	
x
while	attempting
to	compute	
n
!,	we	can	test	for	overflow	by	computing	
x/n
and
seeing	whether	it	equals	(
n
-	1)!	(assuming	that	we	have	already
ensured	that	the	computation	of	(
n
-	1)	!did	not	overflow).	In	this
case	we	get	1,932,053,504/13	=	161,004,458.667.	As	a	second
test,	we	can	see	that	any	factorial	beyond	10!	must	be	a	multiple
of	100	and	therefore	have	zeros	for	the	last	two	digits.	The	correct
value	of	13!	is	6,227,020,800.
B
.	
Doing	the	computation	with	data	type	long	lets	us	go	up	to	20!,
yielding	2,432,902,008,176,640,000.</p>
<p>Solution	to	Problem	
3.23	
(page
222
)
The	code	generated	when	compiling	loops	can	be	tricky	to	analyze,
because	the	compiler	can	perform	many	different	optimizations	on
loop	code,	and	because	it	can	be	difficult	to	match	program	variables
with	registers.	This	particular	example	demonstrates	several	places
where	the	assembly	code	is	not	just	a	direct	translation	of	the	C	code.
A
.	
Although	parameter	
is	passed	to	the	function	in	register
,	we	can	see	that	the	register	is	never	referenced	once	the
loop	is	entered.	Instead,	we	can	see	that	registers	
,
and	
are	initialized	in	lines	2–5	to	
,	and	
.	We	can
conclude,	therefore,	that	these	registers	contain	the	program
variables.
B
.	
The	compiler	determines	that	pointer	
always	points	to	
,	and
hence	the	expression	(*
)++	simply	increments	
.	It	combines
this	incrementing	by	1	with	the	increment	by	
,	via	the	
instruction	of	line	7.
C
.	
The	annotated	code	is	as	follows:</p>
<p>Solution	to	Problem	
3.24	
(page
224
)
This	assembly	code	is	a	fairly	straightforward	translation	of	the	loop
using	the	jump-to-middle	method.	The	full	C	code	is	as	follows:</p>
<p>Solution	to	Problem	
3.25	
(page
226
)
While	the	generated	code	does	not	follow	the	exact	pattern	of	the
guarded-do	translation,	we	can	see	that	it	is	equivalent	to	the
following	C	code:
We	will	often	see	cases,	especially	when	compiling	with	higher	levels
of	optimization,	where	
GCC</p>
<p>takes	some	liberties	in	the	exact	form	of
the	code	it	generates,	while	preserving	the	required	functionality.
Solution	to	Problem	
3.26	
(page</p>
<p>228
)
Being	able	to	work	backward	from	assembly	code	to	C	code	is	a
prime	example	of	reverse	engineering.
A
.	
We	can	see	that	the	code	uses	the	jump-to-middle	translation,
using	the	
instruction	on	line	3.
B
.	
Here	is	the	original	C	code:
C
.	
This	code	computes	the	
parity
of	argument	
.	That	is,	it	returns
1	if	there	is	an	odd	number	of	ones	in	
and	0	if	there	is	an
even	number.
Solution	to	Problem	
3.27	
(page
231
)</p>
<p>This	exercise	is	intended	to	reinforce	your	understanding	of	how	loops
are	implemented.
Solution	to	Problem	
3.28	
(page
231
)
This	problem	is	trickier	than	
Problem	
3.26
,	since	the	code	within
the	loop	is	more	complex	and	the	overall	operation	is	less	familiar.
A
.	
Here	is	the	original	C	code:</p>
<p>B
.	
The	code	was	generated	using	the	guarded-do	transformation,
but	the	compiler	detected	that,	since	
i
is	initialized	to	64,	it	will
satisfy	the	test	
i
≠	0,	and	therefore	the	initial	test	is	not
required.
C
.	
This	code	reverses	the	bits	in	
,	creating	a	mirror	image.	It
does	this	by	shifting	the	bits	of	
from	left	to	right,	and	then
filling	these	bits	in	as	it	shifts	
from	right	to	left.</p>
<p>Solution	to	Problem	
3.29	
(page
232
)
Our	stated	rule	for	translating	a	for	loop	into	a	
loop	is	just	a	bit
too	simplistic—this	is	the	only	aspect	that	requires	special
consideration.
A
.	
Applying	our	translation	rule	would	yield	the	following	code:
This	code	has	an	infinite	loop,	since	the	continue	statement
would	prevent	index	variable	
from	being	updated.</p>
<p>B
.	
The	general	solution	is	to	replace	the	continue	statement	with	a
statement	that	skips	the	rest	of	the	loop	body	and	goes
directly	to	the	update	portion:
Solution	to	Problem	
3.30	
(page
236
)
This	problem	gives	you	a	chance	to	reason	about	the	control	flow	of	a
switch	statement.	Answering	the	questions	requires	you	to	combine
information	from	several	places	in	the	assembly	code.
Line	2	of	the	assembly	code	adds	1	to	
x
to	set	the	lower	range	of
the	cases	to	zero.	That	means	that	the	minimum	case	label	is	–1.</p>
<p>Lines	3	and	4	cause	the	program	to	jump	to	the	default	case	when
the	adjusted	case	value	is	greater	than	8.	This	implies	that	the
maximum	case	label	is	–1	+	8	=	7.
In	the	jump	table,	we	see	that	the	entry	on	lines	6	(case	value	3)
and	9	(case	value	6)	have	the	same	destination	(
)	as	the	jump
instruction	on	line	4,	indicating	the	default	case	behavior.	Thus,
case	labels	3	and	5	are	missing	in	the	switch	statement	body.
In	the	jump	table,	we	see	that	the	entries	on	lines	3	and	10	have
the	same	destination.	These	correspond	to	cases	0	and	7.
In	the	jump	table,	we	see	that	the	entries	on	lines	5	and	7	have	the
same	destination.	These	correspond	to	cases	2	and	4.
From	this	reasoning,	we	draw	the	following	conclusions:
A
.	
The	case	labels	in	the	switch	statement	body	have	values	–1,
0,	1,	2,	4,	5,	and	7.
B
.	
The	case	with	destination	
has	labels	0	and	7.
C
.	
The	case	with	destination	
has	labels	2	and	4.
Solution	to	Problem	
3.31	
(page
237
)
The	key	to	reverse	engineering	compiled	switch	statements	is	to
combine	the	information	from	the	assembly	code	and	the	jump	table
to	sort	out	the	different	cases.	We	can	see	from	the	
instruction</p>
<p>(line	3)	that	the	code	for	the	default	case	has	label	
We	can	see
that	the	only	other	repeated	label	in	the	jump	table	is	
,	and	so	this
must	be	the	code	for	the	cases	C	and	D.	We	can	see	that	the	code
falls	through	at	line	8,	and	so	label	
must	match	case	A	and	label
must	match	case	B.	That	leaves	only	label	
to	match	case	E.
The	original	C	code	is	as	follows:</p>
<p>Solution	to	Problem	
3.32	
(page
244
)
Tracing	through	the	program	execution	at	this	level	of	detail	reinforces
many	aspects	of	procedure	call	and	return.	We	can	see	clearly	how
control	is	passed	to	the	function	when	it	is	called,	and	how	the	calling
function	resumes	upon	return.	We	can	also	see	how	arguments	get
passed	through	registers	
and	
,	and	how	results	are	returned
via	register	
.
Instruction
State	values	(at	beginning)
Label
PC
Instruction
M1
10
—
—
0x7fffffffe820
—
F1
10
—
—
0x7fffffffe818
F2
10
11
—
0x7fffffffe818
F3
9
11
—
0x7fffffffe818
L1
9
11
—
0x7fffffffe810
L2
9
11
9
0x7fffffffe810</p>
<p>L3
9
11
99
0x7fffffffe810
F4
9
11
99
0x7fffffffe818
M2
9
11
99
0x7fffffffe820
—
Solution	to	Problem	
3.33	
(page
246
)
This	problem	is	a	bit	tricky	due	to	the	mixing	of	different	data	sizes.
Let	us	first	describe	one	answer	and	then	explain	the	second
possibility.	If	we	assume	the	first	addition	(line	3)	implements	*
+=	
,
while	the	second	(line	4)	implements	
,	then	we	can	see	that	a
was	passed	as	the	first	argument	in	
and	converted	from	4	bytes
to	8	before	adding	it	to	the	8	bytes	pointed	to	by	
.	This	implies
that	a	must	be	of	type	
and	
must	be	of	type	
*.	We	can	also
see	that	the	low-order	byte	of	argument	
is	added	to	the	byte
pointed	to	by	
.	This	implies	that	
must	be	of	type	
,	but	the
type	of	
is	ambiguous—it	could	be	1,	2,	4,	or	8	bytes	long.	This
ambiguity	is	resolved	by	noting	the	return	value	of	
6,	computed	as	the
sum	of	the	sizes	of	
and	
.	Since	we	know	a	is	4	bytes	long,	we	can
deduce	that	
must	be	2.
An	annotated	version	of	this	function	explains	these	details:</p>
<p>Alternatively,	we	can	see	that	the	same	assembly	code	would	be	valid
if	the	two	sums	were	computed	in	the	assembly	code	in	the	opposite
ordering	as	they	are	in	the	C	code.	This	would	result	in	interchanging
arguments	
and	
and	arguments	
and	
,	yielding	the	following
prototype:
Solution	to	Problem	
3.34	
(page
252
)
This	example	demonstrates	the	use	of	callee-saved	registers	as	well
as	the	stack	for	holding	local	data.</p>
<p>A
.	
We	can	see	that	lines	9-14	save	local	values	
into	callee-
saved	registers	
,	and	
,
respectively.
B
.	
Local	values	
and	
are	stored	on	the	stack	at	offsets	0
and	8	relative	to	the	stack	pointer	(lines	16	and	18).
C
.	
After	storing	six	local	variables,	the	program	has	used	up	the
supply	of	callee-saved	registers.	It	stores	the	remaining	two
local	values	on	the	stack.
Solution	to	Problem	
3.35	
(page
254
)
This	problem	provides	a	chance	to	examine	the	code	for	a	recursive
function.	An	important	lesson	to	learn	is	that	recursive	code	has	the
exact	same	structure	as	the	other	functions	we	have	seen.	The	stack
and	register-saving	disciplines	suffice	to	make	recursive	functions
operate	correctly.
A
.	
Register	
holds	the	value	of	parameter	
,	so	that	it	can	be
used	to	compute	the	result	expression.
B
.	
The	assembly	code	was	generated	from	the	following	C	code:</p>
<p>Solution	to	Problem	
3.36	
(page
256
)
This	exercise	tests	your	understanding	of	data	sizes	and	array
indexing.	Observe	that	a	pointer	of	any	kind	is	8	bytes	long.	Data	type
short	requires	2	bytes,	while	
requires	4.
Array
Element	size
Total	size
Start	address
Element	
i
2
14
x
x
+	2
i
8
24
x
x
+	8
i
8
48
x
x
+8
i
4
32
x
x
+	4
i
8
32
x
x
+	8
i
Solution	to	Problem	
3.37	
(page
S
S
T
T
U
U
V
V
W
W</p>
<p>258
)
This	problem	is	a	variant	of	the	one	shown	for	integer	array	E.	It	is
important	to	understand	the	difference	between	a	pointer	and	the
object	being	pointed	to.	Since	data	type	short	requires	2	bytes,	all	of
the	array	indices	are	scaled	by	a	factor	of	2.	Rather	than	using	
,
as	before,	we	now	use	
.
Expression
Type
Value
Assembly
Solution	to	Problem	
3.38	
(page
259
)
This	problem	requires	you	to	work	through	the	scaling	operations	to
determine	the	address	computations,	and	to	apply	
Equation	
3.1
for
row-major	indexing.	The	first	step	is	to	annotate	the	assembly	code	to
determine	how	the	address	references	are	computed:</p>
<p>We	can	see	that	the	reference	to	matrix	
is	at	byte	offset	8	·	(7
i
+	
j
),
while	the	reference	to	matrix	
is	at	byte	offset	8	·	(5
j
+	
i
).	From	this,
we	can	determine	that	
has	7	columns,	while	
has	5,	giving	
M
=	5
and	
N
=	7.
Solution	to	Problem	
3.39	
(page
262
)
These	computations	are	direct	applications	of	
Equation	
3.1
:
For	
L
=	4,	
C
=	16,	and	
j
=	0,	pointer	
is	computed	as	
x
+	4	·
(16
i
+	0)	=	
x
+	64
i
.
A
A</p>
<p>For	
L
=	4,	
C
=	16,	
i
=	0,	and	
j
=	
k
,	Bptr	is	computed	as	
x
+	4	·	(16	·
0	+	
k
)	=	
x
+	4
k
.
For	
L
=	4,	
C
=	16,	
i
=	16,	and	
j
=	
k
,	Bend	is	computed	as	
x
+	4	·
(16	·	16	+	
k
)	=	
x
+	1,024	+	4
k
.
Solution	to	Problem	
3.40	
(page
262
)
This	exercise	requires	that	you	be	able	to	study	compiler-generated
assembly	code	to	understand	what	optimizations	have	been
performed.	In	this	case,	the	compiler	was	clever	in	its	optimizations.
Let	us	first	study	the	following	C	code,	and	then	see	how	it	is	derived
from	the	assembly	code	generated	for	the	original	function.
B
B
B
B</p>
<p>This	function	introduces	a	variable	Abase,	of	type	
*,	pointing	to
the	start	of	array	A.	This	pointer	designates	a	sequence	of	4-byte
integers	consisting	of	elements	of	A	in	row-major	order.	We	introduce
an	integer	variable	index	that	steps	through	the	diagonal	elements	of
A,	with	the	property	that	diagonal	elements	
i
and	
i
+	1	are	spaced	
N
+
1	elements	apart	in	the	sequence,	and	that	once	we	reach	diagonal
element	
N
(index	value	
N(N
+	1)),	we	have	gone	beyond	the	end.
The	actual	assembly	code	follows	this	general	form,	but	now	the
pointer	increments	must	be	scaled	by	a	factor	of	4.	We	label	register
as	holding	a	value	
equal	to	index	in	our	C	version	but
scaled	by	a	factor	of	4.	For	
N
=	16,	we	can	see	that	our	stopping	point
for	
will	be	4.	16(16	+	1)	=	1,088.</p>
<p>Solution	to	Problem	
3.41	
(page
268
)
This	problem	gets	you	to	think	about	structure	layout	and	the	code
used	to	access	structure	fields.	The	structure	declaration	is	a	variant
of	the	example	shown	in	the	text.	It	shows	that	nested	structures	are
allocated	by	embedding	the	inner	structures	within	the	outer	ones.
A
.	
The	layout	of	the	structure	is	as	follows:
B
.	
It	uses	24	bytes.
C
.	
As	always,	we	start	by	annotating	the	assembly	code:
From	this,	we	can	generate	C	code	as	follows:</p>
<p>Solution	to	Problem	
3.42	
(page
269
)
This	problem	demonstrates	how	a	very	common	data	structure	and
operation	on	it	is	implemented	in	machine	code.	We	solve	the
problem	by	first	annotating	the	assembly	code,	recognizing	that	the
two	fields	of	the	structure	are	at	offsets	0	(for	
)	and	8	(for	
).</p>
<p>A
.	
Based	on	the	annotated	code,	we	can	generate	a	C	version:
B
.	
We	can	see	that	each	structure	is	an	element	in	a	singly	linked
list,	with	field	
being	the	value	of	the	element	and	
being	a
pointer	to	the	next	element.	Function	fun	computes	the	sum	of
the	element	values	in	the	list.
Solution	to	Problem	
3.43	
(page
272
)</p>
<p>Structures	and	unions	involve	a	simple	set	of	concepts,	but	it	takes
practice	to	be	comfortable	with	the	different	referencing	patterns	and
their	implementations.
Solution	to	Problem	
3.44	
(page</p>
<p>275
)
Understanding	structure	layout	and	alignment	is	very	important	for
understanding	how	much	storage	different	data	structures	require	and
for	understanding	the	code	generated	by	the	compiler	for	accessing
structures.	This	problem	lets	you	work	out	the	details	of	some
example	structures.
A
.	
Total
Alignment
0
4
8
12
16
4
B
.	
Total
Alignment
0
4
5
8
16
8
C
.	
C.	
Total
Alignment
0
6
10
2
D
.	
Total
Alignment
0
16
40
8
E
.	</p>
<p>Total
Alignment
0
24
40
8
Solution	to	Problem	
3.45	
(page
275
)
This	is	an	exercise	in	understanding	structure	layout	and	alignment.
A
.	
Here	are	the	object	sizes	and	byte	offsets:
Field
Size
8
2
8
1
4
1
8
4
Offset
0
8
16
24
28
32
40
48
B
.	
The	structure	is	a	total	of	56	bytes	long.	The	end	of	the
structure	must	be	padded	by	4	bytes	to	satisfy	the	8-byte
alignment	requirement.
C
.	
One	strategy	that	works,	when	all	data	elements	have	a	length
equal	to	a	power	of	2,	is	to	order	the	structure	elements	in
descending	order	of	size.	This	leads	to	a	declaration</p>
<p>with	the	following	offsets:
Field
Size
8
8
8
4
4
2
1
1
Offset
0
8
16
24
28
32
34
35
The	structure	must	be	padded	by	4	bytes	to	satisfy	the	8-byte
alignment	requirement,	giving	a	total	of	40	bytes.
Solution	to	Problem	
3.46	
(page
282
)
This	problem	covers	a	wide	range	of	topics,	such	as	stack	frames,
string	representations,	ASCII	code,	and	byte	ordering.	It	demonstrates</p>
<p>the	dangers	of	out-of-bounds	memory	references	and	the	basic	ideas
behind	buffer	overflow.
A
.	
Stack	after	line	3:
B
.	
Stack	after	line	5:
C
.	
The	program	is	attempting	to	return	to	address	
.	The
low-order	2	bytes	were	overwritten	by	the	code	for	character	`4'
and	the	terminating	null	character.
D
.	
The	saved	value	of	register	
was	set	to
.	This	value	will	be	loaded	into	the	register
before	
returns.
E
.	
The	call	to	
should	have	had	
as	its
argument,	and	the	code	should	also	check	that	the	returned
value	is	not	equal	to	
.</p>
<p>Solution	to	Problem	
3.47	
(page
286
)
A
.	
This	corresponds	to	a	range	of	around	2
addresses.
B
.	
A	128-byte	nop	sled	would	cover	2
addresses	with	each	test,	and
so	we	would	only	require	around	2
=	64	attempts.
This	example	clearly	shows	that	the	degree	of	randomization	in
this	version	of	Linux	would	provide	only	minimal	deterrence
against	an	overflow	attack.
Solution	to	Problem	
3.48	
(page
288
)
This	problem	gives	you	another	chance	to	see	how	x86-64	code
manages	the	stack,	and	to	also	better	understand	how	to	defend
against	buffer	overflow	attacks.
A
.	
For	the	unprotected	code,	we	can	see	that	lines	4	and	5
compute	the	positions	of	
and	
to	be	at	offsets	24	and	0
relative	to	
In	the	protected	code,	the	canary	is	stored	at
offset	40	(line	4),	while	
and	
are	at	offsets	8	and	16	(lines
7	and	8).
B
.	
In	the	protected	code,	local	variable	
is	positioned	closer	to
the	top	of	the	stack	than	
,	and	so	an	overrun	of	
will	not
13
7
6</p>
<p>corrupt	the	value	of	
.
Solution	to	Problem	
3.49	
(page
293
)
This	code	combines	many	of	the	tricks	we	have	seen	for	performing
bit-level	arithmetic.	It	requires	careful	study	to	make	any	sense	of	it.
A
.	
The	
instruction	of	line	5	computes	the	value	8
n
+	22,
which	is	then	rounded	down	to	the	nearest	multiple	of	16	by	the
instruction	of	line	6.	The	resulting	value	will	be	8
n
+	8
when	
n
is	odd	and	8
n
+	16	when	
n
is	even,	and	this	value	is
subtracted	from	
s
to	give	
s
.
B
.	
The	three	instructions	in	this	sequence	round	
s
up	to	the
nearest	multiple	of	8.	They	make	use	of	the	combination	of
biasing	and	shifting	that	we	saw	for	dividing	by	a	power	of	2	in
Section	
2.3.7
.
C
.	
These	two	examples	can	be	seen	as	the	cases	that	minimize
and	maximize	the	values	of	
e
and	
e
.
n
s
s
p
e
e
5
2,065
2,017
2,024
1
7
6
2,064
2,000
2,000
16
0
1
2
2
1
2
1
2
1
2</p>
<p>D
.	
We	can	see	that	
s
is	computed	in	a	way	that	preserves
whatever	offset	
s
has	with	the	nearest	multiple	of	16.	We	can
also	see	that	
p
will	be	aligned	on	a	multiple	of	8,	as	is
recommended	for	an	array	of	8-byte	elements.
Solution	to	Problem	
3.50	
(page
300
)
This	exercise	requires	that	you	step	through	the	code,	paying	careful
attention	to	which	conversion	and	data	movement	instructions	are
used.	We	can	see	the	values	being	retrieved	and	converted	as
follows:
The	value	at	
is	retrieved,	converted	to	an	
(line	4),	and	then
stored	at	
.	We	can	therefore	infer	that	
is	
.
The	value	at	
is	retrieved,	converted	to	a	
(line	6),	and
then	stored	at	
.	We	can	therefore	infer	that	
is	
.
The	value	of	
is	converted	to	a	
(line	8)	and	stored	at	
.
We	can	therefore	infer	that	
is	
.
The	value	at	
is	retrieved	on	line	3.	The	two	instructions	at	lines
10-11	convert	this	to	double	precision	as	the	value	returned	in
register	
.	We	can	therefore	infer	that	
is	
.
2
1</p>
<p>Solution	to	Problem	
3.51	
(page
300
)
These	cases	can	be	handled	by	selecting	the	appropriate	entries	from
the	tables	in	
Figures	
3.47
and	
3.48
,	or	using	one	of	the	code
sequences	for	converting	between	floating-point	formats.
T
T
Instruction(s)
Solution	to	Problem	
3.52	
(page
301
)
The	basic	rules	for	mapping	arguments	to	registers	are	fairly	simple
(although	they	become	much	more	complex	with	more	and	other
types	of	arguments	
[77]
).
x
y</p>
<p>A
.	
Registers:	
in	
in	
in	
in	
B
.	
Registers:	
in	
in	
in	
in	
C
.	
Registers:	
in	
in	
in	
in	
D
.	
Registers:	
in	
in	
in	
in	
Solution	to	Problem	
3.53	
(page
303
)
We	can	see	from	the	assembly	code	that	there	are	two	integer
arguments,	passed	in	registers	
and	
.	Let	us	name	these	
and	
.	Similarly,	there	are	two	floating-point	arguments,	passed	in
registers	
and	
,	which	we	name	
and	
.
We	can	then	annotate	the	assembly	code:</p>
<p>From	this	we	see	that	the	code	computes	the	value	
.
We	can	also	see	that	
has	type	
has	type	long,	
has	type
float,	and	
has	type	double.	The	only	ambiguity	in	matching
arguments	to	the	named	values	stems	from	the	commutativity	of
multiplication—yielding	two	possible	results:</p>
<p>Solution	to	Problem	
3.54	
(page
303
)
This	problem	can	readily	be	solved	by	stepping	through	the	assembly
code	and	determining	what	is	computed	on	each	step,	as	shown	with
the	annotations	below:
We	can	conclude	from	this	analysis	that	the	function	computes	</p>
<p>Solution	to	Problem	
3.55	
(page
305
)
This	problem	involves	the	same	reasoning	as	was	required	to	see	that
numbers	declared	at	label	
encode	1.8,	but	with	a	simpler
example.
We	see	that	the	two	values	are	0	and	1077936128	(
).	From
the	high-order	bytes,	we	can	extract	an	exponent	field	of	
(1028),	from	which	we	subtract	a	bias	of	1023	to	get	an	exponent	of	5.
Concatenating	the	fraction	bits	of	the	two	values,	we	get	a	fraction
field	of	0,	but	with	the	implied	leading	value	giving	value	1.0.	The
constant	is	therefore	1.0	×	2
=	32.0.
Solution	to	Problem	
3.56	
(page
305
)
A
.	
We	see	here	that	the	16	bytes	starting	at	address	
form	a
mask,	where	the	low-order	8	bytes	contain	all	ones,	except	for	the
most	significant	bit,	which	is	the	sign	bit	of	a	double-precision
value.	When	we	compute	the	
AND	
of	this	mask	with	
,	it	will
clear	the	sign	bit	of	
,	yielding	the	absolute	value.	In	fact,	we
generated	this	code	by	defining	
to	be	
,	where
is	defined	in	
5</p>
<p>B
.	
We	see	that	the	vxorpd	instruction	sets	the	entire	register	to	zero,
and	so	this	is	a	way	to	generate	floating-point	constant	0.0.
C
.	
We	see	that	the	16	bytes	starting	at	address	
form	a	mask
with	a	single	1	bit,	at	the	position	of	the	sign	bit	for	the	low-order
value	in	the	XMM	register.	When	we	compute	the	
EXCLUSIVE-OR
of	this	mask	with	
,	we	change	the	sign	of	
,	computing	the
expression	
.
Solution	to	Problem	
3.57	
(page
308
)
Again,	we	annotate	the	code,	including	dealing	with	the	conditional
branch:</p>
<p>From	this,	we	can	write	the	following	code	for	</p>
<p>Chapter	
4	
Processor	Architecture
4.1	
The	Y86-64	Instruction	Set	Architecture	
355
4.2	
Logic	Design	and	the	Hardware	Control	Language	HCL	
372
4.3	
Sequential	Y86-64	Implementations	
384
4.4	
General	Principles	of	Pipelining	
412
4.5	
Pipelined	Y86-64	Implementations	
421
4.6	
Summary</p>
<p>470
Bibliographic	Notes	
473
Homework	Problems	
473
Solutions	to	Practice	Problems	
480
Modern	microprocessors	are	among	the	most
complex	systems	ever	created	by	humans.	A	single
silicon	chip,	roughly	the	size	of	a	fingernail,	can
contain	several	high-performance	processors,	large
cache	memories,	and	the	logic	required	to	interface
them	to	external	devices.	In	terms	of	performance,
the	processors	implemented	on	a	single	chip	today</p>
<p>dwarf	the	room-size	supercomputers	that	cost	over
$10	million	just	20	years	ago.	Even	the	embedded
processors	found	in	everyday	appliances	such	as
cell	phones,	navigation	systems,	and	programmable
thermostats	are	far	more	powerful	than	the	early
developers	of	computers	could	ever	have
envisioned.
So	far,	we	have	only	viewed	computer	systems
down	to	the	level	of	machine-language	programs.
We	have	seen	that	a	processor	must	execute	a
sequence	of	instructions,	where	each	instruction
performs	some	primitive	operation,	such	as	adding
two	numbers.	An	instruction	is	encoded	in	binary
form	as	a	sequence	of	1	or	more	bytes.	The
instructions	supported	by	a	particular	processor	and
their	byte-level	encodings	are	known	as	its
instruction	set	architecture
(ISA).	Different	&quot;families&quot;
of	processors,	such	as	Intel	IA32	and	x86-64,
IBM/Freescale	Power,	and	the	ARM	processor
family,	have	different	ISAs.	A	program	compiled	for
one	type	of	machine	will	not	run	on	another.	On	the
other	hand,	there	are	many	different	models	of
processors	within	a	single	family.	Each
manufacturer	produces	processors	of	ever-growing
performance	and	complexity,	but	the	different
models	remain	compatible	at	the	ISA	level.	Popular
families,	such	as	x86-64,	have	processors	supplied
by	multiple	manufacturers.	Thus,	the	ISA	provides	a</p>
<p>conceptual	layer	of	abstraction	between	compiler
writers,	who	need	only	know	what	instructions	are
permitted	and	how	they	are	encoded,	and	processor
designers,	who	must	build	machines	that	execute
those	instructions.
In	this	chapter,	we	take	a	brief	look	at	the	design	of
processor	hardware.	We	study	the	way	a	hardware
system	can	execute	the	instructions	of	a	particular
ISA.	This	view	will	give	you	a	better	understanding
of	how	computers	work	and	the	technological
challenges	faced	by	computer	manufacturers.	One
important	concept	is	that	the	actual	way	a	modern
processor	operates	can	be	quite	different	from	the
model	of	computation	implied	by	the	ISA.	The	ISA
model	would	seem	to	imply	
sequential
instruction
execution,	where	each	instruction	is	fetched	and
executed	to	completion	before	the	next	one	begins.
By	executing	different	parts	of	multiple	instructions
simultaneously,	the	processor	can	achieve	higher
performance	than	if	it	executed	just	one	instruction
at	a	time.	Special	mechanisms	are	used	to	make
sure	the	processor	computes	the	same	results	as	it
would	with	sequential	execution.	This	idea	of	using
clever	tricks	to	improve	performance	while
maintaining	the	functionality	of	a	simpler	and	more
abstract	model	is	well	known	in	computer	science.
Examples	include	the	use	of	caching	in	Web</p>
<p>browsers	and	information	retrieval	data	structures
such	as	balanced	binary	trees	and	hash	tables.
Chances	are	you	will	never	design	your	own
processor.	This	is	a	task	for	experts	working	at
fewer	than	100	companies	worldwide.	Why,	then,
should	you	learn	about	processor	design?
It	is	intellectually	interesting	and	important.
There	is	an	intrinsic	value	in	learning	how	things
work.	It	is	especially	interesting	to	learn	the	inner
workings	of
Aside	
The	progress	of
computer	technology
To	get	a	sense	of	how	much	computer
technology	has	improved	over	the	past
four	decades,	consider	the	following	two
processors.
The	first	Cray	1	supercomputer	was
delivered	to	Los	Alamos	National
Laboratory	in	1976.	It	was	the	fastest
computer	in	the	world,	able	to	perform	as
many	as	250	million	arithmetic	operations
per	second.	It	came	with	8	megabytes	of
random	access	memory,	the	maximum</p>
<p>configuration	allowed	by	the	hardware.
The	machine	was	also	very	large—it
weighed	5,000	kg,	consumed	115
kilowatts,	and	cost	$9	million.	In	total,
around	80	of	them	were	manufactured.
The	Apple	ARM	A7	microprocessor	chip,
introduced	in	2013	to	power	the	iPhone
5S,	contains	two	CPUs,	each	of	which
can	perform	several	billion	arithmetic
operations	per	second,	and	1	gigabyte	of
random	access	memory.	The	entire
phone	weighs	just	112	grams,	consumes
around	1	watt,	and	costs	less	than	$800.
Over	9	million	units	were	sold	in	the	first
weekend	of	its	introduction.	In	addition	to
being	a	powerful	computer,	it	can	be	used
to	take	pictures,	to	place	phone	calls,	and
to	provide	driving	directions,	features
never	considered	for	the	Cray	1.
These	two	systems,	spaced	just	37	years
apart,	demonstrate	the	tremendous
progress	of	semiconductor	technology.
Whereas	the	Cray	l's	CPU	was
constructed	using	around	100,000
semiconductor	chips,	each	containing
less	than	20	transistors,	the	Apple	A7	has
over	1	billion	transistors	on	its	single	chip.
The	Cray	1's	8-megabyte	memory</p>
<p>required	8,192	chips,	whereas	the
iPhone's	gigabyte	memory	is	contained	in
a	single	chip.
a	system	that	is	such	a	part	of	the	daily	lives	of
computer	scientists	and	engineers	and	yet
remains	a	mystery	to	many.	Processor	design
embodies	many	of	the	principles	of	good
engineering	practice.	It	requires	creating	a
simple	and	regular	structure	to	perform	a
complex	task.
Understanding	how	the	processor	works	aids
in	understanding	how	the	overall	computer
system	works.	
In	
Chapter	
6
,	we	will	look	at
the	memory	system	and	the	techniques	used	to
create	an	image	of	a	very	large	memory	with	a
very	fast	access	time.	Seeing	the	processor	side
of	the	processor-memory	interface	will	make	this
presentation	more	complete.
Although	few	people	design	processors,
many	design	hardware	systems	that	contain
processors.	
This	has	become	commonplace	as
processors	are	embedded	into	real-world
systems	such	as	automobiles	and	appliances.
Embedded-system	designers	must	understand
how	processors	work,	because	these	systems
are	generally	designed	and	programmed	at	a
lower	level	of	abstraction	than	is	the	case	for
desktop	and	server-based	systems.
You	just	might	work	on	a	processor	design.</p>
<p>You	just	might	work	on	a	processor	design.
Although	the	number	of	companies	producing
microprocessors	is	small,	the	design	teams
working	on	those	processors	are	already	large
and	growing.	There	can	be	over	1,000	people
involved	in	the	different	aspects	of	a	major
processor	design.
In	this	chapter,	we	start	by	defining	a	simple
instruction	set	that	we	use	as	a	running	example	for
our	processor	implementations.	We	call	this	the
&quot;Y86-64&quot;	
instruction	set,	because	it	was	inspired	by
the	x86-64	instruction	set.	Compared	with	x86-64,
the	Y86-64	instruction	set	has	fewer	data	types,
instructions,	and	addressing	modes.	It	also	has	a
simple	byte-level	encoding,	making	the	machine
code	less	compact	than	the	comparable	x86-64
code,	but	also	much	easier	to	design	the	CPU's
decoding	logic.	Even	though	the	Y86-64	instruction
set	is	very	simple,	it	is	sufficiently	complete	to	allow
us	to	write	programs	manipulating	integer	data.
Designing	a	processor	to	implement	Y86-64
requires	us	to	deal	with	many	of	the	challenges
faced	by	processor	designers.
We	then	provide	some	background	on	digital
hardware	design.	We	describe	the	basic	building
blocks	used	in	a	processor	and	how	they	are
connected	together	and	operated.	This	presentation</p>
<p>builds	on	our	discussion	of	Boolean	algebra	and	bit-
level	operations	from	
Chapter	
2
.	We	also
introduce	a	simple	language,	HCL	(for	&quot;hardware
control	language&quot;),	to	describe	the	control	portions
of	hardware	systems.	We	will	later	use	this
language	to	describe	our	processor	designs.	Even	if
you	already	have	some	background	in	logic	design,
read	this	section	to	understand	our	particular
notation.
As	a	first	step	in	designing	a	processor,	we	present
a	functionally	correct,	but	somewhat	impractical,
Y86-64	processor	based	on	
sequential
operation.
This	processor	executes	a	complete	Y86-64
instruction	on	every	clock	cycle.	The	clock	must	run
slowly	enough	to	allow	an	entire	series	of	actions	to
complete	within	one	cycle.	Such	a	processor	could
be	implemented,	but	its	performance	would	be	well
below	what	could	be	achieved	for	this	much
hardware.
With	the	sequential	design	as	a	basis,	we	then	apply
a	series	of	transformations	to	create	a	
pipelined
processor.	This	processor	breaks	the	execution	of
each	instruction	into	five	steps,	each	of	which	is
handled	by	a	separate	section	or	
stage
of	the
hardware.	Instructions	progress	through	the	stages
of	the	pipeline,	with	one	instruction	entering	the
pipeline	on	each	clock	cycle.	As	a	result,	the</p>
<p>processor	can	be	executing	the	different	steps	of	up
to	five	instructions	simultaneously.	Making	this
processor	preserve	the	sequential	behavior	of	the
Y86-64	ISA	requires	handling	a	variety	of	
hazard
conditions,	where	the	location	or	operands	of	one
instruction	depend	on	those	of	other	instructions
that	are	still	in	the	pipeline.
We	have	devised	a	variety	of	tools	for	studying	and
experimenting	with	our	processor	designs.	These
include	an	assembler	for	Y86-64,	a	simulator	for
running	Y86-64	programs	on	your	machine,	and
simulators	for	two	sequential	and	one	pipelined
processor	design.	The	control	logic	for	these
designs	is	described	by	files	in	HCL	notation.	By
editing	these	files	and	recompiling	the	simulator,	you
can	alter	and	extend	the	simulator's	behavior.	A
number	of	exercises	are	provided	that	involve
implementing	new	instructions	and	modifying	how
the	machine	processes	instructions.	Testing	code	is
provided	to	help	you	evaluate	the	correctness	of
your	modifications.	These	exercises	will	greatly	aid
your	understanding	of	the	material	and	will	give	you
an	appreciation	for	the	many	different	design
alternatives	faced	by	processor	designers.
Web	Aside	
ARCH
:
VLOG</p>
<p>on	page	467	presents	a
representation	of	our	pipelined	Y86-64	processor	in
the	Verilog	hardware	description	language.	This</p>
<p>involves	creating	modules	for	the	basic	hardware
building	blocks	and	for	the	overall	processor
structure.	We	automatically	translate	the	HCL
description	of	the	control	
logic	into	Verilog.	By	first
debugging	the	HCL	description	with	our	simulators,
we	eliminate	many	of	the	tricky	bugs	that	would
otherwise	show	up	in	the	hardware	design.	Given	a
Verilog	description,	there	are	commercial	and	open-
source	tools	to	support	simulation	and	
logic
synthesis
,	generating	actual	circuit	designs	for	the
microprocessors.	So,	although	much	of	the	effort	we
expend	here	is	to	create	pictorial	and	textual
descriptions	of	a	system,	much	as	one	would	when
writing	software,	the	fact	that	these	designs	can	be
automatically	synthesized	demonstrates	that	we	are
indeed	creating	a	system	that	can	be	realized	as
hardware.</p>
<p>4.1	
The	Y86-64	Instruction	Set
Architecture
Defining	an	instruction	set	architecture,	such	as	Y86-64,	includes	defining
the	different	components	of	its	state,	the	set	of	instructions	and	their
encodings,	a	set	of	programming	conventions,	and	the	handling	of
exceptional	events.
4.1.1	
Programmer-Visible	State
As	
Figure	
4.1
illustrates,	each	instruction	in	a	Y86-64	program	can
read	and	modify	some	part	of	the	processor	state.	This	is	referred	to	as
the	
programmer-visible
state,	where	the	&quot;programmer&quot;	in	this	case	is
either	someone	writing	programs	in	assembly	code	or	a	compiler
generating	machine-level	code.	We	will	see	in	our	processor
implementations	that	we	do	not	need	to	represent	and	organize	this	state
in	exactly	the	manner	implied	by	the	ISA,	as	long	as	we	can	make	sure
that	machine-level	programs	appear	to	have	access	to	the	programmer-
visible	state.	The	state	for	Y86-64	is	similar	to	that	for	x86-64.	There	are
15	
program	registers:</p>
<pre><code>through	
(We	omit	the	x86-64	register	
15	to	simplify	the
</code></pre>
<p>instruction	encoding.)	Each	of	these	stores	a	64-bit	word.	Register	
is	used	as	a	stack	pointer	by	the	push,	pop,	call,	and	return	instructions.</p>
<p>Otherwise,	the	registers	have	no	fixed	meanings	or	values.	There	are
three	single-bit	
condition	codes
,	
,	and	
,	storing	information
Figure	
4.1	
Y86-64	programmer-visible	state.
As	with	x86-64,	programs	for	Y86-64	access	and	modify	the	program
registers,	the	condition	codes,	the	program	counter	(PC),	and	the
memory.	The	status	code	indicates	whether	the	program	is	running
normally	or	some	special	event	has	occurred.
about	the	effect	of	the	most	recent	arithmetic	or	logical	instruction.	The
program	counter	(PC)	holds	the	address	of	the	instruction	currently	being
executed.
The	
memory
is	conceptually	a	large	array	of	bytes,	holding	both	program
and	data.	Y86-64	programs	reference	memory	locations	using	
virtual
addresses.
A	combination	of	hardware	and	operating	system	software
translates	these	into	the	actual,	or	
physical
,	addresses	indicating	where
the	values	are	actually	stored	in	memory.	We	will	study	virtual	memory	in
more	detail	in	
Chapter	
9
.	For	now,	we	can	think	of	the	virtual	memory
system	as	providing	Y86-64	programs	with	an	image	of	a	monolithic	byte
array.</p>
<p>A	final	part	of	the	program	state	is	a	status	code	Stat,	indicating	the
overall	state	of	program	execution.	It	will	indicate	either	normal	operation
or	that	some	sort	of	
exception
has	occurred,	such	as	when	an	instruction
attempts	to	read	from	an	invalid	memory	address.	The	possible	status
codes	and	the	handling	of	exceptions	is	described	in	
Section	
4.1.4
.
4.1.2	
Y86-64	Instructions
Figure	
4.2
gives	a	concise	description	of	the	individual	instructions	in
the	Y86-64	ISA.	We	use	this	instruction	set	as	a	target	for	our	processor
implementations.	The	set	of	Y86-64	instructions	is	largely	a	subset	of	the
x86-64	instruction	set.	It	includes	only	8-byte	integer	operations,	has
fewer	addressing	modes,	and	includes	a	smaller	set	of	operations.	Since
we	only	use	8-byte	data,	we	can	refer	to	these	as	&quot;words&quot;	without	any
ambiguity.	In	this	figure,	we	show	the	assembly-code	representation	of
the	instructions	on	the	left	and	the	byte	encodings	on	the	right.	
Figure
4.3
shows	further	details	of	some	of	the	instructions.	The	assembly-
code	format	is	similar	to	the	ATT	format	for	x86-64.
Here	are	some	details	about	the	Y86-64	instructions.
The	x86-64	
instruction	is	split	into	four	different	instructions:
,	and	
,	explicitly	indicating	the	form	of
the	source	and	destination.	The	source	is	either	immediate	(
),
register	(
),	or	memory	(
).	It	is	designated	by	the	first	character	in
the	instruction	name.	The	destination	is	either	register	(
)	or	memory
(
).	It	is	designated	by	the	second	character	in	the	instruction	name.</p>
<p>Explicitly	identifying	the	four	types	of	data	transfer	will	prove	helpful
when	we	decide	how	to	implement	them.
The	memory	references	for	the	two	memory	movement	instructions
have	a	simple	base	and	displacement	format.	We	do	not	support	the
second	index	register	or	any	scaling	of	a	register's	value	in	the
address	computation.
As	with	x86-64,	we	do	not	allow	direct	transfers	from	one	memory
location	to	another.	In	addition,	we	do	not	allow	a	transfer	of
immediate	data	to	memory.
There	are	four	integer	operation	instructions,	shown	in	
Figure	
4.2
as	
These	are	
,	and	
.	They	operate	only	on
register	data,	whereas	x86-64	also	allows	operations	on	memory
data.	These	instructions	set	the	three	condition	codes	
,	and	
(zero,	sign,	and	overflow).
Figure	
4.2	
Y86-64	instruction	set.
Instruction	encodings	range	between	1	and	10	bytes.	An	instruction
consists	of	a	1-byte	instruction	specifier,	possibly	a	1	-byte	register
specifier,	and	possibly	an	8-byte	constant	word.	Field	
specifies	a</p>
<p>particular	integer	operation	(
),	data	movement	condition	(
),
or	branch	condition	(
).	All	numeric	values	are	shown	in
hexadecimal.
The	seven	jump	instructions	(shown	in	
Figure	
4.2
as	
)	are	
,	and	
.	Branches	are	taken	according	to	the
type	of	branch	and	the	settings	of	the	condition	codes.	The	branch
conditions	are	the	same	as	with	x86-64	(
Figure	
3.15
).
There	are	six	conditional	move	instructions	(shown	in	
Figure	
4.2
as
,	and	
.	These
have	the	same	format	as	the	register-register	move	instruction
,	but	the	destination	register	is	updated	only	if	the	condition
codes	satisfy	the	required	constraints.
The	call	instruction	pushes	the	return	address	on	the	stack	and	jumps
to	the	destination	address.	The	
instruction	returns	from	such	a
call.
The	
and	
instructions	implement	push	and	pop,	just	as
they	do	in	x86-64.
The	
instruction	stops	instruction	execution.	x86-64	has	a
comparable	instruction,	called	
.	x86-64	application	programs	are
not	permitted	to	use	
this	instruction,	since	it	causes	the	entire	system
to	suspend	operation.	For	Y86-64,	executing	the	
instruction
causes	the	processor	to	stop,	with	the	status	code	set	to	
.	(See
Section	
4.1.4
.)
4.1.3	
Instruction	Encoding</p>
<p>Figure	
4.2
also	shows	the	byte-level	encoding	of	the	instructions.
Each	instruction	requires	between	1	and	10	bytes,	depending	on	which
fields	are	required.	Every	instruction	has	an	initial	byte	identifying	the
instruction	type.	This	byte	is	split	into	two	4-bit	parts:	the	high-order,	or
code
,	part,	and	the	low-order,	or	
function
,	part.	As	can	be	seen	in	
Figure
4.2
,	code	values	range	from	
to	
.	The	function	values	are
significant	only	for	the	cases	where	a	group	of	related	instructions	share
a	common	code.	These	are	given	in	
Figure	
4.3
,	showing	the	specific
encodings	of	the	integer	operation,	branch,	and	conditional	move
instructions.	Observe	that	
has	the	same	instruction	code	as	the
conditional	moves.	It	can	be	viewed	as	an	&quot;unconditional	move&quot;	just	as
the	
instruction	is	an	unconditional	jump,	both	having	function	code	
.
As	shown	in	
Figure	
4.4
,	each	of	the	15	program	registers	has	an
associated	
register	identifier
(ID)	ranging	from	
to	
.	The	numbering
of	registers	in	Y86-64	matches	what	is	used	in	x86-64.	The	program
registers	are	stored	within	the	CPU	in	a	
register	file
,	a	small	random
access	memory	where	the	register	IDs	serve	as	addresses.	ID	value	
is	used	in	the	instruction	encodings	and	within	our	hardware	designs
when	we	need	to	indicate	that	no	register	should	be	accessed.
Some	instructions	are	just	1	byte	long,	but	those	that	require	operands
have	longer	encodings.	First,	there	can	be	an	additional	
register	specifier
byte
,	specifying	either	one	or	two	registers.	These	register	fields	are
called	rA	and	rB	in	
Figure	
4.2
.	As	the	assembly-code	versions	of	the
instructions	show,	they	can	specify	the	registers	used	for	data	sources
and	destinations,	as	well	as	the	base	register	used	in	an	address
computation,	depending	on	the	instruction	type.	Instructions	that	have	no
register	operands,	such	as	branches	and	call,	do	not	have	a	register</p>
<p>specifier	byte.	Those	that	require	just	one	register	operand	(
,	and	
)	have
Figure	
4.3	
Function	codes	for	Y86-64	instruction	set.
The	code	specifies	a	particular	integer	operation,	branch	condition,	or
data	transfer	condition.	These	instructions	are	shown	as	
,	and
in	
Figure	
4.2
.
Number
Register	name
Number
Register	name
No	register
Figure	
4.4	
Y86-64	program	register	identifiers.</p>
<p>Each	of	the	1	5	program	registers	has	an	associated	identifier	(ID)
ranging	from	
to	
.	ID	
in	a	register	field	of	an	instruction	indicates
the	absence	of	a	register	operand.
the	other	register	specifier	set	to	value	
.	This	convention	will	prove
useful	in	our	processor	implementation.
Some	instructions	require	an	additional	8-byte	
constant	word.
This	word
can	serve	as	the	immediate	data	for	
,	the	displacement	for	
and	
address	specifiers,	and	the	destination	of	branches	and	calls.
Note	that	branch	and	call	destinations	are	given	as	absolute	addresses,
rather	than	using	the	PC-relative	addressing	seen	in	x86-64.	Processors
use	PC-relative	addressing	to	give	more	compact	encodings	of	branch
instructions	and	to	allow	code	to	be	shifted	from	one	part	of	memory	to
another	without	the	need	to	update	all	of	the	branch	target	addresses.
Since	we	are	more	concerned	with	simplicity	in	our	presentation,	we	use
absolute	addressing.	As	with	x86-64,	all	integers	have	a	little-endian
encoding.	When	the	instruction	is	written	in	disassembled	form,	these
bytes	appear	in	reverse	order.
As	an	example,	let	us	generate	the	byte	encoding	of	the	instruction
in	hexadecimal.	From	
Figure	
4.2
,
we	can	see	that	
has	initial	byte	40.	We	can	also	see	that	source
register	
should	be	encoded	in	the	rA	field,	and	base	register	
should	be	encoded	in	the	rB	field.	Using	the	register	numbers	in	
Figure
4.4
,	we	get	a	register	specifier	byte	of	42.	Finally,	the	displacement	is
encoded	in	the	8-byte	constant	word.	We	first	pad	
with
leading	zeros	to	fill	out	8	bytes,	giving	a	byte	sequence	of	
.	We	write	this	in	byte-reversed	order	as	</p>
<p>.	Combining	these,	we	get	an	instruction	encoding	of
One	important	property	of	any	instruction	set	is	that	the	byte	encodings
must	have	a	unique	interpretation.	An	arbitrary	sequence	of	bytes	either
encodes	a	unique	instruction	sequence	or	is	not	a	legal	byte	sequence.
This	property	holds	for	Y86-64,	because	every	instruction	has	a	unique
combination	of	code	and	function	in	its	initial	byte,	and	given	this	byte,	we
can	determine	the	length	and	meaning	of	any	additional	bytes.	This
property	ensures	that	a	processor	can	execute	an	object-code	program
without	any	ambiguity	about	the	meaning	of	the	code.	Even	if	the	code	is
embedded	within	other	bytes	in	the	program,	we	can	readily	determine
Aside	
Comparing	x86-64	to	Y86-64
instruction	encodings
Compared	with	the	instruction	encodings	
used	in	x86-64,	the
encoding	of	Y86-64	is	much	simpler	but	also	less	compact.	The
register	fields	occur	only	in	fixed	positions	in	all	Y86-64
instructions,	whereas	they	are	packed	into	various	positions	in	the
different	x86-64	instructions.	An	x86-64	instruction	can	encode
constant	values	in	1,	2,	4,	or	8	bytes,	whereas	Y86-64	always
requires	8	bytes.
the	instruction	sequence	as	long	as	we	start	from	the	first	byte	in	the
sequence.	On	the	other	hand,	if	we	do	not	know	the	starting	position	of	a
code	sequence,	we	cannot	reliably	determine	how	to	split	the	sequence
into	individual	instructions.	This	causes	problems	for	disassemblers	and</p>
<p>other	tools	that	attempt	to	extract	machine-level	programs	directly	from
object-code	byte	sequences.
Practice	Problem	
4.1	
(solution	page	
480
)
Determine	the	byte	encoding	of	the	Y86-64	instruction	sequence
that	follows.	The	line	
indicates	that	the	starting	address
of	the	object	code	should	be	
Practice	Problem	
4.2	
(solution	page	
481
)
For	each	byte	sequence	listed,	determine	the	Y86-64	instruction
sequence	it	encodes.	If	there	is	some	invalid	byte	in	the	sequence,
show	the	instruction	sequence	up	to	that	point	and	indicate	where
the	invalid	value	occurs.	For	each	sequence,	we	show	the	starting
address,	then	a	colon,	and	then	the	byte	sequence.
A.	
B.	
C.	</p>
<p>D.	
E.	
Aside	
RISC	and	CISC	instruction	sets
x86-64	is	sometimes	labeled	as	a	&quot;complex	instruction	set
computer&quot;	(CISC—pronounced	&quot;sisk&quot;),	and	is	deemed	to	be	the
opposite	of	ISAs	that	are	classified	as	&quot;reduced	instruction	set
computers&quot;	(RISC—pronounced	&quot;risk&quot;).	Historically,	CISC
machines	came	first,	having	evolved	from	the	earliest	computers.
By	the	early	1980s,	instruction	sets	for	mainframe	and
minicomputers	had	grown	quite	large,	as	machine	designers
incorporated	new	instructions	to	support	high-level	tasks,	such	as
manipulating	circular	buffers,	performing	decimal	arithmetic,	and
evaluating	polynomials.	The	first	microprocessors	appeared	in	the
early	1970s	and	had	limited	instruction	sets,	because	the
integrated-circuit	technology	then	posed	severe	constraints	on
what	could	be	implemented	on	a	single	chip.	Microprocessors
evolved	quickly	and,	by	the	early	1980s,	were	following	the	same
path	of	increasing	instruction	set	complexity	that	had	been	the
case	for	mainframes	and	minicomputers.	The	x86	family	took	this
path,	evolving	into	IA32,	and	more	recently	into	x86-64.	The	x86
line	continues	to	evolve	as	new	classes	of	instructions	are	added
based	on	the	needs	of	emerging	applications.
The	RISC	design	philosophy	developed	in	the	early	1980s	as	an
alternative	to	these	trends.	A	group	of	hardware	and	compiler
experts	at	IBM,	strongly	influenced	by	the	ideas	of	IBM	researcher
John	Cocke,	recognized	that	they	could	generate	efficient	code	for
a	much	simpler	form	of	instruction	set.	In	fact,	many	of	the	high-</p>
<p>level	instructions	that	were	being	added	to	instruction	sets	were
very	difficult	to	generate	with	a	compiler	and	were	seldom	used.	A
simpler	instruction	set	could	be	implemented	with	much	less
hardware	and	could	be	organized	in	an	efficient	pipeline	structure,
similar	to	those	described	later	in	this	chapter.	IBM	did	not
commercialize	this	idea	until	many	years	later,	when	it	developed
the	Power	and	PowerPC	ISAs.
The	RISC	concept	was	further	developed	by	Professors	David
Patterson,	of	the	University	of	California	at	Berkeley,	and	John
Hennessy,	of	Stanford	University.	Patterson	gave	the	name	RISC
to	this	new	class	of	machines,	and	CISC	to	the	existing	class,
since	there	had	previously	been	no	need	to	have	a	special
designation	for	a	nearly	universal	form	of	instruction	set.
When	comparing	CISC	with	the	original	RISC	instruction	sets,	we
find	the	following	general	characteristics:
CISC
Early	RISC
A	large	number	of	instructions.
The	Intel	document	describing
the	complete	set	of	instructions
[51]
is	over	1,200	pages	long.
Many	fewer	instructions—typically	less	than	100.
Some	instructions	with	long
execution	times.	These	include
instructions	that	copy	an	entire
block	from	one	part	of	memory
to	another	and	others	that	copy
multiple	registers	to	and	from
memory.
No	instruction	with	a	long	execution	time.	Some
early	RISC	machines	did	not	even	have	an
integer	multiply	instruction,	requiring	compilers	to
implement	multiplication	as	a	sequence	of
additions.</p>
<p>Variable-size	encodings.	x86-64
instructions	can	range	from	1	to
15	bytes.
Fixed-length	encodings.	Typically	all	instructions
are	encoded	as	4	bytes.
Multiple	formats	for	specifying
operands.	In	x86-64,	a	memory
operand	specifier	can	have
many	different	combinations	of
displacement,	base	and	index
registers,	and	scale	factors.
Simple	addressing	formats.	Typically	just	base
and	displacement	addressing.
Arithmetic	and	logical
operations	can	be	applied	to
both	memory	and	register
operands.
Arithmetic	and	logical	operations	only	use
register	operands.	Memory	referencing	is	only
allowed	by	
load
instructions,	reading	from
memory	into	a	register,	and	
store
instructions,
writing	from	a	register	to	memory.	This
convention	is	referred	to	as	a	
load/store
architecture.
Implementation	artifacts	hidden
from	machine-level	programs.
The	ISA	provides	a	clean
abstraction	between	programs
and	how	they	get	executed.
Implementation	artifacts	exposed	to	machine-
level	programs.	Some	RISC	machines	prohibit
particular	instruction	sequences	and	have	jumps
that	do	not	take	effect	until	the	following
instruction	is	executed.	The	compiler	is	given	the
task	of	optimizing	performance	within	these
constraints.
Condition	codes.	Special	flags
are	set	as	a	side	effect	of
instructions	and	then	used	for
conditional	branch	testing.
No	condition	codes.	Instead,	explicit	test
instructions	store	the	test	results	in	normal
registers	for	use	in	conditional	evaluation.
Stack-intensive	procedure
linkage.	The	stack	is	used	for
procedure	arguments	and
return	addresses.
Register-intensive	procedure	linkage.	Registers
are	used	for	procedure	arguments	and	return
addresses.	Some	procedures	can	thereby	avoid
any	memory	references.	Typically,	the	processor</p>
<p>has	many	more	(up	to	32)	registers.
The	Y86-64	instruction	set	includes	attributes	of	both	CISC	and
RISC	instruction	sets.	On	the	CISC	side,	it	has	condition	codes
and	variable-length	instructions,	and	it	uses	the	stack	to	store
return	addresses.	On	the	RISC	side,	it	uses	a	load/store
architecture	and	a	regular	instruction	encoding,	and	it	passes
procedure	arguments	through	registers.	It	can	be	viewed	as	taking
a	CISC	instruction	set	(x86)	and	simplifying	it	by	applying	some	of
the	principles	of	RISC.
Aside	
The	RISC	versus	CISC
controversy
Through	the	1980s,	battles	raged	in	the	computer	architecture
community	regarding	the	merits	of	RISC	versus	CISC	instruction
sets.	Proponents	of	RISC	claimed	they	could	get	more	computing
power	for	a	given	amount	of	hardware	through	a	combination	of
streamlined	instruction	set	design,	advanced	compiler	technology,
and	pipelined	processor	implementation.	CISC	proponents
countered	that	fewer	CISC	instructions	were	required	to	perform	a
given	task,	and	so	their	machines	could	achieve	higher	overall
performance.
Major	companies	introduced	RISC	processor	lines,	including	Sun
Microsystems	(SPARC),	IBM	and	Motorola	(PowerPC),	and	Digital
Equipment	Corporation	(Alpha).	A	British	company,	Acorn
Computers	Ltd.,	developed	its	own	architecture,	ARM	(originally
an	acronym	for	&quot;Acorn	RISC	machine&quot;),	which	has	become	widely
used	in	embedded	applications,	such	as	cell	phones.</p>
<p>In	the	early	1990s,	the	debate	diminished	as	it	became	clear	that
neither	RISC	nor	CISC	in	their	purest	forms	were	better	than
designs	that	incorporated	the	best	ideas	of	both.	RISC	machines
evolved	and	introduced	more	instructions,	many	of	which	take
multiple	cycles	to	execute.	RISC	machines	today	have	hundreds
of	instructions	in	their	repertoire,	hardly	fitting	the	name	&quot;reduced
instruction	set	machine.&quot;	The	idea	of	exposing	implementation
artifacts	to	machine-level	programs	proved	to	be	shortsighted.	As
new	processor	models	were	developed	using	more	advanced
hardware	structures,	many	of	these	artifacts	became	irrelevant,
but	they	still	remained	part	of	the	instruction	set.	Still,	the	core	of
RISC	design	is	an	instruction	set	that	is	well	suited	to	execution	on
a	pipelined	machine.
More	recent	CISC	machines	also	take	advantage	of	high-
performance	pipeline	structures.	As	we	will	discuss	in	
Section
5.7
,	they	fetch	the	CISC	instructions	and	dynamically	translate
them	into	a	sequence	of	simpler,	RISC-like	operations.	For
example,	an	instruction	that	adds	a	register	to	memory	is
translated	into	three	operations:	one	to	read	the	original	memory
value,	one	to	perform	the	addition,	and	a	third	to	write	the	sum	to
memory.	Since	the	dynamic	translation	can	generally	be
performed	well	in	advance	of	the	actual	instruction	execution,	the
processor	can	sustain	a	very	high	execution	rate.
Marketing	issues,	apart	from	technological	ones,	have	also	played
a	major	role	in	determining	the	success	of	different	instruction
sets.	By	maintaining	compatibility	with	its	existing	processors,	Intel
with	x86	made	it	easy	to	keep	moving	from	one	generation	of
processor	to	the	next.	As	integrated-circuit	technology	improved,
Intel	and	other	x86	processor	manufacturers	could	overcome	the</p>
<p>inefficiencies	created	by	the	original	8086	instruction	set	design,
using	RISC	techniques	to	produce	performance	comparable	to	the
best	RISC	machines.	As	we	saw	in	
Section	
3.1
,	the	evolution
of	IA32	into	x86-64	provided	an	opportunity	to	incorporate	several
features	of	RISC	into	the	x86	family.	In	the	areas	of	desktop,
laptop,	and	server-based	computing,	x86	has	achieved	near	total
domination.
RISC	processors	have	done	very	well	in	the	market	for	
embedded
processors
,	controlling	such	systems	as	cellular	telephones,
automobile	brakes,	and	Internet	appliances.	In	these	applications,
saving	on	cost	and	power	is	more	important	than	maintaining
backward	compatibility.	In	terms	of	the	number	of	processors	sold,
this	is	a	very	large	and	growing	market.
4.1.4	
Y86-64	Exceptions
The	programmer-visible	state	for	Y86-64	(
Figure	
4.1
)	includes	a	status
code	Stat	describing	the	overall	state	of	the	executing	program.	The
possible	values	for	this	code	are	shown	in	
Figure	
4.5
.	Code	value	1,
named	
,	indicates	that	the	program
Value
Name
Meaning
1
Normal	operation
2
halt	instruction	encountered
3
Invalid	address	encountered</p>
<p>4
Invalid	instruction	encountered
Figure	
4.5	
Y86-64	status	codes.
In	our	design,	the	processor	halts	for	any	code	other	than	
.
is	executing	normally,	while	the	other	codes	indicate	that	some	type	of
exception
has	occurred.	Code	2,	named	
T,	indicates	that	the	processor
has	executed	a	
instruction.	Code	3,	named	
,	indicates	that	the
processor	attempted	to	read	from	or	write	to	an	invalid	memory	address,
either	while	fetching	an	instruction	or	while	reading	or	writing	data.	We
limit	the	maximum	address	(the	exact	limit	varies	by	implementation),	and
any	access	to	an	address	beyond	this	limit	will	trigger	an	
exception.
Code	4,	named	
,	indicates	that	an	invalid	instruction	code	has	been
encountered.
For	Y86-64,	we	will	simply	have	the	processor	stop	executing	instructions
when	it	encounters	any	of	the	exceptions	listed.	In	a	more	complete
design,	the	processor	would	typically	invoke	an	
exception	handler
,	a
procedure	designated	to	handle	the	specific	type	of	exception
encountered.	As	described	in	
Chapter	
8
,	exception	handlers	can	be
configured	to	have	different	effects,	such	as	aborting	the	program	or
invoking	a	user-defined	
signal	handler.
4.1.5	
Y86-64	Programs
Figure	
4.6
shows	x86-64	and	Y86-64	assembly	code	for	the	following
C	function:</p>
<p>The	x86-64	code	was	generated	by	the	
GCC</p>
<p>compiler.	The	Y86-64	code	is
similar,	but	with	the	following	differences:
The	Y86-64	code	loads	constants	into	registers	(lines	2-3),	since	it
cannot	use	immediate	data	in	arithmetic	instructions.
x86-64	code</p>
<p>Y86-64	code
Figure	
4.6	
Comparison	of	Y86-64	and	x86-64	assembly	programs.
The	sum	function	computes	the	sum	of	an	integer	array.	The	Y86-64
code	follows	the	same	general	pattern	as	the	x86-64	code.
The	Y86-64	code	requires	two	instructions	(lines	8-9)	to	read	a	value
from	memory	and	add	it	to	a	register,	whereas	the	x86-64	code	can</p>
<p>do	this	with	a	single	
instruction	(line	5).
Our	hand-coded	Y86-64	implementation	takes	advantage	of	the
property	that	the	
instruction	(line	11)	also	sets	the	condition
codes,	and	so	the	
instruction	of	the	
GCC
-generated	code	(line	9)
is	not	required.	For	this	to	work,	though,	the	Y86-64	code	must	set	the
condition	codes	prior	to	entering	the	loop	with	an	
instruction	(line
5).
Figure	
4.7
shows	an	example	of	a	complete	program	file	written	in
Y86-64	assembly	code.	The	program	contains	both	data	and	instructions.
Directives	indicate	where	to	place	code	or	data	and	how	to	align	it.	The
program	specifies	issues	such	as	stack	placement,	data	initialization,
program	initialization,	and	program	termination.
In	this	program,	words	beginning	with	`.'	are	
assembler	directives
telling
the	assembler	to	adjust	the	address	at	which	it	is	generating	code	or	to
insert	some	words	of	data.	The	directive	.
(line	2)	indicates	that	the
assembler	should	begin	generating	code	starting	at	address	0.	This	is	the
starting	address	for	all	Y86-64	programs.	The	next	instruction	(line	3)
initializes	the	stack	pointer.	We	can	see	that	the	label	stack	is	declared	at
the	end	of	the	program	(line	40),	to	indicate	address	
using	a	
directive	(line	39).	Our	stack	will	therefore	start	at	this	address	and	grow
toward	lower	addresses.	We	must	ensure	that	the	stack	does	not	grow	so
large	that	it	overwrites	the	code	or	other	program	data.
Lines	8	to	13	of	the	program	declare	an	array	of	four	words,	having	the
values</p>
<p>The	label	array	denotes	the	start	of	this	array,	and	is	aligned	on	an	8-byte
boundary	(using	the	.align	directive).	Lines	16	to	19	show	a	&quot;main&quot;
procedure	that	calls	the	function	sum	on	the	four-word	array	and	then
halts.
As	this	example	shows,	since	our	only	tool	for	creating	Y86-64	code	is	an
assembler,	the	programmer	must	perform	tasks	we	ordinarily	delegate	to
the	compiler,	linker,	and	run-time	system.	Fortunately,	we	only	do	this	for
small	programs,	for	which	simple	mechanisms	suffice.
Figure	
4.8
shows	the	result	of	assembling	the	code	shown	in	
Figure
4.7
by	an	assembler	we	call	
YAS
.	
The	assembler	output	is	in	ASCII
format	to	make	it	more	readable.	On	lines	of	the	assembly	file	that
contain	instructions	or	data,	the	object	code	contains	an	address,
followed	by	the	values	of	between	1	and	10	bytes.
We	have	implemented	an	
instruction	set	simulator
we	call	
YIS
,	the
purpose	of	which	is	to	model	the	execution	of	a	Y86-64	machine-code
program	without	attempting	to	model	the	behavior	of	any	specific
processor	implementation.	This	form	of	simulation	is	useful	for	debugging
programs	before	actual	hardware	is	available,	and	for	checking	the	result
of	either	simulating	the	hardware	or	running</p>
<p>Figure	
4.7	
Sample	program	written	in	Y86-64	assembly	code.
The	sum	function	is	called	to	compute	the	sum	of	a	four-element	array.</p>
<p>Figure	
4.8	
Output	of	
YAS</p>
<p>assembler.
Each	line	includes	a	hexadecimal	address	and	between	1	and	10	bytes
of	object	code.
the	program	on	the	hardware	itself.	Running	on	our	sample	object	code,
YIS</p>
<p>generates	the	following	output:
The	first	line	of	the	simulation	output	summarizes	the	execution	and	the
resulting	values	of	the	PC	and	program	status.	In	printing	register	and
memory	values,	it	only	prints	out	words	that	change	during	simulation,</p>
<p>either	in	registers	or	in	memory.	The	original	values	(here	they	are	all
zero)	are	shown	on	the	left,	and	the	final	values	are	shown	on	the	right.
We	can	see	in	this	output	that	register	
contains	
,
the	sum	of	the	4-element	array	passed	to	procedure	sum.	In	addition,	we
can	see	that	the	stack,	which	starts	at	address	
and	grows	toward
lower	addresses,	has	been	used,	causing	changes	to	words	of	memory
at	addresses	
.	The	maximum	address	for	executable	code	is
,	and	so	the	pushing	and	popping	of	values	on	the	stack	did	not
corrupt	the	executable	code.
Practice	Problem	
4.3	
(solution	page	
482
)
One	common	pattern	in	machine-level	programs	is	to	add	a	constant
value	to	a	register.	With	the	Y86-64	instructions	presented	thus	far,	this
requires	first	using	an	
instruction	to	set	a	register	to	the	constant,
and	then	an	
instruction	to	add	this	value	to	the	destination	register.
Suppose	we	want	to	add	a	new	instruction	
with	the	following
format:
This	instruction	adds	the	constant	value	V	to	register	rB.
Rewrite	the	Y86-64	
function	of	
Figure	
4.6
to	make	use	of	the
instruction.	In	the	original	version,	we	dedicated	registers	
and
to	hold	constant	values.	Now,	we	can	avoid	using	those	registers
altogether.</p>
<p>Practice	Problem	
4.4	
(solution	page	
482
)
Write	Y86-64	code	to	implement	a	recursive	sum	function	
,
based	on	the	following	C	code:
Use	the	same	argument	passing	and	register	saving	conventions
as	x86-64	code	does.	You	might	find	it	helpful	to	compile	the	C
code	on	an	x86-64	machine	and	then	translate	the	instructions	to
Y86-64.
Practice	Problem	
4.5	
(solution	page	
483
)
Modify	the	Y86-64	code	for	the	sum	function	(
Figure	
4.6
)	to
implement	a	function	
that	computes	the	sum	of	absolute
values	of	an	array.	Use	a	
conditional	jump
instruction	within	your
inner	loop.
Practice	Problem	
4.6	
(solution	page	
483
)
Modify	the	Y86-64	code	for	the	
function	(
Figure	
4.6
)	to
implement	a	function	
that	computes	the	sum	of	absolute</p>
<p>values	of	an	array.	Use	a	
conditional	move
instruction	within	your
inner	loop.
4.1.6	
Some	Y86-64	Instruction
Details
Most	Y86-64	instructions	transform	the	program	state	in	a	straightforward
manner,	and	so	defining	the	intended	effect	of	each	instruction	is	not
difficult.	Two	unusual	instruction	combinations,	however,	require	special
attention.
The	
instruction	both	decrements	the	stack	pointer	by	8	and	writes
a	register	value	to	memory.	It	is	therefore	not	totally	clear	what	the
processor	should	do	when	executing	the	instruction	
,	since	the
register	being	pushed	is	being	changed	by	the	same	instruction.	Two
different	conventions	are	possible:	(1)	push	the	original	value	of	
,	or
(2)	push	the	decremented	value	of	
.
For	the	Y86-64	processor,	let	us	adopt	the	same	convention	as	is	used
with	x86-64,	as	determined	in	the	following	problem.
Practice	Problem	
4.7	
(solution	page	
484
)
Let	us	determine	the	behavior	of	the	instruction	
for	an
x86-64	processor.	We	could	try	reading	the	Intel	documentation	on
this	instruction,	but	a	
simpler	approach	is	to	conduct	an</p>
<p>experiment	on	an	actual	machine.	The	C	compiler	would	not
normally	generate	this	instruction,	so	we	must	use	hand-generated
assembly	code	for	this	task.	Here	is	a	test	function	we	have	written
(Web	Aside	
ASM
:
EASM</p>
<p>on	page	178	describes	how	to	write
programs	that	combine	C	code	with	handwritten	assembly	code):
In	our	experiments,	we	find	that	function	
always	returns
0.	What	does	this	imply	about	the	behavior	of	the	instruction	
under	x86-64?
A	similar	ambiguity	occurs	for	the	instruction	
.	It	could	either	set
to	the	value	read	from	memory	or	to	the	incremented	stack	pointer.
As	with	
Problem	
4.7
,	let	us	run	an	experiment	to	determine	how	an
x86-64	machine	would	handle	this	instruction,	and	then	design	our	Y86-
64	machine	to	follow	the	same	convention.
Practice	Problem	
4.8	
(solution	page	
484
)</p>
<p>The	following	assembly-code	function	lets	us	determine	the
behavior	of	the	instruction	
for	x86-64:
We	find	this	function	always	returns	
.	What	does	this	imply
about	the	behavior	of	
What	other	Y86-64	instruction
would	have	the	exact	same	behavior?
Aside	
Getting	the	details	right:
Inconsistencies	across	x86	models
Practice	Problems	
4.7
and	
4.8
are	designed	to	help	us
devise	a	consistent	set	of	conventions	for	instructions	that	push	or
pop	the	stack	pointer.	There	seems	to	be	little	reason	why	one
would	want	to	perform	either	of	these	operations,	and	so	a	natural
question	to	ask	is,	&quot;Why	worry	about	such	picky	details?&quot;</p>
<p>Several	useful	lessons	can	be	learned	about	the	importance	of
consistency	from	the	following	excerpt	from	the	Intel
documentation	of	the	
PUSH</p>
<p>instruction	
[51]
:
For	IA-32	processors	from	the	Intel	286	on,	the	PUSH	ESP	instruction	pushes	the
value	of	the	ESP	register	as	it	existed	before	the	instruction	was	executed.	(This	is
also	true	for	Intel	64	architecture,	real-address	and	virtual-8086	modes	of	IA-32
architecture.)	For	the	Intel(r)	8086	processor,	the	PUSH	SP	instruction	pushes	the
new	value	of	the	SP	register	(that	is	the	value	after	it	has	been	decremented	by	2).
(PUSH	ESP	instruction.	Intel	Corporation.	50.)
Although	the	exact	details	of	this	note	may	be	difficult	to	follow,	we
can	see	that	it	states	that,	depending	on	what	mode	an	x86
processor	operates	under,	it	will	do	different	things	when
instructed	to	push	the	stack	pointer	register.	Some	modes	push
the	original	value,	while	others	push	the	decremented	value.
(Interestingly,	there	is	no	corresponding	ambiguity	about	popping
to	the	stack	pointer	register.)	There	are	two	drawbacks	to	this
inconsistency:
It	decreases	code	portability.	Programs	may	have	different
behavior	depending	on	the	processor	mode.	Although	the
particular	instruction	is	not	at	all	common,	even	the	potential
for	incompatibility	can	have	serious	consequences.
It	complicates	the	documentation.	As	we	see	here,	a	special
note	is	required	to	try	to	clarify	the	differences.	The
documentation	for	x86	is	already	complex	enough	without
special	cases	such	as	this	one.</p>
<p>We	conclude,	therefore,	that	working	out	details	in	advance	and
striving	for	complete	consistency	can	save	a	lot	of	trouble	in	the
long	run.</p>
<p>4.2	
Logic	Design	and	the	Hardware
Control	Language	HCL
In	hardware	design,	electronic	circuits	are	used	to	compute	functions	on
bits	and	to	store	bits	in	different	kinds	of	memory	elements.	Most
contemporary	circuit	technology	represents	different	bit	values	as	high	or
low	voltages	on	signal	wires.	In	current	technology,	logic	value	1	is
represented	by	a	high	voltage	of	around	1.0	volt,	while	logic	value	0	is
represented	by	a	low	voltage	of	around	0.0	volts.	Three	major
components	are	required	to	implement	a	digital	system:	
combinational
logic
to	compute	functions	on	the	bits,	
memory	elements
to	store	bits,
and	
clock	signals
to	regulate	the	updating	of	the	memory	elements.
In	this	section,	we	provide	a	brief	description	of	these	different
components.	We	also	introduce	HCL	(for	&quot;hardware	control	language&quot;),
the	language	that	we	use	to	describe	the	control	logic	of	the	different
processor	designs.	We	only	describe	HCL	informally	here.	A	complete
reference	for	HCL	can	be	found	in	Web	Aside	
ARCH
:
HCL</p>
<p>on	page	472.
Aside	
Modern	logic	design
At	one	time,	hardware	designers	created	circuit	designs	by
drawing	schematic	diagrams	of	logic	circuits	(first	with	paper	and
pencil,	and	later	with	computer	graphics	terminals).	Nowadays,
most	designs	are	expressed	in	a	
hardware	description	language
(HDL),	a	textual	notation	that	looks	similar	to	a	programming</p>
<p>language	but	that	is	used	to	describe	hardware	structures	rather
than	program	behaviors.	The	most	commonly	used	languages	are
Verilog,	having	a	syntax	similar	to	C,	and	VHDL,	having	a	syntax
similar	to	the	Ada	programming	language.	These	languages	were
originally	designed	for	creating	simulation	models	of	digital
circuits.	In	the	mid-1980s,	researchers	developed	
logic	synthesis
programs	that	could	generate	efficient	circuit	designs	from	HDL
descriptions.	There	are	now	a	number	of	commercial	synthesis
programs,	and	this	has	become	the	dominant	technique	for
generating	digital	circuits.	This	shift	from	hand-designed	circuits	to
synthesized	ones	can	be	likened	to	the	shift	from	writing	programs
in	assembly	code	to	writing	them	in	a	high-level	language	and
having	a	compiler	generate	the	machine	code.
Our	HCL	language	expresses	only	the	control	portions	of	a
hardware	design,	with	only	a	limited	set	of	operations	and	with	no
modularity.	As	we	will	see,	however,	the	control	logic	is	the	most
difficult	part	of	designing	a	microprocessor.	We	have	developed
tools	that	can	directly	translate	HCL	into	Verilog,	and	by	combining
this	code	with	Verilog	code	for	the	basic	hardware	units,	we	can
generate	HDL	descriptions	from	which	actual	working
microprocessors	can	be	synthesized.	By	carefully	separating	out,
designing,	and	testing	the	control	logic,	we	can	create	a	working
microprocessor	with	reasonable	effort.	Web	Aside	
ARCH
:
VLOG</p>
<p>on
page	467	describes	how	we	can	generate	Verilog	versions	of	a
Y86-64	processor.
Figure	
4.9	
Logic	gate	types.</p>
<p>Each	gate	generates	output	equal	to	some	Boolean	function	of	its	inputs.
4.2.1	
Logic	Gates
Logic	gates	are	the	basic	computing	elements	for	digital	circuits.	They
generate	an	output	equal	to	some	Boolean	function	of	the	bit	values	at
their	inputs.	
Figure	
4.9
shows	the	standard	symbols	used	for	Boolean
functions	
AND
,	
OR
,	and	
NOT
.	
HCL	expressions	are	shown	below	the	gates
for	the	operators	in	C	(
Section	
2.1.8
):	
for	
AND
,	||	for	
OR
,	and	!	for
NOT
.	
We	use	these	instead	of	the	bit-level	C	operators	
,	|,	and	~,
because	logic	gates	operate	on	single-bit	quantities,	not	entire	words.
Although	the	figure	illustrates	only	two-input	versions	of	the	
AND</p>
<p>and	
OR
gates,	it	is	common	to	see	these	being	used	as	
n
-way	operations	for	
n
&gt;
2.	We	still	write	these	in	HCL	using	binary	operators,	though,	so	the
operation	of	a	three-input	
AND</p>
<p>gate	with	inputs	a,	b,	and	c	is	described
with	the	HCL	expression	a	
b	
c.
Logic	gates	are	always	active.	If	some	input	to	a	gate	changes,	then
within	some	small	amount	of	time,	the	output	will	change	accordingly.
Figure	
4.10	
Combinational	circuit	to	test	for	bit	equality.
The	output	will	equal	1	when	both	inputs	are	0	or	both	are	1.</p>
<p>4.2.2	
Combinational	Circuits	and
HCL	Boolean	Expressions
By	assembling	a	number	of	logic	gates	into	a	network,	we	can	construct
computational	blocks	known	as	
combinational	circuits.
Several
restrictions	are	placed	on	how	the	networks	are	constructed:
Every	logic	gate	input	must	be	connected	to	exactly	one	of	the
following:	(1)	one	of	the	system	inputs	(known	as	a	
primary	input
),	(2)
the	output	connection	of	some	memory	element,	or	(3)	the	output	of
some	logic	gate.
The	outputs	of	two	or	more	logic	gates	cannot	be	connected	together.
Otherwise,	the	two	could	try	to	drive	the	wire	toward	different
voltages,	possibly	causing	an	invalid	voltage	or	a	circuit	malfunction.
The	network	must	be	
acyclic.
That	is,	there	cannot	be	a	path	through
a	series	of	gates	that	forms	a	loop	in	the	network.	Such	loops	can
cause	ambiguity	in	the	function	computed	by	the	network.
Figure	
4.10
shows	an	example	of	a	simple	combinational	circuit	that
we	will	find	useful.	It	has	two	inputs,	a	and	b.	It	generates	a	single	output
eq,	such	that	the	output	will	equal	1	if	either	a	and	b	are	both	1	(detected
by	the	upper	
AND</p>
<p>gate)	or	are	both	0	(detected	by	the	lower	
AND</p>
<p>gate).	We
write	the	function	of	this	network	in	HCL	as</p>
<h2>This	code	simply	defines	the	bit-level	(denoted	by	data	type	
)	signal
eq	as	a	function	of	inputs	a	and	b.	As	this	example	shows,	HCL	uses	C-
style	syntax,	with	`='	associating	a	signal	name	with	an	expression.
Unlike	C,	however,	we	do	not	view	this	as	performing	a	computation	and
assigning	the	result	to	some	memory	location.	Instead,	it	is	simply	a	way
to	give	a	name	to	an	expression.
Practice	Problem	
4.9	
(solution	page	
484
)
Write	an	HCL	expression	for	a	signal	
,	equal	to	the	
EXCLUSIVE</h2>
<p>OR</p>
<p>of
inputs	a	and	b.	What	is	the	relation	between	the	signals	
and	eq
defined	above?
Figure	
4.11
shows	another	example	of	a	simple	but	useful
combinational	circuit	known	as	a	
multiplexor
(commonly	referred	to	as	a
&quot;MUX&quot;).	A	multiplexor
Figure	
4.11	
Single-bit	multiplexor	circuit.
The	output	will	equal	input	a	if	the	control	signal	s	is	1	and	will	equal	input
b	when	s	is	0.
selects	a	value	from	among	a	set	of	different	data	signals,	depending	on
the	value	of	a	control	input	signal.	In	this	single-bit	multiplexor,	the	two</p>
<p>data	signals	are	the	input	bits	a	and	b,	while	the	control	signal	is	the	input
bit	s.	The	output	will	equal	a	when	s	is	1,	and	it	will	equal	b	when	s	is	0.
In	this	circuit,	we	can	see	that	the	two	
AND</p>
<p>gates	determine	whether	to
pass	their	respective	data	inputs	to	the	
OR</p>
<p>gate.	The	upper	
AND</p>
<p>gate
passes	signal	b	when	s	is	0	(since	the	other	input	to	the	gate	is	!s),	while
the	lower	
AND</p>
<p>gate	passes	signal	a	when	s	is	1.	Again,	we	can	write	an
HCL	expression	for	the	output	signal,	using	the	same	operations	as	are
present	in	the	combinational	circuit:
Our	HCL	expressions	demonstrate	a	clear	parallel	between
combinational	logic	circuits	and	logical	expressions	in	C.	They	both	use
Boolean	operations	to	compute	functions	over	their	inputs.	Several
differences	between	these	two	ways	of	expressing	computation	are	worth
noting:
Since	a	combinational	circuit	consists	of	a	series	of	logic	gates,	it	has
the	property	that	the	outputs	continually	respond	to	changes	in	the
inputs.	If	some	input	to	the	circuit	changes,	then	after	some	delay,	the
outputs	will	change	accordingly.	By	contrast,	a	C	expression	is	only
evaluated	when	it	is	encountered	during	the	execution	of	a	program.
Logical	expressions	in	C	allow	arguments	to	be	arbitrary	integers,
interpreting	0	as	
FALSE</p>
<p>and	anything	else	as	
TRUE
.	
In	contrast,	our	logic
gates	only	operate	over	the	bit	values	0	and	1.
Logical	expressions	in	C	have	the	property	that	they	might	only	be
partially	evaluated.	If	the	outcome	of	an	
AND</p>
<p>or	
OR</p>
<p>operation	can	be</p>
<p>determined	by	just	evaluating	the	first	argument,	then	the	second
argument	will	not	be	evaluated.	For	example,	with	the	C	expression
the	function	
will	not	be	called,	because	the	expression	(
)
evaluates	to	0.	In	contrast,	combinational	logic	does	not	have	any	partial
evaluation	rules.	The	gates	simply	respond	to	changing	inputs.
Figure	
4.12	
Word-level	equality	test	circuit.
The	output	will	equal	1	when	each	bit	from	word	A	equals	its	counterpart
from	word	B.	Word-level	equality	is	one	of	the	operations	in	HCL.</p>
<p>4.2.3	
Word-Level	Combinational
Circuits	and	HCL	Integer
Expressions
By	assembling	large	networks	of	logic	gates,	we	can	construct
combinational	circuits	that	compute	much	more	complex	functions.
Typically,	we	design	circuits	that	operate	on	data	
words.
These	are
groups	of	bit-level	signals	that	represent	an	integer	or	some	control
pattern.	For	example,	our	processor	designs	will	contain	numerous
words,	with	word	sizes	ranging	between	4	and	64	bits,	representing
integers,	addresses,	instruction	codes,	and	register	identifiers.
Combinational	circuits	that	perform	word-level	computations	are
constructed	using	logic	gates	to	compute	the	individual	bits	of	the	output
word,	based	on	the	individual	bits	of	the	input	words.	For	example,
Figure	
4.12
shows	a	combinational	circuit	that	tests	whether	two	64-bit
words	A	and	B	are	equal.	That	is,	the	output	will	equal	1	if	and	only	if
each	bit	of	A	equals	the	corresponding	bit	of	B.	This	circuit	is
implemented	using	64	of	the	single-bit	equality	circuits	shown	in	
Figure
4.10
.	The	outputs	of	these	single-bit	circuits	are	combined	with	an	
AND
gate	to	form	the	circuit	output.
In	HCL,	we	will	declare	any	word-level	signal	as	an	
,	without
specifying	the	word	size.	This	is	done	for	simplicity.	In	a	full-featured
hardware	description	language,	every	word	can	be	declared	to	have	a
specific	number	of	bits.	HCL	allows	words	to	be	compared	for	equality,</p>
<h2>and	so	the	functionality	of	the	circuit	shown	in	
Figure	
4.12
can	be
expressed	at	the	word	level	as
where	arguments	A	and	B	are	of	type	int.	Note	that	we	use	the	same
syntax	conventions	as	in	C,	where	<code>='	denotes	assignment	and	</code>=='
denotes	the	equality	operator.
As	is	shown	on	the	right	side	of	
Figure	
4.12
,	we	will	draw	word-level
circuits	using	medium-thickness	lines	to	represent	the	set	of	wires
carrying	the	individual	bits	of	the	word,	and	we	will	show	a	single-bit
signal	as	a	dashed	line.
Practice	Problem	
4.10	
(solution	page	
484
)
Suppose	you	want	to	implement	a	word-level	equality	circuit	using	the
EXCLUSIVE</h2>
<p>OR</p>
<h2>circuits	from	
Problem	
4.9
rather	than	from	bit-level
equality	circuits.	Design	such	a	circuit	for	a	64-bit	word	consisting	of	64
bit-level	
EXCLUSIVE</h2>
<p>OR</p>
<p>circuits	and	two	additional	logic	gates.
Figure	
4.13
shows	the	circuit	for	a	word-level	multiplexor.	This	circuit
generates	a	64-bit	word	Out	equal	to	one	of	the	two	input	words,	A	or	B,
depending	on	the	control	input	bit	s.	The	circuit	consists	of	64	identical
subcircuits,	each	having	a	structure	similar	to	the	bit-level	multiplexor
from	
Figure	
4.11
.	Rather	than	replicating	the	bit-level	multiplexor	64
times,	the	word-level	version	reduces	the	number	of	inverters	by
generating	!s	once	and	reusing	it	at	each	bit	position.</p>
<p>Figure	
4.13	
Word-level	multiplexor	circuit.
The	output	will	equal	input	word	A	when	the	control	signal	s	is	1,	and	it
will	equal	B	otherwise.	Multiplexors	are	described	in	HCL	using	case
expressions.
We	will	use	many	forms	of	multiplexors	in	our	processor	designs.	They
allow	us	to	select	a	word	from	a	number	of	sources	depending	on	some
control	condition.	Multiplexing	functions	are	described	in	HCL	using	
case
expressions.
A	case	expression	has	the	following	general	form:</p>
<p>⋮
The	expression	contains	a	series	of	cases,	where	each	case	
i
consists	of
a	Boolean	expression	
select
,	indicating	when	this	case	should	be
selected,	and	an	integer	expression	
expr
,	indicating	the	resulting	value.
Unlike	the	switch	statement	of	C,	we	do	not	require	the	different	selection
expressions	to	be	mutually	exclusive.	Logically,	the	selection	expressions
are	evaluated	in	sequence,	and	the	case	for	the	first	one	yielding	1	is
selected.	For	example,	the	word-level	multiplexor	of	
Figure	
4.13
can
be	described	in	HCL	as
In	this	code,	the	second	selection	expression	is	simply	1,	indicating	that
this	case	should	be	selected	if	no	prior	one	has	been.	This	is	the	way	to
specify	a	default	case	in	HCL.	Nearly	all	case	expressions	end	in	this
manner.
Allowing	nonexclusive	selection	expressions	makes	the	HCL	code	more
readable.	An	actual	hardware	multiplexor	must	have	mutually	exclusive
signals	controlling	which	input	word	should	be	passed	to	the	output,	such
as	the	signals	s	and	!s	in	
Figure	
4.13
.	To	translate	an	HCL	case
i
i</p>
<p>expression	into	hardware,	a	logic	synthesis	program	would	need	to
analyze	the	set	of	selection	expressions	and	resolve	any	possible
conflicts	by	making	sure	that	only	the	first	matching	case	would	be
selected.
The	selection	expressions	can	be	arbitrary	Boolean	expressions,	and
there	can	be	an	arbitrary	number	of	cases.	This	allows	case	expressions
to	describe	blocks	where	there	are	many	choices	of	input	signals	with
complex	selection	criteria.	For	example,	consider	the	diagram	of	a	4-way
multiplexor	shown	in	
Figure	
4.14
.	This	circuit	selects	from	among	the
four	input	words	A,	B,	C,	and	D	based	on	the	control	signals	s1	and	s0,
treating	the	controls	as	a	2-bit	binary	number.	We	can	express	this	in
HCL	using	Boolean	expressions	to	describe	the	different	combinations	of
control	bit	patterns:
Figure	
4.14	
Four-way	multiplexor.
The	different	combinations	of	control	signals	s1	and	s0	determine	which
data	input	is	transmitted	to	the	output.</p>
<p>The	comments	on	the	right	(any	text	starting	with	#	and	running	for	the
rest	of	the	line	is	a	comment)	show	which	combination	of	s1	and	s0	will
cause	the	case	to	be	selected.	Observe	that	the	selection	expressions
can	sometimes	be	simplified,	since	only	the	first	matching	case	is
selected.	For	example,	the	second	expression	can	be	written	
,	rather
than	the	more	complete	
,	since	the	only	other	possibility	having
equal	to	0	was	given	as	the	first	selection	expression.	Similarly,	the
third	expression	can	be	written	as	
,	while	the	fourth	can	simply	be
written	as	1.
As	a	final	example,	suppose	we	want	to	design	a	logic	circuit	that	finds
the	minimum	value	among	a	set	of	words	A,	B,	and	C,	diagrammed	as
follows:
We	can	express	this	using	an	HCL	case	expression	as</p>
<p>Practice	Problem	
4.11	
(solution	page	
484
)
The	HCL	code	given	for	computing	the	minimum	of	three	words
contains	four	comparison	expressions	of	the	form	
X
&lt;=	
Y.
Rewrite
the	code	to	compute	the	same	result,	but	using	only	three
comparisons.
Figure	
4.15	
Arithmetic/logic	unit	(ALU).
Depending	on	the	setting	of	the	function	input,	the	circuit	will	perform	one
of	four	different	arithmetic	and	logical	operations.
Practice	Problem	
4.12	
(solution	page	
484
)
Write	HCL	code	describing	a	circuit	that	for	word	inputs	A,	B,	and
C	selects	the	
median
of	the	three	values.	That	is,	the	output
equals	the	word	lying	between	the	minimum	and	maximum	of	the
three	inputs.
Combinational	logic	circuits	can	be	designed	to	perform	many	different
types	of	operations	on	word-level	data.	The	detailed	design	of	these	is</p>
<p>beyond	the	scope	of	our	presentation.	One	important	combinational
circuit,	known	as	an	
arithmetic/logic	unit
(ALU),	is	diagrammed	at	an
abstract	level	in	
Figure	
4.15
.	In	our	version,	the	circuit	has	three
inputs:	two	data	inputs	labeled	A	and	B	and	a	control	input.	Depending	on
the	setting	of	the	control	input,	the	circuit	will	perform	different	arithmetic
or	logical	operations	on	the	data	inputs.	Observe	that	the	four	operations
diagrammed	for	this	ALU	correspond	to	the	four	different	integer
operations	supported	by	the	Y86-64	instruction	set,	and	the	control
values	match	the	function	codes	for	these	instructions	(
Figure	
4.3
).
Note	also	the	ordering	of	operands	for	subtraction,	where	the	A	input	is
subtracted	from	the	B	input.	This	ordering	is	chosen	in	anticipation	of	the
ordering	of	arguments	in	the	
instruction.
4.2.4	
Set	Membership
In	our	processor	designs,	we	will	find	many	examples	where	we	want	to
compare	one	signal	against	a	number	of	possible	matching	signals,	such
as	to	test	whether	the	code	for	some	instruction	being	processed
matches	some	category	of	instruction	codes.	As	a	simple	example,
suppose	we	want	to	generate	the	signals	s1	and	s0	for	the	4-way
multiplexor	of	
Figure	
4.14
by	selecting	the	high-	and	low-order	bits
from	a	2-bit	signal	code,	as	follows:</p>
<p>In	this	circuit,	the	2-bit	signal	code	would	then	control	the	selection
among	the	four	data	words	A,	B,	C,	and	D.	We	can	express	the
generation	of	signals	s1	and	s0	using	equality	tests	based	on	the
possible	values	of	code:
A	more	concise	expression	can	be	written	that	expresses	the	property
that	s1	is	1	when	code	is	in	the	set	{2,	3},	and	s0	is	1	when	code	is	in	the
set	{1,	3}:
The	general	form	of	a	set	membership	test	is
where	the	value	being	tested	
(iexpr)
and	the	candidate	matches	(
iexpr
through	
iexpr
)	are	all	integer	expressions.
4.2.5	
Memory	and	Clocking
1
k</p>
<p>Combinational	circuits,	by	their	very	nature,	do	not	store	any	information.
Instead,	they	simply	react	to	the	signals	at	their	inputs,	generating
outputs	equal	to	some	function	of	the	inputs.	To	create	
sequential	circuits
—that	is,	systems	that	have	state	and	perform	computations	on	that	state
—we	must	introduce	devices	that	store	information	represented	as	bits.
Our	storage	devices	are	all	controlled	by	a	single	
clock
,	a	periodic	signal
that	determines	when	new	values	are	to	be	loaded	into	the	devices.	We
consider	two	classes	of	memory	devices:
Clocked	registers
(or	simply	
registers)
store	individual	bits	or	words.	The	clock	signal	controls
the	loading	of	the	register	with	the	value	at	its	input.
Random	access	memories
(or	simply	
memories
)	store	multiple	words,	using	an	address	to
select	which	word	should	be	read	or	written.	Examples	of	random	access	memories	include
(1)	the	virtual	memory	system	of	a	processor,	where	a	combination	of	hardware	and	operating
system	software	make	it	appear	to	a	processor	that	it	can	access	any	word	within	a	large
address	space;	and	(2)	the	register	file,	where	register	identifiers	serve	as	the	addresses.	In	a
Y86-64	processor,	the	register	file	holds	the	15	program	registers	(
through	
).
As	we	can	see,	the	word	&quot;register&quot;	means	two	slightly	different	things
when	speaking	of	hardware	versus	machine-language	programming.	In
hardware,	a	register	is	directly	connected	to	the	rest	of	the	circuit	by	its
input	and	output	wires.	In	machine-level	programming,	the	registers
represent	a	small	collection	of	addressable	words	in	the	CPU,	where	the
addresses	consist	of	register	IDs.	These	words	are	generally	stored	in
the	register	file,	although	we	will	see	that	the	hardware	can	sometimes
pass	a	word	directly	from	one	instruction	to	another	to</p>
<p>Figure	
4.16	
Register	operation.
The	register	outputs	remain	held	at	the	current	register	state	until	the
clock	signal	rises.	When	the	clock	rises,	the	values	at	the	register	inputs
are	captured	to	become	the	new	register	state.
avoid	the	delay	of	first	writing	and	then	reading	the	register	file.	When
necessary	to	avoid	ambiguity,	we	will	call	the	two	classes	of	registers
&quot;hardware	registers&quot;	and	&quot;program	registers,&quot;	respectively.
Figure	
4.16
gives	a	more	detailed	view	of	a	hardware	register	and
how	it	operates.	For	most	of	the	time,	the	register	remains	in	a	fixed	state
(shown	as	x),	generating	an	output	equal	to	its	current	state.	Signals
propagate	through	the	combinational	logic	preceding	the	register,
creating	a	new	value	for	the	register	input	(shown	as	y),	but	the	register
output	remains	fixed	as	long	as	the	clock	is	low.	As	the	clock	rises,	the
input	signals	are	loaded	into	the	register	as	its	next	state	(y),	and	this
becomes	the	new	register	output	until	the	next	rising	clock	edge.	A	key
point	is	that	the	registers	serve	as	barriers	between	the	combinational
logic	in	different	parts	of	the	circuit.	Values	only	propagate	from	a	register
input	to	its	output	once	every	clock	cycle	at	the	rising	clock	edge.	Our
Y86-64	processors	will	use	clocked	registers	to	hold	the	program	counter
(PC),	the	condition	codes	(CC),	and	the	program	status	(Stat).
The	following	diagram	shows	a	typical	register	file:</p>
<p>This	register	file	has	two	
read	ports
,	named	A	and	B,	and	one	
write	port
,
named	W.	Such	a	
multiported
random	access	memory	allows	multiple
read	and	write	operations	to	take	place	simultaneously.	In	the	register	file
diagrammed,	the	circuit	can	read	the	values	of	two	program	registers	and
update	the	state	of	a	third.	Each	port	has	an	address	input,	indicating
which	program	register	should	be	selected,	and	a	data	output	or	input
giving	a	value	for	that	program	register.	The	addresses	are	register
identifiers,	using	the	encoding	shown	in	
Figure	
4.4
.	The	two	read	ports
have	address	inputs	srcA	and	srcB	(short	for	&quot;source	A&quot;	and	&quot;source	B&quot;)
and	data	
outputs	valA	and	valB	(short	for	&quot;value	A&quot;	and	&quot;value	B&quot;).	The
write	port	has	address	input	dstW	(short	for	&quot;destination	W&quot;)	and	data
input	valW	(short	for	&quot;value	W&quot;).
The	register	file	is	not	a	combinational	circuit,	since	it	has	internal
storage.	In	our	implementation,	however,	data	can	be	read	from	the
register	file	as	if	it	were	a	block	of	combinational	logic	having	addresses
as	inputs	and	the	data	as	outputs.	When	either	srcA	or	srcB	is	set	to
some	register	ID,	then,	after	some	delay,	the	value	stored	in	the
corresponding	program	register	will	appear	on	either	valA	or	valB.	For
example,	setting	srcA	to	3	will	cause	the	value	of	program	register	
to	be	read,	and	this	value	will	appear	on	output	valA.
The	writing	of	words	to	the	register	file	is	controlled	by	the	clock	signal	in
a	manner	similar	to	the	loading	of	values	into	a	clocked	register.	Every</p>
<p>time	the	clock	rises,	the	value	on	input	valW	is	written	to	the	program
register	indicated	by	the	register	ID	on	input	dstW.	When	dstW	is	set	to
the	special	ID	value	
,	no	program	register	is	written.	Since	the	register
file	can	be	both	read	and	written,	a	natural	question	to	ask	is,	&quot;What
happens	if	the	circuit	attempts	to	read	and	write	the	same	register
simultaneously?&quot;	The	answer	is	straightforward:	if	the	same	register	ID	is
used	for	both	a	read	port	and	the	write	port,	then,	as	the	clock	rises,	there
will	be	a	transition	on	the	read	port's	data	output	from	the	old	value	to	the
new.	When	we	incorporate	the	register	file	into	our	processor	design,	we
will	make	sure	that	we	take	this	property	into	consideration.
Our	processor	has	a	random	access	memory	for	storing	program	data,
as	illustrated	below:
This	memory	has	a	single	address	input,	a	data	input	for	writing,	and	a
data	output	for	reading.	Like	the	register	file,	reading	from	our	memory
operates	in	a	manner	similar	to	combinational	logic:	If	we	provide	an
address	on	the	address	input	and	set	the	write	control	signal	to	0,	then
after	some	delay,	the	value	stored	at	that	address	will	appear	on	data	out.
The	error	signal	will	be	set	to	1	if	the	address	is	out	of	range,	and	to	0
otherwise.	Writing	to	the	memory	is	controlled	by	the	clock:	We	set
address	to	the	desired	address,	data	in	to	the	desired	value,	and	write	to</p>
<ol>
<li>When	we	then	operate	the	clock,	the	specified	location	in	the	memory
will	be	updated,	as	long	as	the	address	is	valid.	As	with	the	read</li>
</ol>
<p>operation,	the	error	signal	will	be	set	to	1	if	the	address	is	invalid.	This
signal	is	generated	by	combinational	logic,	since	the	required	bounds
checking	is	purely	a	function	of	the	address	input	and	does	not	involve
saving	any	state.
Aside	
Real-life	memory	design
The	memory	system	in	a	full-scale	microprocessor	is	far	more
complex	than	the	simple	one	we	assume	in	our	design.	It	consists
of	several	forms	of	hardware	memories,	including	several	random
access	memories,	plus	nonvolatile	memory	or	magnetic	disk,	as
well	as	a	variety	of	hardware	and	software	mechanisms	for
managing	these	devices.	The	design	and	characteristics	of	the
memory	system	are	described	in	
Chapter	
6
.
Nonetheless,	our	simple	memory	design	can	be	used	for	smaller
systems,	and	it	provides	us	with	an	abstraction	of	the	interface
between	the	processor	and	memory	for	more	complex	systems.
Our	processor	includes	an	additional	read-only	memory	for	reading
instructions.	In	most	actual	systems,	these	memories	are	merged	into	a
single	memory	with	two	ports:	one	for	reading	instructions,	and	the	other
for	reading	or	writing	data.</p>
<p>4.3	
Sequential	Y86-64
Implementations
Now	we	have	the	components	required	to	implement	a	Y86-64
processor.	As	a	first	step,	we	describe	a	processor	called	SEQ	(for
&quot;sequential&quot;	processor).	On	each	clock	cycle,	SEQ	performs	all	the	steps
required	to	process	a	complete	instruction.	This	would	require	a	very	long
cycle	time,	however,	and	so	the	clock	rate	would	be	unacceptably	low.
Our	purpose	in	developing	SEQ	is	to	provide	a	first	step	toward	our
ultimate	goal	of	implementing	an	efficient	pipelined	processor.
4.3.1	
Organizing	Processing	into
Stages
In	general,	processing	an	instruction	involves	a	number	of	operations.
We	organize	them	in	a	particular	sequence	of	stages,	attempting	to	make
all	instructions	follow	a	uniform	sequence,	even	though	the	instructions
differ	greatly	in	their	actions.	The	detailed	processing	at	each	step
depends	on	the	particular	instruction	being	executed.	Creating	this
framework	will	allow	us	to	design	a	processor	that	makes	best	use	of	the
hardware.	The	following	is	an	informal	description	of	the	stages	and	the
operations	performed	within	them:</p>
<p>Fetch.	
The	fetch	stage	reads	the	bytes	of	an	instruction	from	memory,
using	the	program	counter	(PC)	as	the	memory	address.	From	the
instruction	it	extracts	the	two	4-bit	portions	of	the	instruction	specifier
byte,	referred	to	as	icode	(the	instruction	code)	and	ifun	(the
instruction	function).	It	possibly	fetches	a	register	specifier	byte,	giving
one	or	both	of	the	register	operand	specifiers	rA	and	rB.	It	also
possibly	fetches	an	8-byte	constant	word	valC.	It	computes	valP	to	be
the	address	of	the	instruction	following	the	current	one	in	sequential
order.	That	is,	valP	equals	the	value	of	the	PC	plus	the	length	of	the
fetched	instruction.
Decode.	
The	decode	stage	reads	up	to	two	operands	from	the
register	file,	giving	values	valA	and/or	valB.	Typically,	it	reads	the
registers	designated	by	instruction	fields	rA	and	rB,	but	for	some
instructions	it	reads	register	
.
Execute.	
In	the	execute	stage,	the	arithmetic/logic	unit	(ALU)	either
performs	the	operation	specified	by	the	instruction	(according	to	the
value	of	ifun),	computes	the	effective	address	of	a	memory	reference,
or	increments	or	decrements	the	stack	pointer.	We	refer	to	the
resulting	value	as	valE.	The	condition	codes	are	possibly	set.	For	a
conditional	move	instruction,	the	stage	will	evaluate	the	condition
codes	and	move	condition	(given	by	ifun)	and	enable	the	updating	of
the	destination	register	only	if	the	condition	holds.	Similarly,	for	a	jump
instruction,	it	determines	whether	or	not	the	branch	should	be	taken.
Memory.	
The	memory	stage	may	write	data	to	memory,	or	it	may	read
data	from	memory.	We	refer	to	the	value	read	as	valM.
Write	back.	
The	write-back	stage	writes	up	to	two	results	to	the
register	file.</p>
<p>PC	update.	
The	PC	is	set	to	the	address	of	the	next	instruction.
The	processor	loops	indefinitely,	performing	these	stages.	In	our
simplified	implementation,	the	processor	will	stop	when	any	exception
occurs—that	is,	when	it	executes	a	
or	invalid	instruction,	or	it
attempts	to	read	or	write	an	invalid	address.	In	a	more	complete	design,
the	processor	would	enter	an	exception-handling	mode	and	begin
executing	special	code	determined	by	the	type	of	exception.
As	can	be	seen	by	the	preceding	description,	there	is	a	surprising
amount	of	processing	required	to	execute	a	single	instruction.	Not	only
must	we	perform	the	stated	operation	of	the	instruction,	we	must	also
compute	addresses,	update	stack	pointers,	and	determine	the	next
instruction	address.	Fortunately,	the	overall	flow	can	be	similar	for	every
instruction.	Using	a	very	simple	and	uniform	structure	is	important	when
designing	hardware,	since	we	want	to	minimize	the	total	amount	of
hardware	and	we	must	ultimately	map	it	onto	the	two-dimensional
surface	of	an	integrated-circuit	chip.	One	way	to	minimize	the	complexity
is	to	have	the	different	instructions	share	as	much	of	the	hardware	as
possible.	For	example,	each	of	our	processor	designs	contains	a	single
arithmetic/logic	unit	that	is	used	in	different	ways	depending	on	the	type
of	instruction	being	executed.	The	cost	of	duplicating	blocks	of	logic	in
hardware	is	much	higher	than	the	cost	of	having	multiple	copies	of	code
in	software.	It	is	also	more	difficult	to	deal	with	many	special	cases	and
idiosyncrasies	in	a	hardware	system	than	with	software.
Our	challenge	is	to	arrange	the	computing	required	for	each	of	the
different	instructions	to	fit	within	this	general	framework.	We	will	use	the
code	shown	in	
Figure	
4.17
to	illustrate	the	processing	of	different	Y86-</p>
<p>64	instructions.	
Figures	
4.18
through	
4.21
contain	tables	describing
how	the	different	Y86-64	instructions	proceed	through	the	stages.	It	is
worth	the	effort	to	study	these	tables	carefully.	They	are	in	a	form	that
enables	a	straightforward	mapping	into	the	hardware.	Each	line	in	these
tables	describes	an	assignment	to	some	signal	or	stored	state</p>
<p>Figure	
4.17	
Sample	Y86-64	instruction	sequence.
We	will	trace	the	processing	of	these	instructions	through	the	different
stages.
(indicated	by	the	assignment	operation	‘←’).	These	should	be	read	as	if
they	were	evaluated	in	sequence	from	top	to	bottom.	When	we	later	map
the	computations	to	hardware,	we	will	find	that	we	do	not	need	to	perform
these	evaluations	in	strict	sequential	order.
Figure	
4.18
shows	the	processing	required	for	instruction	types	
(integer	and	logical	operations),	
(register-register	move),	and
(immediate-register	move).	Let	us	first	consider	the	integer
operations.	Examining	
Figure	
4.2
,	we	can	see	that	we	have	carefully
chosen	an	encoding	of	instructions	so	that	the	four	integer	operations
(
,	and	
)	all	have	the	same	value	of	
.	We	can
handle	them	all	by	an	identical	sequence	of	steps,	except	that	the	ALU
computation	must	be	set	according	to	the	particular	instruction	operation,
encoded	in	ifun.
The	processing	of	an	integer-operation	instruction	follows	the	general
pattern	listed	above.	In	the	fetch	stage,	we	do	not	require	a	constant
word,	and	so	valP	is	computed	as	PC	+	2.	During	the	decode	stage,	we
read	both	operands.	These	are	supplied	to	the	ALU	in	the	execute	stage,
along	with	the	function	specifier	ifun,	so	that	valE	becomes	the	instruction
result.	This	computation	is	shown	as	the	expression	valB	OP	valA,	where
OP	indicates	the	operation	specified	by	ifun.	Note	the	ordering	of	the	two
arguments—this	order	is	consistent	with	the	conventions	of	Y86-64	(and
x86-64).	For	example,	the	instruction	
is	supposed	to
compute	the	value	
.	Nothing	happens	in	the	memory</p>
<p>stage	for	these	instructions,	but	valE	is	written	to	register	rB	in	the	write-
back	stage,	and	the	PC	is	set	to	valP	to	complete	the	instruction
execution.
Executing	an	
instruction	proceeds	much	like	an	arithmetic
operation.	We	do	not	need	to	fetch	the	second	register	operand,
however.	Instead,	we	set	the	second	ALU	input	to	zero	and	add	this	to
the	first,	giving	valE	=	valA,	which	is
Stage
rA,	rB
rA,	rB
V,	rB
Fetch
icode:ifun	←	M
[PC]
rA:rB	←	M
[PC	+	1]
icode:ifun	←	M
[PC]
rA:rB	←	M
[PC	+	1]
icode:ifun	←	M
[PC]
rA:rB	←	M
[PC	+1]
valC	←	M
[PC	+	2]
valP	←	PC+	2
valP	←	PC+	2
valP	←	PC+	10
Decode
valA	←	R[rA]	valB	←	R[rB]
valA	←	R[rA]
Execute
valE	←	valBOPvalA	SetCC
valE	←	0	+	valA
valE	←	0	+	valC
Memory
Write	back
R[rB]	←	valE
R[rB]	←	valE
R[rB]	←	valE
PC	update
PC	←	valP
PC	←	valP
PC	←	valP
Figure	
4.18	
Computations	in	sequential	implementation	of	Y86-64
instructions	
,	and	
.
These	instructions	compute	a	value	and	store	the	result	in	a	register.	The
notation	
:	ifun	indicates	the	two	components	of	the	instruction	byte,
while	rA	:	rB	indicates	the	two	components	of	the	register	specifier	byte.
1
1
1
1
1
1
8</p>
<p>The	notation	M
[
x
]	indicates	accessing	(either	reading	or	writing)	1	byte	at
memory	location	
x
,	while	M
[
x
]	indicates	accessing	8	bytes.
then	written	to	the	register	file.	Similar	processing	occurs	for	
,
except	that	we	use	constant	value	valC	for	the	first	ALU	input.	In	addition,
we	must	increment	the	program	counter	by	10	for	
due	to	the	long
instruction	format.	Neither	of	these	instructions	changes	the	condition
codes.
Practice	Problem	
4.13	
(solution	page	
485
)
Fill	in	the	right-hand	column	of	the	following	table	to	describe	the
processing	of	the	
instruction	on	line	4	of	the	object	code	in
Figure	
4.17
:
Stage
Generic	
V,	rB
Specific	
Fetch
icode:ifun	←	M
[PC]
rA:rB	←	M
[PC	+	1]
valC	←	M
[PC	+	2]
valP	←	PC+	10
Decode
Execute
valE	←	0	+	valC
Aside	
Tracing	the	execution	of	a	
1
8
1
1
8</p>
<p>instruction
As	an	example,	let	us	follow	the	processing	of	the	
instruction
on	line	3	of	the	object	code	shown	in	
Figure	
4.17
.	We	can	see
that	the	previous	two	instructions	initialize	registers	
and	
to	9	and	21,	respectively.	We	can	also	see	that	the	instruction	is
located	at	address	
and	consists	of	2	bytes,	having	values
and	
.	The	stages	would	proceed	as	shown	in	the
following	table,	which	lists	the	generic	rule	for	processing	an	
instruction	(
Figure	
4.18
)	on	the	left,	and	the	computations	for
this	specific	instruction	on	the	right.
Stage
rA,	rB
Fetch
icode:ifun	←	M
[PC]
rA:rB	←	MT.PC	+	1]
icode:ifun	←	M
[
]	=	6:1
rA:rB	←	M
[
]	=	2:3
valP	←	PC+	2
valP	←	
Decode
valA	←	R[rA]
valB	←	R[rB]
valA	←	R[
]	=	9
va	IB	←	R[
]	=	21
Execute
valE	←	valBOPvalA
SetCC
valE	←	21	-	9=12
ZF	←	0,	SF	←	0,	OF	←	0
Memory
Write	back
R[rB]	←	valE
R[%rbx]	←	valE	=	12
PC	update
PC	←	valP
PC	←	valP	=	
As	this	trace	shows,	we	achieve	the	desired	effect	of	setting
register	
to	12,	setting	all	three	condition	codes	to	zero,	and
1
1
1</p>
<p>incrementing	the	PC	by	2.
Stage
Generic	
V,	rB
Specific	
Memory
Writeback
R[rB]	←	valE
PC	update
PC	←	va	IP
How	does	this	instruction	execution	modify	the	registers	and	the	PC?
Figure	
4.19
shows	the	processing	required	for	the	memory	write	and
read	instructions	
and	
.	We	see	the	same	basic	flow	as
before,	but	using	the	ALU	to	add	valC	to	valB,	giving	the	effective
address	(the	sum	of	the	displacement	and	the	base	register	value)	for	the
memory	operation.	In	the	memory	stage,	we	either	write	the	register
value	valA	to	memory	or	read	valM	from	memory.
Stage
rA,	D(rB)
D	(rB),	rA
Fetch
icode:ifun	←	M
[PC]
rA:rB	←	M
[PC	+	1]
valC	←	M
[PC	+	2]
valP	←	PC+	10
icode:ifun	←	M
[PC]
rA:rB	←	M
[PC	+	1]
valC	←	M
[PC	+	2]
valP	←	PC+	10
Decode
valA	←	R[rA]
valB	←	R[rB]
valB	←	R[rB]
Execute
valE	←	valB	+	valC
valE	←	valB	+	valC
Memory
M
[valE]	←	valA
valM	←	M
[valE]
Write	back
1
1
8
1
1
8
8
8</p>
<p>R[rA]	←	valM
PC	update
PC	←	valP
PC	←	valP
Figure	
4.19	
Computations	in	sequential	implementation	of	Y86-64
instructions	</p>
<p>and</p>
<p>.
These	instructions	read	or	write	memory.
Figure	
4.20
shows	the	steps	required	to	process	
and	
instructions.	These	are	among	the	most	difficult	Y86-64	instructions	to
implement,	because	they	involve	both	accessing	memory	and
incrementing	or	decrementing	the	stack	pointer.	Although	the	two
instructions	have	similar	flows,	they	have	important	differences.
The	
instruction	starts	much	like	our	previous	instructions,	but	in	the
decode	stage	we	use	
as	the	identifier	for	the	second	register
operand,	giving	the	stack	pointer	as	value	valB.	In	the	execute	stage,	we
use	the	ALU	to	decrement	the	stack	pointer	by	8.	This	decremented
value	is	used	for	the	memory	write	address	and	is	also	stored	back	to
in	the	write-back	stage.	By	using	valE	as	the	address	for	the	write
operation,	we	adhere	to	the	Y86-64	(and	x86-64)	convention	that	
should	decrement	the	stack	pointer	before	writing,	even	though	the	actual
updating	of	the	stack	pointer	does	not	occur	until	after	the	memory
operation	has	completed.
The	
instruction	proceeds	much	like	
,	except	that	we	read	two
copies	of	the	stack	pointer	in	the	decode	stage.	This	is	clearly	redundant,
but	we	will	see	that	having	the	stack	pointer	as	both	valA	and	valB	makes</p>
<p>the	subsequent	flow	more	similar	to	that	of	other	instructions,	enhancing
the	overall	uniformity	of	the	design.	We	use	the	ALU	to	increment	the
stack	pointer	by	8	in	the	execute	stage,	but	use	the	unincremented	value
as	the	address	for	the	memory	operation.	In	the	write-back	stage,	we
update	both	the	stack	pointer	register	with	the	incremented	stack	pointer
and	register	rA	with	the	value	read	from	memory.	Using	the
unincremented	stack	pointer	as	the	memory	read	address	preserves	the
Y86-64
Aside	
Tracing	the	execution	of	an
instruction
Let	us	trace	the	processing	of	the	
instruction	on	line	5	of
the	object	code	shown	in	
Figure	
4.17
.	We	can	see	that	the
previous	instruction	initialized	register	
to	128,	while	
still
holds	12,	as	computed	by	the	
instruction	(line	3).	We	can
also	see	that	the	instruction	is	located	at	address	
and
consists	of	10	bytes.	The	first	2	bytes	have	values	
and	
,
while	the	final	8	bytes	are	a	byte-reversed	version	of	the	number
(decimal	100).	The	stages	would	proceed	as
follows:
Stage
Generic	
rA,	D(rB)
Specific	</p>
<p>,	100(
)
Fetch
icode:ifun	←	M
[PC]
rA:rB	←	M
[PC	+	1]
valC	←	M
[PC	+	2]
valP	←	PC+	10
icode:ifun	←	M
[
]	=	4:0
rA:rB	←	M
[
]	=	4:3
valC	←	M
[
]	=	100
valP	←	
Decode
valA	←	R[rA]
valA	←	R[
]	=	128
1
1
8
1
1
8</p>
<p>Decode
valA	←	R[rA]
valB	←	R[rB]
valA	←	R[
]	=	128
va	IB	←	R[
]	=	12
Execute
valE	←	valB	+	valC
valE	←	12	+	100	=	112
Memory
M
[valE]	←	valA
M
[112]	←	128
Write	back
PC	update
PC	←	valP
PC	←	
As	this	trace	shows,	the	instruction	has	the	effect	of	writing	128	to
memory	address	112	and	incrementing	the	PC	by	10.
(and	x86-64)	convention	that	popq	should	first	read	memory	and	then
increment	the	stack	pointer.
Practice	Problem	
4.14	
(solution	page	
486
)
Fill	in	the	right-hand	column	of	the	following	table	to	describe	the
processing	of	the	popq	instruction	on	line	7	of	the	object	code	in
Figure	
4.17
.
Stage
Generic	popq	rA
Specific	
Fetch
icode:ifun	←	M
[PC]	
rA:rB	←	M
[PC	+	1]	
valP	←	PC+	2
Stage
rA
rA
Fetch
icode:ifun	←	M
[PC]
rA:rB	←	M
[PC	+	1]
icode:ifun	←	M
[PC]
rA:rB	←	M
[PC	+	1]
8
8
1
1
1
1
1
1</p>
<p>valP	←	PC+	2
valP	←	PC+	2
Decode
valA	←	R[rA]
valB	←	R[
]
valA	←	R[
]
va	IB	←	R[
]
Execute
valE	←	valB+(-8)
valE	←	valB	+	8
Memory
M
[valE]	←	valA
va	IM	←	M
[valA]
Write	back
R[
]	←	valE
R[
]	←	valE	
R[rA]	←	valM
PC	update
PC	←	valP
PC	←	valP
Figure	
4.20	
Computations	in	sequential	implementation	of	Y86-64
instructions	</p>
<p>and</p>
<p>.
These	instructions	push	and	pop	the	stack.
Stage
Generic	
rA
Specific	
Decode
valA	←	R[
]
valB	←	R[
]
Execute
valE	←	valB	+	8
Memory
valM	←	M
[valA]
Write	back
R[
]	←	valE
R[rA]	←	valM
PC	update
PC	←	valP
8
8
8</p>
<p>What	effect	does	this	instruction	execution	have	on	the	registers	and	the
PC?
Practice	Problem	
4.15	
(solution	page	
486
)
What	would	be	the	effect	of	the	instruction	
according	to
the	steps	listed	in	
Figure	
4.20
?	Does	this	conform	to	the
desired	behavior	for	Y86-64,	as	determined	in	
Problem	
4.7
?
Aside	
Tracing	the	execution	of	a	
instruction
Let	us	trace	the	processing	of	the	
instruction	on	line	6	of	the
object	code	shown	in	
Figure	
4.17
.	At	this	point,	we	have	9	in
register	
and	128	in	register	
.	We	can	also	see	that	the
instruction	is	located	at	address	
and	consists	of	2	bytes
having	values	
and	
.	The	stages	would	proceed	as
follows:
Stage
Generic	
rA
Specific	
Fetch
icode:ifun	←	M
[PC]
rA:rB	←	MT.PC	+	1]
icode:ifun	←	M
[
rA:rB	←	M
[
valP	←	PC+	2
valP	←	
Decode
valA	←	R[rA]
valB	←	R[
]
valA	←	R[
]	=	9
valB	←	R[
]	=	128
1
1
1</p>
<p>Execute
valE	←	valB	+	(-8)
valE	←	128+	(-8)	=	120
Memory
M
[valE]	←	valA
M
[120]	←	9
Write	back
R[
]	←	valE
R[
]	←	120
PC	update
PC	←	valP
PC	←	
As	this	trace	shows,	the	instruction	has	the	effect	of	setting	
to	120,	writing	9	to	address	120,	and	incrementing	the	PC	by	2.
Practice	Problem	
4.16	
(solution	page	
486
)
Assume	the	two	register	writes	in	the	write-back	stage	for	
occur	in	the	order	listed	in	
Figure	
4.20
.	What	would	be	the
effect	of	executing	
?	Does	this	conform	to	the	desired
behavior	for	Y86-64,	as	determined	in	
Problem	
4.8
?
Figure	
4.21
indicates	the	processing	of	our	three	control	transfer
instructions:	the	different	jumps,	
,	and	
.	We	see	that	we	can
implement	these	instructions	with	the	same	overall	flow	as	the	preceding
ones.
As	with	integer	operations,	we	can	process	all	of	the	jumps	in	a	uniform
manner,	since	they	differ	only	when	determining	whether	or	not	to	take
the	branch.	A	jump	instruction	proceeds	through	fetch	and	decode	much
like	the	previous	instructions,	except	that	it	does	not	require	a	register
specifier	byte.	In	the	execute	stage,	we	check	the	condition	codes	and
the	jump	condition	to	determine	whether	or	not	to	take	the	branch,
yielding	a	1-bit	signal	
.	During	the	PC	update	stage,	we	test	this	flag
and	set	the	PC	to	valC	(the	jump	target)	if	the	flag	is	1	and	to	valP	(the
8
8</p>
<p>address	of	the	following	instruction)	if	the	flag	is	0.	Our	notation	
x
?	
a
:	
b
is	similar	to	the	conditional	expression	in	C—it	yields	
a
when	
x
is	1	and	
b
when	
x
is	0.
Stage
Dest
Dest
Fetch
icode:ifun	←	M
[PC]
valC	←	M
[PC	+	1]
valP	←	PC+	9
icode:ifun	←	M
[PC]
valC	←	M
[PC	+	1]
valP	←	PC+	9
icode:ifun	←	M
[PC]
valP	←	PC	+	1
Decode
valB	←	R[
]
valA	←	R[
]
valB	←	R[
]
Execute
Cnd	←	Cond(CC,	ifun)
valE	←	valB	+	(-8)
valE	←	valB	+	8
Memory
M
[valE]	←	valP
valM	←	M
[valA]
Write	back
R[
]	←	valE
R[
]	←	valE
PC	update
PC	←	Cnd?valC:valP
PC	←	valC
PC	←	valM
Figure	
4.21	
Computations	in	sequential	implementation	of	Y86-64
instructions	
,	and	
.
These	instructions	cause	control	transfers.
Practice	Problem	
4.17	
(solution	page	
486
)
We	can	see	by	the	instruction	encodings	(
Figures	
4.2
and
4.3
)	that	the	
instruction	is	the	unconditional	version	of	a
more	general	class	of	instructions	that	include	the	conditional
1
8
1
8
1
8
8</p>
<p>moves.	Show	how	you	would	modify	the	steps	for	the	
instruction	below	to	also	handle	the	six	conditional	move
instructions.	You	may	find	it	useful	to	see	how	the	implementation
of	the	
instructions	(
Figure	
4.21
)	handles	conditional
behavior.
Stage
rA,	rB
Fetch
icode:ifun	←	M
[PC]
rA:rB	←	M
[PC	+	1]
valP	←	PC	+	2
Decode
valA	←	R[rA]
Execute
valE	←	0	+	valA
Memory
Write	back
R[rB]	←	valE
PC	update
PC	←	valP
Aside	
Tracing	the	execution	of	a	
instruction
Let	us	trace	the	processing	of	the	je	instruction	on	line	8	of	the
object	code	shown	in	
Figure	
4.17
.	The	condition	codes	were	all
set	to	zero	by	the	
instruction	(line	3),	and	so	the	branch	will
not	be	taken.	The	instruction	is	located	at	address	
and
1
1</p>
<p>consists	of	9	bytes.	The	first	has	value	
,	while	the	remaining	8
bytes	are	a	byte-reversed	version	of	the	number
,	the	jump	target.	The	stages	would	proceed	as
follows:
Stage
Generic	
Dest
Specific	
Fetch
icode:ifun	←	M
[PC]
icode:ifun	←	M
[
valC	←	M
[PC	+	1]
valC	←	M
valP	←	PC+	9
valP	←	
Decode
Execute
Cnd	←	Cond(CC,	ifun)
Cnd	←	Cond
Memory
Write	back
PC	update
PC	←	Cnd?valC:valP
PC	←	0	?	
As	this	trace	shows,	the	instruction	has	the	effect	of	incrementing
the	PC	by	9.
Instructions	call	and	ret	bear	some	similarity	to	instructions	
and
,	except	that	we	push	and	pop	program	counter	values.	With
instruction	call,	we	push	valP,	the	address	of	the	instruction	that	follows
the	call	instruction.	During	the	PC	update	stage,	we	set	the	PC	to	valC,
the	call	destination.	With	instruction	ret,	we	assign	valM,	the	value
popped	from	the	stack,	to	the	PC	in	the	PC	update	stage.
1
1
8
8</p>
<p>Practice	Problem	
4.18	
(solution	page	
487
)
Fill	in	the	right-hand	column	of	the	following	table	to	describe	the
processing	of	the	call	instruction	on	line	9	of	the	object	code	in	
Figure
4.17
:
Stage
Generic	
Dest
Specific	
Fetch
icode:ifun	←	M
[PC]
valC	←	M
[PC	+	1]
valP	←	PC+	9
Aside	
Tracing	the	execution	of	a	ret
instruction
Let	us	trace	the	processing	of	the	ret	instruction	on	line	13	of	the
object	code	shown	in	
Figure	
4.17
.	The	instruction	address	is
and	is	encoded	by	a	single	byte	
.	The	previous	call
instruction	set	
to	120	and	stored	the	return	address	
at
memory	address	120.	The	stages	would	proceed	as	follows:
Stage
Generic	
Specific	
Fetch
icode:ifun	←	M
[PC]
icode:ifun	←	M
valP	←	PC	+	1
valP	←	
Decode
valA	←	R[
]
valA	←	R[
]	=	120
valB	←	R[
]
valB	←	R[
]	=	120
1
8
1
1</p>
<p>Execute
valE	←	valB	+	8
valE	←	120	+	8=128
Memory
valM	←	M
[valA]
valM	←	M
[120]	=	
Write	back
R[
]	←	valE
R[
]	←	128
PC	update
PC	←	valM
PC	←	
As	this	trace	shows,	the	instruction	has	the	effect	of	setting	the	PC
to	
,	the	address	of	the	
instruction.	It	also	sets	
to
128.
Stage
Generic	
Dest
Specific	
Decode
valB	←	R[
]
Execute
valE	←	valB+(-8)
Memory
M
[valE]	←	valP
Write	back
R[
]	←	valE
PC	update
PC	←	valC
What	effect	would	this	instruction	execution	have	on	the	registers,	the
PC,	and	the	memory?
We	have	created	a	uniform	framework	that	handles	all	of	the	different
types	of	Y86-64	instructions.	Even	though	the	instructions	have	widely
varying	behavior,	we	can	organize	the	processing	into	six	stages.	Our
task	now	is	to	create	a	hardware	design	that	implements	the	stages	and
connects	them	together.
8
8
8</p>
<p>4.3.2	
SEQ	Hardware	Structure
The	computations	required	to	implement	all	of	the	Y86-64	instructions
can	be	organized	as	a	series	of	six	basic	stages:	fetch,	decode,	execute,
memory,	write	back,	and	PC	update.	
Figure	
4.22
shows	an	abstract
view	of	a	hardware	structure	that	can	perform	these	computations.	The
program	counter	is	stored	in	a	register,	shown	in	the	lower	left-hand
corner	(labeled	&quot;PC&quot;).	Information	then	flows	along	wires	(shown	grouped
together	as	a	heavy	gray	line),	first	upward	and	then	around	to	the	right.
Processing	is	performed	by	
hardware	units
associated	with	the	different
stages.	The	feedback	paths	coming	back	down	on	the	right-hand	side
contain	the	updated	values	to	write	to	the	register	file	and	the	updated
program	counter.	In	SEQ,	all	of	the	processing	by	the	hardware	units
occurs	within	a	single	clock	cycle,	as	is	discussed	in	
Section	
4.3.3
.
This	diagram	omits	some	small	blocks	of	combinational	logic	as	well	as
all	of	the	control	logic	needed	to	operate	the	different	hardware	units	and
to	route	the	appropriate	values	to	the	units.	We	will	add	this	detail	later.
Our	method	of	drawing	processors	with	the	flow	going	from	bottom	to	top
is	unconventional.	We	will	explain	the	reason	for	this	convention	when	we
start	designing	pipelined	processors.
The	hardware	units	are	associated	with	the	different	processing	stages:
Fetch.	
Using	the	program	counter	register	as	an	address,	the
instruction	memory	reads	the	bytes	of	an	instruction.	The	PC
incrementer	computes	valP,	the	incremented	program	counter.
Decode.	
The	register	file	has	two	read	ports,	A	and	B,	via	which
register	values	valA	and	valB	are	read	simultaneously.</p>
<p>Execute.	
The	execute	stage	uses	the	arithmetic/logic	(ALU)	unit	for
different	purposes	according	to	the	instruction	type.	For	integer
operations,	it	performs	the	specified	operation.	For	other	instructions,
it	serves	as	an	adder	to	compute	an	incremented	or	decremented
stack	pointer,	to	compute	an	effective	address,	or	simply	to	pass	one
of	its	inputs	to	its	outputs	by	adding	zero.
The	condition	code	register	(CC)	holds	the	three	condition	code	bits.
New	values	for	the	condition	codes	are	computed	by	the	ALU.	When
executing	a	conditional	move	instruction,	the	decision	as	to	whether
or	not	to	update	the	destination	register	is	computed	based	on	the
condition	codes	and	move	condition.	Similarly,	when	executing	a	jump
instruction,	the	branch	signal	Cnd	is	computed	based	on	the	condition
codes	and	the	jump	type.
Memory.	
The	data	memory	reads	or	writes	a	word	of	memory	when
executing	a	memory	instruction.	The	instruction	and	data	memories
access	the	same	memory	locations,	but	for	different	purposes.
Write	back.	
The	register	file	has	two	write	ports.	Port	E	is	used	to
write	values	computed	by	the	ALU,	while	port	M	is	used	to	write
values	read	from	the	data	memory.</p>
<p>Figure	
4.22	
Abstract	view	of	SEQ,	a	sequential	implementation.
The	information	processed	during	execution	of	an	instruction	follows	a
clockwise	flow	starting	with	an	instruction	fetch	using	the	program
counter	(PC),	shown	in	the	lower	left-hand	corner	of	the	figure.</p>
<p>PC	update.	
The	new	value	of	the	program	counter	is	selected	to	be
either	valP,	the	address	of	the	next	instruction,	valC,	the	destination
address	specified	by	a	call	or	jump	instruction,	or	valM,	the	return
address	read	from	memory.
Figure	
4.23
gives	a	more	detailed	view	of	the	hardware	required	to
implement	SEQ	(although	we	will	not	see	the	complete	details	until	we
examine	the	individual	stages).	We	see	the	same	set	of	hardware	units
as	earlier,	but	now	the	wires	are	shown	explicitly.	In	this	figure,	as	well	as
in	our	other	hardware	diagrams,	we	use	the	following	drawing
conventions:
Clocked	registers	are	shown	as	white	rectangles.	
The	program
counter	PC	is	the	only	clocked	register	in	SEQ.
Hardware	units	are	shown	as	light	blue	boxes.	
These	include	the
memories,	the	ALU,	and	so	forth.	We	will	use	the	same	basic	set	of
units	for	all	of	our	processor	implementations.	We	will	treat	these	units
as	&quot;black	boxes&quot;	and	not	go	into	their	detailed	designs.
Control	logic	blocks	are	drawn	as	gray	rounded	rectangles.
These	blocks	serve	to	select	from	among	a	set	of	signal	sources	or	to
compute	some	Boolean	function.	We	will	examine	these	blocks	in
complete	detail,	including	developing	HCL	descriptions.
Wire	names	are	indicated	in	white	circles.	
These	are	simply	labels
on	the	wires,	not	any	kind	of	hardware	element.
Word-wide	data	connections	are	shown	as	medium	lines.	
Each	of
these	lines	actually	represents	a	bundle	of	64	wires,	connected	in
parallel,	for	transferring	a	word	from	one	part	of	the	hardware	to
another.</p>
<p>Byte	and	narrower	data	connections	are	shown	as	thin	lines.
Each	of	these	lines	actually	represents	a	bundle	of	four	or	eight	wires,
depending	on	what	type	of	values	must	be	carried	on	the	wires.
Single-bit	connections	are	shown	as	dotted	lines.	
These
represent	control	values	passed	between	the	units	and	blocks	on	the
chip.
All	of	the	computations	we	have	shown	in	
Figures	
4.18
through
4.21
have	the	property	that	each	line	represents	either	the
computation	of	a	specific	value,	such	as	valP,	or	the	activation	of	some
hardware	unit,	such	as	the	memory.	These	computations	and	actions	are
listed	in	the	second	column	of	
Figure	
4.24
.	In	addition	to	the	signals
we	have	already	described,	this	list	includes	four	register	ID	signals:
srcA,	the	source	of	valA;	srcB,	the	source	of	valB;	dstE,	the	register	to
which	valE	gets	written;	and	dstM,	the	register	to	which	valM	gets	written.
The	two	right-hand	columns	of	this	figure	show	the	computations	for	the
and	
instructions	to	illustrate	the	values	being	computed.	To
map	the	computations	into	hardware,	we	want	to	implement	control	logic
that	will	transfer	the	data	between	the	different	hardware	units	and
operate	these	units	in	such	a	way	that	the	specified	operations	are
performed	for	each	of	the	different	instruction	types.	That	is	the	purpose
of	the	control	logic	blocks,	shown	as	gray	rounded	boxes</p>
<p>Figure	
4.23	
Hardware	structure	of	SEQ,	a	sequential
implementation.
Some	of	the	control	signals,	as	well	as	the	register	and	control	word
connections,	are	not	shown.</p>
<p>Stage
Computation
rA,	rB
D(rB),	rA
Fetch
icode,	ifun
icode:ifun	←	M
[PC]
icode:ifun	←	M
[PC]
rA,	rB
rA:rB	←	M
[PC	+	1]
rA:rB	←	M
[PC	+1]
valC
valC	←	M
[PC	+	2]
valP
valP	←	PC	+	2
valP	←	PC+	10
Decode
valA,	srcA
valA	←	R[rA]
valB,	srcB
valB	←	R[rB]
valB	←	R[rB]
Execute
valE	Cond.	codes
valE	←	valB	OP	valA	Set	CC
valE	←	valB	+	valC
Memory
Read/write
valM	←	M
[valE]
Write	back
E	port,	dstE
R[rB]	←	valE
M	port,	dstM
R[rA]	←	valM
PC	update
PC
PC	←	valP
PC	←	valP
Figure	
4.24	
Identifying	the	different	computation	steps	in	the
sequential	implementation.
The	second	column	identifies	the	value	being	computed	or	the	operation
being	performed	in	the	stages	of	SEQ.	The	computations	for	instructions
and	
are	shown	as	examples	of	the	computations.
in	
Figure	
4.23
.	Our	task	is	to	proceed	through	the	individual	stages
and	create	detailed	designs	for	these	blocks.
1
1
1
1
8
8</p>
<p>4.3.3	
SEQ	Timing
In	introducing	the	tables	of	
Figures	
4.18
through	
4.21
,	we	stated
that	they	should	be	read	as	if	they	were	written	in	a	programming
notation,	with	the	assignments	performed	in	sequence	from	top	to
bottom.	On	the	other	hand,	the	hardware	structure	of	
Figure	
4.23
operates	in	a	fundamentally	different	way,	with	a	single	clock	transition
triggering	a	flow	through	combinational	logic	to	execute	an	entire
instruction.	Let	us	see	how	the	hardware	can	implement	the	behavior
listed	in	these	tables.
Our	implementation	of	SEQ	consists	of	combinational	logic	and	two
forms	of	memory	devices:	clocked	registers	(the	program	counter	and
condition	code	register)	and	random	access	memories	(the	register	file,
the	instruction	memory,	and	the	data	memory).	Combinational	logic	does
not	require	any	sequencing	or	control—values	propagate	through	a
network	of	logic	gates	whenever	the	inputs	change.	As	we	have
described,	we	also	assume	that	reading	from	a	random	access	memory
operates	much	like	combinational	logic,	with	the	output	word	generated
based	on	the	address	input.	This	is	a	reasonable	assumption	for	smaller
memories	(such	as	the	register	file),	and	we	can	mimic	this	effect	for
larger	circuits	using	special	clock	circuits.	Since	our	instruction	memory	is
only	used	to	read	instructions,	we	can	therefore	treat	this	unit	as	if	it	were
combinational	logic.
We	are	left	with	just	four	hardware	units	that	require	an	explicit	control
over	their	sequencing—the	program	counter,	the	condition	code	register,
the	data	memory,	and	the	register	file.	These	are	controlled	via	a	single</p>
<p>clock	signal	that	triggers	the	loading	of	new	values	into	the	registers	and
the	writing	of	values	to	the	random	access	memories.	The	program
counter	is	loaded	with	a	new	instruction	address	every	clock	cycle.	The
condition	code	register	is	loaded	only	when	an	integer	operation
instruction	is	executed.	The	data	memory	is	written	only	when	an	
,	or	
instruction	is	executed.	The	two	write	ports	of	the	register
file	allow	two	program	registers	to	be	updated	on	every	cycle,	but	we	can
use	the	special	register	ID	
as	a	port	address	to	indicate	that	no	write
should	be	performed	for	this	port.
This	clocking	of	the	registers	and	memories	is	all	that	is	required	to
control	the	sequencing	of	activities	in	our	processor.	Our	hardware
achieves	the	same	effect	as	would	a	sequential	execution	of	the
assignments	shown	in	the	tables	of	
Figures	
4.18
through	
4.21
,
even	though	all	of	the	state	updates	actually	occur	simultaneously	and
only	as	the	clock	rises	to	start	the	next	cycle.	This	equivalence	holds
because	of	the	nature	of	the	Y86-64	instruction	set,	and	because	we
have	organized	the	computations	in	such	a	way	that	our	design	obeys
the	following	principle:</p>
<p>Principle:
No	reading	back
The	processor	never	needs	to	read	back	the	state	updated
by	an	instruction	in	order	to	complete	the	processing	of	this
instruction.
This	principle	is	crucial	to	the	success	of	our	implementation.	As	an
illustration,	suppose	we	implemented	the	
instruction	by	first
decrementing	
by	8	and	then	using	the	updated	value	of	
as	the
address	of	a	write	operation.	This	approach	would	violate	the	principle
stated	above.	It	would	require	reading	the	updated	stack	pointer	from	the
register	file	in	order	to	perform	the	memory	operation.	Instead,	our
implementation	(
Figure	
4.20
)	generates	the	decremented	value	of	the
stack	pointer	as	the	signal	valE	and	then	uses	this	signal	both	as	the	data
for	the	register	write	and	the	address	for	the	memory	write.	As	a	result,	it
can	perform	the	register	and	memory	writes	simultaneously	as	the	clock
rises	to	begin	the	next	clock	cycle.
As	another	illustration	of	this	principle,	we	can	see	that	some	instructions
(the	integer	operations)	set	the	condition	codes,	and	some	instructions
(the	conditional	move	and	jump	instructions)	read	these	condition	codes,
but	no	instruction	must	both	set	and	then	read	the	condition	codes.	Even
though	the	condition	codes	are	not	set	until	the	clock	rises	to	begin	the</p>
<p>next	clock	cycle,	they	will	be	updated	before	any	instruction	attempts	to
read	them.
Figure	
4.25
shows	how	the	SEQ	hardware	would	process	the
instructions	at	lines	3	and	4	in	the	following	code	sequence,	shown	in
assembly	code	with	the	instruction	addresses	listed	on	the	left:
Each	of	the	diagrams	labeled	1	through	4	shows	the	four	state	elements
plus	the	combinational	logic	and	the	connections	among	the	state
elements.	We	show	the	combinational	logic	as	being	wrapped	around	the
condition	code	register,	because	some	of	the	combinational	logic	(such
as	the	ALU)	generates	the	input	to	the	condition	code	register,	while
other	parts	(such	as	the	branch	computation	and	the	PC	selection	logic)
have	the	condition	code	register	as	input.	We	show	the	register	file	and
the	data	memory	as	having	separate	connections	for	reading	and	writing,
since	the	read	operations	propagate	through	these	units	as	if	they	were
combinational	logic,	while	the	write	operations	are	controlled	by	the	clock.
The	color	coding	in	
Figure	
4.25
indicates	how	the	circuit	signals	relate
to	the	different	instructions	being	executed.	We	assume	the	processing</p>
<p>starts	with	the	condition	codes,	listed	in	the	order	
,	and	
,	set	to
.	At	the	beginning	of	clock	cycle	3	(point	1),	the	state	elements	hold
the	state	as	updated	by	the	second	
instruction	(line	2	of	the
listing),	shown	in	light	gray.	The	combinational	logic	is	shown	in	white,
indicating	that	it	has	not	yet	had	time	to	react	to	the	changed	state.	The
clock	cycle	begins	with	address	
loaded	into	the	program	counter.
This	causes	the	
instruction	(line	3	of	the	listing),	shown	in	blue,	to
be	fetched	and	processed.	Values	flow	through	the	combinational	logic,
including	the	reading	of	the	random	access	memories.	By	the	end	of	the
cycle	(point	2),	the	combinational	logic	has	generated	new	values	(
)
for	the	condition	codes,	an	update	for	program	register	
,	and	a	new
value	(
)	for	the	program	counter.	At	this	point,	the	combinational
logic	has	been	updated	according	to	the	
instruction	(shown	in	blue),
but	the	state	still	holds	the	values	set	by	the	second	
instruction
(shown	in	light	gray).
As	the	clock	rises	to	begin	cycle	4	(point	3),	the	updates	to	the	program
counter,	the	register	file,	and	the	condition	code	register	occur,	and	so	we
show	these	in	blue,	but	the	combinational	logic	has	not	yet	reacted	to
these	changes,	and	so	we	show	this	in	white.	In	this	cycle,	the	
instruction	(line	4	in	the	listing),	shown	in	dark	gray,	is	fetched	and
executed.	Since	condition	code	
is	0,	the	branch	is	not	taken.	By	the
end	of	the	cycle	(point	4),	a	new	value	of	
has	been	generated	for
the	program	counter.	The	combinational	logic	has	been	updated
according	to	the	
instruction	(shown	in	dark	gray),	but	the	state	still
holds	the	values	set	by	the	
instruction	(shown	in	blue)	until	the	next
cycle	begins.</p>
<p>As	this	example	illustrates,	the	use	of	a	clock	to	control	the	updating	of
the	state	elements,	combined	with	the	propagation	of	values	through
combinational	logic,	suffices	to	control	the	computations	performed	for
each	instruction	in	our	implementation	of	SEQ.	Every	time	the	clock
transitions	from	low	to	high,	the	processor	begins	executing	a	new
instruction.</p>
<p>Figure	
4.25	
Tracing	two	cycles	of	execution	by	SEQ.
Each	cycle	begins	with	the	state	elements	(program	counter,	condition
code	register,	register	file,	and	data	memory)	set	according	to	the
previous	instruction.	Signals	propagate	through	the	combinational	logic,
creating	new	values	for	the	state	elements.	These	values	are	loaded	into
the	state	elements	to	start	the	next	cycle.</p>
<p>4.3.4	
SEQ	Stage	Implementations
In	this	section,	we	devise	HCL	descriptions	for	the	control	logic	blocks
required	to	implement	SEQ.	A	complete	HCL	description	for	SEQ	is	given
in	Web	Aside	
ARCH
:
HCL</p>
<p>on	page	472.	We	show	some	example	blocks
here,	and	others	are	given	as	practice	problems.	We	recommend	that
you	work	these	problems	as	a	way	to	check	your	understanding	of	how
the	blocks	relate	to	the	computational	requirements	of	the	different
instructions.
Part	of	the	HCL	description	of	SEQ	that	we	do	not	include	here	is	a
definition	of	the	different	integer	and	Boolean	signals	that	can	be	used	as
arguments	to	the	HCL	operations.	These	include	the	names	of	the
different	hardware	signals,	as	well	as	constant	values	for	the	different
instruction	codes,	function	codes,	register	names,	ALU	operations,	and
status	codes.	Only	those	that	must	be	explicitly
Name
Value	(hex)
Meaning
Code	for	
instruction
Code	for	
instruction
Code	for	
instruction
Code	for	
instruction
Code	for	
instruction</p>
<p>Code	for	
instruction
Code	for	integer	operation	instructions
Code	for	jump	instructions
Code	for	
instruction
Code	for	
instruction
Code	for	
instruction
Code	for	
instruction
Default	function	code
Register	ID	for	
Indicates	no	register	file	access
Function	for	addition	operation
Status	code	for	normal	operation
Status	code	for	address	exception
Status	code	for	illegal	instruction	exception
Status	code	for	
Figure	
4.26	
Constant	values	used	in	HCL	descriptions.
These	values	represent	the	encodings	of	the	instructions,	function	codes,
register	IDs,	ALU	operations,	and	status	codes.</p>
<p>Figure	
4.27	
SEQ	fetch	stage.
Six	bytes	are	read	from	the	instruction	memory	using	the	PC	as	the
starting	address.	From	these	bytes,	we	generate	the	different	instruction
fields.	The	PC	increment	block	computes	signal	valP.
referenced	in	the	control	logic	are	shown.	The	constants	we	use	are
documented	in	
Figure	
4.26
.	By	convention,	we	use	uppercase	names
for	constant	values.
In	addition	to	the	instructions	shown	in	
Figures	
4.18
to	
4.21
,	we
include	the	processing	for	the	
and	
instructions.	The	
instruction	simply	flows	through	stages	without	much	processing,	except
to	increment	the	PC	by	1.	The	
instruction	causes	the	processor
status	to	be	set	to	
,	causing	it	to	
operation.
Fetch	Stage</p>
<p>As	shown	in	
Figure	
4.27
,	the	fetch	stage	includes	the	instruction
memory	hardware	unit.	This	unit	reads	10	bytes	from	memory	at	a	time,
using	the	PC	as	the	address	of	the	first	byte	(byte	0).	This	byte	is
interpreted	as	the	instruction	byte	and	is	split	(by	the	unit	labeled	&quot;Split&quot;)
into	two	4-bit	quantities.	The	control	logic	blocks	labeled	&quot;icode&quot;	and
&quot;ifun&quot;	then	compute	the	instruction	and	function	codes	as	equaling	either
the	values	read	from	memory	or,	in	the	event	that	the	instruction	address
is	not	valid	(as	indicated	by	the	signal	
),	the	values
corresponding	to	a	
instruction.	Based	on	the	value	of	icode,	we	can
compute	three	1-bit	signals	(shown	as	dashed	lines):
instr_valid.	
Does	this	byte	correspond	to	a	legal	Y86-64	instruction?
This	signal	is	used	to	detect	an	illegal	instruction.
need_regids.	
Does	this	instruction	include	a	register	specifier	byte?
need_valC.	
Does	this	instruction	include	a	constant	word?
The	signals	instr_valid	and	imem_error	(generated	when	the	instruction
address	is	out	of	bounds)	are	used	to	generate	the	status	code	in	the
memory	stage.
As	an	example,	the	HCL	description	for	need_regids	simply	determines
whether	the	value	of	icode	is	one	of	the	instructions	that	has	a	register
specifier	byte:</p>
<p>Practice	Problem	
4.19	
(solution	page	
487
)
Write	HCL	code	for	the	signal	
in	the	SEQ	implementation.
As	
Figure	
4.27
shows,	the	remaining	9	bytes	read	from	the	instruction
memory	encode	some	combination	of	the	register	specifier	byte	and	the
constant	word.	These	bytes	are	processed	by	the	hardware	unit	labeled
&quot;Align&quot;	into	the	register	fields	and	the	constant	word.	Byte	1	is	split	into
register	specifiers	rA	and	rB	when	the	computed	signal	need_regids	is	1.
If	need_regids	is	0,	both	register	specifiers	are	set	to	
,
indicating	there	are	no	registers	specified	by	this	instruction.	Recall	also
(
Figure	
4.2
)	that	for	any	instruction	having	only	one	register	operand,
the	other	field	of	the	register	specifier	byte	will	be	
.	Thus,	we
can	assume	that	the	signals	rA	and	rB	either	encode	registers	we	want	to
access	or	indicate	that	register	access	is	not	required.	The	unit	labeled
&quot;Align&quot;	also	generates	the	constant	word	valC.	This	will	either	be	bytes	1-
8	or	bytes	2-9,	depending	on	the	value	of	signal	need_regids.
The	PC	incrementer	hardware	unit	generates	the	signal	valP,	based	on
the	current	value	of	the	PC,	and	the	two	signals	need_regids	and
need_valC.	For	PC	value	
p
,	need_regids	value	
r
,	and	need_valC	value	
i
,
the	incrementer	generates	the	value	
p
+	1	+	
r
+	8
i.
Decode	and	Write-Back	Stages
Figure	
4.28
provides	a	detailed	view	of	logic	that	implements	both	the
decode	and	write-back	stages	in	SEQ.	These	two	stages	are	combined</p>
<p>because	they	both	access	the	register	file.
The	register	file	has	four	ports.	It	supports	up	to	two	simultaneous	reads
(on	ports	A	and	B)	and	two	simultaneous	writes	(on	ports	E	and	M).	Each
port	has	both	an	address	connection	and	a	data	connection,	where	the
address	connection	is	a	register	ID,	and	the	data	connection	is	a	set	of
64	wires	serving	as	either	an	output	word	(for	a	read	port)	or	an	input
word	(for	a	write	port)	of	the	register	file.	The	two	read	ports	have
address	inputs	srcA	and	srcB,	while	the	two	write	ports	have	address
inputs	dstE	and	dstM.	The	special	identifier	
on	an	address
port	indicates	that	no	register	should	be	accessed.
The	four	blocks	at	the	bottom	of	
Figure	
4.28
generate	the	four	different
register	IDs	for	the	register	file,	based	on	the	instruction	code	icode,	the
register	specifiers	rA	and	rB,	and	possibly	the	condition	signal	Cnd
computed	in	the	execute	stage.	Register	ID	srcA	indicates	which	register
should	be	read	to	generate	valA.
Figure	
4.28	
SEQ	decode	and	write-back	stage.
The	instruction	fields	are	decoded	to	generate	register	identifiers	for	four
addresses	(two	read	and	two	write)	used	by	the	register	file.	The	values</p>
<p>read	from	the	register	file	become	the	signals	valA	and	valB.	The	two
write-back	values	valE	and	valM	serve	as	the	data	for	the	writes.
The	desired	value	depends	on	the	instruction	type,	as	shown	in	the	first
row	for	the	decode	stage	in	
Figures	
4.18
to	
4.21
.	Combining	all	of
these	entries	into	a	single	computation	gives	the	following	HCL
description	of	srcA	(recall	that	
is	the	register	ID	of	
):
Practice	Problem	
4.20	
(solution	page	
488
)
The	register	signal	srcB	indicates	which	register	should	be	read	to
generate	the	signal	valB.	The	desired	value	is	shown	as	the	second	step
in	the	decode	stage	in	
Figures	
4.18
to	
4.21
.	Write	HCL	code	for
srcB.
Register	ID	dstE	indicates	the	destination	register	for	write	port	E,	where
the	computed	value	valE	is	stored.	This	is	shown	in	
Figures	
4.18
to
4.21
as	the	first	step	in	the	write-back	stage.	If	we	ignore	for	the
moment	the	conditional	move	instructions,	then	we	can	combine	the</p>
<p>destination	registers	for	all	of	the	different	instructions	to	give	the
following	HCL	description	of	dstE:
We	will	revisit	this	signal	and	how	to	implement	conditional	moves	when
we	examine	the	execute	stage.
Practice	Problem	
4.21	
(solution	page	
488
)
Register	ID	dstM	indicates	the	destination	register	for	write	port	M,	where
valM,	the	value	read	from	memory,	is	stored.	This	is	shown	in	
Figures
4.18
to	
4.21
as	the	second	step	in	the	write-back	stage.	Write	HCL
code	for	dstM.
Practice	Problem	
4.22	
(solution	page	
488
)
Only	the	
instruction	uses	both	register	file	write	ports
simultaneously.	For	the	instruction	
,	the	same	address	will	be
used	for	both	the	E	and	M	write	ports,	but	with	different	data.	To	handle
this	conflict,	we	must	establish	a	
priority
among	the	two	write	ports	so</p>
<h2>that	when	both	attempt	to	write	the	same	register	on	the	same	cycle,	only
the	write	from	the	higher-priority	port	takes	place.	Which	of	the	two	ports
should	be	given	priority	in	order	to	implement	the	desired	behavior,	as
determined	in	
Practice	Problem	
4.8
?
Execute	Stage
The	execute	stage	includes	the	arithmetic/logic	unit	(ALU).	This	unit
performs	the	operation	
ADD
,	
SUBTRACT
,	
AND
,	or	
EXCLUSIVE</h2>
<p>OR</p>
<p>on	inputs	aluA
and	aluB	based	on	the	setting	of	the	alufun	signal.	These	data	and
control	signals	are	generated	by	three	control	blocks,	as	diagrammed	in
Figure	
4.29
.	The	ALU	output	becomes	the	signal	valE.
In	
Figures	
4.18
to	
4.21
,	the	ALU	computation	for	each	instruction	is
shown	as	the	first	step	in	the	execute	stage.	The	operands	are	listed	with
aluB	first,	followed	by	aluA	to	make	sure	the	
instruction	subtracts
valA	from	valB.	We	can	see	that	the	value	of	aluA	can	be	valA,	valC,	or
either	-8	or	+8,	depending	on	the	instruction	type.	We	can	therefore
express	the	behavior	of	the	control	block	that	generates	aluA	as	follows:</p>
<p>Figure	
4.29	
SEQ	execute	stage.
The	ALU	either	performs	the	operation	for	an	integer	operation	instruction
or	acts	as	an	adder.	The	condition	code	registers	are	set	according	to	the
ALU	value.	The	condition	code	values	are	tested	to	determine	whether	a
branch	should	be	taken.
Practice	Problem	
4.23	
(solution	page	
488
)
Based	on	the	first	operand	of	the	first	step	of	the	execute	stage	in
Figures	
4.18
to	
4.21
,	write	an	HCL	description	for	the	signal	aluB	in
SEQ.
Looking	at	the	operations	performed	by	the	ALU	in	the	execute	stage,	we
can	see	that	it	is	mostly	used	as	an	adder.	For	the	
instructions,</p>
<p>however,	we	want	it	to	use	the	operation	encoded	in	the	ifun	field	of	the
instruction.	We	can	therefore	write	the	HCL	description	for	the	ALU
control	as	follows:
The	execute	stage	also	includes	the	condition	code	register.	Our	ALU
generates	the	three	signals	on	which	the	condition	codes	are	based—
zero,	sign,	and	overflow—every	time	it	operates.	However,	we	only	want
to	set	the	condition	codes	when	an	
instruction	is	executed.	We
therefore	generate	a	signal	set_cc	that	controls	whether	or	not	the
condition	code	register	should	be	updated:
The	hardware	unit	labeled	&quot;cond&quot;	uses	a	combination	of	the	condition
codes	and	the	function	code	to	determine	whether	a	conditional	branch
or	data	transfer	should	take	place	(
Figure	
4.3
).	It	generates	the	Cnd
signal	used	both	for	the	setting	of	dstE	with	conditional	moves	and	in	the
next	PC	logic	for	conditional	branches.	For	other	instructions,	the	Cnd
signal	may	be	set	to	either	1	or	0,	depending	on	the	instruction's	function
code	and	the	setting	of	the	condition	codes,	but	it	will	be	ignored	by	the
control	logic.	We	omit	the	detailed	design	of	this	unit.</p>
<p>Practice	Problem	
4.24	
(solution	page	
488
)
The	conditional	move	instructions,	abbreviated	
,	have	instruction
code	
.	As	
Figure	
4.28
shows,	we	can	implement	these
instructions	by	making	use	of	the	Cnd	signal,	generated	in	the	execute
stage.	Modify	the	HCL	code	for	dstE	to	implement	these	instructions.
Memory	Stage
The	memory	stage	has	the	task	of	either	reading	or	writing	program	data.
As	shown	in	
Figure	
4.30
,	two	control	blocks	generate	the	values	for
the	memory
Figure	
4.30	
SEQ	memory	stage.
The	data	memory	can	either	write	or	read	memory	values.	The	value
read	from	memory	forms	the	signal	valM.
address	and	the	memory	input	data	(for	write	operations).	Two	other
blocks	generate	the	control	signals	indicating	whether	to	perform	a	read</p>
<p>or	a	write	operation.	When	a	read	operation	is	performed,	the	data
memory	generates	the	value	valM.
The	desired	memory	operation	for	each	instruction	type	is	shown	in	the
memory	stage	of	
Figures	
4.18
to	
4.21
.	Observe	that	the	address	for
memory	reads	and	writes	is	always	valE	or	valA.	We	can	describe	this
block	in	HCL	as	follows:
Practice	Problem	
4.25	
(solution	page	
488
)
Looking	at	the	memory	operations	for	the	different	instructions	shown	in
Figures	
4.18
to	
4.21
,	we	can	see	that	the	data	for	memory	writes
are	always	either	valA	or	valP.	Write	HCL	code	for	the	signal	mem_data
in	SEQ.
We	want	to	set	the	control	signal	mem_read	only	for	instructions	that
read	data	from	memory,	as	expressed	by	the	following	HCL	code:</p>
<p>Practice	Problem	
4.26	
(solution	page	
489
)
We	want	to	set	the	control	signal	mem_write	only	for	instructions	that
write	data	to	memory.	Write	HCL	code	for	the	signal	mem_write	in	SEQ.
Figure	
4.31	
SEQ	PC	update	stage.
The	next	value	of	the	PC	is	selected	from	among	the	signals	valC,	valM,
and	valP,	depending	on	the	instruction	code	and	the	branch	flag.
A	final	function	for	the	memory	stage	is	to	compute	the	status	code	Stat
resulting	from	the	instruction	execution	according	to	the	values	of	icode,
imem_error,	and	instr_valid	generated	in	the	fetch	stage	and	the	signal
dmem_error	generated	by	the	data	memory.
Practice	Problem	
4.27	
(solution	page	
489
)
Write	HCL	code	for	Stat,	generating	the	four	status	codes	
,	and	
(see	
Figure	
4.26
).
PC	Update	Stage
The	final	stage	in	SEQ	generates	the	new	value	of	the	program	counter
(see	
Figure	
4.31
).	As	the	final	steps	in	
Figures	
4.18
to	
4.21
show,	the	new	PC	will	be	valC,	valM,	or	valP,	depending	on	the</p>
<p>instruction	type	and	whether	or	not	a	branch	should	be	taken.	This
selection	can	be	described	in	HCL	as	follows:
Surveying	SEQ
We	have	now	stepped	through	a	complete	design	for	a	Y86-64
processor.	We	have	seen	that	by	organizing	the	steps	required	to
execute	each	of	the	different	instructions	into	a	uniform	flow,	we	can
implement	the	entire	processor	with	a	small	number	of	different	hardware
units	and	with	a	single	clock	to	control	the	sequencing	of	computations.
The	control	logic	must	then	route	the	signals	between	these	units	and
generate	the	proper	control	signals	based	on	the	instruction	types	and
the	branch	conditions.
The	only	problem	with	SEQ	is	that	it	is	too	slow.	The	clock	must	run
slowly	enough	so	that	signals	can	propagate	through	all	of	the	stages</p>
<p>within	a	single	cycle.	As	an	example,	consider	the	processing	of	a	
instruction.	Starting	with	an	updated	program	counter	at	the	beginning	of
the	clock	cycle,	the	instruction	must	be	read	from	the	instruction	memory,
the	stack	pointer	must	be	read	from	the	register	file,	the	ALU	must
increment	the	stack	pointer	by	8,	and	the	return	address	must	be	read
from	the	memory	in	order	to	determine	the	next	value	for	the	program
counter.	All	of	these	must	be	completed	by	the	end	of	the	clock	cycle.
This	style	of	implementation	does	not	make	very	good	use	of	our
hardware	units,	since	each	unit	is	only	active	for	a	fraction	of	the	total
clock	cycle.	We	will	see	that	we	can	achieve	much	better	performance	by
introducing	pipelining.</p>
<p>4.4	
General	Principles	of	Pipelining
Before	attempting	to	design	a	pipelined	Y86-64	processor,	let	us	consider
some	general	properties	and	principles	of	pipelined	systems.	Such
systems	are	familiar	to	anyone	who	has	been	through	the	serving	line	at
a	cafeteria	or	run	a	car	through	an	automated	car	wash.	In	a	pipelined
system,	the	task	to	be	performed	is	divided	into	a	series	of	discrete
stages.	In	a	cafeteria,	this	involves	supplying	salad,	a	main	dish,	dessert,
and	beverage.	In	a	car	wash,	this	involves	spraying	water	and	soap,
scrubbing,	applying	wax,	and	drying.	Rather	than	having	one	customer
run	through	the	entire	sequence	from	beginning	to	end	before	the	next
can	begin,	we	allow	multiple	customers	to	proceed	through	the	system	at
once.	In	a	traditional	cafeteria	line,	the	customers	maintain	the	same
order	in	the	pipeline	and	pass	through	all	stages,	even	if	they	do	not	want
some	of	the	courses.	In	the	case	of	the	car	wash,	a	new	car	is	allowed	to
enter	the	spraying	stage	as	the	preceding	car	moves	from	the	spraying
stage	to	the	scrubbing	stage.	In	general,	the	cars	must	move	through	the
system	at	the	same	rate	to	avoid	having	one	car	crash	into	the	next.
A	key	feature	of	pipelining	is	that	it	increases	the	
throughput
of	the
system	(i.e.,	the	number	of	customers	served	per	unit	time),	but	it	may
also	slightly	increase	the	
latency
(i.e.,	the	time	required	to	service	an
individual	customer).	For	example,	a	customer	in	a	cafeteria	who	only
wants	a	dessert	could	pass	through	a	nonpipelined	system	very	quickly,
stopping	only	at	the	dessert	stage.	A	customer	in	a	pipelined	system	who
attempts	to	go	directly	to	the	dessert	stage	risks	incurring	the	wrath	of
other	customers.</p>
<p>4.4.1	
Computational	Pipelines
Shifting	our	focus	to	computational	pipelines,	the	&quot;customers&quot;	are
instructions	and	the	stages	perform	some	portion	of	the	instruction
execution.	
Figure	
4.32
(a)	shows	an	example	of	a	simple	nonpipelined
hardware	system.	It	consists	of	some	logic	that	performs	a	computation,
followed	by	a	register	to	hold	the	results	of	this	computation.	A	clock
signal	controls	the	loading	of	the	register	at	some	regular	time	interval.
An	example	of	such	a	system	is	the	decoder	in	a	compact	disk	(CD)
player.	The	incoming	signals	are	the	bits	read	from	the	surface	of	the	CD,
and
Figure	
4.32	
Unpipelined	computation	hardware.
On	each	320	ps	cycle,	the	system	spends	300	ps	evaluating	a
combinational	logic	function	and	20	ps	storing	the	results	in	an	output
register.
the	logic	decodes	these	to	generate	audio	signals.	The	computational
block	in	the	figure	is	implemented	as	combinational	logic,	meaning	that</p>
<h1>the	signals	will	pass	through	a	series	of	logic	gates,	with	the	outputs
becoming	some	function	of	the	inputs	after	some	time	delay.
In	contemporary	logic	design,	we	measure	circuit	delays	in	units	of
picoseconds
(abbreviated	&quot;ps&quot;),	or	10
seconds.	In	this	example,	we
assume	the	combinational	logic	requires	300	ps,	while	the	loading	of	the
register	requires	20	ps.	
Figure	
4.32
shows	a	form	of	timing	diagram
known	as	a	
pipeline	diagram
.	In	this	diagram,	time	flows	from	left	to	right.
A	series	of	instructions	(here	named	
,	and	
)	are	written	from	top
to	bottom.	The	solid	rectangles	indicate	the	times	during	which	these
instructions	are	executed.	In	this	implementation,	we	must	complete	one
instruction	before	beginning	the	next.	Hence,	the	boxes	do	not	overlap
one	another	vertically.	The	following	formula	gives	the	maximum	rate	at
which	we	could	operate	the	system:
We	express	throughput	in	units	of	giga-instructions	per	second
(abbreviated	GIPS),	or	billions	of	instructions	per	second.	The	total	time
required	to	perform	a	single	instruction	from	beginning	to	end	is	known	as
the	
latency
.	In	this	system,	the	latency	is	320	ps,	the	reciprocal	of	the
throughput.
Suppose	we	could	divide	the	computation	performed	by	our	system	into
three	stages,	A,	B,	and	C,	where	each	requires	100	ps,	as	illustrated	in
Figure	
4.33
.	Then	we	could	put	
pipeline	registers
between	the	stages
so	that	each	instruction	moves	through	the	system	in	three	steps,
requiring	three	complete	clock	cycles	from	beginning	to	end.	As	the
pipeline	diagram	in	
Figure	
4.33
illustrates,	we	could	allow	
to	enter
-12
T
h
r
o
u
g
h
p
u
t</h1>
<p>1
 
instruction
(
20</p>
<ul>
<li></li>
</ul>
<p>300
)
 
picoseconds
.
1
,
000
 
picoseconds
1
 
nanosecond</p>
<p>stage	A	as	soon	as	
moves	from	A	to	B,	and	so	on.	In	steady	state,	all
three	stages	would	be	active,	with	one	instruction	leaving	and	a	new	one
entering	the	system	every	clock	cycle.	We	can	see	this	during	the	third
clock	cycle	in	the	pipeline	diagram	where	
is	in	stage	C,	
is	in	stage
B,	and	
is	in	stage	A.	In
Figure	
4.33	
Three-stage	pipelined	computation	hardware.
The	computation	is	split	into	stages	A,	B,	and	C.	On	each	120	ps	cycle,
each	instruction	progresses	through	one	stage.
Figure	
4.34	
Three-stage	pipeline	timing.
The	rising	edge	of	the	clock	signal	controls	the	movement	of	instructions
from	one	pipeline	stage	to	the	next.</p>
<p>this	system,	we	could	cycle	the	clocks	every	100	+	20	=	120
picoseconds,	giving	a	throughput	of	around	8.33	GIPS.	Since	processing
a	single	instruction	requires	3	clock	cycles,	the	latency	of	this	pipeline	is	3
×	120	=	360	ps.	We	have	increased	the	throughput	of	the	system	by	a
factor	of	8.33/3.12	=	2.67	at	the	expense	of	some	added	hardware	and	a
slight	increase	in	the	latency	(360/320	=	1.12).	The	increased	latency	is
due	to	the	time	overhead	of	the	added	pipeline	registers.
4.4.2	
A	Detailed	Look	at	Pipeline
Operation
To	better	understand	how	pipelining	works,	let	us	look	in	some	detail	at
the	timing	and	operation	of	pipeline	computations.	
Figure	
4.34
shows
the	pipeline	diagram	for	the	three-stage	pipeline	we	have	already	looked
at	(
Figure	
4.33
).	The	transfer	of	the	instructions	between	pipeline
stages	is	controlled	by	a	clock	signal,	as	shown	above	the	pipeline
diagram.	Every	120	ps,	this	signal	rises	from	0	to	1,	initiating	the	next	set
of	pipeline	stage	evaluations.
Figure	
4.35
traces	the	circuit	activity	between	times	240	and	360,	as
instruction	
(shown	in	dark	gray)	propagates	through	stage	C,	
(shown	in	blue)</p>
<p>Figure	
4.35	
One	clock	cycle	of	pipeline	operation.
Just	before	the	clock	rises	at	time	240	(point	1),	instructions	
(shown	in
dark	gray)	and	
(shown	in	blue)	have	completed	stages	B	and	A.	After
the	clock	rises,	these	instructions	begin	propagating	through	stages	C
and	B,	while	instruction	
(shown	in	light	gray)	begins	propagating</p>
<p>through	stage	A	(points	2	and	3).	Just	before	the	clock	rises	again,	the
results	for	the	instructions	have	propagated	to	the	inputs	of	the	pipeline
registers	(point	4).
propagates	through	stage	B,	and	
(shown	in	light	gray)	propagates
through	stage	A.	Just	before	the	rising	clock	at	time	240	(point	1),	the
values	computed	in	stage	A	for	instruction	
have	reached	the	input	of
the	first	pipeline	register,	but	its	state	and	output	remain	set	to	those
computed	during	stage	A	for	instruction	
.	The	values	computed	in
stage	B	for	instruction	
have	reached	the	input	of	the	second	pipeline
register.	As	the	clock	rises,	these	inputs	are	loaded	into	the	pipeline
registers,	becoming	the	register	outputs	(point	2).	In	addition,	the	input	to
stage	A	is	set	to	initiate	the	computation	of	instruction	
.	The	signals
then	propagate	through	the	combinational	logic	for	the	different	stages
(point	3).	As	the	curved	wave	fronts	in	the	diagram	at	point	3	suggest,
signals	can	propagate	through	different	sections	at	different	rates.	Before
time	360,	the	result	values	reach	the	inputs	of	the	pipeline	registers	(point
4).	When	the	clock	rises	at	time	360,	each	of	the	instructions	will	have
progressed	through	one	pipeline	stage.
We	can	see	from	this	detailed	view	of	pipeline	operation	that	slowing
down	the	clock	would	not	change	the	pipeline	behavior.	The	signals
propagate	to	the	pipeline	register	inputs,	but	no	change	in	the	register
states	will	occur	until	the	clock	rises.	On	the	other	hand,	we	could	have
disastrous	effects	if	the	clock	were	run	too	fast.	The	values	would	not
have	time	to	propagate	through	the	combinational	logic,	and	so	the
register	inputs	would	not	yet	be	valid	when	the	clock	rises.</p>
<p>As	with	our	discussion	of	the	timing	for	the	SEQ	processor	(
Section
4.3.3
),	we	see	that	the	simple	mechanism	of	having	clocked	registers
between	blocks	of	combinational	logic	suffices	to	control	the	flow	of
instructions	in	the	pipeline.	As	the	clock	rises	and	falls	repeatedly,	the
different	instructions	flow	through	the	stages	of	the	pipeline	without
interfering	with	one	another.
4.4.3	
Limitations	of	Pipelining
The	example	of	
Figure	
4.33
shows	an	ideal	pipelined	system	in	which
we	are	able	to	divide	the	computation	into	three	independent	stages,
each	requiring	one-third	of	the	time	required	by	the	original	logic.
Unfortunately,	other	factors	often	arise	that	diminish	the	effectiveness	of
pipelining.
Nonuniform	Partitioning
Figure	
4.36
shows	a	system	in	which	we	divide	the	computation	into
three	stages	as	before,	but	the	delays	through	the	stages	range	from	50
to	150	ps.	The	sum	of	the	delays	through	all	of	the	stages	remains	300
ps.	However,	the	rate	at	which	we	can	operate	the	clock	is	limited	by	the
delay	of	the	slowest	stage.	As	the	pipeline	diagram	in	this	figure	shows,
stage	A	will	be	idle	(shown	as	a	white	box)	for	100	ps	every	clock	cycle,
while	stage	C	will	be	idle	for	50	ps	every	clock	cycle.	Only	stage	B	will	be
continuously	active.	We	must	set	the	clock	cycle	to	150	+	20	=	170
picoseconds,	giving	a	throughput	of	5.88	GIPS.	In	addition,	the	latency
would	increase	to	510	ps	due	to	the	slower	clock	rate.</p>
<p>Devising	a	partitioning	of	the	system	computation	into	a	series	of	stages
having	uniform	delays	can	be	a	major	challenge	for	hardware	designers.
Often,
Figure	
4.36	
Limitations	of	pipelining	due	to	nonuniform	stage
delays.
The	system	throughput	is	limited	by	the	speed	of	the	slowest	stage.
some	of	the	hardware	units	in	a	processor,	such	as	the	ALU	and	the
memories,	cannot	be	subdivided	into	multiple	units	with	shorter	delay.
This	makes	it	difficult	to	create	a	set	of	balanced	stages.	We	will	not
concern	ourselves	with	this	level	of	detail	in	designing	our	pipelined	Y86-
64	processor,	but	it	is	important	to	appreciate	the	importance	of	timing
optimization	in	actual	system	design.
Practice	Problem	
4.28	
(solution	page	
489
)
Suppose	we	analyze	the	combinational	logic	of	
Figure	
4.32
and
determine	that	it	can	be	separated	into	a	sequence	of	six	blocks,</p>
<p>named	A	to	F,	having	delays	of	80,	30,	60,	50,	70,	and	10	ps,
respectively,	illustrated	as	follows:
We	can	create	pipelined	versions	of	this	design	by	inserting
pipeline	registers	between	pairs	of	these	blocks.	Different
combinations	of	pipeline	depth	(how	many	stages)	and	maximum
throughput	arise,	depending	on	where	we	insert	the	pipeline
registers.	Assume	that	a	pipeline	register	has	a	delay	of	20	ps.
A
.	
Inserting	a	single	register	gives	a	two-stage	pipeline.	Where
should	the	register	be	inserted	to	maximize	throughput?
What	would	be	the	throughput	and	latency?
B
.	
Where	should	two	registers	be	inserted	to	maximize	the
throughput	of	a	three-stage	pipeline?	What	would	be	the
throughput	and	latency?
C
.	
Where	should	three	registers	be	inserted	to	maximize	the
throughput	of	a	4-stage	pipeline?	What	would	be	the
throughput	and	latency?
D
.	
What	is	the	minimum	number	of	stages	that	would	yield	a
design	with	the	maximum	achievable	throughput?	Describe
this	design,	its	throughput,	and	its	latency.
Diminishing	Returns	of	Deep	Pipelining
Figure	
4.37
illustrates	another	limitation	of	pipelining.	In	this	example,
we	have	divided	the	computation	into	six	stages,	each	requiring	50	ps.</p>
<p>Inserting	a	pipeline	register	between	each	pair	of	stages	yields	a	six-
stage	pipeline.	The	minimum	clock	period	for	this	system	is	50	+	20	=	70
picoseconds,	giving	a	throughput	of	14.29	GIPS.	Thus,	in	doubling	the
number	of	pipeline	stages,	we	improve	the	performance	by	a	factor	of
14.29/8.33	=	1.71.	Even	though	we	have	cut	the	time	required	for	each
computation	block	by	a	factor	of	2,	we	do	not	get	a	doubling	of	the
throughput,	due	to	the	delay	through	the	pipeline	registers.	This	delay
becomes	a	limiting	factor	in	the	throughput	of	the	pipeline.	In	our	new
design,	this	delay	consumes	28.6%	of	the	total	clock	period.
Modern	processors	employ	very	deep	pipelines	(15	or	more	stages)	in	an
attempt	to	maximize	the	processor	clock	rate.	The	processor	architects
divide	the	instruction	execution	into	a	large	number	of	very	simple	steps
so	that	each	stage	can	have	a	very	small	delay.	The	circuit	designers
carefully	design	the	pipeline	registers	to	minimize	their	delay.	The	chip
designers	must	also	carefully	design	the	clock	distribution	network	to
ensure	that	the	clock	changes	at	the	exact	same	time	across	the	entire
chip.	All	of	these	factors	contribute	to	the	challenge	of	designing	high-
speed	microprocessors.
Practice	Problem	
4.29	
(solution	page	
490
)
Suppose	we	could	take	the	system	of	
Figure	
4.32
and	divide	it
into	an	arbitrary	number	of	pipeline	stages	
k
,	each	having	a	delay
of	300/
k
,	and	with	each	pipeline	register	having	a	delay	of	20	ps.</p>
<p>Figure	
4.37	
Limitations	of	pipelining	due	to	overhead.
As	the	combinational	logic	is	split	into	shorter	blocks,	the	delay
due	to	register	updating	becomes	a	limiting	factor.
A
.	
What	would	be	the	latency	and	the	throughput	of	the
system,	as	functions	of	
k
?
B
.	
What	would	be	the	ultimate	limit	on	the	throughput?
4.4.4	
Pipelining	a	System	with
Feedback
Up	to	this	point,	we	have	considered	only	systems	in	which	the	objects
passing	through	the	pipeline—whether	cars,	people,	or	instructions—are
completely	independent	of	one	another.	For	a	system	that	executes
machine	programs	such	as	x86-64	or	Y86-64,	however,	there	are
potential	dependencies	between	successive	instructions.	For	example,
consider	the	following	Y86-64	instruction	sequence:
In	this	three-instruction	sequence,	there	is	a	
data	dependency
between
each	successive	pair	of	instructions,	as	indicated	by	the	circled	register
names	and	the	arrows	between	them.	The	
instruction	(line	1)
stores	its	result	in	
,	which	then	must	be	read	by	the	
instruction</p>
<p>(line	2);	and	this	instruction	stores	its	result	in	
,	which	must	then	be
read	by	the	
instruction	(line	3).
Another	source	of	sequential	dependencies	occurs	due	to	the	instruction
control	flow.	Consider	the	following	Y86-64	instruction	sequence:
The	
instruction	(line	3)	creates	a	
control	dependency
since	the
outcome	of	the	conditional	test	determines	whether	the	next	instruction	to
execute	will	be	the	
instruction	(line	4)	or	the	halt	instruction	(line
7).	In	our	design	for	SEQ,	these	dependencies	were	handled	by	the
feedback	paths	shown	on	the	right-hand	side	of	
Figure	
4.22
.	This
feedback	brings	the	updated	register	values	down	to	the	register	file	and
the	new	PC	value	down	to	the	PC	register.
Figure	
4.38
illustrates	the	perils	of	introducing	pipelining	into	a	system
containing	feedback	paths.	In	the	original	system	(
Figure	
4.38
(a)),	the
result	of	each</p>
<p>Figure	
4.38	
Limitations	of	pipelining	due	to	logical	dependencies.
In	going	from	an	unpipelined	system	with	feedback	(a)	to	a	pipelined	one
(c),	we	change	its	computational	behavior,	as	can	be	seen	by	the	two
pipeline	diagrams	(b	and	d).
instruction	is	fed	back	around	to	the	next	instruction.	This	is	illustrated	by
the	pipeline	diagram	(
Figure	
4.38
(b)),	where	the	result	of	
becomes</p>
<p>an	input	to	
,	and	so	on.	If	we	attempt	to	convert	this	to	a	three-stage
pipeline	in	the	most	straightforward	manner	(
Figure	
4.38
(c)),	we
change	the	behavior	of	the	system.	As	
Figure	
4.38
(c)	shows,	the
result	of	
becomes	an	input	to	
.	In	attempting	to	speed	up	the
system	via	pipelining,	we	have	changed	the	system	behavior.
When	we	introduce	pipelining	into	a	Y86-64	processor,	we	must	deal	with
feedback	effects	properly.	Clearly,	it	would	be	unacceptable	to	alter	the
system	behavior	as	occurred	in	the	example	of	
Figure	
4.38
.	Somehow
we	must	deal	with	the	data	and	control	dependencies	between
instructions	so	that	the	resulting	behavior	matches	the	model	defined	by
the	ISA.</p>
<p>4.5	
Pipelined	Y86-64
Implementations
We	are	finally	ready	for	the	major	task	of	this	chapter—designing	a
pipelined	Y86-64	processor.	We	start	by	making	a	small	adaptation	of	the
sequential	processor	SEQ	to	shift	the	computation	of	the	PC	into	the
fetch	stage.	We	then	add	pipeline	registers	between	the	stages.	Our	first
attempt	at	this	does	not	handle	the	different	data	and	control
dependencies	properly.	By	making	some	modifications,	however,	we
achieve	our	goal	of	an	efficient	pipelined	processor	that	implements	the
Y86-64	ISA.
4.5.1	
SEQ+:	Rearranging	the
Computation	Stages
As	a	transitional	step	toward	a	pipelined	design,	we	must	slightly
rearrange	the	order	of	the	five	stages	in	SEQ	so	that	the	PC	update
stage	comes	at	the	beginning	of	the	clock	cycle,	rather	than	at	the	end.
This	transformation	requires	only	minimal	change	to	the	overall	hardware
structure,	and	it	will	work	better	with	the	sequencing	of	activities	within
the	pipeline	stages.	We	refer	to	this	modified	design	as	SEQ+.</p>
<p>We	can	move	the	PC	update	stage	so	that	its	logic	is	active	at	the
beginning	of	the	clock	cycle	by	making	it	compute	the	PC	value	for	the
current
instruction.	
Figure	
4.39
shows	how	SEQ	and	SEQ+	differ	in
their	PC	computation.	With	SEQ	(
Figure	
4.39
(a)),	the	PC	computation
takes	place	at	the	end	of	the	clock	cycle,	computing	the	new	value	for	the
PC	register	based	on	the	values	of	signals	computed	during	the	current
clock	cycle.	With	SEQ+	(
Figure	
4.39
(b)),	we	create	state	registers	to
hold	the	signals	computed	during	an	instruction.	Then,	as	a	new	clock
cycle	begins,	the	values	propagate	through	the	exact	same	logic	to
compute	the	PC	for	the	now-current	instruction.	We	label	the	registers
“pIcode,”
Figure	
4.39	
Shifting	the	timing	of	the	PC	computation.
With	SEQ+,	we	compute	the	value	of	the	program	counter	for	the	current
state	as	the	first	step	in	instruction	execution.
Aside	
Where	is	the	PC	in	SEQ+?
One	curious	feature	of	SEQ+	is	that	there	is	no	hardware	register
storing	the	program	counter.	Instead,	the	PC	is	computed
dynamically	based	on	some	state	information	stored	from	the
previous	instruction.	This	is	a	small	illustration	of	the	fact	that	we
can	implement	a	processor	in	a	way	that	differs	from	the</p>
<p>conceptual	model	implied	by	the	ISA,	as	long	as	the	processor
correctly	executes	arbitrary	machine-language	programs.	We
need	not	encode	the	state	in	the	form	indicated	by	the
programmer-visible	state,	as	long	as	the	processor	can	generate
correct	values	for	any	part	of	the	programmer-visible	state	(such
as	the	program	counter).	We	will	exploit	this	principle	even	more
in	creating	a	pipelined	design.	Out-of-order	processing
techniques,	as	described	in	
Section	
5.7
,	take	this	idea	to	an
extreme	by	executing	instructions	in	a	completely	different	order
than	they	occur	in	the	machine-level	program.
“pCnd,”	and	so	on,	to	indicate	that	on	any	given	cycle,	they	hold	the
control	signals	generated	during	the	previous	cycle.
Figure	
4.40
shows	a	more	detailed	view	of	the	SEQ+	hardware.	We
can	see	that	it	contains	the	exact	same	hardware	units	and	control	blocks
that	we	had	in	SEQ	(
Figure	
4.23
),	but	with	the	PC	logic	shifted	from
the	top,	where	it	was	active	at	the	end	of	the	clock	cycle,	to	the	bottom,
where	it	is	active	at	the	beginning.
The	shift	of	state	elements	from	SEQ	to	SEQ+	is	an	example	of	a	general
transformation	known	as	
circuit	retiming</p>
<p>[68]
.	Retiming	changes	the	state
representation	for	a	system	without	changing	its	logical	behavior.	It	is
often	used	to	balance	the	delays	between	the	different	stages	of	a
pipelined	system.
4.5.2	
Inserting	Pipeline	Registers</p>
<p>In	our	first	attempt	at	creating	a	pipelined	Y86-64	processor,	we	insert
pipeline	registers	between	the	stages	of	SEQ+	and	rearrange	signals
somewhat,	yielding	the	PIPE—	processor,	where	the	&quot;-&quot;	in	the	name
signifies	that	this	processor	has	somewhat	less	performance	than	our
ultimate	processor	design.	The	structure	of	PIPE—	is	illustrated	in	
Figure
4.41
.	The	pipeline	registers	are	shown	in	this	figure	as	blue	boxes,
each	containing	different	fields	that	are	shown	as	white	boxes.	As
indicated	by	the	multiple	fields,	each	pipeline	register	holds	multiple	bytes
and	words.	Unlike	the	labels	shown	in	rounded	boxes	in	the	hardware
structure	of	the	two	sequential	processors	(
Figures	
4.23
and	
4.40
),
these	white	boxes	represent	actual	hardware	components.
Observe	that	PIPE—	uses	nearly	the	same	set	of	hardware	units	as	our
sequential	design	SEQ	(
Figure	
4.40
),	but	with	the	pipeline	registers
separating	the	stages.	The	differences	between	the	signals	in	the	two
systems	is	discussed	in	
Section	
4.5.3
.
The	pipeline	registers	are	labeled	as	follows:
F	
holds	a	
predicted
value	of	the	program	counter,	as	will	be	discussed
shortly.
D	
sits	between	the	fetch	and	decode	stages.	It	holds	information
about	the	most	recently	fetched	instruction	for	processing	by	the
decode	stage.</p>
<p>Figure	
4.40	
SEQ+	hardware	structure.
Shifting	the	PC	computation	from	the	end	of	the	clock	cycle	to	the
beginning	makes	it	more	suitable	for	pipelining.</p>
<p>Figure	
4.41	
Hardware	structure	of	PIPE—,	an	initial	pipelined
implementation.
By	inserting	pipeline	registers	into	SEQ+	(
Figure	
4.40
),	we	create	a
five-stage	pipeline.	There	are	several	shortcomings	of	this	version
that	we	will	deal	with	shortly.</p>
<p>E	
sits	between	the	decode	and	execute	stages.	It	holds	information
about	the	most	recently	decoded	instruction	and	the	values	read	from
the	register	file	for	processing	by	the	execute	stage.
M	
sits	between	the	execute	and	memory	stages.	It	holds	the	results	of
the	most	recently	executed	instruction	for	processing	by	the	memory
stage.	It	also	holds	information	about	branch	conditions	and	branch
targets	for	processing	conditional	jumps.
W	
sits	between	the	memory	stage	and	the	feedback	paths	that	supply
the	computed	results	to	the	register	file	for	writing	and	the	return
address	to	the	PC	selection	logic	when	completing	a	
instruction.
Figure	
4.42
shows	how	the	following	code	sequence	would	flow
through	our	five-stage	pipeline,	where	the	comments	identify	the
instructions	as	
to	
for	reference:</p>
<p>Figure	
4.42	
Example	of	instruction	flow	through	pipeline.
The	right	side	of	the	figure	shows	a	pipeline	diagram	for	this	instruction
sequence.	As	with	the	pipeline	diagrams	for	the	simple	pipelined
computation	units	of	
Section	
4.4
,	this	diagram	shows	the	progression
of	each	instruction	through	the	pipeline	stages,	with	time	increasing	from
left	to	right.	The	numbers	along	the	top	identify	the	clock	cycles	at	which
the	different	stages	occur.	For	example,	in	cycle	1,	instruction	
is
fetched,	and	it	then	proceeds	through	the	pipeline	stages,	with	its	result
being	written	to	the	register	file	after	the	end	of	cycle	5.	Instruction	
is
fetched	in	cycle	2,	and	its	result	is	written	back	after	the	end	of	cycle	6,
and	so	on.	At	the	bottom,	we	show	an	expanded	view	of	the	pipeline	for
cycle	5.	At	this	point,	there	is	an	instruction	in	each	of	the	pipeline	stages.
From	
Figure	
4.42
,	we	can	also	justify	our	convention	of	drawing
processors	so	that	the	instructions	flow	from	bottom	to	top.	The</p>
<p>expanded	view	for	cycle	5	shows	the	pipeline	stages	with	the	fetch	stage
on	the	bottom	and	the	write-back	stage	on	the	top,	just	as	do	our
diagrams	of	the	pipeline	hardware	(
Figure	
4.41
).	If	we	look	at	the
ordering	of	instructions	in	the	pipeline	stages,	we	see	that	they	appear	in
the	same	order	as	they	do	in	the	program	listing.	Since	normal	program
flow	goes	from	top	to	bottom	of	a	listing,	we	preserve	this	ordering	by
having	the	pipeline	flow	go	from	bottom	to	top.	This	convention	is
particularly	useful	when	working	with	the	simulators	that	accompany	this
text.
4.5.3	
Rearranging	and	Relabeling
Signals
Our	sequential	implementations	SEQ	and	SEQ+	only	process	one
instruction	at	a	time,	and	so	there	are	unique	values	for	signals	such	as
valC,	srcA,	and	valE.	In	our	pipelined	design,	there	will	be	multiple
versions	of	these	values	associated	with	the	different	instructions	flowing
through	the	system.	For	example,	in	the	detailed	structure	of	PIPE—,
there	are	four	white	boxes	labeled	&quot;Stat&quot;	that	hold	the	status	codes	for
four	different	instructions	(see	
Figure	
4.41
).	We	need	to	take	great
care	to	make	sure	we	use	the	proper	version	of	a	signal,	or	else	we	could
have	serious	errors,	such	as	storing	the	result	computed	for	one
instruction	at	the	destination	register	specified	by	another	instruction.	We
adopt	a	naming	scheme	where	a	signal	stored	in	a	pipeline	register	can
be	uniquely	identified	by	prefixing	its	name	with	that	of	the	pipe	register
written	in	uppercase.	For	example,	the	four	status	codes	are	named
D_stat,	E_stat,	M_stat,	and	W_stat.	We	also	need	to	refer	to	some</p>
<p>signals	that	have	just	been	computed	within	a	stage.	These	are	labeled
by	prefixing	the	signal	name	with	the	first	character	of	the	stage	name,
written	in	lowercase.	Using	the	status	codes	as	examples,	we	can	see
control	logic	blocks	labeled	&quot;Stat&quot;	in	the	fetch	and	memory	stages.	The
outputs	of	these	blocks	are	therefore	named	f_stat	and	m_stat.	We	can
also	see	that	the	actual	status	of	the	overall	processor	Stat	is	computed
by	a	block	in	the	write-back	stage,	based	on	the	status	value	in	pipeline
register	W.
The	decode	stages	of	SEQ+	and	PIPE—	both	generate	signals	dstE	and
dstM	indicating	the	destination	register	for	values	valE	and	valM.	In
SEQ+,	we	could	connect	these	signals	directly	to	the	address	inputs	of
the	register	file	write	ports.	With	PIPE-,	these	signals	are	carried	along	in
the	pipeline	through	the	execute	and	memory	stages	and	are	directed	to
the	register	file	only	once	they	reach
Aside	
What	is	the	difference	between
signals	M_stat	and	m_stat?
With	our	naming	system,	the	uppercase	prefixes	<code>D',	</code>E',	<code>M',	and </code>W	refer	to	pipeline	
registers
,	and	so	M_stat	refers	to	the	status
code	field	of	pipeline	register	M.	The	lowercase	prefixes	<code>f',	</code>d',	<code>e', </code>m',	and	`w'	refer	to	the	pipeline	
stages
,	and	so	m_stat	refers	to
the	status	signal	generated	in	the	memory	stage	by	a	control	logic
block.
Understanding	this	naming	convention	is	critical	to	understanding
the	operation	of	our	pipelined	processors.</p>
<p>the	write-back	stage	(shown	in	the	more	detailed	views	of	the	stages).
We	do	this	to	make	sure	the	write	port	address	and	data	inputs	hold
values	from	the	same	instruction.	Otherwise,	the	write	back	would	be
writing	the	values	for	the	instruction	in	the	write-back	stage,	but	with
register	IDs	from	the	instruction	in	the	decode	stage.	As	a	general
principle,	we	want	to	keep	all	of	the	information	about	a	particular
instruction	contained	within	a	single	pipeline	stage.
One	block	of	PIPE—	that	is	not	present	in	SEQ+	in	the	exact	same	form
is	the	block	labeled	&quot;Select	A&quot;	in	the	decode	stage.	We	can	see	that	this
block	generates	the	value	valA	for	the	pipeline	register	E	by	choosing
either	valP	from	pipeline	register	D	or	the	value	read	from	the	A	port	of
the	register	file.	This	block	is	included	to	reduce	the	amount	of	state	that
must	be	carried	forward	to	pipeline	registers	E	and	M.	Of	all	the	different
instructions,	only	the	
requires	valP	in	the	memory	stage.	Only	the
jump	instructions	require	the	value	of	valP	in	the	execute	stage	(in	the
event	the	jump	is	not	taken).	None	of	these	instructions	requires	a	value
read	from	the	register	file.	Therefore,	we	can	reduce	the	amount	of
pipeline	register	state	by	merging	these	two	signals	and	carrying	them
through	the	pipeline	as	a	single	signal	valA.	This	eliminates	the	need	for
the	block	labeled	&quot;Data&quot;	in	SEQ	(
Figure	
4.23
)	and	SEQ+	(
Figure
4.40
),	which	served	a	similar	purpose.	In	hardware	design,	it	is
common	to	carefully	identify	how	signals	get	used	and	then	reduce	the
amount	of	register	state	and	wiring	by	merging	signals	such	as	these.
As	shown	in	
Figure	
4.41
,	our	pipeline	registers	include	a	field	for	the
status	code	stat,	initially	computed	during	the	fetch	stage	and	possibly
modified	during	the	memory	stage.	We	will	discuss	how	to	implement	the
processing	of	exceptional	events	in	
Section	
4.5.6
,	after	we	have</p>
<p>covered	the	implementation	of	normal	instruction	execution.	Suffice	it	to
say	at	this	point	that	the	most	systematic	approach	is	to	associate	a
status	code	with	each	instruction	as	it	passes	through	the	pipeline,	as	we
have	indicated	in	the	figure.
4.5.4	
Next	PC	Prediction
We	have	taken	some	measures	in	the	design	of	PIPE—	to	properly
handle	control	dependencies.	Our	goal	in	the	pipelined	design	is	to	
issue
a	new	instruction	on	every	clock	cycle,	meaning	that	on	each	clock	cycle,
a	new	instruction	proceeds	into	the	execute	stage	and	will	ultimately	be
completed.	Achieving	this	goal	would
Aside	
Other	branch	prediction
strategies
Our	design	uses	an	
always	taken
branch	prediction	strategy.
Studies	show	this	strategy	has	around	a	60%	success	rate	
[44,
122]
.	Conversely,	a	
never	taken
(NT)	strategy	has	around	a	40%
success	rate.	A	slightly	more	sophisticated	strategy,	known	as
backward	taken,	forward	not	taken
(BTFNT),	predicts	that
branches	to	lower	addresses	than	the	next	instruction	will	be
taken,	while	those	to	higher	addresses	will	not	be	taken.	This
strategy	has	a	success	rate	of	around	65%.	This	improvement
stems	from	the	fact	that	loops	are	closed	by	backward	branches
and	loops	are	generally	executed	multiple	times.	Forward
branches	are	used	for	conditional	operations,	and	these	are	less</p>
<p>likely	to	be	taken.	In	Problems	4.55	and	4.56,	you	can	modify	the
Y86-64	pipeline	processor	to	implement	the	NT	and	BTFNT
branch	prediction	strategies.
As	we	saw	in	
Section	
3.6.6
,	mispredicted	branches	can
degrade	the	performance	of	a	program	considerably,	thus
motivating	the	use	of	conditional	data	transfer	rather	than
conditional	control	transfer	when	possible.
yield	a	throughput	of	one	instruction	per	cycle.	To	do	this,	we	must
determine	the	location	of	the	next	instruction	right	after	fetching	the
current	instruction.	Unfortunately,	if	the	fetched	instruction	is	a	conditional
branch,	we	will	not	know	whether	or	not	the	branch	should	be	taken	until
several	cycles	later,	after	the	instruction	has	passed	through	the	execute
stage.	Similarly,	if	the	fetched	instruction	is	a	ret,	we	cannot	determine
the	return	location	until	the	instruction	has	passed	through	the	memory
stage.
With	the	exception	of	conditional	jump	instructions	and	ret,	we	can
determine	the	address	of	the	next	instruction	based	on	information
computed	during	the	fetch	stage.	For	
and	
(unconditional	jump),
it	will	be	valC,	the	constant	word	in	the	instruction,	while	for	all	others	it
will	be	valP,	the	address	of	the	next	instruction.	We	can	therefore	achieve
our	goal	of	issuing	a	new	instruction	every	clock	cycle	in	most	cases	by
predicting
the	next	value	of	the	PC.	For	most	instruction	types,	our
prediction	will	be	completely	reliable.	For	conditional	jumps,	we	can
predict	either	that	a	jump	will	be	taken,	so	that	the	new	PC	value	would
be	valC,	or	that	it	will	not	be	taken,	so	that	the	new	PC	value	would	be
valP.	In	either	case,	we	must	somehow	deal	with	the	case	where	our
prediction	was	incorrect	and	therefore	we	have	fetched	and	partially</p>
<p>executed	the	wrong	instructions.	We	will	return	to	this	matter	in	
Section
4.5.8
.
This	technique	of	guessing	the	branch	direction	and	then	initiating	the
fetching	of	instructions	according	to	our	guess	is	known	as	
branch
prediction
.	It	is	used	in	some	form	by	virtually	all	processors.	Extensive
experiments	have	been	conducted	on	effective	strategies	for	predicting
whether	or	not	branches	will	be	taken	
[46,</p>
<p>Section	
2.3
].	Some
systems	devote	large	amounts	of	hardware	to	this	task.	In	our	design,	we
will	use	the	simple	strategy	of	predicting	that	conditional	branches	are
always	taken,	and	so	we	predict	the	new	value	of	the	PC	to	be	valC.
We	are	still	left	with	predicting	the	new	PC	value	resulting	from	a	
instruction.	Unlike	conditional	jumps,	we	have	a	nearly	unbounded	set	of
possible
Aside	
Return	address	prediction	with	a
stack
With	most	programs,	it	is	very	easy	to	predict	return	addresses,
since	procedure	calls	and	returns	occur	in	matched	pairs.	Most	of
the	time	that	a	procedure	is	called,	it	returns	to	the	instruction
following	the	call.	This	property	is	exploited	in	high-performance
processors	by	including	a	hardware	stack	within	the	instruction
fetch	unit	that	holds	the	return	address	generated	by	procedure
call	instructions.	Every	time	a	procedure	call	instruction	is
executed,	its	return	address	is	pushed	onto	the	stack.	When	a
return	instruction	is	fetched,	the	top	value	is	popped	from	this</p>
<p>stack	and	used	as	the	predicted	return	address.	Like	branch
prediction,	a	mechanism	must	be	provided	to	recover	when	the
prediction	was	incorrect,	since	there	are	times	when	calls	and
returns	do	not	match.	In	general,	the	prediction	is	highly	reliable.
This	hardware	stack	is	not	part	of	the	programmer-visible	state.
results,	since	the	return	address	will	be	whatever	word	is	on	the	top	of
the	stack.	In	our	design,	we	will	not	attempt	to	predict	any	value	for	the
return	address.	Instead,	we	will	simply	hold	off	processing	any	more
instructions	until	the	
instruction	passes	through	the	write-back	stage.
We	will	return	to	this	part	of	the	implementation	in	
Section	
4.5.8
.
The	PIPE—	fetch	stage,	diagrammed	at	the	bottom	of	
Figure	
4.41
,	is
responsible	for	both	predicting	the	next	value	of	the	PC	and	selecting	the
actual	PC	for	the	instruction	fetch.	We	can	see	the	block	labeled	&quot;Predict
PC&quot;	can	choose	either	valP	(as	computed	by	the	PC	incrementer)	or	valC
(from	the	fetched	instruction).	This	value	is	stored	in	pipeline	register	F	as
the	
predicted
value	of	the	program	counter.	The	block	labeled	&quot;Select
PC&quot;	is	similar	to	the	block	labeled	&quot;PC&quot;	in	the	SEQ+	PC	selection	stage
(
Figure	
4.40
).	It	chooses	one	of	three	values	to	serve	as	the	address
for	the	instruction	memory:	the	predicted	PC,	the	value	of	valP	for	a	not-
taken	branch	instruction	that	reaches	pipeline	register	M	(stored	in
register	M_valA),	or	the	value	of	the	return	address	when	a	
instruction	reaches	pipeline	register	W	(stored	in	W_valM).
4.5.5	
Pipeline	Hazards
Our	structure	PIPE—	is	a	good	start	at	creating	a	pipelined	Y86-64
processor.	Recall	from	our	discussion	in	
Section	
4.4.4
,	however,	that</p>
<p>introducing	pipelining	into	a	system	with	feedback	can	lead	to	problems
when	there	are	dependencies	between	successive	instructions.	We	must
resolve	this	issue	before	we	can	complete	our	design.	These
dependencies	can	take	two	forms:	(1)	
data
dependencies,	where	the
results	computed	by	one	instruction	are	used	as	the	data	for	a	following
instruction,	and	(2)	
control
dependencies,	where	one	instruction
determines	the	location	of	the	following	instruction,	such	as	when
executing	a	jump,	call,	or	return.	When	such	dependencies	have	the
potential	to	cause	an	erroneous	computation	by	the	pipeline,	they	are
called	
hazards
.	Like	dependencies,	hazards	can	be	classified	as	either
data	hazards
or	
control	hazards
.	We	first	concern	ourselves	with	data
hazards	and	then	consider	control	hazards.
Figure	
4.43	
Pipelined	execution	of	
without	special	pipeline
control.</p>
<p>In	cycle	6,	the	second	
writes	its	result	to	program	register	
.
The	
instruction	reads	its	source	operands	in	cycle	7,	so	it	gets
correct	values	for	both	
and	
.
Figure	
4.43
illustrates	the	processing	of	a	sequence	of	instructions	we
refer	to	as	
by	the	PIPE—	processor.	Let	us	assume	in	this	example
and	successive	ones	that	the	program	registers	initially	all	have	value	0.
The	code	loads	values	10	and	3	into	program	registers	
and	
,
executes	three	
instructions,	and	then	adds	register	
to	
.	We
focus	our	attention	on	the	potential	data	hazards	resulting	from	the	data
dependencies	between	the	two	
instructions	and	the	
instruction.	On	the	right-hand	side	of	the	figure,	we	show	a	pipeline
diagram	for	the	instruction	sequence.	The	pipeline	stages	for	cycles	6
and	7	are	shown	highlighted	in	the	pipeline	diagram.	Below	this,	we	show
an	expanded	view	of	the	write-back	activity	in	cycle	6	and	the	decode
activity	during	cycle	7.	After	the	start	of	cycle	7,	both	of	the	
instructions	have	passed	through	the	write	back	stage,	and	so	the
register	file	holds	the	updated	values	of	
and	
.	As	the	
instruction	passes	through	the	decode	stage	during	cycle	7,	it	will
therefore	read	the	correct	values	for	its	source	operands.	The	data
dependencies	between	the	two	
instructions	and	the	
instruction	have	not	created	data	hazards	in	this	example.
We	saw	that	
will	flow	through	our	pipeline	and	get	the	correct
results,	because	the	three	
instructions	create	a	delay	between
instructions	with	data</p>
<p>Figure	
4.44	
Pipelined	execution	of	
without	special	pipeline
control.
The	write	to	program	register	
does	not	occur	until	the	start	of	cycle
7,	and	so	the	
instruction	gets	the	incorrect	value	for	this	register	in
the	decode	stage.
dependencies.	Let	us	see	what	happens	as	these	
instructions	are
removed.	
Figure	
4.44
illustrates	the	pipeline	flow	of	a	program,	named
,	containing	two	
instructions	between	the	two	
instructions	generating	values	for	registers	
and	
and	the	
instruction	having	these	two	registers	as	operands.	In	this	case,	the
crucial	step	occurs	in	cycle	6,	when	the	
instruction	reads	its
operands	from	the	register	file.	An	expanded	view	of	the	pipeline
activities	during	this	cycle	is	shown	at	the	bottom	of	the	figure.	The	first
instruction	has	passed	through	the	write-back	stage,	and	so</p>
<p>program	register	
has	been	updated	in	the	register	file.	The	second
instruction	is	in	the	write-back	stage	during	this	cycle,	and	so	the
write	to	program	register	
only	occurs	at	the	start	of	cycle	7	as	the
clock	rises.	As	a	result,	the	incorrect	value	zero	would	be	read	for	register
(recall	that	we	assume	all	registers	are	initially	zero),	since	the
pending	write	for	this	register	has	not	yet	occurred.	Clearly,	we	will	have
to	adapt	our	pipeline	to	handle	this	hazard	properly.
Figure	
4.45
shows	what	happens	when	we	have	only	one	
instruction	between	the	
instructions	and	the	
instruction,
yielding	a	program	
.	Now	we	must	examine	the	behavior	of	the
pipeline	during	cycle	5	as	the	
instruction	passes	through	the	decode
stage.	Unfortunately,	the	pending</p>
<p>Figure	
4.45	
Pipelined	execution	of	
without	special	pipeline
control.
In	cycle	5,	the	
instruction	reads	its	source	operands	from	the
register	file.	The	pending	write	to	register	
is	still	in	the	write-back
stage,	and	the	pending	write	to	register	
is	still	in	the	memory	stage.
Both	operands	valA	and	valB	get	incorrect	values.
write	to	register	
is	still	in	the	write-back	stage,	and	the	pending	write
to	
is	still	in	the	memory	stage.	Therefore,	the	
instruction	would
get	the	incorrect	values	for	both	operands.
Figure	
4.46
shows	what	happens	when	we	remove	all	of	the	
instructions	between	the	
instructions	and	the	
instruction,
yielding	a	program	
.	Now	we	must	examine	the	behavior	of	the
pipeline	during	cycle	4	as	the	
instruction	passes	through	the	decode
stage.	Unfortunately,	the	pending	write	to	register	
is	still	in	the
memory	stage,	and	the	new	value	for	
is	just	being	computed	in	the
execute	stage.	Therefore,	the	
instruction	would	get	the	incorrect
values	for	both	operands.
These	examples	illustrate	that	a	data	hazard	can	arise	for	an	instruction
when	one	of	its	operands	is	updated	by	any	of	the	three	preceding
instructions.	These	hazards	occur	because	our	pipelined	processor	reads
the	operands	for	an	instruction	from	the	register	file	in	the	decode	stage
but	does	not	write	the	results	for	the	instruction	to	the	register	file	until
three	cycles	later,	after	the	instruction	passes	through	the	write-back
stage.</p>
<p>Figure	
4.46	
Pipelined	execution	of	</p>
<p>without	special	pipeline
control.
In	cycle	4,	the	
instruction	reads	its	source	operands	from	the
register	file.	The	pending	write	to	register	
is	still	in	the	memory
stage,	and	the	new	value	for	register	
is	just	being	computed	in	the
execute	stage.	Both	operands	valA	and	valB	get	incorrect	values.
Avoiding	Data	Hazards	by	Stalling
One	very	general	technique	for	avoiding	hazards	involves	
stalling
,	where
the	processor	holds	back	one	or	more	instructions	in	the	pipeline	until	the
hazard	condition	no	longer	holds.	Our	processor	can	avoid	data	hazards
by	holding	back	an	instruction	in	the	decode	stage	until	the	instructions
generating	its	source	operands	have	passed	through	the	write-back
stage.	The	details	of	this	mechanism	will	be	discussed	in	
Section</p>
<p>4.5.8
.	It	involves	simple	enhancements	to	the	pipeline	control	logic.
The	effect	of	stalling	is	diagrammed	in	
Figure	
4.47
(
)	and	
Figure
4.48
(
).	(We	omit	
from	this	discussion,	since	it	operates
similarly	to	the	other	two	examples.)	When	the	
instruction	is	in	the
decode	stage,	the	pipeline	control	logic	detects	that	at	least	one	of	the
instructions	in	the	execute,	memory,	or	write-back	stage	will	update	either
register	
or	register	
.	Rather	than	letting	the	
instruction
pass	through	the	stage	with	the	incorrect	results,	it	stalls	the	instruction,
holding	it	back	in	the	decode	stage	for	either	one	(for	
)	or	three	(for
)	extra	cycles.	For	all	three	programs,	the	
instruction	finally
gets	correct	values	for	its	two	source	operands	in	cycle	7	and	then
proceeds	down	the	pipeline.</p>
<p>Figure	
4.47	
Pipelined	execution	of	
using	stalls.
After	decoding	the	
instruction	in	cycle	6,	the	stall	control	logic
detects	a	data	hazard	due	to	the	pending	write	to	register	
in	the
write-back	stage.	It	injects	a	bubble	into	the	execute	stage	and	repeats
the	decoding	of	the	
instruction	in	cycle	7.	In	effect,	the	machine	has
dynamically	inserted	a	
instruction,	giving	a	flow	similar	to	that	shown
for	
(
Figure	
4.43
).
Figure	
4.48	
Pipelined	execution	of	
using	stalls.
After	decoding	the	
instruction	in	cycle	4,	the	stall	control	logic
detects	data	hazards	for	both	source	registers.	It	injects	a	bubble	into	the
execute	stage	and	repeats	the	decoding	of	the	
instruction	on	cycle
5.	It	again	detects	hazards	for	both	source	registers,	injects	a	bubble	into
the	execute	stage,	and	repeats	the	decoding	of	the	
instruction	on
cycle	6.	Still,	it	detects	a	hazard	for	source	register	
,	injects	a	bubble
into	the	execute	stage,	and	repeats	the	decoding	of	the	
instruction
on	cycle	7.	In	effect,	the	machine	has	dynamically	inserted	three	
instructions,	giving	a	flow	similar	to	that	shown	for	
(
Figure	
4.43
).</p>
<p>In	holding	back	the	
instruction	in	the	decode	stage,	we	must	also
hold	back	the	halt	instruction	following	it	in	the	fetch	stage.	We	can	do
this	by	keeping	the	program	counter	at	a	fixed	value,	so	that	the	halt
instruction	will	be	fetched	repeatedly	until	the	stall	has	completed.
Stalling	involves	holding	back	one	group	of	instructions	in	their	stages
while	allowing	other	instructions	to	continue	flowing	through	the	pipeline.
What	then	should	we	do	in	the	stages	that	would	normally	be	processing
the	
instruction?	We	handle	these	by	injecting	a	
bubble
into	the
execute	stage	each	time	we	hold	an	instruction	back	in	the	decode	stage.
A	bubble	is	like	a	dynamically	generated	
instruction—it	does	not
cause	any	changes	to	the	registers,	the	memory,	the
Aside	
Enumerating	classes	of	data
hazards
Hazards	can	potentially	occur	when	one	instruction	updates	part
of	the	program	state	that	will	be	read	by	a	later	instruction.	For
Y86-64,	the	program	state	includes	the	program	registers,	the
program	counter,	the	memory,	the	condition	code	register,	and	the
status	register.	Let	us	look	at	the	hazard	possibilities	in	our
proposed	design	for	each	of	these	forms	of	state.
Program	registers.	
These	are	the	hazards	we	have	already
identified.	They	arise	because	the	register	file	is	read	in	one
stage	and	written	in	another,	leading	to	possible	unintended
interactions	between	different	instructions.</p>
<p>Program	counter.	
Conflicts	between	updating	and	reading	the
program	counter	give	rise	to	control	hazards.	No	hazard	arises
when	our	fetch-stage	logic	correctly	predicts	the	new	value	of
the	program	counter	before	fetching	the	next	instruction.
Mispredicted	branches	and	
instructions	require	special
handling,	as	will	be	discussed	in	
Section	
4.5.5
.
Memory.	
Writes	and	reads	of	the	data	memory	both	occur	in
the	memory	stage.	By	the	time	an	instruction	reading	memory
reaches	this	stage,	any	preceding	instructions	writing	memory
will	have	already	done	so.	On	the	other	hand,	there	can	be
interference	between	instructions	writing	data	in	the	memory
stage	and	the	reading	of	instructions	in	the	fetch	stage,	since
the	instruction	and	data	memories	reference	a	single	address
space.	This	can	only	happen	with	programs	containing	
self-
modifying	code
,	where	instructions	write	to	a	portion	of
memory	from	which	instructions	are	later	fetched.	Some
systems	have	complex	mechanisms	to	detect	and	avoid	such
hazards,	while	others	simply	mandate	that	programs	should
not	use	self-modifying	code.	We	will	assume	for	simplicity	that
programs	do	not	modify	themselves,	and	therefore	we	do	not
need	to	take	special	measures	to	update	the	instruction
memory	based	on	updates	to	the	data	memory	during	program
execution.
Condition	code	register.	
These	are	written	by	integer
operations	in	the	execute	stage.	They	are	read	by	conditional
moves	in	the	execute	stage	and	by	conditional	jumps	in	the
memory	stage.	By	the	time	a	conditional	move	or	jump</p>
<p>reaches	the	execute	stage,	any	preceding	integer	operation
will	have	already	completed	this	stage.	No	hazards	can	arise.
Status	register.	
The	program	status	can	be	affected	by
instructions	as	they	flow	through	the	pipeline.	Our	mechanism
of	associating	a	status	code	with	each	instruction	in	the
pipeline	enables	the	processor	to	come	to	an	orderly	halt	when
an	exception	occurs,	as	will	be	discussed	in	
Section	
4.5.6
.
This	analysis	shows	that	we	only	need	to	deal	with	register	data
hazards,	control	hazards,	and	making	sure	exceptions	are
handled	properly.	A	systematic	analysis	of	this	form	is	important
when	designing	a	complex	system.	It	can	identify	the	potential
difficulties	in	implementing	the	system,	and	it	can	guide	the
generation	of	test	programs	to	be	used	in	checking	the
correctness	of	the	system.
condition	codes,	or	the	program	status.	These	are	shown	as	white	boxes
in	the	pipeline	diagrams	of	
Figures	
4.47
and	
4.48
.	In	these	figures
the	arrow	between	the	box	labeled	&quot;D&quot;	for	the	
instruction	and	the
box	labeled	&quot;E&quot;	for	one	of	the	pipeline	bubbles	indicates	that	a	bubble
was	injected	into	the	execute	stage	in	place	of	the	
instruction	that
would	normally	have	passed	from	the	decode	to	
the	execute	stage.	We
will	look	at	the	detailed	mechanisms	for	making	the	pipeline	stall	and	for
injecting	bubbles	in	
Section	
4.5.8
.
In	using	stalling	to	handle	data	hazards,	we	effectively	execute	programs
and	
by	dynamically	generating	the	pipeline	flow	seen	for
(
Figure	
4.43
).	Injecting	one	bubble	for	
and	three	for	
has	the	same	effect	as	having	three	
instructions	between	the	second</p>
<pre><code>instruction	and	the	
instruction.	This	mechanism	can	be
</code></pre>
<p>implemented	fairly	easily	(see	
Problem	
4.53
),	but	the	resulting
performance	is	not	very	good.	There	are	numerous	cases	in	which	one
instruction	updates	a	register	and	a	closely	following	instruction	uses	the
same	register.	This	will	cause	the	pipeline	to	stall	for	up	to	three	cycles,
reducing	the	overall	throughput	significantly.
Avoiding	Data	Hazards	by	Forwarding
Our	design	for	PIPE—	reads	source	operands	from	the	register	file	in	the
decode	stage,	but	there	can	also	be	a	pending	write	to	one	of	these
source	registers	in	the	write-back	stage.	Rather	than	stalling	until	the
write	has	completed,	it	can	simply	pass	the	value	that	is	about	to	be
written	to	pipeline	register	E	as	the	source	operand.	
Figure	
4.49
shows	this	strategy	with	an	expanded	view	of	the	pipeline	diagram	for
cycle	6	of	
.	The	decode-stage	logic	detects	that	register</p>
<p>Figure	
4.49	
Pipelined	execution	of	</p>
<p>using	forwarding.
In	cycle	6,	the	decode-stage	logic	detects	the	presence	of	a	pending
write	to	register	
in	the	write-back	stage.	It	uses	this	value	for	source
operand	valB	rather	than	the	value	read	from	the	register	file.
Figure	
4.50	
Pipelined	execution	of	</p>
<p>using	forwarding.
In	cycle	5,	the	decode-stage	logic	detects	a	pending	write	to	register	
in	the	write-back	stage	and	to	register	
in	the	memory	stage.	It	uses
these	as	the	values	for	valA	and	valB	rather	than	the	values	read	from
the	register	file.
is	the	source	register	for	operand	valB,	and	that	there	is	also	a
pending	write	to	
on	write	port	E.	It	can	therefore	avoid	stalling	by</p>
<p>simply	using	the	data	word	supplied	to	port	E	(signal	W_valE)	as	the
value	for	operand	valB.	This	technique	of	passing	a	result	value	directly
from	one	pipeline	stage	to	an	earlier	one	is	commonly	known	as	
data
forwarding
(or	simply	
forwarding
,	and	sometimes	
bypassing
).	It	allows	the
instructions	of	
to	proceed	through	the	pipeline	without	any	stalling.
Data	forwarding	requires	adding	additional	data	connections	and	control
logic	to	the	basic	hardware	structure.
As	
Figure	
4.50
illustrates,	data	forwarding	can	also	be	used	when
there	is	a	pending	write	to	a	register	in	the	memory	stage,	avoiding	the
need	to	stall	for	program	
.	In	cycle	5,	the	decode-stage	logic
detects	a	pending	write	to	register	
on	port	E	in	the	write-back	stage,
as	well	as	a	pending	write	to	register	
that	is	on	its	way	to	port	E	but
is	still	in	the	memory	stage.	Rather	than	stalling	until	the	writes	have
occurred,	it	can	use	the	value	in	the	write-back	stage	(signal	W_valE)	for
operand	valA	and	the	value	in	the	memory	stage	(signal	M_valE)	for
operand	valB.</p>
<p>Figure	
4.51	
Pipelined	execution	of	</p>
<p>using	forwarding.
In	cycle	4,	the	decode-stage	logic	detects	a	pending	write	to	register	
in	the	memory	stage.	It	also	detects	that	a	new	value	is	being	computed
for	register	
in	the	execute	stage.	It	uses	these	as	the	values	for	valA
and	valB	rather	than	the	values	read	from	the	register	file.
To	exploit	data	forwarding	to	its	full	extent,	we	can	also	pass	newly
computed	values	from	the	execute	stage	to	the	decode	stage,	avoiding
the	need	to	stall	for	program	
,	as	illustrated	in	
Figure	
4.51
.	In
cycle	4,	the	decode-stage	logic	detects	a	pending	write	to	register	
in
the	memory	stage,	and	also	that	the	value	being	computed	by	the	ALU	in
the	execute	stage	will	later	be	written	to	register	
.	It	can	use	the
value	in	the	memory	stage	(signal	M_valE)	for	operand	valA.	It	can	also
use	the	ALU	output	(signal	e_valE)	for	operand	valB.	Note	that	using	the
ALU	output	does	not	introduce	any	timing	problems.	The	decode	stage
only	needs	to	generate	signals	valA	and	valB	by	the	end	of	the	clock</p>
<p>cycle	so	that	pipeline	register	E	can	be	loaded	with	the	results	from	the
decode	stage	as	the	clock	rises	to	start	the	next	cycle.	The	ALU	output
will	be	valid	before	this	point.
The	uses	of	forwarding	illustrated	in	programs	
to	
all	involve
the	forwarding	of	values	generated	by	the	ALU	and	destined	for	write	port
E.	Forwarding	can	also	be	used	with	values	read	from	the	memory	and
destined	for	write	port	M.	From	the	memory	stage,	we	can	forward	the
value	that	has	just	been	read	from	the	data	memory	(signal	m_valM).
From	the	write-back	stage,	we	can	forward	the	pending	write	to	port	M
(signal	W_valM).	This	gives	a	total	of	five	different	forwarding	sources
(e_valE,	m_valM,	M_valE,	W_valM,	and	W_valE)	and	two	different
forwarding	destinations	(valA	and	valB).
The	expanded	diagrams	of	
Figures	
4.49
to	
4.51
also	show	how	the
decode-stage	logic	can	determine	whether	to	use	a	value	from	the
register	file	or	to	use	a	forwarded	value.	Associated	with	every	value	that
will	be	written	back	to	the	register	file	is	the	destination	register	ID.	The
logic	can	compare	these	IDs	with	the	source	register	IDs	srcA	and	srcB
to	detect	a	case	for	forwarding.	It	is	possible	to	have	multiple	destination
register	IDs	match	one	of	the	source	IDs.	We	must	establish	a	priority
among	the	different	forwarding	sources	to	handle	such	cases.	This	will	be
discussed	when	we	look	at	the	detailed	design	of	the	forwarding	logic.
Figure	
4.52
shows	the	structure	of	PIPE,	an	extension	of	PIPE—	that
can	handle	data	hazards	by	forwarding.	Comparing	this	to	the	structure
of	PIPE—(
Figure	
4.41
),	we	can	see	that	the	values	from	the	five
forwarding	sources	are	fed	back	to	the	two	blocks	labeled	&quot;Sel+Fwd	A&quot;
and	&quot;Fwd	B&quot;	in	the	decode	stage.	The	block	labeled	&quot;Sel+Fwd	A&quot;</p>
<p>combines	the	role	of	the	block	labeled	&quot;Select	A&quot;	in	PIPE—	with	the
forwarding	logic.	It	allows	valA	for	pipeline	register	E	to	be	either	the
incremented	program	counter	valP,	the	value	read	from	the	A	port	of	the
register	file,	or	one	of	the	forwarded	values.	The	block	labeled	&quot;Fwd	B&quot;
implements	the	forwarding	logic	for	source	operand	valB.
Load/Use	Data	Hazards
One	class	of	data	hazards	cannot	be	handled	purely	by	forwarding,
because	memory	reads	occur	late	in	the	pipeline.	
Figure	
4.53
illustrates	an	example	of	a	
load/use	hazard
,	where	one	instruction	(the
at	address	
)	reads	a	value	from	memory	for	register	
while	the	next	instruction	(the	
at	address	
)	needs	this	value	as
a	source	operand.	Expanded	views	of	cycles	7	and	8	are	shown	in	the
lower	part	of	the	figure,	where	we	assume	all	program	registers	initially
have	value	0.	The	
instruction	requires	the	value	of	the	register	in
cycle	7,	but	it	is	not	generated	by	the	
instruction	until	cycle	8.	In
order	to	&quot;forward&quot;	from	the	
to	the	
,	the	forwarding	logic	would
have	to	make	the	value	go	backward	in	time!	Since	this	is	clearly
impossible,	we	must	find	some	other	mechanism	for	handling	this	form	of
data	hazard.	(The	data	hazard	for	register	
,	with	the	value	being
generated	by	the	
instruction	at	address	
and	used	by	the
instruction	at	address	
,	can	be	handled	by	forwarding.)
As	
Figure	
4.54
demonstrates,	we	can	avoid	a	load/use	data	hazard
with	a	combination	of	stalling	and	forwarding.	This	requires	modifications
of	the	control	logic,	but	it	can	use	existing	bypass	paths.	As	the	
instruction	passes	through	the	execute	stage,	the	pipeline	control	logic</p>
<p>detects	that	the	instruction	in	the	decode	stage	(the	
)	requires	the
result	read	from	memory.	It	stalls	the	instruction	in	the	decode	stage	for
one	cycle,	causing	a	bubble	to	be	injected	into	the	execute	stage.	As	the
expanded	view	of	cycle	8	shows,	the	value	read	from	memory	can	then
be	forwarded	from	the	memory	stage	to	the	
instruction	in	the
decode	stage.	The	value	for	register	
is	also	forwarded	from	the
write-back	to	the	memory	stage.	As	indicated	in	the	pipeline	diagram	by
the	arrow	from	the	box	labeled	&quot;D&quot;	in	cycle	7	to	the	box	labeled	&quot;E&quot;	in
cycle	8,	the	injected	bubble	replaces	the	
instruction	that	would
normally	continue	flowing	through	the	pipeline.</p>
<p>Figure	
4.52	
Hardware	structure	of	PIPE,	our	final	pipelined
implementation.
The	additional	bypassing	paths	enable	forwarding	the	results	from	the
three	preceding	instructions.	This	allows	us	to	handle	most	forms	of	data
hazards	without	stalling	the	pipeline.</p>
<p>Figure	
4.53	
Example	of	load/use	data	hazard.
The	
instruction	requires	the	value	of	register	
during	the
decode	stage	in	cycle	7.	The	preceding	
reads	a	new	value	for	this
register	during	the	memory	stage	in	cycle	8,	which	is	too	late	for	the	
instruction.
This	use	of	a	stall	to	handle	a	load/use	hazard	is	called	a	
load	interlock
.
Load	interlocks	combined	with	forwarding	suffice	to	handle	all	possible
forms	of	data	hazards.	Since	only	load	interlocks	reduce	the	pipeline
throughput,	we	can	nearly	achieve	our	throughput	goal	of	issuing	one
new	instruction	on	every	clock	cycle.
Avoiding	Control	Hazards</p>
<p>Control	hazards	arise	when	the	processor	cannot	reliably	determine	the
address	of	the	next	instruction	based	on	the	current	instruction	in	the
fetch	stage.	As	was	discussed	in	
Section	
4.5.4
,	control	hazards	can
only	occur	in	our	pipelined	processor	for	
and	jump	instructions.
Moreover,	the	latter	case	only	causes	difficulties	when	the	direction	of	a
conditional	jump	is	mispredicted.	In	this	section,	we	provide	a	high-level
view	of	how	these	hazards	can	be	handled.	The	detailed	implementation
will	be	presented	in	
Section	
4.5.8
as	part	of	a	more	general	discussion
of	the	pipeline	control.
For	the	
instruction,	consider	the	following	example	program.	This
program	is	shown	in	assembly	code,	but	with	the	addresses	of	the
different	instructions	on	the	left	for	reference:</p>
<p>Figure	
4.54	
Handling	a	load/use	hazard	by	stalling.
By	stalling	the	
instruction	for	one	cycle	in	the	decode	stage,	the
value	for	valB	can	be	forwarded	from	the	
instruction	in	the
memory	stage	to	the	
instruction	in	the	decode	stage.</p>
<p>Figure	
4.55
shows	how	we	want	the	pipeline	to	process	the	
instruction.	As	with	our	earlier	pipeline	diagrams,	this	figure	shows	the
pipeline	activity	with
Figure	
4.55	
Simplified	view	of	
instruction	processing.
The	pipeline	should	stall	while	the	
passes	through	the	decode,
execute,	and	memory	stages,	injecting	three	bubbles	in	the	process.	The
PC	selection	logic	will	choose	the	return	address	as	the	instruction	fetch
address	once	the	
reaches	the	write-back	stage	(cycle	7).
time	growing	to	the	right.	Unlike	before,	the	instructions	are	not	listed	in
the	same	order	they	occur	in	the	program,	since	this	program	involves	a
control	flow	where	instructions	are	not	executed	in	a	linear	sequence.	It	is
useful	to	look	at	the	instruction	addresses	to	identify	the	different
instructions	in	the	program.</p>
<p>As	this	diagram	shows,	the	
instruction	is	fetched	during	cycle	3	and
proceeds	down	the	pipeline,	reaching	the	write-back	stage	in	cycle	7.
While	it	passes	through	the	decode,	execute,	and	memory	stages,	the
pipeline	cannot	do	any	useful	activity.	Instead,	we	want	to	inject	three
bubbles	into	the	pipeline.	Once	the	
instruction	reaches	the	write-back
stage,	the	PC	selection	logic	will	set	the	program	counter	to	the	return
address,	and	therefore	the	fetch	stage	will	fetch	the	
instruction	at
the	return	point	(address	
).
To	handle	a	mispredicted	branch,	consider	the	following	program,	shown
in	assembly	code	but	with	the	instruction	addresses	shown	on	the	left	for
reference:
Figure	
4.56
shows	how	these	instructions	are	processed.	As	before,
the	instructions	are	listed	in	the	order	they	enter	the	pipeline,	rather	than
the	order	they	occur	in	the	program.	Since	the	jump	instruction	is
predicted	as	being	taken,	the	instruction	at	the	jump	target	will	be	fetched
in	cycle	3,	and	the	instruction	following	this	one	will	be	fetched	in	cycle	4.</p>
<p>By	the	time	the	branch	logic	detects	that	the	jump	should	not	be	taken
during	cycle	4,	two	instructions	have	been	fetched	that	should	not
continue	being	executed.	Fortunately,	neither	of	these	instructions	has
caused	a	change	in	the	programmer-visible	state.	That	can	only	occur
when	an	instruction
Figure	
4.56	
Processing	mispredicted	branch	instructions.
The	pipeline	predicts	branches	will	be	taken	and	so	starts	fetching
instructions	at	the	jump	target.	Two	instructions	are	fetched	before	the
misprediction	is	detected	in	cycle	4	when	the	jump	instruction	flows
through	the	execute	stage.	In	cycle	5,	the	pipeline	
cancels
the	two	target
instructions	by	injecting	bubbles	into	the	decode	and	execute	stages,	and
it	also	fetches	the	instruction	following	the	jump.
reaches	the	execute	stage,	where	it	can	cause	the	condition	codes	to
change.	At	this	point,	the	pipeline	can	simply	
cancel
(sometimes	called
instruction	squashing
)	the	two	misfetched	instructions	by	injecting
bubbles	into	the	decode	and	execute	stages	on	the	following	cycle	while
also	fetching	the	instruction	following	the	jump	instruction.	The	two
misfetched	instructions	will	then	simply	disappear	from	the	pipeline	and
therefore	not	have	any	effect	on	the	programmer-visible	state.	The	only</p>
<p>drawback	is	that	two	clock	cycles'	worth	of	instruction	processing
capability	have	been	wasted.
This	discussion	of	control	hazards	indicates	that	they	can	be	handled	by
careful	consideration	of	the	pipeline	control	logic.	Techniques	such	as
stalling	and	injecting	bubbles	into	the	pipeline	dynamically	adjust	the
pipeline	flow	when	special	conditions	arise.	As	we	will	discuss	in	
Section
4.5.8
,	a	simple	extension	to	the	basic	clocked	register	design	will
enable	us	to	stall	stages	and	to	inject	bubbles	into	pipeline	registers	as
part	of	the	pipeline	control	logic.
4.5.6	
Exception	Handling
As	we	will	discuss	in	
Chapter	
8
,	a	variety	of	activities	in	a	processor
can	lead	to	
exceptional	control	flow
,	where	the	normal	chain	of	program
execution	gets	broken.	Exceptions	can	be	generated	either	
internally
,	by
the	executing	program,	or	
externally
,	by	some	outside	signal.	Our
instruction	set	architecture	includes	three	different	internally	generated
exceptions,	caused	by	(1)	a	halt	instruction,	(2)	an	instruction	with	an
invalid	combination	of	instruction	and	function	code,	and	(3)	an	attempt	to
access	an	invalid	address,	either	for	instruction	fetch	or	data	read	or
write.	A	more	complete	processor	design	would	also	handle	external
exceptions,	such	as	when	the	processor	receives	a	signal	that	the
network	interface	has	received	a	new	packet	or	the	user	has	clicked	a
mouse	button.	Handling	
exceptions	correctly	is	a	challenging	aspect	of
any	microprocessor	design.	They	can	occur	at	unpredictable	times,	and
they	require	creating	a	clean	break	in	the	flow	of	instructions	through	the
processor	pipeline.	Our	handling	of	the	three	internal	exceptions	gives</p>
<p>just	a	glimpse	of	the	true	complexity	of	correctly	detecting	and	handling
exceptions.
Let	us	refer	to	the	instruction	causing	the	exception	as	the	
excepting
instruction.
In	the	case	of	an	invalid	instruction	address,	there	is	no	actual
excepting	instruction,	but	it	is	useful	to	think	of	there	being	a	sort	of
&quot;virtual	instruction&quot;	at	the	invalid	address.	In	our	simplified	ISA	model,	we
want	the	processor	to	halt	when	it	reaches	an	exception	and	to	set	the
appropriate	status	code,	as	listed	in	
Figure	
4.5
.	It	should	appear	that
all	instructions	up	to	the	excepting	instruction	have	completed,	but	none
of	the	following	instructions	should	have	any	effect	on	the	programmer-
visible	state.	In	a	more	complete	design,	the	processor	would	continue	by
invoking	an	exception	handler,	a	procedure	that	is	part	of	the	operating
system,	but	implementing	this	part	of	exception	handling	is	beyond	the
scope	of	our	presentation.
In	a	pipelined	system,	exception	handling	involves	several	subtleties.
First,	it	is	possible	to	have	exceptions	triggered	by	multiple	instructions
simultaneously.	For	example,	during	one	cycle	of	pipeline	operation,	we
could	have	a	halt	instruction	in	the	fetch	stage,	and	the	data	memory
could	report	an	out-of-bounds	data	address	for	the	instruction	in	the
memory	stage.	We	must	determine	which	of	these	exceptions	the
processor	should	report	to	the	operating	system.	The	basic	rule	is	to	put
priority	on	the	exception	triggered	by	the	instruction	that	is	furthest	along
the	pipeline.	In	the	example	above,	this	would	be	the	out-of-bounds
address	attempted	by	the	instruction	in	the	memory	stage.	In	terms	of	the
machine-language	program,	the	instruction	in	the	memory	stage	should
appear	to	execute	before	one	in	the	fetch	stage,	and	therefore	only	this
exception	should	be	reported	to	the	operating	system.</p>
<p>A	second	subtlety	occurs	when	an	instruction	is	first	fetched	and	begins
execution,	causes	an	exception,	and	later	is	canceled	due	to	a
mispredicted	branch.	The	following	is	an	example	of	such	a	program	in
its	object-code	form:
In	this	program,	the	pipeline	will	predict	that	the	branch	should	be	taken,
and	so	it	will	fetch	and	attempt	to	use	a	byte	with	value	
as	an
instruction	(generated	in	the	assembly	code	using	the	
directive).
The	decode	stage	will	therefore	detect	an	invalid	instruction	exception.
Later,	the	pipeline	will	discover	that	the	branch	should	not	be	taken,	and
so	the	instruction	at	address	
should	never	even	have	been
fetched.	The	pipeline	control	logic	will	cancel	this	instruction,	but	we	want
to	avoid	raising	an	exception.
A	third	subtlety	arises	because	a	pipelined	processor	updates	different
parts	of	the	system	state	in	different	stages.	It	is	possible	for	an
instruction	following	one	causing	an	exception	to	alter	some	part	of	the
state	before	the	excepting	instruction	completes.	For	example,	consider</p>
<p>the	following	code	sequence,	in	which	we	assume	that	user	programs	are
not	allowed	to	access	addresses	at	the	upper	end	of	the	64-bit	range:
The	
instruction	causes	an	address	exception,	because
decrementing	the	stack	pointer	causes	it	to	wrap	around	to
.	This	exception	is	detected	in	the	memory	stage.	On
the	same	cycle,	the	
instruction	is	in	the	execute	stage,	and	it	will
cause	the	condition	codes	to	be	set	to	new	values.	This	would	violate	our
requirement	that	none	of	the	instructions	following	the	excepting
instruction	should	have	had	any	effect	on	the	system	state.
In	general,	we	can	both	correctly	choose	among	the	different	exceptions
and	avoid	raising	exceptions	for	instructions	that	are	fetched	due	to
mispredicted	branches	by	merging	the	exception-handling	logic	into	the
pipeline	structure.	That	is	the	motivation	for	us	to	include	a	status	code
stat	in	each	of	our	pipeline	registers	(
Figures	
4.41
and	
4.52
).	If	an
instruction	generates	an	exception	at	some	stage	in	its	processing,	the
status	field	is	set	to	indicate	the	nature	of	the	exception.	The	exception
status	propagates	through	the	pipeline	with	the	rest	of	the	information	for
that	instruction,	until	it	reaches	the	write-back	stage.	At	this	point,	the</p>
<p>pipeline	control	logic	detects	the	occurrence	of	the	exception	and	stops
execution.
To	avoid	having	any	updating	of	the	programmer-visible	state	by
instructions	beyond	the	excepting	instruction,	the	pipeline	control	logic
must	disable	any	updating	of	the	condition	code	register	or	the	data
memory	when	an	instruction	in	the	memory	or	write-back	stages	has
caused	an	exception.	In	the	example	program	above,	the	control	logic	will
detect	that	the	
in	the	memory	stage	has	caused	an	exception,	and
therefore	the	updating	of	the	condition	code	register	by	the	
instruction	in	the	execute	stage	will	be	disabled.
Let	us	consider	how	this	method	of	handling	exceptions	deals	with	the
subtleties	we	have	mentioned.	When	an	exception	occurs	in	one	or	more
stages	of	a	pipeline,	the	information	is	simply	stored	in	the	status	fields	of
the	pipeline	registers.	The	event	has	no	effect	on	the	flow	of	instructions
in	the	pipeline	until	an	excepting	instruction	reaches	the	final	pipeline
stage,	except	to	disable	any	updating	of	the	programmer-visible	state
(the	condition	code	register	and	the	memory)	by	later	instructions	in	the
pipeline.	Since	instructions	reach	the	write-back	stage	in	the	same	order
as	they	would	be	executed	in	a	nonpipelined	processor,	we	are
guaranteed	that	the	first	instruction	encountering	an	exception	will	arrive
first	in	the	write-back	stage,	at	which	point	program	execution	can	stop
and	the	status	code	in	pipeline	register	W	can	be	recorded	as	the
program	status.	If	some	instruction	is	fetched	but	later	canceled,	any
exception	status	information	about	the	
instruction	gets	canceled	as	well.
No	instruction	following	one	that	causes	an	exception	can	alter	the
programmer-visible	state.	The	simple	rule	of	carrying	the	exception	status
together	with	all	other	information	about	an	instruction	through	the</p>
<p>pipeline	provides	a	simple	and	reliable	mechanism	for	handling
exceptions.
4.5.7	
PIPE	Stage	Implementations
We	have	now	created	an	overall	structure	for	PIPE,	our	pipelined	Y86-64
processor	with	forwarding.	It	uses	the	same	set	of	hardware	units	as	the
earlier	sequential	designs,	with	the	addition	of	pipeline	registers,	some
reconfigured	logic	blocks,	and	additional	pipeline	control	logic.	In	this
section,	we	go	through	the	design	of	the	different	logic	blocks,	deferring
the	design	of	the	pipeline	control	logic	to	the	next	section.	Many	of	the
logic	blocks	are	identical	to	their	counterparts	in	SEQ	and	SEQ+,	except
that	we	must	choose	proper	versions	of	the	different	signals	from	the
pipeline	registers	(written	with	the	pipeline	register	name,	written	in
uppercase,	as	a	prefix)	or	from	the	stage	computations	(written	with	the
first	character	of	the	stage	name,	written	in	lowercase,	as	a	prefix).
As	an	example,	compare	the	HCL	code	for	the	logic	that	generates	the
srcA	signal	in	SEQ	to	the	corresponding	code	in	PIPE:</p>
<p>They	differ	only	in	the	prefixes	added	to	the	PIPE	signals:	
for	the
source	values,	to	indicate	that	the	signals	come	from	pipeline	register	D,
and	
for	the	result	value,	to	indicate	that	it	is	generated	in	the	decode
stage.	To	avoid	repetition,	we	will	not	show	the	HCL	code	here	for	blocks
that	only	differ	from	those	in	SEQ	because	of	the	prefixes	on	names.	As
a	reference,	the	complete	HCL	code	for	PIPE	is	given	in	Web	Aside
ARCH
:
HCL</p>
<p>on	page	472.
PC	Selection	and	Fetch	Stage
Figure	
4.57
provides	a	detailed	view	of	the	PIPE	fetch	stage	logic.	As
discussed	earlier,	this	stage	must	also	select	a	current	value	for	the
program	counter	and	predict	the	next	PC	value.	The	hardware	units	for
reading	the	instruction	from</p>
<p>Figure	
4.57	
PIPE	PC	selection	and	fetch	logic.
Within	the	one	cycle	time	limit,	the	processor	can	only	predict	the
address	of	the	next	instruction.
memory	and	for	extracting	the	different	instruction	fields	are	the	same	as
those	we	considered	for	SEQ	(see	the	fetch	stage	in	
Section	
4.3.4
).
The	PC	selection	logic	chooses	between	three	program	counter	sources.
As	a	mispredicted	branch	enters	the	memory	stage,	the	value	of	valP	for
this	instruction	(indicating	the	address	of	the	following	instruction)	is	read
from	pipeline	register	M	(signal	M_valA).	When	a	
instruction	enters
the	write-back	stage,	the	return	address	is	read	from	pipeline	register	W</p>
<p>(signal	W_valM).	All	other	cases	use	the	predicted	value	of	the	PC,
stored	in	pipeline	register	F	(signal	F_predPC):
The	PC	prediction	logic	chooses	valC	for	the	fetched	instruction	when	it
is	either	a	call	or	a	jump,	and	valP	otherwise:
The	logic	blocks	labeled	&quot;Instr	valid,&quot;	&quot;Need	regids,&quot;	and	&quot;Need	valC&quot;	are
the	same	as	for	SEQ,	with	appropriately	named	source	signals.
Unlike	in	SEQ,	we	must	split	the	computation	of	the	instruction	status	into
two	parts.	In	the	fetch	stage,	we	can	test	for	a	memory	error	due	to	an
out-of-range	instruction	address,	and	we	can	detect	an	illegal	instruction</p>
<p>or	a	halt	instruction.	Detecting	an	invalid	data	address	must	be	deferred
to	the	memory	stage.
Practice	Problem	
4.30	
(solution	page	
490
)
Write	HCL	code	for	the	signal	f_stat,	providing	the	provisional	status	for
the	fetched	instruction.
Decode	and	Write-Back	Stages
Figure	
4.58
gives	a	detailed	view	of	the	decode	and	write-back	logic
for	PIPE.	The	blocks	labeled	dstE,	dstM,	srcA,	and	srcB	are	very	similar
to	their	counterparts	in	the	implementation	of	SEQ.	Observe	that	the
register	IDs	supplied	to	the	write	ports	come	from	the	write-back	stage
(signals	W_dstE	and	W_dstM),	rather	than	from	the	decode	stage.	This	is
because	we	want	the	writes	to	occur	to	the	destination	registers	specified
by	the	instruction	in	the	write-back	stage.</p>
<p>Practice	Problem	
4.31	
(solution	page	
490
)
The	block	labeled	&quot;dstE&quot;	in	the	decode	stage	generates	the	register	ID
for	the	E	port	of	the	register	file,	based	on	fields	from	the	fetched
instruction	in	pipeline	register	D.	The	resulting	signal	is	named	d_dstE	in
the	HCL	description	of	PIPE.	Write	HCL	code	for	this	signal,	based	on	the
HCL	description	of	the	SEQ	signal	dstE.	(See	the	decode	stage	for	SEQ
in	
Section	
4.3.4
.)	Do	not	concern	yourself	with	the	logic	to	implement
conditional	moves	yet.
Most	of	the	complexity	of	this	stage	is	associated	with	the	forwarding
logic.	As	mentioned	earlier,	the	block	labeled	&quot;Sel+Fwd	A&quot;	serves	two
roles.	It	merges	the	valP	signal	into	the	valA	signal	for	later	stages	in
order	to	reduce	the	amount	of	state	in	the	pipeline	register.	It	also
implements	the	forwarding	logic	for	source	operand	valA.
The	merging	of	signals	valA	and	valP	exploits	the	fact	that	only	the	
and	jump	instructions	need	the	value	of	valP	in	later	stages,	and	these
instructions</p>
<p>Figure	
4.58	
PIPE	decode	and	write-back	stage	logic.
No	instruction	requires	both	valP	and	the	value	read	from	register	port	A,
and	so	these	two	can	be	merged	to	form	the	signal	valA	for	later	stages.
The	block	labeled	&quot;Sel+Fwd	A&quot;	performs	this	task	and	also	implements
the	forwarding	logic	for	source	operand	valA.	The	block	labeled	&quot;Fwd	B&quot;
implements	the	forwarding	logic	for	source	operand	valB.	The	register
write	locations	are	specified	by	the	dstE	and	dstM	signals	from	the	write-
back	stage	rather	than	from	the	decode	stage,	since	it	is	writing	the
results	of	the	instruction	currently	in	the	write-back	stage.</p>
<p>do	not	need	the	value	read	from	the	A	port	of	the	register	file.	This
selection	is	controlled	by	the	icode	signal	for	this	stage.	When	signal
D_icode	matches	the	instruction	code	for	either	
or	
,	this	block
should	select	D_valP	as	its	output.
As	mentioned	in	
Section	
4.5.5
,	there	are	five	different	forwarding
sources,	each	with	a	data	word	and	a	destination	register	ID:
Data	word
Register	ID
Source	description
e_valE
e_dstE
ALU	output
m_valM
M_dstM
Memory	output
M_valE
M_dstE
Pending	write	to	port	E	in	memory	stage
W_valM
W_dstM
Pending	write	to	port	M	in	write-back	stage
W_valE
W_dstE
Pending	write	to	port	E	in	write-back	stage
If	none	of	the	forwarding	conditions	hold,	the	block	should	select	d_rvalA,
the	value	read	from	register	port	A,	as	its	output.
Putting	all	of	this	together,	we	get	the	following	HCL	description	for	the
new	value	of	valA	for	pipeline	register	E:</p>
<p>The	priority	given	to	the	five	forwarding	sources	in	the	above	HCL	code	is
very	important.	This	priority	is	determined	in	the	HCL	code	by	the	order	in
which	the	five	destination	register	IDs	are	tested.	If	any	order	other	than
the	one	shown	were	chosen,	the	pipeline	would	behave	incorrectly	for
some	programs.	
Figure	
4.59	
shows	an	example	of	a	program	that
requires	a	correct	setting	of	priority	among	the	forwarding	sources	in	the
execute	and	memory	stages.	In	this	program,	the	first	two	instructions
write	to	register	
,	while	the	third	uses	this	register	as	its	source
operand.	When	the	
instruction	reaches	the	decode	stage	in	cycle
4,	the	forwarding	logic	must	choose	between	two	values	destined	for	its
source	register.	Which	one	should	it	choose?	To	set	the	priority,	we	must
consider	the	behavior	of	the	machine-language	program	when	it	is
executed	one	instruction	at	a	time.	The	first	
instruction	would	set
register	
to	10,	the	second	would	set	the	register	to	3,	and	then	the
instruction	would	read	3	from	
.	To	imitate	this	behavior,	our
pipelined	implementation	should	always	give	priority	to	the	forwarding
source	in	the	earliest	pipeline	stage,	since	it	holds	the	latest	instruction	in
the	program	sequence	setting	the	register.	Thus,	the	logic	in	the	HCL
code	above	first	tests	the	forwarding	source	in	the	execute	stage,	then
those	in	the	memory	stage,	and	finally	the	sources	in	the	write-back
stage.	The	forwarding	priority	between	the	two	sources	in	either	the</p>
<p>memory	or	the	write-back	stages	is	only	a	concern	for	the	instruction
popq	
,	since	only	this	instruction	can	attempt	two	simultaneous
writes	to	the	same	register.
Figure	
4.59	
Demonstration	of	forwarding	priority.
In	cycle	4,	values	for	
are	available	from	both	the	execute	and
memory	stages.	The	forwarding	logic	should	choose	the	one	in	the
execute	stage,	since	it	represents	the	most	recently	generated	value	for
this	register.
Practice	Problem	
4.32	
(solution	page	
490
)
Suppose	the	order	of	the	third	and	fourth	cases	(the	two	forwarding
sources	from	the	memory	stage)	in	the	HCL	code	for	d_valA	were
reversed.	Describe	the	resulting	behavior	of	the	
instruction	(line	5)
for	the	following	program:</p>
<p>Practice	Problem	
4.33	
(solution	page	
491
)
Suppose	the	order	of	the	fifth	and	sixth	cases	(the	two	forwarding
sources	from	the	write-back	stage)	in	the	HCL	code	for	d_valA	were
reversed.	Write	a	Y86-64	program	that	would	be	executed	incorrectly.
Describe	how	the	error	would	occur	and	its	effect	on	the	program
behavior.
Practice	Problem	
4.34	
(solution	page	
491
)
Write	HCL	code	for	the	signal	d_valB,	giving	the	value	for	source	operand
valB	supplied	to	pipeline	register	E.
One	small	part	of	the	write-back	stage	remains.	As	shown	in	
Figure
4.52
,	the	overall	processor	status	Stat	is	computed	by	a	block	based
on	the	status	value	in	pipeline	registerW.	Recall	from	
Section	
4.1.1
that	the	code	should	indicate	either	normal	operation	(
)	or	one	of	the
three	exception	conditions.	Since	pipeline	registerWholds	the	state	of	the
most	recently	completed	instruction,	it	is	natural	to	use	this	value	as	an
indication	of	the	overall	processor	status.	The	only	special	case	to
consider	is	when	there	is	a	bubble	in	the	write-back	stage.	This	is	part	of</p>
<p>normal	operation,	and	so	we	want	the	status	code	to	be	
for	this	case
as	well:
Execute	Stage
Figure	
4.60
shows	the	execute	stage	logic	for	PIPE.	The	hardware
units	and	the	logic	blocks	are	identical	to	those	in	SEQ,	with	an
appropriate	renaming	of	signals.	We	can	see	the	signals	e_valE	and
e_dstE	directed	toward	the	decode	stage	as	one	of	the	forwarding
sources.	One	difference	is	that	the	logic	labeled	&quot;Set	CC,&quot;	which
determineswhether	or	not	to	update	the	condition	codes,	has
signalsm_stat	and	W_stat	as	inputs.	These	signals	are	used	to	detect
cases	where	an	instruction</p>
<p>Figure	
4.60	
PIPE	execute	stage	logic.
This	part	of	the	design	is	very	similar	to	the	logic	in	the	SEQ
implementation.
Figure	
4.61	
PIPE	memory	stage	logic.
Many	of	the	signals	from	pipeline	registers	M	and	W	are	passed	down	to
earlier	stages	to	provide	write-back	results,	instruction	addresses,	and
forwarded	results.
causing	an	exception	is	passing	through	later	pipeline	stages,	and
therefore	any	updating	of	the	condition	codes	should	be	suppressed.	This
aspect	of	the	design	is	discussed	in	
Section	
4.5.8
.
Practice	Problem	
4.35	
(solution	page	
491
)
Our	second	case	in	the	HCL	code	for	d_valA	uses	signal	e_dstE	to	see
whether	to	select	the	ALU	output	e_valE	as	the	forwarding	source.
Suppose	instead	that	we	use	signal	E_dstE,	the	destination	register	ID	in</p>
<p>pipeline	register	E	for	this	selection.	Write	a	Y86-64	program	that	would
give	an	incorrect	result	with	this	modified	forwarding	logic.
Memory	Stage
Figure	
4.61
shows	the	memory	stage	logic	for	PIPE.	Comparing	this
to	the	memory	stage	for	SEQ	(
Figure	
4.30
),	we	see	that,	as	noted
before,	the	block	labeled	&quot;Mem.	data&quot;	in	SEQ	is	not	present	in	PIPE.	This
block	served	to	select	between	data	sources	valP	(for	
instructions)
and	valA,	but	this	selection	is	now	performed	by	the	block	labeled
&quot;Sel+Fwd	A&quot;	in	the	decode	stage.	Most	other	blocks	in	this	stage	are
identical	to	their	counterparts	in	SEQ,	with	an	appropriate	renaming	of
the	signals.	In	this	figure,	you	can	also	see	that	many	of	the	values	in
pipeline	registers	and	M	and	W	are	supplied	to	other	parts	of	the	circuit
as	part	of	the	forwarding	and	pipeline	control	logic.
Practice	Problem	
4.36	
(solution	page	
492
)
In	this	stage,	we	can	complete	the	computation	of	the	status	code	Stat	by
detecting	the	case	of	an	invalid	address	for	the	data	memory.	Write	HCL
code	for	the	signal	m_stat.
4.5.8	
Pipeline	Control	Logic
We	are	now	ready	to	complete	our	design	for	PIPE	by	creating	the
pipeline	control	logic.	This	logic	must	handle	the	following	four	control</p>
<p>cases	for	which	other	mechanisms,	such	as	data	forwarding	and	branch
prediction,	do	not	suffice:
Load/use	hazards.	
The	pipeline	must	stall	for	one	cycle	between	an
instruction	that	reads	a	value	from	memory	and	an	instruction	that
uses	this	value.
Processing	ret.	
The	pipeline	must	stall	until	the	
instruction
reaches	the	write-back	stage.
Mispredicted	branches.	
By	the	time	the	branch	logic	detects	that	a
jump	should	not	have	been	taken,	several	instructions	at	the	branch
target	will	have	started	down	the	pipeline.	These	instructions	must	be
canceled,	and	fetching	should	begin	at	the	instruction	following	the
jump	instruction.
Exceptions.	
When	an	instruction	causes	an	exception,	we	want	to
disable	the	updating	of	the	programmer-visible	state	by	later
instructions	and	halt	execution	once	the	excepting	instruction	reaches
the	write-back	stage.
We	will	go	through	the	desired	actions	for	each	of	these	cases	and	then
develop	control	logic	to	handle	all	of	them.
Desired	Handling	of	Special	Control	Cases
For	a	load/use	hazard,	we	have	described	the	desired	pipeline	operation
in	
Section	
4.5.5
,	as	illustrated	by	the	example	of	
Figure	
4.54
.	Only
the	
and	popq	instructions	read	data	from	memory.	When	(1)	either
of	these	is	in	the	execute	stage	and	(2)	an	instruction	requiring	the
destination	register	is	in	the	decode	stage,	we	want	to	hold	back	the</p>
<p>second	instruction	in	the	decode	stage	and	inject	a	bubble	into	the
execute	stage	on	the	next	cycle.	After	this,	the	forwarding	logic	will
resolve	the	data	hazard.	The	pipeline	can	hold	back	an	instruction	in	the
decode	stage	by	keeping	pipeline	register	D	in	a	fixed	state.	In	doing	so,
it	should	also	keep	pipeline	register	F	in	a	fixed	state,	so	that	the	next
instruction	will	be	fetched	a	second	time.	In	summary,	implementing	this
pipeline	flow	requires	detecting	the	hazard	condition,	keeping	pipeline
registers	F	and	D	fixed,	and	injecting	a	bubble	into	the	execute	stage.
For	the	processing	of	a	
instruction,	we	have	described	the	desired
pipeline	operation	in	
Section	
4.5.5
.	The	pipeline	should	stall	for	three
cycles	until	the	return	address	is	read	as	the	
instruction	passes
through	the	memory	stage.
This	was	illustrated	by	a	simplified	pipeline	diagram	in	
Figure	
4.55
for
processing	the	following	program:</p>
<p>Figure	
4.62
provides	a	detailed	view	of	the	processing	of	the	
instruction	for	the	example	program.	The	key	observation	here	is	that
there	is	no	way	to	inject	a	bubble	into	the	fetch	stage	of	our	pipeline.	On
every	cycle,	the	fetch	stage	reads	
some
instruction	from	the	instruction
memory.	Looking	at	the	HCL	code	for	implementing	the	PC	prediction
logic	in	
Section	
4.5.7
,	we	can	see	that	for	the	
instruction,	the	new
value	of	the	PC	is	predicted	to	be	valP,	the	address	of	the	following
instruction.	In	our	example	program,	this	would	be	
,	the	address	of
the	
instruction	following	the	ret.	This	prediction	is	not	correct	for
this	example,	nor	would	it	be	for	most	cases,	but	we	are	not	attempting	to
predict	return	addresses	correctly	in	our	design.	For	three	clock	cycles,
the	fetch	stage	stalls,	causing	the	
instruction	to	be	fetched	but
then	replaced	by	a	bubble	in	the	decode	stage.	This	process	is	illustrated
in	
Figure	
4.62
by	the	three	fetches,	with	an	arrow	leading	down	to	the
bubbles	passing	through	the	remaining	pipeline	stages.	Finally,	the
instruction	is	fetched	on	cycle	7.	Comparing	
Figure	
4.62
with
Figure	
4.62	
Detailed	processing	of	the	
instruction.
The	fetch	stage	repeatedly	fetches	the	
instruction	following	the
instruction,	but	then	the	pipeline	control	logic	injects	a	bubble	into	the</p>
<p>decode	stage	rather	than	allowing	the	
instruction	to	proceed.	The
resulting	behavior	is	equivalent	to	that	shown	in	
Figure	
4.55
.
Figure	
4.55
,	we	see	that	our	implementation	achieves	the	desired
effect,	but	with	a	slightly	peculiar	fetching	of	an	incorrect	instruction	for
three	consecutive	cycles.
When	a	mispredicted	branch	occurs,	we	have	described	the	desired
pipeline	operation	in	
Section	
4.5.5
and	illustrated	it	in	
Figure	
4.56
.
The	misprediction	will	be	detected	as	the	jump	instruction	reaches	the
execute	stage.	The	control	logic	then	injects	bubbles	into	the	decode	and
execute	stages	on	the	next	cycle,	causing	the	two	incorrectly	fetched
instructions	to	be	canceled.	On	the	same	cycle,	the	pipeline	reads	the
correct	instruction	into	the	fetch	stage.
For	an	instruction	that	causes	an	exception,	we	must	make	the	pipelined
implementation	match	the	desired	ISA	behavior,	with	all	prior	instructions
completing	and	with	none	of	the	following	instructions	having	any	effect
on	the	program	state.	Achieving	these	effects	is	complicated	by	the	facts
that	(1)	exceptions	are	detected	during	two	different	stages	(fetch	and
memory)	of	program	execution,	and	(2)	the	program	state	is	updated	in
three	different	stages	(execute,	memory,	and	write-back).
Our	stage	designs	include	a	status	code	stat	in	each	pipeline	register	to
track	the	status	of	each	instruction	as	it	passes	through	the	pipeline
stages.	When	an	exception	occurs,	we	record	that	information	as	part	of
the	instruction's	status	and	continue	fetching,	decoding,	and	executing
instructions	as	if	nothing	were	amiss.	As	the	excepting	instruction</p>
<p>reaches	the	memory	stage,	we	take	steps	to	prevent	later	instructions
from	modifying	the	programmer-visible	state	by	(1)	disabling	the	setting
of	condition	codes	by	instructions	in	the	execute	stage,	(2)	injecting
bubbles	into	the	memory	stage	to	disable	any	writing	to	the	data	memory,
and	(3)	stalling	the	write-back	stage	when	it	has	an	excepting	instruction,
thus	bringing	the	pipeline	to	a	halt.
The	pipeline	diagram	in	
Figure	
4.63
illustrates	how	our	pipeline	control
handles	the	situation	where	an	instruction	causing	an	exception	is
followed	by	one	that	would	change	the	condition	codes.	On	cycle	6,	the
instruction	reaches	the	memory	stage	and	generates	a	memory
error.	On	the	same	cycle,	the	
instruction	in	the	execute	stage
generates	new	values	for	the	condition	codes.	We	disable	the	setting	of
condition	codes	when	an	excepting	instruction	is	in	the	memory	or	write-
back	stage	(by	examining	the	signals	m_stat	and	W_stat	and	then	setting
the	signal	set_cc	to	zero).	We	can	also	see	the	combination	of	inj	ecting
bubbles	into	the	memory	stage	and	stalling	the	excepting	instruction	in
the	write-back	stage	in	the	example	of	
Figure	
4.63
—the	
instruction	remains	stalled	in	the	write-back	stage,	and	none	of	the
subsequent	instructions	get	past	the	execute	stage.
By	this	combination	of	pipelining	the	status	signals,	controlling	the	setting
of	condition	codes,	and	controlling	the	pipeline	stages,	we	achieve	the
desired	behavior	for	exceptions:	all	instructions	prior	to	the	excepting
instruction	are	completed,	while	none	of	the	following	instructions	has
any	effect	on	the	programmer-visible	state.
Detecting	Special	Control	Conditions</p>
<p>Figure	
4.64
summarizes	the	conditions	requiring	special	pipeline
control.	It	gives	expressions	describing	the	conditions	under	which	the
three	special	cases	arise.
Figure	
4.63	
Processing	invalid	memory	reference	exception.
On	cycle	6,	the	invalid	memory	reference	by	the	
instruction	causes
the	updating	of	the	condition	codes	to	be	disabled.	The	pipeline	starts
injecting	bubbles	into	the	memory	stage	and	stalling	the	excepting
instruction	in	the	write-back	stage.
Condition
Trigger
Processing	ret
IRET	
∊
{D_icode,	E_icode,	M_icode}
Load/use	hazard
E_icode	
∊
{IMRMOVQ,	IPOPQ}	&amp;&amp;	E_dstM	
∊
{d_srcA,	d_srcB}
Mispredicted	branch
E_icode	=	IJXX&amp;&amp;	!e_Cnd
Exception
m_stat	
∊
{SADR,	SINS,	SHLT}	||	W_stat	
∊
{SADR,	SINS,	SHLT}
Figure	
4.64	
Detection	conditions	for	pipeline	control	logic.</p>
<p>Four	different	conditions	require	altering	the	pipeline	flow	by	either
stalling	the	pipeline	or	canceling	partially	executed	instructions.
These	expressions	are	implemented	by	simple	blocks	of	combinational
logic	that	must	generate	their	results	before	the	end	of	the	clock	cycle	in
order	to	control	the	action	of	the	pipeline	registers	as	the	clock	rises	to
start	the	next	cycle.	During	a	clock	cycle,	pipeline	registers	D,	E,	and	M
hold	the	states	of	the	instructions	that	are	in	the	decode,	execute,	and
memory	pipeline	stages,	respectively.	As	we	approach	the	end	of	the
clock	cycle,	signals	d_srcA	and	d_srcB	will	be	set	to	the	register	IDs	of
the	source	operands	for	the	instruction	in	the	decode	stage.	Detecting	a
instruction	as	it	passes	through	the	pipeline	simply	involves	checking
the	instruction	codes	of	the	instructions	in	the	decode,	execute,	and
memory	stages.	Detecting	a	load/use	hazard	involves	checking	the
instruction	type	(
or	
)	of	the	instruction	in	the	execute	stage
and	comparing	its	destination	register	with	the	source	registers	of	the
instruction	in	the	decode	stage.	The	pipeline	control	logic	should	detect	a
mispredicted	branch	while	the	jump	
instruction	is	in	the	execute	stage,	so
that	it	can	set	up	the	conditions	required	to	recover	from	the
misprediction	as	the	instruction	enters	the	memory	stage.	When	a	jump
instruction	is	in	the	execute	stage,	the	signal	e_Cnd	indicates	whether	or
not	the	jump	should	be	taken.	We	detect	an	excepting	instruction	by
examining	the	instruction	status	values	in	the	memory	and	write-back
stages.	For	the	memory	stage,	we	use	the	signal	m_stat,	computed
within	the	stage,	rather	than	M_stat	from	the	pipeline	register.	This
internal	signal	incorporates	the	possibility	of	a	data	memory	address
error.</p>
<p>Pipeline	Control	Mechanisms
Figure	
4.65
shows	low-level	mechanisms	that	allow	the	pipeline
control	logic	to	hold	back	an	instruction	in	a	pipeline	register	or	to	inject	a
bubble	into	the	pipeline.	These	mechanisms	involve	small	extensions	to
the	basic	clocked	register	described
Figure	
4.65	
Additional	pipeline	register	operations,
(a)	Under	normal	conditions,	the	state	and	output	of	the	register	are	set
to	the	value	at	the	input	when	the	clock	rises,	(b)	When	operated	in	
stall
mode,	the	state	is	held	fixed	at	its	previous	value,	(c)	When	operated	in
bubble
mode,	the	state	is	overwritten	with	that	of	a	
operation.</p>
<p>Pipeline	resister
Condition
F
D
E
M
W
Processing	ret
stall
bubble
normal
normal
normal
Load/use	hazard
stall
stall
bubble
normal
normal
Mispredicted	branch
normal
bubble
bubble
normal
normal
Figure	
4.66	
Actions	for	pipeline	control	logic.
The	different	conditions	require	altering	the	pipeline	flow	by	either	stalling
the	pipeline	or	canceling	partially	executed	instructions.
in	
Section	
4.2.5
.	Suppose	that	each	pipeline	register	has	two	control
inputs	stall	and	bubble.	The	settings	of	these	signals	determine	how	the
pipeline	register	is	updated	as	the	clock	rises.	Under	normal	operation
(
Figure	
4.65
(a)),	both	of	these	inputs	are	set	to	0,	causing	the	register
to	load	its	input	as	its	new	state.	When	the	stall	signal	is	set	to	1	(
Figure
4.65
(b)),	the	updating	of	the	state	is	disabled.	Instead,	the	register	will
remain	in	its	previous	state.	This	makes	it	possible	to	hold	back	an
instruction	in	some	pipeline	stage.	When	the	bubble	signal	is	set	to	1
(
Figure	
4.65
(c)),	the	state	of	the	register	will	be	set	to	some	fixed	
reset
configuration
,	giving	a	state	equivalent	to	that	of	a	
instruction.	The
particular	pattern	of	ones	and	zeros	for	a	pipeline	register's	reset
configuration	depends	on	the	set	of	fields	in	the	pipeline	register.	For
example,	to	inject	a	bubble	into	pipeline	register	D,	we	want	the	icode
field	to	be	set	to	the	constant	value	
(
Figure	
4.26
).	To	inject	a
bubble	into	pipeline	register	E,	we	want	the	icode	field	to	be	set	to	</p>
<p>and	the	dstE,	dstM,	srcA,	and	srcB	fields	to	be	set	to	the	constant	
.
Determining	the	reset	configuration	is	one	of	the	tasks	for	the	hardware
designer	in	designing	a	pipeline	register.	We	will	not	concern	ourselves
with	the	details	here.	We	will	consider	it	an	error	to	set	both	the	bubble
and	the	stall	signals	to	1.
The	table	in	
Figure	
4.66
shows	the	actions	the	different	pipeline
stages	should	take	for	each	of	the	three	special	conditions.	Each	involves
some	combination	of	normal,	stall,	and	bubble	operations	for	the	pipeline
registers.	In	terms	of	timing,	the	stall	and	bubble	control	signals	for	the
pipeline	registers	are	generated	by	blocks	of	combinational	logic.	These
values	must	be	valid	as	the	clock	rises,	causing	each	of	the	pipeline
registers	to	either	load,	stall,	or	bubble	as	the	next	clock	cycle	begins.
With	this	small	extension	to	the	pipeline	register	designs,	we	can
implement	a	complete	pipeline,	including	all	of	its	control,	using	the	basic
building	blocks	of	combinational	logic,	clocked	registers,	and	random
access	memories.
Combinations	of	Control	Conditions
In	our	discussion	of	the	special	pipeline	control	conditions	so	far,	we
assumed	that	at	most	one	special	case	could	arise	during	any	single
clock	cycle.	A	common	bug	in	designing	a	system	is	to	fail	to	handle
instances	where	multiple	special	conditions	arise	simultaneously.	Let	us
analyze	such	possibilities.	We	need	not	worry	about	combinations
involving	program	exceptions,	since	we	have	carefully	designed	our
exception-handling	mechanism	to	consider	other	instructions	in	the
pipeline.	
Figure	
4.67
diagrams	the	pipeline	states	that	cause	the	other
three	special	control</p>
<p>Figure	
4.67	
Pipeline	states	for	special	control	conditions.
The	two	pairs	indicated	can	arise	simultaneously.
conditions.	These	diagrams	show	blocks	for	the	decode,	execute,	and
memory	stages.	The	shaded	boxes	represent	particular	constraints	that
must	be	satisfied	for	the	condition	to	arise.	A	load/use	hazard	requires
that	the	instruction	in	the	execute	stage	reads	a	value	from	memory	into
a	register,	and	that	the	instruction	in	the	decode	stage	has	this	register	as
a	source	operand.	A	mispredicted	branch	requires	the	instruction	in	the
execute	stage	to	have	a	jump	instruction.	There	are	three	possible	cases
for	
—the	instruction	can	be	in	either	the	decode,	execute,	or	memory
stage.	As	the	
instruction	moves	through	the	pipeline,	the	earlier
pipeline	stages	will	have	bubbles.
We	can	see	by	these	diagrams	that	most	of	the	control	conditions	are
mutually	exclusive.	For	example,	it	is	not	possible	to	have	a	load/use
hazard	and	a	mispredicted	branch	simultaneously,	since	one	requires	a
load	instruction	(
or	
)	in	the	execute	stage,	while	the	other</p>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->
                            <a rel="prev" href="../csapp/part3.html" class="mobile-nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                                <i class="fa fa-angle-left"></i>
                            </a>

                            <a rel="next" href="../csapp/part5.html" class="mobile-nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                                <i class="fa fa-angle-right"></i>
                            </a>

                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">
                    <a rel="prev" href="../csapp/part3.html" class="nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                        <i class="fa fa-angle-left"></i>
                    </a>

                    <a rel="next" href="../csapp/part5.html" class="nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                        <i class="fa fa-angle-right"></i>
                    </a>
            </nav>

        </div>




        <script>
            window.playground_copyable = true;
        </script>



        <script src="../clipboard.min.js"></script>
        <script src="../highlight.js"></script>
        <script src="../book.js"></script>

        <!-- Custom JS scripts -->


    </div>
    </body>
</html>
