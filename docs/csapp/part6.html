<!DOCTYPE HTML>
<html lang="en" class="sidebar-visible no-js light">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>Part6 - KnowledgeBase</title>


        <!-- Custom HTML head -->
        
        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff" />

        <link rel="icon" href="../favicon.svg">
        <link rel="shortcut icon" href="../favicon.png">
        <link rel="stylesheet" href="../css/variables.css">
        <link rel="stylesheet" href="../css/general.css">
        <link rel="stylesheet" href="../css/chrome.css">
        <link rel="stylesheet" href="../css/print.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="../FontAwesome/css/font-awesome.css">
        <link rel="stylesheet" href="../fonts/fonts.css">

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" href="../highlight.css">
        <link rel="stylesheet" href="../tomorrow-night.css">
        <link rel="stylesheet" href="../ayu-highlight.css">

        <!-- Custom theme stylesheets -->

    </head>
    <body>
    <div id="body-container">
        <!-- Provide site root to javascript -->
        <script>
            var path_to_root = "../";
            var default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? "navy" : "light";
        </script>

        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script>
            try {
                var theme = localStorage.getItem('mdbook-theme');
                var sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script>
            var theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            var html = document.querySelector('html');
            html.classList.remove('no-js')
            html.classList.remove('light')
            html.classList.add(theme);
            html.classList.add('js');
        </script>

        <!-- Hide / unhide sidebar before it is displayed -->
        <script>
            var html = document.querySelector('html');
            var sidebar = null;
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            } else {
                sidebar = 'hidden';
            }
            html.classList.remove('sidebar-visible');
            html.classList.add("sidebar-" + sidebar);
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <div class="sidebar-scrollbox">
                <ol class="chapter"><li class="chapter-item expanded "><a href="../HaskellProgramming/HaskellProgramming.html"><strong aria-hidden="true">1.</strong> Haskell Programming</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="../HaskellProgramming/part1.html"><strong aria-hidden="true">1.1.</strong> Part1</a></li><li class="chapter-item expanded "><a href="../HaskellProgramming/part2.html"><strong aria-hidden="true">1.2.</strong> Part2</a></li><li class="chapter-item expanded "><a href="../HaskellProgramming/part3.html"><strong aria-hidden="true">1.3.</strong> Part3</a></li><li class="chapter-item expanded "><a href="../HaskellProgramming/part4.html"><strong aria-hidden="true">1.4.</strong> Part4</a></li><li class="chapter-item expanded "><a href="../HaskellProgramming/part5.html"><strong aria-hidden="true">1.5.</strong> Part5</a></li></ol></li><li class="chapter-item expanded "><a href="../csapp/csapp.html"><strong aria-hidden="true">2.</strong> csapp</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="../csapp/part1.html"><strong aria-hidden="true">2.1.</strong> Part1</a></li><li class="chapter-item expanded "><a href="../csapp/part2.html"><strong aria-hidden="true">2.2.</strong> Part2</a></li><li class="chapter-item expanded "><a href="../csapp/part3.html"><strong aria-hidden="true">2.3.</strong> Part3</a></li><li class="chapter-item expanded "><a href="../csapp/part4.html"><strong aria-hidden="true">2.4.</strong> Part4</a></li><li class="chapter-item expanded "><a href="../csapp/part5.html"><strong aria-hidden="true">2.5.</strong> Part5</a></li><li class="chapter-item expanded "><a href="../csapp/part6.html" class="active"><strong aria-hidden="true">2.6.</strong> Part6</a></li><li class="chapter-item expanded "><a href="../csapp/part7.html"><strong aria-hidden="true">2.7.</strong> Part7</a></li><li class="chapter-item expanded "><a href="../csapp/part8.html"><strong aria-hidden="true">2.8.</strong> Part8</a></li><li class="chapter-item expanded "><a href="../csapp/part9.html"><strong aria-hidden="true">2.9.</strong> Part9</a></li><li class="chapter-item expanded "><a href="../csapp/part10.html"><strong aria-hidden="true">2.10.</strong> Part10</a></li></ol></li><li class="chapter-item expanded "><a href="../midjourney/combined_html_page.html"><strong aria-hidden="true">3.</strong> midjourney</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="../midjourney/mjprompt.html"><strong aria-hidden="true">3.1.</strong> MjPrompt</a></li></ol></li></ol>
            </div>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle"></div>
        </nav>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky bordered">
                    <div class="left-buttons">
                        <button id="sidebar-toggle" class="icon-button" type="button" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </button>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="light">Light</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                    </div>

                    <h1 class="menu-title">KnowledgeBase</h1>

                    <div class="right-buttons">
                        <a href="../print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>

                    </div>
                </div>


                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script>
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <p>6.3.1	
Caching	in	the	Memory
Hierarchy
In	general,	a	
cache
(pronounced	&quot;cash&quot;)	is	a	small,	fast	storage	device
that	acts	as	a	staging	area	for	the	data	objects	stored	in	a	larger,	slower
device.	The	process	of	using	a	cache	is	known	as	
caching
(pronounced
&quot;cashing&quot;).</p>
<p>The	central	idea	of	a	memory	hierarchy	is	that	for	each	
k
,	the	faster	and
smaller	storage	device	at	level	
k
serves	as	a	cache	for	the	larger	and
slower	storage	device
Aside	
Other	memory	hierarchies
We	have	shown	you	one	example	of	a	memory	hierarchy,	but
other	combinations	are	possible,	and	indeed	common.	For
example,	many	sites,	including	Google	datacenters,	back	up	local
disks	onto	archival	magnetic	tapes.	At	some	of	these	sites,	human
operators	manually	mount	the	tapes	onto	tape	drives	as	needed.
At	other	sites,	tape	robots	handle	this	task	automatically.	In	either
case,	the	collection	of	tapes	represents	a	level	in	the	memory
hierarchy,	below	the	local	disk	level,	and	the	same	general
principles	apply.	Tapes	are	cheaper	per	byte	than	disks,	which
allows	sites	to	archive	multiple	snapshots	of	their	local	disks.	The
trade-off	is	that	tapes	take	longer	to	access	than	disks.	As	another
example,	solid	state	disks	are	playing	an	increasingly	important
role	in	the	memory	hierarchy,	bridging	the	gulf	between	DRAM
and	rotating	disk.
Figure	
6.22	
The	basic	principle	of	caching	in	a	memory	hierarchy.</p>
<p>at	level	
k
+	1.	In	other	words,	each	level	in	the	hierarchy	caches	data
objects	from	the	next	lower	level.	For	example,	the	local	disk	serves	as	a
cache	for	files	(such	as	Web	pages)	retrieved	from	remote	disks	over	the
network,	the	main	memory	serves	as	a	cache	for	data	on	the	local	disks,
and	so	on,	until	we	get	to	the	smallest	cache	of	all,	the	set	of	CPU
registers.
Figure	
6.22
shows	the	general	concept	of	caching	in	a	memory
hierarchy.	The	storage	at	level	
k
+	1	is	partitioned	into	contiguous	chunks
of	data	objects	called	
blocks.
Each	block	has	a	unique	address	or	name
that	distinguishes	it	from	other	blocks.	Blocks	can	be	either	fixed	size	(the
usual	case)	or	variable	size	(e.g.,	the	remote	HTML	files	stored	on	Web
servers).	For	example,	the	level	
k
+	1	storage	in	
Figure	
6.22
is
partitioned	into	16	fixed-size	blocks,	numbered	0	to	15.
Similarly,	the	storage	at	level	
k
is	partitioned	into	a	smaller	set	of	blocks
that	are	the	same	size	as	the	blocks	at	level	
k
+	1.	At	any	point	in	time,
the	cache	at	level	
k
contains	copies	of	a	subset	of	the	blocks	from	level	
k</p>
<ul>
<li>
<ol>
<li>For	example,	in	
Figure	
6.22
,	the	cache	at	level	
k
has	room	for
four	blocks	and	currently	contains	copies	of	blocks	4,	9,14,	and	3.
Data	are	always	copied	back	and	forth	between	level	
k
and	level	
k</li>
</ol>
<ul>
<li>1	in
block-size	
transfer	units.
It	is	important	to	realize	that	while	the	block	size
is	fixed	between	any	particular	pair	of	adjacent	levels	in	the	hierarchy,
other	pairs	of	levels	can	have	different	block	sizes.	For	example,	in
Figure	
6.21
,	transfers	between	L1	and	L0	typically	use	word-size
blocks.	Transfers	between	L2	and	L1	(and	L3	and	L2,	and	L4	and	L3)
typically	use	blocks	of	tens	of	bytes.	And	transfers	between	L5	and	L4
use	blocks	with	hundreds	or	thousands	of	bytes.	In	general,	devices
lower	in	the	hierarchy	(further	from	the	CPU)	have	longer	access	times,</li>
</ul>
</li>
</ul>
<p>and	thus	tend	to	use	larger	block	sizes	in	order	to	amortize	these	longer
access	times.
Cache	Hits
When	a	program	needs	a	particular	data	object	
d
from	level	
k
+	1,	it	first
looks	for	
d
in	one	of	the	blocks	currently	stored	at	level	
k.
If	
d
happens	to
be	cached	at	level	
k
,	then	we	have	what	is	called	a	
cache	hit.
The
program	reads	
d
directly	from	level	
k
,	which	by	the	nature	of	the	memory
hierarchy	is	faster	than	reading	
d
from	level	
k	+	1.
For	example,	a
program	with	good	temporal	locality	might	read	a	data	object	from	block
14,	resulting	in	a	cache	hit	from	level	
k.
Cache	Misses
If,	on	the	other	hand,	the	data	object	
d
is	not	cached	at	level	
k
,	then	we
have	what	is	called	a	
cache	miss.
When	there	is	a	miss,	the	cache	at
level	
k
fetches	the	block	containing	
d
from	the	cache	at	level	
k
+	1,
possibly	overwriting	an	existing	block	if	the	level	
k
cache	is	already	full.
This	process	of	overwriting	an	existing	block	is	known	as	
replacing
or
evicting
the	block.	The	block	that	is	evicted	is	sometimes	referred	to	as	a
victim	block.
The	decision	about	which	block	to	replace	is	governed	by
the	cache's	
replacement	policy.
For	example,	a	cache	with	a	
random
replacement	policy
would	choose	a	random	victim	block.	A	cache	with	a
least	recently	used	(LRU)
replacement	policy	would	choose	the	block	that
was	last	accessed	the	furthest	in	the	past.</p>
<p>After	the	cache	at	level	
k
has	fetched	the	block	from	level	
k
+	1,	the
program	can	read	
d
from	level	
k
as	before.	For	example,	in	
Figure
6.22
,	reading	a	data	object	from	block	12	in	the	level	
k
cache	would
result	in	a	cache	miss	because	block	12	is	not	currently	stored	in	the
level	
k
cache.	Once	it	has	been	copied	from	level	
k
+	1	to	level	
k
,	block
12	will	remain	there	in	expectation	of	later	accesses.
Kinds	of	Cache	Misses
It	is	sometimes	helpful	to	distinguish	between	different	kinds	of	cache
misses.	If	the	cache	at	level	
k
is	empty,	then	any	access	of	any	data
object	will	miss.	An	empty	cache	is	sometimes	referred	to	as	a	
cold
cache
,	and	misses	of	this	kind	are	called	
compulsory	misses
or	
cold
misses.
Cold	misses	are	important	because	they	are	often	transient
events	that	might	not	occur	in	steady	state,	after	the	cache	has	been
warmed	up
by	repeated	memory	accesses.
Whenever	there	is	a	miss,	the	cache	at	level	
k
must	implement	some
placement	policy
that	determines	where	to	place	the	block	it	has	retrieved
from	level	
k
+	1.	The	most	flexible	placement	policy	is	to	allow	any	block
from	level	
k
+	1	to	be	stored	in	any	block	at	level	
k.
For	caches	high	in
the	memory	hierarchy	(close	to	the	CPU)	that	are	implemented	in
hardware	and	where	speed	is	at	a	premium,	this	policy	is	usually	too
expensive	to	implement	because	randomly	placed	blocks	are	expensive
to	locate.
Thus,	hardware	caches	typically	implement	a	simpler	placement	policy
that	restricts	a	particular	block	at	level	
k
+	1	to	a	small	subset	(sometimes
a	singleton)	of	the	blocks	at	level	
k.
For	example,	in	
Figure	
6.22
,	we</p>
<p>might	decide	that	a	block	
i
at	level	
k
+	1	must	be	placed	in	block	(
i
mod	4)
at	level	
k.
For	example,	blocks	0,	4,	8,	and	12	at	level	
k
+	1	would	map	to
block	0	at	level	
k;
blocks	1,	5,	9,	and	13	would	map	to	block	1;	and	so	on.
Notice	that	our	example	cache	in	
Figure	
6.22
uses	this	policy.
Restrictive	placement	policies	of	this	kind	lead	to	a	type	of	miss	known	as
a	
conflict	miss
,	in	which	the	cache	is	large	enough	to	hold	the	referenced
data	objects,	but	because	they	map	to	the	same	cache	block,	the	cache
keeps	missing.	For	example,	in	
Figure	
6.22
,	if	the	program	requests
block	0,	then	block	8,	then	block	0,	then	block	8,	and	so	on,	each	of	the
references	to	these	two	blocks	would	miss	in	the	cache	at	level	
k
,	even
though	this	cache	can	hold	a	total	of	four	blocks.
Programs	often	run	as	a	sequence	of	phases	(e.g.,	loops)	where	each
phase	accesses	some	reasonably	constant	set	of	cache	blocks.	For
example,	a	nested	loop	might	access	the	elements	of	the	same	array
over	and	over	again.	This	set	of	blocks	is	called	the	
working	set
of	the
phase.	When	the	size	of	the	working	set	exceeds	the	size	of	the	cache,
the	cache	will	experience	what	are	known	as	
capacity	misses.
In	other
words,	the	cache	is	just	too	small	to	handle	this	particular	working	set.
Cache	Management
As	we	have	noted,	the	essence	of	the	memory	hierarchy	is	that	the
storage	device	at	each	level	is	a	cache	for	the	next	lower	level.	At	each
level,	some	form	of	logic	must	
manage
the	cache.	By	this	we	mean	that
something	has	to	partition	the	cache	storage	into	blocks,	transfer	blocks
between	different	levels,	decide	when	there	are	hits	and	misses,	and</p>
<p>then	deal	with	them.	The	logic	that	manages	the	cache	can	be	hardware,
software,	or	a	combination	of	the	two.
For	example,	the	compiler	manages	the	register	file,	the	highest	level	of
the	cache	hierarchy.	It	decides	when	to	issue	loads	when	there	are
misses,	and	determines	which	register	to	store	the	data	in.	The	caches	at
levels	L1,	L2,	and	L3	are	managed	entirely	by	hardware	logic	built	into
the	caches.	In	a	system	with	virtual	memory,	the	DRAM	main	memory
serves	as	a	cache	for	data	blocks	stored	on	disk,	and	is	managed	by	a
combination	of	operating	system	software	and	address	translation
hardware	on	the	CPU.	For	a	machine	with	a	distributed	file	system	such
as	AFS,	the	local	disk	serves	as	a	cache	that	is	managed	by	the	AFS
client	process	running	on	the	local	machine.	In	most	cases,	caches
operate	automatically	and	do	not	require	any	specific	or	explicit	actions
from	the	program.
Type
What	cached
Where	cached
Latency
(cycles)
Managed	by
CPU
registers
4-byte	or	8-byte
words
On-chip	CPU
registers
0
Compiler
TLB
Address
translations
On-chip	TLB
0
Hardware	MMU
L1	cache
64-byte	blocks
On-chip	L1	cache
4
Hardware
L2	cache
64-byte	blocks
On-chip	L2	cache
10
Hardware
L3	cache
64-byte	blocks
On-chip	L3	cache
50
Hardware
Virtual
4-KB	pages
Main	memory
200
Hardware	+	OS</p>
<p>memory
Buffer
cache
Parts	of	files
Main	memory
200
OS
Disk	cache
Disk	sectors
Disk	controller
100,000
Controller
firmware
Network
cache
Parts	of	files
Local	disk
10,000,000
NFS	client
Browser
cache
Web	pages
Local	disk
10,000,000
Web	browser
Web	cache
Web	pages
Remote	server
disks
1,000,000,000
Web	proxy
server
Figure	
6.23	
The	ubiquity	of	caching	in	modern	computer	systems.
Acronyms:	TLB:	translation	lookaside	buffer;	MMU:	memory
management	unit;	OS:	operating	system;	NFS:	network	file	system.
6.3.2	
Summary	of	Memory
Hierarchy	Concepts
To	summarize,	memory	hierarchies	based	on	caching	work	because
slower	storage	is	cheaper	than	faster	storage	and	because	programs
tend	to	exhibit	locality:
Exploiting	temporal	locality.	
Because	of	temporal	locality,	the	same
data	objects	are	likely	to	be	reused	multiple	times.	Once	a	data	object</p>
<p>has	been	copied	into	the	cache	on	the	first	miss,	we	can	expect	a
number	of	subsequent	hits	on	that	object.	Since	the	cache	is	faster
than	the	storage	at	the	next	lower	level,	these	subsequent	hits	can	be
served	much	faster	than	the	original	miss.
Exploiting	spatial	locality.	
Blocks	usually	contain	multiple	data
objects.	Because	of	spatial	locality,	we	can	expect	that	the	cost	of
copying	a	block	after	a	miss	will	be	amortized	by	subsequent
references	to	other	objects	within	that	block.
Caches	are	used	everywhere	in	modern	systems.	As	you	can	see	from
Figure	
6.23
,	caches	are	used	in	CPU	chips,	operating	systems,
distributed	file	systems,	and	on	the	World	Wide	Web.	They	are	built	from
and	managed	by	various	combinations	of	hardware	and	software.	Note
that	there	are	a	number	of	terms	and	acronyms	in	
Figure	
6.23
that	we
haven't	covered	yet.	We	include	them	here	to	demonstrate	how	common
caches	are.</p>
<p>6.4	
Cache	Memories
The	memory	hierarchies	of	early	computer	systems	consisted	of	only
three	levels:	CPU	registers,	main	memory,	and	disk	storage.	However,
because	of	the	increasing	gap	between	CPU	and	main	memory,	system
designers	were	compelled	to	insert
Figure	
6.24	
Typical	bus	structure	for	cache	memories.
a	small	SRAM	
cache	memory
,	called	an	
L1	cache
(level	1	cache)
between	the	CPU	register	file	and	main	memory,	as	shown	in	
Figure
6.24
.	The	L1	cache	can	be	accessed	nearly	as	fast	as	the	registers,
typically	in	about	4	clock	cycles.
As	the	performance	gap	between	the	CPU	and	main	memory	continued
to	increase,	system	designers	responded	by	inserting	an	additional	larger
cache,	called	an	
L2	cache
,	between	the	L1	cache	and	main	memory,	that
can	be	accessed	in	about	10	clock	cycles.	Many	modern	systems	include
an	even	larger	cache,	called	an	
L3	cache
,	which	sits	between	the	L2
cache	and	main	memory	in	the	memory	hierarchy	and	can	be	accessed
in	about	50	cycles.	While	there	is	considerable	variety	in	the</p>
<p>arrangements,	the	general	principles	are	the	same.	For	our	discussion	in
the	next	section,	we	will	assume	a	simple	memory	hierarchy	with	a	single
L1	cache	between	the	CPU	and	main	memory.
6.4.1	
Generic	Cache	Memory
Organization
Consider	a	computer	system	where	each	memory	address	has	
m
bits
that	form	
M
=	2
unique	addresses.	As	illustrated	in	
Figure	
6.25(a)
,	a
cache	for	such	a	machine	is	organized	as	an	array	of	
S
=	2</p>
<p>cache	sets
.
Each	set	consists	of	
E	cache	lines.
Each	line	consists	of	a	data	
block
of	
B
=	2
bytes,	a	
valid	bit
that	indicates	whether	or	not	the	line	contains
meaningful	information,	and	
t
=	
m
−	(
b
+	
s
)	
tag	bits
(a	subset	of	the	bits
from	the	current	block's	memory	address)	that	uniquely	identify	the	block
stored	in	the	cache	line.
In	general,	a	cache's	organization	can	be	characterized	by	the	tuple	(
S,
E,	B,	m
).	The	size	(or	capacity)	of	a	cache,	
C
,	is	stated	in	terms	of	the
aggregate	size	of	all	the	blocks.	The	tag	bits	and	valid	bit	are	not
included.	Thus,	
C	=	S
×	
E
×	
B.
When	the	CPU	is	instructed	by	a	load	instruction	to	read	a	word	from
address	
A
of	main	memory,	it	sends	address	
A
to	the	cache.	If	the	cache
is	holding	a	copy	of	the	word	at	address	
A
,	it	sends	the	word	immediately
back	to	the	CPU.	So	how	does	the	cache	know	whether	it	contains	a
copy	of	the	word	at	address	
A
?	The	cache	is	organized	so	that	it	can	find
the	requested	word	by	simply	inspecting	the	bits	of	the	address,	similar	to
m
s
b</p>
<p>a	hash	table	with	an	extremely	simple	hash	function.	Here	is	how	it
works:
The	parameters	
S
and	
B
induce	a	partitioning	of	the	
m
address	bits	into
the	three	fields	shown	in	
Figure	
6.25(b)
.	The	
s	set	index	bits
in	A	form
an	index	into
Figure	
6.25	
General	organization	of	cache	
(S,	E,	B,	m).
(a)	A	cache	is	an	array	of	sets.	Each	set	contains	one	or	more	lines.	Each
line	contains	a	valid	bit,	some	tag	bits,	and	a	block	of	data,	(b)	The	cache
organization	induces	a	partition	of	the	
m
address	bits	into	
t
tag	bits,	
s
set
index	bits,	and	
b
block	offset	bits.
the	array	of	
S
sets.	The	first	set	is	set	0,	the	second	set	is	set	1,	and	so
on.	When	interpreted	as	an	unsigned	integer,	the	set	index	bits	tell	us
which	set	the	word	must	be	stored	in.	Once	we	know	which	set	the	word</p>
<p>must	be	contained	in,	the	
t
tag	bits	in	
A
tell	us	which	line	(if	any)	in	the	set
contains	the	word.	A	line	in	the	set	contains	the	word	if	and	only	if	the
valid	bit	is	set	and	the	tag	bits	in	the	line	match	the	tag	bits	in	the	address
A.
Once	we	have	located	the	line	identified	by	the	tag	in	the	set	identified
by	the	set	index,	then	the	
b	block	offset	bits
give	us	the	offset	of	the	word
in	the	B-byte	data	block.
As	you	may	have	noticed,	descriptions	of	caches	use	a	lot	of	symbols.
Figure	
6.26
summarizes	these	symbols	for	your	reference.
Practice	Problem	
6.9	
(solution	page	
663
)
The	following	table	gives	the	parameters	for	a	number	of	different
caches.	For	each	cache,	determine	the	number	of	cache	sets	
(S)
,
tag	bits	
(t)
,	set	index	bits	
(s)
,	and	block	offset	bits	
(b).
Cache
m
C
B
E
S
t
s
b</p>
<ol>
<li></li>
</ol>
<p>32
1,024
4
1</p>
<hr />
<hr />
<hr />
<hr />
<ol start="2">
<li></li>
</ol>
<p>32
1,024
8
4</p>
<hr />
<hr />
<hr />
<hr />
<ol start="3">
<li></li>
</ol>
<p>32
1,024
32
32</p>
<hr />
<hr />
<hr />
<hr />
<p>Parameter
Description
Fundamental	parameters
S
=	2
Number	of	sets
E
Number	of	lines	per	set
s
b</p>
<p>B
=	2
Block	size	(bytes)
m
=	log
(
M
)
Number	of	physical	(main	memory)	address	bits
Derived	quantities
M
=	2
Maximum	number	of	unique	memory	addresses
s
=	log
(S)
Number	of	
set	index	bits
b
=	log
(B)
Number	of	
block	offset	bits
t
=	
m
—	(
s
+	
b
)
Number	of	
tag	bits
C
=	
B
×	
E
×	
S
Cache	size	(bytes),	not	including	overhead	such	as	the	valid	and	tag	bits
Figure	
6.26	
Summary	of	cache	parameters.
Figure	
6.27	
Direct-mapped	cache	(
E
=	1).
There	is	exactly	one	line	per	set.
6.4.2	
Direct-Mapped	Caches
Caches	are	grouped	into	different	classes	based	on	
E
,	the	number	of
cache	lines	per	set.	A	cache	with	exactly	one	line	per	set	(
E
=	1)	is	known
as	a	
direct-mapped
cache	(see	
Figure	
6.27
).	Direct-mapped	caches
b
2
m
2
2</p>
<p>are	the	simplest	both	to	implement	and	to	understand,	so	we	will	use
them	to	illustrate	some	general	concepts	about	how	caches	work.
Suppose	we	have	a	system	with	a	CPU,	a	register	file,	an	L1	cache,	and
a	main	memory.	When	the	CPU	executes	an	instruction	that	reads	a
memory	word	
w
,	it	requests	the	word	from	the	L1	cache.	If	the	L1	cache
has	a	cached	copy	of	
w
,	then	we	have	an	L1	cache	hit,	and	the	cache
quickly	extracts	
w
and	returns	it	to	the	CPU.	Otherwise,	we	have	a	cache
miss,	and	the	CPU	must	wait	while	the	L1	cache	requests	a	copy	of	the
block	containing	
w
from	the	main	memory.	When	the	requested	block
finally	arrives	from	memory,	the	L1	cache	stores	the	block	in	one	of	its
cache	lines,	extracts	word	
w
from	the	stored	block,	and	returns	it	to	the
CPU.	The	process	that	a	cache	goes	through	of	determining	whether	a
request	is	a	hit	or	a	miss	and	then	extracting	the	requested	word	consists
of	three	steps:	(1)	
set	selection
,	(2)	
line	matching
,	and	(3)	
word
extraction.
Figure	
6.28	
Set	selection	in	a	direct-mapped	cache.</p>
<p>Figure	
6.29	
Line	matching	and	word	selection	in	a	direct-mapped
cache.
Within	the	cache	block,	
w
denotes	the	low-order	byte	of	the	word	
w,	w
the	next	byte,	and	so	on.
Set	Selection	in	Direct-Mapped	Caches
In	this	step,	the	cache	extracts	the	
s
set	index	bits	from	the	middle	of	the
address	for	
w.
These	bits	are	interpreted	as	an	unsigned	integer	that
corresponds	to	a	set	number.	In	other	words,	if	we	think	of	the	cache	as	a
one-dimensional	array	of	sets,	then	the	set	index	bits	form	an	index	into
this	array.	
Figure	
6.28
shows	how	set	selection	works	for	a	direct-
mapped	cache.	In	this	example,	the	set	index	bits	00001
are	interpreted
as	an	integer	index	that	selects	set	1.
Line	Matching	in	Direct-Mapped	Caches
Now	that	we	have	selected	some	set	
i
in	the	previous	step,	the	next	step
is	to	determine	if	a	copy	of	the	word	
w
is	stored	in	one	of	the	cache	lines
contained	in	set	
i.
In	a	direct-mapped	cache,	this	is	easy	and	fast
because	there	is	exactly	one	line	per	set.	A	copy	of	
w
is	contained	in	the
line	if	and	only	if	the	valid	bit	is	set	and	the	tag	in	the	cache	line	matches
the	tag	in	the	address	of	
w.
Figure	
6.29
shows	how	line	matching	works	in	a	direct-mapped	cache.
In	this	example,	there	is	exactly	one	cache	line	in	the	selected	set.	The
valid	bit	for	this	line	is	set,	so	we	know	that	the	bits	in	the	tag	and	block
are	meaningful.	Since	the	tag	bits	in	the	cache	line	match	the	tag	bits	in
the	address,	we	know	that	a	copy	of	the	word	we	want	is	indeed	stored	in
0
1
2</p>
<p>the	line.	In	other	words,	we	have	a	cache	hit.	On	the	other	hand,	if	either
the	valid	bit	were	not	set	or	the	tags	did	not	match,	then	we	would	have
had	a	cache	miss.
Word	Selection	in	Direct-Mapped	Caches
Once	we	have	a	hit,	we	know	that	
w
is	somewhere	in	the	block.	This	last
step	determines	where	the	desired	word	starts	in	the	block.	As	shown	in
Figure	
6.29
,	the	block	offset	bits	provide	us	with	the	offset	of	the	first
byte	in	the	desired	word.	Similar	to	our	view	of	a	cache	as	an	array	of
lines,	we	can	think	of	a	block	as	an	array	of	bytes,	and	the	byte	offset	as
an	index	into	that	array.	In	the	example,	the	block	offset	bits	of	100
indicate	that	the	copy	of	
w
starts	at	byte	4	in	the	block.	(We	are	assuming
that	words	are	4	bytes	long.)
Line	Replacement	on	Misses	in	Direct-
Mapped	Caches
If	the	cache	misses,	then	it	needs	to	retrieve	the	requested	block	from	the
next	level	in	the	memory	hierarchy	and	store	the	new	block	in	one	of	the
cache	lines	of	the	set	indicated	by	the	set	index	bits.	In	general,	if	the	set
is	full	of	valid	cache	lines,	then	one	of	the	existing	lines	must	be	evicted.
For	a	direct-mapped	cache,	where	each	set	contains	exactly	one	line,	the
replacement	policy	is	trivial:	the	current	line	is	replaced	by	the	newly
fetched	line.
Putting	It	Together:	A	Direct-Mapped	Cache
2</p>
<h1>in	Action
The	mechanisms	that	a	cache	uses	to	select	sets	and	identify	lines	are
extremely	simple.	They	have	to	be,	because	the	hardware	must	perform
them	in	a	few	nanoseconds.	However,	manipulating	bits	in	this	way	can
be	confusing	to	us	humans.	A	concrete	example	will	help	clarify	the
process.	Suppose	we	have	a	direct-mapped	cache	described	by
In	other	words,	the	cache	has	four	sets,	one	line	per	set,	2	bytes	per
block,	and	4-bit	addresses.	We	will	also	assume	that	each	word	is	a
single	byte.	Of	course,	these	assumptions	are	totally	unrealistic,	but	they
will	help	us	keep	the	example	simple.
When	you	are	first	learning	about	caches,	it	can	be	very	instructive	to
enumerate	the	entire	address	space	and	partition	the	bits,	as	we've	done
in	
Figure	
6.30
for	our	4-bit	example.	There	are	some	interesting	things
to	notice	about	this	enumerated	space:
The	concatenation	of	the	tag	and	index	bits	uniquely	identifies	each
block	in	memory.	For	example,	block	0	consists	of	addresses	0	and	1,
block	1	consists	of	addresses	2	and	3,	block	2	consists	of	addresses
4	and	5,	and	so	on.
Since	there	are	eight	memory	blocks	but	only	four	cache	sets,
multiple	blocks	map	to	the	same	cache	set	(i.e.,	they	have	the	same
set	index).	For	example,	blocks	0	and	4	both	map	to	set	0,	blocks	1
and	5	both	map	to	set	1,	and	so	on.
(
S
,
 
E
,
 
B
,
 
m
)</h1>
<p>(
4
,
 
1
,
 
2
,
 
4
)</p>
<p>Blocks	that	map	to	the	same	cache	set	are	uniquely	identified	by	the
tag.	For	example,	block	0	has	a	tag	bit	of	0	while	block	4	has	a	tag	bit
of	1,	block	1	has	a	tag	bit	of	0	while	block	5	has	a	tag	bit	of	1,	and	so
on.
Address	bits
Address
(decimal)
Tag	bits	(
t
=	1)
Index	bits	(
s
=	2)
Offset	bits	(
b
=	1)
Block	number
(decimal)
0
0
00
0
0
1
0
00
1
0
2
0
01
0
1
3
0
01
1
1
4
0
10
0
2
5
0
10
1
2
6
0
11
0
3
7
0
11
1
3
8
1
00
0
4
9
1
00
1
4
10
1
01
0
5
11
1
01
1
5
12
1
10
0
6</p>
<p>13
1
10
1
6
14
1
11
0
7
15
1
11
1
7
Figure	
6.30	
4-bit	address	space	for	example	direct-mapped	cache.
Let	us	simulate	the	cache	in	action	as	the	CPU	performs	a	sequence	of
reads.	Remember	that	for	this	example	we	are	assuming	that	the	CPU
reads	1-byte	words.	While	this	kind	of	manual	simulation	is	tedious	and
you	may	be	tempted	to	skip	it,	in	our	experience	students	do	not	really
understand	how	caches	work	until	they	work	their	way	through	a	few	of
them.
Initially,	the	cache	is	empty	(i.e.,	each	valid	bit	is	0):
Set
Valid
Tag
block[0]
block[1]
0
0
1
0
2
0
3
0
Each	row	in	the	table	represents	a	cache	line.	The	first	column	indicates
the	set	that	the	line	belongs	to,	but	keep	in	mind	that	this	is	provided	for
convenience	and	is	not	really	part	of	the	cache.	The	next	four	columns
represent	the	actual	bits	in	each	cache	line.	Now,	let's	see	what	happens
when	the	CPU	performs	a	sequence	of	reads:</p>
<p>1
.	
Read	word	at	address	0.	
Since	the	valid	bit	for	set	0	is	0,	this	is	a
cache	miss.	The	cache	fetches	block	0	from	memory	(or	a	lower-
level	cache)	and	stores	the	
block	in	set	0.	Then	the	cache	returns
m[0]	(the	contents	of	memory	location	0)	from	block[0]	of	the	newly
fetched	cache	line.
Set
Valid
Tag
block[0]
block[1]
0
1
0
m[0]
m[1]
1
0
2
0
3
0
2
.	
Read	word	at	address	1.	
This	is	a	cache	hit.	The	cache
immediately	returns	m[1]	from	block[1]	of	the	cache	line.	The	state
of	the	cache	does	not	change.
3
.	
Read	word	at	address	13.	
Since	the	cache	line	in	set	2	is	not
valid,	this	is	a	cache	miss.	The	cache	loads	block	6	into	set	2	and
returns	m[13]	from	block[1]	of	the	new	cache	line.
Set
Valid
Tag
block[0]
block[1]
0
1
0
m[0]
m[1]
1
0
2
1
1
m[12]
m[13]
3
0
4
.	
Read	word	at	address	8.	
This	is	a	miss.	The	cache	line	in	set	0	is
indeed	valid,	but	the	tags	do	not	match.	The	cache	loads	block	4</p>
<p>into	set	0	(replacing	the	line	that	was	there	from	the	read	of
address	0)	and	returns	m[8]	from	block[0]	of	the	new	cache	line.
Set
Valid
Tag
block[0]
block[1]
0
1
1
m[8]
m[9]
1
0
2
1
1
m[12]
m[13]
3
0
5
.	
Read	word	at	address	0.	
This	is	another	miss,	due	to	the
unfortunate	fact	that	we	just	replaced	block	0	during	the	previous
reference	to	address	8.	This	kind	of	miss,	where	we	have	plenty	of
room	in	the	cache	but	keep	alternating	references	to	blocks	that
map	to	the	same	set,	is	an	example	of	a	conflict	miss.
Set
Valid
Tag
block[0]
block[1]
0
1
0
m[0]
m[1]
1
0
2
1
1
m[12]
m[13]
3
0
Conflict	Misses	in	Direct-Mapped	Caches
Conflict	misses	are	common	in	real	programs	and	can	cause	baffling
performance	problems.	Conflict	misses	in	direct-mapped	caches	typically
occur	when	programs	access	arrays	whose	sizes	are	a	power	of	2.	For</p>
<p>example,	consider	a	function	that	computes	the	dot	product	of	two
vectors:
This	function	has	good	spatial	locality	with	respect	to	
and	
,	and	so	we
might	expect	it	to	enjoy	a	good	number	of	cache	hits.	Unfortunately,	this
is	not	always	true.
Suppose	that	floats	are	4	bytes,	that	
is	loaded	into	the	32	bytes	of
contiguous	memory	starting	at	address	0,	and	that	
starts	immediately
after	
at	address	32.	For	simplicity,	suppose	that	a	block	is	16	bytes	(big
enough	to	hold	four	floats)	and	that	the	cache	consists	of	two	sets,	for	a
total	cache	size	of	32	bytes.	We	will	assume	that	the	variable	
is
actually	stored	in	a	CPU	register	and	thus	does	not	require	a	memory
reference.	Given	these	assumptions,	each	
and	
will	map	to	the
identical	cache	set:</p>
<p>Element
Address
Set	index
0
0
4
0
8
0
12
0
16
1
20
1
24
1
28
1
32
0
36
0
40
0
44
0
48
1
52
1
56
1
60
1
At	run	time,	the	first	iteration	of	the	loop	references	
,	a	miss	that
causes	the	block	containing	
to	be	loaded	into	set	0.	The	next</p>
<h2>reference	is	to	
,	another	miss	that	causes	the	block	containing	
to	be	copied	into	set	0,	overwriting	the	values	of	
that	were
copied	in	by	the	previous	reference.	During	the	next	iteration,	the
reference	to	
misses,	which	causes	the	
block	to	be
loaded	back	into	set	0,	overwriting	the	
block.	So	now	we	have
a	conflict	miss,	and	in	fact	each	subsequent	reference	to	
and	
will
result	in	a	conflict	miss	as	we	thrash	back	and	forth	between	blocks	of	
and	
.	The	term	
thrashing
describes	any	situation	where	a	cache	is
repeatedly	loading	and	evicting	the	same	sets	of	cache	blocks.
Aside	
Why	index	with	the	middle	bits?
You	may	be	wondering	why	caches	use	the	middle	bits	for	the	set
index	instead	of	the	high-order	bits.	There	is	a	good	reason	why
the	middle	bits	are	better.	
Figure	
6.31
shows	why.	If	the	high-
order	bits	are	used	as	an	index,	then	some	contiguous	memory
blocks	will	map	to	the	same	cache	set.	For	example,	in	the	figure,
the	first	four	blocks	map	to	the	first	cache	set,	the	second	four
blocks	map	to	the	second	set,	and	so	on.	If	a	program	has	good
spatial	locality	and	scans	the	elements	of	an	array	sequentially,
then	the	cache	can	only	hold	a	block-size	chunk	of	the	array	at
any	point	in	time.	This	is	an	inefficient	use	of	the	cache.	Contrast
this	with	middle-bit	indexing,	where	adjacent	blocks	always	map	to
different	cache	sets.	In	this	case,	the	cache	can	hold	an	entire	
C</h2>
<p>size	chunk	of	the	array,	where	
C
is	the	cache	size.</p>
<p>Figure	
6.31	
Why	caches	index	with	the	middle	bits.
The	bottom	line	is	that	even	though	the	program	has	good	spatial	locality
and	we	have	room	in	the	cache	to	hold	the	blocks	for	both	
and
,	each	reference	results	in	a	conflict	miss	because	the	blocks	map	to
the	same	cache	set.	It	is	not	unusual	for	this	kind	of	thrashing	to	result	in
a	slowdown	by	a	factor	of	2	or	3.	Also,	be	aware	that	even	though	our
example	is	extremely	simple,	the	problem	is	real	for	larger	and	more
realistic	direct-mapped	caches.
Luckily,	thrashing	is	easy	for	programmers	to	fix	once	they	recognize
what	is	going	on.	One	easy	solution	is	to	put	
B
bytes	of	padding	at	the
end	of	each	array.	
For	example,	instead	of	defining	
to	be	float	
,	we
define	it	to	be	float	
.	Assuming	
starts	immediately	after	
in
memory,	we	have	the	following	mapping	of	array	elements	to	sets:</p>
<p>Element
Address
Set	index
0
0
4
0
8
0
12
0
16
1
20
1
24
1
28
1
48
1
52
1
56
1
60
1
64
0
68
0
72
0
76
0
With	the	padding	at	the	end	of	
and	
now	map	to	different
sets,	which	eliminates	the	thrashing	conflict	misses.</p>
<p>Practice	Problem	
6.10	
(solution	page	
663
)
In	the	previous	
example,	what	fraction	of	the	total
references	to	
and	
will	be	hits	once	we	have	padded	array	
?
Practice	Problem	
6.11	
(solution	page	
663
)
Imagine	a	hypothetical	cache	that	uses	the	high-order	
s
bits	of	an
address	as	the	set	index.	For	such	a	cache,	contiguous	chunks	of
memory	blocks	are	mapped	to	the	same	cache	set.
A
.	
How	many	blocks	are	in	each	of	these	contiguous	array
chunks?
B
.	
Consider	the	following	code	that	runs	on	a	system	with	a
cache	of	the	form	
(S,	E,	B,	m)
=	(512,	1,	32,	32):
What	is	the	maximum	number	of	array	blocks	that	are
stored	in	the	cache	at	any	point	in	time?
6.4.3	
Set	Associative	Caches
The	problem	with	conflict	misses	in	direct-mapped	caches	stems	from	the
constraint	that	each	set	has	exactly	one	line	(or	in	our	terminology,	
E
=</p>
<h2>1).	A	
set	associative	cache
relaxes	this	constraint	so	that	each	set	holds
more	than	one	cache	line.	A	cache	with	1	&lt;	
E
&lt;	
C/B
is	often	called	an	
E</h2>
<p>way	set	associative	cache.	We
Figure	
6.32	
Set	associative	cache	(1	&lt;	
E
&lt;	
C/B
).
In	a	set	associative	cache,	each	set	contains	more	than	one	line.	This
particular	example	shows	a	two-way	set	associative	cache.
Figure	
6.33	
Set	selection	in	a	set	associative	cache.
will	discuss	the	special	case,	where	
E
=	
C/B
,	in	the	next	section.	
Figure
6.32
shows	the	organization	of	a	two-way	set	associative	cache.
Set	Selection	in	Set	Associative	Caches</p>
<p>Set	selection	is	identical	to	a	direct-mapped	cache,	with	the	set	index	bits
identifying	the	set.	
Figure	
6.33
summarizes	this	principle.
Line	Matching	and	Word	Selection	in	Set
Associative	Caches
Line	matching	is	more	involved	in	a	set	associative	cache	than	in	a
direct-mapped	cache	because	it	must	check	the	tags	and	valid	bits	of
multiple	lines	in	order	to	determine	if	the	requested	word	is	in	the	set.	A
conventional	memory	is	an	array	of	values	that	takes	an	address	as	input
and	returns	the	value	stored	at	that	address.	An	
associative	memory
,	on
the	other	hand,	is	an	array	of	(key,	value)	pairs	that	takes	as	input	the	key
and	returns	a	value	from	one	of	the	(key,	value)	pairs	that	matches	the
input	key.	Thus,	we	can	think	of	each	set	in	a	set	associative	cache	as	a
small	associative	memory	where	the	keys	are	the	concatenation	of	the
tag	and	valid	bits,	and	the	values	are	the	contents	of	a	block.
Figure	
6.34	
Line	matching	and	word	selection	in	a	set	associative
cache.
Figure	
6.34
shows	the	basic	idea	of	line	matching	in	an	associative
cache.	An	important	idea	here	is	that	any	line	in	the	set	can	contain	any</p>
<p>of	the	memory	blocks	that	map	to	that	set.	So	the	cache	must	search
each	line	in	the	set	for	a	valid	line	whose	tag	matches	the	tag	in	the
address.	If	the	cache	finds	such	a	line,	then	we	have	a	hit	and	the	block
offset	selects	a	word	from	the	block,	as	before.
Line	Replacement	on	Misses	in	Set
Associative	Caches
If	the	word	requested	by	the	CPU	is	not	stored	in	any	of	the	lines	in	the
set,	then	we	have	a	cache	miss,	and	the	cache	must	fetch	the	block	that
contains	the	word	from	memory.	However,	once	the	cache	has	retrieved
the	block,	which	line	should	it	replace?	Of	course,	if	there	is	an	empty
line,	then	it	would	be	a	good	candidate.	But	if	there	are	no	empty	lines	in
the	set,	then	we	must	choose	one	of	the	nonempty	lines	and	hope	that
the	CPU	does	not	reference	the	replaced	line	anytime	soon.
It	is	very	difficult	for	programmers	to	exploit	knowledge	of	the	cache
replacement	policy	in	their	codes,	so	we	will	not	go	into	much	detail	about
it	here.	The	simplest	replacement	policy	is	to	choose	the	line	to	replace	at
random.	Other	more	sophisticated	policies	draw	on	the	principle	of
locality	to	try	to	minimize	the	probability	that	the	replaced	line	will	be
referenced	in	the	near	future.	For	example,	a	
least	frequently	used	(LFU)
policy	will	replace	the	line	that	has	been	referenced	the	fewest	times	over
some	past	time	window.	A	
least	recently	used	(LRU)
policy	will	replace
the	line	that	was	last	accessed	the	furthest	in	the	past.	All	of	these
policies	require	additional	time	and	hardware.	But	as	we	move	further
down	the	memory	hierarchy,	away	from	the	CPU,	the	cost	of	a	miss
becomes	more	expensive	and	it	becomes	more	worthwhile	to	minimize
misses	with	good	replacement	policies.</p>
<p>6.4.4	
Fully	Associative	Caches
A	fully	associative	cache
consists	of	a	single	set	(i.e.,	
E
=	
C/B
)	that
contains	all	of	the	cache	lines.	
Figure	
6.35
shows	the	basic
organization.
Figure	
6.35	
Fully	associative	cache	(
E
=	
C/B
).
In	a	fully	associative	cache,	a	single	set	contains	all	of	the	lines.
Figure	
6.36	
Set	selection	in	a	fully	associative	cache.
Notice	that	there	are	no	set	index	bits.</p>
<p>Figure	
6.37	
Line	matching	and	word	selection	in	a	fully	associative
cache.
Set	Selection	in	Fully	Associative	Caches
Set	selection	in	a	fully	associative	cache	is	trivial	because	there	is	only
one	set,	summarized	in	
Figure	
6.36
.	Notice	that	there	are	no	set	index
bits	in	the	address,	which	is	partitioned	into	only	a	tag	and	a	block	offset.
Line	Matching	and	Word	Selection	in	Fully
Associative	Caches
Line	matching	and	word	selection	in	a	fully	associative	cache	work	the
same	as	with	a	set	associative	cache,	as	we	show	in	
Figure	
6.37
.	The
difference	is	mainly	a	question	of	scale.
Because	the	cache	circuitry	must	search	for	many	matching	tags	in
parallel,	it	is	difficult	and	expensive	to	build	an	associative	cache	that	is
both	large	and	fast.	As	a	result,	fully	associative	caches	are	only
appropriate	for	small	caches,	such	
as	the	translation	lookaside	buffers
(TLBs)	in	virtual	memory	systems	that	cache	page	table	entries	(
Section
9.6.2
).
Practice	Problem	
6.12	
(solution	page	
663
)
The	problems	that	follow	will	help	reinforce	your	understanding	of
how	caches	work.	Assume	the	following:
The	memory	is	byte	addressable.</p>
<p>Memory	accesses	are	to	1-byte	words	(not	to	4-byte	words).
Addresses	are	13	bits	wide.
The	cache	is	two-way	set	associative	(
E
=	2),	with	a	4-byte
block	size	(
B
=	4)	and	eight	sets	(
S
=	8).
The	contents	of	the	cache	are	as	follows,	with	all	numbers	given	in
hexadecimal	notation.
2-way	set	associative	cache
Set
index
Line	0
Line	1
Tag
Valid
Byte
0
Byte
1
Byte
2
Byte
3
Tag
Valid
Byte
0
Byte
1
0
09
1
86
30
3F
10
00
0
—
—
1
45
1
60
4F
E0
23
38
1
00
BC
2
EB
0
—
—
—
—
0B
0
—
—
3
06
0
—
—
—
—
32
1
12
08
4
C7
1
06
78
07
C5
05
1
40
67
5
71
1
OB
DE
18
4B
6E
0
—
—
6
91
1
A0
B7
26
2D
F0
0
—
—
7
46
0
—
—
—
—
DE
1
12
CO
The	following	figure	shows	the	format	of	an	address	(1	bit	per
box).	Indicate	(by	labeling	the	diagram)	the	fields	that	would	be
used	to	determine	the	following:</p>
<p>CO.	The	cache	block	offset
CI.	The	cache	set	index
CT.	The	cache	tag
Practice	Problem	
6.13	
(solution	page	
664
)
Suppose	a	program	running	on	the	machine	in	
Problem	
6.12
references	the	1-byte	word	at	address	
.	Indicate	the	cache
entry	accessed	and	the	cache	byte	
value	returned	in	hexadecimal
notation.	Indicate	whether	a	cache	miss	occurs.	If	there	is	a	cache
miss,	enter	&quot;—&quot;	for	&quot;Cache	byte	returned.&quot;
A
.	
Address	format	(1	bit	per	box):
B
.	
Memory	reference:
Parameter
Value
Cache	block	offset	(CO)</p>
<hr />
<p>Cache	set	index	(CI)</p>
<hr />
<p>Cache	tag	(CT)</p>
<hr />
<p>Cache	hit?	(Y/N)</p>
<hr />
<p>Cache	byte	returned</p>
<hr />
<p>Practice	Problem	
6.14	
(solution	page	
664
)
Repeat	
Problem	
6.13
for	memory	address	
.
A
.	
Address	format	(1	bit	per	box):
B
.	
Memory	reference:
Parameter
Value
Cache	block	offset	(CO)</p>
<hr />
<p>Cache	set	index	(CI)</p>
<hr />
<p>Cache	tag	(CT)</p>
<hr />
<p>Cache	hit?	(Y/N)</p>
<hr />
<p>Cache	byte	returned</p>
<hr />
<p>Practice	Problem	
6.15	
(solution	page	
664
)
Repeat	
Problem	
6.13
for	memory	address	
.
A
.	
Address	format	(1	bit	per	box):
B
.	
Memory	reference:
Parameter
Value
Cache	block	offset	(CO)</p>
<hr />
<p>Cache	set	index	(CI)</p>
<hr />
<p>Cache	tag	(CT)</p>
<hr />
<p>Cache	hit?	(Y/N)</p>
<hr />
<p>Cache	byte	returned</p>
<hr />
<p>Practice	Problem	
6.16	
(solution	page	
665
)
For	the	cache	in	
Problem	
6.12
,	list	all	of	the	hexadecimal	memory
addresses	that	will	hit	in	set	3.
6.4.5	
Issues	with	Writes
As	we	have	seen,	the	operation	of	a	cache	with	respect	to	reads	is
straightforward.	First,	look	for	a	copy	of	the	desired	word	
w
in	the	cache.
If	there	is	a	hit,	return	
w
immediately.	If	there	is	a	miss,	fetch	the	block
that	contains	
w
from	the	next	lower	level	of	the	memory	hierarchy,	store
the	block	in	some	cache	line	(possibly	evicting	a	valid	line),	and	then
return	
w.
The	situation	for	writes	is	a	little	more	complicated.	Suppose	we	write	a
word	
w
that	is	already	cached	(a	
write	hit).
After	the	cache	updates	its
copy	of	
w
,	what	does	it	do	about	updating	the	copy	of	
w
in	the	next	lower
level	of	the	hierarchy?	The	simplest	approach,	known	as	
write-through
,	is
to	immediately	write	w's	cache	block	to	the	next	lower	level.	While
simple,	write-through	has	the	disadvantage	of	causing	bus	traffic	with
every	write.	Another	approach,	known	as	
write-back
,	defers	the	update
as	long	as	possible	by	writing	the	updated	block	to	the	next	lower	level</p>
<p>only	when	it	is	evicted	from	the	cache	by	the	replacement	algorithm.
Because	of	locality,	write-back	can	significantly	reduce	the	amount	of	bus
traffic,	but	it	has	the	disadvantage	of	additional	complexity.	The	cache
must	maintain	an	additional	
dirty	bit
for	each	cache	line	that	indicates
whether	or	not	the	cache	block	has	been	modified.
Another	issue	is	how	to	deal	with	write	misses.	One	approach,	known	as
write-allocate
,	loads	the	corresponding	block	from	the	next	lower	level
into	the	cache	and	then	updates	the	cache	block.	Write-allocate	tries	to
exploit	spatial	locality	of	writes,	but	it	has	the	disadvantage	that	every
miss	results	in	a	block	transfer	from	the	next	lower	level	to	the	cache.
The	alternative,	known	as	
no-write-allocate
,	bypasses	the	cache	and
writes	the	word	directly	to	the	next	lower	level.	Write-through	caches	are
typically	no-write-allocate.	Write-back	caches	are	typically	write-allocate.
Optimizing	caches	for	writes	is	a	subtle	and	difficult	issue,	and	we	are
only	scratching	the	surface	here.	The	details	vary	from	system	to	system
and	are	often	proprietary	and	poorly	documented.	To	the	programmer
trying	to	write	reasonably	
cache-friendly	programs,	we	suggest	adopting
a	mental	model	that	assumes	write-back,	write-allocate	caches.	There
are	several	reasons	for	this	suggestion:	As	a	rule,	caches	at	lower	levels
of	the	memory	hierarchy	are	more	likely	to	use	write-back	instead	of
write-through	because	of	the	larger	transfer	times.	For	example,	virtual
memory	systems	(which	use	main	memory	as	a	cache	for	the	blocks
stored	on	disk)	use	write-back	exclusively.	But	as	logic	densities
increase,	the	increased	complexity	of	write-back	is	becoming	less	of	an
impediment	and	we	are	seeing	write-back	caches	at	all	levels	of	modern
systems.	So	this	assumption	matches	current	trends.	Another	reason	for
assuming	a	write-back,	write-allocate	approach	is	that	it	is	symmetric	to
the	way	reads	are	handled,	in	that	write-back	write-allocate	tries	to	exploit</p>
<p>locality.	Thus,	we	can	develop	our	programs	at	a	high	level	to	exhibit
good	spatial	and	temporal	locality	rather	than	trying	to	optimize	for	a
particular	memory	system.
6.4.6	
Anatomy	of	a	Real	Cache
Hierarchy
So	far,	we	have	assumed	that	caches	hold	only	program	data.	But,	in
fact,	caches	can	hold	instructions	as	well	as	data.	A	cache	that	holds
instructions	only	is	called	an	
i-cache.
A	cache	that	holds	program	data
only	is	called	a	
d-cache.
A	cache	that	holds	both	instructions	and	data	is
known	as	a	
unified	cache.
Modern	processors	include	separate	i-caches
and	d-caches.	There	are	a	number	of	reasons	for	this.	With	two	separate
caches,	the	processor	can	read	an	instruction	word	and	a	data	word	at
the	same	time.	I-caches	are	typically	read-only,	and	thus	simpler.	The	two
caches	are	often	optimized	to	different	access	patterns	and	can	have
different	block	sizes,	associativities,	and	capacities.	Also,	having
separate	caches	ensures	that	data	accesses	do	not	create	conflict
misses	with	instruction	accesses,	and	vice	versa,	at	the	cost	of	a
potential	increase	in	capacity	misses.
Figure	
6.38
shows	the	cache	hierarchy	for	the	Intel	Core	i7	processor.
Each	CPU	chip	has	four	cores.	Each	core	has	its	own	private	L1	i-cache,
L1	d-cache,	and	L2	unified	cache.	All	of	the	cores	share	an	on-chip	L3
unified	cache.	An	interesting	feature	of	this	hierarchy	is	that	all	of	the
SRAM	cache	memories	are	contained	in	the	CPU	chip.</p>
<p>Figure	
6.39
summarizes	the	basic	characteristics	of	the	Core	i7
caches.
6.4.7	
Performance	Impact	of	Cache
Parameters
Cache	performance	is	evaluated	with	a	number	of	metrics:
Miss	rate.	
The	fraction	of	memory	references	during	the	execution	of
a	program,	or	a	part	of	a	program,	that	miss.	It	is	computed	as	#
misses/
#	
references.
Hit	rate.	
The	fraction	of	memory	references	that	hit.	It	is	computed	as
1	−	
miss	rate.
Hit	time.	
The	time	to	deliver	a	word	in	the	cache	to	the	CPU,
including	the	time	for	set	selection,	line	identification,	and	word
selection.	Hit	time	is	on	the	order	of	several	clock	cycles	for	L1
caches.</p>
<p>Figure	
6.38	
Intel	Core	i7	cache	hierarchy.
Cache	type
Access	time
(cycles)
Cache	size
(
C
)
Assoc.
(
E
)
Block	size
(
B
)
Sets
(
S
)
L1	i-cache
4
32	KB
8
64	B
64
L1	d-cache
4
32	KB
8
64	B
64
L2	unified
cache
10
256	KB
8
64	B
512
L3	unified
cache
40−75
8	MB
16
64	B
8,192
Figure	
6.39	
Characteristics	of	the	Intel	Core	i7	cache	hierarchy.
Miss	penalty.	
Any	additional	time	required	because	of	a	miss.	The
penalty	for	Ll	misses	served	from	L2	is	on	the	order	of	10	cycles;	from</p>
<p>L3,50	cycles;	and	from	main	memory,	200	cycles.
Optimizing	the	cost	and	performance	trade-offs	of	cache	memories	is	a
subtle	exercise	that	requires	extensive	simulation	on	realistic	benchmark
codes	and	thus	is	beyond	our	scope.	However,	it	is	possible	to	identify
some	of	the	qualitative	trade-offs.
Impact	of	Cache	Size
On	the	one	hand,	a	larger	cache	will	tend	to	increase	the	hit	rate.	On	the
other	hand,	it	is	always	harder	to	make	large	memories	run	faster.	As	a
result,	larger	caches	tend	to	increase	the	hit	time.	This	explains	why	an
L1	cache	is	smaller	than	an	L2	cache,	and	an	L2	cache	is	smaller	than
an	L3	cache.
Impact	of	Block	Size
Large	blocks	are	a	mixed	blessing.	On	the	one	hand,	larger	blocks	can
help	increase	the	hit	rate	by	exploiting	any	spatial	locality	that	might	exist
in	a	program.	However,	for	a	given	cache	size,	larger	blocks	imply	a
smaller	number	of	cache	lines,	which	can	hurt	the	hit	rate	in	programs
with	more	temporal	locality	than	spatial	locality.	Larger	blocks	also	have	a
negative	impact	on	the	miss	penalty,	since	larger	blocks	cause	larger
transfer	times.	Modern	systems	such	as	the	Core	i7	compromise	with
cache	blocks	that	contain	64	bytes.
Impact	of	Associativity</p>
<p>The	issue	here	is	the	impact	of	the	choice	of	the	parameter	
E
,	the
number	of	cache	lines	per	set.	The	advantage	of	higher	associativity	(i.e.,
larger	values	of	
E
)	is	that	it	decreases	the	vulnerability	of	the	cache	to
thrashing	due	to	conflict	misses.	However,	higher	associativity	comes	at
a	significant	cost.	Higher	associativity	is	expensive	to	implement	and
hard	to	make	fast.	It	requires	more	tag	bits	per	line,	additional	LRU	state
bits	per	line,	and	additional	control	logic.	Higher	associativity	can
increase	hit	time,	because	of	the	increased	complexity,	and	it	can	also
increase	the	miss	penalty	because	of	the	increased	complexity	of
choosing	a	victim	line.
The	choice	of	associativity	ultimately	boils	down	to	a	trade-off	between
the	hit	time	and	the	miss	penalty.	Traditionally,	high-performance	systems
that	pushed	the	clock	rates	would	opt	for	smaller	associativity	for	L1
caches	(where	the	miss	penalty	is	only	a	few	cycles)	and	a	higher	degree
of	associativity	for	the	lower	levels,	where	the	miss	penalty	is	higher.	For
example,	in	Intel	Core	i7	systems,	the	L1	and	L2	caches	are	8-way
associative,	and	the	L3	cache	is	16-way.
Impact	of	Write	Strategy
Write-through	caches	are	simpler	to	implement	and	can	use	a	
write	buffer
that	works	independently	of	the	cache	to	update	memory.	Furthermore,
read	misses	are	less	expensive	because	they	do	not	trigger	a	memory
write.	On	the	other	hand,	write-back	caches	result	in	fewer	transfers,
which	allows	more	bandwidth	to	memory	for	I/O	devices	that	perform
DMA.	Further,	reducing	the	number	of	transfers	becomes	increasingly
important	as	we	move	down	the	hierarchy	and	the	transfer	times</p>
<p>increase.	In	general,	caches	further	down	the	hierarchy	are	more	likely	to
use	write-back	than	write-through.</p>
<p>6.5	
Writing	Cache-Friendly	Code
In	
Section	
6.2
,	we	introduced	the	idea	of	locality	and	talked	in
qualitative	terms	about	what	constitutes	good	locality.	Now	that	we
understand	how	cache	memories	work,	we	can	be	more	precise.
Programs	with	better	locality	will	tend	to	have	lower	miss	rates,	and
programs	with	lower	miss	rates	will	tend	to	run	faster	than	programs	with
higher	miss	rates.	Thus,	good	programmers	should	always	try	to
Aside	
Cache	lines,	sets,	and	blocks:
What's	the	difference?
It	is	easy	to	confuse	the	distinction	between	cache	lines,	sets,	and
blocks.	Let's	review	these	ideas	and	make	sure	they	are	clear:
A	
block
is	a	fixed-size	packet	of	information	that	moves	back
and	forth	between	a	cache	and	main	memory	(or	a	lower-level
cache).
A	
line
is	a	container	in	a	cache	that	stores	a	block,	as	well	as
other	information	such	as	the	valid	bit	and	the	tag	bits.
A	
set
is	a	collection	of	one	or	more	lines.	Sets	in	direct-mapped
caches	consist	of	a	single	line.	Sets	in	set	associative	and	fully
associative	caches	consist	of	multiple	lines.
In	direct-mapped	caches,	sets	and	lines	are	indeed	equivalent.
However,	in	associative	caches,	sets	and	lines	are	very	different</p>
<p>things	and	the	terms	cannot	be	used	interchangeably.
Since	a	line	always	stores	a	single	block,	the	terms	&quot;line&quot;	and
&quot;block&quot;	are	often	used	interchangeably.	For	example,	systems
professionals	usually	refer	to	the	&quot;line	size&quot;	of	a	cache,	when	what
they	really	mean	is	the	block	size.	This	usage	is	very	common	and
shouldn't	cause	any	confusion	as	long	as	you	understand	the
distinction	between	blocks	and	lines.
write	code	that	is	
cache	friendly
,	in	the	sense	that	it	has	good	locality.
Here	is	the	basic	approach	we	use	to	try	to	ensure	that	our	code	is	cache
friendly.
1
.	
Make	the	common	case	go	fast.	
Programs	often	spend	most	of
their	time	in	a	few	core	functions.	These	functions	often	spend
most	of	their	time	in	a	few	loops.	So	focus	on	the	inner	loops	of	the
core	functions	and	ignore	the	rest.
2
.	
Minimize	the	number	of	cache	misses	in	each	inner	loop.	
All
other	things	being	equal,	such	as	the	total	number	of	loads	and
stores,	loops	with	better	miss	rates	will	run	faster.
To	see	how	this	works	in	practice,	consider	the	
function	from
Section	
6.2
:</p>
<p>Is	this	function	cache	friendly?	First,	notice	that	there	is	good	temporal
locality	in	the	loop	body	with	respect	to	the	local	variables	
and	
.	In
fact,	because	these	are	local	variables,	any	reasonable	optimizing
compiler	will	cache	them	in	the	register	file,	the	highest	level	of	the
memory	hierarchy.	Now	consider	the	stride-1	references	to	vector	
.	In
general,	if	a	cache	has	a	block	size	of	
B
bytes,	then	a	
stride-
k
reference
pattern	(where	
k
is	expressed	in	words)	results	in	an	average	of	min	(1,
(
word	size
×	
k
)/
B
)	misses	per	loop	iteration.	This	is	minimized	for	
k
=	1,
so	the	stride-1	references	to	
are	indeed	cache	friendly.	For	example,
suppose	that	
is	block	aligned,	words	are	4	bytes,	cache	blocks	are	4
words,	and	the	cache	is	initially	empty	(a	cold	cache).	Then,	regardless	of
the	cache	organization,	the	references	to	
will	result	in	the	following
pattern	of	hits	and	misses:
i
=	0
i
=	1
i
=	2
i
=	3
i
=	4
i
=	5
i
=	6
i
=	7
Access	order,	[h]it	or	[m]iss
1	
[m]
2	[h]
3	[h]
4	[h]
5	
[m]
6	[h]
7	[h]
8	[h]
In	this	example,	the	reference	to	
misses	and	the	corresponding
block,	which	contains	
,	is	loaded	into	the	cache	from	memory.
Thus,	the	next	three	references	are	all	hits.	The	reference	to	
causes
another	miss	as	a	new	block	is	loaded	into	the	cache,	the	next	three
references	are	hits,	and	so	on.	In	general,	three	out	of	four	references	will
hit,	which	is	the	best	we	can	do	in	this	case	with	a	cold	cache.</p>
<p>To	summarize,	our	simple	
example	illustrates	two	important	points
about	writing	cache-friendly	code:
Repeated	references	to	local	variables	are	good	because	the
compiler	can	cache	them	in	the	register	file	(temporal	locality).
Stride-1	reference	patterns	are	good	because	caches	at	all	levels	of
the	memory	hierarchy	store	data	as	contiguous	blocks	(spatial
locality).
Spatial	locality	is	especially	important	in	programs	that	operate	on
multidimensional	arrays.	For	example,	consider	the	
function
from	
Section	
6.2
,	which	sums	the	elements	of	a	two-dimensional	array
in	row-major	order:
Since	C	stores	arrays	in	row-major	order,	the	inner	loop	of	this	function
has	the	same	desirable	stride-1	access	pattern	as	
.	For	example,
suppose	we	make	the	same	assumptions	about	the	cache	as	for	
.</p>
<p>Then	the	references	to	the	array	a	will	result	in	the	following	pattern	of
hits	and	misses:
j
=	0
j
=	1
j
=	2
j
=	3
j
=	4
j
=	5
j
=	6
j
=	7
i
=	0
1	
[m]
2	[h]
3	[h]
4	[h]
5	
[m]
6	[h]
7	[h]
8	[h]
i
=	1
9	
[m]
10	[h]
11	[h]
12	[h]
13	
[m]
14	[h]
15	[h]
16	[h]
i
=	2
17	
[m]
18	[h]
19	[h]
20	[h]
21	
[m]
22	[h]
23	[h]
24	[h]
i
=	3
25	
[m]
26	[h]
27	[h]
28	[h]
29	
[m]
30	[h]
31	[h]
32	[h]
But	consider	what	happens	if	we	make	the	seemingly	innocuous	change
of	permuting	the	loops:
In	this	case,	we	are	scanning	the	array	column	by	column	instead	of	row
by	row.	If	we	are	lucky	and	the	entire	array	fits	in	the	cache,	then	we	will</p>
<p>enjoy	the	same	miss	rate	of	1/4.	However,	if	the	array	is	larger	than	the
cache	(the	more	likely	case),	then	each	and	every	access	of	
will
miss!
j
=	0
j
=	1
j
=	2
j
=	3
j
=	4
j
=	5
j
=	6
j
=	7
i
=	0
1	
[m]
5	
[m]
9	
[m]
13	
[m]
17	
[m]
21	
[m]
25	
[m]
29	
[m]
i
=	1
2	
[m]
6	
[m]
10	
[m]
14	
[m]
18	
[m]
22	
[m]
26	
[m]
30	
[m]
i
=	2
3	
[m]
7	
[m]
11	
[m]
15	
[m]
19	
[m]
23	
[m]
27	
[m]
31	
[m]
i
=	3
4	
[m]
8	
[m]
12	
[m]
16	
[m]
20	
[m]
24	
[m]
28	
[m]
32	
[m]
Higher	miss	rates	can	have	a	significant	impact	on	running	time.	For
example,	on	our	desktop	machine,	
runs	25	times	faster
than	
for	large	array	sizes.	To	summarize,	programmers
should	be	aware	of	locality	in	their	programs	and	try	to	write	programs
that	exploit	it.
Practice	Problem	
6.17	
(solution
page	
665
)
Transposing	the	rows	and	columns	of	a	matrix	is	an	important
problem	in	signal	processing	and	scientific	computing	applications.
It	is	also	interesting	from	a	locality	point	of	view	because	its
reference	pattern	is	both	row-wise	and	column-wise.	For	example,
consider	the	following	transpose	routine:</p>
<p>Assume	this	code	runs	on	a	machine	with	the	following	properties:
=	4.
The	
array	starts	at	address	0	and	the	
array	starts	at
address	16	(decimal).
There	is	a	single	L1	data	cache	that	is	direct-mapped,	write-
through,	and	write-allocate,	with	a	block	size	of	8	bytes.
The	cache	has	a	total	size	of	16	data	bytes	and	the	cache	is
initially	empty.
Accesses	to	the	
and	
arrays	are	the	only	sources	of
read	and	write	misses,	respectively.
A
.	
For	each	
and	
,	indicate	whether	the	access	to
and	
is	a	hit	(h)	or	a	miss	(m).</p>
<p>For	example,	reading	
is	a	miss	and	writing	
is	also	a	miss.
Col.	0
Col.	1
Col.	0
Col.	1
Row	0
m</p>
<hr />
<p>Row0
m</p>
<hr />
<p>Row	1</p>
<hr />
<hr />
<p>Row	1</p>
<hr />
<hr />
<p>B
.	
Repeat	the	problem	for	a	cache	with	32	data	bytes.
Practice	Problem	
6.18	
(solution
page	
666
)
The	heart	of	the	recent	hit	game	
SimAquarium
is	a	tight	loop	that
calculates	the	average	position	of	256	algae.	You	are	evaluating
its	cache	performance	on	a	machine	with	a	1,024-byte	direct-
mapped	data	cache	with	16-byte	blocks	(
B
=	16).	You	are	given
the	following	definitions:</p>
<p>You	should	also	assume	the	following:
=	4.
begins	at	memory	address	0.
The	cache	is	initially	empty.
The	only	memory	accesses	are	to	the	entries	of	the	array
Variables	
,	and	
are	stored	in
registers.
Determine	the	cache	performance	for	the	following	code:
A
.	
What	is	the	total	number	of	reads?
B
.	
What	is	the	total	number	of	reads	that	miss	in	the	cache?</p>
<p>C
.	
What	is	the	miss	rate?
Practice	Problem	
6.19	
(solution
page	
666
)
Given	the	assumptions	of	
Practice	Problem	
6.18
,	determine
the	cache	performance	of	the	following	code:
A
.	
What	is	the	total	number	of	reads?
B
.	
What	is	the	total	number	of	reads	that	miss	in	the	cache?
C
.	
What	is	the	miss	rate?
D
.	
What	would	the	miss	rate	be	if	the	cache	were	twice	as	big?
Practice	Problem	
6.20	
(solution
page	
666
)</p>
<p>Given	the	assumptions	of	
Practice	Problem	
6.18
,	determine
the	cache	performance	of	the	following	code:
A
.	
What	is	the	total	number	of	reads?
B
.	
What	is	the	total	number	of	reads	that	miss	in	the	cache?
C
.	
What	is	the	miss	rate?
D
.	
What	would	the	miss	rate	be	if	the	cache	were	twice	as	big?</p>
<p>6.6	
Putting	It	Together:	The	Impact
of	Caches	on	Program	Performance
This	section	wraps	up	our	discussion	of	the	memory	hierarchy	by
studying	the	impact	that	caches	have	on	the	performance	of	programs
running	on	real	machines.
6.6.1	
The	Memory	Mountain
The	rate	that	a	program	reads	data	from	the	memory	system	is	called	the
read	throughput
,	or	sometimes	the	
read	bandwidth
.	If	a	program	reads	
n
bytes	over	a	period	of	
s
seconds,	then	the	read	throughput	over	that
period	is	
n/s
,	typically	expressed	in	units	of	megabytes	per	second
(MB/s).
If	we	were	to	write	a	program	that	issued	a	sequence	of	read	requests
from	a	tight	program	loop,	then	the	measured	read	throughput	would	give
us	some	insight	into	the	performance	of	the	memory	system	for	that
particular	sequence	of	reads.	
Figure	
6.40
shows	a	pair	of	functions
that	measure	the	read	throughput	for	a	particular	read	sequence.
The	
function	generates	the	read	sequence	by	scanning	the	first
elements	of	an	array	with	a	stride	of	
.	To	increase	the
available	parallelism	in	the	inner	loop,	it	uses	4	×	4	unrolling	(
Section</p>
<h2>5.9
).	The	
function	is	a	wrapper	that	calls	the	
function	and
returns	the	measured	read	throughput.	The	call	to	the	
function	in
line	37	warms	the	cache.	The	
function	in	line	38	calls	the	
function	with	arguments	
and	estimates	the	running	time	of	the	
function	in	CPU	cycles.	Notice	that	the	
argument	to	the	
function
is	in	units	of	bytes,	while	the	corresponding	
argument	to	the	
function	is	in	units	of	array	elements.	Also,	notice	that	line	39	computes
MB/s	as	10
bytes/s,	as	opposed	to	2
bytes/s.
The	
and	
arguments	to	the	
function	allow	us	to	control
the	degree	of	temporal	and	spatial	locality	in	the	resulting	read	sequence.
Smaller	values	of	
result	in	a	smaller	working	set	size,	and	thus
better	temporal	locality.	Smaller	values	of	
result	in	better	spatial
locality.	If	we	call	the	
function	repeatedly	with	different	values	of	
and	
,	then	we	can	recover	a	fascinating	two-dimensional	function
of	read	throughput	versus	temporal	and	spatial	locality.	This	function	is
called	a	
memory	mountain
[
112
].
Every	computer	has	a	unique	memory	mountain	that	characterizes	the
capabilities	of	its	memory	system.	For	example,	
Figure	
6.41
shows
the	memory	mountain	for	an	Intel	Core	i7	Haswell	system.	In	this
example,	the	
varies	from	16	KB	to	128	MB,	and	the	
varies
from	1	to	12	elements,	where	each	element	is	an	8-byte</h2>
<p>code/mem/mountain/mountain.c
6
20</p>
<hr />
<p>code/mem/mountain/mountain.c
Figure	
6.40	
Functions	that	measure	and	compute	read	throughput.
We	can	generate	a	memory	mountain	for	a	particular	computer	by	calling
the	
function	with	different	values	of	
(which	corresponds	to
temporal	locality)	and	
(which	corresponds	to	spatial	locality).</p>
<p>Figure	
6.41	
A	memory	mountain.
Shows	read	throughput	as	a	function	of	temporal	and	spatial	locality.
The	geography	of	the	Core	i7	mountain	reveals	a	rich	structure.
Perpendicular	to	the	
axis	are	four	
ridges
that	correspond	to	the
regions	of	temporal	locality	where	the	working	set	fits	entirely	in	the	L1
cache,	L2	cache,	L3	cache,	and	main	memory,	respectively.	Notice	that
there	is	more	than	an	order	of	magnitude	difference	between	the	highest
peak	of	the	L1	ridge,	where	the	CPU	reads	at	a	rate	of	over	14	GB/s,	and
the	lowest	point	of	the	main	memory	ridge,	where	the	CPU	reads	at	a
rate	of	900	MB/s.
On	each	of	the	L2,	L3,	and	main	memory	ridges,	there	is	a	slope	of
spatial	locality	that	falls	downhill	as	the	stride	increases	and	spatial
locality	decreases.	Notice	that	even	when	the	working	set	is	too	large	to</p>
<p>fit	in	any	of	the	caches,	the	highest	point	on	the	main	memory	ridge	is	a
factor	of	8	higher	than	its	lowest	point.	So	even	when	a	program	has	poor
temporal	locality,	spatial	locality	can	still	come	to	the	rescue	and	make	a
significant	difference.
There	is	a	particularly	interesting	flat	ridge	line	that	extends	perpendicular
to	the	stride	axis	for	a	stride	of	1,	where	the	read	throughput	is	a
relatively	flat	12	GB/s,	even	though	the	working	set	exceeds	the
capacities	of	L1	and	L2.	This	is	apparently	due	to	a	hardware	
prefetching
mechanism	in	the	Core	i7	memory	system	that	automatically	identifies
sequential	stride-1	reference	patterns	and	attempts	to	fetch	those	blocks
into	the	cache	before	they	are	accessed.	While	the
Figure	
6.42	
Ridges	of	temporal	locality	in	the	memory	mountain.
The	graph	shows	a	slice	through	
Figure	
6.41
with	
=	8.
details	of	the	particular	prefetching	algorithm	are	not	documented,	it	is
clear	from	the	memory	mountain	that	the	algorithm	works	best	for	small</p>
<p>strides—yet	another	reason	to	favor	sequential	stride-1	accesses	in	your
code.
If	we	take	a	slice	through	the	mountain,	holding	the	stride	constant	as	in
Figure	
6.42
,	we	can	see	the	impact	of	cache	size	and	temporal	locality
on	performance.	For	sizes	up	to	32	KB,	the	working	set	fits	entirely	in	the
L1	d-cache,	and	thus	reads	are	served	from	L1	at	throughput	of	about	12
GB/s.	For	sizes	up	to	256	KB,	the	working	set	fits	entirely	in	the	unified
L2	cache,	and	for	sizes	up	to	8	MB,	the	working	set	fits	entirely	in	the
unified	L3	cache.	Larger	working	set	sizes	are	served	primarily	from	main
memory.
The	dips	in	read	throughputs	at	the	leftmost	edges	of	the	L2	and	L3
cache	regions—where	the	working	set	sizes	of	256	KB	and	8	MB	are
equal	to	their	respective	cache	sizes—are	interesting.	It	is	not	entirely
clear	why	these	dips	occur.	The	only	way	to	be	sure	is	to	perform	a
detailed	cache	simulation,	but	it	is	likely	that	the	drops	are	caused	by
conflicts	with	other	code	and	data	lines.
Slicing	through	the	memory	mountain	in	the	opposite	direction,	holding
the	working	set	size	constant,	gives	us	some	insight	into	the	impact	of
spatial	locality	on	the	read	throughput.	For	example,	
Figure	
6.43
shows	the	slice	for	a	fixed	working	set	size	of	4	MB.	This	slice	cuts	along
the	L3	ridge	in	
Figure	
6.41
,	where	the	working	set	fits	entirely	in	the	L3
cache	but	is	too	large	for	the	L2	cache.
Notice	how	the	read	throughput	decreases	steadily	as	the	stride
increases	from	one	to	eight	words.	In	this	region	of	the	mountain,	a	read
miss	in	L2	causes	a	block	to	be	transferred	from	L3	to	L2.	This	is
followed	by	some	number	of	hits</p>
<p>Figure	
6.43	
A	slope	of	spatial	locality.
The	graph	shows	a	slice	through	
Figure	
6.41
with	
=	4	MB.
on	the	block	in	L2,	depending	on	the	stride.	As	the	stride	increases,	the
ratio	of	L2	misses	to	L2	hits	increases.	Since	misses	are	served	more
slowly	than	hits,	the	read	throughput	decreases.	Once	the	stride	reaches
eight	8-byte	words,	which	on	this	system	equals	the	block	size	of	64
bytes,	every	read	request	misses	in	L2	and	must	be	served	from	L3.
Thus,	the	read	throughput	for	strides	of	at	least	eight	is	a	constant	rate
determined	by	the	rate	that	cache	blocks	can	be	transferred	from	L3	into
L2.
To	summarize	our	discussion	of	the	memory	mountain,	the	performance
of	the	memory	system	is	not	characterized	by	a	single	number.	Instead,	it
is	a	mountain	of	temporal	and	spatial	locality	whose	elevations	can	vary
by	over	an	order	of	magnitude.	Wise	programmers	try	to	structure	their
programs	so	that	they	run	in	the	peaks	instead	of	the	valleys.	The	aim	is
to	exploit	temporal	locality	so	that	heavily	used	words	are	fetched	from</p>
<p>the	L1	cache,	and	to	exploit	spatial	locality	so	that	as	many	words	as
possible	are	accessed	from	a	single	L1	cache	line.
Practice	Problem	
6.21	
(solution	page	
666
)
Use	the	memory	mountain	in	
Figure	
6.41
to	estimate	the	time,
in	CPU	cycles,	to	read	an	8-byte	word	from	the	L1	d-cache.
6.6.2	
Rearranging	Loops	to	Increase
Spatial	Locality
Consider	the	problem	of	multiplying	a	pair	of	
n
×	
n
matrices:	
C
=	
AB
.	For
example,	if	
n
=	2,	then
where
A	matrix	multiply	function	is	usually	implemented	using	three	nested
loops,	which	are	identified	by	their	indices	
i,	j
,	and	
k.
If	we	permute	the
loops	and	make	some	other	minor	code	changes,	we	can	create	the	six
functionally	equivalent	versions	of	matrix	multiply	shown	in	
Figure
6.44
.	Each	version	is	uniquely	identified	by	the	ordering	of	its	loops.
[</p>
<p>c
11
c
12
c
21
c
22</p>
<h1 id=""><a class="header" href="#">]</a></h1>
<p>[</p>
<p>a
11
a
12
a
21
a
22</p>
<p>]
 
[</p>
<p>b
11
b
12
b
21
b
22</p>
<h1>]
c
11</h1>
<p>a
11
b
11</p>
<ul>
<li></li>
</ul>
<h1>a
12
b
21
c
12</h1>
<p>a
11
b
12</p>
<ul>
<li></li>
</ul>
<h1>a
12
b
22
c
21</h1>
<p>a
21
b
11</p>
<ul>
<li></li>
</ul>
<h1>a
22
b
21
c
22</h1>
<p>a
21
b
12</p>
<p>At	a	high	level,	the	six	versions	are	quite	similar.	If	addition	is	associative,
then	each	version	computes	an	identical	result.
Each	version	performs
O
(
n
)	total	operations	and	an	identical	number	of	adds	and	multiplies.
Each	of	the	
n
elements	of	
A
and	
B
is	read	
n
times.	Each	of	the	
n
elements	of	
C
is	computed	by	summing	
n
values.	However,	if	we	analyze
the	behavior	of	the	innermost	loop	iterations,	we	find	that	there	are
differences	in	the	number	of	accesses	and	the	locality.	For	the	purposes
of	this	analysis,	we	make	the	following	assumptions:</p>
<ol>
<li></li>
</ol>
<p>As	we	learned	in	
Chapter	
2
,	floating-point	addition	is	commutative,	but	in	general	not
associative.	In	practice,	if	the	matrices	do	not	mix	extremely	large	values	with	extremely	small
ones,	as	often	is	true	when	the	matrices	store	physical	properties,	then	the	assumption	of
associativity	is	reasonable.
Each	array	is	an	
n
×	
n
array	of	
,	with	
.
There	is	a	single	cache	with	a	32-byte	block	size	(
B
=	32).
The	array	size	
n
is	so	large	that	a	single	matrix	row	does	not	fit	in	the
L1	cache.
The	compiler	stores	local	variables	in	registers,	and	thus	references
to	local	variables	inside	loops	do	not	require	any	load	or	store
instructions.
Figure	
6.45
summarizes	the	results	of	our	inner-loop	analysis.	Notice
that	the	six	versions	pair	up	into	three	equivalence	classes,	which	we
denote	by	the	pair	of	matrices	that	are	accessed	in	the	inner	loop.	For
example,	versions	
ijk
and	
jik
are	members	of	class	
AB
because	they
reference	arrays	
A
and	
B
(but	not	
C
)	in	their	innermost	loop.	For	each
class,	we	have	counted	the	number	of	loads	(reads)	and	stores	(writes)	in
each	inner-loop	iteration,	the	number	of	references	to	
A,	B
,	and	
C
that
1
3
2
2</p>
<h2>will	miss	in	the	cache	in	each	loop	iteration,	and	the	total	number	of
cache	misses	per	iteration.
The	inner	loops	of	the	class	
AB
routines	(
Figure	
6.44(a)
and	
(b)
)
scan	a	row	of	array	
A
with	a	stride	of	1.	Since	each	cache	block	holds
four	8-byte	words,	the	miss	rate	for	
A
is	0.25	misses	per	iteration.	On	the
other	hand,	the	inner	loop	scans	a	column	of	
B
with	a	stride	of	
n.
Since	
n
is	large,	each	access	of	array	
B
results	in	a	miss,	for	a	total	of	1.25
misses	per	iteration.
The	inner	loops	in	the	class	
AC
routines	(
Figure	
6.44(c)
and	
(d)
)
have	some	problems.	Each	iteration	performs	two	loads	and	a	store	(as
opposed	to	the
(a)	Version	
i	j	k</h2>
<h2 id="codememmatmultmmc"><a class="header" href="#codememmatmultmmc">code/mem/matmult/mm.c</a></h2>
<p>code/mem/matmult/mm.c</p>
<h2>(b)	Version	
jik</h2>
<h2 id="codememmatmultmmc-1"><a class="header" href="#codememmatmultmmc-1">code/mem/matmult/mm.c</a></h2>
<h2>code/mem/matmult/mm.c
(c)	Version	
jki</h2>
<p>code/mem/matmult/mm.c</p>
<hr />
<h2>code/mem/matmult/mm.c
(d)	Version	
kji</h2>
<h2 id="codememmatmultmmc-2"><a class="header" href="#codememmatmultmmc-2">code/mem/matmult/mm.c</a></h2>
<h2>code/mem/matmult/mm.c
(e)	Version	
kij</h2>
<p>code/mem/matmult/mm.c</p>
<hr />
<h2>code/mem/matmult/mm.c
(f)	Version	
ikj</h2>
<h2 id="codememmatmultmmc-3"><a class="header" href="#codememmatmultmmc-3">code/mem/matmult/mm.c</a></h2>
<p>code/mem/matmult/mm.c
Figure	
6.44	
Six	versions	of	matrix	multiply.
Each	version	is	uniquely	identified	by	the	ordering	of	its	loops.
Matrix	multiply	version
(class)
Per	iteration
Loads
Stores
A
misses
B
misses
C
misses
Total
misses
ijk
&amp;	
jik
(
AB
)
2
0
0.25
1.00
0.00
1.25
jki
&amp;	
kji
(
AC
)
2
1
1.00
0.00
1.00
2.00
kij
&amp;	
ikj
(
BC
)
2
1
0.00
0.25
0.25
0.50</p>
<p>Figure	
6.45	
Analysis	of	matrix	multiply	inner	loops.
The	six	versions	partition	into	three	equivalence	classes,	denoted	by	the
pair	of	arrays	that	are	accessed	in	the	inner	loop.
Figure	
6.46	
Core	i7	matrix	multiply	performance.
class	
AB
routines,	which	perform	two	loads	and	no	stores).	Second,	the
inner	loop	scans	the	columns	of	
A
and	
C
with	a	stride	of	
n
.	The	result	is	a
miss	on	each	load,	for	a	total	of	two	misses	per	iteration.	Notice	that
interchanging	the	loops	has	decreased	the	amount	of	spatial	locality
compared	to	the	class	
AB
routines.
The	
BC
routines	(
Figure	
6.44(e)
and	
(f)
)	present	an	interesting
trade-off:	With	two	loads	and	a	store,	they	require	one	more	memory
operation	than	the	
AB
routines.	On	the	other	hand,	since	the	inner	loop
scans	both	
B
and	
C
row-wise	with	a	stride-1	access	pattern,	the	miss	rate
on	each	array	is	only	0.25	misses	per	iteration,	for	a	total	of	0.50	misses
per	iteration.</p>
<p>Figure	
6.46
summarizes	the	performance	of	different	versions	of
matrix	multiply	on	a	Core	i7	system.	The	graph	plots	the	measured
number	of	CPU	cycles	per	inner-loop	iteration	as	a	function	of	array	size
(
n
).
There	are	a	number	of	interesting	points	to	notice	about	this	graph:
For	large	values	of	
n
,	the	fastest	version	runs	almost	40	times	faster
than	the	slowest	version,	even	though	each	performs	the	same
number	of	floating-point	arithmetic	operations.
Pairs	of	versions	with	the	same	number	of	memory	references	and
misses	per	iteration	have	almost	identical	measured	performance.
The	two	versions	with	the	worst	memory	behavior,	in	terms	of	the
number	of	accesses	and	misses	per	iteration,	run	significantly	slower
than	the	other	four	versions,	which	have	fewer	misses	or	fewer
accesses,	or	both.
Miss	rate,	in	this	case,	is	a	better	predictor	of	performance	than	the
total	number	of	memory	accesses.	For	example,	the	class	
BC
routines,	with	0.5	misses	per	iteration,	perform	much	better	than	the
class	
AB
routines,	with	1.25	misses	per	iteration,	even	though	the
class	
BC
routines	perform	more
Web	Aside	MEM:BLOCKING	
Using
blocking	to	increase	temporal	locality
There	is	an	interesting	technique	called	
blocking
that	can
improve	the	temporal	locality	of	inner	loops.	The	general	idea
of	blocking	is	to	organize	the	data	structures	in	a	program	into
large	chunks	called	
blocks.
(In	this	context,	&quot;block&quot;	refers	to	an</p>
<p>application-level	chunk	of	data,	
not
to	a	cache	block.)	The
program	is	structured	so	that	it	loads	a	chunk	into	the	L1
cache,	does	all	the	reads	and	writes	that	it	needs	to	on	that
chunk,	then	discards	the	chunk,	loads	in	the	next	chunk,	and
so	on.
Unlike	the	simple	loop	transformations	for	improving	spatial
locality,	blocking	makes	the	code	harder	to	read	and
understand.	For	this	reason,	it	is	best	suited	for	optimizing
compilers	or	frequently	executed	library	routines.	Blocking
does	not	improve	the	performance	of	matrix	multiply	on	the
Core	i7,	because	of	its	sophisticated	prefetching	hardware.
Still,	the	technique	is	interesting	to	study	and	understand
because	it	is	a	general	concept	that	can	produce	big
performance	gains	on	systems	that	don't	prefetch.
memory	references	in	the	inner	loop	(two	loads	and	one	store)	than
the	class	
AB
routines	(two	loads).
For	large	values	of	
n
,	the	performance	of	the	fastest	pair	of	versions
(
kij
and	
ikj
)	is	constant.	Even	though	the	array	is	much	larger	than	any
of	the	SRAM	cache	memories,	the	prefetching	hardware	is	smart
enough	to	recognize	the	stride-1	access	pattern,	and	fast	enough	to
keep	up	with	memory	accesses	in	the	tight	inner	loop.	This	is	a
stunning	accomplishment	by	the	Intel	engineers	who	designed	this
memory	system,	providing	even	more	incentive	for	programmers	to
develop	programs	with	good	spatial	locality.
6.6.3	
Exploiting	Locality	in	Your</p>
<p>Programs
As	we	have	seen,	the	memory	system	is	organized	as	a	hierarchy	of
storage	devices,	with	smaller,	faster	devices	toward	the	top	and	larger,
slower	devices	toward	the	bottom.	Because	of	this	hierarchy,	the	effective
rate	that	a	program	can	access	memory	locations	is	not	characterized	by
a	single	number.	Rather,	it	is	a	wildly	varying	function	of	program	locality
(what	we	have	dubbed	the	memory	mountain)	that	can	vary	by	orders	of
magnitude.	Programs	with	good	locality	access	most	of	their	data	from
fast	cache	memories.	Programs	with	poor	locality	access	most	of	their
data	from	the	relatively	slow	DRAM	main	memory.
Programmers	who	understand	the	nature	of	the	memory	hierarchy	can
exploit	this	understanding	to	write	more	efficient	programs,	regardless	of
the	specific	memory	system	organization.	In	particular,	we	recommend
the	following	techniques:
Focus	your	attention	on	the	inner	loops,	where	the	bulk	of	the
computations	and	memory	accesses	occur.
Try	to	maximize	the	spatial	locality	in	your	programs	by	reading	data
objects	sequentially,	with	stride	1,	in	the	order	they	are	stored	in
memory.
Try	to	maximize	the	temporal	locality	in	your	programs	by	using	a
data	object	as	often	as	possible	once	it	has	been	read	from	memory.</p>
<p>6.7	
Summary
The	basic	storage	technologies	are	random	access	memories	(RAMs),
nonvolatile	memories	(ROMs),	and	disks.	RAM	comes	in	two	basic
forms.	Static	RAM	(SRAM)	is	faster	and	more	expensive	and	is	used	for
cache	memories.	Dynamic	RAM	(DRAM)	is	slower	and	less	expensive
and	is	used	for	the	main	memory	and	graphics	frame	buffers.	ROMs
retain	their	information	even	if	the	supply	voltage	is	turned	off.	They	are
used	to	store	firmware.	Rotating	disks	are	mechanical	nonvolatile	storage
devices	that	hold	enormous	amounts	of	data	at	a	low	cost	per	bit,	but
with	much	longer	access	times	than	DRAM.	Solid	state	disks	(SSDs)
based	on	nonvolatile	flash	memory	are	becoming	increasingly	attractive
alternatives	to	rotating	disks	for	some	applications.
In	general,	faster	storage	technologies	are	more	expensive	per	bit	and
have	smaller	capacities.	The	price	and	performance	properties	of	these
technologies	are	changing	at	dramatically	different	rates.	In	particular,
DRAM	and	disk	access	times	are	much	larger	than	CPU	cycle	times.
Systems	bridge	these	gaps	by	organizing	memory	as	a	hierarchy	of
storage	devices,	with	smaller,	faster	devices	at	the	top	and	larger,	slower
devices	at	the	bottom.	Because	well-written	programs	have	good	locality,
most	data	are	served	from	the	higher	levels,	and	the	effect	is	a	memory
system	that	runs	at	the	rate	of	the	higher	levels,	but	at	the	cost	and
capacity	of	the	lower	levels.
Programmers	can	dramatically	improve	the	running	times	of	their
programs	by	writing	programs	with	good	spatial	and	temporal	locality.</p>
<p>Exploiting	SRAM-based	cache	memories	is	especially	important.
Programs	that	fetch	data	primarily	from	cache	memories	can	run	much
faster	than	programs	that	fetch	data	primarily	from	memory.</p>
<p>Bibliographic	Notes
Memory	and	disk	technologies	change	rapidly.	In	our	experience,	the
best	sources	of	technical	information	are	the	Web	pages	maintained	by
the	manufacturers.	Companies	such	as	Micron,	Toshiba,	and	Samsung
provide	a	wealth	of	current	technical	information	on	memory	devices.	The
pages	for	Seagate	and	Western	Digital	provide	similarly	useful
information	about	disks.
Textbooks	on	circuit	and	logic	design	provide	detailed	information	about
memory	technology	[58,	89].	
IEEE	Spectrum
published	a	series	of	survey
articles	on	DRAM	[55].	The	International	Symposiums	on	Computer
Architecture	(ISCA)	and	High	Performance	Computer	Architecture
(HPCA)	are	common	forums	for	characterizations	of	DRAM	memory
performance	[
28
,	
29
,	
18
].
Wilkes	wrote	the	first	paper	on	cache	memories	[
117
].	Smith	wrote	a
classic	survey	[
104
].	Przybylski	wrote	an	authoritative	book	on	cache
design	[
86
].	Hennessy	and	Patterson	provide	a	comprehensive
discussion	of	cache	design	issues	[
46
].	Levinthal	wrote	a	comprehensive
performance	guide	for	the	Intel	Core	i7	[
70
].
Stricker	introduced	the	idea	of	the	memory	mountain	as	a	comprehensive
characterization	of	the	memory	system	in	[
112
]	and	suggested	the	term
&quot;memory	mountain&quot;	informally	in	later	presentations	of	the	work.
Compiler	researchers	
work	to	increase	locality	by	automatically
performing	the	kinds	of	manual	code	transformations	we	discussed	in</p>
<p>Section	
6.6
[
22
,	
32
,	
66
,	
72
,	
79
,	
87
,	
119
].	Carter	and	colleagues	have
proposed	a	cache-aware	memory	controller	[
17
].	Other	researchers	have
developed	
cache-oblivious
algorithms	that	are	designed	to	run	well
without	any	explicit	knowledge	of	the	structure	of	the	underlying	cache
memory	[
30
,	
38
,	
39
,	
9
].
There	is	a	large	body	of	literature	on	building	and	using	disk	storage.
Many	storage	researchers	look	for	ways	to	aggregate	individual	disks	into
larger,	more	robust,	and	more	secure	storage	pools	[
20
,	
40
,	
41
,	
83
,	
121
].
Others	look	for	ways	to	use	caches	and	locality	to	improve	the
performance	of	disk	accesses	[
12
,	
21
].	Systems	such	as	Exokernel
provide	increased	user-level	control	of	disk	and	memory	resources	[
57
].
Systems	such	as	the	Andrew	File	System	[
78
]	and	Coda	[
94
]	extend	the
memory	hierarchy	across	computer	networks	and	mobile	notebook
computers.	Schindler	and	Ganger	developed	an	interesting	tool	that
automatically	characterizes	the	geometry	and	performance	of	SCSI	disk
drives	[
95
].	Researchers	have	investigated	techniques	for	building	and
using	flash-based	SSDs	[
8
,	
81
].</p>
<p>Homework	Problems
6.22
Suppose	you	are	asked	to	design	a	rotating	disk	where	the	number	of
bits	per	track	is	constant.	You	know	that	the	number	of	bits	per	track	is
determined	by	the	circumference	of	the	innermost	track,	which	you	can
assume	is	also	the	circumference	of	the	hole.	Thus,	if	you	make	the	hole
in	the	center	of	the	disk	larger,	the	number	of	bits	per	track	increases,	but
the	total	number	of	tracks	decreases.	If	you	let	
r
denote	the	radius	of	the
platter,	and	
x
·	
r
the	radius	of	the	hole,	what	value	of	
x
maximizes	the
capacity	of	the	disk?
6.23
Estimate	the	average	time	(in	ms)	to	access	a	sector	on	the	following
disk:
Parameter
Value
Rotational	rate
15,000	RPM
T
4	ms
Average	number	of	sectors/track
800
avg	seek</p>
<p>6.24
Suppose	that	a	2	MB	file	consisting	of	512-byte	logical	blocks	is	stored
on	a	disk	drive	with	the	following	characteristics:
Parameter
Value
Rotational	rate
15,000	RPM
T
4	ms
Average	number	of	sectors/track
1,000
Surfaces
8
Sector	size
512	bytes
For	each	case	below,	suppose	that	a	program	reads	the	logical	blocks	of
the	file	sequentially,	one	after	the	other,	and	that	the	time	to	position	the
head	over	the	first	block	is	
T
+	
T
.
A
.	
Best	case:
Estimate	the	optimal	time	(in	ms)	required	to	read	the
file	over	all	possible	mappings	of	logical	blocks	to	disk	sectors.
B
.	
Random	case:
Estimate	the	time	(in	ms)	required	to	read	the	file	if
blocks	are	mapped	randomly	to	disk	sectors.
6.25
avg	seek
avg	seek
avg	rotation</p>
<p>The	following	table	gives	the	parameters	for	a	number	of	different
caches.	For	each	cache,	fill	in	the	missing	fields	in	the	table.	Recall	that
m
is	the	number	of	physical	address	bits,	
C
is	the	cache	size	(number	of
data	bytes),	
B
is	the	block	size	in	bytes,	
E
is	the	associativity,	
S
is	the
number	of	cache	sets,	
t
is	the	number	of	tag	bits,	
s
is	the	number	of	set
index	bits,	and	
b
is	the	number	of	block	offset	bits.
Cache
m
C
B
E
S
t
s
b</p>
<ol>
<li></li>
</ol>
<p>32
1,024
4
4</p>
<hr />
<hr />
<hr />
<hr />
<ol start="2">
<li></li>
</ol>
<p>32
1,024
4
256</p>
<hr />
<hr />
<hr />
<hr />
<ol start="3">
<li></li>
</ol>
<p>32
1,024
8
1</p>
<hr />
<hr />
<hr />
<hr />
<ol start="4">
<li></li>
</ol>
<p>32
1,024
8
128</p>
<hr />
<hr />
<hr />
<hr />
<ol start="5">
<li></li>
</ol>
<p>32
1,024
32
1</p>
<hr />
<hr />
<hr />
<hr />
<ol start="6">
<li></li>
</ol>
<p>32
1,024
32
4</p>
<hr />
<hr />
<hr />
<hr />
<p>6.26
The	following	table	gives	the	parameters	for	a	number	of	different
caches.	Your	task	is	to	fill	in	the	missing	fields	in	the	table.	Recall	that	
m
is	the	number	of	physical	address	bits,	
C
is	the	cache	size	(number	of
data	bytes),	
B
is	the	block	size	in	bytes,	
E
is	the	associativity,	
S
is	the
number	of	cache	sets,	
t
is	the	number	of	tag	bits,	
s
is	the	number	of	set
index	bits,	and	
b
is	the	number	of	block	offset	bits.</p>
<p>Cache
m
C
B
E
S
t
s
b</p>
<ol>
<li></li>
</ol>
<p>32</p>
<hr />
<p>8
1</p>
<hr />
<p>21
8
3
2.
32
2,048</p>
<hr />
<hr />
<p>128
23
7
2
3.
32
1,024
2
8
64</p>
<hr />
<hr />
<p>1
4.
32
1,024</p>
<hr />
<p>2
16
23
4</p>
<hr />
<p>6.27
This	problem	concerns	the	cache	in	
Practice	Problem	
6.12
.
A
.	
List	all	of	the	hex	memory	addresses	that	will	hit	in	set	1.
B
.	
List	all	of	the	hex	memory	addresses	that	will	hit	in	set	6.
6.28
This	problem	concerns	the	cache	in	
Practice	Problem	
6.12
.
A
.	
List	all	of	the	hex	memory	addresses	that	will	hit	in	set	2.
B
.	
List	all	of	the	hex	memory	addresses	that	will	hit	in	set	4.
C
.	
List	all	of	the	hex	memory	addresses	that	will	hit	in	set	5.
D
.	
List	all	of	the	hex	memory	addresses	that	will	hit	in	set	7.</p>
<p>6.29
Suppose	we	have	a	system	with	the	following	properties:
The	memory	is	byte	addressable.
Memory	accesses	are	to	1-byte	words	(not	to	4-byte	words).
Addresses	are	12	bits	wide.
The	cache	is	two-way	set	associative	(
E
=	2),	with	a	4-byte	block	size
(
B
=	4)	and	four	sets	(
S
=	4).
The	contents	of	the	cache	are	as	follows,	with	all	addresses,	tags,	and
values	given	in	hexadecimal	notation:
Set	index
Tag
Valid
Byte	0
Byte	1
Byte	2
Byte	3
0
00
1
40
41
42
43
83
1
FE
97
CC
D0
1
00
1
44
45
46
47
83
0
—
—
—
—
2
00
1
48
49
4A
4B
40
0
—
—
—
—
3
FF
1
9A
C0
03
FF
00
0
—
—
—
—</p>
<p>A
.	
The	following	diagram	shows	the	format	of	an	address	(1	bit	per
box).	Indicate	(by	labeling	the	diagram)	the	fields	that	would	be
used	to	determine	the	following:
CO.	The	cache	block	offset
CI.	The	cache	set	index
CT.	The	cache	tag
B
.	
For	each	of	the	following	memory	accesses,	indicate	if	it	will	be	a
cache	hit	or	miss	when	
carried	out	in	sequence
as	listed.	Also	give
the	value	of	a	read	if	it	can	be	inferred	from	the	information	in	the
cache.
Operation
Address
Hit?
Read	value	(or	unknown)
Read
0x834</p>
<hr />
<hr />
<p>Write
0x836</p>
<hr />
<hr />
<p>Read
0xFFD</p>
<hr />
<hr />
<p>6.30
Suppose	we	have	a	system	with	the	following	properties:
The	memory	is	byte	addressable.
Memory	accesses	are	to	1-byte	words	(not	to	4-byte	words).</p>
<p>Addresses	are	13	bits	wide.
The	cache	is	4-way	set	associative	(
E
=	4),	with	a	4-byte	block	size	(
B
=	4)	and	eight	sets	(
S
=	8).
Consider	the	following	cache	state.	All	addresses,	tags,	and	values	are
given	in	hexadecimal	format.	The	Index	column	contains	the	set	index	for
each	set	of	four	lines.	The	Tag	columns	contain	the	tag	value	for	each
line.	The	V	columns	contain	the	valid	bit	for	each	line.	The	Bytes	0−3
columns	contain	the	data	for	each	line,	numbered	left	to	right	starting	with
byte	0	on	the	left.
4-way	set	associative	cache
Index
Tag
V
Bytes
0−3
Tag
V
Bytes
0−3
Tag
V
Bytes
0−3
Tag
V
Bytes
0−3
0
F0
1
ED
32	0A
A2
8A
1
BF	80
1D
FC
14
1
EF	09
86	2A
BC
0
25	44
6F	1A
1
BC
0
03	3E
CD
38
A0
0
16	7B
ED
5A
BC
1
8E	4C
DF	18
E4
1
FB
B7	12
02
2
BC
1
54	9E
1E	FA
B6
1
DC
81	B2
14
00
0
B6	1F
7B	44
74
0
10	F5
B8	2E
3
BE
0
2F	7E
3D
A8
C0
1
27	95
A4	74
C4
0
07	11
6B	D8
BC
0
C7
B7
AF
C2</p>
<p>4
7E
1
32	21
1C
2C
8A
1
22	C2
DC
34
BC
1
BA
DD
37	D8
DC
0
E7	A2
39	BA
5
98
0
A9	76
2B
EE
54
0
BC	91
D5	92
98
1
80	BA
9B	F6
BC
1
48	16
81	0A
6
38
0
5D
4D	F7
DA
BC
1
69	C2
8C	74
8A
1
A8
CE
7F
DA
38
1
FA	93
EB	48
7
8A
1
04	2A
32	6A
9E
0
B1	86
56	0E
CC
1
96	30
47	F2
BC
1
F8	1D
42	30
A
.	
What	is	the	size	(
C
)	of	this	cache	in	bytes?
B
.	
The	box	that	follows	shows	the	format	of	an	address	(1	bit	per
box).	Indicate	(by	labeling	the	diagram)	the	fields	that	would	be
used	to	determine	the	following:
CO.	The	cache	block	offset
CI.	The	cache	set	index
CT.	The	cache	tag
6.31</p>
<p>Suppose	that	a	program	using	the	cache	in	
Problem	
6.30
references
the	1-byte	word	at	address	
.	Indicate	the	cache	entry	accessed
and	the	cache	byte	value	returned	
in	hex
.	Indicate	whether	a	cache	miss
occurs.	If	there	is	a	cache	miss,	enter	&quot;—&quot;	for	&quot;Cache	byte	returned.&quot;
Hint:
Pay	attention	to	those	valid	bits!
A
.	
Address	format	(1	bit	per	box):
B
.	
Memory	reference:
Parameter
Value
Block	offset	(CO)
0x_____
Index	(CI)
0x_____
Cache	tag	(CT)
0x_____
Cache	hit?	(Y/N)</p>
<hr />
<p>Cache	byte	returned
0x_____
6.32
Repeat	
Problem	
6.31
for	memory	address	
.
A
.	
Address	format	(1	bit	per	box):</p>
<p>B
.	
Memory	reference:
Parameter
Value
Cache	offset	(CO)
0x_____
Cache	index	(CI)
0x_____
Cache	tag	(CT)
0x_____
Cache	hit?	(Y/N)</p>
<hr />
<p>Cache	byte	returned
0x_____
6.33
For	the	cache	in	
Problem	
6.30
,	list	the	eight	memory	addresses	(in
hex)	that	will	hit	in	set	2.
6.34
Consider	the	following	matrix	transpose	routine:</p>
<p>Assume	this	code	runs	on	a	machine	with	the	following	properties:
=	4.
The	
array	starts	at	address	0	and	the	
array	starts	at	address
64	(decimal).
There	is	a	single	L1	data	cache	that	is	direct-mapped,	write-through,
write-allocate,	with	a	block	size	of	16	bytes.
The	cache	has	a	total	size	of	32	data	bytes,	and	the	cache	is	initially
empty.
Accesses	to	the	
and	
arrays	are	the	only	sources	of	read	and
write	misses,	respectively.
A
.	
For	each	row	and	
,	indicate	whether	the	access	to	
and	
is	a	hit	(h)	or	a	miss	(m).	For	example,
reading	
is	a	miss	and	writing	
is	also	a	miss.
Col.	0
Col.	1
Col.	2
Col.	3
Col.	0
Col.	1
Col.	2
Col.	3
Row
m</p>
<hr />
<hr />
<hr />
<p>Row
m</p>
<hr />
<hr />
<hr />
<p>0
0
Row
1</p>
<hr />
<hr />
<hr />
<hr />
<p>Row
1</p>
<hr />
<hr />
<hr />
<hr />
<p>Row
2</p>
<hr />
<hr />
<hr />
<hr />
<p>Row
2</p>
<hr />
<hr />
<hr />
<hr />
<p>Row
3</p>
<hr />
<hr />
<hr />
<hr />
<p>Row
3</p>
<hr />
<hr />
<hr />
<hr />
<p>6.35
Repeat	
Problem	
6.34
for	a	cache	with	a	total	size	of	128	data	bytes.
Col.	0
Col.	1
Col.	2
Col.	3
Col.	0
Col.	1
Col.	2
Col.	3
Row
0</p>
<hr />
<hr />
<hr />
<hr />
<p>Row
0</p>
<hr />
<hr />
<hr />
<hr />
<p>Row
1</p>
<hr />
<hr />
<hr />
<hr />
<p>Row
1</p>
<hr />
<hr />
<hr />
<hr />
<p>Row
2</p>
<hr />
<hr />
<hr />
<hr />
<p>Row
2</p>
<hr />
<hr />
<hr />
<hr />
<p>Row
3</p>
<hr />
<hr />
<hr />
<hr />
<p>Row
3</p>
<hr />
<hr />
<hr />
<hr />
<p>6.36
This	problem	tests	your	ability	to	predict	the	cache	behavior	of	C	code.
You	are	given	the	following	code	to	analyze:
Assume	we	execute	this	under	the	following	conditions:
=	4.
Array	
begins	at	memory	address	
and	is	stored	in	row-major
order.
In	each	case	below,	the	cache	is	initially	empty.
The	only	memory	accesses	are	to	the	entries	of	the	array	
.	All	other
variables	are	stored	in	registers.
Given	these	assumptions,	estimate	the	miss	rates	for	the	following	cases:</p>
<p>A
.	
Case	1:	Assume	the	cache	is	512	bytes,	direct-mapped,	with	16-
byte	cache	blocks.	What	is	the	miss	rate?
B
.	
Case	2:	What	is	the	miss	rate	if	we	double	the	cache	size	to	1,024
bytes?
C
.	
Case	3:	Now	assume	the	cache	is	512	bytes,	two-way	set
associative	using	an	LRU	replacement	policy,	with	16-byte	cache
blocks.	What	is	the	cache	miss	rate?
D
.	
For	case	3,	will	a	larger	cache	size	help	to	reduce	the	miss	rate?
Why	or	why	not?
E
.	
For	case	3,	will	a	larger	block	size	help	to	reduce	the	miss	rate?
Why	or	why	not?
6.37
This	is	another	problem	that	tests	your	ability	to	analyze	the	cache
behavior	of	C	code.	Assume	we	execute	the	three	summation	functions
in	
Figure	
6.47
under	the	following	conditions:
=	4.
The	machine	has	a	4	KB	direct-mapped	cache	with	a	16-byte	block
size.
Within	the	two	loops,	the	code	uses	memory	accesses	only	for	the
array	data.	The	loop	indices	and	the	value	
are	held	in	registers.
Array	a	is	stored	starting	at	memory	address	
.
Fill	in	the	table	for	the	approximate	cache	miss	rate	for	the	two	cases	
N
=
64	and	
N
=	60.</p>
<p>Function
N
=	64
N
=	60</p>
<hr />
<hr />
<hr />
<hr />
<hr />
<hr />
<p>Figure	
6.47	
Functions	referenced	in	
Problem	
6.37
.
6.38
3M	decides	to	make	Post-its	by	printing	yellow	squares	on	white	pieces
of	paper.	As	part	of	the	printing	process,	they	need	to	set	the	CMYK
(cyan,	magenta,	yellow,	black)	value	for	every	point	in	the	square.	3M
hires	you	to	determine	the	efficiency	of	the	following	algorithms	on	a
machine	with	a	2,048-byte	direct-mapped	data	cache	with	32-byte
blocks.	You	are	given	the	following	definitions:</p>
<p>Assume	the	following:
=	4.
begins	at	memory	address	0.
The	cache	is	initially	empty.
The	only	memory	accesses	are	to	the	entries	of	the	array	square.
Variables	
and	
are	stored	in	registers.
Determine	the	cache	performance	of	the	following	code:</p>
<p>A
.	
What	is	the	total	number	of	writes?
B
.	
What	is	the	total	number	of	writes	that	miss	in	the	cache?
C
.	
What	is	the	miss	rate?
6.39
Given	the	assumptions	in	
Problem	
6.38
,	determine	the	cache
performance	of	the	following	code:
A
.	
What	is	the	total	number	of	writes?
B
.	
What	is	the	total	number	of	writes	that	miss	in	the	cache?
C
.	
What	is	the	miss	rate?</p>
<p>6.40
Given	the	assumptions	in	
Problem	
6.38
,	determine	the	cache
performance	of	the	following	code:
A
.	
What	is	the	total	number	of	writes?
B
.	
What	is	the	total	number	of	writes	that	miss	in	the	cache?
C
.	
What	is	the	miss	rate?
6.41</p>
<p>You	are	writing	a	new	3D	game	that	you	hope	will	earn	you	fame	and
fortune.	You	are	currently	working	on	a	function	to	blank	the	screen	buffer
before	drawing	the	next	frame.	The	screen	you	are	working	with	is	a	640
×	480	array	of	pixels.	The	machine	you	are	working	on	has	a	64	KB
direct-mapped	cache	with	4-byte	lines.	The	C	structures	you	are	using
are	as	follows:
Assume	the	following:
=	1	and	
=	4.
begins	at	memory	address	0.
The	cache	is	initially	empty.
The	only	memory	accesses	are	to	the	entries	of	the	array	buffer.
Variables	
,	and	
are	stored	in	registers.</p>
<p>What	percentage	of	writes	in	the	following	code	will	miss	in	the	cache?
6.42
Given	the	assumptions	in	
Problem	
6.41
,	what	percentage	of	writes	in
the	following	code	will	miss	in	the	cache?
6.43</p>
<p>Given	the	assumptions	in	
Problem	
6.41
,	what	percentage	of	writes	in
the	following	code	will	miss	in	the	cache?
6.44
Download	the	
program	from	the	CS:APP	Web	site	and	run	it	on
your	favorite	PC/Linux	system.	Use	the	results	to	estimate	the	sizes	of
the	caches	on	your	system.
6.45
In	this	assignment,	you	will	apply	the	concepts	you	learned	in	
Chapters
5
and	
6
to	the	problem	of	optimizing	code	for	a	memory-intensive
application.	Consider	a	procedure	to	copy	and	transpose	the	elements	of
an	
N
×	
N
matrix	of	type	int.	That	is,	for	source	matrix	
S
and	destination
matrix	
D
,	we	want	to	copy	each	element	
s
to	
d
.	This	code	can	be
written	with	a	simple	loop,
i,j
j,i</p>
<p>where	the	arguments	to	the	procedure	are	pointers	to	the	destination
(
)	and	source	(
)	matrices,	as	well	as	the	matrix	size	
N
(
).	Your
job	is	to	devise	a	transpose	routine	that	runs	as	fast	as	possible.
6.46
This	assignment	is	an	intriguing	variation	of	
Problem	
6.45
.	Consider
the	problem	of	converting	a	directed	graph	
g
into	its	undirected
counterpart	
g′.
The	graph	
g′
has	an	edge	from	vertex	
u
to	vertex	
v
if	and
only	if	there	is	an	edge	from	
u
to	
v
or	from	
v
to	
u
in	the	original	graph	
g.
The	graph	
g
is	represented	by	its	
adjacency	matrix	G
as	follows.	If	
N
is
the	number	of	vertices	in	
g
,	then	
G
is	an	
N
×	
N
matrix	and	its	entries	are
all	either	0	or	1.	Suppose	the	vertices	of	
g
are	named	
v
,	
v
,	
v
,	...,	
v
.
Then	
G
[
i
][
j
]	is	1	if	there	is	an	edge	from	
v
to	
v
and	is	0	otherwise.
Observe	that	the	elements	on	the	diagonal	of	an	adjacency	matrix	are
always	1	and	that	the	adjacency	matrix	of	an	undirected	graph	is
symmetric.	This	code	can	be	written	with	a	simple	loop:
0
1
2
N
-1
i
j</p>
<p>Your	job	is	to	devise	a	conversion	routine	that	runs	as	fast	as	possible.
As	before,	you	will	need	to	apply	concepts	you	learned	in	
Chapters	
5
and	
6
to	come	up	with	a	good	solution.</p>
<p>Solutions	to	Practice	Problems
Solution	to	Problem	
6.1	
(page
584
)
The	idea	here	is	to	minimize	the	number	of	address	bits	by	minimizing
the	aspect	ratio	max(
r,	c
)/	min(
r,	c
).	In	other	words,	the	squarer	the	array,
the	fewer	the	address	bits.
Organization
r
c
b
b
max(
b
,	b
)
16	×	1
4
4
2
2
2
16	×	4
4
4
2
2
2
128	×	8
16
8
4
3
4
512	×	4
32
16
5
4
5
1,024	×	4
32
32
5
5
5
Solution	to	Problem	
6.2	
(page
592
)
r
c
r
c</p>
<h1>The	point	of	this	little	drill	is	to	make	sure	you	understand	the	relationship
between	cylinders	and	tracks.	Once	you	have	that	straight,	just	plug	and
chug:
Solution	to	Problem	
6.3	
(page
595
)
The	solution	to	this	problem	is	a	straightforward	application	of	the	formula
for	disk	access	time.	The	average	rotational	latency	(in	ms)	is
The	average	transfer	time	is
Putting	it	all	together,	the	total	estimated	access	time	is
Solution	to	Problem	
6.4	
(page
Disk
 
capacity</h1>
<h1>512
 
bytes
sector
×
400
 
sectors
track
×
10
,
000
 
tracks
surface
×
2
 
surfaces
T
avg
 
rotation</h1>
<h1>1
/
2
×
T
max
 
rotation</h1>
<h1>1
/
2
×
(
60
 
secs
/
15
,
000
 
RPM
)
×
1
,
000
 
ms/sec
T
avg
 
transfer</h1>
<h1>(
60
 
secs
/
15
,
000
 
RPM
)
×
1
/
500
 
sectors/track
×
1
,
000
 
ms/sec
≈
0.008
 
ms
T
access</h1>
<p>T
avg
 
seek</p>
<ul>
<li></li>
</ul>
<p>T
avg
 
rotation</p>
<ul>
<li></li>
</ul>
<h1>T
avg
 
transfer</h1>
<p>8
 
ms</p>
<ul>
<li></li>
</ul>
<p>2
 
ms</p>
<ul>
<li></li>
</ul>
<p>0.008
 
ms
≈
10</p>
<p>595
)
This	is	a	good	check	of	your	understanding	of	the	factors	that	affect	disk
performance.	First	we	need	to	determine	a	few	basic	properties	of	the	file
and	the	disk.	The	file	consists	of	2,000	512-byte	logical	blocks.	For	the
disk,	
T
=	5	ms,	
T
=	6	ms,	and	
T
=	3	ms.
A
.	
Best	case:	
In	the	optimal	case,	the	blocks	are	mapped	to
contiguous	sectors,	on	the	same	cylinder,	that	can	be	read	one
after	the	other	without	moving	the	head.	Once	the	head	is
positioned	over	the	first	sector	it	takes	two	full	rotations	(1,000
sectors	per	rotation)	of	the	disk	to	read	all	2,000	blocks.	So	the
total	time	to	read	the	file	is	
T
+	
T
+	2	×	
T
=	5	+
3	+	12	=	20	ms.
B
.	
Random	case:	
In	this	case,	where	blocks	are	mapped	randomly
to	sectors,	reading	each	of	the	2,000	blocks	requires	
T
+	
T
ms,	so	the	total	time	to	read	the	file	is	(
T
+	
T
)	×
2,000	=	16,000	ms	(16	seconds!).
You	can	see	now	why	it's	often	a	good	idea	to	defragment	your	disk
drive!
Solution	to	Problem	
6.5	
(page
601
)
avg	seek
max	rotation
avg	rotation
avg	seek
avg	rotation
max	rotation
avg	seek
avg
rotation
avg	seek
avg	rotation</p>
<p>This	is	a	simple	problem	that	will	give	you	some	interesting	insights	into
the	feasibility	of	SSDs.	Recall	that	for	disks,	1	PB	=	10
MB.	Then	the
following	straightforward	translation	of	units	yields	the	following	predicted
times	for	each	case:
A
.	
Worst-case	sequential	writes	(470	MB/s):
B
.	
Worst-case	random	writes	(303	MB/s):
C
.	
Average	case	(20	GB/day):
So	even	if	the	SSD	operates	continuously,	it	should	last	for	at	least	8
years,	which	is	longer	than	the	expected	lifetime	of	most	computers.
Solution	to	Problem	
6.6	
(page
604
)
In	the	10-year	period	between	2005	and	2015,	the	unit	price	of	rotating
disks	dropped	by	a	factor	of	166,	which	means	the	price	is	dropping	by
roughly	a	factor	of	2	every	18	months	or	so.	Assuming	this	trend
continues,	a	petabyte	of	storage,	which	costs	about	$30,000	in	2015,	will
drop	below	$500	after	about	seven	of	these	factor-of-2	reductions.	Since
these	are	occurring	every	18	months,	we	might	expect	a	petabyte	of
storage	to	be	available	for	$500	around	the	year	2025.
9
(
10
9
×
128
)
×
(
1
/
470
)
×
(
1
/
(
86
,
400
×
365
)
)
≈
8
 
years
(
10
9
×
128
)
×
(
1
/
303
)
×
(
1
/
(
86
,
400
×
365
)
)
≈
13
 
years
(
10
9
×
128
)
×
(
1
/
20
,
000
)
×
(
1
/
365
)
≈
140
 
years</p>
<p>Solution	to	Problem	
6.7	
(page
608
)
To	create	a	stride-1	reference	pattern,	the	loops	must	be	permuted	so
that	the	rightmost	indices	change	most	rapidly.
This	is	an	important	idea.	Make	sure	you	understand	why	this	particular
loop	permutation	results	in	a	stride-1	access	pattern.
Solution	to	Problem	
6.8	
(page</p>
<p>609
)
The	key	to	solving	this	problem	is	to	visualize	how	the	array	is	laid	out	in
memory	and	then	analyze	the	reference	patterns.	Function	
accesses	the	array	using	a	stride-1	reference	pattern	and	thus	clearly
has	the	best	spatial	locality.	Function	
scans	each	of	the	
N
structs
in	order,	which	is	good,	but	within	each	struct	it	hops	around	in	a	non-
stride-1	pattern	at	the	following	offsets	from	the	beginning	of	the	struct:	0,
12,	4,	16,	8,	20.	So	
has	worse	spatial	locality	than	
.
Function	
not	only	hops	around	within	each	struct,	but	also	hops
from	struct	to	struct.	So	
exhibits	worse	spatial	locality	than	
and	
.
Solution	to	Problem	
6.9	
(page
616
)
The	solution	is	a	straightforward	application	of	the	definitions	of	the
various	cache	parameters	in	
Figure	
6.26
.	Not	very	exciting,	but	you
need	to	understand	how	the	cache	organization	induces	these	partitions
in	the	address	bits	before	you	can	really	understand	how	caches	work.
Cache
m
C
B
E
S
t
s
b</p>
<ol>
<li></li>
</ol>
<p>32
1,024
4
1
256
22
8
2
2.
32
1,024
8
4
32
24
5
3</p>
<ol start="3">
<li></li>
</ol>
<p>32
1,024
32
32
1
27
0
5
Solution	to	Problem	
6.10	
(page
624
)
The	padding	eliminates	the	conflict	misses.	Thus,	three-fourths	of	the
references	are	hits.
Solution	to	Problem	
6.11	
(page
624
)
Sometimes,	understanding	why	something	is	a	bad	idea	helps	you
understand	why	the	alternative	is	a	good	idea.	Here,	the	bad	idea	we	are
looking	at	is	indexing	the	cache	with	the	high-order	bits	instead	of	the
middle	bits.
A
.	
With	high-order	bit	indexing,	each	contiguous	array	chunk	consists
of	2
blocks,	where	
t
is	the	number	of	tag	bits.	Thus,	the	first	2
contiguous	blocks	of	the	array	would	map	to	set	0,	the	next	2
blocks	would	map	to	set	1,	and	so	on.
B
.	
For	a	direct-mapped	cache	where	(
S,	E,	B,	m
)	=	(512,	1,	32,	32),
the	cache	capacity	is	512	32-byte	blocks	with	
t
=	18	tag	bits	in
each	cache	line.	Thus,	the	first	2
blocks	in	the	array	would	map
t
t
t
18
18</p>
<p>to	set	0,	the	next	2
blocks	to	set	1.	Since	our	array	consists	of
only	(4,096	×	4
)/
32	=	512	blocks,	all	of	the	blocks	in	the	array	map
to	set	0.	Thus,	the	cache	will	hold	at	most	1	array	block	at	any
point	in	time,	even	though	the	array	is	small	enough	to	fit	entirely
in	the	cache.	Clearly,	using	high-order	bit	indexing	makes	poor	use
of	the	cache.
Solution	to	Problem	
6.12	
(page
628
)
The	2	low-order	bits	are	the	block	offset	(CO),	followed	by	3	bits	of	set
index	(CI),	with	the	remaining	bits	serving	as	the	tag	(CT):
Solution	to	Problem	
6.13	
(page
628
)
Address:	
A
.	
Address	format	(1	bit	per	box):
18</p>
<p>B
.	
Memory	reference:
Parameter
Value
Cache	block	offset	(CO)
Cache	set	index	(CI)
Cache	tag	(CT)
Cache	hit?	(Y/N)
Y
Cache	byte	returned
Solution	to	Problem	
6.14	
(page
629
)
Address:	
A
.	
Address	format	(1	bit	per	box):
B
.	
Memory	reference:
Parameter
Value
Cache	block	offset	(CO)
Cache	set	index	(CI)</p>
<p>Cache	tag	(CT)
Cache	hit?	(Y/N)
N
Cache	byte	returned
—
Solution	to	Problem	
6.15	
(page
629
)
Address:	
A
.	
Address	format	(1	bit	per	box):
B
.	
Memory	reference:
Parameter
Value
Cache	block	offset
Cache	set	index
Cache	tag
Cache	hit?	(Y/N)
N
Cache	byte	returned
—</p>
<p>Solution	to	Problem	
6.16	
(page
630
)
This	problem	is	a	sort	of	inverse	version	of	Practice	Problems	6.12−6.15
that	requires	you	to	work	backward	from	the	contents	of	the	cache	to
derive	the	addresses	that	will	hit	in	a	particular	set.	In	this	case,	set	3
contains	one	valid	line	with	a	tag	of	
.	Since	there	is	only	one	valid
line	in	the	set,	four	addresses	will	hit.	These	addresses	have	the	binary
form	
.	Thus,	the	four	hex	addresses	that	hit	in	set	3	are
Solution	to	Problem	
6.17	
(page
636
)
A
.	
The	key	to	solving	this	problem	is	to	visualize	the	picture	in	
Figure
6.48
.	Notice	that	each	cache	line	holds	exactly	one	row	of	the
array,	that	the	cache	is	exactly	large	enough	to	hold	one	array,	and
that	for	all	
i
,	row	
i
of	
and	
maps	to	the	same	cache	line.
Because	the	cache	is	too	small	to	hold	both	arrays,	references	to
one	array	keep	evicting	useful	lines	from	the	other	array.	For
example,	the	write	to	
evicts	the	line	that	was	loaded</p>
<p>when	we	read	
.	So	when	we	next	read	
,	we
have	a	miss.
array
array
Col.	0
Col.	1
Col.	0
Col.	1
Row	0
m
m
Row	0
m
m
Row	1
m
m
Row	1
m
h
B
.	
When	the	cache	is	32	bytes,	it	is	large	enough	to	hold	both	arrays.
Thus,	the	only	misses	are	the	initial	cold	misses.
array
array
Col.	0
Col.	1
Col.	0
Col.	1
Row	0
m
h
Row	0
m
h
Row	1
m
h
Row	1
m
h
Figure	
6.48	
Figure	for	solution	to	
Problem	
6.17
.
Solution	to	Problem	
6.18	
(page
637
)</p>
<p>Each	16-byte	cache	line	holds	two	contiguous	
structures.
Each	loop	visits	these	structures	in	memory	order,	reading	one	integer
element	each	time.	So	the	pattern	for	each	loop	is	miss,	hit,	miss,	hit,	and
so	on.	Notice	that	for	this	problem	we	could	have	predicted	the	miss	rate
without	actually	enumerating	the	total	number	of	reads	and	misses.
A
.	
What	is	the	total	number	of	read	accesses?	512	reads.
B
.	
What	is	the	total	number	of	read	accesses	that	miss	in	the	cache?
256	misses.
C
.	
What	is	the	miss	rate?	256/512	=	50%.
Solution	to	Problem	
6.19	
(page
638
)
The	key	to	this	problem	is	noticing	that	the	cache	can	only	hold	1/2	of	the
array.	So	the	column-wise	scan	of	the	second	half	of	the	array	evicts	the
lines	that	were	loaded	during	the	scan	of	the	first	half.	For	example,
reading	the	first	element	of	
evicts	the	line	that	was	loaded
when	we	read	elements	from	
.	This	line	also	contained	
.	So	when	we	begin	scanning	the	next	column,	the	reference	to	the
first	element	of	
misses.
A
.	
What	is	the	total	number	of	read	accesses?	512	reads.
B
.	
What	is	the	total	number	of	read	accesses	that	miss	in	the	cache?
256	misses.
C
.	
What	is	the	miss	rate?	256/512	=	50%.</p>
<p>D
.	
What	would	the	miss	rate	be	if	the	cache	were	twice	as	big?	If	the
cache	were	twice	as	big,	it	could	hold	the	entire	grid	array.	The
only	misses	would	be	the	initial	cold	misses,	and	the	miss	rate
would	be	1/4	=	25%.
Solution	to	Problem	
6.20	
(page
638
)
This	loop	has	a	nice	stride-1	reference	pattern,	and	thus	the	only	misses
are	the	initial	cold	misses.
A
.	
What	is	the	total	number	of	read	accesses?	512	reads.
B
.	
What	is	the	total	number	of	read	accesses	that	miss	in	the	cache?
128	misses.
C
.	
What	is	the	miss	rate?	128/512	=	25%.
D
.	
What	would	the	miss	rate	be	if	the	cache	were	twice	as	big?
Increasing	the	cache	size	by	any	amount	would	not	change	the
miss	rate,	since	cold	misses	are	unavoidable.
Solution	to	Problem	
6.21	
(page
643
)
The	sustained	throughput	using	large	strides	from	L1	is	about	12,000
MB/s,	the	clock	frequency	is	2,100	MHz,	and	the	individual	read</p>
<p>accesses	are	in	units	of	8-byte	longs.	Thus,	from	this	graph	we	can
estimate	that	it	takes	roughly	2,100/12,000	×	8	=	1.4	≈	1.5	cycles	to
access	a	word	from	L1	on	this	machine,	which	is	roughly	2.5	times	faster
than	the	nominal	4-cycle	latency	from	L1.	This	is	due	to	the	parallelism	of
the	4	×	4	unrolled	loop,	which	allows	multiple	loads	to	be	in	flight	at	the
same	time.</p>
<p>Part	
II	
Running	Programs	on	a
System
Our	exploration	of	computer	systems	continues	with	a	closer	look	at	the
systems	software	that	builds	and	runs	application	programs.	The	linker
combines	different	parts	of	our	programs	into	a	single	file	that	can	be
loaded	into	memory	and	executed	by	the	processor.	Modern	operating
systems	cooperate	with	the	hardware	to	provide	each	program	with	the
illusion	that	it	has	exclusive	use	of	a	processor	and	the	main	memory,
when	in	reality	multiple	programs	are	running	on	the	system	at	any	point
in	time.
In	the	first	part	of	this	book,	you	developed	a	good	understanding	of	the
interaction	between	your	programs	and	the	hardware.	Part	II	of	the	book
will	broaden	your	view	of	systems	by	giving	you	a	solid	understanding	of
the	interactions	between	your	programs	and	the	operating	system.	You
will	learn	how	to	use	services	provided	by	the	operating	system	to	build
system-level	programs	such	as	Unix	shells	and	dynamic	memory
allocation	packages.</p>
<p>Chapter	
7	
Linking
7.1	
Compiler	Drivers	
671
7.2	
Static	Linking	
672
7.3	
Object	Files	
673
7.4	
Relocatable	Object	Files	
674
7.5	
Symbols	and	Symbol	Tables	
675
7.6	
Symbol	Resolution	
679
7.7	
Relocation	
689
7.8	
Executable	Object	Files	
695
7.9	
Loading	Executable	Object	Files	
697
7.10	
Dynamic	Linking	with	Shared	Libraries	
698
7.11	
Loading	and	Linking	Shared	Libraries	from	Applications	
701
7.12	
Position-Independent	Code	(PIC)	
704
7.13	
Library	Interpositioning	
707
7.14	
Tools	for	Manipulating	Object	Files	
713</p>
<p>7.15	
Summary</p>
<p>713
Bibliographic	Notes	
714
Homework	Problems	
714
Solutions	to	Practice	Problems	
717
Linking	is	the	process	of	collecting	and	combining
various	pieces	of	code	and	data	into	a	single	file
that	can	be	
loaded
(copied)	into	memory	and
executed.	Linking	can	be	performed	at	
compile	time
,
when	the	source	code	is	translated	into	machine
code;	at	
load	time
,	when	the	program	is	loaded	into
memory	and	executed	by	the	
loader
;	and	even	at
run	time
,	by	application	programs.	On	early
computer	systems,	linking	was	performed	manually.
On	modern	systems,	linking	is	performed
automatically	by	programs	called	
linkers
.
Linkers	play	a	crucial	role	in	software	development
because	they	enable	
separate	compilation
.	Instead
of	organizing	a	large	application	as	one	monolithic
source	file,	we	can	decompose	it	into	smaller,	more
manageable	modules	that	can	be	modified	and
compiled	separately.	When	we	change	one	of	these
modules,	we	simply	recompile	it	and	relink	the
application,	without	having	to	recompile	the	other
files.</p>
<p>Linking	is	usually	handled	quietly	by	the	linker	and	is
not	an	important	issue	for	students	who	are	building
small	programs	in	introductory	programming
classes.	So	why	bother	learning	about	linking?
Understanding	linkers	will	help	you	build
large	programs.	
Programmers	who	build	large
programs	often	encounter	linker	errors	caused
by	missing	modules,	missing	libraries,	or
incompatible	library	versions.	Unless	you
understand	how	a	linker	resolves	references,
what	a	library	is,	and	how	a	linker	uses	a	library
to	resolve	references,	these	kinds	of	errors	will
be	baffling	and	frustrating.
Understanding	linkers	will	help	you	avoid
dangerous	programming	errors.	
The
decisions	that	Linux	linkers	make	when	they
resolve	symbol	references	can	silently	affect	the
correctness	of	your	programs.	Programs	that
incorrectly	define	multiple	global	variables	can
pass	through	the	linker	without	any	warnings	in
the	default	case.	The	resulting	programs	can
exhibit	baffling	run-time	behavior	and	are
extremely	difficult	to	debug.	We	will	show	you
how	this	happens	and	how	to	avoid	it.
Understanding	linking	will	help	you
understand	how	language	scoping	rules	are
implemented.	
For	example,	what	is	the
difference	between	global	and	local	variables?</p>
<h2>What	does	it	really	mean	when	you	define	a
variable	or	function	with	the	static	attribute?
Understanding	linking	will	help	you
understand	other	important	systems
concepts.	
The	executable	object	files	produced
by	linkers	play	key	roles	in	important	systems
functions	such	as	loading	and	running	programs,
virtual	memory,	paging,	and	memory	mapping.
Understanding	linking	will	enable	you	to
exploit	shared	libraries.	
For	many	years,
linking	was	considered	to	be	fairly
straightforward	and	uninteresting.	However,	with
the	increased	importance	of	shared	libraries	and
dynamic	linking	in	modern	operating	systems,
linking	is	a	sophisticated	process	that	provides
the	knowledgeable	programmer	with	significant
power.	For	example,	many	software	products
use	shared	libraries	to	upgrade	shrink-wrapped
binaries	at	run	time.	Also,	many	Web	servers
rely	on	dynamic	linking	of	shared	libraries	to
serve	dynamic	content.
(a)</h2>
<p>code/link/main.c</p>
<hr />
<h2>code/link/main.c
(b)</h2>
<p>code/link/sum.c</p>
<hr />
<p>code/link/sum.c
Figure	
7.1	
Example	program	1.
The	example	program	consists	of	two	source	files,
and	
.	The	
initializes	an
array	of	
,	and	then	calls	the	
function	to	sum
the	array	elements.
This	chapter	provides	a	thorough	discussion	of	all
aspects	of	linking,	from	traditional	static	linking,	to
dynamic	linking	of	shared	libraries	at	load	time,	to
dynamic	linking	of	shared	libraries	at	run	time.	We
will	describe	the	basic	mechanisms	using	real
examples,	and	we	will	identify	situations	in	which
linking	issues	can	affect	the	performance	and
correctness	of	your	programs.	To	keep	things
concrete	and	understandable,	we	will	couch	our
discussion	in	the	context	of	an	x86-64	system
running	Linux	and	using	the	standard	ELF-64
(hereafter	referred	to	as	ELF)	object	file	format.
However,	it	is	important	to	realize	that	the	basic
concepts	of	linking	are	universal,	regardless	of	the
operating	system,	the	ISA,	or	the	object	file	format.
Details	may	vary,	but	the	concepts	are	the	same.</p>
<p>7.1	
Compiler	Drivers
Consider	the	C	program	in	
Figure	
7.1
.	It	will	serve	as	a	simple	running
example	throughout	this	chapter	that	will	allow	us	to	make	some
important	points	about	how	linkers	work.
Most	compilation	systems	provide	a	
compiler	driver
that	invokes	the
language	preprocessor,	compiler,	assembler,	and	linker,	as	needed	on
behalf	of	the	user.	For	example,	to	build	the	example	program	using	the
GNU	compilation	system,	we	might	invoke	the	
GCC</p>
<p>driver	by	typing	the
following	command	to	the	shell:
Figure	
7.2
summarizes	the	activities	of	the	driver	as	it	translates	the
example	program	from	an	ASCII	source	file	into	an	executable	object	file.
(If	you	want	to	see	these	steps	for	yourself,	run	
GCC</p>
<p>with	the	-
option.)
The	driver	first	runs	the	C	preprocessor	(
),
which	translates	the	C
source	file	
into	an	ASCII	intermediate	file	
:</p>
<ol>
<li></li>
</ol>
<p>In	some	versions	of	
GCC
,	the	preprocessor	is	integrated	into	the	compiler	driver.
1</p>
<p>Figure	
7.2	
Static	linking.
The	linker	combines	relocatable	object	files	to	form	an	executable	object
file	
.
Next,	the	driver	runs	the	C	compiler	(
),	which	translates	
into
an	ASCII	assembly-language	file	
:
Then,	the	driver	runs	the	assembler	(as),	which	translates	
into	a
binary	
relocatable	object	file</p>
<p>The	driver	goes	through	the	same	process	to	generate	
Finally,	it
runs	the	linker	program	
,	which	combines	
and	
,	along</p>
<p>with	the	necessary	system	object	files,	to	create	the	binary	
executable
object	file</p>
<p>:
To	run	the	executable	
,	we	type	its	name	on	the	Linux	shell's
command	line:
The	shell	invokes	a	function	in	the	operating	system	called	the	
loader
,
which	copies	the	code	and	data	in	the	executable	file	
into	memory,
and	then	transfers	control	to	the	beginning	of	the	program.</p>
<p>7.2	
Static	Linking
Static	linkers
such	as	the	Linux	
LD</p>
<p>program	take	as	input	a	collection	of
relocatable	object	files	and	command-line	arguments	and	generate	as
output	a	fully	linked	executable	object	file	that	can	be	loaded	and	run.
The	input	relocatable	object	files	consist	of	various	code	and	data
sections,	where	each	section	is	a	contiguous	sequence	of	bytes.
Instructions	are	in	one	section,	initialized	global	variables	are	in	another
section,	and	uninitialized	variables	are	in	yet	another	section.
To	build	the	executable,	the	linker	must	perform	two	main	tasks:
Step	</p>
<ol>
<li></li>
</ol>
<p>Symbol	resolution.	
Object	files	define	and	reference
symbols
,	where	each	symbol	corresponds	to	a	function,	a	global
variable,	or	a	
static	variable
(i.e.,	any	C	variable	declared	with	the
attribute).	The	purpose	of	symbol	resolution	is	to	associate
each	symbol	
reference
with	exactly	one	symbol	
definition
.
Step	
2.	
Relocation.	
Compilers	and	assemblers	generate	code
and	data	sections	that	start	at	address	0.	The	linker	
relocates
these	sections	by	associating	a	memory	location	with	each	symbol
definition,	and	then	modifying	all	of	the	references	to	those
symbols	so	that	they	point	to	this	memory	location.	The	linker
blindly	performs	these	relocations	using	detailed	instructions,
generated	by	the	assembler,	called	
relocation	entries
.
The	sections	that	follow	describe	these	tasks	in	more	detail.	As	you	read,
keep	in	mind	some	basic	facts	about	linkers:	Object	files	are	merely</p>
<p>collections	of	blocks	of	bytes.	Some	of	these	blocks	contain	program
code,	others	contain	program	data,	and	others	contain	data	structures
that	guide	the	linker	and	loader.	A	linker	concatenates	blocks	together,
decides	on	run-time	locations	for	the	concatenated	blocks,	and	modifies
various	locations	within	the	code	and	data	blocks.	Linkers	have	minimal
understanding	of	the	target	machine.	The	compilers	and	assemblers	that
generate	the	object	files	have	already	done	most	of	the	work.</p>
<p>7.3	
Object	Files
Object	files	come	in	three	forms:
Relocatable	object	file.	
Contains	binary	code	and	data	in	a	form	that
can	be	combined	with	other	relocatable	object	files	at	compile	time	to
create	an	executable	object	file.
Executable	object	file.	
Contains	binary	code	and	data	in	a	form	that
can	be	copied	directly	into	memory	and	executed.
Shared	object	file.	
A	special	type	of	relocatable	object	file	that	can
be	loaded	into	memory	and	linked	dynamically,	at	either	load	time	or
run	time.
Compilers	and	assemblers	generate	relocatable	object	files	(including
shared	object	files).	Linkers	generate	executable	object	files.	Technically,
an	
object	module
is	a	sequence	of	bytes,	and	an	
object	file
is	an	object
module	stored	on	disk	in	a	file.	However,	we	will	use	these	terms
interchangeably.
Object	files	are	organized	according	to	specific	
object	file	formats
,	which
vary	from	system	to	system.	The	first	Unix	systems	from	Bell	Labs	used
the	
format.	(To	this	day,	executables	are	still	referred	to	as	
files.)	Windows	uses	the	Portable	Executable	(PE)	format.	Mac	OS-X
uses	the	Mach-O	format.	Modern	x86-64	Linux	and	Unix	systems	use
Executable	and	Linkable	Format	(ELF
).	Although	our	discussion	will</p>
<p>focus	on	ELF,	the	basic	concepts	are	similar,	regardless	of	the	particular
format.
Figure	
7.3	
Typical	ELF	relocatable	object	file.</p>
<p>7.4	
Relocatable	Object	Files
Figure	
7.3
shows	the	format	of	a	typical	ELF	relocatable	object	file.
The	
ELF	header
begins	with	a	16-byte	sequence	that	describes	the	word
size	and	byte	ordering	of	the	system	that	generated	the	file.	The	rest	of
the	ELF	header	contains	information	that	allows	a	linker	to	parse	and
interpret	the	object	file.	This	includes	the	size	of	the	ELF	header,	the
object	file	type	(e.g.,	relocatable,	executable,	or	shared),	the	machine
type	(e.g.,	x86-64),	the	file	offset	of	the	section	header	table,	and	the	size
and	number	of	entries	in	the	section	header	table.	The	locations	and
sizes	of	the	various	sections	are	described	by	the	
section	header	table
,
which	contains	a	fixed-size	entry	for	each	section	in	the	object	file.
Sandwiched	between	the	ELF	header	and	the	section	header	table	are
the	sections	themselves.	A	typical	ELF	relocatable	object	file	contains	the
following	sections:
The	machine	code	of	the	compiled	program.
Read-only	data	such	as	the	format	strings	in	
statements,	and	jump	tables	for	switch	statements.</p>
<p>Initialized
global	and	static	C	variables.	Local	C	variables	are
maintained	at	run	time	on	the	stack	and	do	
not
appear	in	either	the
or	
sections.</p>
<p>Uninitialized
global	and	static	C	variables,	along	with	any	global
or	static	variables	that	are	initialized	to	zero.	This	section	occupies	no</p>
<p>actual	space	in	the	object	file;	it	is	merely	a	placeholder.	Object	file
formats	distinguish	between	initialized	and	uninitialized	variables	for
space	efficiency:	uninitialized	variables	do	not	have	to	occupy	any
actual	disk	space	in	the	object	file.	At	run	time,	these	variables	are
allocated	in	memory	with	an	initial	value	of	zero.
Aside	
Why	is	uninitialized	data	called
?
The	use	of	the	term	
to	denote	uninitialized	data	is
universal.	It	was	originally	an	acronym	for	the	&quot;block	started	by
symbol&quot;	directive	from	the	IBM	704	assembly	language	(circa
1957)	and	the	acronym	has	stuck.	A	simple	way	to	remember
the	difference	between	the	
and	
sections	is	to	think
of	&quot;bss&quot;	as	an	abbreviation	for	&quot;Better	Save	Space!&quot;
A	
symbol	table
with	information	about	functions	and	global
variables	that	are	defined	and	referenced	in	the	program.	Some
programmers	mistakenly	believe	that	a	program	must	be	compiled
with	the	-
option	to	get	symbol	table	information.	In	fact,	every
relocatable	object	file	has	a	symbol	table	in	
(unless	the
programmer	has	specifically	removed	it	with	the	
command).
However,	unlike	the	symbol	table	inside	a	compiler,	the	
symbol	table	does	not	contain	entries	for	local	variables.
A	list	of	locations	in	the	
section	that	will	need	to	be
modified	when	the	linker	combines	this	object	file	with	others.	In
general,	any	instruction	that	calls	an	external	function	or	references	a
global	variable	will	need	to	be	modified.	On	the	other	hand,</p>
<p>instructions	that	call	local	functions	do	not	need	to	be	modified.	Note
that	relocation	information	is	not	needed	in	executable	object	files,
and	is	usually	omitted	unless	the	user	explicitly	instructs	the	linker	to
include	it.
Relocation	information	for	any	global	variables	that	are
referenced	or	defined	by	the	module.	In	general,	any	initialized	global
variable	whose	initial	value	is	the	address	of	a	global	variable	or
externally	defined	function	will	need	to	be	modified.
A	debugging	symbol	table	with	entries	for	local	variables	and
typedefs	defined	in	the	program,	global	variables	defined	and
referenced	in	the	program,	and	the	original	C	source	file.	It	is	only
present	if	the	compiler	driver	is	invoked	with	the	-
option.
A	mapping	between	line	numbers	in	the	original	C	source
program	and	machine	code	instructions	in	the	
section.	It	is	only
present	if	the	compiler	driver	is	invoked	with	the	-
option.
A	string	table	for	the	symbol	tables	in	the	
and	
sections	and	for	the	section	names	in	the	section	headers.	A	string
table	is	a	sequence	of	null-terminated	character	strings.</p>
<p>7.5	
Symbols	and	Symbol	Tables
Each	relocatable	object	module,	
m
,	has	a	symbol	table	that	contains
information	about	the	symbols	that	are	defined	and	referenced	by	
m.
In
the	context	of	a	linker,	there	are	three	different	kinds	of	symbols:
Global	symbols
that	are	defined	by	module	
m
and	that	can	be
referenced	by	other	modules.	Global	linker	symbols	correspond	to
nonstatic
C	functions	and	global	variables.
Global	symbols	that	are	referenced	by	module	
m
but	defined	by	some
other	module.	Such	symbols	are	called	
externals
and	correspond	to
nonstatic	C	functions	and	global	variables	that	are	defined	in	other
modules.
Local	symbols
that	are	defined	and	referenced	exclusively	by	module
m
.These	correspond	to	static	C	functions	and	global	variables	that	are
defined	with	the	static	attribute.	These	symbols	are	visible	anywhere
within	module	
m
,	but	cannot	be	referenced	by	other	modules.
It	is	important	to	realize	that	local	linker	symbols	are	not	the	same	as
local	program	variables.	The	symbol	table	in	
does	not	contain
any	symbols	that	correspond	to	local	nonstatic	program	variables.	These
are	managed	at	run	time	on	the	stack	and	are	not	of	interest	to	the	linker.
Interestingly,	local	procedure	variables	that	are	defined	with	the	C	
attribute	are	not	managed	on	the	stack.	Instead,	the	compiler	allocates
space	in	
or	
for	each	definition	and	creates	a	local	linker</p>
<p>symbol	in	the	symbol	table	with	a	unique	name.	For	example,	suppose	a
pair	of	functions	in	the	same	module	define	a	static	local	variable	
:
In	this	case,	the	compiler	exports	a	pair	of	local	linker	symbols	with
different	names	to	the	assembler.	For	example,	it	might	use	
for	the
definition	in	function	
and	
for	the	definition	in	function	
.
Symbol	tables	are	built	by	assemblers,	using	symbols	exported	by	the
compiler	into	the	assembly-language	
file.	An	ELF	symbol	table	is
contained	in	the	
section.	It	contains	an	array	of	entries.	
Figure
7.4
shows	the	format	of	each	entry.
The	
is	a	byte	offset	into	the	string	table	that	points	to	the	null-
terminated	string	name	of	the	symbol.	The	
is	the	symbol's	address.</p>
<h2>For	relocatable	modules,	the	
is	an	offset	from	the	beginning	of	the
section	where	the	object	is	defined.	For	executable	object	files,	the	
is	an	absolute	run-time	address.	The	
is	the	size	(in	bytes)	of	the
object.	The	
is	usually	either	
or	
.	The	symbol	table	can
also	contain	entries	for	the	individual	sections
New	to	C?	
Hiding	variable	and	function
names	with	
C	programmers	use	the	
attribute	to	hide	variable	and
function	declarations	inside	modules,	much	as	you	would	use
public
and	
private
declarations	in	Java	and	C++.	In	C,	source	files
play	the	role	of	modules.	Any	global	variable	or	function	declared
with	the	
attribute	is	private	to	that	module.	Similarly,	any
global	variable	or	function	declared	without	the	
attribute	is
public	and	can	be	accessed	by	any	other	module.	It	is	good
programming	practice	to	protect	your	variables	and	functions	with
the	
attribute	wherever	possible.</h2>
<p>code/link/elfstructs.c</p>
<hr />
<p>code/link/elfstructs.c
Figure	
7.4	
ELF	symbol	table	entry.
The	type	and	binding	fields	are	4	bits	each.
and	for	the	path	name	of	the	original	source	file.	So	there	are	distinct
types	for	these	objects	as	well.	The	binding	field	indicates	whether	the
symbol	is	local	or	global.
Each	symbol	is	assigned	to	some	section	of	the	object	file,	denoted	by
the	section	field,	which	is	an	index	into	the	section	header	table.	There
are	three	special	pseudosections	that	don't	have	entries	in	the	section
header	table:	ABS	is	for	symbols	that	should	not	be	relocated.	UNDEF	is
for	undefined	symbols—that	is,	symbols	that	are	referenced	in	this	object
module	but	defined	elsewhere.	COMMON	is	for	uninitialized	data	objects
that	are	not	yet	allocated.	For	COMMON	symbols,	the	
field	gives
the	alignment	requirement,	and	
gives	the	minimum	size.	Note	that
these	pseudosections	exist	only	in	relocatable	object	files;	they	do	not
exist	in	executable	object	files.
The	distinction	between	COMMON	and	
is	subtle.	Modern	versions
of	
GCC</p>
<p>assign	symbols	in	relocatable	object	files	to	COMMON	and	
using	the	following	convention:</p>
<p>COMMON
Uninitialized	global	variables
Uninitialized	static	variables,	and	global	or	static	variables	that	are	initialized	to
zero
The	reason	for	this	seemingly	arbitrary	distinction	stems	from	the	way	the
linker	performs	symbol	resolution,	which	we	will	explain	in	
Section	
7.6
.
The	GNU	
program	is	a	handy	tool	for	viewing	the	contents	of
object	files.	For	example,	here	are	the	last	three	symbol	table	entries	for
the	relocatable	object	
,	from	the	example	program	in	
Figure
7.1
.	The	first	eight	entries,	which	are	not	shown,	are	local	symbols	that
the	linker	uses	internally.
In	this	example,	we	see	an	entry	for	the	definition	of	global	symbol	
,
a	24-byte	function	located	at	an	offset	(i.e.,	
)	of	zero	in	the	
section.	This	is	followed	by	the	definition	of	the	global	symbol	
,	an
8-byte	object	located	at	an	offset	of	zero	in	the	
section.	The	last
entry	comes	from	the	reference	to	the	external	symbol	
identifies	each	section	by	an	integer	index.	
denotes	the	
section,	and	
denotes	the	
section.</p>
<h2>Practice	Problem	
7.1	
(solution	page
717
)
This	problem	concerns	the	
and	
modules	from	
Figure
7.5
.	For	each	symbol	that	is	defined	or	referenced	in	
,
indicate	whether	or	not	it	will	have	a	symbol	table	entry	in	the
section	in	module	
.	If	so,	indicate	the	module	that
defines	the	symbol	(
),	the	symbol	type	(local,	global,	or
extern),	and	the	section	(
,	or	COMMON)	it	is
assigned	to	in	the	module.
(a)	m.c</h2>
<h2 id="codelinkmc"><a class="header" href="#codelinkmc">code/link/m.c</a></h2>
<h2>code/link/m.c
(b)	swap.c</h2>
<p>code/link/swap.c</p>
<hr />
<p>code/link/swap.c
Figure	
7.5	
Example	program	for	
Practice	Problem	
7.1
.
Symbol
entry?
Symbol
type
Module	where
defined
Section</p>
<hr />
<hr />
<hr />
<hr />
<hr />
<hr />
<hr />
<hr />
<hr />
<hr />
<hr />
<hr />
<hr />
<hr />
<hr />
<hr />
<hr />
<hr />
<hr />
<hr />
<p>7.6	
Symbol	Resolution
The	linker	resolves	symbol	references	by	associating	each	reference	with
exactly	one	symbol	definition	from	the	symbol	tables	of	its	input
relocatable	object	files.	Symbol	resolution	is	straightforward	for
references	to	local	symbols	that	are	defined	in	the	same	module	as	the
reference.	The	compiler	allows	only	one	definition	of	each	local	symbol
per	module.	The	compiler	also	ensures	that	static	local	variables,	which
get	local	linker	symbols,	have	unique	names.
Resolving	references	to	global	symbols,	however,	is	trickier.	When	the
compiler	encounters	a	symbol	(either	a	variable	or	function	name)	that	is
not	defined	in	the	current	module,	it	assumes	that	it	is	defined	in	some
other	module,	generates	a	linker	symbol	table	entry,	and	leaves	it	for	the
linker	to	handle.	If	the	linker	is	unable	to	find	a	definition	for	the
referenced	symbol	in	any	of	its	input	modules,	it	prints	an	(often	cryptic)
error	message	and	terminates.	For	example,	if	we	try	to	compile	and	link
the	following	source	file	on	a	Linux	machine,</p>
<p>then	the	compiler	runs	without	a	hitch,	but	the	linker	terminates	when	it
cannot	resolve	the	reference	to	
Symbol	resolution	for	global	symbols	is	also	tricky	because	multiple
object	modules	might	define	global	symbols	with	the	same	name.	In	this
case,	the	linker	must	either	flag	an	error	or	somehow	choose	one	of	the
definitions	and	discard	the	rest.	The	approach	adopted	by	Linux	systems
involves	cooperation	between	the	compiler,	assembler,	and	linker	and
can	introduce	some	baffling	bugs	to	the	unwary	programmer.
Aside	
Mangling	of	linker	symbols	in	C++
and	Java
Both	C++	and	Java	allow	overloaded	methods	that	have	the	same
name	in	the	source	code	but	different	parameter	lists.	So	how
does	the	linker	tell	the	difference	between	these	different
overloaded	functions?	Overloaded	functions	in	C++	and	Java
work	because	the	compiler	encodes	each	unique	method	and
parameter	list	combination	into	a	unique	name	for	the	linker.	This
encoding	process	is	called	
mangling
,	and	the	inverse	process	is
known	as	
demangling.</p>
<p>Happily,	C++	and	Java	use	compatible	mangling	schemes.	A
mangled	class	name	consists	of	the	integer	number	of	characters
in	the	name	followed	by	the	original	name.	For	example,	the	class
is	encoded	as	
.	A	method	is	encoded	as	the	original
method	name,	followed	by	__,	followed	by	the	mangled	class
name,	followed	by	single	letter	encodings	of	each	argument.	For
example,	
is	encoded	as	
.	Similar
schemes	are	used	to	mangle	global	variable	and	template	names.
7.6.1	
How	Linkers	Resolve
Duplicate	Symbol	Names
The	input	to	the	linker	is	a	collection	of	relocatable	object	modules.	Each
of	these	modules	defines	a	set	of	symbols,	some	of	which	are	local
(visible	only	to	the	module	that	defines	it),	and	some	of	which	are	global
(visible	to	other	modules).	What	happens	if	multiple	modules	define
global	symbols	with	the	same	name?	Here	is	the	approach	that	Linux
compilation	systems	use.
At	compile	time,	the	compiler	exports	each	global	symbol	to	the
assembler	as	either	
strong
or	
weak
,	and	the	assembler	encodes	this
information	implicitly	in	the	symbol	table	of	the	relocatable	object	file.
Functions	and	initialized	global	variables	get	strong	symbols.	Uninitialized
global	variables	get	weak	symbols.
Given	this	notion	of	strong	and	weak	symbols,	Linux	linkers	use	the
following	rules	for	dealing	with	duplicate	symbol	names:</p>
<p>Rule	1.	Multiple	strong	symbols	with	the	same	name	are	not	allowed.
Rule	2.	Given	a	strong	symbol	and	multiple	weak	symbols	with	the
same	name,	choose	the	strong	symbol.
Rule	3.	Given	multiple	weak	symbols	with	the	same	name,	choose
any	of	the	weak	symbols.
For	example,	suppose	we	attempt	to	compile	and	link	the	following	two	C
modules:
In	this	case,	the	linker	will	generate	an	error	message	because	the	strong
symbol	main	is	defined	multiple	times	(rule	1):</p>
<p>Similarly,	the	linker	will	generate	an	error	message	for	the	following
modules	because	the	strong	symbol	
is	defined	twice	(rule	1):
However,	if	
is	uninitialized	in	one	module,	then	the	linker	will	quietly
choose	the	strong	symbol	defined	in	the	other	(rule	2):</p>
<p>″
″
At	run	time,	function	
changes	the	value	of	
from	15213	to	15212,
which	might	come	as	an	unwelcome	surprise	to	the	author	of	function
!	Notice	that	the	linker	normally	gives	no	indication	that	it	has
detected	multiple	definitions	of	
:</p>
<p>The	same	thing	can	happen	if	there	are	two	weak	definitions	of	
(rule
3):
″
″</p>
<p>The	application	of	rules	2	and	3	can	introduce	some	insidious	run-time
bugs	that	are	incomprehensible	to	the	unwary	programmer,	especially	if
the	duplicate	symbol	definitions	have	different	types.	Consider	the
following	example,	in	which	
is	inadvertently	defined	as	an	
in	one
module	and	a	double	in	another:
″
″</p>
<p>On	an	x86-64/Linux	machine,	
are	8	bytes	and	
are	4	bytes.
On	our	system,	the	address	of	
is	
and	the	address	of	
is
.	Thus,	the	assignment	
=	
in	line	6	of	
will	overwrite
the	memory	locations	for	
and	
(lines	5	and	6	in	
)	with	the
double-precision	floating-point	representation	of	negative	zero!
This	is	a	subtle	and	nasty	bug,	especially	because	it	triggers	only	a
warning	from	the	linker,	and	because	it	typically	manifests	itself	much
later	in	the	execution	of	the	program,	far	away	from	where	the	error
occurred.	In	a	large	system	with	hundreds	of	modules,	a	bug	of	this	kind
is	extremely	hard	to	fix,	especially	because	many	programmers	are	not
aware	of	how	linkers	work,	and	because	they	often	ignore	compiler
warnings.	When	in	doubt,	invoke	the	linker	with	a	flag	such	as	the	
GCC</p>
<pre><code>flag,	which	triggers	an	error	if	it	encounters	multiply-defined
</code></pre>
<p>global	symbols.	Or	use	the	
option,	which	turns	all	warnings	into
errors.
In	
Section	
7.5
,	we	saw	how	the	compiler	assigns	symbols	to
COMMON	and	
using	a	seemingly	arbitrary	convention.	Actually,	this
convention	is	due	to	the	fact	that	in	some	cases	the	linker	allows	multiple
modules	to	define	global	symbols	with	the	same	name.	When	the
compiler	is	translating	some	module	and	encounters	a	weak	global
symbol,	say,	
,	it	does	not	know	if	other	modules	also	define	
,	and	if	so,
it	cannot	predict	which	of	the	multiple	instances	of	
the	linker	might
choose.	So	the	compiler	defers	the	decision	to	the	linker	by	assigning	
to	COMMON.	On	the	other	hand,	if	
is	initialized	to	zero,	then	it	is	a
strong	symbol	(and	thus	must	be	unique	by	rule	2),	so	the	compiler	can
confidently	assign	it	to	
.	Similarly,	static	symbols	are	unique	by
construction,	so	the	compiler	can	confidently	assign	them	to	either	
or	
.
Practice	Problem	
7.2	
(solution	page	
718
)
In	this	problem,	let	
denote	that	the	linker	will
associate	an	arbitrary	reference	to	symbol	
in	module	
to	the
definition	of	
in	module	
.	For	each	example	that	follows,	use
this	notation	to	indicate	how	the	linker	would	resolve	references	to
the	multiply-defined	symbol	in	each	module.	If	there	is	a	link-time
error	(rule	1),	write	&quot;
ERROR
&quot;.	If	the	linker	arbitrarily	chooses	one	of
the	definitions	(rule	3),	write	&quot;
UNKNOWN
&quot;.
A
.	</p>
<p>B
.	
C
.	
7.6.2	
Linking	with	Static	Libraries</p>
<p>So	far,	we	have	assumed	that	the	linker	reads	a	collection	of	relocatable
object	files	and	links	them	together	into	an	output	executable	file.	In
practice,	all	compilation	systems	provide	a	mechanism	for	packaging
related	object	modules	into	a	single	file	called	a	
static	library
,	which	can
then	be	supplied	as	input	to	the	linker.	When	it	builds	the	output
executable,	the	linker	copies	only	the	object	modules	in	the	library	that
are	referenced	by	the	application	program.
Why	do	systems	support	the	notion	of	libraries?	Consider	ISO	C99,
which	defines	an	extensive	collection	of	standard	I/O,	string	manipulation,
and	integer	math	functions	such	as	
,	and
.	They	are	available	
to	every	C	program	in	the	
library.	ISO
C99	also	defines	an	extensive	collection	of	floating-point	math	functions
such	as	
,	and	
in	the	
library.
Consider	the	different	approaches	that	compiler	developers	might	use	to
provide	these	functions	to	users	without	the	benefit	of	static	libraries.	One
approach	would	be	to	have	the	compiler	recognize	calls	to	the	standard
functions	and	to	generate	the	appropriate	code	directly.	Pascal,	which
provides	a	small	set	of	standard	functions,	takes	this	approach,	but	it	is
not	feasible	for	C,	because	of	the	large	number	of	standard	functions
defined	by	the	C	standard.	It	would	add	significant	complexity	to	the
compiler	and	would	require	a	new	compiler	version	each	time	a	function
was	added,	deleted,	or	modified.	To	application	programmers,	however,
this	approach	would	be	quite	convenient	because	the	standard	functions
would	always	be	available.
Another	approach	would	be	to	put	all	of	the	standard	C	functions	in	a
single	relocatable	object	module,	say,	
,	that	application</p>
<p>programmers	could	link	into	their	executables:
This	approach	has	the	advantage	that	it	would	decouple	the
implementation	of	the	standard	functions	from	the	implementation	of	the
compiler,	and	would	still	be	reasonably	convenient	for	programmers.
However,	a	big	disadvantage	is	that	every	executable	file	in	a	system
would	now	contain	a	complete	copy	of	the	collection	of	standard
functions,	which	would	be	extremely	wasteful	of	disk	space.	(On	our
system,	
is	about	5	MB	and	
is	about	2	MB.)	Worse,	each
running	program	would	now	contain	its	own	copy	of	these	functions	in
memory,	which	would	be	extremely	wasteful	of	memory.	Another	big
disadvantage	is	that	any	change	to	any	standard	function,	no	matter	how
small,	would	require	the	library	developer	to	recompile	the	entire	source
file,	a	time-consuming	operation	that	would	complicate	the	development
and	maintenance	of	the	standard	functions.
We	could	address	some	of	these	problems	by	creating	a	separate
relocatable	file	for	each	standard	function	and	storing	them	in	a	well-
known	directory.	However,	this	approach	would	require	application
programmers	to	explicitly	link	the	appropriate	object	modules	into	their
executables,	a	process	that	would	be	error	prone	and	time	consuming:</p>
<h2>The	notion	of	a	static	library	was	developed	to	resolve	the	disadvantages
of	these	various	approaches.	Related	functions	can	be	compiled	into
separate	object	modules	and	then	packaged	in	a	single	static	library	file.
Application	programs	can	then	use	any	of	the	functions	defined	in	the
library	by	specifying	a	single	filename	on	the	command	line.	For	example,
a	program	that	uses	functions	from	the	C	standard	library	and	the	math
library	could	be	compiled	and	linked	with	a	command	of	the	form
(a)</h2>
<p>code/link/addvec.c</p>
<hr />
<h2>code/link/addvec.c
(b)</h2>
<h2 id="codelinkmultvecc"><a class="header" href="#codelinkmultvecc">code/link/multvec.c</a></h2>
<p>code/link/multvec.c
Figure	
7.6	
Member	object	files	in	the	
library.
At	link	time,	the	linker	will	only	copy	the	object	modules	that	are
referenced	by	the	program,	which	reduces	the	size	of	the	executable	on
disk	and	in	memory.	On	the	other	hand,	the	application	programmer	only
needs	to	include	the	names	of	a	few	library	files.	(In	fact,	C	compiler</p>
<p>drivers	always	pass	
to	the	linker,	so	the	reference	to	
mentioned	previously	is	unnecessary.)
On	Linux	systems,	static	libraries	are	stored	on	disk	in	a	particular	file
format	known	as	an	
archive
.	An	archive	is	a	collection	of	concatenated
relocatable	object	files,	with	a	header	that	describes	the	size	and	location
of	each	member	object	file.	Archive	filenames	are	denoted	with	the	
To	make	our	discussion	of	libraries	concrete,	consider	the	pair	of	vector
routines	in	
Figure	
7.6
.	Each	routine,	defined	in	its	own	object	module,
performs	a	vector	operation	on	a	pair	of	input	vectors	and	stores	the
result	in	an	output	vector.	As	a	side	effect,	each	routine	records	the
number	of	times	it	has	been	called	by	incrementing	a	global	variable.
(This	will	be	useful	when	we	explain	the	idea	of	position-independent
code	in	
Section	
7.12
.)
To	create	a	static	library	of	these	functions,	we	would	use	the	
tool	as
follows:
To	use	the	library,	we	might	write	an	application	such	as	
in
Figure	
7.7
,	which	invokes	the	
library	routine.	The	include	(or
header)	file	
defines	the	function	prototypes	for	the	routines	in
,</p>
<h2>To	build	the	executable,	we	would	compile	and	link	the	input	files	
and	
:</h2>
<h2 id="codelinkmain2c"><a class="header" href="#codelinkmain2c">code/link/main2.c</a></h2>
<p>code/link/main2.c
Figure	
7.7	
Example	program	2.
This	program	invokes	a	function	in	the	
library.</p>
<p>Figure	
7.8	
Linking	with	static	libraries.
or	equivalently,
Figure	
7.8
summarizes	the	activity	of	the	linker.	The	
argument
tells	the	compiler	driver	that	the	linker	should	build	a	fully	linked
executable	object	file	that	can	be	loaded	into	memory	and	run	without
any	further	linking	at	load	time.	The	
argument	is	a	shorthand	for
,	and	the	
.	argument	tells	the	linker	to	look	for	
in	the	current	directory.
When	the	linker	runs,	it	determines	that	the	
symbol	defined	by
is	referenced	by	
,	so	it	copies	
into	the
executable.	
Since	the	program	doesn't	reference	any	symbols	defined	by
,	the	linker	does	
not
copy	this	module	into	the	executable.	The
linker	also	copies	the	
module	from	
,	along	with	a	number
of	other	modules	from	the	C	run-time	system.</p>
<p>7.6.3	
How	Linkers	Use	Static
Libraries	to	Resolve	References
While	static	libraries	are	useful,	they	are	also	a	source	of	confusion	to
programmers	because	of	the	way	the	Linux	linker	uses	them	to	resolve
external	references.	During	the	symbol	resolution	phase,	the	linker	scans
the	relocatable	object	files	and	archives	left	to	right	in	the	same
sequential	order	that	they	appear	on	the	compiler	driver's	command	line.
(The	driver	automatically	translates	any	
on	the	command	line
into	
.)	During	this	scan,	the	linker	maintains	a	set	
E
of
relocatable	object	files	that	will	be	merged	to	form	the	executable,	a	set	
U
of	unresolved	symbols	(i.e.,	symbols	referred	to	but	not	yet	defined),	and
a	set	
D
of	symbols	that	have	been	defined	in	previous	input	files.	Initially,
E
,	
U
,	and	
D
are	empty.
For	each	input	file	
f
on	the	command	line,	the	linker	determines	if	
f
is
an	object	file	or	an	archive.	If	
f
is	an	object	file,	the	linker	adds	
f
to	
E
,
updates	
U
and	
D
to	reflect	the	symbol	definitions	and	references	in	
f
,
and	proceeds	to	the	next	input	file.
If	
f
is	an	archive,	the	linker	attempts	to	match	the	unresolved	symbols
in	
U
against	the	symbols	defined	by	the	members	of	the	archive.	If
some	archive	member	
m
defines	a	symbol	that	resolves	a	reference
in	
U
,	then	
m
is	added	to	
E
,	and	the	linker	updates	
U
and	
D
to	reflect
the	symbol	definitions	and	references	in	
m
.	This	process	iterates	over
the	member	object	files	in	the	archive	until	a	fixed	point	is	reached
where	
U
and	
D
no	longer	change.	At	this	point,	any	member	object</p>
<p>files	not	contained	in	
E
are	simply	discarded	and	the	linker	proceeds
to	the	next	input	file.
If	
U
is	nonempty	when	the	linker	finishes	scanning	the	input	files	on
the	command	line,	it	prints	an	error	and	terminates.	Otherwise,	it
merges	and	relocates	the	object	files	in	
E
to	build	the	output
executable	file.
Unfortunately,	this	algorithm	can	result	in	some	baffling	link-time	errors
because	the	ordering	of	libraries	and	object	files	on	the	command	line	is
significant.	If	the	library	that	defines	a	symbol	appears	on	the	command
line	before	the	object	file	that	references	that	symbol,	then	the	reference
will	not	be	resolved	and	linking	will	fail.	For	example,	consider	the
following:
What	happened?	When	
is	processed,	
U
is	empty,	so	no
member	object	files	from	
are	added	to	
E
.	Thus,	the
reference	to	
is	never	resolved	and	the	linker	emits	an	error
message	and	terminates.
The	general	rule	for	libraries	is	to	place	them	at	the	end	of	the	command
line.	If	the	members	of	the	different	libraries	are	independent,	in	that	no
member	references	a	symbol	defined	by	another	member,	then	the
libraries	can	be	placed	at	the	end	of	the	command	line	in	any	order.	If,	on
the	other	hand,	the	libraries	are	not	independent,	then	they	must	be</p>
<p>ordered	so	that	for	each	symbol	
s
that	is	referenced	externally	by	a
member	of	an	archive,	at	least	one	definition	of	
s
follows	a	reference	to	
s
on	the	command	line.	For	example,	suppose	
calls	functions	in
and	
that	call	functions	in	
.	Then	
and	
must	precede	
on	the	command	line:
Libraries	can	be	repeated	on	the	command	line	if	necessary	to	satisfy	the
dependence	requirements.	For	example,	suppose	
calls	a	function
in	
that	calls	a	function	in	
that	calls	a	function	in	
.
Then	
must	be	repeated	on	the	command	line:
Alternatively,	we	could	combine	
and	
into	a	single	archive.
Practice	Problem	
7.3	
(solution	page	
718
)
Let	
and	
denote	object	modules	or	static	libraries	in	the	current
directory,	and	let	
denote	that	
a
depends	on	
b
,	in	the	sense
that	
b
defines	a	symbol	that	is	referenced	by	
a
.	For	each	of	the
following	scenarios,	show	the	minimal	command	line	(i.e.,	one	with
the	least	number	of	object	file	and	library	arguments)	that	will	allow
the	static	linker	to	resolve	all	symbol	references.</p>
<p>A
.	
B
.	
C
.	</p>
<p>7.7	
Relocation
Once	the	linker	has	completed	the	symbol	resolution	step,	it	has
associated	each	symbol	reference	in	the	code	with	exactly	one	symbol
definition	(i.e.,	a	symbol	table	entry	in	one	of	its	input	object	modules).	At
this	point,	the	linker	knows	the	exact	sizes	of	the	code	and	data	sections
in	its	input	object	modules.	It	is	now	ready	to	begin	the	relocation	step,
where	it	merges	the	input	modules	and	assigns	run-time	addresses	to
each	symbol.	Relocation	consists	of	two	steps:
1
.	
Relocating	sections	and	symbol	definitions.	
In	this	step,	the
linker	merges	all	sections	of	the	same	type	into	a	new	aggregate
section	of	the	same	type.	For	example,	the	
sections	from
the	input	modules	are	all	merged	into	one	section	that	will	become
the	
section	for	the	output	executable	object	
file.	The	linker
then	assigns	run-time	memory	addresses	to	the	new	aggregate
sections,	to	each	section	defined	by	the	input	modules,	and	to
each	symbol	defined	by	the	input	modules.	When	this	step	is
complete,	each	instruction	and	global	variable	in	the	program	has
a	unique	run-time	memory	address.
2
.	
Relocating	symbol	references	within	sections.	
In	this	step,	the
linker	modifies	every	symbol	reference	in	the	bodies	of	the	code
and	data	sections	so	that	they	point	to	the	correct	run-time
addresses.	To	perform	this	step,	the	linker	relies	on	data	structures
in	the	relocatable	object	modules	known	as	relocation	entries,
which	we	describe	next.</p>
<p>7.7.1	
Relocation	Entries
When	an	assembler	generates	an	object	module,	it	does	not	know	where
the	code	and	data	will	ultimately	be	stored	in	memory.	Nor	does	it	know
the	locations	of	any	externally	defined	functions	or	global	variables	that
are	referenced	by	the	module.	So	whenever	the	assembler	encounters	a
reference	to	an	object	whose	ultimate	location	is	unknown,	it	generates	a
relocation	entry
that	tells	the	linker	how	to	modify	the	reference	when	it
merges	the	object	file	into	an	executable.	Relocation	entries	for	code	are
placed	in	
.	Relocation	entries	for	data	are	placed	in	
Figure	
7.9
shows	the	format	of	an	ELF	relocation	entry.	The	
is
the	section	offset	of	the	reference	that	will	need	to	be	modified.	The
identifies	the	symbol	that	the	modified	reference	should	point	to.
The	
tells	the	linker	how	to	modify	the	new	reference.	The	
is
a	signed	constant	that	is	used	by	some	types	of	relocations	to	bias	the
value	of	the	modified	reference.
ELF	defines	32	different	relocation	types,	many	quite	arcane.	We	are
concerned	with	only	the	two	most	basic	relocation	types:
Relocate	a	reference	that	uses	a	32-bit	PC-relative
address.	Recall	from	
Section	
3.6.3
that	a	PC-relative	address	is	an
offset	from	the	current	run-time	value	of	the	program	counter	(PC).
When	the	CPU	executes	an	instruction	using	PC-relative	addressing,
it	forms	the	
effective	address
(e.g.,	the	target	of	the	call	instruction)	by
adding	the	32-bit	value</p>
<hr />
<h2 id="codelinkelfstructsc"><a class="header" href="#codelinkelfstructsc">code/link/elfstructs.c</a></h2>
<p>code/link/elfstructs.c
Figure	
7.9	
ELF	relocation	entry.
Each	entry	identifies	a	reference	that	must	be	relocated	and	specifies
how	to	compute	the	modified	reference.
encoded	in	the	instruction	to	the	current	run-time	value	of	the	PC,
which	is	always	the	address	of	the	next	instruction	in	memory.
.	Relocate	a	reference	that	uses	a	32-bit	absolute
address.	With	absolute	addressing,	the	CPU	directly	uses	the	32-bit
value	encoded	in	the	instruction	as	the	effective	address,	without
further	modifications.
These	two	relocation	types	support	the	x86-64	
small	code	model
,	which
assumes	that	the	total	size	of	the	code	and	data	in	the	executable	object
file	is	smaller	than	2	GB,	and	thus	can	be	accessed	at	run-time	using	32-
bit	PC-relative	addresses.	The	small	code	model	is	the	default	for	
.</p>
<p>Programs	larger	than	2	GB	can	be	compiled	using	the	
(
medium	code	model
)	and	
(
large	code	model
)	flags,	but
we	won't	discuss	those.
7.7.2	
Relocating	Symbol	References
Figure	
7.10
shows	the	pseudocode	for	the	linker's	relocation
algorithm.	Lines	1	and	2	iterate	over	each	section	
and	each	relocation
entry	
associated	with	each	section.	For	concreteness,	assume	that
each	section	
is	an	array	of	bytes	and	that	each	relocation	entry	
is	a
of	type	
,	as	defined	in	
Figure	
7.9
.	Also,	assume	that
when	the	algorithm	runs,	the	linker	has	already	chosen	runtime
addresses	for	each	section	(denoted	
)	and	each	symbol	(denoted
).	Line	3	computes	the	address	in	the	
array	of	the	4-byte
reference	that	needs	to	be	relocated.	If	this	reference	uses	PC-relative
addressing,	then	it	is	relocated	by	lines	5−9.	If	the	reference	uses
absolute	addressing,	then	it	is	relocated	by	lines	11−13.</p>
<h2>Figure	
7.10	
Relocation	algorithm.</h2>
<p>code/link/main-relo.d</p>
<hr />
<p>code/link/main-relo.d
Figure	
7.11	
Code	and	relocation	entries	from	
The	original	C	code	is	in	
Figure	
7.1
.
Let's	see	how	the	linker	uses	this	algorithm	to	relocate	the	references	in
our	example	program	in	
Figure	
7.1
.	
Figure	
7.11
shows	the
disassembled	code	from	
,	as	generated	by	the	GNU	
OBJDUMP</p>
<p>tool
(
).
The	main	function	references	two	global	symbols,	
and	
.	For
each	reference,	the	assembler	has	generated	a	relocation	entry,	which	is
displayed	on	the	following	line.
The	relocation	entries	tell	the	linker	that
the	reference	to	
should	be	relocated	using	a	32-bit	PC-relative
address,	and	the	reference	to	
should	be	relocated	using	a	32-bit
absolute	address.	The	next	two	sections	detail	how	the	linker	relocates
these	references.
2.	
Recall	that	relocation	entries	and	instructions	are	actually	stored	in	different	sections	of	the
object	file.	The	
tool	displays	them	together	for	convenience.
Relocating	PC-Relative	References
2</p>
<p>In	line	6	in	
Figure	
7.11
,	function	
calls	the	
function,	which	is
defined	in	module	
.	The	
instruction	begins	at	section	offset
and	consists	of	the	1-byte	opcode	
,	followed	by	a	placeholder
for	the	32-bit	PC-relative	reference	to	the	target	
.
The	corresponding	relocation	entry	
consists	of	four	fields:
These	fields	tell	the	linker	to	modify	the	32-bit	PC-relative	reference
starting	at	offset	
so	that	it	will	point	to	the	
routine	at	run	time.
Now,	suppose	that	the	linker	has	determined	that
and</p>
<p>Using	the	algorithm	in	
Figure	
7.10
,	the	linker	first	computes	the	run-
time	address	of	the	reference	(line	7):
It	then	updates	the	reference	so	that	it	will	point	to	the	sum	routine	at	run
time	(line	8):
In	the	resulting	executable	object	file,	the	call	instruction	has	the	following
relocated	form:
At	run	time,	the	call	instruction	will	be	located	at	address	
.	When
the	CPU	executes	the	call	instruction,	the	PC	has	a	value	of	
,
which	is	the	address	of	the	instruction	immediately	following	the	</p>
<p>instruction.	To	execute	the	call	instruction,	the	CPU	performs	the
following	steps:
1
.	
Push	PC	onto	stack
2
.	
PC	←	PC	+	
Thus,	the	next	instruction	to	execute	is	the	first	instruction	of	the	
routine,	which	of	course	is	what	we	want!
Relocating	Absolute	References
Relocating	absolute	references	is	straightforward.	For	example,	in	line	4
in	
Figure	
7.11
,	the	
instruction	copies	the	address	of	
(a	32-
bit	immediate	value)	into	register	
.	The	
instruction	begins	at
section	offset	
and	consists	of	the	1-byte	opcode	
,	followed	by	a
placeholder	for	the	32-bit	absolute	reference	to	
.
The	corresponding	relocation	entry	
consists	of	four	fields:
These	fields	tell	the	linker	to	modify	the	absolute	reference	starting	at
offset	
so	that	it	will	point	to	the	first	byte	of	
at	run	time.	Now,
suppose	that	the	linker	has	determined	that</p>
<p>(a)	Relocated	
section
(b)	Relocated	.data	section</p>
<p>Figure	
7.12	
Relocated	
and	
sections	for	the	executable
file	
The	original	C	code	is	in	
Figure	
7.1
.
The	linker	updates	the	reference	using	line	13	of	the	algorithm	in	
Figure
7.10
:
In	the	resulting	executable	object	file,	the	reference	has	the	following
relocated	form:
Putting	it	all	together,	
Figure	
7.12
shows	the	relocated	
and
sections	in	the	final	executable	object	file.	At	load	time,	the	loader
can	copy	the	bytes	from	these	sections	directly	into	memory	and	execute
the	instructions	without	any	further	modifications.</p>
<p>Practice	Problem	
7.4	
(solution	page	
718
)
This	problem	concerns	the	relocated	program	in	
Figure	
7.12(a)
.
A
.	
What	is	the	hex	address	of	the	relocated	reference	to	
in
line	5?
B
.	
What	is	the	hex	value	of	the	relocated	reference	to	
in
line	5?
Practice	Problem	
7.5	
(solution	page	
718
)
Consider	the	call	to	function	
in	object	file	
(
Figure	
7.5
).
with	the	following	relocation	entry:
Now	suppose	that	the	linker	relocates	
in	
to	address
and	swap	to	address	
.	Then	what	is	the	value	of
the	relocated	reference	to	
in	the	
instruction?</p>
<h2>7.8	
Executable	Object	Files
We	have	seen	how	the	linker	merges	multiple	object	files	into	a	single
executable	object	file.	Our	example	C	program,	which	began	life	as	a
collection	of	ASCII	text	files,	has	been	transformed	into	a	single	binary
file	that	contains	all	of	the	information	needed	to	load	the	program	into
memory	and	run	it.	
Figure	
7.13
summarizes	the	kinds	of	information	in
a	typical	ELF	executable	file.
Figure	
7.13	
Typical	ELF	executable	object	file.</h2>
<p>code/link/prog-exe.d</p>
<hr />
<p>code/link/prog-exe.d
Figure	
7.14	
Program	header	table	for	the	example	executable	
:	offset	in	object	file;	
:	memory	address;	
:	alignment
requirement;	
:	segment	size	in	object	file;	
:	segment	size	in
memory;	
:	run-time	permissions.
The	format	of	an	executable	object	file	is	similar	to	that	of	a	relocatable
object	file.	The	ELF	header	describes	the	overall	format	of	the	file.	It	also
includes	the	program's	
entry	point
,	which	is	the	address	of	the	first
instruction	to	execute	when	the	program	runs.	The	
,	and
sections	are	similar	to	those	in	a	relocatable	object	file,	except	that
these	sections	have	been	relocated	to	their	eventual	run-time	memory
addresses.	The	
section	defines	a	small	function,	called	
,	that
will	be	called	by	the	program's	initialization	code.	Since	the	executable	is
fully	linked
(relocated),	it	needs	no	
sections.
ELF	executables	are	designed	to	be	easy	to	load	into	memory,	with
contiguous	chunks	of	the	executable	file	mapped	to	contiguous	memory
segments.	This	mapping	is	described	by	the	
program	header	table
.</p>
<p>Figure	
7.14
shows	part	of	the	program	header	table	for	our	example
executable	
,	as	displayed	by	
.
From	the	program	header	table,	we	see	that	two	memory	segments	will
be	initialized	with	the	contents	of	the	executable	object	file.	Lines	1	and	2
tell	us	that	the	first	segment	(the	
code	segment
)	has	read/execute
permissions,	starts	at	memory	address	
,	has	a	total	size	in
memory	of	
bytes,	and	is	initialized	with	the	first	
bytes	of	the
executable	object	file,	which	includes	the	ELF	header,	the	program
header	table,	and	the	
,	and	
sections.
Lines	3	and	4	tell	us	that	the	second	segment	(the	
data	segment
)	has
read/write	permissions,	starts	at	memory	address	
,	has	a	total
memory	size	of	
bytes,	and	is	initialized	with	the	
bytes	in	the
.data	section	starting	at	offset	
in	the	object	file.	The	remaining	8
bytes	in	the	segment	correspond	to	
data	that	will	be	initialized	to
zero	at	run	time.
For	any	segment	
s
,	the	linker	must	choose	a	starting	address,	
,
such	that
where	
is	the	offset	of	the	segment's	first	section	in	the	object	file,	and
is	the	alignment	specified	in	the	program	header	(2
=	
).
For	example,	in	the	data	segment	in	
Figure	
7.14
,
21</p>
<p>and
This	alignment	requirement	is	an	optimization	that	enables	segments	in
the	object	file	to	be	transferred	efficiently	to	memory	when	the	program
executes.	The	reason	is	somewhat	subtle	and	is	due	to	the	way	that
virtual	memory	is	organized	as	large	contiguous	power-of-2	chunks	of
bytes.	You	will	learn	all	about	virtual	memory	in	
Chapter	
9
.</p>
<p>7.9	
Loading	Executable	Object	Files
To	run	an	executable	object	file	
,	we	can	type	its	name	to	the	Linux
shell's	command	line:
Since	
does	not	correspond	to	a	built-in	shell	command,	the	shell
assumes	that	
is	an	executable	object	file,	which	it	runs	for	us	by
invoking	some	memory-resident	operating	system	code	known	as	the
.	Any	Linux	program	can	invoke	the	loader	by	calling	the	
function,	which	we	will	describe	in	detail	in	
Section	
8.4.6
.	The	loader
copies	the	code	and	data	in	the	executable	object	file	from	disk	into
memory	and	then	runs	the	program	by	jumping	to	its	first	instruction,	or
entry	point
.	This	process	of	copying	the	program	into	memory	and	then
running	it	is	known	as	
loading
.
Every	running	Linux	program	has	a	run-time	memory	image	similar	to	the
one	in	
Figure	
7.15
.	On	Linux	x86-64	systems,	the	code	segment	starts
at	address	
,	followed	by	the	data	segment.	The	run-time	
heap
follows	the	data	segment	and	grows	upward	via	calls	to	the	
library.(We	will	describe	
and	the	heap	in	detail	in	
Section	
9.9
.)
This	is	followed	by	a	region	that	is	reserved	for	shared	modules.	The	user
stack	starts	below	the	largest	legal	user	address	(2
-	1)	and	grows
down,	toward	smaller	memory	addresses.	The	region	above	the	stack,
48
48</p>
<p>starting	at	address	2
,	is	reserved	for	the	code	and	data	in	the	
kernel
,
which	is	the	memory-resident	part	of	the	operating	system.
For	simplicity,	we've	drawn	the	heap,	data,	and	code	segments	as
abutting	each	other,	and	we've	placed	the	top	of	the	stack	at	the	largest
legal	user	address.	In	practice,	there	is	a	gap	between	the	code	and	data
segments	due	to	the	alignment	requirement	on	the	
segment
(
Section	
7.8
).	Also,	the	linker	uses	address-space	layout
randomization	(ASLR,	
Section	
3.10.4
)	when	it	assigns	runtime
addresses	to	the	stack,	shared	library,	and	heap	segments.	Even	though
the	locations	of	these	regions	change	each	time	the	program	is	run,	their
relative	positions	are	the	same.
When	the	loader	runs,	it	creates	a	memory	image	similar	to	the	one
shown	in	
Figure	
7.15
.	Guided	by	the	program	header	table,	it	copies
chunks	of	the
Figure	
7.15	
Linux	x86-64	run-time	memory	image.
48</p>
<p>Gaps	due	to	segment	alignment	requirements	and	address-space	layout
randomization	(ASLR)	are	not	shown.	Not	to	scale.
executable	object	file	into	the	code	and	data	segments.	Next,	the	loader
jumps	to	the	program's	entry	point,	which	is	always	the	address	of	the
function.	This	function	is	defined	in	the	system	object	file	
and	is	the	same	for	all	C	programs.	The	
function	calls	the	
system
startup	function
,	
,	which	is	defined	in	
.	It
initializes	the	execution	environment,	calls	the	user-level	
function,
handles	its	return	value,	and	if	necessary	returns	control	to	the	kernel.</p>
<p>7.10	
Dynamic	Linking	with	Shared
Libraries
The	static	libraries	that	we	studied	in	
Section	
7.6.2
address	many	of
the	issues	associated	with	making	large	collections	of	related	functions
available	to	application	programs.	However,	static	libraries	still	have
some	significant	disadvantages.	Static	libraries,	like	all	software,	need	to
be	maintained	and	updated	periodically.	If	application	programmers	want
to	use	the	most	recent	version	of	a	library,	they	must	somehow	become
aware	that	the	library	has	changed	and	then	explicitly	relink	their
programs	against	the	updated	library.
Another	issue	is	that	almost	every	C	program	uses	standard	I/O	functions
such	as	
and	
.	At	run	time,	the	code	for	these	functions	is
duplicated	in	the	text	segment	of	each	running	process.	On	a	typical
system	that	is	running	hundreds	of	processes,	this	can	be	a	significant
waste	of	scarce	memory	system	resources.	(An	interesting	property	of
memory	is	that	it	is	
always
a	scarce	resource,	regardless
Aside	
How	do	loaders	really	work?
Our	description	of	loading	is	conceptually	correct	but	intentionally
not	entirely	accurate.	To	understand	how	loading	really	works,	you
must	understand	the	concepts	of	
processes
,	
virtual	memory
,	and
memory	mapping
,	which	we	haven't	discussed	yet.	As	we</p>
<p>encounter	these	concepts	later	in	
Chapters	
8
and	
9
,	we	will
revisit	loading	and	gradually	reveal	the	mystery	to	you.
For	the	impatient	reader,	here	is	a	preview	of	how	loading	really
works:	Each	program	in	a	Linux	system	runs	in	the	context	of	a
process	with	its	own	virtual	address	space.	When	the	shell	runs	a
program,	the	parent	shell	process	forks	a	child	process	that	is	a
duplicate	of	the	parent.	The	child	process	invokes	the	loader	via
the	
system	call.	The	loader	deletes	the	child's	existing
virtual	memory	segments	and	creates	a	new	set	of	code,	data,
heap,	and	stack	segments.	The	new	stack	and	heap	segments
are	initialized	to	zero.	The	new	code	and	data	segments	are
initialized	to	the	contents	of	the	executable	file	by	mapping	pages
in	the	virtual	address	space	to	page-size	chunks	of	the	executable
file.	Finally,	the	loader	jumps	to	the	
address,	which
eventually	calls	the	application's	
routine.	Aside	from	some
header	information,	there	is	no	copying	of	data	from	disk	to
memory	during	loading.	The	copying	is	deferred	until	the	CPU
references	a	mapped	virtual	page,	at	which	point	the	operating
system	automatically	transfers	the	page	from	disk	to	memory
using	its	paging	mechanism.
of	how	much	there	is	in	a	system.	Disk	space	and	kitchen	trash	cans
share	this	same	property.)
Shared	libraries
are	modern	innovations	that	address	the	disadvantages
of	static	libraries.	A	shared	library	is	an	object	module	that,	at	either	run
time	or	load	time,	can	be	loaded	at	an	arbitrary	memory	address	and
linked	with	a	program	in	memory.	This	process	is	known	as	
dynamic
linking
and	is	performed	by	a	program	called	a	
dynamic	linker
.	Shared
libraries	are	also	referred	to	as	
shared	objects
,	and	on	Linux	systems</p>
<p>they	are	indicated	by	the	
suffix.	Microsoft	operating	systems	make
heavy	use	of	shared	libraries,	which	they	refer	to	as	DLLs	(dynamic	link
libraries).
Shared	libraries	are	&quot;shared&quot;	in	two	different	ways.	First,	in	any	given	file
system,	there	is	exactly	one	
file	for	a	particular	library.	The	code	and
data	in	this	
file	are	shared	by	all	of	the	executable	object	files	that
reference	the	library,	as	opposed	to	the	contents	of	static	libraries,	which
are	copied	and	embedded	in	the	executables	that	reference	them.
Second,	a	single	copy	of	the	
section	of	a	shared	library	in	memory
can	be	shared	by	different	running	processes.	We	will	explore	this	in
more	detail	when	we	study	virtual	memory	in	
Chapter	
9
.
Figure	
7.16
summarizes	the	dynamic	linking	process	for	the	example
program	in	
Figure	
7.7
.	To	build	a	shared	library	
of	our
example	vector	routines	in	
Figure	
7.6
,	we	invoke	the	compiler	driver
with	some	special	directives	to	the	compiler	and	linker:
The	
flag	directs	the	compiler	to	generate	
position-independent
code
(more	on	this	in	the	next	section).	The	
flag	directs	the	linker
to	create	a	shared</p>
<p>Figure	
7.16	
Dynamic	linking	with	shared	libraries.
object	file.	Once	we	have	created	the	library,	we	would	then	link	it	into	our
example	program	in	
Figure	
7.7
:
This	creates	an	executable	object	file	
in	a	form	that	can	be	linked
with	
at	run	time.	The	basic	idea	is	to	do	some	of	the	linking
statically	when	the	executable	file	is	created,	and	then	complete	the
linking	process	dynamically	when	the	program	is	loaded.	It	is	important	to
realize	that	none	of	the	code	or	data	sections	from	
are
actually	copied	into	the	executable	
at	this	point.	Instead,	the	linker
copies	some	relocation	and	symbol	table	information	that	will	allow
references	to	code	and	data	in	
to	be	resolved	at	load	time.
When	the	loader	loads	and	runs	the	executable	
,	it	loads	the
partially	linked	executable	
,	using	the	techniques	discussed	in</p>
<p>Section	
7.9
.	Next,	it	notices	that	
contains	a	
section,
which	contains	the	path	name	of	the	dynamic	linker,	which	is	itself	a
shared	object	(e.g.,	
on	Linux	systems).	Instead	of	passing
control	to	the	application,	as	it	would	normally	do,	the	loader	loads	and
runs	the	dynamic	linker.	The	dynamic	linker	then	finishes	the	linking	task
by	performing	the	following	relocations:
Relocating	the	text	and	data	of	
into	some	memory	segment
Relocating	the	text	and	data	of	
into	another	memory
segment
Relocating	any	references	in	
to	symbols	defined	by	
and	
Finally,	the	dynamic	linker	passes	control	to	the	application.	From	this
point	on,	the	locations	of	the	shared	libraries	are	fixed	and	do	not	change
during	execution	of	the	program.</p>
<p>7.11	
Loading	and	Linking	Shared
Libraries	from	Applications
Up	to	this	point,	we	have	discussed	the	scenario	in	which	the	dynamic
linker	loads	and	links	shared	libraries	when	an	application	is	loaded,	just
before	it	executes.	However,	it	is	also	possible	for	an	application	to
request	the	dynamic	linker	to	load	and	link	arbitrary	shared	libraries	while
the	application	is	running,	without	having	to	link	in	the	applications
against	those	libraries	at	compile	time.
Dynamic	linking	is	a	powerful	and	useful	technique.	Here	are	some
examples	in	the	real	world:
Distributing	software.	
Developers	of	Microsoft	Windows	applications
frequently	use	shared	libraries	to	distribute	software	updates.	They
generate	a	new	copy	of	a	shared	library,	which	users	can	then
download	and	use	as	a	replacement	for	the	current	version.	The	next
time	they	run	their	application,	it	will	automatically	link	and	load	the
new	shared	library.
Building	high-performance	Web	servers.	
Many	Web	servers
generate	
dynamic	content
,	such	as	personalized	Web	pages,	account
balances,	and	banner	ads.	Early	Web	servers	generated	dynamic
content	by	using	
and	
to	create	a	child	process	and	run	a
&quot;CGI	program&quot;	in	the	context	of	the	child.	However,	modern	high-
performance	Web	servers	can	generate	dynamic	content	using	a
more	efficient	and	sophisticated	approach	based	on	dynamic	linking.</p>
<p>The	idea	is	to	package	each	function	that	generates	dynamic	content
in	a	shared	library.	When	a	request	arrives	from	a	Web	browser,	the
server	dynamically	loads	and	links	the	appropriate	function	and	then
calls	it	directly,	as	opposed	to	using	
and	
to	run	the
function	in	the	context	of	a	child	process.	The	function	remains
cached	in	the	server's	address	space,	so	subsequent	requests	can	be
handled	at	the	cost	of	a	simple	function	call.	This	can	have	a
significant	impact	on	the	throughput	of	a	busy	site.	Further,	existing
functions	can	be	updated	and	new	functions	can	be	added	at	run
time,	without	stopping	the	server.
Linux	systems	provide	a	simple	interface	to	the	dynamic	linker	that	allows
application	programs	to	load	and	link	shared	libraries	at	run	time.
The	
function	loads	and	links	the	shared	library	
.	The
external	symbols	in	
are	resolved	using	libraries	previously
opened	with	the	
flag.	If	the	current	executable	was	compiled
with	the	
flag,	then	its	global	symbols	are	also	available	for
symbol	resolution.	The	
argument	must	include	either	
,
which	tells	the	linker	to	resolve	references	to	external	symbols
immediately,	or	the	
flag,	which	instructs	the	linker	to	defer</p>
<p>symbol	resolution	until	code	from	the	library	is	executed.	Either	of	these
values	can	be	
OR
ed	with	the	
flag.
The	
function	takes	a	handle	to	a	previously	opened	shared	library
and	a	
name	and	returns	the	address	of	the	symbol,	if	it	exists,	or
NULL	otherwise.
The	
function	unloads	the	shared	library	if	no	other	shared
libraries	are	still	using	it.</p>
<p>The	
function	returns	a	string	describing	the	most	recent	error
that	occurred	as	a	result	of	calling	
,	or	
,	or	NULL	if
no	error	occurred.
Figure	
7.17
shows	how	we	would	use	this	interface	to	dynamically	link
our	
shared	library	at	run	time	and	then	invoke	its	
routine.	To	compile	the	program,	we	would	invoke	
GCC</p>
<h2>in	the	following
way:</h2>
<p>code/link/dll.c</p>
<hr />
<p>code/link/dll.c
Figure	
7.17	
Example	program	3.
Dynamically	loads	and	links	the	shared	library	
at	run	time.
Aside	
Shared	libraries	and	the	Java
Native	Interface
Java	defines	a	standard	calling	convention	called	
Java	Native
Interface	(JNI)
that	allows	&quot;native&quot;	C	and	C++	functions	to	be
called	from	Java	programs.	The	basic	idea	of	JNI	is	to	compile	the
native	C	function,	say,	
,	into	a	shared	library,	say,	
.
When	a	running	Java	program	attempts	to	invoke	function	
,
the	Java	interpreter	uses	the	
interface	(or	something	like	it)
to	dynamically	link	and	load	
and	then	call	
.</p>
<p>7.12	
Position-Independent	Code
(PIC)
A	key	purpose	of	shared	libraries	is	to	allow	multiple	running	processes	to
share	the	same	library	code	in	memory	and	thus	save	precious	memory
resources.	So	how	can	multiple	processes	share	a	single	copy	of	a
program?	One	approach	would	be	to	assign	a	priori	a	dedicated	chunk	of
the	address	space	to	each	shared	library,	and	then	require	the	loader	to
always	load	the	shared	library	at	that	address.	While	straightforward,	this
approach	creates	some	serious	problems.	It	would	be	an	inefficient	use
of	the	address	space	because	portions	of	the	space	would	be	allocated
even	if	a	process	didn't	use	the	library.	It	would	also	be	difficult	to
manage.	We	would	have	to	ensure	that	none	of	the	chunks	overlapped.
Each	time	a	library	was	modified,	we	would	have	to	make	sure	that	it	still
fit	in	its	assigned	chunk.	If	not,	then	we	would	have	to	find	a	new	chunk.
And	if	we	created	a	new	library,	we	would	have	to	find	room	for	it.	Over
time,	given	the	hundreds	of	libraries	and	versions	of	libraries	in	a	system,
it	would	be	difficult	to	keep	the	address	space	from	fragmenting	into	lots
of	small	unused	but	unusable	holes.	Even	worse,	the	assignment	of
libraries	to	memory	would	be	different	for	each	system,	thus	creating
even	more	management	headaches.
To	avoid	these	problems,	modern	systems	compile	the	code	segments	of
shared	modules	so	that	they	can	be	loaded	anywhere	in	memory	without
having	to	be	modified	by	the	linker.	With	this	approach,	a	single	copy	of	a
shared	module's	code	segment	can	be	shared	by	an	unlimited	number	of</p>
<p>processes.	(Of	course,	each	process	will	still	get	its	own	copy	of	the
read/write	data	segment.)
Code	that	can	be	loaded	without	needing	any	relocations	is	known	as
position-independent	code	(PIC).
Users	direct	GNU	compilation	systems
to	generate	PIC	code	with	the	
option	to	
GCC
.	
Shared	libraries	must
always	be	compiled	with	this	option.
On	x86-64	systems,	references	to	symbols	in	the	same	executable	object
module	require	no	special	treatment	to	be	PIC.	These	references	can	be
compiled	using	PC-relative	addressing	and	relocated	by	the	static	linker
when	it	builds	the	object	file.	However,	references	to	external	procedures
and	global	variables	that	are	defined	by	shared	modules	require	some
special	techniques,	which	we	describe	next.
PIC	Data	References
Compilers	generate	PIC	references	to	global	variables	by	exploiting	the
following	interesting	fact:	no	matter	where	we	load	an	object	module
(including	shared
Figure	
7.18	
Using	the	GOT	to	reference	a	global	variable.</p>
<p>The	
routine	in	
references	
indirectly	through
the	GOT	for	
.
object	modules)	in	memory,	the	data	segment	is	always	the	same
distance	from	the	code	segment.	Thus,	the	
distance
between	any
instruction	in	the	code	segment	and	any	variable	in	the	data	segment	is	a
run-time	constant,	independent	of	the	absolute	memory	locations	of	the
code	and	data	segments.
Compilers	that	want	to	generate	PIC	references	to	global	variables
exploit	this	fact	by	creating	a	table	called	the	
global	offset	table	(GOT)
at
the	beginning	of	the	data	segment.	The	GOT	contains	an	8-byte	entry	for
each	global	data	object	(procedure	or	global	variable)	that	is	referenced
by	the	object	module.	The	compiler	also	generates	a	relocation	record	for
each	entry	in	the	GOT.	At	load	time,	the	dynamic	linker	relocates	each
GOT	entry	so	that	it	contains	the	absolute	address	of	the	object.	Each
object	module	that	references	global	objects	has	its	own	GOT.
Figure	
7.18
shows	the	GOT	from	our	example	
shared
module.	The	
routine	loads	the	address	of	the	global	variable
indirectly	via	GOT[3]	and	then	increments	
in	memory.	The
key	idea	here	is	that	the	offset	in	the	PC-relative	reference	to	
is	a
run-time	constant.
Since	
is	defined	by	the	
module,	the	compiler	could
have	exploited	the	constant	distance	between	the	code	and	data
segments	by	generating	a	direct	PC-relative	reference	to	
and
adding	a	relocation	for	the	linker	to	resolve	when	it	builds	the	shared
module.	However,	if	
were	defined	by	another	shared	module,	then</p>
<p>the	indirect	access	through	the	GOT	would	be	necessary.	In	this	case,
the	compiler	has	chosen	to	use	the	most	general	solution,	the	GOT,	for
all	references.
PIC	Function	Calls
Suppose	that	a	program	calls	a	function	that	is	defined	by	a	shared
library.	The	compiler	has	no	way	of	predicting	the	run-time	address	of	the
function,	since	the	shared	module	that	defines	it	could	be	loaded
anywhere	at	run	time.	The	normal	approach	would	be	to	generate	a
relocation	record	for	the	reference,	which	
the	dynamic	linker	could	then
resolve	when	the	program	was	loaded.	However,	this	approach	would	not
be	PIC,	since	it	would	require	the	linker	to	modify	the	code	segment	of
the	calling	module.	GNU	compilation	systems	solve	this	problem	using	an
interesting	technique,	called	
lazy	binding
,	that	defers	the	binding	of	each
procedure	address	until	the	
first	time
the	procedure	is	called.
The	motivation	for	lazy	binding	is	that	a	typical	application	program	will
call	only	a	handful	of	the	hundreds	or	thousands	of	functions	exported	by
a	shared	library	such	as	
.	By	deferring	the	resolution	of	a
function's	address	until	it	is	actually	called,	the	dynamic	linker	can	avoid
hundreds	or	thousands	of	unnecessary	relocations	at	load	time.	There	is
a	nontrivial	run-time	overhead	the	first	time	the	function	is	called,	but
each	call	thereafter	costs	only	a	single	instruction	and	a	memory
reference	for	the	indirection.
Lazy	binding	is	implemented	with	a	compact	yet	somewhat	complex
interaction	between	two	data	structures:	the	GOT	and	the	
procedure
linkage	table	(PLT)
.	If	an	object	module	calls	any	functions	that	are</p>
<p>defined	in	shared	libraries,	then	it	has	its	own	GOT	and	PLT.	The	GOT	is
part	of	the	data	segment.	The	PLT	is	part	of	the	code	segment.
Figure	
7.19
shows	how	the	PLT	and	GOT	work	together	to	resolve	the
address	of	a	function	at	run	time.	First,	let's	examine	the	contents	of	each
of	these	tables.
Procedure	linkage	table	(PLT).	
The	PLT	is	an	array	of	16-byte	code
entries.	
is	a	special	entry	that	jumps	into	the	dynamic	linker.
Each	shared	library	function	called	by	the	executable	has	its	own	PLT
entry.	Each	of
Figure	
7.19	
Using	the	PLT	and	GOT	to	call	external	functions.
The	dynamic	linker	resolves	the	address	of	
the	first	time	it	is
called.</p>
<p>these	entries	is	responsible	for	invoking	a	specific	function.	
(not	shown	here)	invokes	the	system	startup	function
(
),	which	initializes	the	execution	environment,	calls
the	main	function,	and	handles	its	return	value.	Entries	starting	at
invoke	functions	called	by	the	user	code.	In	our	example,
invokes	
and	
(not	shown)	invokes	
.
Global	offset	table	(GOT).	
As	we	have	seen,	the	GOT	is	an	array	of
8-byte	address	entries.	When	used	in	conjunction	with	the	PLT,
and	
contain	information	that	the	dynamic	linker	uses
when	it	resolves	function	addresses.	
is	the	entry	point	for	the
dynamic	linker	in	the	
module.	Each	of	the	remaining
entries	corresponds	to	a	called	function	whose	address	needs	to	be
resolved	at	run	time.	Each	has	a	matching	PLT	entry.	For	example,
and	
correspond	to	
.	Initially,	each	GOT	entry
points	to	the	second	instruction	in	the	corresponding	PLT	entry.
Figure	
7.19(a)
shows	how	the	GOT	and	PLT	work	together	to	lazily
resolve	the	run-time	address	of	function	
the	first	time	it	is	called:
Step	</p>
<ol>
<li></li>
</ol>
<p>Instead	of	directly	calling	
,	the	program	calls	into
,	which	is	the	PLT	entry	for	
.
Step	
2.	
The	first	PLT	instruction	does	an	indirect	jump	through
.	Since	each	GOT	entry	initially	points	to	the	second
instruction	in	its	corresponding	PLT	entry,	the	indirect	jump	simply
transfers	control	back	to	the	next	instruction	in	
.
Step	
3.	
After	pushing	an	ID	for	
onto	the	stack,	
jumps	to	
.</p>
<p>Step	
4.	
pushes	an	argument	for	the	dynamic	linker
indirectly	through	
and	then	jumps	into	the	dynamic	linker
indirectly	through	
.	The	dynamic	linker	uses	the	two	stack
entries	to	determine	the	runtime	location	of	
,	overwrites
with	this	address,	and	passes	control	to	
.
Figure	
7.19(b)
shows	the	control	flow	for	any	subsequent	invocations
of	
:
Step	</p>
<ol>
<li></li>
</ol>
<p>Control	passes	to	
as	before.
Step	
2.	
However,	this	time	the	indirect	jump	through	
transfers	control	directly	to	
.</p>
<p>7.13	
Library	Interpositioning
Linux	linkers	support	a	powerful	technique,	called	
library	interpositioning
,
that	allows	you	to	intercept	calls	to	shared	library	functions	and	execute
your	own	code	instead.	Using	interpositioning,	you	could	trace	the
number	of	times	a	particular	
library	function	is	called,	validate	and	trace
its	input	and	output	values,	or	even	replace	it	with	a	completely	different
implementation.
Here's	the	basic	idea:	Given	some	
target	function
to	be	interposed	on,
you	create	a	
wrapper	function
whose	prototype	is	identical	to	the	target
function.	Using	some	particular	interpositioning	mechanism,	you	then
trick	the	system	into	calling	the	wrapper	function	instead	of	the	target
function.	The	wrapper	function	typically	executes	its	own	logic,	then	calls
the	target	function	and	passes	its	return	value	back	to	the	caller.
Interpositioning	can	occur	at	compile	time,	link	time,	or	run	time	as	the
program	is	being	loaded	and	executed.	To	explore	these	different
mechanisms,	we	will	use	the	example	program	in	
Figure	
7.20(a)
as	a
running	example.	It	calls	the	
and	
functions	from	the	C
standard	library	(
).	The	call	to	
allocates	a	block	of	32
bytes	from	the	heap	and	returns	a	pointer	to	the	block.	The	call	to	
gives	the	block	back	to	the	heap,	for	use	by	subsequent	calls	to	
.
Our	goal	is	to	use	interpositioning	to	trace	the	calls	to	
and	
as
the	program	runs.</p>
<p>7.13.1	
Compile-Time
Interpositioning
Figure	
7.20
shows	how	to	use	the	C	preprocessor	to	interpose	at
compile	time.	Each	wrapper	function	in	
(
Figure	
7.20(c)
)
calls	the	target	function,	prints	a	trace,	and	returns.	The	local	
header	file	(
Figure	
7.20(b)
)	instructs	the	preprocessor	to	replace	each
call	to	a	target	function	with	a	call	to	its	wrapper.	Here	is	how	to	compile
and	link	the	program:
The	interpositioning	happens	because	of	the	
.	argument,	which	tells
the	C	preprocessor	to	look	for	
in	the	current	directory	before
looking	in	the	usual	system	directories.	Notice	that	the	wrappers	in
are	compiled	with	the	standard	
header	file.
Running	the	program	gives	the	following	trace:</p>
<h2>7.13.2	
Link-Time	Interpositioning
The	Linux	static	linker	supports	link-time	interpositioning	with	the	
flag.	This	flag	tells	the	linker	to	resolve	references	to	symbol	
as
(two	underscores	for	the	prefix),	and	to	resolve	references	to
symbol	
(two	underscores	for	the	prefix)	as	
.	
Figure	
7.21
shows	the	wrappers	for	our	example	program.
Here	is	how	to	compile	the	source	files	into	relocatable	object	files:
(a)	Example	program	int.c</h2>
<p>code/link/interpose/int.c</p>
<hr />
<h2>code/link/interpose/int.c
(b)	Local	malloc.h	file</h2>
<h2 id="codelinkinterposemalloch"><a class="header" href="#codelinkinterposemalloch">code/link/interpose/malloc.h</a></h2>
<h2>code/link/interpose/malloc.h
(c)	Wrapper	functions	in	mymalloc.c</h2>
<p>code/link/interpose/mymalloc.c</p>
<hr />
<h2>code/link/interpose/mymalloc.c
Figure	
7.20	
Compile-time	interpositioning	with	the	C	preprocessor.</h2>
<p>code/link/interpose/mymalloc.c</p>
<hr />
<p>code/link/interpose/mymalloc.c
Figure	
7.21	
Link-time	interpositioning	with	the	
flag.
And	here	is	how	to	link	the	object	files	into	an	executable:</p>
<p>The	
,	option	flag	passes	option	to	the	linker.	Each	comma	in	option	is
replaced	with	a	space.	So	
passes	
to
the	linker,	and	similarly	for	
.
Running	the	program	gives	the	following	trace:
7.13.3	
Run-Time	Interpositioning
Compile-time	interpositioning	requires	access	to	a	program's	source	files.
Link-time	interpositioning	requires	access	to	its	relocatable	object	files.
However,	there	is	a	mechanism	for	interpositioning	at	run	time	that
requires	access	only	to	the	executable	object	file.	This	fascinating
mechanism	is	based	on	the	dynamic	linker's	
environment
variable.
If	the	
environment	variable	is	set	to	a	list	of	shared	library
pathnames	(separated	by	spaces	or	colons),	then	when	you	load	and
execute	a	program,	the	dynamic	linker	(
)	will	search	the
libraries	first,	before	any	other	shared	libraries,	when	it
resolves	undefined	references.	With	this	mechanism,	you	can	interpose
on	any	function	in	any	shared	library,	including	
,	when	you	load
and	execute	any	executable.</p>
<p>Figure	
7.22
shows	the	wrappers	for	
and	
.	In	each
wrapper,	the	call	to	
returns	the	pointer	to	the	target	
function.
The	wrapper	then	calls	the	target	function,	prints	a	trace,	and	returns.
Here	is	how	to	build	the	shared	library	that	contains	the	wrapper
functions:
Here	is	how	to	compile	the	main	program:
Here	is	how	to	run	the	program	from	the	bash	shell:
3.	
If	you	don't	know	what	shell	you	are	running,	type	
at	the	command	line.
And	here	is	how	to	run	it	from	the	
or	
shells:
3</p>
<h2>Notice	that	you	can	use	
to	interpose	on	the	library	calls	of	
any
executable	program!</h2>
<p>code/link/interpose/mymalloc.c</p>
<hr />
<p>code/link/interpose/mymalloc.c
Figure	
7.22	
Run-time	interpositioning	with	
.</p>
<p>7.14	
Tools	for	Manipulating	Object
Files
There	are	a	number	of	tools	available	on	Linux	systems	to	help	you
understand	and	manipulate	object	files.	In	particular,	the	GNU	
binutils
package	is	especially	helpful	and	runs	on	every	Linux	platform.
AR
.	Creates	static	libraries,	and	inserts,	deletes,	lists,	and	extracts
members.
STRINGS
.	Lists	all	of	the	printable	strings	contained	in	an	object	file.
STRIP
.	Deletes	symbol	table	information	from	an	object	file.
NM
.	Lists	the	symbols	defined	in	the	symbol	table	of	an	object	file.
SIZE
.	Lists	the	names	and	sizes	of	the	sections	in	an	object	file.
READELF
.	Displays	the	complete	structure	of	an	object	file,	including	all
of	the	information	encoded	in	the	ELF	header.	Subsumes	the
functionality	of	
SIZE</p>
<p>and	
NM
.
OBJDUMP
.	The	mother	of	all	binary	tools.	Can	display	all	of	the
information	in	an	object	file.	Its	most	useful	function	is	disassembling
the	binary	instructions	in	the	
section.
Linux	systems	also	provide	the	
LDD</p>
<p>program	for	manipulating	shared
libraries:</p>
<p>LDD
:	Lists	the	shared	libraries	that	an	executable	needs	at	run	time.</p>
<p>7.15	
Summary
Linking	can	be	performed	at	compile	time	by	static	linkers	and	at	load
time	and	run	time	by	dynamic	linkers.	Linkers	manipulate	binary	files
called	object	files,	which	come	in	three	different	forms:	relocatable,
executable,	and	shared.	Relocatable	object	files	are	combined	by	static
linkers	into	an	executable	object	file	that	can	be	loaded	into	memory	and
executed.	Shared	object	files	(shared	libraries)	are	linked	and	loaded	by
dynamic	linkers	at	run	time,	either	implicitly	when	the	calling	program	is
loaded	and	begins	executing,	or	on	demand,	when	the	program	calls
functions	from	the	
library.
The	two	main	tasks	of	linkers	are	symbol	resolution,	where	each	global
symbol	in	an	object	file	is	bound	to	a	unique	definition,	and	relocation,
where	the	ultimate	memory	address	for	each	symbol	is	determined	and
where	references	to	those	objects	are	modified.
Static	linkers	are	invoked	by	compiler	drivers	such	as	
GCC
.	They	combine
multiple	relocatable	object	files	into	a	single	executable	object	file.
Multiple	object	files	can	define	the	same	symbol,	and	the	rules	that
linkers	use	for	silently	resolving	these	multiple	definitions	can	introduce
subtle	bugs	in	user	programs.
Multiple	object	files	can	be	concatenated	in	a	single	static	library.	Linkers
use	libraries	to	resolve	symbol	references	in	other	object	modules.	The
left-to-right	sequential	scan	that	many	linkers	use	to	resolve	symbol
references	is	another	source	of	confusing	link-time	errors.</p>
<p>Loaders	map	the	contents	of	executable	files	into	memory	and	run	the
program.	Linkers	can	also	produce	partially	linked	executable	object	files
with	unresolved	references	to	the	routines	and	data	defined	in	a	shared
library.	At	load	time,	the	loader	maps	the	partially	linked	executable	into
memory	and	then	calls	a	dynamic	linker,	which	completes	the	linking	task
by	loading	the	shared	library	and	relocating	the	references	in	the
program.
Shared	libraries	that	are	compiled	as	position-independent	code	can	be
loaded	anywhere	and	shared	at	run	time	by	multiple	processes.
Applications	can	also	use	the	dynamic	linker	at	run	time	in	order	to	load,
link,	and	access	the	functions	and	data	in	shared	libraries.</p>
<p>Bibliographic	Notes
Linking	is	poorly	documented	in	the	computer	systems	literature.	Since	it
lies	at	the	intersection	of	compilers,	computer	architecture,	and	operating
systems,	linking	requires	an	understanding	of	code	generation,	machine-
language	programming,	program	instantiation,	and	virtual	memory.	It
does	not	fit	neatly	into	any	of	the	usual	computer	systems	specialties	and
thus	is	not	well	covered	by	the	classic	texts	in	these	areas.	However,
Levine's	monograph	provides	a	good	general	reference	on	the	subject
[
69
].	The	original	IA	32	specifications	for	ELF	and	DWARF	(a
specification	for	the	contents	of	the	
and	
sections)	are
described	in	[
54
].	The	x86-64	extensions	to	the	ELF	file	format	are
described	in	[
36
].	The	x86-64	application	binary	interface	(ABI)	describes
the	conventions	for	compiling,	linking,	and	running	x86-64	programs,
including	the	rules	for	relocation	and	position-independent	code	[
77
].</p>
<p>Homework	Problems
7.6	
♦
This	problem	concerns	the	
module	from	
Figure	
7.5
and	the
following	version	of	the	
function	that	counts	the	number	of	times	it
has	been	called:</p>
<p>For	each	symbol	that	is	defined	and	referenced	in	
,	indicate	if	it	will
have	a	symbol	table	entry	in	the	
section	in	module	
.	If	so,
indicate	the	module	that	defines	the	symbol	(
),	the	symbol
type(local,	global,	or	extern),	and	the	section	(
,	or	
)	it
occupies	in	that	module.
Symbol
entry?
Symbol
type
Module	where
defined
Section</p>
<hr />
<hr />
<hr />
<hr />
<hr />
<hr />
<hr />
<hr />
<hr />
<hr />
<hr />
<hr />
<hr />
<hr />
<hr />
<hr />
<hr />
<hr />
<hr />
<hr />
<hr />
<hr />
<hr />
<hr />
<hr />
<hr />
<hr />
<hr />
<p>7.7	
♦
Without	changing	any	variable	names,	modify	
on	page	683	so
that	
prints	the	correct	values	of	
and	
(i.e.,	the	hex
representations	of	integers	15213	and	15212).
7.8	
♦
In	this	problem,	let	
denote	that	the	linker	will
associate	an	arbitrary	reference	to	symbol	
in	module	
to	the	definition
of	
in	module	
.	For	each	example	below,	use	this	notation	to	indicate
how	the	linker	would	resolve	references	to	the	multiply-defined	symbol	in
each	module.	If	there	is	a	link-time	error	(rule	1),	write	&quot;
&quot;.	If	the	linker
arbitrarily	chooses	one	of	the	definitions	(rule	3),	write	&quot;
&quot;.
A
.	</p>
<p>B
.	
C
.	
7.9	
♦
Consider	the	following	program,	which	consists	of	two	object	modules:</p>
<p>When	this	program	is	compiled	and	executed	on	an	x86-64	Linux	system,
it	prints	the	string	
and	terminates	normally,	even	though	function
never	initializes	variable	
.	Can	you	explain	this?
7.10	
♦♦</p>
<p>Let	
and	
denote	object	modules	or	static	libraries	in	the	current
directory,	and	let	
→
denote	that	
depends	on	
,	in	the	sense	that	
defines	a	symbol	that	is	
referenced	by	
.	For	each	of	the	following
scenarios,	show	the	minimal	command	line	(i.e.,	one	with	the	least
number	of	object	file	and	library	arguments)	that	will	allow	the	static	linker
to	resolve	all	symbol	references:
A
.	
B
.	
C
.	
7.11	
♦♦
The	program	header	in	
Figure	
7.14
indicates	that	the	data	segment
occupies	
bytes	in	memory.	However,	only	the	first	
bytes	of
these	come	from	the	sections	of	the	executable	file.	What	causes	this
discrepancy?
7.12	
♦♦</p>
<p>Consider	the	call	to	function	swap	in	object	file	
(
Problem	
7.6
).
with	the	following	relocation	entry:
A
.	
Suppose	that	the	linker	relocates	
in	
to	address
and	
to	address	
.	Then	what	is	the	value	of
the	relocated	reference	to	
in	the	
instruction?
B
.	
Suppose	that	the	linker	relocates	
in	
to	address
and	
to	address	
.	Then	what	is	the	value	of
the	relocated	reference	to	
in	the	
instruction?
7.13	
♦♦
Performing	the	following	tasks	will	help	you	become	more	familiar	with
the	various	tools	for	manipulating	object	files.</p>
<p>A
.	
How	many	object	files	are	contained	in	the	versions	of	
and
on	your	system?
B
.	
Does	
produce	different	executable	code	than	
?
C
.	
What	shared	libraries	does	the	
GCC</p>
<p>driver	on	your	system	use?</p>
<p>Solutions	to	Practice	Problems
Solution	to	Problem	
7.1	
(page
678
)
The	purpose	of	this	problem	is	to	help	you	understand	the	relationship
between	linker	symbols	and	C	variables	and	functions.	Notice	that	the	C
local	variable	
does	
not
have	a	symbol	table	entry.
Symbol
entry?
Symbol	type
Module	where	defined
Section
Yes
extern
Yes
global
Yes
global
COMMON
Yes
global
No
—
—
—
Solution	to	Problem	
7.2	
(page
684
)</p>
<p>This	is	a	simple	drill	that	checks	your	understanding	of	the	rules	that	a
Unix	linker	uses	when	it	resolves	global	symbols	that	are	defined	in	more
than	one	module.	Understanding	these	rules	can	help	you	avoid	some
nasty	programming	bugs.
A
.	
The	linker	chooses	the	strong	symbol	defined	in	module	1	over	the
weak	symbol	defined	in	module	2	(rule	2):
a
.	
b
.	
B
.	
This	is	an	
,	because	each	module	defines	a	strong	symbol
(rule	1).
C
.	
The	linker	chooses	the	strong	symbol	defined	in	module	2	over	the
weak	symbol	defined	in	module	1	(rule	2):
a
.	
b
.	
Solution	to	Problem	
7.3	
(page
689
)
Placing	static	libraries	in	the	wrong	order	on	the	command	line	is	a
common	source	of	linker	errors	that	confuses	many	programmers.
However,	once	you	understand	how	linkers	use	static	libraries	to	resolve
references,	it's	pretty	straightforward.	This	little	drill	checks	your
understanding	of	this	idea:</p>
<p>A
.	
B
.	
C
.	
Solution	to	Problem	
7.4	
(page
694
)
This	problem	concerns	the	disassembly	listing	in	
Figure	
7.12(a)
.	Our
purpose	here	is	to	give	you	some	practice	reading	disassembly	listings
and	to	check	your	understanding	of	PC-relative	addressing.
A
.	
The	hex	address	of	the	relocated	reference	in	line	5	is	
.
B
.	
The	hex	value	of	the	relocated	reference	in	line	5	is	
.
Remember	that	the	disassembly	listing	shows	the	value	of	the
reference	in	little-endian	byte	order.
Solution	to	Problem	
7.5	
(page
695
)
This	problem	tests	your	understanding	of	how	the	linker	relocates	PC-
relative	references.	You	were	given	that</p>
<p>and
Using	the	algorithm	in	
Figure	
7.10
,	the	linker	first	computes	the	run-
time	address	of	the	reference:
It	then	updates	the	reference:
Thus,	in	the	resulting	executable	object	file,	the	PC-relative	reference	to
has	a	value	of	
:</p>
<p>Chapter	
8	
Exceptional	Control	Flow
8.1	
Exceptions	723
8.2	
Processes	732
8.3	
System	Call	Error	Handling	737
8.4	
Process	Control	738
8.5	
Signals	756
8.6	
Nonlocal	Jumps	781
8.7	
Tools	for	Manipulating	Processes	786
8.8	
Summary
787
Bibliographic	Notes	787
Homework	Problems	788
Solutions	to	Practice	Problems	795
From	the	time	you	first	apply	power	to	a	processor
until	the	time	you	shut	it	off,	the	program	counter
assumes	a	sequence	of	values</p>
<p>where	each	
a
is	the	address	of	some	corresponding
instruction	
I
.	Each	transition	from	
a
to	
a
is	called
a	
control	transfer
.	A	sequence	of	such	control
transfers	is	called	the	
flow	of	control
,	or	
control	flow
,
of	the	processor.
The	simplest	kind	of	control	flow	is	a	“smooth”
sequence	where	each	
I
and	
I
are	adjacent	in
memory.	Typically,	abrupt	changes	to	this	smooth
flow,	where	
I
is	not	adjacent	to	
I
,	are	caused	by
familiar	program	instructions	such	as	jumps,	calls,
and	returns.	Such	instructions	are	necessary
mechanisms	that	allow	programs	to	react	to
changes	in	internal	program	state	represented	by
program	variables.
But	systems	must	also	be	able	to	react	to	changes
in	system	state	that	are	not	captured	by	internal
program	variables	and	are	not	necessarily	related	to
the	execution	of	the	program.	For	example,	a
hardware	timer	goes	off	at	regular	intervals	and
must	be	dealt	with.	Packets	arrive	at	the	network
adapter	and	must	be	stored	in	memory.	Programs
request	data	from	a	disk	and	then	sleep	until	they
are	notified	that	the	data	are	ready.	Parent
a
0
,
a
1
,
...
,
a
n
−
1
k
k
k
k
+1
k
k
+1
k
+1
k</p>
<p>processes	that	create	child	processes	must	be
notified	when	their	children	terminate.
Modern	systems	react	to	these	situations	by	making
abrupt	changes	in	the	control	flow.	In	general,	we
refer	to	these	abrupt	changes	as	
exceptional	control
flow	(ECF
).	ECF	occurs	at	all	levels	of	a	computer
system.	For	example,	at	the	hardware	level,	events
detected	by	the	hardware	trigger	abrupt	control
transfers	to	exception	handlers.	At	the	operating
systems	level,	the	kernel	transfers	control	from	one
user	process	to	another	via	context	switches.	At	the
application	level,	a	process	can	send	a	
signal
to
another	process	that	abruptly	transfers	control	to	a
signal	handler	in	the	recipient.	An	individual
program	can	react	to	errors	by	sidestepping	the
usual	stack	discipline	and	making	nonlocal	jumps	to
arbitrary	locations	in	other	functions.
As	programmers,	there	are	a	number	of	reasons
why	it	is	important	for	you	to	understand	ECF:
Understanding	ECF	will	help	you	understand
important	systems	concepts.	
ECF	is	the	basic
mechanism	that	operating	systems	use	to
implement	I/O,	processes,	and	virtual	memory.
Before	you	can	really	understand	these
important	ideas,	you	need	to	understand	ECF.</p>
<p>Understanding	ECF	will	help	you	understand
how	applications	interact	with	the	operating
system.	
Applications	request	services	from	the
operating	system	by	using	a	form	of	ECF	known
as	a	
trap
or	
system	call
.	For	example,	writing
data	to	a	disk,	reading	data	from	a	network,
creating	a	new	process,	and	terminating	the
current	process	are	all	accomplished	by
application	programs	invoking	system	calls.
Understanding	the	basic	system	call	mechanism
will	help	you	understand	how	these	services	are
provided	to	applications.
Understanding	ECF	will	help	you	write
interesting	new	application	programs.	
The
operating	system	provides	application	programs
with	powerful	ECF	
mechanisms	for	creating	new
processes,	waiting	for	processes	to	terminate,
notifying	other	processes	of	exceptional	events
in	the	system,	and	detecting	and	responding	to
these	events.	If	you	understand	these	ECF
mechanisms,	then	you	can	use	them	to	write
interesting	programs	such	as	Unix	shells	and
Web	servers.
Understanding	ECF	will	help	you	understand
concurrency.	
ECF	is	a	basic	mechanism	for
implementing	concurrency	in	computer	systems.
The	following	are	all	examples	of	concurrency	in
action:	an	exception	handler	that	interrupts	the
execution	of	an	application	program;	processes</p>
<p>and	threads	whose	execution	overlap	in	time;
and	a	signal	handler	that	interrupts	the	execution
of	an	application	program.	Understanding	ECF	is
a	first	step	to	understanding	concurrency.	We	will
return	to	study	it	in	more	detail	in	
Chapter	
12
.
Understanding	ECF	will	help	you	understand
how	software	exceptions	work.	
Languages
such	as	C++	and	Java	provide	software
exception	mechanisms	via	
,	and	
statements.	Software	exceptions	allow	the
program	to	make	
nonlocal
jumps	(i.e.,	jumps	that
violate	the	usual	call/return	stack	discipline)	in
response	to	error	conditions.	Nonlocal	jumps	are
a	form	of	application-level	ECF	and	are	provided
in	C	via	the	
and	
functions.
Understanding	these	low-level	functions	will	help
you	understand	how	higher-level	software
exceptions	can	be	implemented.
Up	to	this	point	in	your	study	of	systems,	you	have
learned	how	applications	interact	with	the	hardware.
This	chapter	is	pivotal	in	the	sense	that	you	will
begin	to	learn	how	your	applications	interact	with
the	operating	system.	Interestingly,	these
interactions	all	revolve	around	ECF.	We	describe	the
various	forms	of	ECF	that	exist	at	all	levels	of	a
computer	system.	We	start	with	exceptions,	which
lie	at	the	intersection	of	the	hardware	and	the
operating	system.	We	also	discuss	system	calls,</p>
<p>which	are	exceptions	that	provide	applications	with
entry	points	into	the	operating	system.	We	then
move	up	a	level	of	abstraction	and	describe
processes	and	signals,	which	lie	at	the	intersection
of	applications	and	the	operating	system.	Finally,	we
discuss	nonlocal	jumps,	which	are	an	application-
level	form	of	ECF.</p>
<p>8.1	
Exceptions
Exceptions	are	a	form	of	exceptional	control	flow	that	are	implemented
partly	by	the	hardware	and	partly	by	the	operating	system.	Because	they
are	partly	implemented	in	hardware,	the	details	vary	from	system	to
system.	However,	the	basic	ideas	are	the	same	for	every	system.	Our
aim	in	this	section	is	to	give	you	a	general	understanding	of	exceptions
and	exception	handling	and	to	help	demystify	what	is	often	a	confusing
aspect	of	modern	computer	systems.
An	
exception
is	an	abrupt	change	in	the	control	flow	in	response	to	some
change	in	the	processor's	state.	
Figure	
8.1
shows	the	basic	idea.
In	the	figure,	the	processor	is	executing	some	current	instruction	
I
when	a	significant	change	in	the	processor's	
state
occurs.	The	state	is
encoded	in	various	bits	and	signals	inside	the	processor.	The	change	in
state	is	known	as	an	
event
.
Aside	
Hardware	versus	software
exceptions
C++	and	Java	programmers	will	have	noticed	that	the	term
“exception”	is	also	used	to	describe	the	application-level	ECF
mechanism	provided	by	C++	and	Java	in	the	form	of	
,	and	
statements.	If	we	wanted	to	be	perfectly	clear,	we
might	distinguish	between	“hardware”	and	“software”	exceptions,
curr</p>
<p>but	this	is	usually	unnecessary	because	the	meaning	is	clear	from
the	context.
Figure	
8.1	
Anatomy	of	an	exception.
A	change	in	the	processor's	state	(an	event)	triggers	an	abrupt	control
transfer	(an	exception)	from	the	application	program	to	an	exception
handler.	After	it	finishes	processing,	the	handler	either	returns	control	to
the	interrupted	program	or	aborts.
The	event	might	be	directly	related	to	the	execution	of	the	current
instruction.	For	example,	a	virtual	memory	page	fault	occurs,	an
arithmetic	overflow	occurs,	or	an	instruction	attempts	a	divide	by	zero.	On
the	other	hand,	the	event	might	be	unrelated	to	the	execution	of	the
current	instruction.	For	example,	a	system	timer	goes	off	or	an	I/O
request	completes.
In	any	case,	when	the	processor	detects	that	the	event	has	occurred,	it
makes	an	indirect	procedure	call	(the	exception),	through	a	jump	table
called	an	
exception	table
,	to	an	operating	system	subroutine	(the
exception	handler
)	that	is	specifically	designed	to	process	this	particular
kind	of	event.	When	the	exception	handler	finishes	processing,	one	of</p>
<p>three	things	happens,	depending	on	the	type	of	event	that	caused	the
exception:
1
.	
The	handler	returns	control	to	the	current	instruction	
I
,	the
instruction	that	was	executing	when	the	event	occurred.
2
.	
The	handler	returns	control	to	
I
,	the	instruction	that	would	have
executed	next	had	the	exception	not	occurred.
3
.	
The	handler	aborts	the	interrupted	program.
Section	
8.1.2
says	more	about	these	possibilities.
8.1.1	
Exception	Handling
Exceptions	can	be	difficult	to	understand	because	handling	them	involves
close	cooperation	between	hardware	and	software.	It	is	easy	to	get
confused	about
Figure	
8.2	
Exception	table.
curr
next</p>
<p>The	exception	table	is	a	jump	table	where	entry	
k
contains	the	address	of
the	handler	code	for	exception	
k
.
Figure	
8.3	
Generating	the	address	of	an	exception	handler.
The	exception	number	is	an	index	into	the	exception	table.
which	component	performs	which	task.	Let's	look	at	the	division	of	labor
between	hardware	and	software	in	more	detail.
Each	type	of	possible	exception	in	a	system	is	assigned	a	unique
nonnegative	integer	
exception	number
.	Some	of	these	numbers	are
assigned	by	the	designers	of	the	processor.	Other	numbers	are	assigned
by	the	designers	of	the	operating	system	
kernel
(the	memory-resident
part	of	the	operating	system).	Examples	of	the	former	include	divide	by
zero,	page	faults,	memory	access	violations,	breakpoints,	and	arithmetic
overflows.	Examples	of	the	latter	include	system	calls	and	signals	from
external	I/O	devices.
At	system	boot	time	(when	the	computer	is	reset	or	powered	on),	the
operating	system	allocates	and	initializes	a	jump	table	called	an
exception	table
,	so	that	entry	
k
contains	the	address	of	the	handler	for
exception	
k
.	
Figure	
8.2
shows	the	format	of	an	exception	table.
At	run	time	(when	the	system	is	executing	some	program),	the	processor
detects	that	an	event	has	occurred	and	determines	the	corresponding</p>
<p>exception	number	
k
.	The	processor	then	triggers	the	exception	by
making	an	indirect	procedure	call,	through	entry	
k
of	the	exception	table,
to	the	corresponding	handler.	
Figure	
8.3
shows	how	the	processor
uses	the	exception	table	to	form	the	address	of	the	appropriate	exception
handler.	The	exception	number	is	an	index	into	the	exception	table,
whose	starting	address	is	contained	in	a	special	CPU	register	called	the
exception	table	base	register
.
An	exception	is	akin	to	a	procedure	call,	but	with	some	important
differences:
As	with	a	procedure	call,	the	processor	pushes	a	return	address	on
the	stack	before	branching	to	the	handler.	However,	depending	on	the
class	of	exception,	the	return	address	is	either	the	current	instruction
(the	instruction	that	
was	executing	when	the	event	occurred)	or	the
next	instruction	(the	instruction	that	would	have	executed	after	the
current	instruction	had	the	event	not	occurred).
The	processor	also	pushes	some	additional	processor	state	onto	the
stack	that	will	be	necessary	to	restart	the	interrupted	program	when
the	handler	returns.	For	example,	an	x86-64	system	pushes	the
EFLAGS	register	containing	the	current	condition	codes,	among	other
things,	onto	the	stack.
When	control	is	being	transferred	from	a	user	program	to	the	kernel,
all	of	these	items	are	pushed	onto	the	kernel's	stack	rather	than	onto
the	user's	stack.
Exception	handlers	run	in	
kernel	mode
(
Section	
8.2.4
),	which
means	they	have	complete	access	to	all	system	resources.
Once	the	hardware	triggers	the	exception,	the	rest	of	the	work	is	done	in
software	by	the	exception	handler.	After	the	handler	has	processed	the</p>
<p>event,	it	optionally	returns	to	the	interrupted	program	by	executing	a
special	“return	from	interrupt”	instruction,	which	pops	the	appropriate
state	back	into	the	processor's	control	and	data	registers,	restores	the
state	to	
user	mode
(
Section	
8.2.4
)	if	the	exception	interrupted	a	user
program,	and	then	returns	control	to	the	interrupted	program.
8.1.2	
Classes	of	Exceptions
Exceptions	can	be	divided	into	four	classes:	
interrupts
,	
traps
,	
faults
,	and
aborts
.	The	table	in	
Figure	
8.4
summarizes	the	attributes	of	these
classes.
Interrupts
Interrupts
occur	
asynchronously
as	a	result	of	signals	from	I/O	devices
that	are	external	to	the	processor.	Hardware	interrupts	are	asynchronous
in	the	sense	that	they	are	not	caused	by	the	execution	of	any	particular
instruction.	Exception	handlers	for	hardware	interrupts	are	often	called
interrupt	handlers
.
Figure	
8.5	
summarizes	the	processing	for	an	interrupt.	I/O	devices
such	as	network	adapters,	disk	controllers,	and	timer	chips	trigger
interrupts	by	signaling	a	pin	on	the	processor	chip	and	placing	onto	the
system	bus	the	exception	number	that	identifies	the	device	that	caused
the	interrupt.
Class
Cause
Async/sync
Return	behavior</p>
<p>Interrupt
Signal	from	I/O	device
Async
Always	returns	to	next	instruction
Trap
Intentional	exception
Sync
Always	returns	to	next	instruction
Fault
Potentially	recoverable	error
Sync
Might	return	to	current	instruction
Abort
Nonrecoverable	error
Sync
Never	returns
Figure	
8.4	
Classes	of	exceptions.
Asynchronous	exceptions	occur	as	a	result	of	events	in	I/O	devices	that
are	external	to	the	processor.	Synchronous	exceptions	occur	as	a	direct
result	of	executing	an	instruction.
Figure	
8.5	
Interrupt	handling.
The	interrupt	handler	returns	control	to	the	next	instruction	in	the
application	program's	control	flow.</p>
<p>Figure	
8.6	
Trap	handling.
The	trap	handler	returns	control	to	the	next	instruction	in	the	application
program's	control	flow.
After	the	current	instruction	finishes	executing,	the	processor	notices	that
the	interrupt	pin	has	gone	high,	reads	the	exception	number	from	the
system	bus,	and	then	calls	the	appropriate	interrupt	handler.	When	the
handler	returns,	it	returns	control	to	the	next	instruction	(i.e.,	the
instruction	that	would	have	followed	the	current	instruction	in	the	control
flow	had	the	interrupt	not	occurred).	The	effect	is	that	the	program
continues	executing	as	though	the	interrupt	had	never	happened.
The	remaining	classes	of	exceptions	(traps,	faults,	and	aborts)	occur
synchronously
as	a	result	of	executing	the	current	instruction.	We	refer	to
this	instruction	as	the	
faulting	instruction
.
Traps	and	System	Calls
Traps
are	
intentional
exceptions	that	occur	as	a	result	of	executing	an
instruction.	Like	interrupt	handlers,	trap	handlers	return	control	to	the	next
instruction.	The	most	important	use	of	traps	is	to	provide	a	procedure-like
interface	between	user	programs	and	the	kernel,	known	as	a	
system	call
.
User	programs	often	need	to	request	services	from	the	kernel	such	as
reading	a	file	(
),	creating	a	new	process	(
),	loading	a	new
program	(
),	and	terminating	the	current	process	(
).	To	allow
controlled	access	to	such	kernel	services,	processors	provide	a	special</p>
<p>n
instruction	that	user	programs	can	execute	when	they	want	to
request	service	
n
.	Executing	the	
instruction	causes	a	trap	to	an</p>
<p>exception	handler	that	decodes	the	argument	and	calls	the	appropriate
kernel	routine.	
Figure	
8.6
summarizes	the	processing	for	a	system
call.
From	a	programmer's	perspective,	a	system	call	is	identical	to	a	regular
function	call.	However,	their	implementations	are	quite	different.	Regular
functions
Figure	
8.7	
Fault	handling.
Depending	on	whether	the	fault	can	be	repaired	or	not,	the	fault	handler
either	re-executes	the	faulting	instruction	or	aborts.
Figure	
8.8	
Abort	handling.
The	abort	handler	passes	control	to	a	kernel	
routine	that
terminates	the	application	program.</p>
<p>run	in	
user	mode
,	which	restricts	the	types	of	instructions	they	can
execute,	and	they	access	the	same	stack	as	the	calling	function.	A
system	call	runs	in	
kernel	mode
,	which	allows	it	to	execute	privileged
instructions	and	access	a	stack	defined	in	the	kernel.	
Section	
8.2.4
discusses	user	and	kernel	modes	in	more	detail.
Faults
Faults	result	from	error	conditions	that	a	handler	might	be	able	to	correct.
When	a	fault	occurs,	the	processor	transfers	control	to	the	fault	handler.	If
the	handler	is	able	to	correct	the	error	condition,	it	returns	control	to	the
faulting	instruction,	thereby	re-executing	it.	Otherwise,	the	handler	returns
to	an	
routine	in	the	kernel	that	terminates	the	application	program
that	caused	the	fault.	
Figure	
8.7	
summarizes	the	processing	for	a
fault.
A	classic	example	of	a	fault	is	the	page	fault	exception,	which	occurs
when	an	instruction	references	a	virtual	address	whose	corresponding
page	is	not	resident	in	memory	and	must	therefore	be	retrieved	from	disk.
As	we	will	see	in	
Chapter	
9
,	a	page	is	a	contiguous	block	(typically	4
KB)	of	virtual	memory.	The	page	fault	handler	loads	the	appropriate	page
from	disk	and	then	returns	control	to	the	instruction	that	caused	the	fault.
When	the	instruction	executes	again,	the	appropriate	page	is	now
resident	in	memory	and	the	instruction	is	able	to	run	to	completion
without	faulting.
Aborts</p>
<p>Aborts	result	from	unrecoverable	fatal	errors,	typically	hardware	errors
such	as	parity	errors	that	occur	when	DRAM	or	SRAM	bits	are	corrupted.
Abort	handlers	never	return	control	to	the	application	program.	As	shown
in	
Figure	
8.8
,	the	handler	returns	control	to	an	
routine	that
terminates	the	application	program.
Exception	number
Description
Exception	class
0
Divide	error
Fault
13
General	protection	fault
Fault
14
Page	fault
Fault
18
Machine	check
Abort
32-255
OS-defined	exceptions
Interrupt	or	trap
Figure	
8.9	
Examples	of	exceptions	in	x86-64	systems.
8.1.3	
Exceptions	in	Linux/x86-64
Systems
To	help	make	things	more	concrete,	let's	look	at	some	of	the	exceptions
defined	for	x86-64	systems.	There	are	up	to	256	different	exception	types
[
50
].	Numbers	in	the	range	from	0	to	31	correspond	to	exceptions	that
are	defined	by	the	Intel	architects	and	thus	are	identical	for	any	x86-64
system.	Numbers	in	the	range	from	32	to	255	correspond	to	interrupts</p>
<p>and	traps	that	are	defined	by	the	operating	system.	
Figure	
8.9	
shows
a	few	examples.
Linux/x86-64	Faults	and	Aborts
Divide	error.	
A	divide	error	(exception	0)	occurs	when	an	application
attempts	to	divide	by	zero	or	when	the	result	of	a	divide	instruction	is
too	big	for	the	destination	operand.	Unix	does	not	attempt	to	recover
from	divide	errors,	opting	instead	to	abort	the	program.	Linux	shells
typically	report	divide	errors	as	“Floating	exceptions.”
General	protection	fault.	
The	infamous	general	protection	fault
(exception	13)	occurs	for	many	reasons,	usually	because	a	program
references	an	undefined	area	of	virtual	memory	or	because	the
program	attempts	to	write	to	a	read-only	text	segment.	Linux	does	not
attempt	to	recover	from	this	fault.	Linux	shells	typically	report	general
protection	faults	as	“Segmentation	faults.”
Page	fault.	
A	page	fault	(exception	14)	is	an	example	of	an	exception
where	the	faulting	instruction	is	restarted.	The	handler	maps	the
appropriate	page	of	virtual	memory	on	disk	into	a	page	of	physical
memory	and	then	restarts	the	faulting	instruction.	We	will	see	how
page	faults	work	in	detail	in	
Chapter	
9
.
Machine	check.	
A	machine	check	(exception	18)	occurs	as	a	result
of	a	fatal	hardware	error	that	is	detected	during	the	execution	of	the
faulting	instruction.	Machine	check	handlers	never	return	control	to
the	application	program.
Linux/x86-64	System	Calls</p>
<p>Linux	provides	hundreds	of	system	calls	that	application	programs	use
when	they	want	to	request	services	from	the	kernel,	such	as	reading	a
file,	writing	a	file,	and
Number
Name
Description
Number
Name
Description
0
Read	file
33
Suspend	process	until
signal	arrives
1
Write	file
37
Schedule	delivery	of
alarm	signal
2
Open	file
39
Get	process	ID
3
Close	file
57
Create	process
4
Get	info	about	file
59
Execute	a	program
9
Map	memory	page
to	file
60
Terminate	process
12
Reset	the	top	of
the	heap
61
Wait	for	a	process	to
terminate
32
Copy	file	descriptor
62
Send	signal	to	a
process
Figure	
8.10	
Examples	of	popular	system	calls	in	Linux	x86-64
systems.
creating	a	new	process.	
Figure	
8.10	
lists	some	popular	Linux	system
calls.	Each	system	call	has	a	unique	integer	number	that	corresponds	to</p>
<p>an	offset	in	a	jump	table	in	the	kernel.	(Notice	that	this	jump	table	is	not
the	same	as	the	exception	table.)
C	programs	can	invoke	any	system	call	directly	by	using	the	
function.	However,	this	is	rarely	necessary	in	practice.	The	C	standard
library	provides	a	set	of	convenient	wrapper	functions	for	most	system
calls.	The	wrapper	functions	package	up	the	arguments,	trap	to	the
kernel	with	the	appropriate	system	call	instruction,	and	then	pass	the
return	status	of	the	system	call	back	to	the	calling	program.	Throughout
this	text,	we	will	refer	to	system	calls	and	their	associated	wrapper
functions	interchangeably	as	
system-level	functions
.
System	calls	are	provided	on	x86-64	systems	via	a	trapping	instruction
called	
.	It	is	quite	interesting	to	study	how	programs	can	use	this
instruction	to	invoke	Linux	system	calls	directly.	All	arguments	to	Linux
system	calls	are	passed	through	general-purpose	registers	rather	than
the	stack.	By	convention,	register	
contains	the	syscall	number,	with
up	to	six	arguments	in	
,	and	
.	The	first
argument	is	in	
,	the	second	in	
,	and	so	on.	On	return	from	the
system	call,	registers	
and	
are	destroyed,	and	
contains
the	return	value.	A	negative	return	value	between	-4,095	and	-1	indicates
an	error	corresponding	to	negative	
.
For	example,	consider	the	following	version	of	the	familiar	
program,	written	using	the	
system-level	function	(
Section	
10.4
)
instead	of	
:</p>
<h2>The	first	argument	to	
sends	the	output	to	
.	The	second
argument	is	the	sequence	of	bytes	to	write,	and	the	third	argument	gives
the	number	of	bytes	to	write.
Aside	
A	note	on	terminology
The	terminology	for	the	various	classes	of	exceptions	varies	from
system	to	system.	Processor	ISA	specifications	often	distinguish
between	asynchronous	“interrupts”	and	synchronous	“exceptions”
yet	provide	no	umbrella	term	to	refer	to	these	very	similar
concepts.	To	avoid	having	to	constantly	refer	to	“exceptions	and
interrupts”	and	“exceptions	or	interrupts,”	we	use	the	word
“exception”	as	the	general	term	and	distinguish	between
asynchronous	exceptions	(interrupts)	and	synchronous	exceptions
(traps,	faults,	and	aborts)	only	when	it	is	appropriate.	As	we	have
noted,	the	basic	ideas	are	the	same	for	every	system,	but	you
should	be	aware	that	some	manufacturers'	manuals	use	the	word
“exception”	to	refer	only	to	those	changes	in	control	flow	caused
by	synchronous	events.</h2>
<hr />
<p>code/ecf/hello-asm64.sa</p>
<hr />
<hr />
<p>code/ecf/hello-asm64.sa
Figure	
8.11	
Implementing	the	
program	directly	with	Linux
system	calls.</p>
<p>Figure	
8.11
shows	an	assembly-language	version	of	
that	uses
the	
instruction	to	invoke	the	
and	
system	calls	directly.
Lines	9-13	invoke	the	
function.	First,	line	9	stores	the	number	of
the	
system	call	in	
,	and	lines	10-12	set	up	the	argument	list.
Then,	line	13	uses	the	
instruction	to	invoke	the	system	call.
Similarly,	lines	14-16	invoke	the	
system	call.</p>
<p>8.2	
Processes
Exceptions	are	the	basic	building	blocks	that	allow	the	operating	system
kernel	to	provide	the	notion	of	a	
process
,	one	of	the	most	profound	and
successful	ideas	in	computer	science.
When	we	run	a	program	on	a	modern	system,	we	are	presented	with	the
illusion	that	our	program	is	the	only	one	currently	running	in	the	system.
Our	program	appears	to	have	exclusive	use	of	both	the	processor	and
the	memory.	The	processor	appears	to	execute	the	instructions	in	our
program,	one	after	the	other,	without	interruption.	Finally,	the	code	and
data	of	our	program	appear	to	be	the	only	objects	in	the	system's
memory.	These	illusions	are	provided	to	us	by	the	notion	of	a	process.
The	classic	definition	of	a	process	is	
an	instance	of	a	program	in
execution
.	Each	program	in	the	system	runs	in	the	
context
of	some
process.	The	context	consists	of	the	state	that	the	program	needs	to	run
correctly.	This	state	includes	the	program's	code	and	data	stored	in
memory,	its	stack,	the	contents	of	its	general	purpose	registers,	its
program	counter,	environment	variables,	and	the	set	of	open	file
descriptors.
Each	time	a	user	runs	a	program	by	typing	the	name	of	an	executable
object	file	to	the	shell,	the	shell	creates	a	new	process	and	then	runs	the
executable	object	file	in	the	context	of	this	new	process.	Application
programs	can	also	create	new	processes	and	run	either	their	own	code
or	other	applications	in	the	context	of	the	new	process.</p>
<p>A	detailed	discussion	of	how	operating	systems	implement	processes	is
beyond	our	scope.	Instead,	we	will	focus	on	the	key	abstractions	that	a
process	provides	to	the	application:
An	independent	logical	control	flow	that	provides	the	illusion	that	our
program	has	exclusive	use	of	the	processor.
A	private	address	space	that	provides	the	illusion	that	our	program
has	exclusive	use	of	the	memory	system.
Let's	look	more	closely	at	these	abstractions.
8.2.1	
Logical	Control	Flow
A	process	provides	each	program	with	the	illusion	that	it	has	exclusive
use	of	the	processor,	even	though	many	other	programs	are	typically
running	concurrently	on	the	system.	If	we	were	to	use	a	debugger	to
single-step	the	execution	of	our	program,	we	would	observe	a	series	of
program	counter	(PC)	values	that	corresponded	exclusively	to
instructions	contained	in	our	program's	executable	object	file	or	in	shared
objects	linked	into	our	program	dynamically	at	run	time.	This	sequence	of
PC	values	is	known	as	a	
logical	control	flow
,	or	simply	
logical	flow
.
Consider	a	system	that	runs	three	processes,	as	shown	in	
Figure
8.12
.	The	single	physical	control	flow	of	the	processor	is	partitioned
into	three	logical	flows,	one	for	each	process.	Each	vertical	line
represents	a	portion	of	the	logical	flow	for</p>
<p>Figure	
8.12	
Logical	control	flows.
Processes	provide	each	program	with	the	illusion	that	it	has	exclusive
use	of	the	processor.	Each	vertical	bar	represents	a	portion	of	the	logical
control	flow	for	a	process.
a	process.	In	the	example,	the	execution	of	the	three	logical	flows	is
interleaved.	Process	A	runs	for	a	while,	followed	by	B,	which	runs	to
completion.	Process	C	then	runs	for	a	while,	followed	by	A,	which	runs	to
completion.	Finally,	C	is	able	to	run	to	completion.
The	key	point	in	
Figure	
8.12
is	that	processes	take	turns	using	the
processor.	Each	process	executes	a	portion	of	its	flow	and	then	is
preempted
(temporarily	suspended)	while	other	processes	take	their
turns.	To	a	program	running	in	the	context	of	one	of	these	processes,	it
appears	to	have	exclusive	use	of	the	processor.	The	only	evidence	to	the
contrary	is	that	if	we	were	to	precisely	measure	the	elapsed	time	of	each
instruction,	we	would	notice	that	the	CPU	appears	to	periodically	stall
between	the	execution	of	some	of	the	instructions	in	our	program.
However,	each	time	the	processor	stalls,	it	subsequently	resumes
execution	of	our	program	without	any	change	to	the	contents	of	the
program's	memory	locations	or	registers.</p>
<p>8.2.2	
Concurrent	Flows
Logical	flows	take	many	different	forms	in	computer	systems.	Exception
handlers,	processes,	signal	handlers,	threads,	and	Java	processes	are
all	examples	of	logical	flows.
A	logical	flow	whose	execution	overlaps	in	time	with	another	flow	is	called
a	
concurrent	flow
,	and	the	two	flows	are	said	to	
run	concurrently
.	More
precisely,	flows	X	and	Y	are	concurrent	with	respect	to	each	other	if	and
only	if	X	begins	after	Y	begins	and	before	Y	finishes,	or	Y	begins	after	X
begins	and	before	X	finishes.	For	example,	in	
Figure	
8.12
,	processes
A	and	B	run	concurrently,	as	do	A	and	C.	On	the	other	hand,	B	and	C	do
not	run	concurrently,	because	the	last	instruction	of	B	executes	before
the	first	instruction	of	C.
The	general	phenomenon	of	multiple	flows	executing	concurrently	is
known	as	
concurrency
.	The	notion	of	a	process	taking	turns	with	other
processes	is	also	known	as	
multitasking
.	Each	time	period	that	a	process
executes	a	portion	of	its	flow	is	called	a	
time	slice
.	Thus,	multitasking	is
also	referred	to	as	
time	slicing
.	For	example,	in	
Figure	
8.12
,	the	flow
for	process	A	consists	of	two	time	slices.
Notice	that	the	idea	of	concurrent	flows	is	independent	of	the	number	of
processor	cores	or	computers	that	the	flows	are	running	on.	If	two	flows
overlap	in	time,	then	they	are	concurrent,	even	if	they	are	running	on	the
same	processor.	However,	we	will	sometimes	find	it	useful	to	identify	a
proper	subset	of	concurrent	
flows	known	as	
parallel	flows
.	If	two	flows
are	running	concurrently	on	different	processor	cores	or	computers,	then</p>
<p>we	say	that	they	are	
parallel	flows
,	that	they	are	
running	in	parallel
,	and
have	
parallel	execution
.
Practice	Problem	
8.1	
(solution	page	
795
)
Consider	three	processes	with	the	following	starting	and	ending	times:
Process
Start	time
End	time
A
0
2
B
1
4
C
3
5
For	each	pair	of	processes,	indicate	whether	they	run	concurrently	(Y)	or
not	(N):
Process	pair
Concurrent?
AB</p>
<p>AC</p>
<p>BC</p>
<p>8.2.3	
Private	Address	Space
A	process	provides	each	program	with	the	illusion	that	it	has	exclusive
use	of	the	system's	address	space.	On	a	machine	with	
n
-bit	addresses,
n
n</p>
<p>the	
address	space
is	the	set	of	2
possible	addresses,	0,	1,	...	,	2
-	1.	A
process	provides	each	program	with	its	own	
private	address	space
.	This
space	is	private	in	the	sense	that	a	byte	of	memory	associated	with	a
particular	address	in	the	space	cannot	in	general	be	read	or	written	by
any	other	process.
Although	the	contents	of	the	memory	associated	with	each	private
address	space	is	different	in	general,	each	such	space	has	the	same
general	organization.	For	example,	
Figure	
8.13
shows	the
organization	of	the	address	space	for	an	x86-64	Linux	process.
The	bottom	portion	of	the	address	space	is	reserved	for	the	user
program,	with	the	usual	code,	data,	heap,	and	stack	segments.	The	code
segment	always	begins	at	address	
.	The	top	portion	of	the
address	space	is	reserved	for	the	kernel	(the	memory-resident	part	of	the
operating	system).	This	part	of	the	address	space	contains	the	code,
data,	and	stack	that	the	kernel	uses	when	it	executes	instructions	on
behalf	of	the	process	(e.g.,	when	the	application	program	executes	a
system	call).
8.2.4	
User	and	Kernel	Modes
In	order	for	the	operating	system	kernel	to	provide	an	airtight	process
abstraction,	the	processor	must	provide	a	mechanism	that	restricts	the
instructions	that	an
n
n</p>
<p>Figure	
8.13	
Process	address	space.
application	can	execute,	as	well	as	the	portions	of	the	address	space	that
it	can	access.
Processors	typically	provide	this	capability	with	a	
mode	bit
in	some
control	register	that	characterizes	the	privileges	that	the	process	currently
enjoys.	When	the	mode	bit	is	set,	the	process	is	running	in	
kernel	mode
(sometimes	called	
supervisor	mode
).	A	process	running	in	kernel	mode
can	execute	any	instruction	in	the	instruction	set	and	access	any	memory
location	in	the	system.</p>
<p>When	the	mode	bit	is	not	set,	the	process	is	running	in	
user	mode
.	A
process	in	user	mode	is	not	allowed	to	execute	
privileged	instructions
that	do	things	such	as	halt	the	processor,	change	the	mode	bit,	or	initiate
an	I/O	operation.	Nor	is	it	allowed	to	directly	reference	code	or	data	in	the
kernel	area	of	the	address	space.	Any	such	attempt	results	in	a	fatal
protection	fault.	User	programs	must	instead	access	kernel	code	and
data	indirectly	via	the	system	call	interface.
A	process	running	application	code	is	initially	in	user	mode.	The	only	way
for	the	process	to	change	from	user	mode	to	kernel	mode	is	via	an
exception	such	as	an	interrupt,	a	fault,	or	a	trapping	system	call.	When
the	exception	occurs,	and	control	passes	to	the	exception	handler,	the
processor	changes	the	mode	from	user	mode	to	kernel	mode.	The
handler	runs	in	kernel	mode.	When	it	returns	to	the	application	code,	the
processor	changes	the	mode	from	kernel	mode	back	to	user	mode.
Linux	provides	a	clever	mechanism,	called	the	
filesystem,	that
allows	user	mode	processes	to	access	the	contents	of	kernel	data
structures.	The	
filesystem	exports	the	contents	of	many	kernel
data	structures	as	a	hierarchy	of	text	
files	that	can	be	read	by	user
programs.	For	example,	you	can	use	the	
filesystem	to	find	out
general	system	attributes	such	as	CPU	type	(
),	or	the
memory	segments	used	by	a	particular	process	(
).
The	2.6	version	of	the	Linux	kernel	introduced	a	
filesystem,	which
exports	additional	low-level	information	about	system	buses	and	devices.
8.2.5	
Context	Switches</p>
<p>The	operating	system	kernel	implements	multitasking	using	a	higher-level
form	of	exceptional	control	flow	known	as	a	
context	switch
.	The	context
switch	mechanism	is	built	on	top	of	the	lower-level	exception	mechanism
that	we	discussed	in	
Section	
8.1
.
The	kernel	maintains	a	
context
for	each	process.	The	context	is	the	state
that	the	kernel	needs	to	restart	a	preempted	process.	It	consists	of	the
values	of	objects	such	as	the	general-purpose	registers,	the	floating-point
registers,	the	program	counter,	user's	stack,	status	registers,	kernel's
stack,	and	various	kernel	data	structures	such	as	a	
page	table
that
characterizes	the	address	space,	a	
process	table
that	contains
information	about	the	current	process,	and	a	
file	table
that	contains
information	about	the	files	that	the	process	has	opened.
At	certain	points	during	the	execution	of	a	process,	the	kernel	can	decide
to	preempt	the	current	process	and	restart	a	previously	preempted
process.	This	decision	is	known	as	
scheduling
and	is	handled	by	code	in
the	kernel,	called	the	
scheduler
.	When	the	kernel	selects	a	new	process
to	run,	we	say	that	the	kernel	has	
scheduled
that	process.	After	the
kernel	has	scheduled	a	new	process	to	run,	it	preempts	the	current
process	and	transfers	control	to	the	new	process	using	a	mechanism
called	a	
context	switch
that	(1)	saves	the	context	of	the	current	process,
(2)	restores	the	saved	context	of	some	previously	preempted	process,
and	(3)	passes	control	to	this	newly	restored	process.
A	context	switch	can	occur	while	the	kernel	is	executing	a	system	call	on
behalf	of	the	user.	If	the	system	call	blocks	because	it	is	waiting	for	some
event	to	occur,	then	the	kernel	can	put	the	current	process	to	sleep	and
switch	to	another	process.	For	example,	if	a	
system	call	requires	a</p>
<p>disk	access,	the	kernel	can	opt	to	perform	a	context	switch	and	run
another	process	instead	of	waiting	for	the	data	to	arrive	from	the	disk.
Another	example	is	the	
system	call,	which	is	an	explicit	request	to
put	the	calling	process	to	sleep.	In	general,	even	if	a	system	call	does	not
block,	the	kernel	can	decide	to	perform	a	context	switch	rather	than
return	control	to	the	calling	process.
A	context	switch	can	also	occur	as	a	result	of	an	interrupt.	For	example,
all	systems	have	some	mechanism	for	generating	periodic	timer
interrupts,	typically	every	1	ms	or	10	ms.	Each	time	a	timer	interrupt
occurs,	the	kernel	can	decide	that	the	current	process	has	run	long
enough	and	switch	to	a	new	process.
Figure	
8.13	
shows	an	example	of	context	switching	between	a	pair	of
processes	A	and	B.	In	this	example,	initially	process	A	is	running	in	user
mode	until	it	traps	to	the	kernel	by	executing	a	
system	call.	The	trap
handler	in	the	kernel	requests	a	DMA	transfer	from	the	disk	controller	and
arranges	for	the	disk	to	interrupt	the
Figure	
8.14	
Anatomy	of	a	process	context	switch.</p>
<p>processor	after	the	disk	controller	has	finished	transferring	the	data	from
disk	to	memory.
The	disk	will	take	a	relatively	long	time	to	fetch	the	data	(on	the	order	of
tens	of	milliseconds),	so	instead	of	waiting	and	doing	nothing	in	the
interim,	the	kernel	performs	a	context	switch	from	process	A	to	B.	Note
that,	before	the	switch,	the	kernel	is	executing	instructions	in	user	mode
on	behalf	of	process	A	(i.e.,	there	is	no	separate	kernel	process).	During
the	first	part	of	the	switch,	the	kernel	is	executing	instructions	in	kernel
mode	on	behalf	of	process	A.	Then	at	some	point	it	begins	executing
instructions	(still	in	kernel	mode)	on	behalf	of	process	B.	And	after	the
switch,	the	kernel	is	executing	instructions	in	user	mode	on	behalf	of
process	B.
Process	B	then	runs	for	a	while	in	user	mode	until	the	disk	sends	an
interrupt	to	signal	that	data	have	been	transferred	from	disk	to	memory.
The	kernel	decides	that	process	B	has	run	long	enough	and	performs	a
context	switch	from	process	B	to	A,	returning	control	in	process	A	to	the
instruction	immediately	following	the	
system	call.	Process	A
continues	to	run	until	the	next	exception	occurs,	and	so	on.</p>
<p>8.3	
System	Call	Error	Handling
When	Unix	system-level	functions	encounter	an	error,	they	typically
return	-1	and	set	the	global	integer	variable	
to	indicate	what	went
wrong.	Programmers	should	
always
check	for	errors,	but	unfortunately,
many	skip	error	checking	because	it	bloats	the	code	and	makes	it	harder
to	read.	For	example,	here	is	how	we	might	check	for	errors	when	we	call
the	Linux	
function:
The	
function	returns	a	text	string	that	describes	the	error
associated	with	a	particular	value	of	
We	can	simplify	this	code
somewhat	by	defining	the	following	
error-reporting	function:</p>
<p>Given	this	function,	our	call	to	
reduces	from	four	lines	to	two	lines:
We	can	simplify	our	code	even	further	by	using	
error-handling	wrappers
,
as	pioneered	by	Stevens	in	[
110
].	For	a	given	base	function	
,	we
define	a	wrapper	function	
with	identical	arguments	but	with	the	first
letter	of	the	name	capitalized.	The	wrapper	calls	the	base	function,
checks	for	errors,	and	terminates	if	there	are	any	problems.	For	example,
here	is	the	error-handling	wrapper	for	the	
function:</p>
<p>Given	this	wrapper,	our	call	to	
shrinks	to	a	single	compact	line:
We	will	use	error-handling	wrappers	throughout	the	remainder	of	this
book.	They	allow	us	to	keep	our	code	examples	concise	without	giving
you	the	mistaken	impression	that	it	is	permissible	to	ignore	error
checking.	Note	that	when	we	discuss	system-level	functions	in	the	text,
we	will	always	refer	to	them	by	their	lowercase	base	names,	rather	than
by	their	uppercase	wrapper	names.
See	Appendix	A	for	a	discussion	of	Unix	error	handling	and	the	error-
handling	wrappers	used	throughout	this	book.	The	wrappers	are	defined
in	a	file	called	
,	and	their	prototypes	are	defined	in	a	header	file
called	
These	are	available	online	from	the	CS:APP	Web	site.</p>
<p>8.4	
Process	Control
Unix	provides	a	number	of	system	calls	for	manipulating	processes	from
C	programs.	This	section	describes	the	important	functions	and	gives
examples	of	how	they	are	used.
8.4.1	
Obtaining	Process	IDs
Each	process	has	a	unique	positive	(nonzero)	
process	ID	(PID)
.	The
function	returns	the	PID	of	the	calling	process.	The	
function	returns	the	PID	of	its	
parent
(i.e.,	the	process	that	created	the
calling	process).
The	
and	
routines	return	an	integer	value	of	type	
,
which	on	Linux	systems	is	defined	in	
as	an	int.</p>
<p>8.4.2	
Creating	and	Terminating
Processes
From	a	programmer's	perspective,	we	can	think	of	a	process	as	being	in
one	of	three	states:
Running.	
The	process	is	either	executing	on	the	CPU	or	waiting	to	be
executed	and	will	eventually	be	scheduled	by	the	kernel.
Stopped.	
The	execution	of	the	process	is	
suspended
and	will	not	be
scheduled.	A	process	stops	as	a	result	of	receiving	a	SIGSTOP,
SIGTSTP,	SIGTTIN,	or	SIGTTOU	signal,	and	it	remains	stopped	until
it	receives	a	SIGCONT	signal,	at	which	point	it	becomes	running
again.	(A	
signal
is	a	form	of	software	interrupt	that	we	will	describe	in
detail	in	
Section	
8.5
.)
Terminated.	
The	process	is	stopped	permanently.	A	process
becomes	terminated	for	one	of	three	reasons:	(1)	receiving	a	signal
whose	default	action	is	to	terminate	the	process,	(2)	returning	from
the	main	routine,	or	(3)	calling	the	
function.</p>
<p>The	
function	terminates	the	process	with	an	
exit	status
of	
.
(The	other	way	to	set	the	exit	status	is	to	return	an	integer	value	from	the
main	routine.)
A	
parent	process
creates	a	new	running	
child	process
by	calling	the	
function.
The	newly	created	child	process	is	almost,	but	not	quite,	identical	to	the
parent.	The	child	gets	an	identical	(but	separate)	copy	of	the	parent's
user-level	virtual	address	space,	including	the	code	and	data	segments,
heap,	shared	libraries,	and	user	stack.	The	child	also	gets	identical
copies	of	any	of	the	parent's	open	file	descriptors,	which	means	the	child
can	read	and	write	any	files	that	were	open	in	the	parent	when	it	called
.	The	most	significant	difference	between	the	parent	and	the	newly
created	child	is	that	they	have	different	PIDs.
The	
function	is	interesting	(and	often	confusing)	because	it	is	called
once
but	it	returns	
twice:
once	in	the	calling	process	(the	parent),	and
once	in	the	newly	created	child	process.	In	the	parent,	
returns	the
PID	of	the	child.	In	the	child,	
returns	a	value	of	0.	Since	the	PID	of</p>
<p>the	child	is	always	nonzero,	the	return	value	provides	an	unambiguous
way	to	tell	whether	the	program	is	executing	in	the	parent	or	the	child.
Figure	
8.15	
shows	a	simple	example	of	a	parent	process	that	uses
to	create	a	child	process.	When	the	
call	returns	in	line	6,	
has	a	value	of	1	in	both	the	parent	and	child.	The	child	increments	and
prints	its	copy	of	
in	line	8.	Similarly,	the	parent	decrements	and	prints
its	copy	of	
in	line	13.
When	we	run	the	program	on	our	Unix	system,	we	get	the	following
result:
There	are	some	subtle	aspects	to	this	simple	example.
Call	once,	return	twice.	
The	
function	is	called	once	by	the
parent,	but	it	returns	twice:	once	to	the	parent	and	once	to	the	newly
created	child.	This	is	fairly	straightforward	for	programs	that	create	a
single	child.	But	programs	with	multiple	instances	of	
can	be
confusing	and	need	to	be	reasoned	about	carefully.
Concurrent	execution.	
The	parent	and	the	child	are	separate
processes	that	run	concurrently.	The	instructions	in	their	logical</p>
<h2>control	flows	can	be	interleaved	by	the	kernel	in	an	arbitrary	way.
When	we	run	the	program	on	our	system,	the	parent	process
completes	its	
statement	first,	followed	by	the	child.	However,
on	another	system	the	reverse	might	be	true.	In	general,	as
programmers	we	can	never	make	assumptions	about	the	interleaving
of	the	instructions	in	different	processes.</h2>
<hr />
<p>code/ecf/fork.c</p>
<hr />
<hr />
<p>code/ecf/fork.c
Figure	
8.15	
Using	
to	create	a	new	process.
Duplicate	but	separate	address	spaces.	
If	we	could	halt	both	the
parent	and	the	child	immediately	after	the	
function	returned	in
each	process,	we	would	see	that	the	address	space	of	each	process
is	identical.	Each	process	has	the	same	user	stack,	the	same	local
variable	values,	the	same	heap,	the	same	global	variable	values,	and
the	same	code.	Thus,	in	our	example	program,	local	variable	
has	a
value	of	1	in	both	the	parent	and	the	child	when	the	
function
returns	in	line	6.	However,	since	the	parent	and	the	child	are	separate
processes,	they	each	have	their	own	private	address	spaces.	Any
subsequent	changes	that	a	parent	or	child	makes	to	
are	private	and
are	not	reflected	in	the	memory	of	the	other	process.	This	is	why	the
variable	
has	different	values	in	the	parent	and	child	when	they	call
their	respective	
statements.
Shared	files.	
When	we	run	the	example	program,	we	notice	that	both
parent	and	child	print	their	output	on	the	screen.	The	reason	is	that
the	child	inherits	all	of	the	parent's	open	files.	When	the	parent	calls
,	the	
file	is	open	and	directed	to	the	screen.	The	child
inherits	this	file,	and	thus	its	output	is	also	directed	to	the	screen.
When	you	are	first	learning	about	the	
function,	it	is	often	helpful	to
sketch	the	
process	graph
,	which	is	a	simple	kind	of	precedence	graph
that	captures	the	partial	ordering	of	program	statements.	Each	vertex	
a
corresponds	to	the	execution	of	a	program	statement.	A	directed	edge	
a
→	
b
denotes	that	statement	
a
“happens	before”	statement	
b
.	Edges	can</p>
<p>be	labeled	with	information	such	as	the	current	value	of	a	variable.
Vertices	corresponding	to	
statements	can	be	labeled	with	the
output	of	the	
.	Each	graph	begins	with	a	vertex	that
Figure	
8.16	
Process	graph	for	the	example	program	in	
Figure
8.15
.
Figure	
8.17	
Process	graph	for	a	nested	
.
corresponds	to	the	parent	process	calling	main.	This	vertex	has	no
inedges	and	exactly	one	outedge.	The	sequence	of	vertices	for	each
process	ends	with	a	vertex	corresponding	to	a	call	to	
.	This	vertex
has	one	inedge	and	no	outedges.
For	example,	
Figure	
8.16	
shows	the	process	graph	for	the	example
program	in	
Figure	
8.15
.	Initially,	the	parent	sets	variable	
to	1.	The</p>
<h2>parent	calls	
,	which	creates	a	child	process	that	runs	concurrently
with	the	parent	in	its	own	private	address	space.
For	a	program	running	on	a	single	processor,	any	
topological	sort
of	the
vertices	in	the	corresponding	process	graph	represents	a	feasible	total
ordering	of	the	statements	in	the	program.	Here's	a	simple	way	to
understand	the	idea	of	a	topological	sort:	Given	some	permutation	of	the
vertices	in	the	process	graph,	draw	the	sequence	of	vertices	in	a	line
from	left	to	right,	and	then	draw	each	of	the	directed	edges.	The
permutation	is	a	topological	sort	if	and	only	if	each	edge	in	the	drawing
goes	from	left	to	right.	Thus,	in	our	example	program	in	
Figure	
8.15
,
the	
statements	in	the	parent	and	child	can	occur	in	either	order
because	each	of	the	orderings	corresponds	to	some	topological	sort	of
the	graph	vertices.
The	process	graph	can	be	especially	helpful	in	understanding	programs
with	nested	
calls.	For	example,	
Figure	
8.17
shows	a	program
with	two	calls	to	
in	the	source	code.	The	corresponding	process
graph	helps	us	see	that	this	program	runs	four	processes,	each	of	which
makes	a	call	to	
and	which	can	execute	in	any	order.
Practice	Problem	
8.2	
(solution	page	
795
)
Consider	the	following	program:</h2>
<hr />
<p>code/ecf/forkprob0.c</p>
<hr />
<hr />
<p>code/ecf/forkprob0.c
A
.	
What	is	the	output	of	the	child	process?
B
.	
What	is	the	output	of	the	parent	process?
8.4.3	
Reaping	Child	Processes
When	a	process	terminates	for	any	reason,	the	kernel	does	not	remove	it
from	the	system	immediately.	Instead,	the	process	is	kept	around	in	a
terminated	state	until	it	is	
reaped
by	its	parent.	When	the	parent	reaps
the	terminated	child,	the	kernel	passes	the	child's	exit	status	to	the	parent
and	then	discards	the	terminated	process,	at	which	point	it	ceases	to
exist.	A	terminated	process	that	has	not	yet	been	reaped	is	called	a
zombie
.</p>
<p>When	a	parent	process	terminates,	the	kernel	arranges	for	the	
process	to	become	the	adopted	parent	of	any	orphaned	children.	The
process,	which	has	a	PID	of	1,	is	created	by	the	kernel	during
system	start-up,	never	terminates,	and	is	the	ancestor	of	every	process.
If	a	parent	process	terminates	without	reaping	its	zombie	children,	then
the	kernel	arranges	for	the	
process	to	reap	them.	However,	long-
running	programs	such	as	shells	or	servers	should	always	reap	their
zombie	children.	Even	though	zombies	are	not	running,	they	still
consume	system	memory	resources.
A	process	waits	for	its	children	to	terminate	or	stop	by	calling	the	
function.
Aside	
Why	are	terminated	children
called	zombies?
In	folklore,	a	zombie	is	a	living	corpse,	an	entity	that	is	half	alive
and	half	dead.	A	zombie	process	is	similar	in	the	sense	that</p>
<p>although	it	has	already	terminated,	the	kernel	maintains	some	of
its	state	until	it	can	be	reaped	by	the	parent.
The	
function	is	complicated.	By	default	(when	
),
suspends	execution	of	the	calling	process	until	a	child	process	in
its	
wait	set
terminates.	If	a	process	in	the	wait	set	has	already	terminated
at	the	time	of	the	call,	then	
returns	immediately.	In	either	case,
returns	the	PID	of	the	terminated	child	that	caused	
to
return.	At	this	point,	the	terminated	child	has	been	reaped	and	the	kernel
removes	all	traces	of	it	from	the	system.
Determining	the	Members	of	the	Wait	Set
The	members	of	the	wait	set	are	determined	by	the	
argument:
If	
,	then	the	wait	set	is	the	singleton	child	process	whose
process	ID	is	equal	to	
.
If	
,	then	the	wait	set	consists	of	all	of	the	parent's	child
processes.
The	
function	also	supports	other	kinds	of	wait	sets,	involving
Unix	process	groups,	which	we	will	not	discuss.
Modifying	the	Default	Behavior
The	default	behavior	can	be	modified	by	setting	
to	various
combinations	of	the	WNOHANG,	WUNTRACED,	and	WCONTINUED
constants:</p>
<p>WNOHANG.	
Return	immediately	(with	a	return	value	of	0)	if	none	of
the	child	processes	in	the	wait	set	has	terminated	yet.	The	default
behavior	suspends	the	calling	process	until	a	child	terminates;	this
option	is	useful	in	those	cases	where	you	want	to	continue	doing
useful	work	while	waiting	for	a	child	to	terminate.
WUNTRACED.	
Suspend	execution	of	the	calling	process	until	a
process	in	the	wait	set	becomes	either	terminated	or	stopped.	Return
the	PID	of	the	terminated	or	stopped	child	that	caused	the	return.	The
default	behavior	returns	only	for	terminated	children;	this	option	is
useful	when	you	want	to	check	for	both	terminated	
and
stopped
children.
WCONTINUED.	
Suspend	execution	of	the	calling	process	until	a
running	process	in	the	wait	set	is	terminated	or	until	a	stopped
process	in	the	wait	set	has	been	resumed	by	the	receipt	of	a
SIGCONT	signal.	(Signals	are	explained	in	
Section	
8.5
.)
You	can	combine	options	by	
OR
ing	them	together.	For	example:
WNOHANG	|	WUNTRACED:	Return	immediately,	with	a	return	value
of	0,	if	none	of	the	children	in	the	wait	set	has	stopped	or	terminated,
or	with	a	return	value	equal	to	the	PID	of	one	of	the	stopped	or
terminated	children.
Checking	the	Exit	Status	of	a	Reaped	Child
If	the	
argument	is	non-NULL,	then	
encodes	status
information	about	the	child	that	caused	the	return	in	status,	which	is	the</p>
<p>value	pointed	to	by	
.	The	
include	file	defines	several
macros	for	interpreting	the	
argument:
WIFEXITED(
).	
Returns	true	if	the	child	terminated	normally,	via
a	call	to	
or	a	return.
WEXITSTATUS(
).	
Returns	the	exit	status	of	a	normally
terminated	child.	This	status	is	only	defined	if	WIFEXITED()	returned
true.
WIFSIGNALED(
).	
Returns	true	if	the	child	process	terminated
because	of	a	signal	that	was	not	caught.
WTERMSIG(
).	
Returns	the	number	of	the	signal	that	caused
the	child	process	to	terminate.	This	status	is	only	defined	if
WIFSIGNALED()	returned	true.
WIFSTOPPED(
).	
Returns	true	if	the	child	that	caused	the
return	is	currently	stopped.
WSTOPSIG(
).	
Returns	the	number	of	the	signal	that	caused
the	child	to	stop.	This	status	is	only	defined	if	WIFSTOPPED()
returned	true.
WIFCONTINUED(
).	
Returns	true	if	the	child	process	was
restarted	by	receipt	of	a	SIGCONT	signal.
Error	Conditions
If	the	calling	process	has	no	children,	then	
returns	-1	and	sets
to	ECHILD.	If	the	
function	was	interrupted	by	a	signal,</p>
<h2>then	it	returns	-1	and	sets	
to	EINTR.
Practice	Problem	
8.3	
(solution	page	
797
)
List	all	of	the	possible	output	sequences	for	the	following	program:</h2>
<hr />
<h2 id="codeecfwaitprob0c"><a class="header" href="#codeecfwaitprob0c">code/ecf/waitprob0.c</a></h2>
<hr />
<p>code/ecf/waitprob0.c</p>
<p>The	
Function
The	
function	is	a	simpler	version	of	
.
Calling	
is	equivalent	to	calling	
Examples	of	Using	
Because	the	
function	is	somewhat	complicated,	it	is	helpful	to
look	at	a	few	examples.	
Figure	
8.18	
shows	a	program	that	uses
to	wait,	in	no	particular	order,	for	all	of	its	
N
children	to	terminate.
In	line	11,	the	parent	creates	each	of	the	
N
children,	and	in	line	12,	each
child	exits	with	a	unique	exit	status.
Aside	
Constants	associated	with	Unix
functions</p>
<h2>Constants	such	as	WNOHANG	and	WUNTRACED	are	defined	by
system	header	files.	For	example,	WNOHANG	and	WUNTRACED
are	defined	(indirectly)	by	the	
header	file:
In	order	to	use	these	constants,	you	must	include	the	
header	file	in	your	code:
The	
page	for	each	Unix	function	lists	the	header	files	to
include	whenever	you	use	that	function	in	your	code.	Also,	in
order	to	check	return	codes	such	as	ECHILD	and	EINTR,	you
must	include	
To	simplify	our	code	examples,	we	include
a	single	header	file	called	
that	includes	the	header	files
for	all	of	the	functions	used	in	the	book.	The	
header	file	is
available	online	from	the	CS:APP	Web	site.</h2>
<hr />
<p>code/ecf/waitpid1.c</p>
<hr />
<hr />
<h2>code/ecf/waitpid1.c
Figure	
8.18	
Using	the	
function	to	reap	zombie	children	in	no
particular	order.</h2>
<p>code/ecf/waitpid1.c
Before	moving	on,	make	sure	you	understand	why	line	12	is	executed	by
each	of	the	children,	but	not	the	parent.
In	line	15,	the	parent	waits	for	all	of	its	children	to	terminate	by	using
as	the	test	condition	of	a	while	loop.	Because	the	first	argument
is	-1,	the	call	to	
blocks	until	an	arbitrary	child	has	terminated.	As
each	child	terminates,	the	call	to	
returns	with	the	nonzero	PID	of
that	child.	Line	16	checks	the	exit	status	of	the	child.	If	the	child
terminated	normally—in	this	case,	by	calling	the	
function—then	the
parent	extracts	the	exit	status	and	prints	it	on	
.
When	all	of	the	children	have	been	reaped,	the	next	call	to	
returns	-1	and	sets	
to	ECHILD.	Line	24	checks	that	the	
function	terminated	normally,	and	prints	an	error	message	otherwise.</p>
<p>When	we	run	the	program	on	our	Linux	system,	it	produces	the	following
output:
Notice	that	the	program	reaps	its	children	in	no	particular	order.	The
order	that	they	were	reaped	is	a	property	of	this	specific	computer
system.	On	another	system,	or	even	another	execution	on	the	same
system,	the	two	children	might	have	been	reaped	in	the	opposite	order.
This	is	an	example	of	the	
nondeterministic
behavior	that	can	make
reasoning	about	concurrency	so	difficult.	Either	of	the	two	possible
outcomes	is	equally	correct,	and	as	a	programmer	you	may	
never
assume	that	one	outcome	will	always	occur,	no	matter	how	unlikely	the
other	outcome	appears	to	be.	The	only	correct	assumption	is	that	each
possible	outcome	is	equally	likely.
Figure	
8.19	
shows	a	simple	change	that	eliminates	this
nondeterminism	in	the	output	order	by	reaping	the	children	in	the	same
order	that	they	were	created	by	the	parent.	In	line	11,	the	parent	stores
the	PIDs	of	its	children	in	order	and	then	waits	for	each	child	in	this	same
order	by	calling	
with	the	appropriate	PID	in	the	first	argument.
Practice	Problem	
8.4	
(solution	page	
797
)</p>
<h2 id="consider-the-following-program"><a class="header" href="#consider-the-following-program">Consider	the	following	program:</a></h2>
<hr />
<h2 id="codeecfwaitprob1c"><a class="header" href="#codeecfwaitprob1c">code/ecf/waitprob1.c</a></h2>
<hr />
<p>code/ecf/waitprob1.c</p>
<h2>A
.	
How	many	output	lines	does	this	program	generate?
B
.	
What	is	one	possible	ordering	of	these	output	lines?</h2>
<hr />
<p>code/ecf/waitpid2.c</p>
<hr />
<hr />
<p>code/ecf/waitpid2.c
Figure	
8.19	
Using	
to	reap	zombie	children	in	the	order	they
were	created.
8.4.4	
Putting	Processes	to	Sleep
The	sleep	function	suspends	a	process	for	a	specified	period	of	time.</p>
<pre><code>returns	zero	if	the	requested	amount	of	time	has	elapsed,	and	the
</code></pre>
<p>number	of	seconds	still	left	to	sleep	otherwise.	The	latter	case	is	possible
if	the	
function	
returns	prematurely	because	it	was	interrupted	by	a
signal.
We	will	discuss	signals	in	detail	in	
Section	
8.5
.
Another	function	that	we	will	find	useful	is	the	
function,	which	puts
the	calling	function	to	sleep	until	a	signal	is	received	by	the	process.
Practice	Problem	
8.5	
(solution	page	
797
)
Write	a	wrapper	function	for	
,	called	
,	with	the	following
interface:</p>
<p>The	
function	behaves	exactly	as	the	
function,	except	that	it
prints	a	message	describing	how	long	the	process	actually	slept:
8.4.5	
Loading	and	Running
Programs
The	
function	loads	and	runs	a	new	program	in	the	context	of	the
current	process.
The	
function	loads	and	runs	the	executable	object	file	
with	the	argument	list	
and	the	environment	variable	list	
returns	to	the	calling	program	only	if	there	is	an	error,	such	as	not	being
able	to	find	
.	So	unlike	
,	which	is	called	once	but	returns
twice,	
is	called	once	and	never	returns.</p>
<p>The	argument	list	is	represented	by	the	data	structure	shown	in	
Figure
8.20
.	The	
variable	points	to	a	null-terminated	array	of	pointers,
each	of	which	points	to	an	argument	string.	By	convention,	
is
the	name	of	the	executable	object	file.	The	list	of	environment	variables	is
represented	by	a	similar	data	structure,	shown	in	
Figure	
8.21
.	The
variable	points	to	a	null-terminated	array	of	pointers	to	environment
variable	strings,	each	of	which	is	a	name-value	pair	of	the	form
name=value.
Figure	
8.20	
Organization	of	an	argument	list.
Figure	
8.21	
Organization	of	an	environment	variable	list.
After	
loads	
,	it	calls	the	start-up	code	described	in
Section	
7.9
.	The	start-up	code	sets	up	the	stack	and	passes	control	to
the	main	routine	of	the	new	program,	which	has	a	prototype	of	the	form</p>
<p>or	equivalently,
When	
begins	executing,	the	user	stack	has	the	organization	shown
in	
Figure	
8.22
.	Let's	work	our	way	from	the	bottom	of	the	stack	(the
highest	address)	to	the	top	(the	lowest	address).	First	are	the	argument
and	environment	strings.	These	are	followed	further	up	the	stack	by	a
null-terminated	array	of	pointers,	each	of	which	points	to	an	environment
variable	string	on	the	stack.	The	global	variable	
points	to	the	first
of	these	pointers,	
.	The	environment	array	is	followed	by	the
null-terminated	
array,	with	each	element	pointing	to	an	argument
string	on	the	stack.	At	the	top	of	the	stack	is	the	stack	frame	for	the
system	start-up	function,	
(
Section	
7.9
).
There	are	three	arguments	to	function	main,	each	stored	in	a	register
according	to	the	x86-64	stack	discipline:	(1)	
,	which	gives	the
number	of	non-null	pointers	in	the	
array;	(2)	
,	which	points
to	the	first	entry	in	the	
array;	and	(3)	
,	which	points	to	the
first	entry	in	the	
array.
Linux	provides	several	functions	for	manipulating	the	environment	array:</p>
<p>Figure	
8.22	
Typical	organization	of	the	user	stack	when	a	new
program	starts.
The	
function	searches	the	environment	array	for	a	string
If	found,	it	returns	a	pointer	to	
value;
otherwise,	it	returns
.</p>
<p>If	the	environment	array	contains	a	string	of	the	form	
,	then
deletes	it	and	
replaces	
oldvalue
with	
,	but	only
if	
is	nonzero.	If	name	does	not	exist,	then	
adds
to	the	array.
Practice	Problem	</p>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->
                            <a rel="prev" href="../csapp/part5.html" class="mobile-nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                                <i class="fa fa-angle-left"></i>
                            </a>

                            <a rel="next" href="../csapp/part7.html" class="mobile-nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                                <i class="fa fa-angle-right"></i>
                            </a>

                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">
                    <a rel="prev" href="../csapp/part5.html" class="nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                        <i class="fa fa-angle-left"></i>
                    </a>

                    <a rel="next" href="../csapp/part7.html" class="nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                        <i class="fa fa-angle-right"></i>
                    </a>
            </nav>

        </div>




        <script>
            window.playground_copyable = true;
        </script>



        <script src="../clipboard.min.js"></script>
        <script src="../highlight.js"></script>
        <script src="../book.js"></script>

        <!-- Custom JS scripts -->


    </div>
    </body>
</html>
