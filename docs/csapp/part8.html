<!DOCTYPE HTML>
<html lang="en" class="sidebar-visible no-js light">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>Part8 - KnowledgeBase</title>


        <!-- Custom HTML head -->
        
        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff" />

        <link rel="icon" href="../favicon.svg">
        <link rel="shortcut icon" href="../favicon.png">
        <link rel="stylesheet" href="../css/variables.css">
        <link rel="stylesheet" href="../css/general.css">
        <link rel="stylesheet" href="../css/chrome.css">
        <link rel="stylesheet" href="../css/print.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="../FontAwesome/css/font-awesome.css">
        <link rel="stylesheet" href="../fonts/fonts.css">

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" href="../highlight.css">
        <link rel="stylesheet" href="../tomorrow-night.css">
        <link rel="stylesheet" href="../ayu-highlight.css">

        <!-- Custom theme stylesheets -->

    </head>
    <body>
    <div id="body-container">
        <!-- Provide site root to javascript -->
        <script>
            var path_to_root = "../";
            var default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? "navy" : "light";
        </script>

        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script>
            try {
                var theme = localStorage.getItem('mdbook-theme');
                var sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script>
            var theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            var html = document.querySelector('html');
            html.classList.remove('no-js')
            html.classList.remove('light')
            html.classList.add(theme);
            html.classList.add('js');
        </script>

        <!-- Hide / unhide sidebar before it is displayed -->
        <script>
            var html = document.querySelector('html');
            var sidebar = null;
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            } else {
                sidebar = 'hidden';
            }
            html.classList.remove('sidebar-visible');
            html.classList.add("sidebar-" + sidebar);
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <div class="sidebar-scrollbox">
                <ol class="chapter"><li class="chapter-item expanded "><a href="../HaskellProgramming/HaskellProgramming.html"><strong aria-hidden="true">1.</strong> Haskell Programming</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="../HaskellProgramming/part1.html"><strong aria-hidden="true">1.1.</strong> Part1</a></li><li class="chapter-item expanded "><a href="../HaskellProgramming/part2.html"><strong aria-hidden="true">1.2.</strong> Part2</a></li><li class="chapter-item expanded "><a href="../HaskellProgramming/part3.html"><strong aria-hidden="true">1.3.</strong> Part3</a></li><li class="chapter-item expanded "><a href="../HaskellProgramming/part4.html"><strong aria-hidden="true">1.4.</strong> Part4</a></li><li class="chapter-item expanded "><a href="../HaskellProgramming/part5.html"><strong aria-hidden="true">1.5.</strong> Part5</a></li></ol></li><li class="chapter-item expanded "><a href="../csapp/csapp.html"><strong aria-hidden="true">2.</strong> csapp</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="../csapp/part1.html"><strong aria-hidden="true">2.1.</strong> Part1</a></li><li class="chapter-item expanded "><a href="../csapp/part2.html"><strong aria-hidden="true">2.2.</strong> Part2</a></li><li class="chapter-item expanded "><a href="../csapp/part3.html"><strong aria-hidden="true">2.3.</strong> Part3</a></li><li class="chapter-item expanded "><a href="../csapp/part4.html"><strong aria-hidden="true">2.4.</strong> Part4</a></li><li class="chapter-item expanded "><a href="../csapp/part5.html"><strong aria-hidden="true">2.5.</strong> Part5</a></li><li class="chapter-item expanded "><a href="../csapp/part6.html"><strong aria-hidden="true">2.6.</strong> Part6</a></li><li class="chapter-item expanded "><a href="../csapp/part7.html"><strong aria-hidden="true">2.7.</strong> Part7</a></li><li class="chapter-item expanded "><a href="../csapp/part8.html" class="active"><strong aria-hidden="true">2.8.</strong> Part8</a></li><li class="chapter-item expanded "><a href="../csapp/part9.html"><strong aria-hidden="true">2.9.</strong> Part9</a></li><li class="chapter-item expanded "><a href="../csapp/part10.html"><strong aria-hidden="true">2.10.</strong> Part10</a></li></ol></li><li class="chapter-item expanded "><a href="../midjourney/combined_html_page.html"><strong aria-hidden="true">3.</strong> midjourney</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="../midjourney/mjprompt.html"><strong aria-hidden="true">3.1.</strong> MjPrompt</a></li></ol></li></ol>
            </div>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle"></div>
        </nav>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky bordered">
                    <div class="left-buttons">
                        <button id="sidebar-toggle" class="icon-button" type="button" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </button>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="light">Light</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                    </div>

                    <h1 class="menu-title">KnowledgeBase</h1>

                    <div class="right-buttons">
                        <a href="../print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>

                    </div>
                </div>


                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script>
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <p>size	
bytes.	The	
function	converts	the	socket	address
structure	
to	the	corresponding	
and	
name	strings	and
copies	them	to	the	
and	
buffers.	If	
returns	a
nonzero	error	code,	the	application	can	convert	it	to	a	string	by	calling
.
If	we	don't	want	the	hostname,	we	can	
to	NULL	and	
to
zero.	The	same	holds	for	the	service	fields.	However,	one	or	the	other
must	be	set.
The	
argument	is	a	bit	mask	that	modifies	the	default	behavior.	You
create	it	by	
OR
ing	combinations	of	various	values.	Here	are	a	couple	of
useful	ones:
NI_NUMERICHOST.	By	default,	
tries	to	return	a	domain
name	in	
.	Setting	this	flag	will	cause	it	to	return	a	numeric
address	string	instead.
NI_NUMERICSERV.	By	default,	
will	look	in	
and	if	possible,	return	a	service	name	instead	of	a	port	number.
Setting	this	flag	forces	it	to	skip	the	lookup	and	simply	return	the	port
number.</p>
<p>Figure	
11.17	
H
OSTINFO
displays	the	mapping	of	a	domain	name	to	its
associated	IP	addresses.
Figure	
11.17
shows	a	simple	program,	called	
HOSTINFO
,	that	uses
and	
to	display	the	mapping	of	a	domain	name	to
its	associated	IP	addresses.	It	is	similar	to	the	
NSLOOKUP</p>
<p>program	from
Section	
11.3.2
.
First,	we	initialize	the	
structure	so	that	
returns	the
addresses	we	want.	In	this	case,	we	are	looking	for	32-bit	IP	addresses
(line	16)	
that	can	be	used	as	end	points	of	connections	(line	17).	Since
we	are	only	asking	
to	convert	domain	names,	we	call	it	with
a	NULL	
argument.
After	the	call	to	
,	we	walk	the	list	of	
structures,	using
to	convert	each	socket	address	to	a	dotted-decimal	address
string.	After	walking	the	list,	we	are	careful	to	free	it	by	calling
(although	for	this	simple	program	it	is	not	strictly
necessary).</p>
<p>When	we	run	
HOSTINFO
,	we	see	that	
maps	to	four	IP
addresses,	which	is	what	we	saw	using	
NSLOOKUP</p>
<p>in	
Section	
11.3.2
.
Practice	Problem	
11.4	
(solution	page	
968
)
The	
and	
functions	subsume	the
functionality	of	
and	
,	respectively,	and	they
provide	a	higher-level	of	abstraction	that	is	independent	of	any
particular	address	format.	To	convince	yourself	how	handy	this	is,
write	a	version	of	
HOSTINFO</p>
<p>(
Figure	
11.17
)	that	uses	
instead	of	
to	convert	each	socket	address	to	a	dotted-
decimal	address	string.
11.4.8	
Helper	Functions	for	the
Sockets	Interface
The	
function	and	the	sockets	interface	can	seem	somewhat
daunting	when	you	first	learn	about	them.	We	find	it	convenient	to	wrap</p>
<p>them	with	higher-level	helper	functions,	called	
and
,	that	clients	and	servers	can	use	when	they	want	to
communicate	with	each	other.
The	
Function
A	client	establishes	a	connection	with	a	server	by	calling	
.
The	
function	establishes	a	connection	with	a	server
running	on	host	
and	listening	for	connection	requests	on	port
number	port.	It	returns	an	open	socket	descriptor	that	is	ready	for	input
and	output	using	the	Unix	I/O	functions.	
Figure	
11.18
shows	the	code
for	
We	call	
,	which	returns	a	list	of	
structures,	each	of
which	points	to	a	socket	address	structure	that	is	suitable	for	establishing
a	connection</p>
<p>Figure	
11.18	
:	Helper	function	that	establishes	a
connection	with	a	server.
It	is	reentrant	and	protocol-independent.
with	a	server	running	on	
and	listening	on	
.	We	then	walk
the	list,	trying	each	list	entry	in	turn,	until	the	calls	to	
and	
succeed.	If	the	
fails,	we	are	careful	to	close	the	socket	descriptor
before	trying	the	next	entry.	If	the	
succeeds,	we	free	the	list
memory	and	return	the	socket	descriptor	to	the	client,	which	can
immediately	begin	using	Unix	I/O	to	communicate	with	the	server.
Notice	how	there	is	no	dependence	on	any	particular	version	of	IP
anywhere	in	the	code.	The	arguments	to	
and	
are
generated	for	us	automatically	by	
,	which	allows	our	code	to
be	clean	and	portable.
The	
Function
A	server	creates	a	listening	descriptor	that	is	ready	to	receive	connection
requests	by	calling	the	
function.</p>
<p>The	
function	returns	a	listening	descriptor	that	is	ready	to
receive	connection	requests	on	port	
.	
Figure	
11.19
shows	the
code	for	
.
The	style	is	similar	to	
.	We	call	
and	then	walk
the	resulting	list	until	the	calls	to	
and	
succeed.	Note	that	in
line	20	we	use	the	
function	(not	described	here)	to	configure
the	server	so	that	it	can	be	terminated,	be	restarted,	and	begin	accepting
connection	requests	immediately.	By	default,	a	restarted	server	will	deny
connection	requests	from	clients	for	approximately	30	seconds,	which
seriously	hinders	debugging.
Since	we	have	called	
with	the	AI_PASSIVE	flag	and	a	NULL
argument,	the	address	field	in	each	socket	address	structure	is	set
to	the	wildcard	address,	which	tells	the	kernel	that	this	server	will	accept
requests	to	any	of	the	IP	addresses	for	this	host.
Finally,	we	call	the	
function	to	convert	
to	a	listening
descriptor	and	return	it	to	the	caller.	If	the	
fails,	we	are	careful	to
avoid	a	memory	leak	by	closing	the	descriptor	before	returning.</p>
<p>11.4.9	
Example	Echo	Client	and
Server
The	best	way	to	learn	the	sockets	interface	is	to	study	example	code.
Figure	
11.20
shows	the	code	for	an	echo	client.	After	establishing	a
connection	with	the	server,	the	client	enters	a	loop	that	repeatedly	reads
a	text	line	from	standard	input,	sends	the	text	line	to	the	server,	reads	the
echo	line	from	the	server,	and	prints	the	result	to	standard	output.	The
loop	terminates	when	
encounters	EOF	on	standard	input,	either
because	the	user	typed	Ctrl+D	at	the	keyboard	or	because	it	has
exhausted	the	text	lines	in	a	redirected	input	file.
After	the	loop	terminates,	the	client	closes	the	descriptor.	This	results	in
an	EOF	notification	being	sent	to	the	server,	which	it	detects	when	it
receives	a	return	code	of	zero	from	its	
function.	After
closing	its	descriptor,	the	client	terminates.	Since	the	client's	kernel
automatically	closes	all	open	descriptors	when	a	process	terminates,	the
in	line	24	is	not	necessary.	However,	it	is	good	programming
practice	to	explicitly	close	any	descriptors	that	you	have	opened.
Figure	
11.21
shows	the	main	routine	for	the	echo	server.	After
opening	the	listening	descriptor,	it	enters	an	infinite	loop.	Each	iteration
waits	for	a	connection	request	from	a	client,	prints	the	domain	name	and
port	of	the	connected	client,	and	then	calls	the	
function	that	services
the	client.	After	the	echo	routine	returns,</p>
<p>Figure	
11.19	
:	Helper	function	that	opens	and	returns	a
listening	descriptor.
It	is	reentrant	and	protocol-independent.</p>
<p>Figure	
11.20	
Echo	client	main	routine.
the	main	routine	closes	the	connected	descriptor.	Once	the	client	and
server	have	closed	their	respective	descriptors,	the	connection	is
terminated.
The	
variable	in	line	9	is	a	socket	address	structure	that	is
passed	to	
.	Before	
returns,	it	fills	in	
with	the
socket	address	of	the	client	on	the	other	end	of	the	connection.	Notice
how	we	declare	
as	type	struct	
rather	than
struct	
.	By	definition,	the	
structure	is	large
enough	to	hold	any	type	of	socket	address,	which	keeps	the	code
protocol-independent.
Notice	that	our	simple	echo	server	can	only	handle	one	client	at	a	time.	A
server	of	this	type	that	iterates	through	clients,	one	at	a	time,	is	called	an
iterative	server
.	In	
Chapter	
12
,	we	will	learn	how	to	build	more
sophisticated	
concurrent	servers
that	can	handle	multiple	clients
simultaneously.
Finally,	
Figure	
11.22
shows	the	code	for	the	
routine,	which
repeatedly	reads	and	writes	lines	of	text	until	the	
function
encounters	EOF	in	line	10.</p>
<p>Figure	
11.21	
Iterative	echo	server	main	routine.
Figure	
11.22	
function	that	reads	and	echoes	text	lines.
Aside	
What	does	EOF	on	a	connection
mean?
The	idea	of	EOF	is	often	confusing	to	students,	especially	in	the
context	of	Internet	connections.	First,	we	need	to	understand	that</p>
<p>there	is	no	such	thing	as	an	EOF	character.	Rather,	EOF	is	a
condition	that	is	detected	by	the	kernel.	An	application	finds	out
about	the	EOF	condition	when	it	receives	a	zero	return	code	from
the	read	function.	For	disk	files,	EOF	occurs	when	the	current	file
position	exceeds	the	file	length.	For	Internet	connections,	EOF
occurs	when	a	process	closes	its	end	of	the	connection.	The
process	at	the	other	end	of	the	connection	detects	the	EOF	when
it	attempts	to	read	past	the	last	byte	in	the	stream.</p>
<p>11.5	
Web	Servers
So	far	we	have	discussed	network	programming	in	the	context	of	a
simple	echo	server.	In	this	section,	we	will	show	you	how	to	use	the	basic
ideas	of	network	programming	to	build	your	own	small,	but	quite
functional,	Web	server.
11.5.1	
Web	Basics
Web	clients	and	servers	interact	using	a	text-based	application-level
protocol	known	as	
HTTP	(hypertext	transfer	protocol).
HTTP	is	a	simple
protocol.	A	Web	client	(known	as	a	
browser)
opens	an	Internet
connection	to	a	server	and	requests	some	
content.
The	server	responds
with	the	requested	content	and	then	closes	the	connection.	The	browser
reads	the	content	and	displays	it	on	the	screen.
What	distinguishes	Web	services	from	conventional	file	retrieval	services
such	as	FTP?	The	main	difference	is	that	Web	content	can	be	written	in	a
language	known	as	
HTML	(hypertext	markup	language).
An	HTML
program	(page)	contains	instructions	(tags)	that	tell	the	browser	how	to
display	various	text	and	graphical	objects	in	the	page.	For	example,	the
code</p>
<p>tells	the	browser	to	print	the	text	between	the	
and	
tags	in	boldface	type.
However,	the	real	power	of	HTML	is	that	a	page	can	contain	pointers
(hyperlinks)	to	content	stored	on	any	Internet	host.	For	example,	an
HTML	line	of	the	form
tells	the	browser	to	highlight	the	text	object	
and	to	create
a	hyperlink	to	an	HTML	file	called	
that	is	stored	on	the	CMU
Web	server.	If	the	user	clicks	on	the	highlighted	text	object,	the	browser
requests	the	corresponding	HTML	file	from	the	CMU	server	and	displays
it.
Aside	
Origins	of	the	World	Wide	Web
The	World	Wide	Web	was	invented	by	Tim	Berners-Lee,	a
software	engineer	working	at	CERN,	a	Swiss	physics	lab.	In	1989,
Berners-Lee	wrote	an	internal	memo	proposing	a	distributed
hypertext	system	that	would	connect	a	&quot;web	of	notes	with	links.&quot;
The	intent	of	the	proposed	system	was	to	help	CERN	scientists
share	and	manage	information.	Over	the	next	two	years,	after
Berners-Lee	implemented	the	first	Web	server	and	Web	browser,
the	Web	developed	a	small	following	within	CERN	and	a	few	other
sites.	A	pivotal	event	occurred	in	1993,	when	Marc	Andreesen
(who	later	founded	Netscape	and	Andreessen	Horowitz)	and	his
colleagues	at	NCSA	released	a	graphical	browser	called	
MOSAIC</p>
<p>for
all	three	major	platforms:	Linux,	Windows,	and	Macintosh.	After
the	release	of	
MOSAIC
,	interest	in	the	Web	exploded,	with	the</p>
<p>number	of	Web	sites	increasing	at	an	exponential	rate.	By	2015,
there	were	over	975,000,000	sites	worldwide.
(
Source:
Netcraft	Web	Survey)
MIME	type
Description
HTML	page
Unformatted	text
Postscript	document
Binary	image	encoded	in	GIF	format
Binary	image	encoded	in	PNG	format
Binary	image	encoded	in	JPEG	format
Figure	
11.23	
Example	MIME	types.
11.5.2	
Web	Content
To	Web	clients	and	servers,	
content
is	a	sequence	of	bytes	with	an
associated	
MIME	(multipurpose	internet	mail	extensions)
type.	
Figure
11.23
shows	some	common	MIME	types.
Web	servers	provide	content	to	clients	in	two	different	ways:
Fetch	a	disk	file	and	return	its	contents	to	the	client.	The	disk	file	is
known	as	
static	content
and	the	process	of	returning	the	file	to	the
client	is	known	as	
serving	static	content
.</p>
<p>Run	an	executable	file	and	return	its	output	to	the	client.	The	output
produced	by	the	executable	at	run	time	is	known	as	
dynamic	content
,
and	the	process	of	running	the	program	and	returning	its	output	to	the
client	is	known	as	
serving	dynamic	content
.
Every	piece	of	content	returned	by	a	Web	server	is	associated	with	some
file	that	it	manages.	Each	of	these	files	has	a	unique	name	known	as	a
URL	(universal	resource	locator)
.	For	example,	the	URL
identifies	an	HTML	file	called	
on	Internet	host	
that	is	managed	by	a	Web	server	listening	on	port	80.	The	port	number	is
optional	and	defaults	to	the	well-known	HTTP	port	80.	URLs	for
executable	files	can	include	program	arguments	after	the	filename.	A	<code>?' character	separates	the	filename	from	the	arguments,	and	each argument	is	separated	by	an	</code>&amp;'	character.	For	example,	the	URL
identifies	an	executable	called	
that	will	be	called	with	two
argument	strings:	15000	and	213.	Clients	and	servers	use	different	parts
of	the	URL	during	a	transaction.	For	instance,	a	client	uses	the	prefix
http:/
/
www.google.com:80</p>
<p>to	determine	what	kind	of	server	to	contact,	where	the	server	is,	and	what
port	it	is	listening	on.	The	server	uses	the	suffix
to	find	the	file	on	its	filesystem	and	to	determine	whether	the	request	is
for	static	or	dynamic	content.
There	are	several	points	to	understand	about	how	servers	interpret	the
suffix	of	a	URL:
There	are	no	standard	rules	for	determining	whether	a	URL	refers	to
static	or	dynamic	content.	Each	server	has	its	own	rules	for	the	files	it
manages.	A	classic	(old-fashioned)	approach	is	to	identify	a	set	of
directories,	such	as	
,	where	all	executables	must	reside.
The	initial	<code>/'	in	the	suffix	does	 not denote	the	Linux	root	directory. Rather,	it	denotes	the	home	directory	for	whatever	kind	of	content	is being	requested.	For	example,	a	server	might	be	configured	so	that all	static	content	is	stored	in	directory	 and	all	dynamic content	is	stored	in	directory	 . The	minimal	URL	suffix	is	the	</code>/'	character,	which	all	servers	expand
to	some	default	home	page	such	as	
.	This	explains	why	it
is	possible	to	fetch	the	home	page	of	a	site	by	simply	typing	a	domain
name	to	the	browser.	The	browser	appends	the	missing	<code>/'	to	the	URL and	passes	it	to	the	server,	which	expands	the	</code>/'	to	some	default
filename.</p>
<p>11.5.3	
HTTP	Transactions
Since	HTTP	is	based	on	text	lines	transmitted	over	Internet	connections,
we	can	use	the	Linux	
TELNET</p>
<p>program	to	conduct	transactions	with	any
Web	server	on	the	Internet.	The	
TELNET</p>
<p>program	has	been	largely
supplanted	by	
SSH</p>
<p>as	a	remote	login	tool,	but	it	is	very	handy	for
debugging	servers	that	talk	to	clients	with	text	lines	over	connections.	For
example,	
Figure	
11.24
uses	
TELNET</p>
<p>to	request	the	home	page	from	the
AOL	Web	server.
⁁</p>
<p>Figure	
11.24	
Example	of	an	HTTP	transaction	that	serves	static
content.
In	line	1,	we	run	
TELNET</p>
<p>from	a	Linux	shell	and	ask	it	to	open	a	connection
to	the	AOL	Web	server.	
TELNET</p>
<p>prints	three	lines	of	output	to	the	terminal,
opens	the	connection,	and	then	waits	for	us	to	enter	text	(line	5).	Each
time	we	enter	a	text	line	and	hit	the	
key,	
TELNET</p>
<p>reads	the	line,
appends	carriage	return	and	line	feed	characters	(
in	C	notation),
and	sends	the	line	to	the	server.	This	is	consistent	with	the	HTTP
standard,	which	requires	every	text	line	to	be	terminated	by	a	carriage
return	and	line	feed	pair.	To	initiate	the	transaction,	we	enter	an	HTTP</p>
<p>request	(lines	5−7).	The	server	replies	with	an	HTTP	response	(lines
8−17)	and	then	closes	the	connection	(line	18).
HTTP	Requests
An	
HTTP	request
consists	of	a	
request	line
(line	5),	followed	by	zero	or
more	
request	headers
(line	6),	followed	by	an	empty	text	line	that
terminates	the	list	of	headers	(line	7).	A	request	line	has	the	form
method	URI	version
HTTP	supports	a	number	of	different	
methods
,	including	GET,	POST,
OPTIONS,	HEAD,	PUT,	DELETE,	and	TRACE.	We	will	only	discuss	the
workhorse	GET	method,	which	accounts	for	a	majority	of	HTTP	requests.
The	GET	method	instructs	the	server	to	generate	and	return	the	content
identified	by	the	
URI</p>
<p>(uniform	resource	identifier)
.	The	URI	is	the	suffix	of
the	corresponding	URL	that	includes	the	filename	and	optional
arguments.
3.	
Actually,	this	is	only	true	when	a	browser	requests	content.	If	a	proxy	server	requests	content,
then	the	URI	must	be	the	complete	URL.
The	
version
field	in	the	request	line	indicates	the	HTTP	version	to	which
the	request	conforms.	The	most	recent	HTTP	version	is	HTTP/1.1	
[37]
.
HTTP/1.0	is	an	earlier,	much	simpler	version	from	1996	
[6]
.	HTTP/1.1
defines	additional	headers	that	provide	support	for	advanced	features
such	as	caching	and	security,	as	well	as	a	mechanism	that	allows	a	client
and	server	to	perform	multiple	transactions	over	the	same	
persistent
connection
.	In	practice,	the	two	versions	are	compatible	because
HTTP/1.0	clients	and	servers	simply	ignore	unknown	HTTP/1.1	headers.
3</p>
<p>To	summarize,	the	request	line	in	line	5	asks	the	server	to	fetch	and
return	the	HTML	file	
.	It	also	informs	the	server	that	the
remainder	of	the	request	will	be	in	HTTP/1.1	format.
Request	headers	provide	additional	information	to	the	server,	such	as	the
brand	name	of	the	browser	or	the	MIME	types	that	the	browser
understands.	Request	headers	have	the	form
header-name
:	
header-data
For	our	purposes,	the	only	header	to	be	concerned	with	is	the	
header	(line	6),	which	is	required	in	HTTP/1.1	requests,	but	not	in
HTTP/1.0	requests.	The	
header	is	used	by	
proxy	caches
,	which
sometimes	serve	as	intermediaries	between	a	browser	and	the	
origin
server
that	manages	the	requested	file.	Multiple	proxies	can	exist
between	a	client	and	an	origin	server	in	a	so-called	proxy	chain.	The	data
in	the	
header,	which	identifies	the	domain	name	of	the	origin	server,
allow	a	proxy	in	the	middle	of	a	proxy	chain	to	determine	if	it	might	have	a
locally	cached	copy	of	the	requested	content.
Continuing	with	our	example	in	
Figure	
11.24
,	the	empty	text	line	in	line
7	(generated	by	hitting	
on	our	keyboard)	terminates	the	headers
and	instructs	the	server	to	send	the	requested	HTML	file.
HTTP	Responses
HTTP	responses	are	similar	to	HTTP	requests.	An	
HTTP	response
consists	of	a	
response	line
(line	8),	followed	by	zero	or	more	
response
headers
(lines	9−13),	followed	by	an	empty	line	that	terminates	the</p>
<p>headers	(line	14),	followed	by	the	
response	body
(lines	15−17).	A
response	line	has	the	form
version	status-code	status-message
The	
version
field	describes	the	HTTP	version	that	the	response	conforms
to.	The	
status-code
is	a	three-digit	positive	integer	that	indicates	the
disposition	of	the	request.	The	
status-message
gives	the	English
equivalent	of	the	error	code.	
Figure	
11.25
lists	some	common	status
codes	and	their	corresponding	messages.
Aside	
Passing	arguments	in	HTTP
POST	requests
Arguments	for	HTTP	POST	requests	are	passed	in	the	request
body	rather	than	in	the	URI.
Status
code
Status	message
Description
200
OK
Request	was	handled	without	error.
301
Moved	permanently
Content	has	moved	to	the	hostname	in	the	Location
header.
400
Bad	request
Request	could	not	be	understood	by	the	server.
403
Forbidden
Server	lacks	permission	to	access	the	requested
file.
404
Not	found
Server	could	not	find	the	requested	file.</p>
<p>501
Not	implemented
Server	does	not	support	the	request	method.
505
HTTP	version	not
supported
Server	does	not	support	version	in	request.
Figure	
11.25	
Some	HTTP	status	codes.
The	response	headers	in	lines	9−13	provide	additional	information	about
the	response.	For	our	purposes,	the	two	most	important	headers	are
(line	12),	which	tells	the	client	the	MIME	type	of	the	content
in	the	response	body,	and	
(line	13),	which	indicates	its
size	in	bytes.
The	empty	text	line	in	line	14	that	terminates	the	response	headers	is
followed	by	the	response	body,	which	contains	the	requested	content.
11.5.4	
Serving	Dynamic	Content
If	we	stop	to	think	for	a	moment	how	a	server	might	provide	dynamic
content	to	a	client,	certain	questions	arise.	For	example,	how	does	the
client	pass	any	program	arguments	to	the	server?	How	does	the	server
pass	these	arguments	to	the	child	process	that	it	creates?	How	does	the
server	pass	other	information	to	the	child	that	it	might	need	to	generate
the	content?	Where	does	the	child	send	its	output?	These	questions	are
addressed	by	a	de	facto	standard	called	
CGI	(common	gateway
interface)
.
How	Does	the	Client	Pass	Program</p>
<p>Arguments	to	the	Server?
Arguments	for	GET	requests	are	passed	in	the	URI.	As	we	have	seen,	a
<code>?'	character	separates	the	filename	from	the	arguments,	and	each argument	is	separated	by	an	</code>&amp;'	character.	Spaces	are	not	allowed	in
arguments	and	must	be	represented	with	the	%20	string.	Similar
encodings	exist	for	other	special	characters.
How	Does	the	Server	Pass	Arguments	to
the	Child?
After	a	server	receives	a	request	such	as
Environment	variable
Description
QUERY_STRING
Program	arguments
SERVER_PORT
Port	that	the	parent	is	listening	on
REQUEST_METHOD
GET	or	POST
REMOTE_HOST
Domain	name	of	client
REMOTE_ADDR
Dotted-decimal	IP	address	of	client
CONTENT_TYPE
POST	only:	MIME	type	of	the	request	body
CONTENT_LENGTH
POST	only:	Size	in	bytes	of	the	request	body</p>
<p>Figure	
11.26	
Examples	of	CGI	environment	variables.
it	calls	
to	create	a	child	process	and	calls	
to	run	the	
program	in	the	context	of	the	child.	Programs	like	the	
program	are	often	referred	to	as	
CGI	programs
because	they	obey	the
rules	of	the	CGI	standard.	Before	the	call	to	
,	the	child	process
sets	the	CGI	environment	variable	QUERY_STRING	to	
,	which
the	
program	can	reference	at	run	time	using	the	Linux	
function.
How	Does	the	Server	Pass	Other
Information	to	the	Child?
CGI	defines	a	number	of	other	environment	variables	that	a	CGI	program
can	expect	to	be	set	when	it	runs.	
Figure	
11.26
shows	a	subset.
Where	Does	the	Child	Send	Its	Output?
A	CGI	program	sends	its	dynamic	content	to	the	standard	output.	Before
the	child	process	loads	and	runs	the	CGI	program,	it	uses	the	Linux	
function	to	redirect	standard	output	to	the	connected	descriptor	that	is
associated	with	the	client.	Thus,	anything	that	the	CGI	program	writes	to
standard	output	goes	directly	to	the	client.
Notice	that	since	the	parent	does	not	know	the	type	or	size	of	the	content
that	the	child	generates,	the	child	is	responsible	for	generating	the</p>
<pre><code>and	
response	headers,	as	well	as	the	empty
</code></pre>
<p>line	that	terminates	the	headers.
Figure	
11.27
shows	a	simple	CGI	program	that	sums	its	two
arguments	and	returns	an	HTML	file	with	the	result	to	the	client.	
Figure
11.28
shows	an	HTTP	transaction	that	serves	dynamic	content	from
the	
program.
Practice	Problem	
11.5	
(solution	page	
969
)
In	
Section	
10.11
,	we	warned	you	about	the	dangers	of	using
the	C	standard	I/O	functions	in	network	applications.	Yet	the	CGI
program	in	
Figure	
11.27
is	able	to	use	standard	I/O	without	any
problems.	Why?
Aside	
Passing	arguments	in	HTTP
POST	requests	to	CGI	programs
For	POST	requests,	the	child	would	also	need	to	redirect	standard
input	to	the	connected	descriptor.	The	CGI	program	would	then
read	the	arguments	in	the	request	body	from	standard	input.</p>
<p>Figure	
11.27	
CGI	program	that	sums	two	integers.
⁁</p>
<p>Figure	
11.28	
An	HTTP	transaction	that	serves	dynamic	HTML
content.</p>
<p>11.6	
Putting	It	Together:	The	T
INY
Web	Server
We	conclude	our	discussion	of	network	programming	by	developing	a
small	but	functioning	Web	server	called	T
INY
.	T
INY</p>
<p>is	an	interesting
program.	It	combines	many	of	the	ideas	that	we	have	learned	about,
such	as	process	control,	Unix	I/O,	the	sockets	interface,	and	HTTP,	in
only	250	lines	of	code.	While	it	lacks	the	functionality,	robustness,	and
security	of	a	real	server,	it	is	powerful	enough	to	serve	both	static	and
dynamic	content	to	real	Web	browsers.	We	encourage	you	to	study	it	and
implement	it	yourself.	It	is	quite	exciting	(even	for	the	authors!)	to	point	a
real	browser	at	your	own	server	and	watch	it	display	a	complicated	Web
page	with	text	and	graphics.
The	T
INY</p>
<pre><code>Routine
</code></pre>
<p>Figure	
11.29
shows	T
INY
'
S</p>
<p>main	routine.	T
INY</p>
<p>is	an	iterative	server	that
listens	for	connection	requests	on	the	port	that	is	passed	in	the	command
line.	After	opening	a	listening	socket	by	calling	the	
function,	T
INY</p>
<p>executes	the	typical	infinite	server	loop,	repeatedly
accepting	a	connection	request	(line	32),	performing	a	transaction	(line
36),	and	closing	its	end	of	the	connection	(line	37).
The	
Function</p>
<p>The	
function	in	
Figure	
11.30
handles	one	HTTP	transaction.
First,	we	read	and	parse	the	request	line	(lines	11−14).	Notice	that	we	are
using	the	
function	from	Figure	
Figure	
10.8
to	read	the
request	line.
T
INY</p>
<p>supports	only	the	GET	method.	If	the	client	requests	another	method
(such	as	POST),	we	send	it	an	error	message	and	return	to	the	main
routine</p>
<p>Figure	
11.29	
The	T
INY</p>
<p>Web	server.</p>
<p>Figure	
11.30	
handles	one	HTTP	transaction.
(lines	15−19),	which	then	closes	the	connection	and	awaits	the	next
connection	request.	Otherwise,	we	read	and	(as	we	shall	see)	ignore	any
request	headers	(line	20).
Next,	we	parse	the	URI	into	a	filename	and	a	possibly	empty	CGI
argument	string,	and	we	set	a	flag	that	indicates	whether	the	request	is
for	static	or	dynamic	content	(line	23).	If	the	file	does	not	exist	on	disk,	we
immediately	send	an	error	message	to	the	client	and	return.</p>
<p>Finally,	if	the	request	is	for	static	content,	we	verify	that	the	file	is	a
regular	file	and	that	we	have	read	permission	(line	31).	If	so,	we	serve	the
static	content	(line	36)	to	the	client.	Similarly,	if	the	request	is	for	dynamic
content,	we	verify	that	the	file	is	executable	(line	39),	and,	if	so,	we	go
ahead	and	serve	the	dynamic	content	(line	44).
The	
Function
T
INY</p>
<p>lacks	many	of	the	error-handling	features	of	a	real	server.	However,
it	does	check	for	some	obvious	errors	and	reports	them	to	the	client.	The
function	in	
Figure	
11.31
sends	an	HTTP	response	to	the
client	with	the	appropriate</p>
<p>Figure	
11.31	
sends	an	error	message	to	the	client.</p>
<p>Figure	
11.32	
reads	and	ignores	request
headers.
status	code	and	status	message	in	the	response	line,	along	with	an
HTML	file	in	the	response	body	that	explains	the	error	to	the	browser's
user.
Recall	that	an	HTML	response	should	indicate	the	size	and	type	of	the
content	in	the	body.	Thus,	we	have	opted	to	build	the	HTML	content	as	a
single	string	so	that	we	can	easily	determine	its	size.	Also,	notice	that	we
are	using	the	robust	
function	from	
Figure	
10.4
for	all
output.
The	
Function
T
INY</p>
<p>does	not	use	any	of	the	information	in	the	request	headers.	It	simply
reads	and	ignores	them	by	calling	the	
function	in
Figure	
11.32
.	Notice	that	the	empty	text	line	that	terminates	the
request	headers	consists	of	a	carriage	return	and	line	feed	pair,	which	we
check	for	in	line	6.
The	
Function
T
INY</p>
<p>assumes	that	the	home	directory	for	static	content	is	its	current
directory	and	that	the	home	directory	for	executables	is	
.	Any
URI	that	contains	the	string	
is	assumed	to	denote	a	request	for
dynamic	content.	The	default	filename	is	
.</p>
<p>The	
function	in	
Figure	
11.33
implements	these	policies.	It
parses	the	URI	into	a	filename	and	an	optional	CGI	argument	string.	If
the	request	is	for	static	content	(line	5),	we	clear	the	CGI	argument	string
(line	6)	and	then	convert	the	URI	into	a	relative	Linux	pathname	such	as
(lines	7−8).	If	the	URI	ends	with	a	`/'	character	(line	9),	then
we	append	the	default	filename	(line	10).	On	the	other	hand,	if	the
request	is	for	dynamic	content	(line	13),	we	extract	any	CGI	arguments
(lines	14−20)	and	convert	the	remaining	portion	of	the	URI	to	a	relative
Linux	filename	(lines	21−22).</p>
<p>Figure	
11.33	
T
INY</p>
<p>parse_uri	parses	an	HTTP	URI.
The	
Function
T
INY</p>
<p>serves	five	common	types	of	static	content:	HTML	files,	unformatted
text	files,	and	images	encoded	in	GIF,	PNG,	and	JPEG	formats.
The	
function	in	
Figure	
11.34
sends	an	HTTP	response
whose	body	contains	the	contents	of	a	local	file.	First,	we	determine	the
file	type	by	inspecting	the	suffix	in	the	filename	(line	7)	and	then	send	the
response	line	and	response	headers	to	the	client	(lines	8−13).	Notice	that
a	blank	line	terminates	the	headers.
Next,	we	send	the	response	body	by	copying	the	contents	of	the
requested	file	to	the	connected	descriptor	
.	The	code	here	is
somewhat	subtle	and	needs	to	be	studied	carefully.	Line	18	opens
for	reading	and	gets	its	descriptor.	In	line	19,	the	Linux	
function	maps	the	requested	file	to	a	virtual	memory	area.	Recall	from
our	discussion	of	
in	
Section	
9.8
that	the	call	to	
maps	the</p>
<p>Figure	
11.34	
T
INY</p>
<p>serve_static	serves	static	content	to	a	client.
first	
bytes	of	file	
to	a	private	read-only	area	of	virtual
memory	that	starts	at	address	
.
Once	we	have	mapped	the	file	to	memory,	we	no	longer	need	its
descriptor,	so	we	close	the	file	(line	20).	Failing	to	do	this	would	introduce
a	potentially	fatal	memory	leak.	Line	21	performs	the	actual	transfer	of
the	file	to	the	client.	The	
function	copies	the	
bytes
starting	at	location	
(which	of	course	is	mapped	to	the	requested	file)
to	the	client's	connected	descriptor.	Finally,	line	22	frees	the	mapped
virtual	memory	area.	This	is	important	to	avoid	a	potentially	fatal	memory
leak.</p>
<p>The	
Function
T
INY</p>
<p>serves	any	type	of	dynamic	content	by	forking	a	child	process	and
then	running	a	CGI	program	in	the	context	of	the	child.
The	
function	in	
Figure	
11.35
begins	by	sending	a
response	line	indicating	success	to	the	client,	along	with	an	informational
header.	The	CGI	program	is	responsible	for	sending	the	rest	of
the	response.	Notice	that	this	is	not	as	robust	as	we	might	wish,	since	it
doesn't	allow	for	the	possibility	that	the	CGI	program	might	encounter
some	error.
After	sending	the	first	part	of	the	response,	we	fork	a	new	child	process
(line	11).	The	child	initializes	the	QUERY_STRING	environment	variable
with	the	CGI	arguments	from	the	request	URI	(line	13).	Notice	that	a	real
server	would</p>
<p>Figure	
11.35	
T
INY</p>
<p>serve_dynamic	serves	dynamic	content	to	a	client.
Aside	
Dealing	with	prematurely	closed
connections
Although	the	basic	functions	of	a	Web	server	are	quite	simple,	we
don't	want	to	give	you	the	false	impression	that	writing	a	real	Web
server	is	easy.	Building	a	robust	Web	server	that	runs	for
extended	periods	without	crashing	is	a	difficult	task	that	requires	a
deeper	understanding	of	Linux	systems	programming	than	we've
learned	here.	For	example,	if	a	server	writes	to	a	connection	that
has	already	been	closed	by	the	client	(say,	because	you	clicked
the	&quot;Stop&quot;	button	on	your	browser),	then	the	first	such	write
returns	normally,	but	the	second	write	causes	the	delivery	of	a
SIGPIPE	signal	whose	default	behavior	is	to	terminate	the
process.	If	the	SIGPIPE	signal	is	caught	or	ignored,	then	the</p>
<p>second	write	operation	returns	−1	with	
set	to	EPIPE.	The
and	
functions	report	the	EPIPE	error	as	a	&quot;Broken
pipe,&quot;	a	nonintuitive	message	that	has	confused	generations	of
students.	The	bottom	line	is	that	a	robust	server	must	catch	these
SIGPIPE	signals	and	check	
function	calls	for	EPIPE	errors.
set	the	other	CGI	environment	variables	here	as	well.	For	brevity,	we
have	omitted	this	step.
Next,	the	child	redirects	the	child's	standard	output	to	the	connected	file
descriptor	(line	14)	and	then	loads	and	runs	the	CGI	program	(line	15).
Since	the	CGI	program	runs	in	the	context	of	the	child,	it	has	access	to
the	same	open	files	and	environment	variables	that	existed	before	the
call	to	the	
function.	Thus,	everything	that	the	CGI	program	writes
to	standard	output	goes	directly	to	the	client	process,	without	any
intervention	from	the	parent	process.	Meanwhile,	the	parent	blocks	in	a
call	to	
,	waiting	to	reap	the	child	when	it	terminates	(line	17).</p>
<p>11.7	
Summary
Every	network	application	is	based	on	the	client-server	model.	With	this
model,	an	application	consists	of	a	server	and	one	or	more	clients.	The
server	manages	resources,	providing	a	service	for	its	clients	by
manipulating	the	resources	in	some	way.	The	basic	operation	in	the
client-server	model	is	a	client-server	transaction,	which	consists	of	a
request	from	a	client,	followed	by	a	response	from	the	server.
Clients	and	servers	communicate	over	a	global	network	known	as	the
Internet.	From	a	programmer's	point	of	view,	we	can	think	of	the	Internet
as	a	worldwide	collection	of	hosts	with	the	following	properties:	(1)	Each
Internet	host	has	a	unique	32-bit	name	called	its	IP	address.	(2)	The	set
of	IP	addresses	is	mapped	to	a	set	of	Internet	domain	names.	(3)
Processes	on	different	Internet	hosts	can	communicate	with	each	other
over	connections.
Clients	and	servers	establish	connections	by	using	the	sockets	interface.
A	socket	is	an	end	point	of	a	connection	that	is	presented	to	applications
in	the	form	of	a	file	descriptor.	The	sockets	interface	provides	functions
for	opening	and	closing	socket	descriptors.	Clients	and	servers
communicate	with	each	other	by	reading	and	writing	these	descriptors.
Web	servers	and	their	clients	(such	as	browsers)	communicate	with	each
other	using	the	HTTP	protocol.	A	browser	requests	either	static	or
dynamic	content	from	the	server.	A	request	for	static	content	is	served	by
fetching	a	file	from	the	server's	disk	and	returning	it	to	the	client.	A</p>
<p>request	for	dynamic	content	is	served	by	running	a	program	in	the
context	of	a	child	process	on	the	server	and	returning	its	output	to	the
client.	The	CGI	standard	provides	a	set	of	rules	that	govern	how	the
client	passes	program	arguments	to	the	server,	how	the	server	passes
these	arguments	and	other	information	to	the	child	process,	and	how	the
child	sends	its	output	back	to	the	client.	A	simple	but	functioning	Web
server	that	serves	both	static	and	dynamic	content	can	be	implemented
in	a	few	hundred	lines	of	C	code.</p>
<p>Bibliographic	Notes
The	official	source	of	information	for	the	Internet	is	contained	in	a	set	of
freely	available	numbered	documents	known	as	
RFCs	(requests	for
comments)
.	A	searchable	index	of	RFCs	is	available	on	the	Web	at
RFCs	are	typically	written	for	developers	of	Internet	infrastructure,	and
thus	they	are	usually	too	detailed	for	the	casual	reader.	However,	for
authoritative	information,	there	is	no	better	source.	The	HTTP/1.1
protocol	is	documented	in	RFC	2616.	The	authoritative	list	of	MIME	types
is	maintained	at
http:/
/
www.iana.org/
assignments/
media-types
Kerrisk	is	the	bible	for	all	aspects	of	Linux	programming	and	provides	a
detailed	discussion	of	modern	network	programming	
[62]
.	There	are	a
number	of	good	general	texts	on	computer	networking	
[65
,	
84
,	
114]
.	The
great	technical	writer	W.	Richard	Stevens	developed	a	series	of	classic
texts	on	such	topics	as	advanced	Unix	programming	
[111]
,	the	Internet
protocols	
[109
,	
120
,	
107]
,	and	Unix	network	programming	
[108
,	
110]
.
Serious	students	of	Unix	systems	programming	will	want	to	study	all	of
them.	Tragically,	Stevens	died	on	September	1,	1999.	His	contributions
are	greatly	missed.</p>
<p>Homework	Problems
11.6
Modify	T
INY</p>
<p>so	that	it	echoes	every	request	line	and	request
header.
Use	your	favorite	browser	to	make	a	request	to	T
INY</p>
<p>for	static
content.	Capture	the	output	from	T
INY</p>
<p>in	a	file.
Inspect	the	output	from	T
INY</p>
<p>to	determine	the	version	of	HTTP	your
browser	uses.
Consult	the	HTTP/1.1	standard	in	RFC	2616	to	determine	the
meaning	of	each	header	in	the	HTTP	request	from	your	browser.
You	can	obtain	RFC	2616	from	
www.rfc-editor.org/
rfc.html
.
11.7
Extend	T
INY</p>
<p>so	that	it	serves	MPG	video	files.	Check	your	work	using	a
real	browser.
11.8</p>
<p>Modify	T
INY</p>
<p>so	that	it	reaps	CGI	children	inside	a	SIGCHLD	handler
instead	of	explicitly	waiting	for	them	to	terminate.
11.9
Modify	T
INY</p>
<p>so	that	when	it	serves	static	content,	it	copies	the	requested
file	to	the	connected	descriptor	using	
,	and	
,
instead	of	
and	
.
11.10
A
.	
Write	an	HTML	form	for	the	CGI	
function	in	
Figure	
11.27
.
Your	form	should	include	two	text	boxes	that	users	fill	in	with	the
two	numbers	to	be	added	together.	Your	form	should	request
content	using	the	GET	method.
B
.	
Check	your	work	by	using	a	real	browser	to	request	the	form	from
T
INY
,	submit	the	filled-in	form	to	T
INY
,	and	then	display	the	dynamic
content	generated	by	
.
11.11
Extend	T
INY</p>
<p>to	support	the	HTTP	HEAD	method.	Check	your	work	using
TELNET</p>
<p>as	a	Web	client.</p>
<p>11.12
Extend	T
INY</p>
<p>so	that	it	serves	dynamic	content	requested	by	the	HTTP
POST	method.	Check	your	work	using	your	favorite	Web	browser.
11.13
Modify	T
INY</p>
<p>so	that	it	deals	cleanly	(without	terminating)	with	the	SIGPIPE
signals	and	EPIPE	errors	that	occur	when	the	write	function	attempts	to
write	to	a	prematurely	closed	connection.</p>
<p>Solutions	to	Practice	Problems
Solution	to	Problem	
11.1	
(page
927
)
Hex	address
Dotted-decimal	address
Solution	to	Problem	
11.2	
(page
927
)</p>
<p>Solution	to	Problem	
11.3	
(page</p>
<p>927
)</p>
<p>Solution	to	Problem	
11.4	
(page
942
)
Here's	a	solution.	Notice	how	much	more	difficult	it	is	to	use	
,
which	requires	messy	casting	and	deep	structure	references.	The
function	is	much	simpler	because	it	does	all	of	that	work	for
us.</p>
<p>Solution	to	Problem	
11.5	
(page</p>
<p>954
)
The	reason	that	standard	I/O	works	in	CGI	programs	is	that	the	CGI
program	running	in	the	child	process	does	not	need	to	explicitly	close	any
of	its	input	or	output	streams.	When	the	child	terminates,	the	kernel
closes	all	descriptors	automatically.</p>
<p>Chapter	
12	
Concurrent
Programming
12.1	
Concurrent	Programming	with	Processes	
973
12.2	
Concurrent	Programming	with	I/O	Multiplexing	
977
12.3	
Concurrent	Programming	with	Threads	
985
12.4	
Shared	Variables	in	Threaded	Programs	
992
12.5	
Synchronizing	Threads	with	Semaphores	
995
12.6	
Using	Threads	for	Parallelism	
1013
12.7	
Other	Concurrency	Issues	
1020
12.8	
Summary</p>
<p>1030
Bibliographic	Notes	
1030
Homework	Problems	
1031
Solutions	to	Practice	Problems	
1036
As	we	learned	in	
Chapter	
8
,	logical	control	flows
are	
concurrent
if	they	overlap	in	time.	This	general</p>
<p>phenomenon,	known	as	
concurrency
,	shows	up	at
many	different	levels	of	a	computer	system.
Hardware	exception	handlers,	processes,	and	Linux
signal	handlers	are	all	familiar	examples.
Thus	far,	we	have	treated	concurrency	mainly	as	a
mechanism	that	the	operating	system	kernel	uses	to
run	multiple	application	programs.	But	concurrency
is	not	just	limited	to	the	kernel.	It	can	play	an
important	role	in	application	programs	as	well.	For
example,	we	have	seen	how	Linux	signal	handlers
allow	applications	to	respond	to	asynchronous
events	such	as	the	user	typing	Ctrl+C	or	the
program	accessing	an	undefined	area	of	virtual
memory.	Application-level	concurrency	is	useful	in
other	ways	as	well:
Accessing	slow	I/O	devices.	
When	an
application	is	waiting	for	data	to	arrive	from	a
slow	I/O	device	such	as	a	disk,	the	kernel	keeps
the	CPU	busy	by	running	other	processes.
Individual	applications	can	exploit	concurrency	in
a	similar	way	by	overlapping	useful	work	with	I/O
requests.
Interacting	with	humans.	
People	who	interact
with	computers	demand	the	ability	to	perform
multiple	tasks	at	the	same	time.	For	example,
they	might	want	to	resize	a	window	while	they
are	printing	a	document.	Modern	windowing</p>
<p>systems	use	concurrency	to	provide	this
capability.	Each	time	the	user	requests	some
action	(say,	by	clicking	the	mouse),	a	separate
concurrent	logical	flow	is	created	to	perform	the
action.
Reducing	latency	by	deferring	work.
Sometimes,	applications	can	use	concurrency	to
reduce	the	latency	of	certain	operations	by
deferring	other	operations	and	performing	them
concurrently.	For	example,	a	dynamic	storage
allocator	might	reduce	the	latency	of	individual
free	operations	by	deferring	coalescing	to	a
concurrent	&quot;coalescing&quot;	flow	that	runs	at	a	lower
priority,	soaking	up	spare	CPU	cycles	as	they
become	available.
Servicing	multiple	network	clients.	
The
iterative	network	servers	that	we	studied	in
Chapter	
11
are	unrealistic	because	they	can
only	service	one	client	at	a	time.	Thus,	a	single
slow	client	can	deny	service	to	every	other	client.
For	a	real	server	that	might	be	expected	to
service	hundreds	or	thousands	of	clients	per
second,	it	is	not	acceptable	to	allow	one	slow
client	to	deny	service	to	the	others.	A	better
approach	is	to	build	a	
concurrent	server
that
creates	a	separate	logical	flow	for	each	client.
This	allows	the	server	to	service	multiple	clients
concurrently	and	precludes	slow	clients	from
monopolizing	the	server.</p>
<p>Computing	in	parallel	on	multi-core
machines.	
Many	modern	systems	are	equipped
with	multi-core	processors	that	contain	multiple
CPUs.	Applications	that	are	partitioned	into
concurrent	flows	often	run	faster	on	multi-core
machines	than	on	uniprocessor	machines
because	the	flows	execute	in	parallel	rather	than
being	interleaved.
Applications	that	use	application-level	concurrency
are	known	as	
concurrent	programs
.	Modern
operating	systems	provide	three	basic	approaches
for	building	concurrent	programs:
Processes.	
With	this	approach,	each	logical
control	flow	is	a	process	that	is	scheduled	and
maintained	by	the	kernel.	Since	processes	have
separate	virtual	address	spaces,	flows	that	want
to	communicate	with	each	other	must	use	some
kind	of	explicit	
interprocess	communication	(IPC)
mechanism.
I/O	multiplexing.	
his	is	a	form	of	concurrent
programming	where	applications	explicitly
schedule	their	own	logical	flows	in	the	context	of
a	single	process.	Logical	flows	are	modeled	as
state	machines	that	the	main	program	explicitly
transitions	from	state	to	state	as	a	result	of	data
arriving	on	file	descriptors.	Since	the	program	is</p>
<p>a	single	process,	all	flows	share	the	same
address	space.
Threads.	
Threads	are	logical	flows	that	run	in
the	context	of	a	single	process	and	are
scheduled	by	the	kernel.	You	can	think	of
threads	as	a	hybrid	of	the	other	two	approaches,
scheduled	by	the	kernel	like	process	flows	and
sharing	the	same	virtual	address	space	like	I/O
multiplexing	flows.
This	chapter	investigates	these	three	different
concurrent	programming	techniques.	To	keep	our
discussion	concrete,	we	will	work	with	the	same
motivating	application	throughout—a	concurrent
version	of	the	iterative	echo	server	from	
Section
11.4.9
.</p>
<p>12.1	
Concurrent	Programming	with
Processes
The	simplest	way	to	build	a	concurrent	program	is	with	processes,	using
familiar	functions	such	as	
,	and	
.	For	example,	a
natural	approach	for	building	a	concurrent	server	is	to	accept	client
connection	requests	in	the	parent	and	then	create	a	new	child	process	to
service	each	new	client.
To	see	how	this	might	work,	suppose	we	have	two	clients	and	a	server
that	is	listening	for	connection	requests	on	a	listening	descriptor	(say,	3).
Now	suppose	that	the	server	accepts	a	connection	request	from	client	1
and	returns	a	connected	descriptor	(say,	4),	as	shown	in	
Figure	
12.1
.
After	accepting	the	connection	request,	the	server	forks	a	child,	which
gets	a	complete	copy	of	the	server's	descriptor	table.	The	child	closes	its
copy	of	listening	descriptor	3,	and	the	parent	closes	its	copy	of	connected
descriptor	4,	since	they	are	no	longer	needed.	This	gives	us	the	situation
shown	in	
Figure	
12.2
,	where	the	child	process	is	busy	servicing	the
client.
Since	the	connected	descriptors	in	the	parent	and	child	each	point	to	the
same	file	table	entry,	it	is	crucial	for	the	parent	to	close	its	copy	of	the
connected</p>
<p>Figure	
12.1	
Step	1:	Server	accepts	connection	request	from	client.
Figure	
12.2	
Step	2:	Server	forks	a	child	process	to	service	the	client.
Figure	
12.3	
Step	3:	Server	accepts	another	connection	request.</p>
<p>descriptor.	Otherwise,	the	file	table	entry	for	connected	descriptor	4	will
never	be	released,	and	the	resulting	memory	leak	will	eventually
consume	the	available	memory	and	crash	the	system.
Now	suppose	that	after	the	parent	creates	the	child	for	client	1,	it	accepts
a	new	connection	request	from	client	2	and	returns	a	new	connected
descriptor	(say,	5),	as	shown	in	
Figure	
12.3
.	The	parent	then	forks
another	child,	which	begins	servicing	its	client	using	connected	descriptor
5,	as	shown	in	
Figure	
12.4
.	At	this	point,	the	parent	is	waiting	for	the
next	connection	request	and	the	two	children	are	servicing	their
respective	clients	concurrently.
12.1.1	
A	Concurrent	Server	Based
on	Processes
Figure	
12.5
shows	the	code	for	a	concurrent	echo	server	based	on
processes.	The	echo	function	called	in	line	29	comes	from	
Figure
11.22
.	There	are	several	important	points	to	make	about	this	server:
First,	servers	typically	run	for	long	periods	of	time,	so	we	must	include
a	SIGCHLD	handler	that	reaps	zombie	children	(lines	4−9).	Since
SIGCHLD	signals	are	blocked	while	the	SIGCHLD	handler	is
executing,	and	since	Linux	signals	are	not	queued,	the	SIGCHLD
handler	must	be	prepared	to	reap	multiple	zombie	children.
Second,	the	parent	and	the	child	must	close	their	respective	copies	of
(lines	33	and	30,	respectively).	As	we	have	mentioned,	this	is
especially	important</p>
<p>Figure	
12.4	
Step	4:	Server	forks	another	child	to	service	the	new
client.
for	the	parent,	which	must	close	its	copy	of	the	connected	descriptor
to	avoid	a	memory	leak.
Finally,	because	of	the	reference	count	in	the	socket's	file	table	entry,
the	connection	to	the	client	will	not	be	terminated	until	both	the
parent's	and	child's	copies	of	
are	closed.
12.1.2	
Pros	and	Cons	of	Processes
Processes	have	a	clean	model	for	sharing	state	information	between
parents	and	children:	file	tables	are	shared	and	user	address	spaces	are
not.	Having	separate	address	spaces	for	processes	is	both	an	advantage
and	a	disadvantage.	It	is	impossible	for	one	process	to	accidentally
overwrite	the	virtual	memory	of	another	process,	which	eliminates	a	lot	of
confusing	failures—an	obvious	advantage.</p>
<h2>On	the	other	hand,	separate	address	spaces	make	it	more	difficult	for
processes	to	share	state	information.	To	share	information,	they	must	use
explicit	IPC	(interprocess	communications)	mechanisms.	(See	the	Aside
on	
page	977
.)	Another	disadvantage	of	process-based	designs	is	that
they	tend	to	be	slower	because	the	overhead	for	process	control	and	IPC
is	high.
Practice	Problem	
12.1	
(solution	page	
1036
)
After	the	parent	closes	the	connected	descriptor	in	line	33	of	the
concurrent	server	in	
Figure	
12.5
,	the	child	is	still	able	to
communicate	with	the	client	using	its	copy	of	the	descriptor.	Why?
Practice	Problem	
12.2	
(solution	page	
1036
)
If	we	were	to	delete	line	30	of	
Figure	
12.5
,	which	closes	the
connected	descriptor,	the	code	would	still	be	correct,	in	the	sense
that	there	would	be	no	memory	leak.	Why?</h2>
<p>code/conc/echoserverp.c</p>
<hr />
<p>code/conc/echoserverp.c
Figure	
12.5	
Concurrent	echo	server	based	on	processes.
The	parent	forks	a	child	to	handle	each	new	connection	request.
Aside	
Unix	IPC
You	have	already	encountered	several	examples	of	IPC	in	this
text.	The	
function	and	signals	from	
Chapter	
8
are
primitive	IPC	mechanisms	that	allow	processes	to	send	tiny
messages	to	process	running	on	the	same	host.	The	sockets
interface	from	
Chapter	
11
is	an	important	form	of	IPC	that
allows	processes	on	different	hosts	to	exchange	arbitrary	byte
streams.	However,	the	term	
Unix	IPC
is	typically	reserved	for	a
hodgepodge	of	techniques	that	allow	processes	to	communicate
with	other	processes	that	are	running	on	the	same	host.	Examples
include	pipes,	FIFOs,	System	V	shared	memory,	and	System	V
semaphores.	These	mechanisms	are	beyond	our	scope.	The	book
by	Kerrisk	[
62
]	is	an	excellent	reference.</p>
<p>12.2	
Concurrent	Programming	with
I/O	Multiplexing
Suppose	you	are	asked	to	write	an	echo	server	that	can	also	respond	to
interactive	commands	that	the	user	types	to	standard	input.	In	this	case,
the	server	must	respond	to	two	independent	I/O	events:	(1)	a	network
client	making	a	connection	request,	and	(2)	a	user	typing	a	command	line
at	the	keyboard.	Which	event	do	we	wait	for	first?	Neither	option	is	ideal.
If	we	are	waiting	for	a	connection	request	in	accept,	then	we	cannot
respond	to	input	commands.	Similarly,	if	we	are	waiting	for	an	input
command	in	read,	then	we	cannot	respond	to	any	connection	requests.
One	solution	to	this	dilemma	is	a	technique	called	
I/O	multiplexing
.	The
basic	idea	is	to	use	the	
function	to	ask	the	kernel	to	suspend	the
process,	returning	control	to	the	application	only	after	one	or	more	I/O
events	have	occurred,	as	in	the	following	examples:
Return	when	any	descriptor	in	the	set	{0,	4}	is	ready	for	reading.
Return	when	any	descriptor	in	the	set	{1,	2,	7}	is	ready	for	writing.
Time	out	if	152.13	seconds	have	elapsed	waiting	for	an	I/O	event	to
occur.
is	a	complicated	function	with	many	different	usage	scenarios.	We
will	only	discuss	the	first	scenario:	waiting	for	a	set	of	descriptors	to	be
ready	for	reading.	See	[
62
,	
110
]	for	a	complete	discussion.</p>
<h2>The	
function	manipulates	sets	of	type	
,	which	are	known	as
descriptor	sets
.	Logically,	we	think	of	a	descriptor	set	as	a	bit	vector
(introduced	in	
Section	
2.1
)	of	size	
n
:
Each	bit	
b
corresponds	to	descriptor	
k
.	Descriptor	
k
is	a	member	of	the
descriptor	set	if	and	only	if	
b
=	1.	You	are	only	allowed	to	do	three	things
with	descriptor	sets:	(1)	allocate	them,	(2)	assign	one	variable	of	this	type
to	another,	and	(3)	modify	and	inspect	them	using	the	FD_ZERO,
FD_SET,	FD_CLR,	and	FD_ISSET	macros.
For	our	purposes,	the	select	function	takes	two	inputs:	a	descriptor	set
(
)	called	the	
read	set
,	and	the	cardinality	(n)	of	the	read	set	(actually
b
n</h2>
<p>1
,
 
…
,
 
b
1
,
 
b
0
k
k</p>
<p>the	maximum	cardinality	of	any	descriptor	set).	The	
function
blocks	until	at	least	one	descriptor	in	the	read	set	is	ready	for	reading.	A
descriptor	
k
is	
ready	for	reading
if	and	only	if	a	request	to	read	1	byte
from	that	descriptor	would	not	block.	As	a	side	effect,	
modifies	the
pointed	to	by	argument	
to	indicate	a	subset	of	the	read	set
called	the	
ready	set
,	consisting	of	the	descriptors	in	the	read	set	that	are
ready	for	reading.	The	value	returned	by	the	function	indicates	the
cardinality	of	the	ready	set.	Note	that	because	of	the	side	effect,	we	must
update	the	read	set	every	time	
is	called.
The	best	way	to	understand	
is	to	study	a	concrete	example.
Figure	
12.6
shows	how	we	might	use	
to	implement	an	iterative
echo	server	that	also	accepts	user	commands	on	the	standard	input.	We
begin	by	using	the	
function	from	
Figure	
11.19
to	open	a
listening	descriptor	(line	16),	and	then	using	FD_ZERO	to	create	an
empty	read	set	(line	18):
Next,	in	lines	19	and	20,	we	define	the	read	set	to	consist	of	descriptor	0
(standard	input)	and	descriptor	3	(the	listening	descriptor),	respectively:</p>
<h2>At	this	point,	we	begin	the	typical	server	loop.	But	instead	of	waiting	for	a
connection	request	by	calling	the	
function,	we	call	the	
function,	which	blocks	until	either	the	listening	descriptor	or	standard
input	is	ready	for	reading	(line	24).	For	example,	here	is	the	value	of
that	
would	return	if	the	user	hit	the	enter	key,	thus
causing	the	standard	input	descriptor	to</h2>
<p>code/conc/select.c</p>
<hr />
<p>code/conc/select.c
Figure	
12.6	
An	iterative	echo	server	that	uses	I/O	multiplexing.
The	server	uses	
to	wait	for	connection	requests	on	a	listening
descriptor	and	commands	on	standard	input.
become	ready	for	reading:
Once	
returns,	we	use	the	FD_ISSET	macro	to	determine	which
descriptors	are	ready	for	reading.	If	standard	input	is	ready	(line	25),	we
call	the	
function,	which	reads,	parses,	and	responds	to	the
command	before	returning	to	the	main	routine.	If	the	listening	descriptor
is	ready	(line	27),	we	call	
to	get	a	connected	descriptor	and	then
call	the	echo	function	from	
Figure	
11.22
,	which	echoes	each	line	from
the	client	until	the	client	closes	its	end	of	the	connection.
While	this	program	is	a	good	example	of	using	
,	it	still	leaves
something	to	be	desired.	The	problem	is	that	once	it	connects	to	a	client,
it	continues	echoing	input	lines	until	the	client	closes	its	end	of	the
connection.	Thus,	if	you	type	a	command	to	standard	input,	you	will	not
get	a	response	until	the	server	is	finished	with	the	client.	A	better
approach	would	be	to	multiplex	at	a	finer	granularity,	echoing	(at	most)
one	text	line	each	time	through	the	server	loop.</p>
<p>Practice	Problem	
12.3	
(solution
page	
1036
)
In	Linux	systems,	typing	Ctrl+D	indicates	EOF	on	standard	input.
What	happens	if	you	type	Ctrl+D	to	the	program	in	
Figure	
12.6
while	it	is	blocked	in	the	call	to	
?
12.2.1	
A	Concurrent	Event-Driven
Server	Based	on	I/O	Multiplexing
I/O	multiplexing	can	be	used	as	the	basis	for	concurrent	
event-driven
programs,	where	flows	make	progress	as	a	result	of	certain	events.	The
general	idea	is	to	model	logical	flows	as	state	machines.	Informally,	a
state	machine
is	a	collection	of	
states
,	
input	events
,	and	
transitions
that
map	states	and	input	events	to	states.	Each	transition	maps	an	(input
state,	input	event)	pair	to	an	output	state.	A	
self-loop
is	a	transition
between	the	same	input	and	output	state.	State	machines	are	typically
drawn	as	directed	graphs,	where	nodes	represent	states,	directed	arcs
represent	transitions,	and	arc	labels	represent	input	events.	A	state
machine	begins	execution	in	some	initial	state.	Each	input	event	triggers
a	transition	from	the	current	state	to	the	next	state.
For	each	new	client	
k
,	a	concurrent	server	based	on	I/O	multiplexing
creates	a	new	state	machine	
s
and	associates	it	with	connected
descriptor	
d
.	As	shown	in	
Figure	
12.7
,	each	state	machine	
s
has	one
k
k
k</p>
<p>state	(&quot;waiting	for	descriptor	
d
to	be	ready	for	reading&quot;),	one	input	event
(&quot;descriptor	
d
is	ready	for	reading&quot;),	and	one	transition	(&quot;read	a	text	line
from	descriptor	
d
&quot;).
Figure	
12.7	
State	machine	for	a	logical	flow	in	a	concurrent	event-
driven	echo	server.
The	server	uses	the	I/O	multiplexing,	courtesy	of	the	
function,	to
detect	the	occurrence	of	input	events.	As	each	connected	descriptor
becomes	ready	for	reading,	the	server	executes	the	transition	for	the
corresponding	state	machine—in	this	case,	reading	and	echoing	a	text
line	from	the	descriptor.
Figure	
12.8
shows	the	complete	example	code	for	a	concurrent	event-
driven	server	based	on	I/O	multiplexing.	The	set	of	active	clients	is
maintained	in	a	pool	structure	(lines	3−11).	After	initializing	the	pool	by
calling	
(line	27),	the	server	enters	an	infinite	loop.	During	each
iteration	of	this	loop,	the	server	calls	the	
function	to	detect	two
different	kinds	of	input	events:	(1)	a	connection	request	arriving	from	a
new	client,	and	(2)	a	connected	descriptor	for	an	existing	client	being
ready	for	reading.	When	a	connection	request	arrives	(line	35),	the	server
k
k
k</p>
<p>opens	the	connection	(line	37)	and	calls	the	
function	to	add
the	client	to	the	pool	(line	38).	Finally,	the	server	calls	the	
function	to	echo	a	single	text	line	from	each	ready	connected	descriptor
(line	42).
The	
function	(
Figure	
12.9
)	initializes	the	client	pool.	The
array	represents	a	set	of	connected	descriptors,	with	the	integer
−1	denoting	an	available	slot.	Initially,	the	set	of	connected	descriptors	is
empty	(lines	5−7),	and	the	listening	descriptor	is	the	only	descriptor	in	the
read	set	(lines	10−12).
The	
function	(
Figure	
12.10
)	adds	a	new	client	to	the	pool
of	active	clients.	After	finding	an	empty	slot	in	the	
array,	the
server	adds	the	connected	descriptor	to	the	array	and	initializes	a
corresponding	R
IO</p>
<p>read	buffer	so	that	we	can	call	
on	the
descriptor	(lines	8−9).	We	then	add	the	connected	descriptor	to	the
read	set	(line	12),	and	we	update	some	global	properties	of	the
pool.	The	
variable	(lines	15−16)	keeps	track	of	the	largest	file
descriptor	for	
.	The	maxi	variable	(lines	17−18)	keeps	track	of	the
largest	index	into	the	
array	so	that	the	
function
does	not	have	to	search	the	entire	array.
The	
function	in	
Figure	
12.11
echoes	a	text	line	from
each	ready	connected	descriptor.	If	we	are	successful	in	reading	a	text
line	from	the	descriptor,	then	we	echo	that	line	back	to	the	client	(lines
15−18).	Notice	that	in	line	15,	we	are	maintaining	a	cumulative	count	of
total	bytes	received	from	all	clients.	If	we	detect	EOF	because	the	client
has	closed	its	end	of	the	connection,	then	we	close	our	end	of	the</p>
<h2>connection	(line	23)	and	remove	the	descriptor	from	the	pool	(lines
24−25).</h2>
<p>code/conc/echoservers.c</p>
<hr />
<h2>code/conc/echoservers.c
Figure	
12.8	
Concurrent	echo	server	based	on	I/O	multiplexing.
Each	server	iteration	echoes	a	text	line	from	each	ready	descriptor.</h2>
<p>code/conc/echoservers.c</p>
<hr />
<h2>code/conc/echoservers.c
Figure	
12.9	
initializes	the	pool	of	active	clients.</h2>
<p>code/conc/echoservers.c</p>
<hr />
<h2>code/conc/echoservers.c
Figure	
12.10	
adds	a	new	client	connection	to	the	pool.</h2>
<p>code/conc/echoservers.c</p>
<hr />
<p>code/conc/echoservers.c
Figure	
12.11	
services	ready	client	connections.</p>
<p>In	terms	of	the	finite	state	model	in	
Figure	
12.7
,	the	
function
detects	input	events,	and	the	
function	creates	a	new	logical
flow	(state	machine).	The	
function	performs	state
transitions	by	echoing	input	lines,	and	it	also	deletes	the	state	machine
when	the	client	has	finished	sending	text	lines.
Practice	Problem	
12.4	
(solution	page	
1036
)
In	the	server	in	
Figure	
12.8
,	we	are	careful	to	reinitialize	the
variable	immediately	before	every	call	to	
.
Why?
Aside	
Event-driven	Web	servers
Despite	the	disadvantages	outlined	in	
Section	
12.2.2
,	modern
high-performance	servers	such	as	Node.js,	nginx,	and	Tornado
use	event-driven	programming	based	on	I/O	multiplexing,	mainly
because	of	the	significant	performance	advantage	compared	to
processes	and	threads.
12.2.2	
Pros	and	Cons	of	I/O
Multiplexing
The	server	in	
Figure	
12.8
provides	a	nice	example	of	the	advantages
and	disadvantages	of	event-driven	programming	based	on	I/O
multiplexing.	One	advantage	is	that	event-driven	designs	give</p>
<p>programmers	more	control	over	the	behavior	of	their	programs	than
process-based	designs.	For	example,	we	can	imagine	writing	an	event-
driven	concurrent	server	that	gives	preferred	service	to	some	clients,
which	would	be	difficult	for	a	concurrent	server	based	on	processes.
Another	advantage	is	that	an	event-driven	server	based	on	I/O
multiplexing	runs	in	the	context	of	a	single	process,	and	thus	every
logical	flow	has	access	to	the	entire	address	space	of	the	process.	This
makes	it	easy	to	share	data	between	flows.	A	related	advantage	of
running	as	a	single	process	is	that	you	can	debug	your	concurrent	server
as	you	would	any	sequential	program,	using	a	familiar	debugging	tool
such	as	
GDB
.	Finally,	event-driven	designs	are	often	significantly	more
efficient	than	process-based	designs	because	they	do	not	require	a
process	context	switch	to	schedule	a	new	flow.
A	significant	disadvantage	of	event-driven	designs	is	coding	complexity.
Our	event-driven	concurrent	echo	server	requires	three	times	more	code
than	the	process-based	server.	Unfortunately,	the	complexity	increases
as	the	granularity	of	the	concurrency	decreases.	By	
granularity
,	we	mean
the	number	of	instructions	that	each	logical	flow	executes	per	time	slice.
For	instance,	in	our	example	concurrent	server,	the	granularity	of
concurrency	is	the	number	of	instructions	required	to	read	an	entire	text
line.	As	long	as	some	logical	flow	is	busy	reading	a	text	line,	no	other
logical	flow	can	make	progress.	This	is	fine	for	our	example,	but	it	makes
our	event-driven	server	vulnerable	to	a	malicious	client	that	sends	only	a
partial	text	line	and	then	halts.	Modifying	an	event-driven	server	to	handle
partial	text	lines	is	a	nontrivial	task,	but	it	is	handled	cleanly	and
automatically	by	a	process-based	design.	Another	significant
disadvantage	of	event-based	designs	is	that	they	cannot	fully	utilize
multi-core	processors.</p>
<p>12.3	
Concurrent	Programming	with
Threads
To	this	point,	we	have	looked	at	two	approaches	for	creating	concurrent
logical	flows.	With	the	first	approach,	we	use	a	separate	process	for	each
flow.	The	kernel	schedules	each	process	automatically,	and	each	process
has	its	own	private	address	space,	which	makes	it	difficult	for	flows	to
share	data.	With	the	second	approach,	we	create	our	own	logical	flows
and	use	I/O	multiplexing	to	explicitly	schedule	the	flows.	Because	there	is
only	one	process,	flows	share	the	entire	address	space.	
This	section
introduces	a	third	approach—based	on	threads—that	is	a	hybrid	of	these
two.
A	
thread
is	a	logical	flow	that	runs	in	the	context	of	a	process.	Thus	far	in
this	book,	our	programs	have	consisted	of	a	single	thread	per	process.
But	modern	systems	also	allow	us	to	write	programs	that	have	multiple
threads	running	concurrently	in	a	single	process.	The	threads	are
scheduled	automatically	by	the	kernel.	Each	thread	has	its	own	
thread
context
,	including	a	unique	integer	
thread	ID	(TID)
,	stack,	stack	pointer,
program	counter,	general-purpose	registers,	and	condition	codes.	All
threads	running	in	a	process	share	the	entire	virtual	address	space	of
that	process.
Logical	flows	based	on	threads	combine	qualities	of	flows	based	on
processes	and	I/O	multiplexing.	Like	processes,	threads	are	scheduled
automatically	by	the	kernel	and	are	known	to	the	kernel	by	an	integer	ID.</p>
<p>Like	flows	based	on	I/O	multiplexing,	multiple	threads	run	in	the	context
of	a	single	process,	and	thus	they	share	the	entire	contents	of	the
process	virtual	address	space,	including	its	code,	data,	heap,	shared
libraries,	and	open	files.
12.3.1	
Thread	Execution	Model
The	execution	model	for	multiple	threads	is	similar	in	some	ways	to	the
execution	model	for	multiple	processes.	Consider	the	example	in	
Figure
12.12
.	Each	process	begins	life	as	a	single	thread	called	the	
main
thread
.	At	some	point,	the	main	thread	creates	a	
peer	thread
,	and	from
this	point	in	time	the	two	threads	run	concurrently.	Eventually,	control
passes	to	the	peer	thread	via	a	context	switch,	either	because	the	main
thread	executes	a	slow	system	call	such	as	
or	
or	because	it
is	interrupted	by	the	system's	interval	timer.	The	peer	thread	executes	for
a	while	before	control	passes	back	to	the	main	thread,	and	so	on.
Thread	execution	differs	from	processes	in	some	important	ways.
Because	a	thread	context	is	much	smaller	than	a	process	context,	a
thread	context	switch	is	faster	than	a	process	context	switch.	Another
difference	is	that	threads,	unlike	processes,	are	not	organized	in	a	rigid
parent-child	hierarchy.	The	threads	associated</p>
<p>Figure	
12.12	
Concurrent	thread	execution.
with	a	process	form	a	
pool
of	peers,	independent	of	which	threads	were
created	by	which	other	threads.	The	main	thread	is	distinguished	from
other	threads	only	in	the	sense	that	it	is	always	the	first	thread	to	run	in
the	process.	The	main	impact	of	this	notion	of	a	pool	of	peers	is	that	a
thread	can	kill	any	of	its	peers	or	wait	for	any	of	its	peers	to	terminate.
Further,	each	peer	can	read	and	write	the	same	shared	data.
12.3.2	
Posix	Threads
Posix	threads	(Pthreads)	is	a	standard	interface	for	manipulating	threads
from	C	programs.	It	was	adopted	in	1995	and	is	available	on	all	Linux
systems.	Pthreads	defines	about	60	functions	that	allow	programs	to
create,	kill,	and	reap	threads,	to	share	data	safely	with	peer	threads,	and
to	notify	peers	about	changes	in	the	system	state.</p>
<h2>Figure	
12.13
shows	a	simple	Pthreads	program.	The	main	thread
creates	a	peer	thread	and	then	waits	for	it	to	terminate.	The	peer	thread
prints	
and	terminates.	When	the	main	thread	detects	that
the	peer	thread	has	terminated,	it	terminates	the	process	by	calling	
.
This	is	the	first	threaded	program	we	have	seen,	so	let	us	dissect	it
carefully.	The	code	and	local	data	for	a	thread	are	encapsulated	in	a
thread	routine
.	As	shown	by	the	prototype	in	line	2,	each	thread	routine
takes	as	input	a	single	generic	pointer	and	returns	a	generic	pointer.	If
you	want	to	pass	multiple	arguments	to	a	thread	routine,	then	you	should
put	the	arguments	into	a	structure	and	pass	a	pointer	to	the	structure.
Similarly,	if	you</h2>
<p>code/conc/hello.c</p>
<hr />
<p>code/conc/hello.c
Figure	
12.13	
:	The	Pthreads	&quot;Hello,	world!&quot;	program.
want	the	thread	routine	to	return	multiple	arguments,	you	can	return	a
pointer	to	a	structure.
Line	4	marks	the	beginning	of	the	code	for	the	main	thread.	The	main
thread	declares	a	single	local	variable	
,	which	will	be	used	to	store
the	thread	ID	of	the	peer	thread	(line	6).	The	main	thread	creates	a	new
peer	thread	by	calling	the	
function	(line	7).	When	the	call
to	
returns,	the	main	thread	and	the	newly	created	peer
thread	are	running	concurrently,	and	
contains	the	ID	of	the	new
thread.	The	main	thread	waits	for	the	peer	thread	to	terminate	with	the
call	to	
in	line	8.	Finally,	the	main	thread	calls	
(line	9),
which	terminates	all	threads	(in	this	case,	just	the	main	thread)	currently
running	in	the	process.
Lines	12−16	define	the	thread	routine	for	the	peer	thread.	It	simply	prints
a	string	and	then	terminates	the	peer	thread	by	executing	the	
statement	in	line	15.
12.3.3	
Creating	Threads</p>
<p>Threads	create	other	threads	by	calling	the	
function.
The	
function	creates	a	new	thread	and	runs	the	
thread
routine</p>
<pre><code>in	the	context	of	the	new	thread	and	with	an	input	argument	of
</code></pre>
<p>.	The	
argument	can	be	used	to	change	the	default	attributes	of
the	newly	created	thread.	Changing	these	attributes	is	beyond	our	scope,
and	in	our	examples,	we	will	always	call	
with	a	NULL
argument.
When	
returns,	argument	
contains	the	ID	of	the	newly
created	thread.	The	new	thread	can	determine	its	own	thread	ID	by
calling	the	
function.</p>
<p>12.3.4	
Terminating	Threads
A	thread	terminates	in	one	of	the	following	ways:
The	thread	terminates	
implicitly
when	its	top-level	thread	routine
returns.
The	thread	terminates	
explicitly
by	calling	the	
function.	If
the	main	thread	calls	
,	it	waits	for	all	other	peer	threads
to	terminate	and	then	terminates	the	main	thread	and	the	entire
process	with	a	return	value	of	
.
Some	peer	thread	calls	the	Linux	
function,	which	terminates	the
process	and	all	threads	associated	with	the	process.
Another	peer	thread	terminates	the	current	thread	by	calling	the
function	with	the	ID	of	the	current	thread.</p>
<p>12.3.5	
Reaping	Terminated	Threads
Threads	wait	for	other	threads	to	terminate	by	calling	the	
function.
The	
function	blocks	until	thread	
terminates,	assigns	the
generic	(
)	pointer	returned	by	the	thread	routine	to	the	location
pointed	to	by	
,	and	then	
reaps
any	memory	resources	held
by	the	terminated	thread.
Notice	that,	unlike	the	Linux	
function,	the	
function	can
only	wait	for	a	specific	thread	to	terminate.	There	is	no	way	to	instruct
to	wait	for	an	
arbitrary
thread	to	terminate.	This	can
complicate	our	code	by	forcing	us	to	use	other,	less	intuitive	mechanisms
to	detect	process	termination.	Indeed,	Stevens	argues	convincingly	that
this	is	a	bug	in	the	specification	[
110
].
12.3.6	
Detaching	Threads</p>
<p>At	any	point	in	time,	a	thread	is	
joinable
or	
detached
.	A	joinable	thread
can	be	reaped	and	killed	by	other	threads.	Its	memory	resources	(such
as	the	stack)	are	not	freed	until	it	is	reaped	by	another	thread.	In	contrast,
a	detached	thread	cannot	
be	reaped	or	killed	by	other	threads.	Its
memory	resources	are	freed	automatically	by	the	system	when	it
terminates.
By	default,	threads	are	created	joinable.	In	order	to	avoid	memory	leaks,
each	joinable	thread	should	be	either	explicitly	reaped	by	another	thread
or	detached	by	a	call	to	the	
function.
The	
function	detaches	the	joinable	thread	
.	Threads
can	detach	themselves	by	calling	
with	an	argument	of
.
Although	some	of	our	examples	will	use	joinable	threads,	there	are	good
reasons	to	use	detached	threads	in	real	programs.	For	example,	a	high-
performance	Web	server	might	create	a	new	peer	thread	each	time	it
receives	a	connection	request	from	a	Web	browser.	Since	each
connection	is	handled	independently	by	a	separate	thread,	it	is
unnecessary—and	indeed	undesirable—for	the	server	to	explicitly	wait
for	each	peer	thread	to	terminate.	In	this	case,	each	peer	thread	should</p>
<p>detach	itself	before	it	begins	processing	the	request	so	that	its	memory
resources	can	be	reclaimed	after	it	terminates.
12.3.7	
Initializing	Threads
The	
function	allows	you	to	initialize	the	state	associated
with	a	thread	routine.
The	
variable	is	a	global	or	static	variable	that	is	always
initialized	to	PTHREAD_ONCE_INIT.	The	first	time	you	call	
with	an	argument	of	
,	it	invokes	
,	which	is	a
function	with	no	input	arguments	that	returns	nothing.	Subsequent	calls
to	
with	the	same	
variable	do	nothing.	The
function	is	useful	whenever	you	need	to	dynamically
initialize	global	variables	that	are	shared	by	multiple	threads.	We	will	look
at	an	example	in	
Section	
12.5.5
.</p>
<h2>12.3.8	
A	Concurrent	Server	Based
on	Threads
Figure	
12.14
shows	the	code	for	a	concurrent	echo	server	based	on
threads.	The	overall	structure	is	similar	to	the	process-based	design.	The
main	thread	repeatedly	waits	for	a	connection	request	and	then	creates	a
peer	thread	to	handle	the	request.	While	the	code	looks	simple,	there	are
a	couple	of	general	and	somewhat	subtle	issues	we	need	to	look	at	more
closely.	The	first	issue	is	how	to	pass</h2>
<p>code/conc/echoservert.c</p>
<hr />
<p>code/conc/echoservert.c
Figure	
12.14	
Concurrent	echo	server	based	on	threads.</p>
<p>the	connected	descriptor	to	the	peer	thread	when	we	call	
.
The	obvious	approach	is	to	pass	a	pointer	to	the	descriptor,	as	in	the
following:
Then	we	have	the	peer	thread	dereference	the	pointer	and	assign	it	to	a
local	variable,	as	follows:
⋮
This	would	be	wrong,	however,	because	it	introduces	a	
race
between	the
assignment	statement	in	the	peer	thread	and	the	
statement	in	the
main	thread.	If	the	assignment	statement	completes	before	the	next
,	then	the	local	
variable	in	the	peer	thread	gets	the	correct
descriptor	value.	However,	if	the	assignment	completes	
after
the	
,
then	the	local	
variable	in	the	peer	thread	gets	the	descriptor
number	of	the	
next
connection.	The	unhappy	result	is	that	two	threads
are	now	performing	input	and	output	on	the	same	descriptor.	In	order	to
avoid	the	potentially	deadly	race,	we	must	assign	each	connected</p>
<p>descriptor	returned	by	
to	its	own	dynamically	allocated	memory
block,	as	shown	in	lines	21−22.	We	will	return	to	the	issue	of	races	in
Section	
12.7.4
.
Another	issue	is	avoiding	memory	leaks	in	the	thread	routine.	Since	we
are	not	explicitly	reaping	threads,	we	must	detach	each	thread	so	that	its
memory	resources	will	be	reclaimed	when	it	terminates	(line	31).	Further,
we	must	be	careful	to	free	the	memory	block	that	was	allocated	by	the
main	thread	(line	32).
Practice	Problem	
12.5	
(solution	page	
1036
)
In	the	process-based	server	in	
Figure	
12.5
,	we	were	careful	to
close	the	connected	descriptor	in	two	places:	the	parent	process
and	the	child	process.	However,	in	the	threads-based	server	in
Figure	
12.14
,	we	only	closed	the	connected	descriptor	in	one
place:	the	peer	thread.	Why?</p>
<h2>12.4	
Shared	Variables	in	Threaded
Programs
From	a	programmer's	perspective,	one	of	the	attractive	aspects	of
threads	is	the	ease	with	which	multiple	threads	can	share	the	same
program	variables.	However,	this	sharing	can	be	tricky.	In	order	to	write
correctly	threaded	programs,	we	must	have	a	clear	understanding	of
what	we	mean	by	sharing	and	how	it	works.
There	are	some	basic	questions	to	work	through	in	order	to	understand
whether	a	variable	in	a	C	program	is	shared	or	not:	(1)	What	is	the
underlying	memory	model	for	threads?	(2)	Given	this	model,	how	are
instances	of	the	variable	mapped	to	memory?	(3)	Finally,	how	many
threads	reference	each	of	these</h2>
<p>code/conc/sharing.c</p>
<hr />
<p>code/conc/sharing.c
Figure	
12.15	
Example	program	that	illustrates	different	aspects	of
sharing.</p>
<p>instances?	The	variable	is	
shared
if	and	only	if	multiple	threads	reference
some	instance	of	the	variable.
To	keep	our	discussion	of	sharing	concrete,	we	will	use	the	program	in
Figure	
12.15
as	a	running	example.	Although	somewhat	contrived,	it
is	nonetheless	useful	to	study	because	it	illustrates	a	number	of	subtle
points	about	sharing.	The	example	program	consists	of	a	main	thread
that	creates	two	peer	threads.	The	main	thread	passes	a	unique	ID	to
each	peer	thread,	which	uses	the	ID	to	print	a	personalized	message
along	with	a	count	of	the	total	number	of	times	that	the	thread	routine	has
been	invoked.
12.4.1	
Threads	Memory	Model
A	pool	of	concurrent	threads	runs	in	the	context	of	a	process.	Each
thread	has	its	own	separate	
thread	context
,	which	includes	a	thread	ID,
stack,	stack	pointer,	
program	counter,	condition	codes,	and	general-
purpose	register	values.	Each	thread	shares	the	rest	of	the	process
context	with	the	other	threads.	This	includes	the	entire	user	virtual
address	space,	which	consists	of	read-only	text	(code),	read/write	data,
the	heap,	and	any	shared	library	code	and	data	areas.	The	threads	also
share	the	same	set	of	open	files.
In	an	operational	sense,	it	is	impossible	for	one	thread	to	read	or	write
the	register	values	of	another	thread.	On	the	other	hand,	any	thread	can
access	any	location	in	the	shared	virtual	memory.	If	some	thread	modifies
a	memory	location,	then	every	other	thread	will	eventually	see	the</p>
<p>change	if	it	reads	that	location.	Thus,	registers	are	never	shared,
whereas	virtual	memory	is	always	shared.
The	memory	model	for	the	separate	thread	stacks	is	not	as	clean.	These
stacks	are	contained	in	the	stack	area	of	the	virtual	address	space	and
are	
usually
accessed	independently	by	their	respective	threads.	We	say
usually
rather	than	
always
,	because	different	thread	stacks	are	not
protected	from	other	threads.	So	if	a	thread	somehow	manages	to
acquire	a	pointer	to	another	thread's	stack,	then	it	can	read	and	write	any
part	of	that	stack.	Our	example	program	shows	this	in	line	26,	where	the
peer	threads	reference	the	contents	of	the	main	thread's	stack	indirectly
through	the	global	
variable.
12.4.2	
Mapping	Variables	to
Memory
Variables	in	threaded	C	programs	are	mapped	to	virtual	memory
according	to	their	storage	classes:
Global	variables.	
A	
global	variable
is	any	variable	declared	outside	of
a	function.	At	run	time,	the	read/write	area	of	virtual	memory	contains
exactly	one	instance	of	each	global	variable	that	can	be	referenced	by
any	thread.	For	example,	the	global	
variable	declared	in	line	5
has	one	run-time	instance	in	the	read/write	area	of	virtual	memory.
When	there	is	only	one	instance	of	a	variable,	we	will	denote	the
instance	by	simply	using	the	variable	name—in	this	case,	
.</p>
<p>Local	automatic	variables.	
A	
local	automatic	variable
is	one	that	is
declared	inside	a	function	without	the	
attribute.	At	run	time,
each	thread's	stack	contains	its	own	instances	of	any	local	automatic
variables.	This	is	true	even	if	multiple	threads	execute	the	same
thread	routine.	For	example,	there	is	one	instance	of	the	local	variable
,	and	it	resides	on	the	stack	of	the	main	thread.	We	will	denote
this	instance	as	
As	another	example,	there	are	two	instances
of	the	local	variable	myid,	one	instance	on	the	stack	of	peer	thread	0
and	the	other	on	the	stack	of	peer	thread	1.	We	will	denote	these
instances	as	
and	
,	respectively.
Local	static	variables.	
A	
local	
variable
is	one	that	is	declared
inside	a	function	with	the	
attribute.	As	with	global	variables,
the	read/write	area	of	virtual	memory	contains	exactly	one	instance	of
each	local	static	
variable	declared	in	a	program.	For	example,	even
though	each	peer	thread	in	our	example	program	declares	
in	line
25,	at	run	time	there	is	only	one	instance	of	
residing	in	the
read/write	area	of	virtual	memory.	Each	peer	thread	reads	and	writes
this	instance.
12.4.3	
Shared	Variables
We	say	that	a	variable	
v
is	
shared
if	and	only	if	one	of	its	instances	is
referenced	by	more	than	one	thread.	For	example,	variable	
in	our
example	program	is	shared	because	it	has	only	one	run-time	instance
and	this	instance	is	referenced	by	both	peer	threads.	On	the	other	hand,
is	not	shared,	because	each	of	its	two	instances	is	referenced	by</p>
<p>exactly	one	thread.	However,	it	is	important	to	realize	that	local	automatic
variables	such	as	
can	also	be	shared.
Practice	Problem	
12.6	
(solution	page	
1036
)
Using	the	analysis	from	
Section	
12.4
,	fill	each	entry	in	the
following	table	with	&quot;Yes&quot;	or	&quot;No&quot;	for	the	example	program	in
Figure	
12.15
.	In	the	first	column,	the	notation	
v.t
denotes	an
instance	of	variable	
v
residing	on	the	local	stack	for	thread	
t
,
where	
t
is	either	
(main	thread),	
(peer	thread	0),	or	
(peer
thread	1).
Variable	instance
Referenced	by
main	thread?
peer	thread	0?
peer	thread	1?</p>
<hr />
<hr />
<hr />
<hr />
<hr />
<hr />
<hr />
<hr />
<hr />
<hr />
<hr />
<hr />
<hr />
<hr />
<hr />
<hr />
<hr />
<hr />
<p>Given	the	analysis	in	part	A,	which	of	the	variables	
,	and	
are	shared?</p>
<h2>12.5	
Synchronizing	Threads	with
Semaphores
Shared	variables	can	be	convenient,	but	they	introduce	the	possibility	of
nasty	
synchronization	errors
.	Consider	the	
program	in	
Figure
12.16
,	which	creates	two	threads,	each	of	which	increments	a	global
shared	counter	variable	called	
.
Since	each	thread	increments	the	counter	niters	times,	we	expect	its	final
value	to	be	
.	This	seems	quite	simple	and	straightforward.
However,	when	we	run	
on	our	Linux	system,	we	not	only	get
wrong	answers,	we	get	different	answers	each	time!</h2>
<p>code/conc/badcnt.c</p>
<hr />
<p>code/conc/badcnt.c
Figure	
12.16	
:	An	improperly	synchronized	counter
program.
So	what	went	wrong?	To	understand	the	problem	clearly,	we	need	to
study	the	assembly	code	for	the	counter	loop	(lines	40−41),	as	shown	in
Figure	
12.17
.	We	will	find	it	helpful	to	partition	the	loop	code	for	thread
i
into	five	parts:
H
:	The	block	of	instructions	at	the	head	of	the	loop
L
:	The	instruction	that	loads	the	shared	variable	
into	the
accumulator	register	%rdx
,	where	%rdx
denotes	the	value	of	register
%rdx	in	thread	
i
i
i
i
i</p>
<p>U
:	The	instruction	that	updates	(increments)	%rdx
S
:	The	instruction	that	stores	the	updated	value	of	
back	to	the
shared	variable	
T
:	The	block	of	instructions	at	the	tail	of	the	loop
Notice	that	the	head	and	tail	manipulate	only	local	stack	variables,	while
L
,	
U
,	and	
S
manipulate	the	contents	of	the	shared	counter	variable.
When	the	two	peer	threads	in	
run	concurrently	on	a
uniprocessor,	the	machine	instructions	are	completed	one	after	the	other
in	some	order.	Thus,	each	concurrent	execution	defines	some	total
ordering	(or	interleaving)	of	the	instructions	in	the	two	threads.
Unfortunately,	some	of	these	orderings	will	produce	correct	results,	but
others	will	not.
Figure	
12.17	
Assembly	code	for	the	counter	loop	(lines	40−41)	in
(a)	Correct	ordering
i
i
i
i
i
i
i</p>
<p>Step
Thread
Instr.
1
1
H
—
—
0
2
1
L
0
—
0
3
1
U
1
—
0
4
1
S
1
—
1
5
2
H
—
—
1
6
2
L
—
1
1
7
2
U
—
2
1
8
2
S
—
2
2
9
2
T
—
2
2
10
1
T
1
—
2
(b)	Incorrect	ordering
Step
Thread
Instr.
1
1
H
—
—
0
2
1
L
0
—
0
3
1
U
1
—
0
4
2
H
—
—
0
5
2
L
—
0
0
1
1
1
1
2
2
2
2
2
1
1
1
1
2
2</p>
<p>6
1
S
1
—
1
7
1
T
1
—
1
8
2
U
—
1
1
9
2
S
—
1
1
10
2
T
—
1
1
Figure	
12.18	
Instruction	orderings	for	the	first	loop	iteration	in
Here	is	the	crucial	point:	
In	general,	there	is	no	way	for	you	to	predict
whether	the	operating	system	will	choose	a	correct	ordering	for	your
threads.
For	example,	
Figure	
12.18(a)
shows	the	step-by-step
operation	of	a	correct	instruction	ordering.	After	each	thread	has	updated
the	shared	variable	cnt,	its	value	in	memory	is	2,	which	is	the	expected
result.
Ontheother	hand,	the	ordering	in	
Figure	
12.18(b)
produces	an
incorrect	value	for	
.	The	problem	occurs	because	thread	2	loads	
in	step	5,	after	thread	1	loads	
in	step	2	but	before	thread	1	stores	its
updated	value	in	step	6.	Thus,	each	thread	ends	up	storing	an	updated
counter	value	of	1.	We	can	clarify	these	notions	of	correct	and	incorrect
instruction	orderings	with	the	help	of	a	device	known	as	a	
progress
graph
,	which	we	introduce	in	the	next	section.
Practice	Problem	
12.7	
(solution
1
1
2
2
2</p>
<p>page	
1037
)
Complete	the	table	for	the	following	instruction	ordering	of
:
Step
Thread
Instr.
1
1
H
—
—
0
2
1
L</p>
<hr />
<hr />
<hr />
<p>3
2
H</p>
<hr />
<hr />
<hr />
<p>4
2
L</p>
<hr />
<hr />
<hr />
<p>5
2
U</p>
<hr />
<hr />
<hr />
<p>6
2
S</p>
<hr />
<hr />
<hr />
<p>7
1
U</p>
<hr />
<hr />
<hr />
<p>Step
Thread
Instr.
8
1
S</p>
<hr />
<hr />
<hr />
<p>9
1
T</p>
<hr />
<hr />
<hr />
<p>10
2
T</p>
<hr />
<hr />
<hr />
<p>Does	this	ordering	result	in	a	correct	value	for	
?
12.5.1	
Progress	Graphs
1
1
2
2
2
2
1
1
1
2</p>
<p>A	
progress	graph
models	the	execution	of	
n
concurrent	threads	as	a
trajectory	through	an	
n
-dimensional	Cartesian	space.	Each	axis	
k
corresponds	to	the	progress	of	thread	
k
.	Each	point	(
I
,	
I
,	.	.	.	,	
I
)
represents	the	state	where	thread	
k
(
k
=	1,	.	.	.	,	
n
)	has	completed
instruction	
I
.	The	origin	of	the	graph	corresponds	to	the	
initial	state
where	none	of	the	threads	has	yet	completed	an	instruction.
Figure	
12.19
shows	the	two-dimensional	progress	graph	for	the	first
loop	iteration	of	the	
program.	The	horizontal	axis	corresponds
to	thread	1,	the	vertical	axis	to	thread	2.	Point	(
L
,	
S
)	corresponds	to	the
state	where	thread	1	has	completed	
L
and	thread	2	has	completed	
S
.
A	progress	graph	models	instruction	execution	as	a	
transition
from	one
state	to	another.	A	transition	is	represented	as	a	directed	edge	from	one
point	to	an	adjacent	point.	Legal	transitions	move	to	the	right	(an
instruction	in	thread	1	completes)	or	up	(an	instruction	in	thread	2
completes).	Two	instructions	cannot	complete	at	the	same	time—
diagonal	transitions	are	not	allowed.	Programs	never	run	backward	so
transitions	that	move	down	or	to	the	left	are	not	legal	either.
1
2
n
k
1
2
1
2</p>
<p>Figure	
12.19	
Progress	graph	for	the	first	loop	iteration	of	
.
Figure	
12.20	
An	example	trajectory.</p>
<p>The	execution	history	of	a	program	is	modeled	as	a	
trajectory
through	the
state	space.	
Figure	
12.20
shows	the	trajectory	that	corresponds	to	the
following	instruction	ordering:
For	thread	
i
,	the	instructions	(
L
,	U
,	S
)	that	manipulate	the	contents	of	the
shared	variable	
constitute	a	
critical	section
(with	respect	to	shared
variable	
)	that	should	not	be	interleaved	with	the	critical	section	of	the
other	thread.	In	other	words,	we	want	to	ensure	that	each	thread	has
mutually	exclusive	access
to	the	shared	variable	while	it	is	executing	the
instructions	in	its	critical	section.	The	phenomenon	in	general	is	known	as
mutual	exclusion
.
On	the	progress	graph,	the	intersection	of	the	two	critical	sections	defines
a	region	of	the	state	space	known	as	an	
unsafe	region
.	
Figure	
12.21
shows	the	unsafe	region	for	the	variable	
.	Notice	that	the	unsafe
region	abuts,	but	does	not	include,	the	states	along	its	perimeter.	For
example,	states	(
H
,	H
)	and	(
S
,	U
)	abut	the	unsafe	region,	but	they	are
not	part	of	it.	A	trajectory	that	skirts	the	unsafe	region	is	known	as	a	
safe
trajectory
.	Conversely,	a	trajectory	that	touches	any	part	of	the	unsafe
region	is	an	
unsafe	trajectory
.	
Figure	
12.21
shows	examples	of	safe
and	unsafe	trajectories	through	the	state	space	of	our	example	
program.	The	upper	trajectory	skirts	the	unsafe	region	along	its	left	and
top	sides,	and	thus	is	safe.	The	lower	trajectory	crosses	the	unsafe
region,	and	thus	is	unsafe.
Any	safe	trajectory	will	correctly	update	the	shared	counter.	In	order	to
guarantee	correct	execution	of	our	example	threaded	program—and
H
1
,
 
L
1
,
 
U
1
,
 
H
2
,
 
L
2
,
 
S
1
,
 
T
1
,
 
U
2
,
 
S
2
,
 
T
2
i
i
i
1
2
1
2</p>
<p>indeed	any	concurrent	program	that	shares	global	data	structures—we
must	somehow	
synchronize
the	threads	so	that	they	always	have	a	safe
trajectory.	A	classic	approach	is	based	on	the	idea	of	a	semaphore,	which
we	introduce	next.
Figure	
12.21	
Safe	and	unsafe	trajectories.
The	intersection	of	the	critical	regions	forms	an	unsafe	region.
Trajectories	that	skirt	the	unsafe	region	correctly	update	the	counter
variable.
Practice	Problem	
12.8	
(solution	page	
1038
)
Using	the	progress	graph	in	
Figure	
12.21
,	classify	the	following
trajectories	as	either	
safe
or	
unsafe
.
A
.	
H
,	
L
,	
U
,	
S
,	
H
,	
L
,	
U
,	
S
,	
T
,	
T
1
1
1
1
2
2
2
2
2
1</p>
<p>B
.	
H
,	
L
,	
H
,	
L
,	
U
,	
S
,	
T
,	
U
,	
S
,	
T
C
.	
H
,	
H
,	
L
,	
U
,	
S
,	
L
,	
U
,	
S
,	
T
,	
T
12.5.2	
Semaphores
Edsger	Dijkstra,	a	pioneer	of	concurrent	programming,	proposed	a
classic	solution	to	the	problem	of	synchronizing	different	execution
threads	based	on	a	special	type	of	variable	called	a	
semaphore
.	A
semaphore,	
s
,	is	a	global	variable	with	a	nonnegative	integer	value	that
can	only	be	manipulated	by	two	special	operations,	called	
P
and	
V
:
P	(s:
If	
s
is	nonzero,	then	
P
decrements	
s
and	returns	immediately.	If
s
is	zero,	then	suspend	the	thread	until	
s
becomes	nonzero	and	the
thread	is	restarted	by	a	
V
operation.	After	restarting,	the	
P
operation
decrements	
s
and	returns	control	to	the	caller.
V	(s):
The	
V
operation	increments	
s
by	1.	If	there	are	any	threads
blocked	at	a	
P
operation	waiting	for	
s
to	become	nonzero,	then	the	
V
operation	restarts	exactly	one	of	these	threads,	which	then	completes
its	
P
operation	by	decrementing	
s
.
Aside	
Origin	of	the	names	
P
and	
V
Edsger	Dijkstra	(1930−2002)	was	originally	from	the	Netherlands.
The	names	
P
and	
V
come	from	the	Dutch	words	
proberen
(to	test)
and	
verhogen
(to	increment).
The	test	and	decrement	operations	in	
P
occur	indivisibly,	in	the	sense
that	once	the	semaphore	
s
becomes	nonzero,	the	decrement	of	
s
occurs
2
2
1
1
1
1
1
2
2
2
1
2
2
2
2
1
1
1
1
2</p>
<p>without	interruption.	The	increment	operation	in	
V
also	occurs	indivisibly,
in	that	it	loads,	increments,	and	stores	the	semaphore	without
interruption.	Notice	that	the	definition	of	
V
does	
not
define	the	order	in
which	waiting	threads	are	restarted.	The	only	requirement	is	that	the	
V
must	restart	exactly	one	waiting	thread.	
Thus,	when	several	threads	are
waiting	at	a	semaphore,	you	cannot	predict	which	one	will	be	restarted	as
a	result	of	the	V.
The	definitions	of	
P
and	
V
ensure	that	a	running	program	can	never	enter
a	state	where	a	properly	initialized	semaphore	has	a	negative	value.	This
property,	known	as	the	
semaphore	invariant
,	provides	a	powerful	tool	for
controlling	the	trajectories	of	concurrent	programs,	as	we	shall	see	in	the
next	section.
The	Posix	standard	defines	a	variety	of	functions	for	manipulating
semaphores.
The	
function	initializes	semaphore	
to	
.	Each
semaphore	must	be	initialized	before	it	can	be	used.	For	our	purposes,
the	middle	argument	is	always	0.	Programs	perform	
P
and	
V
operations</p>
<p>by	calling	the	
and	
functions,	respectively.	For
conciseness,	we	prefer	to	use	the	following	equivalent	
P
and	
V
wrapper
functions	instead:
12.5.3	
Using	Semaphores	for
Mutual	Exclusion
Semaphores	provide	a	convenient	way	to	ensure	mutually	exclusive
access	to	shared	variables.	The	basic	idea	is	to	associate	a	semaphore
s
,	initially	1,	with</p>
<p>Figure	
12.22	
Using	semaphores	for	mutual	exclusion.
The	infeasible	states	where	
s
&lt;	0	define	a	
forbidden	region
that
surrounds	the	unsafe	region	and	prevents	any	feasible	trajectory	from
touching	the	unsafe	region.
each	shared	variable	(or	related	set	of	shared	variables)	and	then
surround	the	corresponding	critical	section	with	
P	(s)
and	
V	(s)
operations.
A	semaphore	that	is	used	in	this	way	to	protect	shared	variables	is	called
a	
binary	semaphore
because	its	value	is	always	0	or	1.	Binary
semaphores	whose	purpose	is	to	provide	mutual	exclusion	are	often
called	
mutexes
.	Performing	a	
P
operation	on	a	mutex	is	called	
locking
the</p>
<p>mutex.	Similarly,	performing	the	
V
operation	is	called	
unlocking
the
mutex.	A	thread	that	has	locked	but	not	yet	unlocked	a	mutex	is	said	to
be	
holding
the	mutex.	A	semaphore	that	is	used	as	a	counter	for	a	set	of
available	resources	is	called	a	
counting	semaphore
.
The	progress	graph	in	
Figure	
12.22
shows	how	we	would	use	binary
semaphores	to	properly	synchronize	our	example	counter	program.
Each	state	is	labeled	with	the	value	of	semaphore	
s
in	that	state.	The
crucial	idea	is	that	this	combination	of	
P
and	
V
operations	creates	a
collection	of	states,	called	a	
forbidden	region
,	where	
s
&lt;	0.	Because	of
the	semaphore	invariant,	no	feasible	trajectory	can	include	one	of	the
states	in	the	forbidden	region.	And	since	the	forbidden	region	completely
encloses	the	unsafe	region,	no	feasible	trajectory	can	touch	any	part	of
the	unsafe	region.	Thus,	every	feasible	trajectory	is	safe,	and	regardless
of	the	ordering	of	the	instructions	at	run	time,	the	program	correctly
increments	the	counter.
Aside	
Limitations	of	progress	graphs
Progress	graphs	give	us	a	nice	way	to	visualize	concurrent
program	execution	on	uniprocessors	and	to	understand	why	we
need	synchronization.	However,	they	do	have	limitations,
particularly	with	respect	to	concurrent	execution	on
multiprocessors,	where	a	set	of	CPU/cache	pairs	share	the	same
main	memory.	Multiprocessors	behave	in	ways	that	cannot	be
explained	by	progress	graphs.	In	particular,	a	multiprocessor
memory	system	can	be	in	a	state	that	does	not	correspond	to	any
trajectory	in	a	progress	graph.	Regardless,	the	message	remains</p>
<p>the	same:	always	synchronize	accesses	to	your	shared	variables,
regardless	if	you're	running	on	a	uniprocessor	or	a	multiprocessor.
In	an	operational	sense,	the	forbidden	region	created	by	the	
P
and	
V
operations	makes	it	impossible	for	multiple	threads	to	be	executing
instructions	in	the	enclosed	critical	region	at	any	point	in	time.	In	other
words,	the	semaphore	operations	ensure	mutually	exclusive	access	to
the	critical	region.
Putting	it	all	together,	to	properly	synchronize	the	example	counter
program	in	
Figure	
12.16
using	semaphores,	we	first	declare	a
semaphore	called	mutex:
and	then	we	initialize	it	to	unity	in	the	main	routine:
Finally,	we	protect	the	update	of	the	shared	
variable	in	the	thread
routine	by	surrounding	it	with	
P
and	
V
operations:</p>
<p>When	we	run	the	properly	synchronized	program,	it	now	produces	the
correct	answer	each	time.
12.5.4	
Using	Semaphores	to
Schedule	Shared	Resources
Another	important	use	of	semaphores,	besides	providing	mutual
exclusion,	is	to	schedule	accesses	to	shared	resources.	In	this	scenario,
a	thread	uses	a	semaphore
Figure	
12.23	
Producer-consumer	problem.</p>
<p>The	producer	generates	items	and	inserts	them	into	a	bounded	buffer.
The	consumer	removes	items	from	the	buffer	and	then	consumes	them.
operation	to	notify	another	thread	that	some	condition	in	the	program
state	has	become	true.	Two	classical	and	useful	examples	are	the
producer-consumer
and	
readers-writers
problems.
Producer-Consumer	Problem
The	
producer-consumer
problem	is	shown	in	
Figure	
12.23
.	A	producer
and	consumer	thread	share	a	
bounded	buffer
with	
n	slots
.	The	producer
thread	repeatedly	produces	new	
items
and	inserts	them	in	the	buffer.	The
consumer	thread	repeatedly	removes	items	from	the	buffer	and	then
consumes	(uses)	them.	Variants	with	multiple	producers	and	consumers
are	also	possible.
Since	inserting	and	removing	items	involves	updating	shared	variables,
we	must	guarantee	mutually	exclusive	access	to	the	buffer.	But
guaranteeing	mutual	exclusion	is	not	sufficient.	We	also	need	to	schedule
accesses	to	the	buffer.	If	the	buffer	is	full	(there	are	no	empty	slots),	then
the	producer	must	wait	until	a	slot	becomes	available.	Similarly,	if	the
buffer	is	empty	(there	are	no	available	items),	then	the	consumer	must
wait	until	an	item	becomes	available.
Producer-consumer	interactions	occur	frequently	in	real	systems.	For
example,	in	a	multimedia	system,	the	producer	might	encode	video
frames	while	the	consumer	decodes	and	renders	them	on	the	screen.
The	purpose	of	the	buffer	is	to	reduce	jitter	in	the	video	stream	caused	by
data-dependent	differences	in	the	encoding	and	decoding	times	for</p>
<h2>individual	frames.	The	buffer	provides	a	reservoir	of	slots	to	the	producer
and	a	reservoir	of	encoded	frames	to	the	consumer.	Another	common
example	is	the	design	of	graphical	user	interfaces.	The	producer	detects
mouse	and	keyboard	events	and	inserts	them	in	the	buffer.	The
consumer	removes	the	events	from	the	buffer	in	some	priority-based
manner	and	paints	the	screen.
In	this	section,	we	will	develop	a	simple	package,	called	S
BUF
,	for	building
producer-consumer	programs.	In	the	next	section,	we	look	at	how	to	use
it	to	build	an	interesting	concurrent	server	based	on	prethreading.	S
BUF
manipulates	bounded	buffers	of	type	
(
Figure	
12.24
).	Items	are
stored	in	a	dynamically	allocated	integer	array	(
)	with	n	items.	The
and	
indices	keep	track	of	the	first	and	last	items	in	the	array.
Three	semaphores	synchronize	access	to	the	buffer.	The	
semaphore	provides	mutually	exclusive	buffer	access.	Semaphores
and	
are	counting	semaphores	that	count	the	number	of
empty	slots	and	available	items,	respectively.</h2>
<p>code/conc/sbuf.h</p>
<hr />
<p>code/conc/sbuf.h
Figure	
12.24	
:	Bounded	buffer	used	by	the	
package.
Figure	
12.25
shows	the	implementation	of	the	S
BUF</p>
<p>package.	The
function	allocates	heap	memory	for	the	buffer,	sets	
and
to	indicate	an	empty	buffer,	and	assigns	initial	values	to	the	three
semaphores.	This	function	is	called	once,	before	calls	to	any	of	the	other
three	functions.	The	
function	frees	the	buffer	storage	when
the	application	is	through	using	it.	The	
function	waits	for	an
available	slot,	locks	the	mutex,	adds	the	item,	unlocks	the	mutex,	and
then	announces	the	availability	of	a	new	item.	The	
function	is
symmetric.	After	waiting	for	an	available	buffer	item,	it	locks	the	mutex,
removes	the	item	from	the	front	of	the	buffer,	unlocks	the	mutex,	and	then
signals	the	availability	of	a	new	slot.
Practice	Problem	
12.9	
(solution	page	
1038
)
Let	
p
denote	the	number	of	producers,	
c
the	number	of
consumers,	and	
n
the	buffer	size	in	units	of	items.	For	each	of	the</p>
<h2>following	scenarios,	indicate	whether	the	mutex	semaphore	in
and	
is	necessary	or	not.
A
.	
p
=	1,	
c
=	1,	
n
&gt;	1
B
.	
p
=	1,	
c
=	1,	
n
=	1
C
.	
p
&gt;	1,	
c
&gt;	1,	
n
=	1
Readers-Writers	Problem
The	
readers-writers	problem
is	a	generalization	of	the	mutual	exclusion
problem.	A	collection	of	concurrent	threads	is	accessing	a	shared	object
such	as	a	data	structure	in	main	memory	or	a	database	on	disk.	Some
threads	only	read	the	object,	while	others	modify	it.	Threads	that	modify
the	object	are	called	
writers
.	Threads	that	only	read	it	are	called	
readers
.
Writers	must	have	exclusive	access	to	the	object,	but	readers	may	share
the	object	with	an	unlimited	number	of	other	readers.	In	general,	there
are	an	unbounded	number	of	concurrent	readers	and	writers.</h2>
<p>code/conc/sbuf.c</p>
<hr />
<p>code/conc/sbuf.c
Figure	
12.25	
:	A	package	for	synchronizing	concurrent	access	to
bounded	buffers.
Readers-writers	interactions	occur	frequently	in	real	systems.	For
example,	in	an	online	airline	reservation	system,	an	unlimited	number	of
customers	are	al-lowed	to	concurrently	inspect	the	seat	assignments,	but
a	customer	who	is	booking	a	seat	must	have	exclusive	access	to	the</p>
<p>database.	As	another	example,	in	a	multithreaded	caching	Web	proxy,	an
unlimited	number	of	threads	can	fetch	existing	pages	from	the	shared
page	cache,	but	any	thread	that	writes	a	new	page	to	the	cache	must
have	exclusive	access.
The	readers-writers	problem	has	several	variations,	each	based	on	the
priorities	of	readers	and	writers.	The	
first	readers-writers	problem
,	which
favors	readers,	requires	that	no	reader	be	kept	waiting	unless	a	writer
has	already	been	granted	permission	to	use	the	object.	In	other	words,
no	reader	should	wait	simply	because	a	writer	is	waiting.	The	
second
readers-writers	problem
,	which	favors	writers,	requires	that	once	a	writer
is	ready	to	write,	it	performs	its	write	as	soon	as	possible.	Unlike	the	first
problem,	a	reader	that	arrives	after	a	writer	must	wait,	even	if	the	writer	is
also	waiting.
Figure	
12.26
shows	a	solution	to	the	first	readers-writers	problem.
Like	the	solutions	to	many	synchronization	problems,	it	is	subtle	and
deceptively	simple.	The	
semaphore	controls	access	to	the	critical
sections	that	access	the	shared	object.	The	mutex	semaphore	protects
access	to	the	shared	
variable,	which	counts	the	number	of
readers	currently	in	the	critical	section.	A	writer	locks	thew	mutex	each
time	it	enters	the	critical	section	and	unlocks	it	each	time	it	leaves.	This
guarantees	that	there	is	at	most	one	writer	in	the	critical	section	at	any
point	in	time.	On	the	other	hand,	only	the	first	reader	to	enter	the	critical
section	locks	
,	and	only	the	last	reader	to	leave	the	critical	section
unlocks	it.	The	
mutex	is	ignored	by	readers	who	enter	and	leave	while
other	readers	are	present.	This	means	that	as	long	as	a	single	reader
holds	the	
mutex,	an	unbounded	number	of	readers	can	enter	the
critical	section	unimpeded.</p>
<p>A	correct	solution	to	either	of	the	readers-writers	problems	can	result	in
starvation
,	where	a	thread	blocks	indefinitely	and	fails	to	make	progress.
For	example,	in	the	solution	in	
Figure	
12.26
,	a	writer	could	wait
indefinitely	while	a	stream	of	readers	arrived.
Practice	Problem	
12.10	
(solution	page	
1038
)
The	solution	to	the	first	readers-writers	problem	in	
Figure	
12.26
gives	priority	to	readers,	but	this	priority	is	weak	in	the	sense	that	a
writer	leaving	its	critical	section	might	restart	a	waiting	writer
instead	of	a	waiting	reader.	Describe	a	scenario	where	this	weak
priority	would	allow	a	collection	of	writers	to	starve	a	reader.
12.5.5	
Putting	It	Together:	A
Concurrent	Server	Based	on
Prethreading
We	have	seen	how	semaphores	can	be	used	to	access	shared	variables
and	to	schedule	accesses	to	shared	resources.	To	help	you	understand
these	ideas	more	clearly,	let	us	apply	them	to	a	concurrent	server	based
on	a	technique	called	
prethreading
.</p>
<p>Figure	
12.26	
Solution	to	the	first	readers-writers	problem.</p>
<p>Favors	readers	over	writers.
In	the	concurrent	server	in	
Figure	
12.14
,	we	created	a	new	thread	for
each	new	client.	A	disadvantage	of	this	approach	is	that	we	incur	the
nontrivial	cost	of	creating	a	new	thread	for	each	new	client.	A	server
based	on	prethreading	tries	to	reduce	this	overhead	by	using	the
producer-consumer	model	shown	in	
Figure	
12.27
.	The	server	consists
of	a	main	thread	and	a	set	of	worker	threads.	The	main	thread	repeatedly
accepts	connection	requests	from	clients	and	places
Aside	
Other	synchronization
mechanisms
We	have	shown	you	how	to	synchronize	threads	using
semaphores,	mainly	because	they	are	simple,	classical,	and	have
a	clean	semantic	model.	But	you	should	know	that	other
synchronization	techniques	exist	as	well.	For	example,	Java
threads	are	synchronized	with	a	mechanism	called	a	
Java	monitor
[
48
],	which	provides	a	higher-level	abstraction	of	the	mutual
exclusion	and	scheduling	capabilities	of	semaphores;	in	fact,
monitors	can	be	implemented	with	semaphores.	As	another
example,	the	Pthreads	interface	defines	a	set	of	synchronization
operations	on	
mutex
and	
condition
variables.	Pthreads	mutexes
are	used	for	mutual	exclusion.	Condition	variables	are	used	for
scheduling	accesses	to	shared	resources,	such	as	the	bounded
buffer	in	a	producer-consumer	program.</p>
<p>Figure	
12.27	
Organization	of	a	prethreaded	concurrent	server.
A	set	of	existing	threads	repeatedly	remove	and	process	connected
descriptors	from	a	bounded	buffer.
the	resulting	connected	descriptors	in	a	bounded	buffer.	Each	worker
thread	repeatedly	removes	a	descriptor	from	the	buffer,	services	the
client,	and	then	waits	for	the	next	descriptor.
Figure	
12.28
shows	how	we	would	use	the	S
BUF</p>
<p>package	to
implement	a	prethreaded	concurrent	echo	server.	After	initializing	buffer
(line	24),	the	main	thread	creates	the	set	of	worker	threads	(lines
25−26).	Then	it	enters	the	infinite	server	loop,	accepting	connection
requests	and	inserting	the	resulting	connected	descriptors	in	
.	Each
worker	thread	has	a	very	simple	behavior.	It	waits	until	it	is	able	to
remove	a	connected	descriptor	from	the	buffer	(line	39)	and	then	calls	the
function	to	echo	client	input.
The	
function	in	
Figure	
12.29
is	a	version	of	the	
function
from	
Figure	
11.22
that	records	the	cumulative	number	of	bytes
received	from	all	clients	in	a	global	variable	called	
.	This	is</p>
<p>interesting	code	to	study	because	it	shows	you	a	general	technique	for
initializing	packages	that	are	called	from	thread	routines.	In	our	case,	we
need	to	initialize	the	
counter	and	the	
semaphore.	One
approach,	which	we	used	for	the	S
BUF</p>
<p>and	R
IO</p>
<h2>packages,	is	to	require	the
main	thread	to	explicitly	call	an	initialization	function.	Another	approach,
shown	here,	uses	the	
function	(line	19)	to	call</h2>
<p>code/conc/echoservert-pre.c</p>
<hr />
<h2>code/conc/echoservert-pre.c
Figure	
12.28	
A	prethreaded	concurrent	echo	server.
The	server	uses	a	producer-consumer	model	with	one	producer	and
multiple	consumers.</h2>
<p>code/conc/echo-cnt.c</p>
<hr />
<p>code/conc/echo-cnt.c
Figure	
12.29	
:	A	version	of	
that	counts	all	bytes
received	from	clients.
the	initialization	function	the	first	time	some	thread	calls	the	
function.	The	advantage	of	this	approach	is	that	it	makes	the	package
easier	to	use.	The	disadvantage	is	that	every	call	to	
makes	a
call	to	
,	which	most	times	does	nothing	useful.
Once	the	package	is	initialized,	the	
function	initializes	the	R
IO
buffered	I/O	package	(line	20)	and	then	echoes	each	text	line	that	is
received	from	the	client.	Notice	that	the	accesses	to	the	shared	
variable	in	lines	23−25	are	protected	by	
P
and	
V
operations.</p>
<p>Aside	
Event-driven	programs	based	on
threads
I/O	multiplexing	is	not	the	only	way	to	write	an	event-driven
program.	For	example,	you	might	have	noticed	that	the	concurrent
prethreaded	server	that	we	just	developed	is	really	an	event-
driven	server	with	simple	state	machines	for	the	main	and	worker
threads.	The	main	thread	has	two	states	(&quot;waiting	for	connection
request&quot;	and	&quot;waiting	for	available	buffer	slot&quot;),	two	I/O	events
(&quot;connection	request	arrives&quot;	and	&quot;buffer	slot	becomes	available&quot;),
and	two	transitions	(&quot;accept	connection	request&quot;	and	&quot;insert	buffer
item&quot;).	Similarly,	each	worker	thread	has	one	state	(&quot;waiting	for
available	buffer	item&quot;),	one	I/O	event	(&quot;buffer	item	becomes
available&quot;),	and	one	transition	(&quot;remove	buffer	item&quot;).
Figure	
12.30	
Relationships	between	the	sets	of	sequential,
concurrent,	and	parallel	programs.</p>
<p>12.6	
Using	Threads	for	Parallelism
Thus	far	in	our	study	of	concurrency,	we	have	assumed	concurrent
threads	exe-cuting	on	uniprocessor	systems.	However,	most	modern
machines	have	multi-core	processors.	Concurrent	programs	often	run
faster	on	such	machines	because	the	operating	system	kernel	schedules
the	concurrent	threads	in	parallel	on	multiple	cores,	rather	than
sequentially	on	a	single	core.	Exploiting	such	parallelism	is	critically
important	in	applications	such	as	busy	Web	servers,	database	servers,
and	large	scientific	codes,	and	it	is	becoming	increasingly	useful	in
mainstream	applications	such	as	Web	browsers,	spreadsheets,	and
document	processors.
Figure	
12.30
shows	the	set	relationships	between	sequential,
concurrent,	and	parallel	programs.	The	set	of	all	programs	can	be
partitioned	into	the	disjoint	sets	of	sequential	and	concurrent	programs.	A
sequential	program	is	written	as	a	single	logical	flow.	A	concurrent
program	is	written	as	multiple	concurrent	flows.	A	parallel	program	is	a
concurrent	program	running	on	multiple	processors.	Thus,	the	set	of
parallel	programs	is	a	proper	subset	of	the	set	of	concurrent	programs.
A	detailed	treatment	of	parallel	programs	is	beyond	our	scope,	but
studying	a	few	simple	example	programs	will	help	you	understand	some
important	aspects	of	parallel	programming.	For	example,	consider	how
we	might	sum	the	sequence	of	integers	0,	.	.	.	,	
n
−	1	in	parallel.	Of
course,	there	is	a	closed-form	solution	for	this	particular	problem,	but</p>
<p>nonetheless	it	is	a	concise	and	easy-to-understand	exemplar	that	will
allow	us	to	make	some	interesting	points	about	parallel	programs.
The	most	straightforward	approach	for	assigning	work	to	different	threads
is	to	partition	the	sequence	into	
t
disjoint	regions	and	then	assign	each	of
t
different	
threads	to	work	on	its	own	region.	For	simplicity,	assume	that	
n
is	a	multiple	of	
t
,	such	that	each	region	has	
n/t
elements.	Let's	look	at
some	of	the	different	ways	that	multiple	threads	might	work	on	their
assigned	regions	in	parallel.
The	simplest	and	most	straightforward	option	is	to	have	the	threads	sum
into	a	shared	global	variable	that	is	protected	by	a	mutex.	
Figure
12.31
shows	how	we	might	implement	this.	In	lines	28−33,	the	main
thread	creates	the	peer	threads	and	then	waits	for	them	to	terminate.
Notice	that	the	main	thread	passes	a	small	integer	to	each	peer	thread
that	serves	as	a	unique	thread	ID.	Each	peer	thread	will	use	its	thread	ID
to	determine	which	portion	of	the	sequence	it	should	work	on.	This	idea
of	passing	a	small	unique	thread	ID	to	the	peer	threads	is	a	general
technique	that	is	used	in	many	parallel	applications.	After	the	peer
threads	have	terminated,	the	global	variable	
contains	the	final	sum.
The	main	thread	then	uses	the	closed-form	solution	to	verify	the	result
(lines	36−37).
Figure	
12.32
shows	the	function	that	each	peer	thread	executes.	In
line	4,	the	thread	extracts	the	thread	ID	from	the	thread	argument	and
then	uses	this	ID	to	determine	the	region	of	the	sequence	it	should	work
on	(lines	5−6).	In	lines	9−13,	the	thread	iterates	over	its	portion	of	the
sequence,	updating	the	shared	global	variable	
on	each	iteration.</p>
<p>Notice	that	we	are	careful	to	protect	each	update	with	
P
and	
V
mutex
operations.
When	we	run	
on	a	system	with	four	cores	on	a	sequence	of
size	
n
=	2
and	measure	its	running	time	(in	seconds)	as	a	function	of
the	number	of	threads,	we	get	a	nasty	surprise:
Number	of	threads
Version
1
2
4
8
16
68
432
719
552
599
Not	only	is	the	program	extremely	slow	when	it	runs	sequentially	as	a
single	thread,	it	is	nearly	an	order	of	magnitude	slower	when	it	runs	in
parallel	as	multiple	threads.	And	the	performance	gets	worse	as	we	add
more	cores.	The	reason	for	this	poor	performance	is	that	the
synchronization	operations	(
P
and	
V
)	are	very	expensive	relative	to	the
cost	of	a	single	memory	update.	This	highlights	an	important	lesson
about	parallel	programming:	
Synchronization	overhead	is	expensive	and
should	be	avoided	if	possible.	If	it	cannot	be	avoided,	the	overhead
should	be	amortized	by	as	much	useful	computation	as	possible.
One	way	to	avoid	synchronization	in	our	example	program	is	to	have
each	peer	thread	compute	its	partial	sum	in	a	private	variable	that	is	not
shared	with	any	other	thread,	as	shown	in	
Figure	
12.33
.	The	main
thread	(not	shown)	defines	a	global	array	called	
,	and	each	peer
thread	
i
accumulates	its	partial	sum	in	
.	Since	we	are	careful	to
give	each	peer	thread	a	unique	memory	location	to	update,	it	is	not
necessary	to	protect	these	updates	with	mutexes.	The	only	necessary
31</p>
<h2>synchronization	is	that	the	main	thread	must	wait	for	all	of	the	children	to
finish.	After	the	peer	threads	have	terminated,	the	main	thread	sums	up
the	elements	of	the	
vector	to	arrive	at	the	final	result.</h2>
<p>code/conc/psum-mutex.c</p>
<hr />
<h2>code/conc/psum-mutex.c
Figure	
12.31	
Main	routine	for	
.
Uses	multiple	threads	to	sum	the	elements	of	a	sequence	into	a	shared
global	variable	protected	by	a	mutex.</h2>
<p>code/conc/psum-mutex.c</p>
<hr />
<h2>code/conc/psum-mutex.c
Figure	
12.32	
Thread	routine	for	
.
Each	peer	thread	sums	into	a	shared	global	variable	protected	by	a
mutex.</h2>
<p>code/conc/psum-array.c</p>
<hr />
<p>code/conc/psum-array.c
Figure	
12.33	
Thread	routine	for	
.
Each	peer	thread	accumulates	its	partial	sum	in	a	private	array	element
that	is	not	shared	with	any	other	peer	thread.
When	we	run	
on	our	four-core	system,	we	see	that	it	runs
orders	of	magnitude	faster	than	
:
Number	of	threads</p>
<h2>Version
1
2
4
8
16
68.00
432.00
719.00
552.00
599.00
7.26
3.64
1.91
1.85
1.84
In	
Chapter	
5
,	we	learned	how	to	use	local	variables	to	eliminate
unnecessary	memory	references.	
Figure	
12.34
shows	how	we	can
apply	this	principle	by	having	each	peer	thread	accumulate	its	partial	sum
into	a	local	variable	rather	than	a	global	variable.	When	we	run	
on	our	four-core	machine,	we	get	another	order-of-magnitude
decrease	in	running	time:
Number	of	threads
Version
1
2
4
8
16
68.00
432.00
719.00
552.00
599.00
7.26
3.64
1.91
1.85
1.84
1.06
0.54
0.28
0.29
0.30</h2>
<p>code/conc/psum-local.c</p>
<hr />
<p>code/conc/psum-local.c
Figure	
12.34	
Thread	routine	for	
.
Each	peer	thread	accumulates	its	partial	sum	in	a	local	variable.</p>
<p>Figure	
12.35	
Performance	of	
(
Figure	
12.34
).
Summing	a	sequence	of	2
elements	using	four	processor	cores.
An	important	lesson	to	take	away	from	this	exercise	is	that	writing	parallel
programs	is	tricky.	Seemingly	small	changes	to	the	code	have	a
significant	impact	on	performance.
Characterizing	the	Performance	of	Parallel
Programs
Figure	
12.35
plots	the	total	elapsed	running	time	of	the	
program	in	
Figure	
12.34
as	a	function	of	the	number	of	threads.	In
each	case,	the	program	runs	on	a	system	with	four	processor	cores	and
sums	a	sequence	of	
n
=	2
elements.	We	see	that	running	time
decreases	as	we	increase	the	number	of	threads,	up	to	four	threads,	at
which	point	it	levels	off	and	even	starts	to	increase	a	little.
In	the	ideal	case,	we	would	expect	the	running	time	to	decrease	linearly
with	the	number	of	cores.	That	is,	we	would	expect	running	time	to	drop
by	half	each	time	we	double	the	number	of	threads.	This	is	indeed	the
case	until	we	reach	the	point	(
t
&gt;	4)	where	each	of	the	four	cores	is	busy
running	at	least	one	thread.	Running	time	actually	increases	a	bit	as	we
increase	the	number	of	threads	because	of	the	overhead	of	context
switching	multiple	threads	on	the	same	core.	For	this	reason,	parallel
programs	are	often	written	so	that	each	core	runs	exactly	one	thread.
Although	absolute	running	time	is	the	ultimate	measure	of	any	program's
performance,	there	are	some	useful	relative	measures	that	can	provide
31
31</p>
<h1>insight	into	how	well	a	parallel	program	is	exploiting	potential	parallelism.
The	
speedup
of	a	parallel	program	is	typically	defined	as
where	
p
is	the	number	of	processor	cores	and	
T
is	the	running	time	on	
k
cores.	This	formulation	is	sometimes	referred	to	as	
strong	scaling.
When
T
is	the	execution
Threads	(
t
)
1
2
4
8
16
Cores	(
p
)
1
2
4
4
4
Running	time	(
T
)
1.06
0.54
0.28
0.29
0.30
Speedup	(
S
)
1
1.9
3.8
3.7
3.5
Efficiency	(
E
)
100%
98%
95%
91%
88%
Figure	
12.36	
Speedup	and	parallel	efficiency	for	the	execution	times
in	
Figure	
12.35
.
time	of	a	sequential	version	of	the	program,	then	
S
is	called	the	
absolute
speedup
.	When	
T
is	the	execution	time	of	the	parallel	version	of	the
program	running	on	one	core,	then	
S
is	called	the	
relative	speedup
.
Absolute	speedup	is	a	truer	measure	of	the	benefits	of	parallelism	than
relative	speedup.	Parallel	programs	often	suffer	from	synchronization
overheads,	even	when	they	run	on	one	processor,	and	these	overheads
can	artificially	inflate	the	relative	speedup	numbers	because	they
increase	the	size	of	the	numerator.	On	the	other	hand,	absolute	speedup
S
p</h1>
<p>T
1
T
p
k
1
p
p
p
1
p</p>
<h1>is	more	difficult	to	measure	than	relative	speedup	because	measuring
absolute	speedup	requires	two	different	versions	of	the	program.	For
complex	parallel	codes,	creating	a	separate	sequential	version	might	not
be	feasible,	either	because	the	code	is	too	complex	or	because	the
source	code	is	not	available.
A	related	measure,	known	as	
efficiency
,	is	defined	as
and	is	typically	reported	as	a	percentage	in	the	range	(0,	100].	Efficiency
is	a	measure	of	the	overhead	due	to	parallelization.	Programs	with	high
efficiency	are	spending	more	time	doing	useful	work	and	less	time
synchronizing	and	communicating	than	programs	with	low	efficiency.
Figure	
12.36
shows	the	different	speedup	and	efficiency	measures	for
our	example	parallel	sum	program.	Efficiencies	over	90	percent	such	as
these	are	very	good,	but	do	not	be	fooled.	We	were	able	to	achieve	high
efficiency	because	our	problem	was	trivially	easy	to	parallelize.	In
practice,	this	is	not	usually	the	case.	Parallel	programming	has	been	an
active	area	of	research	for	decades.	With	the	advent	of	commodity	multi-
core	machines	whose	core	count	is	doubling	every	few	years,	parallel
programming	continues	to	be	a	deep,	difficult,	and	active	area	of
research.
There	is	another	view	of	speedup,	known	as	
weak	scaling
,	which
increases	the	problem	size	along	with	the	number	of	processors,	such
that	the	amount	of	work	performed	on	each	processor	is	held	constant	as
the	number	of	processors	increases.	With	this	formulation,	speedup	and
efficiency	are	expressed	in	terms	of	the	total	amount	of	work
E
p</h1>
<h1>S
p
p</h1>
<p>T
1
p
T
p</p>
<p>accomplished	per	unit	time.	For	example,	if	we	can	double	the	number	of
processors	and	do	twice	the	amount	of	work	per	hour,	then	we	are
enjoying	linear	speedup	and	100	percent	efficiency.
Weak	scaling	is	often	a	truer	measure	than	strong	scaling	because	it
more	accurately	reflects	our	desire	to	use	bigger	machines	to	do	more
work.	This	is	particularly	true	for	scientific	codes,	where	the	problem	size
can	be	easily	increased	and	where	bigger	problem	sizes	translate	directly
to	better	predictions	of	nature.	However,	there	exist	applications	whose
sizes	are	not	so	easily	increased,	and	for	these	applications	strong
scaling	is	more	appropriate.	For	example,	the	amount	of	work	performed
by	real-time	signal-processing	applications	is	often	determined	by	the
properties	of	the	physical	sensors	that	are	generating	the	signals.
Changing	the	total	amount	of	work	requires	using	different	physical
sensors,	which	might	not	be	feasible	or	necessary.	For	these
applications,	we	typically	want	to	use	parallelism	to	accomplish	a	fixed
amount	of	work	as	quickly	as	possible.
Practice	Problem	
12.11	
(solution	page	
1038
)
Fill	in	the	blanks	for	the	parallel	program	in	the	following	table.	Assume
strong	scaling.
Threads	(
t
)
1
2
4
Cores	(
p
)
1
2
4
Running	time	(
T
)
12
8
6
Speedup	(
S
p
)</p>
<hr />
<p>1.5</p>
<hr />
<p>p</p>
<p>Efficiency	(
E
)
100%</p>
<hr />
<p>50%
p</p>
<p>12.7	
Other	Concurrency	Issues
You	probably	noticed	that	life	got	much	more	complicated	once	we	were
asked	to	synchronize	accesses	to	shared	data.	So	far,	we	have	looked	at
techniques	for	mutual	exclusion	and	producer-consumer	synchronization,
but	this	is	only	the	tip	of	the	iceberg.	Synchronization	is	a	fundamentally
difficult	problem	that	raises	issues	that	simply	do	not	arise	in	ordinary
sequential	programs.	This	section	is	a	survey	(by	no	means	complete)	of
some	of	the	issues	you	need	to	be	aware	of	when	you	write	concurrent
programs.	To	keep	things	concrete,	we	will	couch	our	discussion	in	terms
of	threads.	Keep	in	mind,	however,	that	these	are	typical	of	the	issues
that	arise	when	concurrent	flows	of	any	kind	manipulate	shared
resources.
12.7.1	
Thread	Safety
When	we	program	with	threads,	we	must	be	careful	to	write	functions	that
have	a	property	called	thread	safety.	A	function	is	said	to	be	
thread-safe
if
and	only	if	it	will	always	produce	correct	results	when	called	repeatedly
from	multiple	concurrent	threads.	If	a	function	is	not	thread-safe,	then	we
say	it	is	
thread-unsafe
.
We	can	identify	four	(nondisjoint)	classes	of	thread-unsafe	functions:</p>
<h2>Class	1:	
Functions	that	do	not	protect	shared	variables.
We	have
already	encountered	this	problem	with	the	thread	function	in	
Figure
12.16
,	which</h2>
<h2 id="codeconcrandc"><a class="header" href="#codeconcrandc">code/conc/rand.c</a></h2>
<p>code/conc/rand.c
Figure	
12.37	
A	thread-unsafe	pseudorandom	number	generator.
(Based	on	[
61
])
increments	an	unprotected	global	counter	variable.	This	class	of
thread-unsafe	functions	is	relatively	easy	to	make	thread-safe:	protect</p>
<h2>the	shared	variables	with	synchronization	operations	such	as	
P
and
V
.	An	advantage	is	that	it	does	not	require	any	changes	in	the	calling
program.	A	disadvantage	is	that	the	synchronization	operations	slow
down	the	function.
Class	2:	
Functions	that	keep	state	across	multiple	invocations.
A
pseudorandom	number	generator	is	a	simple	example	of	this	class	of
thread-unsafe	functions.	Consider	the	pseudorandom	number
generator	package	in	
Figure	
12.37
.
The	
function	is	thread-unsafe	because	the	result	of	the	current
invocation	depends	on	an	intermediate	result	from	the	previous
iteration.	When	we	call	
repeatedly	from	a	single	thread	after
seeding	it	with	a	call	to	
,	we	can	expect	a	repeatable	sequence
of	numbers.	However,	this	assumption	no	longer	holds	if	multiple
threads	are	calling	
.
The	only	way	to	make	a	function	such	as	
thread-safe	is	to
rewrite	it	so	that	it	does	not	use	any	
data,	relying	instead	on
the	caller	to	pass	the	state	information	in	arguments.	The
disadvantage	is	that	the	programmer	is	now	forced	to	change	the
code	in	the	calling	routine	as	well.	In	a	large	program	where	there	are
potentially	hundreds	of	different	call	sites,	making	such	modifications
could	be	nontrivial	and	prone	to	error.
Class	3:	
Functions	that	return	a	pointer	to	a	static	variable.
Some
functions,	such	as	
and	
,	compute	a	result	in	a
variable	and	then	return	a	pointer	to	that	variable.	If	we	call
such	functions	from</h2>
<p>code/conc/ctime-ts.c</p>
<hr />
<p>code/conc/ctime-ts.c
Figure	
12.38	
Thread-safe	wrapper	function	for	the	C	standard
library	
function.
This	example	uses	the	lock-and-copy	technique	to	call	a	class	3
thread-unsafe	function.
concurrent	threads,	then	disaster	is	likely,	as	results	being	used	by
one	thread	are	silently	overwritten	by	another	thread.
There	are	two	ways	to	deal	with	this	class	of	thread-unsafe	functions.
One	option	is	to	rewrite	the	function	so	that	the	caller	passes	the
address	of	the	variable	in	which	to	store	the	results.	This	eliminates	all
shared	data,	but	it	requires	the	programmer	to	have	access	to	the
function	source	code.</p>
<p>If	the	thread-unsafe	function	is	difficult	or	impossible	to	modify	(e.g.,
the	code	is	very	complex	or	there	is	no	source	code	available),	then
another	option	is	to	use	the	
lock-and-copy
technique.	The	basic	idea
is	to	associate	a	mutex	with	the	thread-unsafe	function.	At	each	call
site,	lock	the	mutex,	call	the	thread-unsafe	function,	copy	the	result
returned	by	the	function	to	a	private	memory	location,	and	then	unlock
the	mutex.	To	minimize	changes	to	the	caller,	you	should	define	a
thread-safe	wrapper	function	that	performs	the	lock-and-copy	and
then	replace	all	calls	to	the	thread-unsafe	function	with	calls	to	the
wrapper.	For	example,	
Figure	
12.38
shows	a	thread-safe	wrapper
for	
that	uses	the	lock-and-copy	technique.
Class	4:	
Functions	that	call	thread-unsafe	functions.
If	a	function	
f
calls	a	thread-unsafe	function	
g
,	is	
f
thread-unsafe?	It	depends.	If	
g
is
a	class	2	function	that	relies	on	state	across	multiple	invocations,	then
f
is	also	thread-unsafe	and	there	is	no	recourse	short	of	rewriting	
g
.
However,	if	
g
is	a	class	1	or	class	3	function,	then	
f
can	still	be	thread-
safe	if	you	protect	the	call	site	and	any	resulting	shared	data	with	a
mutex.	We	see	a	good	example	of	this	in	
Figure	
12.38
,	where	we
use	lock-and-copy	to	write	a	thread-safe	function	that	calls	a	thread-
unsafe	function.
Figure	
12.39	
Relationships	between	the	sets	of	reentrant,	thread-
safe,	and	thread-unsafe	functions.</p>
<hr />
<h2 id="codeconcrand-rc"><a class="header" href="#codeconcrand-rc">code/conc/rand-r.c</a></h2>
<p>code/conc/rand-r.c
Figure	
12.40	
:	A	reentrant	version	of	the	
function	from
Figure	
12.37
.
12.7.2	
Reentrancy
There	is	an	important	class	of	thread-safe	functions,	known	as	
reentrant
functions
,	that	are	characterized	by	the	property	that	they	do	not
reference	
any
shared	data	when	they	are	called	by	multiple	threads.
Although	the	terms	
thread-safe
and	
reentrant
are	sometimes	used
(incorrectly)	as	synonyms,	there	is	a	clear	technical	distinction	that	is
worth	preserving.	
Figure	
12.39
shows	the	set	relationships	between
reentrant,	thread-safe,	and	thread-unsafe	functions.	The	set	of	all
functions	is	partitioned	into	the	disjoint	sets	of	thread-safe	and	thread-
unsafe	functions.	The	set	of	reentrant	functions	is	a	proper	subset	of	the
thread-safe	functions.</p>
<p>Reentrant	functions	are	typically	more	efficient	than	non-reentrant	thread-
safe	functions	because	they	require	no	synchronization	operations.
Furthermore,	the	only	way	to	convert	a	class	2	thread-unsafe	function
into	a	thread-safe	one	is	to	rewrite	it	so	that	it	is	reentrant.	For	example,
Figure	
12.40
shows	a	reentrant	version	of	the	
function	from
Figure	
12.37
.	The	key	idea	is	that	we	have	replaced	the	static	
variable	with	a	pointer	that	is	passed	in	by	the	caller.
Is	it	possible	to	inspect	the	code	of	some	function	and	declare	a	priori
that	it	is	reentrant?	Unfortunately,	it	depends.	If	all	function	arguments	are
passed	by	value	(i.e.,	no	pointers)	and	all	data	references	are	to	local
automatic	stack	variables	(i.e.,	no	references	to	static	or	global
variables),	then	the	function	is	
explicitly	reentrant
,	in	the	sense	that	we
can	assert	its	reentrancy	regardless	of	how	it	is	called.
However,	if	we	loosen	our	assumptions	a	bit	and	allow	some	parameters
in	our	otherwise	explicitly	reentrant	function	to	be	passed	by	reference
(i.e.,	we	allow	them	to	pass	pointers),	then	we	have	an	
implicitly	reentrant
function,	in	the	sense	that	it	is	only	reentrant	if	the	calling	threads	are
careful	to	pass	pointers	
to	nonshared	data.	For	example,	the	
function	in	
Figure	
12.40
is	implicitly	reentrant.
We	always	use	the	term	
reentrant
to	include	both	explicit	and	implicit
reentrant	functions.	However,	it	is	important	to	realize	that	reentrancy	is
sometimes	a	property	of	both	the	caller	and	the	callee,	and	not	just	the
callee	alone.
Practice	Problem	
12.12	
(solution	page</p>
<p>1038
)
The	
function	in	
Figure	
12.38
is	thread-safe	but	not
reentrant.	Explain.
12.7.3	
Using	Existing	Library
Functions	in	Threaded	Programs
Most	Linux	functions,	including	the	functions	defined	in	the	standard	C
library	(such	as	
,	and	
),	are	thread-
safe,	with	only	a	few	exceptions.	
Figure	
12.41
lists	some	common
exceptions.	(See	[
110
]	for	a	complete	list.)	The	
function	is	a
deprecated	function	(one	whose	use	is	discouraged)	for	parsing	strings.
The	
,	and	
functions	are	popular	functions	for
converting	back	and	forth	between	different	time	and	date	formats.	The
,	and	
functions	are	obsolete
network	programming	functions	that	have	been	replaced	by	the	reentrant
getaddrinfo,	
,	and	
functions,	respectively	(see
Chapter	
11
).	With	the	exceptions	of	
and	
,	they	are	of	the
class	3	variety	that	return	a	pointer	to	a	static	variable.	If	we	need	to	call
one	of	these	functions	in	a	threaded	program,	the	least	disruptive
approach	to	the	caller	is	to	lock	and	copy.	However,	the	lock-and-copy
approach	has	a	number	of	disadvantages.	First,	the	additional
synchronization	slows	down	the	program.	Second,	functions	that	return
pointers	to	complex	structures	of	structures	require	a	
deep	copy
of	the
structures	in	order	to	copy	the	entire	structure	hierarchy.	Third,	the	lock-</p>
<p>and-copy	approach	will	not	work	for	a	class	2	thread-unsafe	function
such	as	
that	relies	on	static	state	across	calls.
Thread-unsafe	function
Thread-unsafe	class
Linux	thread-safe	version
2
2
3
3
3
3
3
(none)
3
Figure	
12.41	
Common	thread-unsafe	library	functions.
Therefore,	Linux	systems	provide	reentrant	versions	of	most	thread-
unsafe	functions.	The	names	of	the	reentrant	versions	always	end	with
the	
suffix.	For	example,	the	reentrant	version	of	
is	called
.	We	recommend	using	these	functions	whenever	possible.
12.7.4	
Races</p>
<h2>A	
race
occurs	when	the	correctness	of	a	program	depends	on	one	thread
reaching	point	
x
in	its	control	flow	before	another	thread	reaches	point	
y
.
Races	usually	occur	because	programmers	assume	that	threads	will	take
some	particular	trajectory	through	the	execution	state	space,	forgetting
the	golden	rule	that	threaded	programs	must	work	correctly	for	any
feasible	trajectory.
An	example	is	the	easiest	way	to	understand	the	nature	of	races.
Consider	the	simple	program	in	
Figure	
12.42
.	The	main	thread
creates	four	peer	threads	and	passes	a	pointer	to	a	unique	integer	ID	to
each	one.	Each	peer	thread	copies	the</h2>
<p>code/conc/race.c</p>
<hr />
<p>code/conc/race.c
Figure	
12.42	
program	with	a	race.
ID	passed	in	its	argument	to	a	local	variable	(line	22)	and	then	prints	a
message	containing	the	ID.	It	looks	simple	enough,	but	when	we	run	this
program	on	our	system,	we	get	the	following	incorrect	result:
The	problem	is	caused	by	a	race	between	each	peer	thread	and	the	main
thread.	Can	you	spot	the	race?	Here	is	what	happens.	When	the	main</p>
<p>thread	creates	a	peer	thread	in	line	13,	it	passes	a	pointer	to	the	local
stack	variable	
i
.	At	this	point,	the	race	is	on	between	the	next	increment
of	
in	line	12	and	the	dereferencing	and	assignment	of	the	argument	in
line	22.	If	the	peer	thread	executes	line	22	before	the	main	thread
increments	
in	line	12,	then	the	
variable	gets	the	correct	ID.
Otherwise,	it	will	contain	the	ID	of	some	other	thread.	The	scary	thing	is
that	whether	we	get	the	correct	answer	depends	on	how	the	kernel
schedules	the	execution	of	the	threads.	On	our	system	it	fails,	but	on
other	systems	it	might	work	correctly,	leaving	the	programmer	blissfully
unaware	of	a	serious	bug.
To	eliminate	the	race,	we	can	dynamically	allocate	a	separate	block	for
each	integer	ID	and	pass	the	thread	routine	a	pointer	to	this	block,	as
shown	in	
Figure	
12.43
(lines	12−14).	Notice	that	the	thread	routine
must	free	the	block	in	order	to	avoid	a	memory	leak.
When	we	run	this	program	on	our	system,	we	now	get	the	correct	result:
Practice	Problem	
12.13	
(solution	page
1039
)</p>
<h2>In	
Figure	
12.43
,	we	might	be	tempted	to	free	the	allocated
memory	block	immediately	after	line	14	in	the	main	thread,	instead
of	freeing	it	in	the	peer	thread.	But	this	would	be	a	bad	idea.	Why?
Practice	Problem	
12.14	
(solution	page
1039
)
1
.	
In	
Figure	
12.43
,	we	eliminated	the	race	by	allocating	a	separate
block	for	each	integer	ID.	Outline	a	different	approach	that	does
not	call	the	
or	
functions.
2
.	
What	are	the	advantages	and	disadvantages	of	this	approach?</h2>
<p>code/conc/norace.c</p>
<hr />
<p>code/conc/norace.c
Figure	
12.43
A	correct	version	of	the	program	in	
Figure	
12.42
without	a	race.
12.7.5	
Deadlocks
Semaphores	introduce	the	potential	for	a	nasty	kind	of	run-time	error,
called	
deadlock
,	where	a	collection	of	threads	is	blocked,	waiting	for	a
condition	that	will	never	be	true.	The	progress	graph	is	an	invaluable	tool
for	understanding	deadlock.	For	example,	
Figure	
12.44
shows	the</p>
<p>progress	graph	for	a	pair	of	threads	that	use	two	semaphores	for	mutual
exclusion.	From	this	graph,	we	can	glean	some	important	insights	about
deadlock:
The	programmer	has	incorrectly	ordered	the	
P
and	
V
operations	such
that	the	forbidden	regions	for	the	two	semaphores	overlap.	If	some
execution	trajectory	happens	to	reach	the	
deadlock	state	d
,	then	no
further	progress	is
Figure	
12.44	
Progress	graph	for	a	program	that	can	deadlock.</p>
<p>possible	because	the	overlapping	forbidden	regions	block	progress	in
every	legal	direction.	In	other	words,	the	program	is	deadlocked
because	each	thread	is	waiting	for	the	other	to	do	a	
V
operation	that
will	never	occur.
The	overlapping	forbidden	regions	induce	a	set	of	states	called	the
deadlock	region
.	If	a	trajectory	happens	to	touch	a	state	in	the
deadlock	region,	then	deadlock	is	inevitable.	Trajectories	can	enter
deadlock	regions,	but	they	can	never	leave.
Deadlock	is	an	especially	difficult	issue	because	it	is	not	always
predictable.	Some	lucky	execution	trajectories	will	skirt	the	deadlock
region,	while	others	will	be	trapped	by	it.	
Figure	
12.44
shows	an
example	of	each.	The	implications	for	a	programmer	are	scary.	You
might	run	the	same	program	a	thousand	times	without	any	problem,
but	then	the	next	time	it	deadlocks.	Or	the	program	might	work	fine	on
one	machine	but	deadlock	on	another.	Worst	of	all,	the	error	is	often
not	repeatable	because	different	executions	have	different
trajectories.
Programs	deadlock	for	many	reasons,	and	preventing	them	is	a	difficult
problem	in	general.	However,	when	binary	semaphores	are	used	for
mutual	exclusion,	as	in	
Figure	
12.44
,	then	you	can	apply	the	following
simple	and	effective	rule	to	prevent	deadlocks:</p>
<p>Figure	
12.45	
Progress	graph	for	a	deadlock-free	program.
Mutex	lock	ordering	rule:	
Given	a	total	ordering	of	all	mutexes,	a
program	is	deadlock-free	if	each	thread	acquires	its	mutexes	in	order
and	releases	them	in	reverse	order.
For	example,	we	can	fix	the	deadlock	in	
Figure	
12.44
by	locking	
s
first,
then	
t
,	in	each	thread.	
Figure	
12.45
shows	the	resulting	progress
graph.
Practice	Problem	
12.15	
(solution	page</p>
<p>1039
)
Consider	the	following	program,	which	attempts	to	use	a	pair	of
semaphores	for	mutual	exclusion.
A
.	
Draw	the	progress	graph	for	this	program.
B
.	
Does	it	always	deadlock?
C
.	
If	so,	what	simple	change	to	the	initial	semaphore	values
will	eliminate	the	potential	for	deadlock?
D
.	
Draw	the	progress	graph	for	the	resulting	deadlock-free
program.</p>
<p>12.8	
Summary
A	concurrent	program	consists	of	a	collection	of	logical	flows	that	overlap
in	time.	In	this	chapter,	we	have	studied	three	different	mechanisms	for
building	concurrent	programs:	processes,	I/O	multiplexing,	and	threads.
We	used	a	concurrent	network	server	as	the	motivating	application
throughout.
Processes	are	scheduled	automatically	by	the	kernel,	and	because	of
their	separate	virtual	address	spaces,	they	require	explicit	IPC
mechanisms	in	order	to	share	data.	Event-driven	programs	create	their
own	concurrent	logical	flows,	which	are	modeled	as	state	machines,	and
use	I/O	multiplexing	to	explicitly	schedule	the	flows.	Because	the
program	runs	in	a	single	process,	sharing	data	between	flows	is	fast	and
easy.	Threads	are	a	hybrid	of	these	approaches.	Like	flows	based	on
processes,	threads	are	scheduled	automatically	by	the	kernel.	Like	flows
based	on	I/O	multiplexing,	threads	run	in	the	context	of	a	single	process,
and	thus	can	share	data	quickly	and	easily.
Regardless	of	the	concurrency	mechanism,	synchronizing	concurrent
accesses	to	shared	data	is	a	difficult	problem.	The	
P
and	
V
operations	on
semaphores	have	been	developed	to	help	deal	with	this	problem.
Semaphore	operations	can	be	used	to	provide	mutually	exclusive	access
to	shared	data,	as	well	as	to	schedule	access	to	resources	such	as	the
bounded	buffers	in	producer-consumer	systems	and	shared	objects	in
readers-writers	systems.	A	concurrent	prethreaded	echo	server	provides
a	compelling	example	of	these	usage	scenarios	for	semaphores.</p>
<p>Concurrency	introduces	other	difficult	issues	as	well.	Functions	that	are
called	by	threads	must	have	a	property	known	as	thread	safety.	We	have
identified	four	classes	of	thread-unsafe	functions,	along	with	suggestions
for	making	them	thread-safe.	Reentrant	functions	are	the	proper	subset
of	thread-safe	functions	that	do	not	access	any	shared	data.	Reentrant
functions	are	often	more	efficient	than	non-reentrant	functions	because
they	do	not	require	any	synchronization	primitives.	Some	other	difficult
issues	that	arise	in	concurrent	programs	are	races	and	dead	locks.
Races	occur	when	programmers	make	incorrect	assumptions	about	how
logical	flows	are	scheduled.	Deadlocks	occur	when	a	flow	is	waiting	for
an	event	that	will	never	happen.</p>
<p>Bibliographic	Notes
Semaphore	operations	were	introduced	by	Dijkstra	[
31
].	The	progress
graph	concept	was	introduced	by	Coffman	[
23
]	and	later	formalized	by
Carson	and	Reynolds	[
16
].	The	readers-writers	problem	was	introduced
by	Courtois	et	al	[
25
].	Operating	systems	texts	describe	classical
synchronization	problems	such	as	the	dining	philosophers,	sleeping
barber,	and	cigarette	smokers	problems	in	more	detail	
[
102
,	
106
,	
113
].
The	book	by	Butenhof	[
15
]	is	a	comprehensive	description	of	the	Posix
threads	interface.	The	paper	by	Birrell	[
7
]	is	an	excellent	introduction	to
threads	programming	and	its	pitfalls.	The	book	by	Reinders	[
90
]
describes	a	C/C++	library	that	simplifies	the	design	and	implementation
of	threaded	programs.	Several	texts	cover	the	fundamentals	of	parallel
programming	on	multi-core	systems	[
47
,	
71
].	Pugh	identifies	weaknesses
with	the	way	that	Java	threads	interact	through	memory	and	proposes
replacement	memory	models	[
88
].	Gustafson	proposed	the	weak-scaling
speedup	model	[
43
]	as	an	alternative	to	strong	scaling.</p>
<h2>Homework	Problems
12.16	
♦
Write	a	version	of	
(
Figure	
12.13
)	that	creates	and	reaps	
n
joinable	peer	threads,	where	
n
is	a	command-line	argument.
12.17	
♦
A
.	
The	program	in	
Figure	
12.46
has	a	bug.	The	thread	is
supposed	to	sleep	for	1	second	and	then	print	a	string.	However,
when	we	run	it	on	our	system,	nothing	prints.	Why?
B
.	
You	can	fix	this	bug	by	replacing	the	exit	function	in	line	10	with
one	of	two	different	Pthreads	function	calls.	Which	ones?</h2>
<p>code/conc/hellobug.c</p>
<hr />
<p>code/conc/hellobug.c
Figure	
12.46	
Buggy	program	for	
Problem	
12.17
.
12.18
Using	the	progress	graph	in	
Figure	
12.21
,	classify	the	following
trajectories	as	either	safe	or	unsafe.
A
.	
H
,	
L
,	
U
,	
H
,	
L
,	
S
,	
U
,	
S
,	
T
,	
T
B
.	
H
,	
H
,	
L
,	
U
,	
S
,	
L
,	
T
,	
U
,	
S
,	
T
C
.	
H
,	
L
,	
H
,	
L
,	
U
,	
S
,	
U
,	
S
,	
T
,	
T
2
2
2
1
1
2
1
1
1
2
2
1
1
1
1
2
1
2
2
2
1
1
2
2
2
2
1
1
1
2</p>
<p>12.19	
♦♦
The	solution	to	the	first	readers-writers	problem	in	
Figure	
12.26
gives
a	somewhat	weak	priority	to	readers	because	a	writer	leaving	its	critical
section	might	restart	a	waiting	writer	instead	of	a	waiting	reader.	Derive	a
solution	that	gives	stronger	priority	to	readers,	where	a	writer	leaving	its
critical	section	will	always	restart	a	waiting	reader	if	one	exists.
12.20	
♦♦♦
Consider	a	simpler	variant	of	the	readers-writers	problem	where	there	are
at	most	
N
readers.	Derive	a	solution	that	gives	equal	priority	to	readers
and	writers,	in	the	sense	that	pending	readers	and	writers	have	an	equal
chance	of	being	granted	access	to	the	resource.	
Hint:
You	can	solve	this
problem	using	a	single	counting	semaphore	and	a	single	mutex.
12.21	
♦♦♦♦
Derive	a	solution	to	the	second	readers-writers	problem,	which	favors
writers	instead	of	readers.
12.22	
♦♦</p>
<p>Test	your	understanding	of	the	
function	by	modifying	the	server	in
Figure	
12.6
so	that	it	echoes	at	most	one	text	line	per	iteration	of	the
main	server	loop.
12.23	
♦♦
The	event-driven	concurrent	echo	server	in	
Figure	
12.8
is	flawed
because	a	malicious	client	can	deny	service	to	other	clients	by	sending	a
partial	text	line.	Write	an	improved	version	of	the	server	that	can	handle
these	partial	text	lines	without	blocking.
12.24	
♦
The	functions	in	the	R
IO</p>
<p>I/O	package	(
Section	
10.5
)	are	thread-safe.
Are	they	reentrant	as	well?
12.25	
♦
In	the	prethreaded	concurrent	echo	server	in	
Figure	
12.28
,	each
thread	calls	the	
function	(
Figure	
12.29
).	Is	
thread-
safe?	Is	it	reentrant?	Why	or	why	not?
12.26	
♦♦♦</p>
<p>Use	the	lock-and-copy	technique	to	implement	a	thread-safe	non-
reentrant	version	of	
called	
.	A	correct
solution	will	use	a	deep	copy	of	the	
structure	protected	by	a
mutex.
12.27	
♦♦
Some	network	programming	texts	suggest	the	following	approach	for
reading	and	writing	sockets:	Before	interacting	with	the	client,	open	two
standard	I/O	streams	on	the	same	open	connected	socket	descriptor,	one
for	reading	and	one	for	writing:
When	the	server	finishes	interacting	with	the	client,	close	both	streams	as
follows:</p>
<p>However,	if	you	try	this	approach	in	a	concurrent	server	based	on
threads,	you	will	create	a	deadly	race	condition.	Explain.
12.28	
♦
In	
Figure	
12.45
,	does	swapping	the	order	of	the	two	
V
operations
have	any	effect	on	whether	or	not	the	program	deadlocks?	Justify	your
answer	by	drawing	the	progress	graphs	for	the	four	possible	cases:
Case1
Case2
Case3
Case	4
Thread
1
Thread
2
Thread
1
Thread
2
Thread
1
Thread
2
Thread
1
Thread
2
12.29	
♦
Can	the	following	program	deadlock?	Why	or	why	not?</p>
<p>12.30	
♦
Consider	the	following	program	that	deadlocks.</p>
<h2>A
.	
For	each	thread,	list	the	pairs	of	mutexes	that	it	holds
simultaneously.
B
.	
If	
a	&lt;	b	&lt;	c
,	which	threads	violate	the	mutex	lock	ordering	rule?
C
.	
For	these	threads,	show	a	new	lock	ordering	that	guarantees
freedom	from	deadlock.
12.31	
♦♦♦
Implement	a	version	of	the	standard	I/O	
function,	called	
,
that	times	out	and	returns	
if	it	does	not	receive	an	input	line	on
standard	input	within	5	seconds.	Your	function	should	be	implemented	in
a	package	called	
using	processes,	signals,	and	nonlocal
jumps.	It	should	not	use	the	Linux	alarm	function.	Test	your	solution	using
the	driver	program	in	
Figure	
12.47
.</h2>
<p>code/conc/tfgets-main.c</p>
<hr />
<p>code/conc/tfgets-main.c
Figure	
12.47	
Driver	program	for	Problems	12.31−12.33.
12.32	
♦♦♦
Implement	a	version	of	the	
function	from	
Problem	
12.31
that
uses	the	
function.	Your	function	should	be	implemented	in	a
package	called	
Test	your	solution	using	the	driver
program	from	
Problem	
12.31
.	You	may	assume	that	standard	input	is
assigned	to	descriptor	0.
12.33	
♦♦♦
Implement	a	threaded	version	of	the	
function	from	
Problem
12.31
.	Your	function	should	be	implemented	in	a	package	called
Test	your	solution	using	the	driver	program	from
Problem	
12.31
.</p>
<p>12.34	
♦♦♦
Write	a	parallel	threaded	version	of	an	
N
×	
M
matrix	multiplication	kernel.
Compare	the	performance	to	the	sequential	case.
12.35	
♦♦♦
Implement	a	concurrent	version	of	the	T
INY</p>
<p>Web	server	based	on
processes.	Your	solution	should	create	a	new	child	process	for	each	new
connection	request.	Test	your	solution	using	a	real	Web	browser.
12.36	
♦♦♦
Implement	a	concurrent	version	of	the	T
INY</p>
<p>Web	server	based	on	I/O
multiplexing.	Test	your	solution	using	a	real	Web	browser.
12.37	
♦♦♦
Implement	a	concurrent	version	of	the	T
INY</p>
<p>Web	server	based	on	threads.
Your	solution	should	create	a	new	thread	for	each	new	connection
request.	Test	your	solution	using	a	real	Web	browser.</p>
<p>12.38	
♦♦♦♦
Implement	a	concurrent	prethreaded	version	of	the	T
INY</p>
<p>Web	server.	Your
solution	should	dynamically	increase	or	decrease	the	number	of	threads
in	response	to	the	current	load.	One	strategy	is	to	double	the	number	of
threads	when	the	buffer	becomes	full,	and	halve	the	number	of	threads
when	the	buffer	becomes	empty.	Test	your	solution	using	a	real	Web
browser.
12.39	
♦♦♦♦
A	Web	proxy	is	a	program	that	acts	as	a	middleman	between	a	Web
server	and	browser.	Instead	of	contacting	the	server	directly	to	get	a	Web
page,	the	browser	contacts	the	proxy,	which	forwards	the	request	to	the
server.	When	the	server	replies	to	the	proxy,	the	proxy	sends	the	reply	to
the	browser.	For	this	lab,	you	will	write	a	simple	Web	proxy	that	filters	and
logs	requests:
A
.	
In	the	first	part	of	the	lab,	you	will	set	up	the	proxy	to	accept
requests,	parse	the	HTTP,	forward	the	requests	to	the	server,	and
return	the	results	to	the	browser.	Your	proxy	should	log	the	URLs
of	all	requests	in	a	log	file	on	disk,	and	it	should	also	block
requests	to	any	URL	contained	in	a	filter	file	on	disk.
B
.	
In	the	second	part	of	the	lab,	you	will	upgrade	your	proxy	to	deal
with	multiple	open	connections	at	once	by	spawning	a	separate
thread	to	handle	each	request.	While	your	proxy	is	waiting	for	a
remote	server	to	respond	to	a	request	so	that	it	can	serve	one</p>
<p>browser,	it	should	be	working	on	a	pending	request	from	another
browser.
Check	your	proxy	solution	using	a	real	Web	browser.</p>
<p>Solutions	to	Practice	Problems
Solution	to	Problem	
12.1	
(page
975
)
When	the	parent	forks	the	child,	it	gets	a	copy	of	the	connected
descriptor,	and	the	reference	count	for	the	associated	file	table	is
incremented	from	1	to	2.	When	the	parent	closes	its	copy	of	the
descriptor,	the	reference	count	is	decremented	from	2	to	1.	Since	the
kernel	will	not	close	a	file	until	the	reference	counter	in	its	file	table	goes
to	0,	the	child's	end	of	the	connection	stays	open.
Solution	to	Problem	
12.2	
(page
975
)
When	a	process	terminates	for	any	reason,	the	kernel	closes	all	open
descriptors.	Thus,	the	child's	copy	of	the	connected	file	descriptor	will	be
closed	automatically	when	the	child	exits.
Solution	to	Problem	
12.3	
(page</p>
<p>980
)
Recall	that	a	descriptor	is	ready	for	reading	if	a	request	to	read	1	byte
from	that	descriptor	would	not	block.	If	EOF	becomes	true	on	a
descriptor,	then	the	descriptor	is	ready	for	reading	because	the	read
operation	will	return	immediately	with	a	zero	return	code	indicating	EOF.
Thus,	typing	Ctrl+D	causes	the	
function	to	return	with	descriptor	0
in	the	ready	set.
Solution	to	Problem	
12.4	
(page
984
)
We	reinitialize	the	
variable	before	every	call	to	
because	it	serves	as	both	an	input	and	output	argument.	On	input,	it
contains	the	read	set.	On	output,	it	contains	the	ready	set.
Solution	to	Problem	
12.5	
(page
992
)
Since	threads	run	in	the	same	process,	they	all	share	the	same
descriptor	table.	No	matter	how	many	threads	use	the	connected
descriptor,	the	reference	count	for	the	connected	descriptor's	file	table	is
equal	to	1.	Thus,	a	single	
operation	is	sufficient	to	free	the	memory</p>
<p>resources	associated	with	the	connected	descriptor	when	we	are	through
with	it.
Solution	to	Problem	
12.6	
(page
995
)
The	main	idea	here	is	that	stack	variables	are	private,	whereas	global
and	static	variables	are	shared.	Static	variables	such	as	
are	a	little
tricky	because	the	sharing	is	limited	to	the	functions	within	their	scope—
in	this	case,	the	thread	routine.
A
.	
Here	is	the	table:
Variable	instance
Referenced	by
main	thread?
peer	thread	0?
peer	thread	1?
yes
yes
yes
no
yes
yes
yes
no
no
yes
yes
yes
no
yes
no
no
no
yes
Notes:</p>
<pre><code>A	global	variable	that	is	written	by	the	main	thread	and
</code></pre>
<p>read	by	the	peer	threads.
A	static	variable	with	only	one	instance	in	memory	that	is
read	and	written	by	the	two	peer	threads.
A	local	automatic	variable	stored	on	the	stack	of	the	main
thread.	Even	though	its	value	is	passed	to	the	peer	threads,
the	peer	threads	never	reference	it	on	the	stack,	and	thus	it	is
not	shared.
A	local	automatic	variable	stored	on	the	main	thread's
stack	and	referenced	indirectly	through	
by	both	peer
threads.
and	
Instances	of	a	local	automatic	variable
residing	on	the	stacks	of	peer	threads	0	and	1,	respectively.
B
.	
Variables	
,	and	
are	referenced	by	more	than	one
thread	and	thus	are	shared.
Solution	to	Problem	
12.7	
(page
998
)
The	important	idea	here	is	that	you	cannot	make	any	assumptions	about
the	ordering	that	the	kernel	chooses	when	it	schedules	your	threads.
Step
Thread
Instr.
1
2</p>
<p>1
1
H
—
—
0
2
1
L
0
—
0
3
2
H
—
—
0
4
2
L
—
0
0
5
2
U
—
1
0
6
2
S
—
1
1
7
1
U
1
—
1
8
1
S
1
—
1
9
1
T
1
—
1
10
2
T
—
1
1
Variable	
has	a	final	incorrect	value	of	1.
Solution	to	Problem	
12.8	
(page
1001
)
This	problem	is	a	simple	test	of	your	understanding	of	safe	and	unsafe
trajectories	in	progress	graphs.	Trajectories	such	as	A	and	C	that	skirt	the
critical	region	are	safe	and	will	produce	correct	results.
A
.	
H
,	
L
,	
U
,	
S
,	
H
,	
L
,	
U
,	
S
,	
T
,	
T
:	safe
1
1
2
2
2
2
1
1
1
2
1
1
1
1
2
2
2
2
2
1</p>
<p>B
.	
H
,	
L
,	
H
,	
L
,	
U
,	
S
,	
T
,	
U
,	
S
,	
T
:	unsafe
C
.	
H
,	
H
,	
L
,	
U
,	
S
,	
L
,	
U
,	
S
,	
T
,	
T
:	safe
Solution	to	Problem	
12.9	
(page
1006
)
A
.	
p
=	1,	
c
=	1,	
n
&gt;	1:	Yes,	the	mutex	semaphore	is	necessary
because	the	producer	and	consumer	can	concurrently	access	the
buffer.
B
.	
p
=	1,	
c
=	1,	
n
=	1:	No,	the	mutex	semaphore	is	not	necessary	in
this	case,	because	a	nonempty	buffer	is	equivalent	to	a	full	buffer.
When	the	buffer	contains	an	item,	the	producer	is	blocked.	When
the	buffer	is	empty,	the	consumer	is	blocked.	So	at	any	point	in
time,	only	a	single	thread	can	access	the	buffer,	and	thus	mutual
exclusion	is	guaranteed	without	using	the	mutex.
C
.	
p
&gt;	1,	
c
&gt;	1,	
n
=	1:	No,	the	mutex	semaphore	is	not	necessary	in
this	case	either,	by	the	same	argument	as	the	previous	case.
Solution	to	Problem	
12.10	
(page
1008
)
Suppose	that	a	particular	semaphore	implementation	uses	a	LIFO	stack
of	threads	for	each	semaphore.	When	a	thread	blocks	on	a	semaphore	in
a	
P
operation,	its	ID	is	pushed	onto	the	stack.	Similarly,	the	
V
operation
2
2
1
1
1
1
1
2
2
2
1
2
2
2
2
1
1
1
1
2</p>
<p>pops	the	top	thread	ID	from	the	stack	and	restarts	that	thread.	Given	this
stack	implementation,	an	adversarial	writer	in	its	critical	section	could
simply	wait	until	another	writer	blocks	on	the	semaphore	before	releasing
the	semaphore.	In	this	scenario,	a	waiting	reader	might	wait	forever	as
two	writers	passed	control	back	and	forth.
Notice	that	although	it	might	seem	more	intuitive	to	use	a	FIFO	queue
rather	than	a	LIFO	stack,	using	such	a	stack	is	not	incorrect	and	does	not
violate	the	semantics	of	the	
P
and	
V
operations.
Solution	to	Problem	
12.11	
(page
1020
)
This	problem	is	a	simple	sanity	check	of	your	understanding	of	speedup
and	parallel	efficiency:
Threads	(
t
)
1
2
4
Cores	(
p
)
1
2
4
Running	time	(
T
)
12
8
6
Speedup	(
S
)
1
1.5
2
Efficiency	(
E
)
100%
75%
50%
p
p
p</p>
<p>Solution	to	Problem	
12.12	
(page
1024
)
The	
function	is	not	reentrant,	because	each	invocation	shares
the	same	
variable	returned	by	the	
function.	However,	it	is
thread-safe	because	
the	accesses	to	the	shared	variable	are	protected
by	
P
and	
V
operations,	and	thus	are	mutually	exclusive.
Solution	to	Problem	
12.13	
(page
1026
)
If	we	free	the	block	immediately	after	the	call	to	
in	line	14,
then	we	will	introduce	a	new	race,	this	time	between	the	call	to	
in
the	main	thread	and	the	assignment	statement	in	line	24	of	the	thread
routine.
Solution	to	Problem	
12.14	
(page
1026
)
A
.	
Another	approach	is	to	pass	the	integer	
directly,	rather	than
passing	a	pointer	to	
:</p>
<p>In	the	thread	routine,	we	cast	the	argument	back	to	an	
and
assign	it	to	
:
B
.	
The	advantage	is	that	it	reduces	overhead	by	eliminating	the	calls
to	
and	
.	A	significant	disadvantage	is	that	it	assumes
that	pointers	are	at	least	as	large	as	
.	While	this	assumption
is	true	for	all	modern	systems,	it	might	not	be	true	for	legacy	or
future	systems.
Solution	to	Problem	
12.15	
(page
1029
)
A
.	
The	progress	graph	for	the	original	program	is	shown	in	
Figure
12.48
on	the	next	page.
B
.	
The	program	always	deadlocks,	since	any	feasible	trajectory	is
eventually	trapped	in	a	deadlock	state.
C
.	
To	eliminate	the	deadlock	potential,	initialize	the	binary	semaphore
to	1	instead	of	0.
D
.	
The	progress	graph	for	the	corrected	program	is	shown	in	
Figure
12.49
.</p>
<p>Figure	
12.48	
Progress	graph	for	a	program	that	deadlocks.</p>
<p>Figure	
12.49	
Progress	graph	for	the	corrected	deadlock-free
program.</p>
<p>Appendix	
A	
Error	Handling
Programmers	should	
always
check	the	error	codes	returned	by	system-
level	functions.	There	are	many	subtle	ways	that	things	can	go	wrong,
and	it	only	makes	sense	to	use	the	status	information	that	the	kernel	is
able	to	provide	us.	Unfortunately,	programmers	are	often	reluctant	to	do
error	checking	because	it	clutters	their	code,	turning	a	single	line	of	code
into	a	multi-line	conditional	statement.	Error	checking	is	also	confusing
because	different	functions	indicate	errors	in	different	ways.
We	were	faced	with	a	similar	problem	when	writing	this	text.	On	the	one
hand,	we	would	like	our	code	examples	to	be	concise	and	simple	to	read.
On	the	other	hand,	we	do	not	want	to	give	students	the	wrong	impression
that	it	is	OK	to	skip	error	checking.	To	resolve	these	issues,	we	have
adopted	an	approach	based	on	
error-handling	wrappers
that	was
pioneered	by	W.	Richard	Stevens	in	his	network	programming	text	[110].
The	idea	is	that	given	some	base	system-level	function	
,	we	define	a
wrapper	function	
with	identical	arguments,	but	with	the	first	letter
capitalized.	The	wrapper	calls	the	base	function	and	checks	for	errors.	If
it	detects	an	error,	the	wrapper	prints	an	informative	message	and
terminates	the	process.	Otherwise,	it	returns	to	the	caller.	Notice	that	if
there	are	no	errors,	the	wrapper	behaves	exactly	like	the	base	function.
Put	another	way,	if	a	program	runs	correctly	with	wrappers,	it	will	run
correctly	if	we	render	the	first	letter	of	each	wrapper	in	lowercase	and
recompile.</p>
<p>The	wrappers	are	packaged	in	a	single	source	file	(
)	that	is
compiled	and	linked	into	each	program.	A	separate	header	file	(
)
contains	the	function	prototypes	for	the	wrappers.
This	appendix	gives	a	tutorial	on	the	different	kinds	of	error	handling	in
Unix	systems	and	gives	examples	of	the	different	styles	of	error-handling
wrappers.	Copies	of	the	
and	
files	are	available	at	the
CS:APP	Web	site.</p>
<p>A.1	
Error	Handling	in	Unix	Systems
The	systems-level	function	calls	that	we	will	encounter	in	this	book	use
three	different	styles	for	returning	errors:	
Unix-style
,	
Posix-style
,	and	
GAI-
style
.
Unix-Style	Error	Handling
Functions	such	as	
and	
that	were	developed	in	the	early	days
of	Unix	(as	well	as	some	older	Posix	functions)	overload	the	function
return	value	with	both	error	codes	
and
useful	results.	For	example,	when
the	Unix-style	
function	encounters	an	error	(e.g.,	there	is	no	child
process	to	reap),	it	returns	-1	and	sets	the	global	variable	
to	an
error	code	that	indicates	the	cause	of	the	error.	If	
completes
successfully,	then	it	returns	the	useful	result,	which	is	the	PID	of	the
reaped	child.	Unix-style	error-handling	code	is	typically	of	the	following
form:</p>
<p>The	
function	returns	a	text	description	for	a	particular	value	of
.
Posix-Style	Error	Handling
Many	of	the	newer	Posix	functions	such	as	Pthreads	use	the	return	value
only	to	indicate	success	(zero)	or	failure	(nonzero).	Any	useful	results	are
returned	in	function	arguments	that	are	passed	by	reference.	We	refer	to
this	approach	as	
Posix-style	error	handling
.	For	example,	the	Posix-style
function	indicates	success	or	failure	with	its	return	value
and	returns	the	ID	of	the	newly	created	thread	(the	useful	result)	by
reference	in	its	first	argument.	Posix-style	error-handling	code	is	typically
of	the	following	form:
The	
function	returns	a	text	description	for	a	particular	value	of
.</p>
<p>GAI-Style	Error	Handling
The	
(GAI)	and	
functions	return	zero	on	success
and	a	nonzero	value	on	failure.	GAI	error-handling	code	is	typically	of	the
following	form:
The	
function	returns	a	text	description	for	a	particular	value
of	
.
Summary	of	Error-Reporting
Functions
Thoughout	this	book,	we	use	the	following	error-reporting	functions	to
accommodate	different	error-handling	styles.</p>
<p>As	their	names	suggest,	the	
,	and	
functions	report	Unix-style,	Posix-style,	and	GAI-style	errors	and	then
terminate.	The	
function	is	included	as	a	convenience	for
application	errors.	It	simply	prints	its	input	and	then	terminates.	
Figure
A.1
shows	the	code	for	the	error-reporting	functions.</p>
<h2>A.2	
Error-Handling	Wrappers
Here	are	some	examples	of	the	different	error-handling	wrappers.
Unix-style	error-handling	wrappers.	
Figure	
A.2
shows	the
wrapper	for	the	Unix-style	
function.	If	the	
returns	with	an
error,	the	wrapper	prints	an	informative	message	and	then	exits.
Otherwise,	it	returns	a	PID	to	the	caller.	
Figure	
A.3
shows	the
wrapper	for	the	Unix-style	kill	function.	Notice	that	this	function,	unlike
,	returns	void	on	success.
Posix-style	error-handling	wrappers.	
Figure	
A.4
shows	the
wrapper	for	the	Posix-style	
function.	Like	most	Posix-
style	functions,	it	does	not	overload	useful	results	with	error-return
codes,	so	the	wrapper	returns	void	on	success.
GAI-style	error-handling	wrappers.	
Figure	
A.5
shows	the	error-
handling	wrapper	for	the	GAI-style	
function.</h2>
<p>code/src/csapp.c</p>
<hr />
<h2>code/src/csapp.c
Figure	
A.1	
Error-reporting	functions.</h2>
<p>code/src/csapp.c</p>
<hr />
<h2>code/src/csapp.c
Figure	
A.2	
Wrapper	for	Unix-style	
function.</h2>
<h2 id="codesrccsappc"><a class="header" href="#codesrccsappc">code/src/csapp.c</a></h2>
<p>code/src/csapp.c</p>
<h2>Figure	
A.3	
Wrapper	for	Unix-style	
function.</h2>
<h2 id="codesrccsappc-1"><a class="header" href="#codesrccsappc-1">code/src/csapp.c</a></h2>
<h2>code/src/csapp.c
Figure	
A.4	
Wrapper	for	Posix-style	
function.</h2>
<p>code/src/csapp.c</p>
<hr />
<p>code/src/csapp.c
Figure	
A.5	
Wrapper	for	GAI-style	
function.</p>
<p>References
[1]	
Advanced	Micro	Devices,	Inc.	
Software	Optimization	Guide	for
AMD64	Processors
,	2005.	Publication	Number	25112.
[2]	
Advanced	Micro	Devices,	Inc.	
AMD64	Architecture	Programmer's
Manual,	Volume	1:	Application	Programming
,	2013.	Publication
Number	24592.
[3]	
Advanced	Micro	Devices,	Inc.	
AMD64	Architecture	Programmer's
Manual,	Volume	3:	General-Purpose	and	System	Instructions
,	2013.
Publication	Number	24594.
[4]	
Advanced	Micro	Devices,	Inc.	
AMD64	Architecture	Programmer's
Manual,	Volume	4:	128-Bit	and	256-Bit	Media	Instructions
,	2013.
Publication	Number	26568.
[5]	
K.	Arnold,	J.	Gosling,	and	D.	Holmes.	
The	Java	Programming
Language,	Fourth	Edition
.	Prentice	Hall,	2005.
[6]	
T.	Berners-Lee,	R.	Fielding,	and	H.	Frystyk.	Hypertext	transfer
protocol	-	HTTP/1.0.	RFC	1945,	1996.
[7]	
A.	Birrell.	An	introduction	to	programming	with	threads.	Technical
Report	35,	Digital	Systems	Research	Center,	1989.</p>
<p>[8]	
A.	Birrell,	M.	Isard,	C.	Thacker,	and	T.	Wobber.	A	design	for	high-
performance	flash	disks.	
SIGOPS	Operating	Systems	Review
41(2):88–93,	2007.
[9]	
G.	E.	Blelloch,	J.	T.	Fineman,	P.	B.	Gibbons,	and	H.	V.	Simhadri.
Scheduling	irregular	parallel	computations	on	hierarchical	caches.	In
Proceedings	of	the	23rd	Symposium	on	Parallelism	in	Algorithms	and
Architectures	(SPAA)
,	pages	355–366.	ACM,	June	2011.
[10]	
S.	Borkar.	Thousand	core	chips:	A	technology	perspective.	In
Proceedings	of	the	44th	Design	Automation	Conference
,	pages	746–
749.	ACM,	2007.
[11]	
D.	Bovet	and	M.	Cesati.	
Understanding	the	Linux	Kernel,	Third
Edition
.	O'Reilly	Media,	Inc.,	2005.
[12]	
A.	Demke	Brown	and	T.	Mowry.	Taming	the	memory	hogs:	Using
compiler-inserted	releases	to	manage	physical	memory	intelligently.
In	
Proceedings	of	the	4th	Symposium	on	Operating	Systems	Design
and	Implementation	(OSDI)
,	pages	31–44.	Usenix,	October	2000.
[13]	
R.	E.	Bryant.	Term-level	verification	of	a	pipelined	CISC
microprocessor.	Technical	Report	CMU-CS-05–195,	Carnegie	Mellon
University,	School	of	Computer	Science,	2005.
[14]	
R.	E.	Bryant	and	D.	R.	O'Hallaron.	Introducing	computer	systems
from	a	programmer's	perspective.	In	
Proceedings	of	the	Technical
Symposium	on	Computer	Science	Education	(SIGCSE)
,	pages	90–
94.	ACM,	February	2001.</p>
<p>[15]	
D.	Butenhof.	
Programming	with	Posix	Threads
.	Addison-Wesley,
1997.
[16]	
S.	Carson	and	P.	Reynolds.	The	geometry	of	semaphore	programs.
ACM	Transactions	on	Programming	Languages	and	Systems
9(1):25–53,	1987.
[17]	
J.	B.	Carter,	W.	C.	Hsieh,	L.	B.	Stoller,	M.	R.	Swanson,	L.	Zhang,	E.
L.	Brunvand,	A.	Davis,	C.-C.	Kuo,	R.	Kuramkote,	M.	A.	Parker,	L.
Schaelicke,	and	T.	Tateyama.	Impulse:	Building	a	smarter	memory
controller.	In	
Proceedings	of	the	5th	International	Symposium	on	High
Performance	Computer	Architecture	(HPCA)
,	pages	70–79.	ACM,
January	1999.
[18]	
K.	Chang,	D.	Lee,	Z.	Chishti,	A.	Alameldeen,	C.	Wilkerson,	Y.	Kim,
and	O.	Mutlu.	Improving	DRAM	performance	by	parallelizing
refreshes	with	accesses.	In	
Proceedings	of	the	20th	International
Symposium	on	High-Performance	Computer	Architecture	(HPCA)
.
ACM,	February	2014.
[19]	
S.	Chellappa,	F.	Franchetti,	and	M.	Püschel.	How	to	write	fast
numerical	code:	A	small	introduction.	In	
Generative	and
Transformational	Techniques	in	Software	Engineering	II
,	volume	5235
of	
Lecture	Notes	in	Computer	Science
,	pages	196–259.	Springer-
Verlag,	2008.
[20]	
P.	Chen,	E.	Lee,	G.	Gibson,	R.	Katz,	and	D.	Patterson.	RAID:	High-
performance,	reliable	secondary	storage.	
ACM	Computing	Surveys
26(2):145–185,	June	1994.</p>
<p>[21]	
S.	Chen,	P.	Gibbons,	and	T.	Mowry.	Improving	index	performance
through	prefetching.	In	
Proceedings	of	the	2001	ACM	SIGMOD
International	Conference	on	Management	of	Data
,	pages	235–246.
ACM,	May	2001.
[22]	
T.	Chilimbi,	M.	Hill,	and	J.	Larus.	Cache-conscious	structure	layout.
In	
Proceedings	of	the	1999	ACM	Conference	on	Programming
Language	Design	and	Implementation	(PLDI)
,	pages	1–12.	ACM,
May	1999.
[23]	
E.	Coffman,	M.	Elphick,	and	A.	Shoshani.	System	deadlocks.	
ACM
Computing	Surveys
3(2):67–78,	June	1971.
[24]	
D.	Cohen.	On	holy	wars	and	a	plea	for	peace.	
IEEE	Computer
14(10):48–54,	October	1981.
[25]	
P.	J.	Courtois,	F.	Heymans,	and	D.	L.	Parnas.	Concurrent	control
with	&quot;readers&quot;	and	&quot;writers.&quot;	
Communications	of	the	ACM
14(10):667–
668,	1971.
[26]	
C.	Cowan,	P.	Wagle,	C.	Pu,	S.	Beattie,	and	J.	Walpole.	Buffer
overflows:	Attacks	and	defenses	for	the	vulnerability	of	the	decade.	In
DARPA	Information	Survivability	Conference	and	Expo	(DISCEX)
,
volume	2,	pages	119–129,	March	2000.
[27]	
J.	H.	Crawford.	The	i486	CPU:	Executing	instructions	in	one	clock
cycle.	
IEEE	Micro
10(1):27–36,	February	1990.
[28]	
V.	Cuppu,	B.	Jacob,	B.	Davis,	and	T.	Mudge.	A	performance
comparison	of	contemporary	DRAM	architectures.	In	
Proceedings	of</p>
<p>the	26th	International	Symposium	on	Computer	Architecture	(ISCA)
,
pages	222–233,	ACM,	1999.
[29]	
B.	Davis,	B.	Jacob,	and	T.	Mudge.	The	new	DRAM	interfaces:
SDRAM,	RDRAM,	and	variants.	In	
Proceedings	of	the	3rd
International	Symposium	on	High	Performance	Computing	(ISHPC)
,
volume	1940	of	
Lecture	Notes	in	Computer	Science
,	pages	26–31.
Springer-Verlag,	October	2000.
[30]	
E.	Demaine.	Cache-oblivious	algorithms	and	data	structures.	In
Lecture	Notes	from	the	EEF	Summer	School	on	Massive	Data	Sets
.
BRICS,	University	of	Aarhus,	Denmark,	2002.
[31]	
E.	W.	Dijkstra.	Cooperating	sequential	processes.	Technical	Report
EWD-123,	Technological	University,	Eindhoven,	the	Netherlands,
1965.
[32]	
C.	Ding	and	K.	Kennedy.	Improving	cache	performance	of	dynamic
applications	through	data	and	computation	reorganizations	at	run
time.	In	
Proceedings	of	the	1999	ACM	Conference	on	Programming
Language	Design	and	Implementation	(PLDI)
,	pages	229–241.	ACM,
May	1999.
[33]	
M.	Dowson.	The	Ariane	5	software	failure.	
SIGSOFT	Software
Engineering	Notes
22(2):84,	1997.
[34]	
U.	Drepper.	User-level	IPv6	programming	introduction.	Available	at
http:/
/
www.akkadia.org/
drepper/
userapi-ipv6.html
,	2008.
[35]	
M.	W.	Eichen	and	J.	A.	Rochlis.	With	micro-	scope	and	tweezers:	An</p>
<p>analysis	of	the	Internet	virus	of	November,	1988.	In	
Proceedings	of
the	IEEE	Symposium	on	Research	in	Security	and	Privacy
,	pages
326–343.	IEEE,	1989.
[36]	
ELF-64	Object	File	Format,	Version	1.5	Draft	2
,	1998.	Available	at
http:/
/
www.uclibc.org/
docs/
elf-64-gen.pdf
.
[37]	
R.	Fielding,	J.	Gettys,	J.	Mogul,	H.	Frystyk,	L.	Masinter,	P.	Leach,
and	T.	Berners-Lee.	Hypertext	transfer	protocol	-	HTTP/1.1.	RFC
2616,	1999.
[38]	
M.	Frigo,	C.	E.	Leiserson,	H.	Prokop,	and	S.	Ramachandran.	Cache-
oblivious	algorithms.	In	
Proceedings	of	the	40th	IEEE	Symposium	on
Foundations	of	Computer	Science	(FOCS)
,	pages	285–297.	IEEE,
August	1999.
[39]	
M.	Frigo	and	V.	Strumpen.	The	cache	complexity	of	multithreaded
cache	oblivious	algorithms.	In	
Proceedings	of	the	18th	Symposium	on
Parallelism</p>
<p>in	Algorithms	and	Architectures	(SPAA)
,	pages	271–280.
ACM,	2006.
[40]	
G.	Gibson,	D.	Nagle,	K.	Amiri,	J.	Butler,	F.	Chang,	H.	Gobioff,	C.
Hardin,	E.	Riedel,	D.	Rochberg,	and	J.	Zelenka.	A	cost-effective,	high-
bandwidth	storage	architecture.	In	
Proceedings	of	the	8th
International	Conference	on	Architectural	Support	for	Programming
Languages	and	Operating	Systems	(ASPLOS)
,	pages	92–103.	ACM,
October	1998.
[41]	
G.	Gibson	and	R.	Van	Meter.	Network	attached	storage	architecture.
Communications	of	the	ACM
43(11):37–45,	November	2000.</p>
<p>[42]	
Google.	IPv6	Adoption.	Available	at	
http:/
/
www.google.com/
intl/
en/
ipv6/
statistics.html
.
[43]	
J.	Gustafson.	Reevaluating	Amdahl's	law.	
Communications	of	the
ACM
31(5):532–533,	August	1988.
[44]	
L.	Gwennap.	New	algorithm	improves	branch	prediction.
Microprocessor	Report
9(4),	March	1995.
[45]	
S.	P.	Harbison	and	G.	L.	Steele,	Jr.	
C,	A	Reference	Manual,	Fifth
Edition
.	Prentice	Hall,	2002.
[46]	
J.	L.	Hennessy	and	D.	A.	Patterson.	
Computer	Architecture:	A
Quantitative	Approach,	Fifth	Edition
.	Morgan	Kaufmann,	2011.
[47]	
M.	Herlihy	and	N.	Shavit.	
The	Art	of	Multi-	processor	Programming
.
Morgan	Kaufmann,	2008.
[48]	
C.	A.	R.	Hoare.	Monitors:	An	operating	system	structuring	concept.
Communications	of	the	ACM
17(10):549–557,	October	1974.
[49]	
Intel	Corporation.	
Intel	64	and	IA-32	Architectures	Optimization
Reference	Manual
.	Available	at	
http:/
/
www.intel.com/
content/
www/
us/
en/
processors/
architectures-software-developer-
manuals.html
.
[50]	
Intel	Corporation.	
Intel	64	and	IA-32	Architectures	Software
Developer's	Manual,	Volume	1:	Basic	Architecture
.	Available	at	
http:/
/
www.intel.com/
content/
www/
us/
en/
processors/
architectures-</p>
<p>software-developer-manuals.html
.
[51]	
Intel	Corporation.	
Intel	64	and	IA-32	Architectures	Software
Developer's	Manual,	Volume	2:	Instruction	Set	Reference.
Available
at	
http:/
/
www.intel.com/
content/
www/
us/
en/
processors/
architectures-software-developer-manuals.html
.
[52]	
Intel	Corporation.	
Intel	64	and	IA-32	Architectures	Software
Developer's	Manual,	Volume	3a:	System	Programming	Guide,	Part	1.
Available	at	
http:/
/
www.intel.com/
content/
www/
us/
en/
processors/
architectures-software-developer-manuals.html
.
[53]	
Intel	Corporation.	
Intel	Solid-State	Drive	730	Series:	Product
Specification.
Available	at	
http:/
/
www.intel.com/
content/
www/
us/
en/
solid-state-drives/
ssd-730-series-spec.html
.
[54]	
Intel	Corporation.	
Tool	Interface	Standards	Portable	Formats
Specification,	Version	1.1
,	1993.	Order	number	241597.
[55]	
F.	Jones,	B.	Prince,	R.	Norwood,	J.	Hartigan,	W.	Vogley,	C.	Hart,	and
D.	Bondurant.	Memory–-a	new	era	of	fast	dynamic	RAMs	(for	video
applications).	
IEEE	Spectrum
,	pages	43–45,	October	1992.
[56]	
R.	Jones	and	R.	Lins.	
Garbage	Collection:	Algorithms	for	Automatic
Dynamic	Memory	Management.
Wiley,	1996.
[57]	
M.	Kaashoek,	D.	Engler,	G.	Ganger,	H.	Briceo,	R.	Hunt,	D.	Maziers,
T.	Pinckney,	R.	Grimm,	J.	Jannotti,	and	K.	MacKenzie.	Application
performance	and	flexibility	on	Exokernel	systems.	In	
Proceedings	of</p>
<p>the	16th	ACM	Symposium	on	Operating	System	Principles	(SOSP)
,
pages	52–65.	ACM,	October	1997.
[58]	
R.	Katz	and	G.	Borriello.	
Contemporary	Logic	Design,	Second
Edition.
Prentice	Hall,	2005.
[59]	
B.	W.	Kernighan	and	R.	Pike.	
The	Practice	of	Programming.
Addison-Wesley,	1999.
[60]	
B.	Kernighan	and	D.	Ritchie.	
The	C	Programming	Language,	First
Edition.
Prentice	Hall,	1978.
[61]	
B.	Kernighan	and	D.	Ritchie.	
The	C	Programming	Language,	Second
Edition.
Prentice	Hall,	1988.
[62]	
Michael	Kerrisk.	
The	Linux	Programming	Interface.
No	Starch	Press,
2010.
[63]	
T.	Kilburn,	B.	Edwards,	M.	Lanigan,	and	F.	Sumner.	One-level
storage	system.	
IRE</p>
<p>Transactions	on	Electronic	Computers
EC-
11:223–235,	April	1962.
[64]	
D.	Knuth.	
The	Art	of	Computer	Programming,	Volume	1:
Fundamental	Algorithms,	Third	Edition.
Addison-Wesley,	1997.
[65]	
J.	Kurose	and	K.	Ross.	
Computer	Networking:	A	Top-Down
Approach,	Sixth	Edition.
Addison-Wesley,	2012.
[66]	
M.	Lam,	E.	Rothberg,	and	M.	Wolf.	The	cache	performance	and
optimizations	of	blocked	algorithms.	In	
Proceedings	of	the	4th</p>
<p>International	Conference	on	Architectural	Support	for	Programming
Languages	and	Operating	Systems	(ASPLOS)
,	pages	63–74.	ACM,
April	1991.
[67]	
D.	Lea.	A	memory	allocator.	Available	at	
http:/
/
gee.cs.oswego.edu/
dl/
html/
malloc.html
,	1996.
[68]	
C.	E.	Leiserson	and	J.	B.	Saxe.	Retiming	synchronous	circuitry.
Algorithmica
6(1–6),	June	1991.
[69]	
J.	R.	Levine.	
Linkers	and	Loaders.
Morgan	Kaufmann,	1999.
[70]	
David	Levinthal.	
Performance	Analysis	Guide	for	Intel	Core	i7
Processor	and	Intel	Xeon	5500	Processors.
Available	at
https://software.intel.com/sites/products/collateral/hpc/vtune/performance_analysis_guide.pdf
[71]	
C.	Lin	and	L.	Snyder.	
Principles	of	Parallel	Programming.
Addison
Wesley,	2008.
[72]	
Y.	Lin	and	D.	Padua.	Compiler	analysis	of	irregular	memory
accesses.	In	
Proceedings	of	the	2000	ACM	Conference	on
Programming	Language	Design	and	Implementation	(PLDI)
,	pages
157–168.	ACM,	June	2000.
[73]	
J.	L.	Lions.	Ariane	5	Flight	501	failure.	Technical	Report,	European
Space	Agency,	July	1996.
[74]	
S.	Macguire.	
Writing	Solid	Code.
Microsoft	Press,	1993.
[75]	
S.	A.	Mahlke,	W.	Y.	Chen,	J.	C.	Gyllenhal,	and	W.	W.	Hwu.	Compiler</p>
<p>code	transformations	for	superscalar-based	high-performance
systems.	In	
Proceedings	of	the	1992	ACM/IEEE	Conference	on
Supercomputing
,	pages	808–817.	ACM,	1992.
[76]	
E.	Marshall.	Fatal	error:	How	Patriot	over-	looked	a	Scud.	
Science
,
page	1347,	March	13,	1992.
[77]	
M.	Matz,	J.	Hubička,	A.	Jaeger,	and	M.	Mitchell.	System	V
application	binary	interface	AMD64	architecture	processor
supplement.	Technical	Report,	x86–64.org,	2013.	Available	at	
http:/
/
www.x86-64.org/
documentation_folder/
abi-0.99.pdf
.
[78]	
J.	Morris,	M.	Satyanarayanan,	M.	Conner,	J.	Howard,	D.	Rosenthal,
and	F.	Smith.	Andrew:	A	distributed	personal	computing	environment.
Communications	of	the	ACM
,	pages	184–201,	March	1986.
[79]	
T.	Mowry,	M.	Lam,	and	A.	Gupta.	Design	and	evaluation	of	a
compiler	algorithm	for	prefetching.	In	
Proceedings	of	the	5th
International	Conference	on	Architectural	Support	for	Programming
Languages	and	Operating	Systems	(ASPLOS)
,	pages	62–73.	ACM,
October	1992.
[80]	
S.	S.	Muchnick.	
Advanced	Compiler	Design	and	Implementation
.
Morgan	Kaufmann,	1997.
[81]	
S.	Nath	and	P.	Gibbons.	Online	maintenance	of	very	large	random
samples	on	flash	storage.	In	
Proceedings	of	VLDB
,	pages	970–983.
VLDB	Endowment,	August	2008.
[82]	
M.	Overton.	
Numerical	Computing	with	IEEE	Floating	Point</p>
<p>Arithmetic
.	SIAM,	2001.
[83]	
D.	Patterson,	G.	Gibson,	and	R.	Katz.	A	case	for	redundant	arrays	of
inexpensive	disks	(RAID).	In	
Proceedings	of	the	1998	ACM	SIGMOD
International	Conference	on	Management	of	Data
,	pages	109–116.
ACM,	June	1988.
[84]	
L.	Peterson	and	B.	Davie.	
Computer	Networks:	A	Systems
Approach,	Fifth	Edition
.	Morgan	Kaufmann,	2011.
[85]	
J.	Pincus	and	B.	Baker.	Beyond	stack	smashing:	Recent	advances	in
exploiting	buffer	overruns.	
IEEE	Security	and	Privacy
2(4):20–27,
2004.
[86]	
S.	Przybylski.	
Cache	and	Memory	Hierarchy	Design:	A	Performance-
Directed	Approach
.	Morgan	Kaufmann,	1990.
[87]	
W.	Pugh.	The	Omega	test:	A	fast	and	practical	integer	programming
algorithm	for	dependence	
analysis.	
Communications	of	the	ACM
35(8):102–114,	August	1992.
[88]	
W.	Pugh.	Fixing	the	Java	memory	model.	In	
Proceedings	of	the	ACM
Conference	on	Java	Grande
,	pages	89–98.	ACM,	June	1999.
[89]	
J.	Rabaey,	A.	Chandrakasan,	and	B.	Nikolic.	
Digital	Integrated
Circuits:	A	Design	Perspective,	Second	Edition
.	Prentice	Hall,	2003.
[90]	
J.	Reinders.	
Intel	Threading	Building	Blocks
.	O'Reilly,	2007.
[91]	
D.	Ritchie.	The	evolution	of	the	Unix	time-	sharing	system.	
AT&amp;T	Bell</p>
<p>Laboratories	Technical	Journal
63(6	Part	2):1577–1593,	October
1984.
[92]	
D.	Ritchie.	The	development	of	the	C	language.	In	
Proceedings	of
the	2nd	ACM	SIGPLAN	Conference	on	History	of	Programming
Languages
,	pages	201–208.	ACM,	April	1993.
[93]	
D.	Ritchie	and	K.	Thompson.	The	Unix	time-sharing	system.
Communications	of	the	ACM
17(7):365–367,	July	1974.
[94]	
M.	Satyanarayanan,	J.	Kistler,	P.	Kumar,	M.	Okasaki,	E.	Siegel,	and
D.	Steere.	Coda:	A	highly	available	file	system	for	a	distributed
workstation	environment.	
IEEE	Transactions	on	Computers
39(4):447–459,	April	1990.
[95]	
J.	Schindler	and	G.	Ganger.	Automated	disk	drive	characterization.
Technical	Report	CMU-	CS-99–176,	School	of	Computer	Science,
Carnegie	Mellon	University,	1999.
[96]	
F.	B.	Schneider	and	K.	P.	Birman.	The	monoculture	risk	put	into
context.	
IEEE	Security	and	Privacy
7(1):14–17,	January	2009.
[97]	
R.	C.	Seacord.	
Secure	Coding	in	C	and	C++,	Second	Edition
.
Addison-Wesley,	2013.
[98]	
R.	Sedgewick	and	K.	Wayne.	
Algorithms,	Fourth	Edition
.	Addison-
Wesley,	2011.
[99]	
H.	Shacham,	M.	Page,	B.	Pfaff,	E.-J.	Goh,	N.	Modadugu,	and	D.
Boneh.	On	the	effectiveness	of	address-space	randomization.	In</p>
<p>Proceedings	of	the	11th	ACM	Conference	on	Computer	and
Communications	Security	(CCS)
,	pages	298–307.	ACM,	2004.
[100]	
J.	P.	Shen	and	M.	Lipasti.	
Modern	Processor	Design:	Fundamentals
of	Superscalar	Processors.
McGraw	Hill,	2005.
[101]	
B.	Shriver	and	B.	Smith.	
The	Anatomy	of	a	High-Performance
Microprocessor:	A	Systems	Perspective.
IEEE	Computer	Society,
1998.
[102]	
A.	Silberschatz,	P.	Galvin,	and	G.	Gagne.	
Operating	Systems
Concepts,	Ninth	Edition.
Wiley,	2014.
[103]	
R.	Skeel.	Roundoff	error	and	the	Patriot	missile.	
SIAM	News
25(4):11,	July	1992.
[104]	
A.	Smith.	Cache	memories.	
ACM	Computing	Surveys
14(3),
September	1982.
[105]	
E.	H.	Spafford.	The	Internet	worm	program:	An	analysis.	Technical
Report	CSD-TR-823,	Department	of	Computer	Science,	Purdue
University,	1988.
[106]	
W.	Stallings.	
Operating	Systems:	Internals	and	Design	Principles,
Eighth	Edition.
Prentice	Hall,	2014.
[107]	
W.	R.	Stevens.	
TCP/IP	Illustrated,	Volume	3:	TCP	for	Transactions,
HTTP,	NNTP	and	the	Unix	Domain	Protocols.
Addison-Wesley,	1996.
[108]	
W.	R.	Stevens.	
Unix	Network	Programming:	Interprocess</p>
<p>Communications,	Second	Edition
,	volume	2.	Prentice	Hall,	1998.
[109]	
W.	R.	Stevens	and	K.	R.	Fall.	
TCP/IP	Illustrated,	Volume	1:	The
Protocols,	Second	Edition.
Addison-Wesley,	2011.
[110]	
W.	R.	Stevens,	B.	Fenner,	and	A.	M.	Rudoff.	
Unix	Network
Programming:	The	Sockets	Networking	API,	Third	Edition
,	volume	1.
Prentice	Hall,	2003.
[111]	
W.	R.	Stevens	and	S.	A.	Rago.	
Advanced	Programming	in	the	Unix
Environment,	Third	Edition.
Addison-Wesley,	2013.
[112]	
T.	Stricker	and	T.	Gross.	Global	address	space,	non-uniform
bandwidth:	A	memory	system	performance	characterization	of	parallel
systems.	In	
Proceedings	of	the	3rd	International	Symposium	on	High
Performance	Computer	Architecture	(HPCA)
,	pages	168–179.	IEEE,
February	1997.
[113]	
A.	S.	Tanenbaum	and	H.	Bos.	
Modern	Operating	Systems,	Fourth
Edition
.	Prentice	Hall,	2015.
[114]	
A.	S.	Tanenbaum	and	D.	Wetherall.	
Computer	Networks,	Fifth
Edition
.	Prentice	Hall,	2010.
[115]	
K.	P.	Wadleigh	and	I.	L.	Crawford.	
Software	Optimization	for	High-
Performance	Computing:	Creating	Faster	Applications
.	Prentice	Hall,
2000.
[116]	
J.	F.	Wakerly.	
Digital	Design	Principles	and	Practices,	Fourth
Edition
.	Prentice	Hall,	2005.</p>
<p>[117]	
M.	V.	Wilkes.	Slave	memories	and	dynamic	storage	allocation.	
IEEE
Transactions	on	Electronic	Computers
,	EC-14(2),	April	1965.
[118]	
P.Wilson,	M.	Johnstone,	M.	Neely,	and	D.	Boles.	Dynamic	storage
allocation:	A	survey	and	critical	review.	In	
International	Workshop	on
Memory	Management
,	volume	986	of	
Lecture	Notes	in	Computer
Science
,	pages	1–116.	Springer-Verlag,	1995.
[119]	
M.	Wolf	and	M.	Lam.	A	data	locality	algorithm.	In	
Proceedings	of	the
1991	ACM	Conference	on	Programming	Language	Design	and
Implementation	(PLDI)
,	pages	30–44,	June	1991.
[120]	
G.	R.	Wright	and	W.	R.	Stevens.	
TCP/IP	Illustrated,	Volume	2:	The
Implementation
.	Addison-Wesley,	1995.
[121]	
J.	Wylie,	M.	Bigrigg,	J.	Strunk,	G.	Ganger,	H.	Kiliccote,	and	P.
Khosla.	Survivable	information	storage	systems.	
IEEE	Computer
33:61–68,	August	2000.
[122]	
T.-Y.	Yeh	and	Y.	N.	Patt.	Alternative	implementation	of	two-level
adaptive	branch	prediction.	In	
Proceedings	of	the	19th	Annual
International	Symposium	on	Computer	Architecture	(ISCA)
,	pages
451–461.	ACM,	1998.</p>
<p>Index
Page	numbers	of	defining	references	are	
italicized
.	Entries	that	belong	to
a	hardware	or	software	system	are	followed	by	a	tag	in	brackets	that
identifies	the	system,	along	with	a	brief	description	to	jog	your	memory.
Here	is	the	list	of	tags	and	their	meanings.
[C]
C	language	construct
[C	Stdlib]
C	standard	library	function
[CS:APP]
Program	or	function	developed	in	this	text
[HCL]
HCL	language	construct
[Unix]
Unix	program,	function,	variable,	or	constant
[x86−64]
x86−64	machine-language	instruction
[Y86−64]
Y86−64	machine-language	instruction
!	[HCL]	
NOT
operation,	
373
$	for	immediate	operands,	
181
&amp;	[C]	address	of	operation
local	variables,	
248
logic	gates,	
373
pointers,	
48
,	
188
,	
257
,	
277</p>
<ul>
<li>[C]	dereference	pointer	operation,	
188
-&gt;	[C]	dereference	and	select	field	operation,	
266
.	(periods)	in	dotted-decimal	notation,	
926</li>
</ul>
<p>||	[HCL]	
OR
operation,	
373
&lt;	operator	for	left	hoinkies,	
909
&lt;&lt;	&quot;put	to&quot;	operator	(C++),	
890
&gt;	operator	for	right	hoinkies,	
909
&gt;&gt;	&quot;get	from&quot;	operator	(C++),	
890</p>
<ul>
<li>(two's-complement	addition),	
60
,	
90</li>
</ul>
<ul>
<li>(two's-complement	multiplication),	
60
,	
97
−
(two's-complement	negation),	
60
,	
95</li>
</ul>
<ul>
<li>(unsigned	addition),	
60
,	
85
,	
89</li>
</ul>
<ul>
<li>
<p>(unsigned	multiplication),	
60
,	
96
−
(unsigned	negation),	
60
,	
89
8086	microprocessor,	
167
8087	floating-point	coprocessor,	
109
,	
137
,	
167
80286	microprocessor,	
167
t
w
t
w
t
w
u
w
u
w
u
w</p>
<p>archive	files,	
686
object	file,	
673
Abel,	Niels	Henrik,	
89
abelian	group,	
89
ABI	(application	binary	interface),	
310
abort	exception	class,	
726
aborts,	
728
absolute	addressing	relocation	type,	
691
,	
693
–
694
absolute	pathnames,	
893
absolute	speedupof	parallel	programs,	
1019
abstract	operation	model	for	Core	i7,	
525
–
531
abstractions,	
27
[Unix]	wait	for	client	connection	request,	
933
,	
936
,	
936
–
937
access
disks,	
597
–
600
IA32	registers,	
179
–
180
main	memory,	
587
–
589
x86–64	registers
data	movement,	
182
–
189
operand	specifiers,	
180
–
182
access	permission	bits,	
894
access	time	for	disks,	
593
,	
593
–
595
accumulator	variable	expansion,	
570
accumulators,	multiple,	
536
–
541
Acorn	RISC	machine	(ARM)
ISAs,	
352
processor	architecture,	
363
actions,	signal,	
762
active	sockets,	
935</p>
</li>
</ul>
<p>actuator	arms,	
592
acyclic	networks,	
374
adapters,	
9
,	
597
ADD
[instruction	class]	add,	
192
function,	
981
,	
983
add	every	signal	to	signal	set	instruction,	
765
add	instruction,	
192
ADD
operation	in	execute	stage,	
408
add	signal	to	signal	set	instruction,	
765
[CS:APP]	CGI	adder,	
955
addition
floating	point,	
122
–
124
,	
302
two's	complement,	
90
,	
90
–
95
unsigned,	
84
–
90
,	
85
Y86–64,	
356
additive	inverse,	
52
[Y86–64]	add,	
356
,	
402
address	exceptions,	status	code	for,	
404
address	of	operator	(&amp;)	[C]
local	variables,	
248
logic	gates,	
373
pointers,	
48
,	
188
,	
257
,	
277
address	order	of	free	lists,	
863
address	partitioning	in	caches,	
615
,	
615
–
616
address-space	layout	randomization	(ASLR),	
285
,	
285
–
286
address	spaces,	
804
child	processes,	
741
linear,	
804
private,	
734</p>
<p>virtual,	
804
–
805
address	translation,	
804
caches	and	VM	integration,	
817
Core	i7,	
826
–
828
end-to-end,	
821
–
825
multi-level	page	tables,	
819
–
821
optimizing,	
830
overview,	
813
–
816
TLBs	for,	
817
–
819
addresses	and	addressing
byte	ordering,	
42
–
49
effective,	
690
flat,	
167
internet,	
922
invalid	address	status	code,	
364
I/O	devices,	
598
IP,	
924
,	
925
–
927
machine-level	programming,	
170
–
171
operands,	
181
out	of	bounds.	
See	
buffer	overflow
physical	vs.	virtual,	
803
–
804
pointers,	
257
,	
277
procedure	return,	
240
segmented,	
287
–
288
sockets,	
930
,	
933
–
934
structures,	
265
–
267
symbol	relocation,	
690
–
691
virtual,	
804
virtual	memory,	
34
Y86–64
,	
356
,	
359</p>
<p>addressing	modes,	
181
adjacency	matrices,	
660
[Y86–64]	status	code	indicating	invalid	address,	
364
Advanced	Micro	Devices	(AMD),	
165
,	
168
Intel	compatibility,	
168
x86–64.	
See	
x86–64	microprocessors
Advanced	Research	Projects	Administration	(ARPA),	
931
advanced	vector	extensions	(AVX)	instructions,	
294
,	
546
–
547
AFS	(Andrew	File	System),	
610
aggregate	data	types,	
171
aggregate	payloads,	
845
[x86–64]	low	order	8	of	register	
,	
180
[Unix]	schedule	alarm	to	self,	
762
,	
763
algebra,	Boolean,	
50
–
53
,	
52
aliasing	memory,	
499
,	
500
.align	directive,	
366
alignment
data,	
273
,	
273
–
276
memory	blocks,	
844
[Unix]	stack	storage	allocation	function,	
285
,	
290
,	
324
allocate	and	initialize	bounded	buffer	function,	
1007
allocate	heap	block	function,	
860
,	
861
allocate	heap	storage	function,	
840
allocated	bit,	
848
allocated	blocks
vs.	free,	
839
placement,	
849
allocation
blocks,	
860</p>
<p>dynamic	memory.	
See	
dynamic	memory	allocation
pages,	
810
allocators
block	allocation,	
860
block	freeing	and	coalescing,	
860
free	list	creation,	
857
–
859
free	list	manipulation,	
856
–
857
general	design,	
854
–
856
practice	problems,	
861
–
862
requirements	and	goals,	
844
–
845
styles,	
839
–
840
Alpha	(Compaq	Computer	Corp.)
RISC	processors,	
363
alternate	representations	of	signed	integers,	
68
[Y86–64]	function	code	for
instruction,	
404
ALUs	(arithmetic/logic	units),	
10
combinational	circuits,	
380
in	execute	stage,	
385
sequential	Y86–64	implementation,	
408
–
409
always	taken	branch	prediction	strategy,	
428
AMD	(Advanced	Micro	Devices),	
165
,	
168
Intel	compatibility,	
168
microprocessor	data	alignment,	
276
x86–64.	
See	
x86–64	microprocessors
Amdahl,	Gene,	
22
Amdahl's	law,	
22
,	
22
–
24
,	
562
,	
568
American	National	Standards	Institute	(ANSI),	
4
,	
35
ampersands	(&amp;)	address	operator,	
248</p>
<p>local	addresses,	
248
logic	gates,	
373
pointers,	
48
,	
188
,	
257
,	
277
AND
[instruction	class]	and,	
192
and	instruction,	
192
AND
operations
Boolean,	
51
–
52
execute	stage,	
408
HCL	expressions,	
374
–
375
logic	gates,	
373
logical,	
56
–
57
AND
packed	double	precision	instruction,	
305
AND
packed	single	precision	instruction,	
305
[Y86–64]	and,	
356
Andreesen,	Marc,	
949
Andrew	File	System	(AFS),	
610
anonymous	files,	
833
ANSI	(American	National	Standards	Institute),	
4
,	
35
[Y86–64]	status	code	for	normal	operation,	
363
[CS:APP]	reports	application	errors,	
1043
application	binary	interface	(ABI),	
310
applications,	loading	and	linking	shared	libraries	from,	
701
–
703
AR
Linux	archiver,	
686
,	
713
arbitrary	size	arithmetic,	
85
Archimedes,	
140
architecture
floating-point,	
293
,	
293
–
296
Y86.	
See	Y86–64	instruction	set	architecture
archives,	
686</p>
<p>areal	density	of	disks,	
591
areas
shared,	
834
swap,	
833
virtual	memory,	
830
arguments
function,	
750
Web	servers,	
953
–
954
arithmetic,	
33
,	
191
discussion,	
196
–
197
floating-point	code,	
302
–
304
integer.	
See	
integer	arithmetic
latency	and	issue	time,	
523
load	effective	address,	
191
–
193
pointers,	
257
–
258
,	
873
saturating,	
134
shift	operations,	
58
,	
104
–
106
,	
192
,	
194
–
196
special,	
197
–
200
unary	and	binary,	
194
–
196
arithmetic/logic	units	(ALUs),	
10
combinational	circuits,	
380
in	execute	stage,	
385
sequential	Y86–64	implementation,	
408
–
409
ARM	(Acorn	RISC	machine),	
43
ISAs,	
352
processor	architecture,	
363
ARM	A7	microprocessor,	
353
arms,	actuator,	
592
ARPA	(Advanced	Research	Projects	Administration),	
931</p>
<p>ARPANET,	
931
arrays,	
255
basic	principles,	
255
–
257
declarations,	
255
–
256
,	
263
DRAM,	
582
fixed-size,	
260
–
262
machine-code	representation,	
171
nested,	
258
–
260
pointer	arithmetic,	
257
–
258
pointer	relationships,	
48
,	
277
stride,	
606
variable-size,	
262
–
265
ASCII	standard,	
3
character	codes,	
49
limitations,	
50
function,	
1024
ASLR	(address-space	layout	randomization),	
285
,	
285
–
286
directive,	
178
directives,	
366
,	
5
,	
5
,	
164
,	
170
code,	
5
,	
164
with	C	programs,	
289
–
290
formatting,	
175
–
177
Y86–64,	
359
assembly	phase,	
5
associate	socket	address	with	descriptor	function,	
935
,	
935
associative	caches,	
624
–
626
associative	memory,	
625
associativity</p>
<p>caches,	
633
floating-point	addition,	
123
–
124
asterisks	(*)	dereference	pointer	operation,	
188
,	
257
,	
277
asymmetric	ranges	in	two's-complement	representation,	
66
,	
77
async-signal-safe	function,	
766
async-signal	safety,	
766
asynchronous	interrupts,	
726
atomic	reads	and	writes,	
770
ATT	assembly	code	format,	
177
,	
294
,	
311
argument	listing,	
306
condition	codes,	
201
–
202
instruction,	
199
vs.	Intel,	
177
operands,	
181
,	
192
Y86–64,	
356
automatic	variables,	
994
AVX	(advanced	vector	extensions)	instructions,	
276
,	
294
,	
546
–
547
[x86–64]	low	order	16	bits	of	register	
,	
180</p>
<p>B2T
(binary	to	two's-complement	conversion),	
60
,	
64
,	
72
,	
97
B2U
(binary	to	unsigned	conversion),	
60
,	
62
,	
72
,	
82
,	
97
background	processes,	
753
,	
753
–
756
backlogs	for	listening	sockets,	
935
backups	for	disks,	
611
backward	compatibility,	
35
backward	taken,	forward	not	taken	(BTFNT)	branch	prediction
strategy,	
428
bad	pointers	and	virtual	memory,	
870
–
871
[CS:APP]	improperly	synchronized	program,	
995
–
999
,	
996
bandwidth,	read,	
639
Barracuda	7400	drives,	
600
base	pointers,	
290
base	registers,	
181
[Unix]	Unix	shell	program,	
753
basic	blocks,	
569
Bell	Laboratories,	
35
Berkeley	sockets,	
932
Berners-Lee,	Tim,	
949
best-fit	block	placement	policy,	
849
,	
849
bi-endian	ordering	convention,	
43
biased	number	encoding,	
113
,	
113
–
117
biasing	in	division,	
106
big-endian	ordering	convention,	
42
,	
42
–
44
bigrams	statistics,	
565
bijections,	
64
,	
64
program,	
760
binary	files,	
3
,	
891
binary	notation,	
32</p>
<p>binary	points,	
110
,	
110
–
111
binary	representations
conversions
with	hexadecimal,	
36
–
37
signed	and	unsigned,	
70
–
76
to	two's	complement,	
64
,	
72
–
73
,	
97
to	unsigned,	
62
–
63
fractional,	
109
–
112
machine	language,	
194
binary	semaphores,	
1003
binary	tree	structure,	
270
–
271
[Unix]	associate	socket	address	with	descriptor,	
933
,	
935
,	
935
binding,	lazy,	
706
binutils	package,	
713
bistable	memory	cells,	
581
bit-level	operations,	
54
–
56
bit	representation	expansion,	
76
–
80
bit	vectors,	
51
,	
51
–
52
bits,	
3
overview,	
32
union	access	to,	
271
–
272
bitwise	operations,	
305
–
306
[x86–64]	low	order	8	of	register	
,	
180
block	and	unblock	signals	instruction,	
765
block	devices,	
892
block	offset	bits,	
616
block	pointers,	
856
block	size
caches,	
633</p>
<p>minimum,	
848
blocked	bit	vectors,	
759
blocked	signals,	
758
,	
759
,	
764
–
765
blocking
signals,	
764
–
765
for	temporal	locality,	
647
blocks
aligning,	
844
allocated,	
839
,	
849
vs.	cache	lines,	
634
caches,	
611
,	
611
–
612
,	
615
,	
633
coalescing,	
850
–
851
,	
860
epilogue,	
855
free	lists,	
847
–
849
freeing,	
860
heap,	
839
logical	disk,	
595
,	
595
–
596
,	
601
prologue,	
855
referencing	data	in,	
874
–
875
splitting,	
849
–
850
bodies,	response,	
952
[HCL]	bit-level	signal,	
374
Boole,	George,	
50
Boolean	algebra	and	functions,	
50
HCL,	
374
–
375
logic	gates,	
373
properties,	
52
working	with,	
50
–
53
Boolean	rings,	
52</p>
<p>bottlenecks,	
562
profilers,	
565
–
568
program	profiling,	
562
–
564
bottom	of	stack,	
190
boundary	tags,	
851
,	
851
–
854
,	
859
bounded	buffers,	
1004
,	
1005
–
1006
bounds
latency,	
518
,	
524
throughput,	
518
,	
524
[x86–64]	low	order	16	bits	of	register	
,	
180
[x86–64]	low	order	8	of	register	
,	
180
branch	prediction,	
519
,	
519
misprediction	handling,	
443
–
444
performance,	
549
–
553
Y86–64	pipelining,	
428
branch	prediction	logic,	
215
branches,	conditional,	
172
,	
209
assembly	form,	
211
condition	codes,	
201
–
202
condition	control,	
209
–
213
moves,	
214
–
220
,	
550
–
553
,	
232
–
238
command
in	
GDB
,	
280
with	
,	
233
break	
command	in	
GDB
,	
280
breakpoints,	
279
–
280
bridged	Ethernet,	
920
,	
921
bridges</p>
<p>Ethernet,	
920
I/O,	
587
browsers,	
948
,	
949
section,	
674
BTFNT	(backward	taken,	forward	not	taken)	branch	prediction
strategy,	
428
bubbles,	pipeline,	
434
,	
434
–
435
,	
459
–
460
buddies,	
865
buddy	systems,	
865
,	
865
buffer	overflow,	
279
execution	code	regions	limits	for,	
289
–
290
memory-related	bugs,	
871
overview,	
279
–
284
stack	corruption	detection	for,	
286
–
289
stack	randomization	for,	
284
–
286
vulnerabilities,	
7
buffered	I/O	functions,	
898
–
902
buffers
bounded,	
1004
,	
1005
–
1006
read,	
898
,	
900
–
901
store,	
557
–
558
streams,	
911
bus	transactions,	
587
buses,	
8
,	
587
designs,	
588
,	
598
I/O,	
596
memory,	
587
bypassing	for	data	hazards,	
436
–
439
byte	data	connections	in	hardware	diagrams,	
398</p>
<p>byte	order,	
42
–
49
disassembled	code,	
209
network,	
925
unions,	
272
bytes,	
3
,	
34
copying,	
133
range,	
36
register	operations,	
181
Y86	encoding,	
359
–
360
[x86–64]	low	order	16	bits	of	register	
,	
180</p>
<p>C	language
bit-level	operations,	
54
–
56
floating-point	representation,	
124
–
126
history,	
35
logical	operations,	
56
–
57
origins,	
4
shift	operations,	
57
–
59
static	libraries,	
684
–
688
C++	language,	
677
linker	symbols,	
680
objects,	
266
–
267
software	exceptions,	
723
–
724
,	
786
source	files,	
671
C	standard	library,	
4
–
5
,	
6
C11	standard,	
35
C90	standard,	
35
C99	standard,	
35
fixed	data	sizes,	
41
integral	data	types,	
67
cache	block	offset	(CO),	
823
cache	blocks,	
615
cache-friendly	code,	
633
–
639
,	
634
cache	lines
cache	sets,	
615
vs.	sets	and	blocks,	
634
cache-oblivious	algorithms,	
649
cache	set	index	(CI),	
823
cache	tags	(CT),	
823
cached	pages,	
806</p>
<p>caches	and	cache	memory,	
610
,	
615
address	translation,	
823
anatomy,	
631
associativity,	
633
cache-friendly	code,	
633
–
639
,	
634
data,	
520
,	
631
,	
631
direct-mapped.	
See	
direct-mapped	caches
DRAM,	
806
fully	associative,	
627
–
628
hits,	
612
importance,	
11
–
14
instruction,	
518
,	
631
,	
631
locality	in,	
605
,	
643
–
647
,	
810
managing,	
613
memory	mountains,	
639
–
643
misses,	
470
,	
612
,	
612
–
613
organization,	
615
–
617
overview,	
610
–
612
page	allocation,	
810
page	faults,	
808
,	
808
–
809
page	hits,	
808
page	tables,	
806
–
808
,	
807
performance,	
533
,	
631
–
633
,	
639
–
647
practice	problems,	
628
–
630
proxy,	
952
purpose,	
580
set	associative,	
624
,	
624
–
626
size,	
632
SRAM,	
806
symbols,	
617</p>
<p>virtual	memory	with,	
805
–
811
,	
817
write	issues,	
630
–
631
write	strategies,	
633
Y86–64	pipelining,	
469
–
470
[x86–64]	procedure	call,	
241
–
242
,	
357
[Y86–64]	instruction,	
404
,	
428
callee	procedures,	
251
callee-save	registers,	
251
,	
251
–
252
caller	procedures,	
251
caller-save	registers,	
251
,	
251
–
252
calling	environments,	
783
function	[C	Stdlib]	memory	allocation
declaration,	
134
dynamic	memory	allocation,	
841
security	vulnerability,	
100
–
101
[x86–64]	procedure	call,	
241
calls,	
17
,	
727
–
728
error	handling,	
737
–
738
Linux/x86–64	systems,	
730
–
731
in	performance,	
512
–
513
canary	values,	
286
–
287
canceling	mispredicted	branch	handling,	
444
capacity
caches,	
615
disks,	
591
,	
591
–
592
functional	units,	
523
capacity	misses,	
613
cards,	graphics,	
597
carriage	return	(CR)	characters,	
892</p>
<p>carry	flag	condition	code,	
201
,	
306
CAS	(column	access	strobe)	requests,	
583
case	expressions	in	HCL,	
378
,	
378
casting,	
44
explicit,	
75
floating-point	values,	
125
pointers,	
278
,	
854
signed	values,	
70
–
71
catching	signals,	
758
,	
761
,	
763
cells
DRAM,	
582
,	
583
SRAM,	
581
central	processing	units	(CPUs),	
9
,	
9
–
10
Core	i7.	
See	
Core	i7	microprocessors
early	instruction	sets,	
361
effective	cycle	time,	
602
embedded,	
363
Intel.	
See	
Intel	microprocessors
logic	design.	
See	
logic	design
many-core,	
471
multi-core,	
16
,	
24
–
25
,	
168
,	
605
,	
972
overview,	
352
–
354
pipelining.	
See	
pipelining
RAM,	
384
sequential	Y86	implementation.	
See	
sequential	Y86–64
implementation
superscalar,	
26
,	
471
,	
518
trends,	
602
–
603
Y86.	
See	
Y86–64	instruction	set	architecture
Cerf,	Vinton,	
931</p>
<p>CERT	(Computer	Emergency	Response	Team),	
100
[x86–64]	carry	flag	condition	code,	
201
,	
306
CGI	(common	gateway	interface)	program,	
953
,	
953
–
955
CGI	adder	function,	
955
chains,	proxy,	
952
[C]	data	types,	
40
,	
61
character	codes,	
49
character	devices,	
892
function,	
981
,	
984
child	processes,	
740
creating,	
741
–
743
default	behavior,	
744
error	conditions,	
745
–
746
exit	status,	
745
reaping,	
743
,	
743
–
749
function,	
746
–
749
CI	(cache	set	index),	
823
circuits
combinational,	
374
,	
374
–
380
retiming,	
421
sequential,	
381
CISC	(complex	instruction	set	computers),	
361
,	
361
–
363
[x86–64]	low	order	8	of	register	
,	
180
Clarke,	Dave,	
931
classes
data	hazards,	
435
exceptions,	
726
–
728
instructions,	
182
size,	
863</p>
<p>storage,	
994
–
995
clear	bit	in	descriptor	set	macro,	
978
clear	descriptor	set	macro,	
978
clear	signal	set	instruction,	
765
client-server	model,	
918
,	
918
–
919
[CS:APP]	T
INY
helper	function,	
959
–
960
clients
client-server	model,	
918
telnet,	
21
clock	signals,	
381
clocked	registers,	
401
–
402
clocking	in	logic	design,	
381
–
384
[Unix]	close	file,	
894
,	
894
–
895
close	operations	for	files,	
891
,	
894
–
895
close	shared	library	function,	
702
functions,	
905
[x86–64]	Sign	extend	
to	
,	
185
[x86–64]	move	if	unsigned	greater,	
217
[x86–64]	move	if	unsigned	greater	or	equal,	
217
[x86–64]	move	if	unsigned	less,	
217
[x86–64]	move	if	unsigned	less	or	equal,	
217
[Y86–64]	move	when	equal,	
357
[x86–64]	move	if	greater,	
217
,	
357
[x86–64]	move	if	greater	or	equal,	
217
,	
357
[x86–64]	move	if	less,	
217
,	
357
[x86–64]	move	if	less	or	equal,	
217
,	
357
[x86–64]	move	if	not	unsigned	greater,	
217
[x86–64]	move	if	unsigned	greater	or	equal,	
217</p>
<pre><code>[x86–64]	move	if	not	unsigned	less,	
</code></pre>
<p>217
[x86–64]	move	if	not	unsigned	less	or	equal,	
217
[x86–64]	move	if	not	equal,	
217
,	
357
[x86–64]	move	if	not	greater,	
217
[x86–64]	move	if	not	greater	or	equal,	
217
[x86–64]	move	if	not	less,	
217
[x86–64]	move	if	not	less	or	equal,	
217
[x86–64]	move	if	nonnegative,	
217
[x86–64]	move	if	not	zero,	
217
[x86–64]	move	if	even	parity,	
324
[x86–64]	move	if	negative,	
217
[x86–64]	move	if	zero,	
217
CMP
[instruction	class]	Compare,	
202
[x86–64]	compare	byte,	
202
[x86–64]	compare	double	word,	
202
[x86–64]	compare	double	word,	
202
[x86–64]	compare	word,	
202
cmtest	script,	
465
CO	(cache	block	offset),	
823
coalescing	blocks,	
860
with	boundary	tags,	
851
–
854
free,	
850
memory,	
847
Cocke,	John,	
361
code
performance	strategies,	
561
–
562
profilers,	
562
–
564
representing,	
49
–
50</p>
<p>self-modifying,	
435
Y86	instructions,	
358
,	
359
–
360
code	motion,	
508
code	segments,	
696
,	
697
–
698
Cohen,	Danny,	
43
cold	caches,	
612
cold	misses,	
612
Cold	War,	
931
collectors,	garbage,	
839
,	
866
basics,	
866
–
867
conservative,	
867
,	
869
–
870
Mark	&amp;	Sweep,	
867
–
870
column	access	strobe	(CAS)	requests,	
583</p>
<p>column-major	sum	function,	
636
combinational	circuits,	
374
,	
374
–
380
combinational	pipelines,	
412
–
414
,	
460
–
462
common	gateway	interface	(CGI)	program,	
953
,	
953
–
955
Compaq	Computer	Corp.	RISC	processors,	
363
compare	byte	instruction,	
202
compare	double	precision,	
306
compare	double	word	instruction,	
202
compare	instructions,	
202
compare	single	precision,	
306
compare	word	instruction,	
202
comparison	operations	for	floating-point	code,	
306
–
309
compilation	phase,	
5
compilation	systems,	
6
,	
6
–
7
compile	time,	
670
compile-time	interpositioning,	
708
–
709
compiler	drivers,	
4
,	
671
–
672
compilers,	
6
,	
164
optimizing	capabilities	and	limitations,	
498
–
502
process,	
169
–
170
purpose,	
171
complement	instruction,	
192
complex	instruction	set	computers	(CISC),	
361
,	
361
–
363
compulsory	misses,	
612
computation	stages	in	pipelining,	
421
–
422
computed	goto,	
233
Computer	Emergency	Response	Team	(CERT),	
100
computer	systems,	
2
concurrency,	
972
ECF	for,	
723</p>
<p>flow	synchronizing,	
776
–
778
and	parallelism,	
24
run,	
733
thread-level,	
24
–
26
concurrent	execution,	
733
concurrent	flow,	
733
,	
733
–
734
concurrent	processes,	
15
,	
16
concurrent	programming,	
972
–
973
deadlocks,	
1027
–
1030
with	I/O	multiplexing,	
978
–
985
library	functions	in,	
1024
–
1025
with	processes,	
973
–
977
races,	
1025
–
1027
reentrancy	issues,	
1023
–
1024
shared	variables,	
992
–
995
summary,	
1030
threads,	
985
–
992
for	parallelism,	
1013
–
1018
safety	issues,	
1020
–
1022
concurrent	programs,	
972
concurrent	servers,	
972
based	on	prethreading,	
1005
–
1013
based	on	processes,	
974
–
975
based	on	threads,	
991
–
992
condition	code	registers,	
171
hazards,	
435
SEQ	timing,	
401
–
402
condition	codes,	
201
,	
201
–
202
accessing,	
202
–
205
x86–64,	
201</p>
<p>Y86–64,	
355
–
357
condition	variables,	
1010
conditional	branches,	
172
,	
209
assembly	form,	
211
condition	codes,	
201
–
202
condition	control,	
209
–
213
moves,	
214
–
220
,	
550
–
553
,	
232
–
238
conflict	misses,	
613
,	
622
–
624
[Unix]	establish	connection	with	server,	
934
,	
934
–
935
connected	descriptors,	
936
,	
936
–
937
connections
EOF	on,	
948
Internet,	
925
,	
929
–
931
I/O	devices,	
596
–
597
persistent,	
952
conservative	garbage	collectors,	
867
,	
869
–
870
constant	words	in	Y86–64,	
359
constants
floating-point	code,	
304
–
305
free	lists,	
856
–
857
maximum	and	minimum	values,	
68
multiplication,	
101
–
103
for	ranges,	
67
–
68
Unix,	
746
content
dynamic,	
953
–
954
serving,	
949
Web,	
948
,	
949
–
950</p>
<p>context	switches,	
16
,	
736
–
737
contexts,	
736
processes,	
16
,	
732
thread,	
986
,	
993
command,	
280
Control	Data	Corporation	6600	processor,	
522
control	dependencies	in	pipelining,	
419
,	
429
control	flow,	
722
exceptional.	
See	
exceptional	control	flow	(ECF)
logical,	
732
,	
732
–
733
machine-language	procedures,	
239
control	hazards,	
429
control	logic	blocks,	
398
,	
398
,	
405
,	
426
control	logic	in	pipelining,	
455
control	mechanism	combinations,	
460
–
462
control	mechanisms,	
459
–
460
design	testing	and	verifying,	
465
implementation,	
462
–
464
special	cases,	
455
–
457
special	conditions,	
457
–
459
control	structures,	
200
–
201
condition	codes,	
200
–
205
conditional	branches,	
209
–
213
conditional	move	instructions,	
214
–
220
jumps,	
205
–
209
loops.	
See	
loops
statements,	
232
–
238
control	transfer,	
241
–
245
,	
722
controllers</p>
<p>disk,	
595
,	
595
–
596
I/O	devices,	
9
memory,	
583
,	
584
conventional	DRAMs,	
582
–
584
conversions
binary
with	hexadecimal,	
36
–
37
signed	and	unsigned,	
70
–
76
to	two's	complement,	
64
,	
72
–
73
,	
97
to	unsigned,	
62
–
63
floating	point,	
125
,	
296
–
301
lowercase,	
509
–
511
number	systems,	
36
–
39
convert	active	socket	to	listening	socket	function,	
935
convert	application-to-network	function,	
926
convert	double	precision	to	integer	instruction,	
297
convert	double	precision	to	quad-word	integer	instruction,	
297
convert	double	to	single	precision	instruction,	
299
convert	host	and	service	names	function,	
937
,	
937
–
940
convert	host-to-network	long	function,	
925
convert	host-to-network	short	function,	
925
convert	integer	to	double	precision	instruction,	
297
convert	integer	to	single	precision	instruction,	
297
convert	network-to-application	function,	
926
convert	network-to-host	long	function,	
925
convert	network-to-host	short	function,	
925
convert	packed	single	to	packed	double	precision	instruction,	
298
convert	quad-word	integer	to	double	precision	instruction,	
297
convert	quad-word	integer	to	single	precision	instruction,	
297
convert	quad	word	to	oct	word	instruction,	
198</p>
<p>convert	single	precision	to	integer	instruction,	
297
convert	single	precision	to	quad-word	integer	instruction,	
297
convert	single	to	double	precision	instruction,	
298
convert	socket	address	to	host	and	service	names	function,	
940
,
940
–
942
function,	
100
descriptor	function,	
909
function,	
86
–
87
copy-on-write	technique,	
835
,	
835
–
836
copying
bytes	in	memory,	
133
descriptor	tables,	
909
text	files,	
900
Core	2	microprocessors,	
168
,	
588
Core	i7	microprocessors,	
25
abstract	operation	model,	
525
–
531
address	translation,	
826
–
828
caches,	
631
Haswell,	
507
memory	mountain,	
641
Nehalem,	
168
page	table	entries,	
826
–
828
QuickPath	interconnect,	
588
virtual	memory,	
825
–
828
core	memory,	
757
cores	in	multi-core	processors,	
168
,	
605
,	
972
correct	signal	handling,	
770
–
774
counting	semaphores,	
1003
CPE	(cycles	per	element)	metric,	
502
,	
504
,	
507
–
508</p>
<pre><code>[CS:APP]	text	file	copy,	
</code></pre>
<p>900
CPI	(cycles	per	instruction)
five-stage	pipelines,	
471
in	performance	analysis,	
464
–
468
CPUs.	
See	
central	processing	units	(CPUs)
[x86–64]	convert	quad	word	to	oct	word,	
198
,	
199
CR	(carriage	return)	characters,	
892
CR3	register,	
826
Cray	1	supercomputer,	
353
create/change	environment	variable	function,	
752
create	child	process	function,	
740
,	
741
–
743
create	thread	function,	
988
critical	path	analysis,	
498
critical	paths,	
525
,	
529
critical	sections	in	progress	graphs,	
1000
CS:APP
header	files,	
746
wrapper	functions,	
738
,	
1041
[CS:APP]	CS:APP	wrapper	functions,	
738
,	
1041
[CS:APP]	CS:APP	header	file,	
738
,	
746
,	
1041
[Unix]	Unix	shell	program,	
753
CT	(cache	tags),	
823
ctest	script,	
465
function,	
1024
[CS:APP]	thread-safe	non-reentrant	wrapper	for	
,	
1022
Ctrl+C	key
nonlocal	jumps,	
785
signals,	
758
,	
761
,	
795
Ctrl+Z	key,	
761
,	
795</p>
<p>current	working	directory,	
892
[x86–64]	convert	double	to	single	precision,	
299
[x86–64]	convert	single	to	double	precision,	
298
cycles	per	element	(CPE)	metric,	
502
,	
504
,	
507
–
508
cycles	per	instruction	(CPI)
five-stage	pipelines,	
471
in	performance	analysis,	
464
–
468
cylinders
disk,	
591
spare,	
596
[x86–64]	low	order	16	bits	of	register	
,	
180</p>
<p>d-caches	(data	caches),	
520
,	
631
data
conditional	transfers,	
214
–
220
forwarding,	
436
–
439
,	
437
sizes,	
39
–
42
data	alignment,	
273
,	
273
–
276
data	caches	(d-caches),	
520
,	
631
data	dependencies	in	pipelining,	
419
,	
429
–
431
data-flow	graphs,	
525
–
530
data	formats	in	machine-level	programming,	
177
–
179
data	hazards,	
429
avoiding,	
441
–
444
classes,	
435
forwarding	for,	
436
–
439
load/use,	
439
–
441
stalling,	
433
–
436
Y86–64	pipelining,	
429
–
433
data	memory	in	SEQ	timing,	
401
data	movement	instructions,	
182
–
189
data	references
locality,	
606
–
607
PIC,	
704
–
705
section,	
674
data	segments,	
696
data	structures,	
265
data	alignment,	
273
–
276
structures,	
265
–
269
unions,	
269
–
273
data	transfer,	procedures,	
245
–
248</p>
<p>data	types.	
See	
types
database	transactions,	
919
datagrams,	
924
DDD
debugger	with	graphical	user	interface,	
279
DDR	SDRAM	(double	data-rate	synchronous	DRAM),	
586
deadlocks,	
1027
,	
1027
–
1030
deallocate	heap	storage	function,	
841
section,	
675
debugging,	
279
–
280
DEC
[instruction	class]	decrement,	
192
decimal	notation,	
32
decimal	system	conversions,	
37
–
39
declarations
arrays,	
255
–
256
,	
263
pointers,	
41
public	and	private,	
677
structures,	
265
–
269
unions,	
269
–
273
decode	stage
instruction	processing,	
385
,	
387
–
397
PIPE	processor,	
449
–
453
sequential	processing,	
400
Y86–64	implementation,	
406
–
408
Y86–64	pipelining,	
423
decoding	instructions,	
519
decrement	instruction,	
192
,	
194
deep	copies,	
1024
deep	pipelining,	
418
–
419
default	actions	with	signal,	
762</p>
<p>default	behavior	for	child	processes,	
744
default	function	code,	
404
deferred	coalescing,	
850
[C]	preprocessor	directive
command,	
280
delete	environment	variable	function,	
752
DELETE	method	in	HTTP,	
951
delete	signal	from	signal	set	instruction,	
765
delivering	signals,	
758
delivery	mechanisms	for	protocols,	
922
demand	paging,	
810
demand-zero	pages,	
833
demangling	process	(C++	and	Java),	
680
,	
680
denormalized	floating-point	value,	
114
,	
114
–
116
dependencies
control	in	pipelining	systems,	
419
,	
429
data	in	pipelining	systems,	
419
,	
429
–
431
reassociation	transformations,	
542
write/read,	
557
–
559</p>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->
                            <a rel="prev" href="../csapp/part7.html" class="mobile-nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                                <i class="fa fa-angle-left"></i>
                            </a>

                            <a rel="next" href="../csapp/part9.html" class="mobile-nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                                <i class="fa fa-angle-right"></i>
                            </a>

                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">
                    <a rel="prev" href="../csapp/part7.html" class="nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                        <i class="fa fa-angle-left"></i>
                    </a>

                    <a rel="next" href="../csapp/part9.html" class="nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                        <i class="fa fa-angle-right"></i>
                    </a>
            </nav>

        </div>




        <script>
            window.playground_copyable = true;
        </script>



        <script src="../clipboard.min.js"></script>
        <script src="../highlight.js"></script>
        <script src="../book.js"></script>

        <!-- Custom JS scripts -->


    </div>
    </body>
</html>
